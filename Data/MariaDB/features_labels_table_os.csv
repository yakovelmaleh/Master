,issue_key,issue_type,project_key,created,epic_link,has_change_story_point_sprint,summary,description,acceptance_criteria,summary_description_acceptance,original_story_points_sprint,creator,reporter,priority,num_all_changes,story_point,num_bugs_issue_link,num_comments,num_issue_links,num_sprints,num_changes_story_points_new,num_changes_summary_description_acceptance,num_sub_tasks,num_changes_sprint,num_changes_story_points_new_sprint,num_comments_before_sprint,num_comments_after_sprint,num_changes_text_before_sprint,num_changes_story_point_before_sprint,time_add_to_sprint,original_summary_sprint,original_description_sprint,original_acceptance_criteria_sprint,num_changes_summary_sprint,num_changes_description_sprint,num_changes_acceptance_criteria_sprint,num_different_words_all_text_sprint,num_ratio_words_all_text_sprint,original_summary_description_acceptance_sprint,num_changes_summary_description_acceptance_sprint,is_change_text_num_words_1,is_change_text_num_words_5,is_change_text_num_words_10,is_change_text_num_words_15,is_change_text_num_words_20,is_change_text_sp_sprint,time_until_add_to_sprint,num_issues_cretor_prev,num_unusable_issues_cretor_prev_text_word_1,num_unusable_issues_cretor_prev_text_word_1_ratio,num_unusable_issues_cretor_prev_text_word_5,num_unusable_issues_cretor_prev_text_word_5_ratio,num_unusable_issues_cretor_prev_text_word_10,num_unusable_issues_cretor_prev_text_word_10_ratio,num_unusable_issues_cretor_prev_text_word_15,num_unusable_issues_cretor_prev_text_word_15_ratio,num_unusable_issues_cretor_prev_text_word_20,num_unusable_issues_cretor_prev_text_word_20_ratio
0,CONJ-125,Task,CONJ,2014-12-03 16:15:16,,0,Optimize cached ResultSet memory footprint,"*How to reproduce:*
# Download the attached test case BigResultSetMemoryOptimization.java
# Use Oracle Java (perhaps also the JDK is required, I’m not sure)
# Run the test case (for example in Eclipse) as is to get a heap dump for 100K rows of ResultSet using MariaDB JDBC
# Edit the test case to use Mysql JDBC (see instructions in the comment in the file)
# Run the test case to get a heap dump using Mysql JDBC
# Open the two heap dumps side by side in Eclipse Memory Analyzer https://eclipse.org/mat/ (I recommend to download the standalone version).
# Run the Leak Suspects Report to get values for the “Retained Heap” column
# Open Histogram view (the second icon).
# Sort on the “Retained Heap” column.
# Select the row that represents the main java object holding the memory In MariaDB it is org.mariadb.jdbc.internal.common.queryresults.CachedSelectResult, in Mysql it is com.mysql.jdbc.JDBC4ResultSet.

*Actual:*
MariaDB is using 41 226 960 bytes distributed on about 1,5 M objects
Mysql is using 26 829 400 bytes distributed on about 0,9 M objects

In other words: MariaDB is using 53 % more memory and 67 % more objects for this specific table pattern.

*Expected:*
For MariaDB JDBC to be a drop in replacement for Mysql I would expect that the memory foot print (in terms of bytes and objects) to be about equivalent. Would it be possible to optimize this?

I have run into memory problems for this exact table structure but with 5,7M rows. I know about the stream ResultSet feature and that might be a workaround for me in this case, but so far I have not gotten it to work, perhaps due to some bug in either my code or in MariaDB JDBC. If I find a bug in MariaDB ResultSet stream feature I will write a separate ticket for this.
",,"Optimize cached ResultSet memory footprint $end$ *How to reproduce:*
# Download the attached test case BigResultSetMemoryOptimization.java
# Use Oracle Java (perhaps also the JDK is required, I’m not sure)
# Run the test case (for example in Eclipse) as is to get a heap dump for 100K rows of ResultSet using MariaDB JDBC
# Edit the test case to use Mysql JDBC (see instructions in the comment in the file)
# Run the test case to get a heap dump using Mysql JDBC
# Open the two heap dumps side by side in Eclipse Memory Analyzer https://eclipse.org/mat/ (I recommend to download the standalone version).
# Run the Leak Suspects Report to get values for the “Retained Heap” column
# Open Histogram view (the second icon).
# Sort on the “Retained Heap” column.
# Select the row that represents the main java object holding the memory In MariaDB it is org.mariadb.jdbc.internal.common.queryresults.CachedSelectResult, in Mysql it is com.mysql.jdbc.JDBC4ResultSet.

*Actual:*
MariaDB is using 41 226 960 bytes distributed on about 1,5 M objects
Mysql is using 26 829 400 bytes distributed on about 0,9 M objects

In other words: MariaDB is using 53 % more memory and 67 % more objects for this specific table pattern.

*Expected:*
For MariaDB JDBC to be a drop in replacement for Mysql I would expect that the memory foot print (in terms of bytes and objects) to be about equivalent. Would it be possible to optimize this?

I have run into memory problems for this exact table structure but with 5,7M rows. I know about the stream ResultSet feature and that might be a workaround for me in this case, but so far I have not gotten it to work, perhaps due to some bug in either my code or in MariaDB JDBC. If I find a bug in MariaDB ResultSet stream feature I will write a separate ticket for this.
 $acceptance criteria:$",,Lennart Schedin,Lennart Schedin,Minor,9,,0,4,2,1,0,0,0,,0,850,1,0,0,2015-08-06 18:46:51,Optimize cached ResultSet memory footprint,"*How to reproduce:*
# Download the attached test case BigResultSetMemoryOptimization.java
# Use Oracle Java (perhaps also the JDK is required, I’m not sure)
# Run the test case (for example in Eclipse) as is to get a heap dump for 100K rows of ResultSet using MariaDB JDBC
# Edit the test case to use Mysql JDBC (see instructions in the comment in the file)
# Run the test case to get a heap dump using Mysql JDBC
# Open the two heap dumps side by side in Eclipse Memory Analyzer https://eclipse.org/mat/ (I recommend to download the standalone version).
# Run the Leak Suspects Report to get values for the “Retained Heap” column
# Open Histogram view (the second icon).
# Sort on the “Retained Heap” column.
# Select the row that represents the main java object holding the memory In MariaDB it is org.mariadb.jdbc.internal.common.queryresults.CachedSelectResult, in Mysql it is com.mysql.jdbc.JDBC4ResultSet.

*Actual:*
MariaDB is using 41 226 960 bytes distributed on about 1,5 M objects
Mysql is using 26 829 400 bytes distributed on about 0,9 M objects

In other words: MariaDB is using 53 % more memory and 67 % more objects for this specific table pattern.

*Expected:*
For MariaDB JDBC to be a drop in replacement for Mysql I would expect that the memory foot print (in terms of bytes and objects) to be about equivalent. Would it be possible to optimize this?

I have run into memory problems for this exact table structure but with 5,7M rows. I know about the stream ResultSet feature and that might be a workaround for me in this case, but so far I have not gotten it to work, perhaps due to some bug in either my code or in MariaDB JDBC. If I find a bug in MariaDB ResultSet stream feature I will write a separate ticket for this.
",,0,0,0,0,0.0,"Optimize cached ResultSet memory footprint $end$ *How to reproduce:*
# Download the attached test case BigResultSetMemoryOptimization.java
# Use Oracle Java (perhaps also the JDK is required, I’m not sure)
# Run the test case (for example in Eclipse) as is to get a heap dump for 100K rows of ResultSet using MariaDB JDBC
# Edit the test case to use Mysql JDBC (see instructions in the comment in the file)
# Run the test case to get a heap dump using Mysql JDBC
# Open the two heap dumps side by side in Eclipse Memory Analyzer https://eclipse.org/mat/ (I recommend to download the standalone version).
# Run the Leak Suspects Report to get values for the “Retained Heap” column
# Open Histogram view (the second icon).
# Sort on the “Retained Heap” column.
# Select the row that represents the main java object holding the memory In MariaDB it is org.mariadb.jdbc.internal.common.queryresults.CachedSelectResult, in Mysql it is com.mysql.jdbc.JDBC4ResultSet.

*Actual:*
MariaDB is using 41 226 960 bytes distributed on about 1,5 M objects
Mysql is using 26 829 400 bytes distributed on about 0,9 M objects

In other words: MariaDB is using 53 % more memory and 67 % more objects for this specific table pattern.

*Expected:*
For MariaDB JDBC to be a drop in replacement for Mysql I would expect that the memory foot print (in terms of bytes and objects) to be about equivalent. Would it be possible to optimize this?

I have run into memory problems for this exact table structure but with 5,7M rows. I know about the stream ResultSet feature and that might be a workaround for me in this case, but so far I have not gotten it to work, perhaps due to some bug in either my code or in MariaDB JDBC. If I find a bug in MariaDB ResultSet stream feature I will write a separate ticket for this.
 $acceptance criteria:$",0,0,0,0,0,0,0,5906.52,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1,CONJ-141,Task,CONJ,2015-02-25 22:44:16,,0,Batch Statement Rewrite: Support for ON DUPLICATE KEY ...,"CONJ-99 introduced support for the rewriteBatchedStatements=true JDBC URL parameter.  This rewrites batched prepared statements into a single statement, when using PreparedStatement.addBatch() & executeBatch().

This works fine for INSERT statements which do not specify an ON DUPLICATE KEY UPDATE clause.  However for statements which do, the generated SQL cannot be parsed because the ON DUPLICATE ... clause is repeated for each set of values.  E.g. for a prepared statement such as:

{code:title=prep.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (?, ?) ON DUPLICATE KEY UPDATE col1 = VALUES(col1)
{code}

after setting the parameters multiple times and calling addBatch() then executeBatch(), the generated SQL looks like this:

{code:title=gen.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (1, 'a') ON DUPLICATE KEY UPDATE col1 = VALUES(col1),(2,'b') ON DUPLICATE KEY UPDATE col1 = VALUES(col1), ...
{code}

I've written a small patch with a test case to duplicate the issue, along with a suggested fix.  The test will fail against 1.1.8 but should pass with the suggested fix.",,"Batch Statement Rewrite: Support for ON DUPLICATE KEY ... $end$ CONJ-99 introduced support for the rewriteBatchedStatements=true JDBC URL parameter.  This rewrites batched prepared statements into a single statement, when using PreparedStatement.addBatch() & executeBatch().

This works fine for INSERT statements which do not specify an ON DUPLICATE KEY UPDATE clause.  However for statements which do, the generated SQL cannot be parsed because the ON DUPLICATE ... clause is repeated for each set of values.  E.g. for a prepared statement such as:

{code:title=prep.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (?, ?) ON DUPLICATE KEY UPDATE col1 = VALUES(col1)
{code}

after setting the parameters multiple times and calling addBatch() then executeBatch(), the generated SQL looks like this:

{code:title=gen.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (1, 'a') ON DUPLICATE KEY UPDATE col1 = VALUES(col1),(2,'b') ON DUPLICATE KEY UPDATE col1 = VALUES(col1), ...
{code}

I've written a small patch with a test case to duplicate the issue, along with a suggested fix.  The test will fail against 1.1.8 but should pass with the suggested fix. $acceptance criteria:$",,Ben Rowland,Ben Rowland,Minor,6,,0,1,0,1,0,0,0,,0,850,0,0,0,2015-08-06 18:56:07,Batch Statement Rewrite: Support for ON DUPLICATE KEY ...,"CONJ-99 introduced support for the rewriteBatchedStatements=true JDBC URL parameter.  This rewrites batched prepared statements into a single statement, when using PreparedStatement.addBatch() & executeBatch().

This works fine for INSERT statements which do not specify an ON DUPLICATE KEY UPDATE clause.  However for statements which do, the generated SQL cannot be parsed because the ON DUPLICATE ... clause is repeated for each set of values.  E.g. for a prepared statement such as:

{code:title=prep.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (?, ?) ON DUPLICATE KEY UPDATE col1 = VALUES(col1)
{code}

after setting the parameters multiple times and calling addBatch() then executeBatch(), the generated SQL looks like this:

{code:title=gen.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (1, 'a') ON DUPLICATE KEY UPDATE col1 = VALUES(col1),(2,'b') ON DUPLICATE KEY UPDATE col1 = VALUES(col1), ...
{code}

I've written a small patch with a test case to duplicate the issue, along with a suggested fix.  The test will fail against 1.1.8 but should pass with the suggested fix.",,0,0,0,0,0.0,"Batch Statement Rewrite: Support for ON DUPLICATE KEY ... $end$ CONJ-99 introduced support for the rewriteBatchedStatements=true JDBC URL parameter.  This rewrites batched prepared statements into a single statement, when using PreparedStatement.addBatch() & executeBatch().

This works fine for INSERT statements which do not specify an ON DUPLICATE KEY UPDATE clause.  However for statements which do, the generated SQL cannot be parsed because the ON DUPLICATE ... clause is repeated for each set of values.  E.g. for a prepared statement such as:

{code:title=prep.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (?, ?) ON DUPLICATE KEY UPDATE col1 = VALUES(col1)
{code}

after setting the parameters multiple times and calling addBatch() then executeBatch(), the generated SQL looks like this:

{code:title=gen.sql|borderStyle=solid}
INSERT INTO my_table (pkey, col1) VALUES (1, 'a') ON DUPLICATE KEY UPDATE col1 = VALUES(col1),(2,'b') ON DUPLICATE KEY UPDATE col1 = VALUES(col1), ...
{code}

I've written a small patch with a test case to duplicate the issue, along with a suggested fix.  The test will fail against 1.1.8 but should pass with the suggested fix. $acceptance criteria:$",0,0,0,0,0,0,0,3884.18,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
2,CONJ-145,Task,CONJ,2015-03-06 23:13:49,CONJ-196,0,Support for load balance URLs,"We currently use the jdbc:mysql:loadbalance feature provided by MySQL Connector/J to connect to a collection of MariaDB replicas. We would like to be able to do the same with the MariaDB JDBC driver, so that we can switch between the two drivers without introducing a new load balancing service such as HAProxy.

http://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-managing-load-balanced-connections.html",,"Support for load balance URLs $end$ We currently use the jdbc:mysql:loadbalance feature provided by MySQL Connector/J to connect to a collection of MariaDB replicas. We would like to be able to do the same with the MariaDB JDBC driver, so that we can switch between the two drivers without introducing a new load balancing service such as HAProxy.

http://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-managing-load-balanced-connections.html $acceptance criteria:$",,Jeff Flanigan,Jeff Flanigan,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2015-08-06 18:56:54,Support for load balance URLs,"We currently use the jdbc:mysql:loadbalance feature provided by MySQL Connector/J to connect to a collection of MariaDB replicas. We would like to be able to do the same with the MariaDB JDBC driver, so that we can switch between the two drivers without introducing a new load balancing service such as HAProxy.

http://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-managing-load-balanced-connections.html",,0,0,0,0,0.0,"Support for load balance URLs $end$ We currently use the jdbc:mysql:loadbalance feature provided by MySQL Connector/J to connect to a collection of MariaDB replicas. We would like to be able to do the same with the MariaDB JDBC driver, so that we can switch between the two drivers without introducing a new load balancing service such as HAProxy.

http://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-managing-load-balanced-connections.html $acceptance criteria:$",0,0,0,0,0,0,0,3667.72,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
3,CONJ-163,Task,CONJ,2015-06-24 04:31:56,,0,make column label name display instead column name  when useOldAliasMetadataBehavior option true,"Like CONJ-149, column alias display feature need.

as_is

{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_1

{code}

i need feature about useOldAliasMetadataBehavior option (like CONJ-149 TABLE)
when useOldAliasMetadataBehavior  is true 


{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_A
{code}


that make column label name display instead column name  when useOldAliasMetadataBehavior option true

how about my idea, 
and sample my code is below

MySQLResultSetMetaData.java
{code:java}
 public String getColumnName(final int column) throws SQLException {
        String s =  getColumnInformation(column).getOriginalName();      
+        if (returnTableAlias == true)    // if useOldAliasMetadataBehavior=true then getColumnName return getCoulmnLabel
+         s =  getColumnLabel(column); 
      
        if ("""".equals(s))  // odd things that are no columns, e.g count(*)
            s =  getColumnLabel(column);
        return s;
    }
{code}",,"make column label name display instead column name  when useOldAliasMetadataBehavior option true $end$ Like CONJ-149, column alias display feature need.

as_is

{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_1

{code}

i need feature about useOldAliasMetadataBehavior option (like CONJ-149 TABLE)
when useOldAliasMetadataBehavior  is true 


{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_A
{code}


that make column label name display instead column name  when useOldAliasMetadataBehavior option true

how about my idea, 
and sample my code is below

MySQLResultSetMetaData.java
{code:java}
 public String getColumnName(final int column) throws SQLException {
        String s =  getColumnInformation(column).getOriginalName();      
+        if (returnTableAlias == true)    // if useOldAliasMetadataBehavior=true then getColumnName return getCoulmnLabel
+         s =  getColumnLabel(column); 
      
        if ("""".equals(s))  // odd things that are no columns, e.g count(*)
            s =  getColumnLabel(column);
        return s;
    }
{code} $acceptance criteria:$",,seung hoon yoo,seung hoon yoo,Major,5,,1,3,1,1,0,0,0,,0,850,0,0,0,2015-08-06 18:58:17,make column label name display instead column name  when useOldAliasMetadataBehavior option true,"Like CONJ-149, column alias display feature need.

as_is

{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_1

{code}

i need feature about useOldAliasMetadataBehavior option (like CONJ-149 TABLE)
when useOldAliasMetadataBehavior  is true 


{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_A
{code}


that make column label name display instead column name  when useOldAliasMetadataBehavior option true

how about my idea, 
and sample my code is below

MySQLResultSetMetaData.java
{code:java}
 public String getColumnName(final int column) throws SQLException {
        String s =  getColumnInformation(column).getOriginalName();      
+        if (returnTableAlias == true)    // if useOldAliasMetadataBehavior=true then getColumnName return getCoulmnLabel
+         s =  getColumnLabel(column); 
      
        if ("""".equals(s))  // odd things that are no columns, e.g count(*)
            s =  getColumnLabel(column);
        return s;
    }
{code}",,0,0,0,0,0.0,"make column label name display instead column name  when useOldAliasMetadataBehavior option true $end$ Like CONJ-149, column alias display feature need.

as_is

{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_1

{code}

i need feature about useOldAliasMetadataBehavior option (like CONJ-149 TABLE)
when useOldAliasMetadataBehavior  is true 


{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_A
{code}


that make column label name display instead column name  when useOldAliasMetadataBehavior option true

how about my idea, 
and sample my code is below

MySQLResultSetMetaData.java
{code:java}
 public String getColumnName(final int column) throws SQLException {
        String s =  getColumnInformation(column).getOriginalName();      
+        if (returnTableAlias == true)    // if useOldAliasMetadataBehavior=true then getColumnName return getCoulmnLabel
+         s =  getColumnLabel(column); 
      
        if ("""".equals(s))  // odd things that are no columns, e.g count(*)
            s =  getColumnLabel(column);
        return s;
    }
{code} $acceptance criteria:$",0,0,0,0,0,0,0,1046.43,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
4,CONJ-177,Task,CONJ,2015-08-05 13:17:54,,0,new high availability option : assureReadOnly,"When in a multiMaster cluster, when setting connection.setReadOnly(XXX), if database allow it, a SET SESSION TRANSACTION READ ONLY / READ WRITE depending on the XXX value will be executed. 

That's permit to assure never execute write query when in read only mode.
The downside of this is a query is executing every change of connection.setReadOnly() just to assure that. 

A new option assureReadOnly can be created to tell if this can be avoid.",,"new high availability option : assureReadOnly $end$ When in a multiMaster cluster, when setting connection.setReadOnly(XXX), if database allow it, a SET SESSION TRANSACTION READ ONLY / READ WRITE depending on the XXX value will be executed. 

That's permit to assure never execute write query when in read only mode.
The downside of this is a query is executing every change of connection.setReadOnly() just to assure that. 

A new option assureReadOnly can be created to tell if this can be avoid. $acceptance criteria:$",,Diego Dupin,Diego Dupin,Minor,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2015-08-06 18:46:56,new high availability option : assureReadOnly,"When in a multiMaster cluster, when setting connection.setReadOnly(XXX), if database allow it, a SET SESSION TRANSACTION READ ONLY / READ WRITE depending on the XXX value will be executed. 

That's permit to assure never execute write query when in read only mode.
The downside of this is a query is executing every change of connection.setReadOnly() just to assure that. 

A new option assureReadOnly can be created to tell if this can be avoid.",,0,0,0,0,0.0,"new high availability option : assureReadOnly $end$ When in a multiMaster cluster, when setting connection.setReadOnly(XXX), if database allow it, a SET SESSION TRANSACTION READ ONLY / READ WRITE depending on the XXX value will be executed. 

That's permit to assure never execute write query when in read only mode.
The downside of this is a query is executing every change of connection.setReadOnly() just to assure that. 

A new option assureReadOnly can be created to tell if this can be avoid. $acceptance criteria:$",0,0,0,0,0,0,0,29.4833,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
5,CONJ-22,Epic,CONJ,2013-02-06 22:01:47,,0,Java Client library does not support useServerPrepStmts,"To enable prepared statements on the server side, MySQL Connector/J provides {{useServerPrepStmts}} parameter.
It doesn't work for MariaDB client. If there is a way to get prepared statements work through MariaDB client, I haven't yet found it.

The provided test case checks the global value of {{Prepared_stmt_count}} status variable, then attempts to prepare a statement on a connection with {{useServerPrepStmts}}, then checks the variable value again.

Output with MariaDB client library 1.1.0:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 0
{noformat}

With MySQL Connector/J 5.1.23:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 1
{noformat}

Test case:
{code:java}
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.PreparedStatement;


public class BugPreparedStatement
{
  public static void main (String argv[])  
  {
    try {

      Statement status = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test"",""root"","""").createStatement();

      ResultSet rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count before prepare: "" + rs.getInt(2));  
      }

      PreparedStatement pst = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test?useServerPrepStmts=true"",""root"","""").prepareStatement(""select 1"");

      rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count after prepare: "" + rs.getInt(2));  
      }
    }
    catch (Exception e)
    {
      System.out.println(""Exception: "" + e + ""\n"");
      e.printStackTrace();
    }
  } 
}

{code}",,"Java Client library does not support useServerPrepStmts $end$ To enable prepared statements on the server side, MySQL Connector/J provides {{useServerPrepStmts}} parameter.
It doesn't work for MariaDB client. If there is a way to get prepared statements work through MariaDB client, I haven't yet found it.

The provided test case checks the global value of {{Prepared_stmt_count}} status variable, then attempts to prepare a statement on a connection with {{useServerPrepStmts}}, then checks the variable value again.

Output with MariaDB client library 1.1.0:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 0
{noformat}

With MySQL Connector/J 5.1.23:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 1
{noformat}

Test case:
{code:java}
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.PreparedStatement;


public class BugPreparedStatement
{
  public static void main (String argv[])  
  {
    try {

      Statement status = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test"",""root"","""").createStatement();

      ResultSet rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count before prepare: "" + rs.getInt(2));  
      }

      PreparedStatement pst = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test?useServerPrepStmts=true"",""root"","""").prepareStatement(""select 1"");

      rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count after prepare: "" + rs.getInt(2));  
      }
    }
    catch (Exception e)
    {
      System.out.println(""Exception: "" + e + ""\n"");
      e.printStackTrace();
    }
  } 
}

{code} $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,16,,1,10,3,1,0,0,0,,0,850,5,0,0,2015-08-06 18:54:34,Java Client library does not support useServerPrepStmts,"To enable prepared statements on the server side, MySQL Connector/J provides {{useServerPrepStmts}} parameter.
It doesn't work for MariaDB client. If there is a way to get prepared statements work through MariaDB client, I haven't yet found it.

The provided test case checks the global value of {{Prepared_stmt_count}} status variable, then attempts to prepare a statement on a connection with {{useServerPrepStmts}}, then checks the variable value again.

Output with MariaDB client library 1.1.0:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 0
{noformat}

With MySQL Connector/J 5.1.23:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 1
{noformat}

Test case:
{code:java}
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.PreparedStatement;


public class BugPreparedStatement
{
  public static void main (String argv[])  
  {
    try {

      Statement status = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test"",""root"","""").createStatement();

      ResultSet rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count before prepare: "" + rs.getInt(2));  
      }

      PreparedStatement pst = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test?useServerPrepStmts=true"",""root"","""").prepareStatement(""select 1"");

      rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count after prepare: "" + rs.getInt(2));  
      }
    }
    catch (Exception e)
    {
      System.out.println(""Exception: "" + e + ""\n"");
      e.printStackTrace();
    }
  } 
}

{code}",,0,0,0,0,0.0,"Java Client library does not support useServerPrepStmts $end$ To enable prepared statements on the server side, MySQL Connector/J provides {{useServerPrepStmts}} parameter.
It doesn't work for MariaDB client. If there is a way to get prepared statements work through MariaDB client, I haven't yet found it.

The provided test case checks the global value of {{Prepared_stmt_count}} status variable, then attempts to prepare a statement on a connection with {{useServerPrepStmts}}, then checks the variable value again.

Output with MariaDB client library 1.1.0:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 0
{noformat}

With MySQL Connector/J 5.1.23:

{noformat}
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 1
{noformat}

Test case:
{code:java}
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.PreparedStatement;


public class BugPreparedStatement
{
  public static void main (String argv[])  
  {
    try {

      Statement status = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test"",""root"","""").createStatement();

      ResultSet rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count before prepare: "" + rs.getInt(2));  
      }

      PreparedStatement pst = DriverManager.getConnection(""jdbc:mysql://localhost:3306/test?useServerPrepStmts=true"",""root"","""").prepareStatement(""select 1"");

      rs = status.executeQuery(""show global status like 'Prepared_stmt_count'"");
      if (rs.first()) {
        System.out.println(""Prepared_stmt_count after prepare: "" + rs.getInt(2));  
      }
    }
    catch (Exception e)
    {
      System.out.println(""Exception: "" + e + ""\n"");
      e.printStackTrace();
    }
  } 
}

{code} $acceptance criteria:$",0,0,0,0,0,0,0,21860.9,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
6,CONJ-26,Epic,CONJ,2013-02-14 22:17:15,,0,[Feature Request] Implement configurable fetch size and fetch direction for Statement/ResultSet,"As discussed earlier, currently fetch size is ""one or all"". It would be good to implement it fully.
Also, setFetchDirection for ResultSet and Statement are currently just stubs.",,"[Feature Request] Implement configurable fetch size and fetch direction for Statement/ResultSet $end$ As discussed earlier, currently fetch size is ""one or all"". It would be good to implement it fully.
Also, setFetchDirection for ResultSet and Statement are currently just stubs. $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Minor,15,,0,7,3,1,0,0,0,,0,850,6,0,0,2015-08-06 18:55:22,[Feature Request] Implement configurable fetch size and fetch direction for Statement/ResultSet,"As discussed earlier, currently fetch size is ""one or all"". It would be good to implement it fully.
Also, setFetchDirection for ResultSet and Statement are currently just stubs.",,0,0,0,0,0.0,"[Feature Request] Implement configurable fetch size and fetch direction for Statement/ResultSet $end$ As discussed earlier, currently fetch size is ""one or all"". It would be good to implement it fully.
Also, setFetchDirection for ResultSet and Statement are currently just stubs. $acceptance criteria:$",0,0,0,0,0,0,0,21668.6,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
7,CONJ-322,Task,CONJ,2016-07-08 11:14:37,,0,ResultSet.update* methods aren't implemented,"ResultSet.update* methods aren't implemented

statement using  ResultSet.CONCUR_UPDATABLE must be able to update record.
exemple:
{code:java}
       Statement stmt = con.createStatement(
                                      ResultSet.TYPE_SCROLL_INSENSITIVE,
                                      ResultSet.CONCUR_UPDATABLE);
       ResultSet rs = stmt.executeQuery(""SELECT age FROM TABLE2"");
       // rs will be scrollable, will not show changes made by others,
       // and will be updatable
	while(rs.next()){
		//Retrieve by column name
		int newAge = rs.getInt(1) + 5;
		rs.updateDouble( 1 , newAge );
		rs.updateRow();
	}

{code}
",,"ResultSet.update* methods aren't implemented $end$ ResultSet.update* methods aren't implemented

statement using  ResultSet.CONCUR_UPDATABLE must be able to update record.
exemple:
{code:java}
       Statement stmt = con.createStatement(
                                      ResultSet.TYPE_SCROLL_INSENSITIVE,
                                      ResultSet.CONCUR_UPDATABLE);
       ResultSet rs = stmt.executeQuery(""SELECT age FROM TABLE2"");
       // rs will be scrollable, will not show changes made by others,
       // and will be updatable
	while(rs.next()){
		//Retrieve by column name
		int newAge = rs.getInt(1) + 5;
		rs.updateDouble( 1 , newAge );
		rs.updateRow();
	}

{code}
 $acceptance criteria:$",,Diego Dupin,Diego Dupin,Major,7,,1,4,2,1,0,0,0,,0,850,1,0,0,2017-06-15 14:30:32,ResultSet.update* methods aren't implemented,"ResultSet.update* methods aren't implemented

statement using  ResultSet.CONCUR_UPDATABLE must be able to update record.
exemple:
{code:java}
       Statement stmt = con.createStatement(
                                      ResultSet.TYPE_SCROLL_INSENSITIVE,
                                      ResultSet.CONCUR_UPDATABLE);
       ResultSet rs = stmt.executeQuery(""SELECT age FROM TABLE2"");
       // rs will be scrollable, will not show changes made by others,
       // and will be updatable
	while(rs.next()){
		//Retrieve by column name
		int newAge = rs.getInt(1) + 5;
		rs.updateDouble( 1 , newAge );
		rs.updateRow();
	}

{code}
",,0,0,0,0,0.0,"ResultSet.update* methods aren't implemented $end$ ResultSet.update* methods aren't implemented

statement using  ResultSet.CONCUR_UPDATABLE must be able to update record.
exemple:
{code:java}
       Statement stmt = con.createStatement(
                                      ResultSet.TYPE_SCROLL_INSENSITIVE,
                                      ResultSet.CONCUR_UPDATABLE);
       ResultSet rs = stmt.executeQuery(""SELECT age FROM TABLE2"");
       // rs will be scrollable, will not show changes made by others,
       // and will be updatable
	while(rs.next()){
		//Retrieve by column name
		int newAge = rs.getInt(1) + 5;
		rs.updateDouble( 1 , newAge );
		rs.updateRow();
	}

{code}
 $acceptance criteria:$",0,0,0,0,0,0,0,8211.25,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
8,CONJ-327,Task,CONJ,2016-08-02 08:21:55,,0,Handle sha256_password plugin,"SHA1 password security has known weakness. 
Authentication plugins for SHA256 has to be implemented for compatibility with MySQL",,"Handle sha256_password plugin $end$ SHA1 password security has known weakness. 
Authentication plugins for SHA256 has to be implemented for compatibility with MySQL $acceptance criteria:$",,Diego Dupin,Diego Dupin,Major,17,,0,1,8,1,0,3,0,,0,850,1,3,0,2017-06-15 14:30:36,Handle sha256_password plugin,"SHA1 password security has known weakness. 
Authentication plugins for SHA256 has to be implemented for compatibility with MySQL",,0,0,0,0,0.0,"Handle sha256_password plugin $end$ SHA1 password security has known weakness. 
Authentication plugins for SHA256 has to be implemented for compatibility with MySQL $acceptance criteria:$",0,0,0,0,0,0,0,7614.13,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
9,CONJ-400,Task,CONJ,2016-12-09 15:24:31,,0,Pool : galera validation,"JDBC Connection.valid() implementation execute a COM_PING, indicating that socket is OK.
 
When connecting a Galera server in non-primary state: 
- Connection can be established
- Connection.valid() will validate that socket is ok, with a COM_PING. 

When a master become non-primary, a SQLException will be thrown for any INSERT or SELECT (if variable wsrep_dirty_reads is false), problem is all other connection is pool are still considered valid until having these kind of query. 

Goal is to permit that pool discard those connection with the standard JDBC Connection.valid() verification, and avoid connect to non primary server when driver is configured with multi-master configuration.

When driver use multi-master configuration, when establishing the connection, or validating the connection, driver must ensure that if wsrep_cluster_status variable exists, it's set to PRIMARY
When this occur, server must be blacklisted (Driver will try to reconnect only if no other master are found, or after a certain amount of time). 

 

 ",,"Pool : galera validation $end$ JDBC Connection.valid() implementation execute a COM_PING, indicating that socket is OK.
 
When connecting a Galera server in non-primary state: 
- Connection can be established
- Connection.valid() will validate that socket is ok, with a COM_PING. 

When a master become non-primary, a SQLException will be thrown for any INSERT or SELECT (if variable wsrep_dirty_reads is false), problem is all other connection is pool are still considered valid until having these kind of query. 

Goal is to permit that pool discard those connection with the standard JDBC Connection.valid() verification, and avoid connect to non primary server when driver is configured with multi-master configuration.

When driver use multi-master configuration, when establishing the connection, or validating the connection, driver must ensure that if wsrep_cluster_status variable exists, it's set to PRIMARY
When this occur, server must be blacklisted (Driver will try to reconnect only if no other master are found, or after a certain amount of time). 

 

  $acceptance criteria:$",,Diego Dupin,Diego Dupin,Major,7,,0,3,0,1,0,1,0,,0,850,3,0,0,2017-06-15 14:30:45,Pool : galera validation,"JDBC Connection.valid() implementation execute a COM_PING, indicating that socket is OK.
 
When connecting a Galera server in non-primary state: 
- Connection can be established
- Connection.valid() will validate that socket is ok, with a COM_PING. 

When a master become non-primary, a SQLException will be thrown for any INSERT or SELECT (if variable wsrep_dirty_reads is false), problem is all other connection is pool are still considered valid until having these kind of query. 

Goal is to permit that pool discard those connection with the standard JDBC Connection.valid() verification, and avoid connect to non primary server when driver is configured with multi-master configuration.

When driver use multi-master configuration, when establishing the connection, or validating the connection, driver must ensure that 
- for a master if wsrep_ready variable exists, it's set to ON
- for a slave, if wsrep_ready variable exists, it's set to ON or that wsrep_dirty_reads is set to ON
When this occur, server must be blacklisted (Driver will try to reconnect only if no other master are found, or after a certain amount of time). 

A new variable must permit to disable this feature.
 

 ",,0,1,0,36,0.17801,"Pool : galera validation $end$ JDBC Connection.valid() implementation execute a COM_PING, indicating that socket is OK.
 
When connecting a Galera server in non-primary state: 
- Connection can be established
- Connection.valid() will validate that socket is ok, with a COM_PING. 

When a master become non-primary, a SQLException will be thrown for any INSERT or SELECT (if variable wsrep_dirty_reads is false), problem is all other connection is pool are still considered valid until having these kind of query. 

Goal is to permit that pool discard those connection with the standard JDBC Connection.valid() verification, and avoid connect to non primary server when driver is configured with multi-master configuration.

When driver use multi-master configuration, when establishing the connection, or validating the connection, driver must ensure that 
- for a master if wsrep_ready variable exists, it's set to ON
- for a slave, if wsrep_ready variable exists, it's set to ON or that wsrep_dirty_reads is set to ON
When this occur, server must be blacklisted (Driver will try to reconnect only if no other master are found, or after a certain amount of time). 

A new variable must permit to disable this feature.
 

  $acceptance criteria:$",1,1,1,1,1,1,1,4511.1,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
10,CONJ-422,Task,CONJ,2017-02-02 16:48:09,,0,Provide verification of SSL Certificate Name Mismatch,"Goal is to provide a solution to valid server certificats according to hostname. 

Host(IPv4/IPv6/DNS) in connection string must be valid according to certicats CN (fully qualified domain name / wildcard) and Subject Alternative Name.
(equivalent of navigator ""SSL Certificate Name Mismatch Error"")

example : connecting to server1.example.com must throw an error if certificat is issue to *.another.com.

This verification must be disabled by option, with a default [HostnameVerifier|https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/HostnameVerifier.html] implementation, but with a possible user implementation. ",,"Provide verification of SSL Certificate Name Mismatch $end$ Goal is to provide a solution to valid server certificats according to hostname. 

Host(IPv4/IPv6/DNS) in connection string must be valid according to certicats CN (fully qualified domain name / wildcard) and Subject Alternative Name.
(equivalent of navigator ""SSL Certificate Name Mismatch Error"")

example : connecting to server1.example.com must throw an error if certificat is issue to *.another.com.

This verification must be disabled by option, with a default [HostnameVerifier|https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/HostnameVerifier.html] implementation, but with a possible user implementation.  $acceptance criteria:$",,Diego Dupin,Diego Dupin,Major,6,,1,0,1,1,0,0,0,,0,850,0,0,0,2017-06-15 14:30:51,Provide verification of SSL Certificate Name Mismatch,"Goal is to provide a solution to valid server certificats according to hostname. 

Host(IPv4/IPv6/DNS) in connection string must be valid according to certicats CN (fully qualified domain name / wildcard) and Subject Alternative Name.
(equivalent of navigator ""SSL Certificate Name Mismatch Error"")

example : connecting to server1.example.com must throw an error if certificat is issue to *.another.com.

This verification must be disabled by option, with a default [HostnameVerifier|https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/HostnameVerifier.html] implementation, but with a possible user implementation. ",,0,0,0,0,0.0,"Provide verification of SSL Certificate Name Mismatch $end$ Goal is to provide a solution to valid server certificats according to hostname. 

Host(IPv4/IPv6/DNS) in connection string must be valid according to certicats CN (fully qualified domain name / wildcard) and Subject Alternative Name.
(equivalent of navigator ""SSL Certificate Name Mismatch Error"")

example : connecting to server1.example.com must throw an error if certificat is issue to *.another.com.

This verification must be disabled by option, with a default [HostnameVerifier|https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/HostnameVerifier.html] implementation, but with a possible user implementation.  $acceptance criteria:$",0,0,0,0,0,0,0,3189.7,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
11,CONJ-430,Task,CONJ,2017-02-10 15:23:13,,0,validConnectionTimeout default must be set to 0,"When Aurora is set, option validConnectionTimeout is actually default to 120 seconds. 

Setting a value validConnectionTimeout mean having an additional thread that will check valid connections every ""validConnectionTimeout "" seconds. 

Java is most of the time set with a pool that take care of it, meaning there is 2 validations. 

Default as to be set to 0 (no validation, no thread)",,"validConnectionTimeout default must be set to 0 $end$ When Aurora is set, option validConnectionTimeout is actually default to 120 seconds. 

Setting a value validConnectionTimeout mean having an additional thread that will check valid connections every ""validConnectionTimeout "" seconds. 

Java is most of the time set with a pool that take care of it, meaning there is 2 validations. 

Default as to be set to 0 (no validation, no thread) $acceptance criteria:$",,Diego Dupin,Diego Dupin,Trivial,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-06-15 14:30:57,validConnectionTimeout default must be set to 0,"When Aurora is set, option validConnectionTimeout is actually default to 120 seconds. 

Setting a value validConnectionTimeout mean having an additional thread that will check valid connections every ""validConnectionTimeout "" seconds. 

Java is most of the time set with a pool that take care of it, meaning there is 2 validations. 

Default as to be set to 0 (no validation, no thread)",,0,0,0,0,0.0,"validConnectionTimeout default must be set to 0 $end$ When Aurora is set, option validConnectionTimeout is actually default to 120 seconds. 

Setting a value validConnectionTimeout mean having an additional thread that will check valid connections every ""validConnectionTimeout "" seconds. 

Java is most of the time set with a pool that take care of it, meaning there is 2 validations. 

Default as to be set to 0 (no validation, no thread) $acceptance criteria:$",0,0,0,0,0,0,0,2999.12,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
12,CONJ-492,Task,CONJ,2017-06-07 08:39:07,,0,Failover must handle 70100 error code,"Driver failover handling is based on 08xxx sql state that indicate a connection exception. 
A ""KILL CONNECTION"" command will return a 70100 that is not automatically handled by failover.
70xxx sql state familly stand for interrupted exception. 

specifically 70100 sql state can be issue with different kind of error : 

* ER_STATEMENT_TIMEOUT (error 1969)
* ER_QUERY_INTERRUPTED (error 1317)
* ER_CONNECTION_KILLED (error 1927)

The 70100 sql state with error code 1927 (= KILL CONNECTION) must be handled by driver to reconnect automatically WITHOUT relaunching query
",,"Failover must handle 70100 error code $end$ Driver failover handling is based on 08xxx sql state that indicate a connection exception. 
A ""KILL CONNECTION"" command will return a 70100 that is not automatically handled by failover.
70xxx sql state familly stand for interrupted exception. 

specifically 70100 sql state can be issue with different kind of error : 

* ER_STATEMENT_TIMEOUT (error 1969)
* ER_QUERY_INTERRUPTED (error 1317)
* ER_CONNECTION_KILLED (error 1927)

The 70100 sql state with error code 1927 (= KILL CONNECTION) must be handled by driver to reconnect automatically WITHOUT relaunching query
 $acceptance criteria:$",,Diego Dupin,Diego Dupin,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-06-15 14:31:14,Failover must handle 70100 error code,"Driver failover handling is based on 08xxx sql state that indicate a connection exception. 
A ""KILL CONNECTION"" command will return a 70100 that is not automatically handled by failover.
70xxx sql state familly stand for interrupted exception. 

specifically 70100 sql state can be issue with different kind of error : 

* ER_STATEMENT_TIMEOUT (error 1969)
* ER_QUERY_INTERRUPTED (error 1317)
* ER_CONNECTION_KILLED (error 1927)

The 70100 sql state with error code 1927 (= KILL CONNECTION) must be handled by driver to reconnect automatically WITHOUT relaunching query
",,0,0,0,0,0.0,"Failover must handle 70100 error code $end$ Driver failover handling is based on 08xxx sql state that indicate a connection exception. 
A ""KILL CONNECTION"" command will return a 70100 that is not automatically handled by failover.
70xxx sql state familly stand for interrupted exception. 

specifically 70100 sql state can be issue with different kind of error : 

* ER_STATEMENT_TIMEOUT (error 1969)
* ER_QUERY_INTERRUPTED (error 1317)
* ER_CONNECTION_KILLED (error 1927)

The 70100 sql state with error code 1927 (= KILL CONNECTION) must be handled by driver to reconnect automatically WITHOUT relaunching query
 $acceptance criteria:$",0,0,0,0,0,0,0,197.867,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
13,CONJS-11,Task,CONJS,2018-03-13 10:52:22,,0,implement LOCAL INFILE,permit command LOCAL INFILE,,implement LOCAL INFILE $end$ permit command LOCAL INFILE $acceptance criteria:$,,Diego Dupin,Diego Dupin,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-03-23 17:24:39,implement LOCAL INFILE,permit command LOCAL INFILE,,0,0,0,0,0.0,implement LOCAL INFILE $end$ permit command LOCAL INFILE $acceptance criteria:$,0,0,0,0,0,0,0,246.533,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
14,CONJS-14,Task,CONJS,2018-03-13 10:55:36,,0,plugin authentification base testing,plugin authentification base testing (no GSSAPI),,plugin authentification base testing $end$ plugin authentification base testing (no GSSAPI) $acceptance criteria:$,,Diego Dupin,Diego Dupin,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-03-23 17:24:45,plugin authentification base testing,plugin authentification base testing (no GSSAPI),,0,0,0,0,0.0,plugin authentification base testing $end$ plugin authentification base testing (no GSSAPI) $acceptance criteria:$,0,0,0,0,0,0,0,246.483,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
15,CONJS-15,Task,CONJS,2018-03-13 10:57:27,,0,basic changeUser implementation,basic changeUser implementation ( no workaround for mysql bug),,basic changeUser implementation $end$ basic changeUser implementation ( no workaround for mysql bug) $acceptance criteria:$,,Diego Dupin,Diego Dupin,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-03-23 17:24:46,basic changeUser implementation,basic changeUser implementation ( no workaround for mysql bug),,0,0,0,0,0.0,basic changeUser implementation $end$ basic changeUser implementation ( no workaround for mysql bug) $acceptance criteria:$,0,0,0,0,0,0,0,246.45,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
16,CONJS-16,Task,CONJS,2018-03-13 10:57:51,,0,Implement compression protocol,Implement compression protocol,,Implement compression protocol $end$ Implement compression protocol $acceptance criteria:$,,Diego Dupin,Diego Dupin,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-03-23 17:24:30,Implement compression protocol,Implement compression protocol,,0,0,0,0,0.0,Implement compression protocol $end$ Implement compression protocol $acceptance criteria:$,0,0,0,0,0,0,0,246.433,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
17,CONJS-4,Task,CONJS,2018-02-15 15:48:44,,0,report new created test,report new created test,,report new created test $end$ report new created test $acceptance criteria:$,,Diego Dupin,Diego Dupin,Major,8,,0,0,1,1,0,0,0,,0,850,0,0,0,2018-03-23 17:24:32,report new created test,report new created test,,0,0,0,0,0.0,report new created test $end$ report new created test $acceptance criteria:$,0,0,0,0,0,0,0,865.583,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
18,MCOL-1,Task,MCOL,2016-04-27 14:46:31,,0,Query Failed after a redistributeDB while ddl/dml/queries were active,"Started with a 1um / 2pm system. I added a 3rd pm.
test scenario:

1. started a script that continually did a few queries on um1

2. started a script that ran the dbhealth.sh (DDl/DML test) continuely on um1

3. ran ./redistributeDB start   // from pm1

So while redistributeDB was running, the scripts were successfully passin, but when it complete, the query started failing with:

{code:java}

MariaDB [tpch100g]> select count(*) from lineitem;
ERROR 1815 (HY000): Internal error: IDB-2039: Data file does not exist, please contact your system administrator for more information. Started working after restartsystem was performed
{code}



In my next test, I added another pm and re-ran with just doing the query script only. This time the query continued to worked after completion... So this means that no ETL DDl DML changes should be made to the DB while the redistributeDB is running.

We could add in the code to suspend/resume database writes during this time, BUT cpimport doesn't look at the setting. only DML/DDLproc do.",,"Query Failed after a redistributeDB while ddl/dml/queries were active $end$ Started with a 1um / 2pm system. I added a 3rd pm.
test scenario:

1. started a script that continually did a few queries on um1

2. started a script that ran the dbhealth.sh (DDl/DML test) continuely on um1

3. ran ./redistributeDB start   // from pm1

So while redistributeDB was running, the scripts were successfully passin, but when it complete, the query started failing with:

{code:java}

MariaDB [tpch100g]> select count(*) from lineitem;
ERROR 1815 (HY000): Internal error: IDB-2039: Data file does not exist, please contact your system administrator for more information. Started working after restartsystem was performed
{code}



In my next test, I added another pm and re-ran with just doing the query script only. This time the query continued to worked after completion... So this means that no ETL DDl DML changes should be made to the DB while the redistributeDB is running.

We could add in the code to suspend/resume database writes during this time, BUT cpimport doesn't look at the setting. only DML/DDLproc do. $acceptance criteria:$",,David Hill,David Hill,Minor,33,,0,10,0,8,0,1,0,,0,850,6,1,0,2016-11-18 19:49:07,Query Failed after a redistributeDB while ddl/dml/queries were active,"Started with a 1um / 2pm system. I added a 3rd pm.
test scenario:

1. started a script that continually did a few queries on um1

2. started a script that ran the dbhealth.sh (DDl/DML test) continuely on um1

3. ran ./redistributeDB start   // from pm1

So while redistributeDB was running, the scripts were successfully passin, but when it complete, the query started failing with:

{code:java}

MariaDB [tpch100g]> select count(*) from lineitem;
ERROR 1815 (HY000): Internal error: IDB-2039: Data file does not exist, please contact your system administrator for more information. Started working after restartsystem was performed
{code}



In my next test, I added another pm and re-ran with just doing the query script only. This time the query continued to worked after completion... So this means that no ETL DDl DML changes should be made to the DB while the redistributeDB is running.

We could add in the code to suspend/resume database writes during this time, BUT cpimport doesn't look at the setting. only DML/DDLproc do.",,0,0,0,0,0.0,"Query Failed after a redistributeDB while ddl/dml/queries were active $end$ Started with a 1um / 2pm system. I added a 3rd pm.
test scenario:

1. started a script that continually did a few queries on um1

2. started a script that ran the dbhealth.sh (DDl/DML test) continuely on um1

3. ran ./redistributeDB start   // from pm1

So while redistributeDB was running, the scripts were successfully passin, but when it complete, the query started failing with:

{code:java}

MariaDB [tpch100g]> select count(*) from lineitem;
ERROR 1815 (HY000): Internal error: IDB-2039: Data file does not exist, please contact your system administrator for more information. Started working after restartsystem was performed
{code}



In my next test, I added another pm and re-ran with just doing the query script only. This time the query continued to worked after completion... So this means that no ETL DDl DML changes should be made to the DB while the redistributeDB is running.

We could add in the code to suspend/resume database writes during this time, BUT cpimport doesn't look at the setting. only DML/DDLproc do. $acceptance criteria:$",0,0,0,0,0,0,1,4925.03,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
19,MCOL-1000,Task,MCOL,2017-11-01 10:28:37,,0,Merge MariaDB 10.2.10,Merge MariaDB 10.2.10 into 1.1,,Merge MariaDB 10.2.10 $end$ Merge MariaDB 10.2.10 into 1.1 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-11-01 13:57:55,Merge MariaDB 10.2.10,Merge MariaDB 10.2.10 into 1.1,,0,0,0,0,0.0,Merge MariaDB 10.2.10 $end$ Merge MariaDB 10.2.10 into 1.1 $acceptance criteria:$,0,0,0,0,0,0,0,3.48333,38,2,0.0526316,2,0.0526316,1,0.0263158,1,0.0263158,1,0.0263158
20,MCOL-1015,Task,MCOL,2017-11-06 18:46:03,,0,builds and buildbot for mariadb-columnstore-data-adapters,same os list as for api and will also have the same issue for centos7 to run scl to use gcc5.,,builds and buildbot for mariadb-columnstore-data-adapters $end$ same os list as for api and will also have the same issue for centos7 to run scl to use gcc5. $acceptance criteria:$,,David Thompson,David Thompson,Major,16,,0,4,0,12,0,0,0,,0,850,4,0,0,2017-11-06 18:46:03,builds and buildbot for mariadb-columnstore-data-adapters,same os list as for api and will also have the same issue for centos7 to run scl to use gcc5.,,0,0,0,0,0.0,builds and buildbot for mariadb-columnstore-data-adapters $end$ same os list as for api and will also have the same issue for centos7 to run scl to use gcc5. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,47,4,0.0851064,4,0.0851064,3,0.0638298,2,0.0425532,2,0.0425532
21,MCOL-102,Task,MCOL,2016-06-03 15:52:36,,0,Test Staging area download experience,See the comment on how to test this,,Test Staging area download experience $end$ See the comment on how to test this $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,4,,0,3,0,1,0,0,0,,0,850,2,0,0,2016-06-04 02:28:57,Test Staging area download experience,See the comment on how to test this,,0,0,0,0,0.0,Test Staging area download experience $end$ See the comment on how to test this $acceptance criteria:$,0,0,0,0,0,0,0,10.6,23,6,0.26087,1,0.0434783,1,0.0434783,1,0.0434783,1,0.0434783
22,MCOL-1032,Task,MCOL,2017-11-15 03:14:28,,0,merge server 10.1.29,,,merge server 10.1.29 $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,6,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-11-15 03:14:28,merge server 10.1.29,,,0,0,0,0,0.0,merge server 10.1.29 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,48,4,0.0833333,4,0.0833333,3,0.0625,2,0.0416667,2,0.0416667
23,MCOL-104,Task,MCOL,2016-06-03 18:42:49,,0,remove INFINIDB from list of engines,"remove INFINIDB ""alias"" from storage engines",,"remove INFINIDB from list of engines $end$ remove INFINIDB ""alias"" from storage engines $acceptance criteria:$",,Justin Swanhart,Justin Swanhart,Major,14,,1,1,1,1,0,0,0,,0,850,1,0,0,2019-08-09 10:51:43,remove INFINIDB from list of engines,"remove INFINIDB ""alias"" from storage engines",,0,0,0,0,0.0,"remove INFINIDB from list of engines $end$ remove INFINIDB ""alias"" from storage engines $acceptance criteria:$",0,0,0,0,0,0,0,27880.1,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
24,MCOL-105,Task,MCOL,2016-06-03 18:44:35,,0,"Integrate build process into buildbot, build for multiple OS releases","Automated building (and testing?) should be done by buildbot, to build for CentOS, Debian, Ubuntu, and any other supported platform.",,"Integrate build process into buildbot, build for multiple OS releases $end$ Automated building (and testing?) should be done by buildbot, to build for CentOS, Debian, Ubuntu, and any other supported platform. $acceptance criteria:$",,Justin Swanhart,Justin Swanhart,Major,5,,0,7,0,3,0,0,0,,0,850,7,0,0,2017-01-16 15:57:34,"Integrate build process into buildbot, build for multiple OS releases","Automated building (and testing?) should be done by buildbot, to build for CentOS, Debian, Ubuntu, and any other supported platform.",,0,0,0,0,0.0,"Integrate build process into buildbot, build for multiple OS releases $end$ Automated building (and testing?) should be done by buildbot, to build for CentOS, Debian, Ubuntu, and any other supported platform. $acceptance criteria:$",0,0,0,0,0,0,1,5445.2,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
25,MCOL-1052,New Feature,MCOL,2017-11-27 14:17:01,MCOL-1096,0,Implement GROUP BY pushdown support,MariaDB has a group_by_handler with example code to push down the group by condition. This is required to help us become a generic engine.,,Implement GROUP BY pushdown support $end$ MariaDB has a group_by_handler with example code to push down the group by condition. This is required to help us become a generic engine. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,13,,3,5,4,5,0,0,0,,0,850,5,0,0,2018-03-26 07:21:55,Implement GROUP BY pushdown support,MariaDB has a group_by_handler with example code to push down the group by condition. This is required to help us become a generic engine.,,0,0,0,0,0.0,Implement GROUP BY pushdown support $end$ MariaDB has a group_by_handler with example code to push down the group by condition. This is required to help us become a generic engine. $acceptance criteria:$,0,0,0,0,0,0,1,2849.07,39,2,0.0512821,2,0.0512821,1,0.025641,1,0.025641,1,0.025641
26,MCOL-1058,New Feature,MCOL,2017-11-29 16:26:19,,0,cluster tester enhancements - check for mysql password and mariadb-libs package,"cluster tester enhancements

1. check for mysql password
2. check that mariadb-libs package isnt installed",,"cluster tester enhancements - check for mysql password and mariadb-libs package $end$ cluster tester enhancements

1. check for mysql password
2. check that mariadb-libs package isnt installed $acceptance criteria:$",,David Hill,David Hill,Minor,8,,0,5,0,1,0,0,0,,0,850,5,0,0,2018-01-12 19:48:37,cluster tester enhancements - check for mysql password and mariadb-libs package,"cluster tester enhancements

1. check for mysql password
2. check that mariadb-libs package isnt installed",,0,0,0,0,0.0,"cluster tester enhancements - check for mysql password and mariadb-libs package $end$ cluster tester enhancements

1. check for mysql password
2. check that mariadb-libs package isnt installed $acceptance criteria:$",0,0,0,0,0,0,0,1059.37,23,3,0.130435,0,0.0,0,0.0,0,0.0,0,0.0
27,MCOL-1060,Task,MCOL,2017-11-29 19:12:42,,0,ColumnStore Cluster Test tool - wording improvmenets,"corrections in working of output:

1. change ""all test passed"" to ""all tests passed""
2. heading ""test tool"" to ""Test Tool""",,"ColumnStore Cluster Test tool - wording improvmenets $end$ corrections in working of output:

1. change ""all test passed"" to ""all tests passed""
2. heading ""test tool"" to ""Test Tool"" $acceptance criteria:$",,David Hill,David Hill,Minor,14,,0,5,0,1,0,0,0,,0,850,5,0,0,2018-01-12 19:48:47,ColumnStore Cluster Test tool - wording improvmenets,"corrections in working of output:

1. change ""all test passed"" to ""all tests passed""
2. heading ""test tool"" to ""Test Tool""",,0,0,0,0,0.0,"ColumnStore Cluster Test tool - wording improvmenets $end$ corrections in working of output:

1. change ""all test passed"" to ""all tests passed""
2. heading ""test tool"" to ""Test Tool"" $acceptance criteria:$",0,0,0,0,0,0,0,1056.6,24,3,0.125,0,0.0,0,0.0,0,0.0,0,0.0
28,MCOL-1069,Task,MCOL,2017-12-01 20:10:28,,0,Merge MariaDB 10.2.11,Merge MariaDB Server 10.2.11 into develop-1.1,,Merge MariaDB 10.2.11 $end$ Merge MariaDB Server 10.2.11 into develop-1.1 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,9,,0,2,0,4,0,0,0,,0,850,1,0,0,2017-12-07 14:52:04,Merge MariaDB 10.2.11,Merge MariaDB Server 10.2.11 into develop-1.1,,0,0,0,0,0.0,Merge MariaDB 10.2.11 $end$ Merge MariaDB Server 10.2.11 into develop-1.1 $acceptance criteria:$,0,0,0,0,0,0,1,138.683,40,2,0.05,2,0.05,1,0.025,1,0.025,1,0.025
29,MCOL-1073,New Feature,MCOL,2017-12-04 09:05:13,,0,DecomSvr should be removed,It is an unused legacy service.,,DecomSvr should be removed $end$ It is an unused legacy service. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,0,2,0,0,0,,0,850,1,0,0,2018-05-15 14:33:42,DecomSvr should be removed,It is an unused legacy service.,,0,0,0,0,0.0,DecomSvr should be removed $end$ It is an unused legacy service. $acceptance criteria:$,0,0,0,0,0,0,1,3893.47,41,2,0.0487805,2,0.0487805,1,0.0243902,1,0.0243902,1,0.0243902
30,MCOL-1075,Task,MCOL,2017-12-04 16:46:03,,0,Clarifications for the Bulk Write SDK documentation,"*5.1.2 createBulkInsert()*

I expect that the future use variables mode and pm will provide the same functionality as cpimport. Could you please document the corresponding mode and PM used by default in the current Bulk Insert API?

*5.2.6 commit()*

Since the ColumnStoreBulkInsert class can not be re-used upon the failed commit, a  guidance on what to do upon the commit failure would be appreciated in this the section.

*5.2.11 setBatchSize()*

Do not implement this function to avoid confusion with the RowsPerBatch value set in the Columnstore.xml

It appears to be some inconsistency already, as the default RowsPerBatch in the Columnstore.xml is 10,000, while the SDK buffers up to 100,000 rows by default (despite setting the batchSize to 10,000).",,"Clarifications for the Bulk Write SDK documentation $end$ *5.1.2 createBulkInsert()*

I expect that the future use variables mode and pm will provide the same functionality as cpimport. Could you please document the corresponding mode and PM used by default in the current Bulk Insert API?

*5.2.6 commit()*

Since the ColumnStoreBulkInsert class can not be re-used upon the failed commit, a  guidance on what to do upon the commit failure would be appreciated in this the section.

*5.2.11 setBatchSize()*

Do not implement this function to avoid confusion with the RowsPerBatch value set in the Columnstore.xml

It appears to be some inconsistency already, as the default RowsPerBatch in the Columnstore.xml is 10,000, while the SDK buffers up to 100,000 rows by default (despite setting the batchSize to 10,000). $acceptance criteria:$",,Sasha V,Sasha V,Trivial,7,,0,3,1,1,0,0,0,,0,850,2,0,0,2017-12-04 17:25:49,Clarifications for the Bulk Write SDK documentation,"*5.1.2 createBulkInsert()*

I expect that the future use variables mode and pm will provide the same functionality as cpimport. Could you please document the corresponding mode and PM used by default in the current Bulk Insert API?

*5.2.6 commit()*

Since the ColumnStoreBulkInsert class can not be re-used upon the failed commit, a  guidance on what to do upon the commit failure would be appreciated in this the section.

*5.2.11 setBatchSize()*

Do not implement this function to avoid confusion with the RowsPerBatch value set in the Columnstore.xml

It appears to be some inconsistency already, as the default RowsPerBatch in the Columnstore.xml is 10,000, while the SDK buffers up to 100,000 rows by default (despite setting the batchSize to 10,000).",,0,0,0,0,0.0,"Clarifications for the Bulk Write SDK documentation $end$ *5.1.2 createBulkInsert()*

I expect that the future use variables mode and pm will provide the same functionality as cpimport. Could you please document the corresponding mode and PM used by default in the current Bulk Insert API?

*5.2.6 commit()*

Since the ColumnStoreBulkInsert class can not be re-used upon the failed commit, a  guidance on what to do upon the commit failure would be appreciated in this the section.

*5.2.11 setBatchSize()*

Do not implement this function to avoid confusion with the RowsPerBatch value set in the Columnstore.xml

It appears to be some inconsistency already, as the default RowsPerBatch in the Columnstore.xml is 10,000, while the SDK buffers up to 100,000 rows by default (despite setting the batchSize to 10,000). $acceptance criteria:$",0,0,0,0,0,0,0,0.65,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
31,MCOL-1085,New Feature,MCOL,2017-12-06 16:16:26,,0,Add automatic stack trace to ColumnStore binaries,"See this pull request as one example of how to do it:

https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/pull/5/files",,"Add automatic stack trace to ColumnStore binaries $end$ See this pull request as one example of how to do it:

https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/pull/5/files $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,11,,0,4,0,2,0,0,0,,0,850,3,0,0,2018-01-03 09:23:33,Add automatic stack trace to ColumnStore binaries,"See this pull request as one example of how to do it:

https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/pull/5/files",,0,0,0,0,0.0,"Add automatic stack trace to ColumnStore binaries $end$ See this pull request as one example of how to do it:

https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/pull/5/files $acceptance criteria:$",0,0,0,0,0,0,1,665.117,42,2,0.047619,2,0.047619,1,0.0238095,1,0.0238095,1,0.0238095
32,MCOL-1094,New Feature,MCOL,2017-12-11 14:04:42,,0,mcsapi should have view/clear table lock features,"You can't start a transaction when a table lock exists so there should be functionality to view and clear table locks, thereby rolling back transactions stuck due to crashes in applications using mcsapi.

Discussed in MCOL-1077",,"mcsapi should have view/clear table lock features $end$ You can't start a transaction when a table lock exists so there should be functionality to view and clear table locks, thereby rolling back transactions stuck due to crashes in applications using mcsapi.

Discussed in MCOL-1077 $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,9,,0,2,2,1,0,0,0,,0,850,1,0,0,2018-11-19 19:04:26,mcsapi should have view/clear table lock features,"You can't start a transaction when a table lock exists so there should be functionality to view and clear table locks, thereby rolling back transactions stuck due to crashes in applications using mcsapi.

Discussed in MCOL-1077",,0,0,0,0,0.0,"mcsapi should have view/clear table lock features $end$ You can't start a transaction when a table lock exists so there should be functionality to view and clear table locks, thereby rolling back transactions stuck due to crashes in applications using mcsapi.

Discussed in MCOL-1077 $acceptance criteria:$",0,0,0,0,0,0,0,8236.98,43,2,0.0465116,2,0.0465116,1,0.0232558,1,0.0232558,1,0.0232558
33,MCOL-1099,Task,MCOL,2017-12-11 18:24:56,,0,Clarification for the Bulk Write SDK documentation,"Sections *5.2.6 commit()* and *5.2.7 rollback()* have:

 *Note:*  After making this call the transaction is completed and the class should not be used for anything but {{ColumnStoreBulkInsert::getSummary()}}. 

It appears that the {{ColumnStoreBulkInsert::isActive()}} also can be used after commit/rollback calls. Thus, I propose extending the *Note* accordingly.

",,"Clarification for the Bulk Write SDK documentation $end$ Sections *5.2.6 commit()* and *5.2.7 rollback()* have:

 *Note:*  After making this call the transaction is completed and the class should not be used for anything but {{ColumnStoreBulkInsert::getSummary()}}. 

It appears that the {{ColumnStoreBulkInsert::isActive()}} also can be used after commit/rollback calls. Thus, I propose extending the *Note* accordingly.

 $acceptance criteria:$",,Sasha V,Sasha V,Trivial,4,,0,1,1,1,0,0,0,,0,850,1,0,0,2017-12-11 18:58:51,Clarification for the Bulk Write SDK documentation,"Sections *5.2.6 commit()* and *5.2.7 rollback()* have:

 *Note:*  After making this call the transaction is completed and the class should not be used for anything but {{ColumnStoreBulkInsert::getSummary()}}. 

It appears that the {{ColumnStoreBulkInsert::isActive()}} also can be used after commit/rollback calls. Thus, I propose extending the *Note* accordingly.

",,0,0,0,0,0.0,"Clarification for the Bulk Write SDK documentation $end$ Sections *5.2.6 commit()* and *5.2.7 rollback()* have:

 *Note:*  After making this call the transaction is completed and the class should not be used for anything but {{ColumnStoreBulkInsert::getSummary()}}. 

It appears that the {{ColumnStoreBulkInsert::isActive()}} also can be used after commit/rollback calls. Thus, I propose extending the *Note* accordingly.

 $acceptance criteria:$",0,0,0,0,0,0,0,0.55,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
34,MCOL-11,Task,MCOL,2016-05-02 18:27:57,,0,Rename calpontConsole script,Raname the calpontConsole(cc) script name mcsAdmin,,Rename calpontConsole script $end$ Raname the calpontConsole(cc) script name mcsAdmin $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,14,,0,5,0,1,0,1,0,,0,850,5,0,0,2016-05-03 21:21:16,Rename calpontConsole script,Raname the calpontConsole(cc) script name mdbcolAdmin,,0,1,0,2,0.0833333,Rename calpontConsole script $end$ Raname the calpontConsole(cc) script name mdbcolAdmin $acceptance criteria:$,1,1,0,0,0,0,0,26.8833,6,2,0.333333,0,0.0,0,0.0,0,0.0,0,0.0
35,MCOL-1101,New Feature,MCOL,2017-12-12 13:41:08,MCOL-1097,0,Move system variables into plugin,The ColumnStore system variables should be moved into the plugin and renamed from InfiniDB to ColumnStore.,,Move system variables into plugin $end$ The ColumnStore system variables should be moved into the plugin and renamed from InfiniDB to ColumnStore. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,12,,1,3,1,4,0,0,0,,0,850,2,0,0,2019-03-08 10:12:56,Move system variables into plugin,The ColumnStore system variables should be moved into the plugin and renamed from InfiniDB to ColumnStore.,,0,0,0,0,0.0,Move system variables into plugin $end$ The ColumnStore system variables should be moved into the plugin and renamed from InfiniDB to ColumnStore. $acceptance criteria:$,0,0,0,0,0,0,1,10820.5,44,2,0.0454545,2,0.0454545,1,0.0227273,1,0.0227273,1,0.0227273
36,MCOL-1107,New Feature,MCOL,2017-12-12 23:04:14,,0,Basic Java example of cpimport which uses the columnstore API,Java version of cpimport which can use SimpleDateFormat notation to parse arbitrary dates to columnstore standard date and datetime formats.,,Basic Java example of cpimport which uses the columnstore API $end$ Java version of cpimport which can use SimpleDateFormat notation to parse arbitrary dates to columnstore standard date and datetime formats. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Minor,11,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-12-14 23:14:17,Basic Java example of cpimport which uses the columnstore API,Java version of cpimport which can use SimpleDateFormat notation to parse arbitrary dates to columnstore standard date and datetime formats.,,0,0,0,0,0.0,Basic Java example of cpimport which uses the columnstore API $end$ Java version of cpimport which can use SimpleDateFormat notation to parse arbitrary dates to columnstore standard date and datetime formats. $acceptance criteria:$,0,0,0,0,0,0,0,48.1667,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
37,MCOL-1119,New Feature,MCOL,2017-12-18 23:39:26,,0,spark connector for publishing dataframe results using mcsapi to columnstore.,"We should support a data adapter that allows bridging spark (both scala and pyspark) to columnstore. The intended use case is to support publishing ML results to column store as both results system of record and to enable easier consumption of that data with SQL and other data already stored in MariaDB.

Broadly speaking the goal is to take a DataFrame object and serialize that to a ColumnStore table using mcsapi. This requires creation of new code to bridge the spark world to mcsapi. The first implementation can make assumptions that an appropriate table exists but it would be valuable to either create or adapt some code to generate appropriate columnstore create table statements that could be run as stage 1 before writing the data.  ",,"spark connector for publishing dataframe results using mcsapi to columnstore. $end$ We should support a data adapter that allows bridging spark (both scala and pyspark) to columnstore. The intended use case is to support publishing ML results to column store as both results system of record and to enable easier consumption of that data with SQL and other data already stored in MariaDB.

Broadly speaking the goal is to take a DataFrame object and serialize that to a ColumnStore table using mcsapi. This requires creation of new code to bridge the spark world to mcsapi. The first implementation can make assumptions that an appropriate table exists but it would be valuable to either create or adapt some code to generate appropriate columnstore create table statements that could be run as stage 1 before writing the data.   $acceptance criteria:$",,David Thompson,David Thompson,Major,14,,0,3,0,8,0,0,0,,0,850,3,0,0,2017-12-18 23:39:26,spark connector for publishing dataframe results using mcsapi to columnstore.,"We should support a data adapter that allows bridging spark (both scala and pyspark) to columnstore. The intended use case is to support publishing ML results to column store as both results system of record and to enable easier consumption of that data with SQL and other data already stored in MariaDB.

Broadly speaking the goal is to take a DataFrame object and serialize that to a ColumnStore table using mcsapi. This requires creation of new code to bridge the spark world to mcsapi. The first implementation can make assumptions that an appropriate table exists but it would be valuable to either create or adapt some code to generate appropriate columnstore create table statements that could be run as stage 1 before writing the data.  ",,0,0,0,0,0.0,"spark connector for publishing dataframe results using mcsapi to columnstore. $end$ We should support a data adapter that allows bridging spark (both scala and pyspark) to columnstore. The intended use case is to support publishing ML results to column store as both results system of record and to enable easier consumption of that data with SQL and other data already stored in MariaDB.

Broadly speaking the goal is to take a DataFrame object and serialize that to a ColumnStore table using mcsapi. This requires creation of new code to bridge the spark world to mcsapi. The first implementation can make assumptions that an appropriate table exists but it would be valuable to either create or adapt some code to generate appropriate columnstore create table statements that could be run as stage 1 before writing the data.   $acceptance criteria:$",0,0,0,0,0,0,1,0.0,49,4,0.0816327,4,0.0816327,3,0.0612245,2,0.0408163,2,0.0408163
38,MCOL-112,Task,MCOL,2016-06-09 16:44:37,,0,10.1.17 merge,"When MariaDB 10.1.17 is available, we need to merge with upstream to pick up all bug fixes, especially MDEV-10181",,"10.1.17 merge $end$ When MariaDB 10.1.17 is available, we need to merge with upstream to pick up all bug fixes, especially MDEV-10181 $acceptance criteria:$",,David Hall,David Hall,Major,16,,0,5,1,1,0,4,0,,0,850,5,4,0,2016-08-25 11:14:42,10.1.17 merge,"When MariaDB 10.1.17 is available, we need to merge with upstream to pick up all bug fixes, especially MDEV-10181",,0,0,0,0,0.0,"10.1.17 merge $end$ When MariaDB 10.1.17 is available, we need to merge with upstream to pick up all bug fixes, especially MDEV-10181 $acceptance criteria:$",0,0,0,0,0,0,0,1842.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
39,MCOL-1121,Task,MCOL,2017-12-19 00:38:42,,0,Generic Kafka Data Adapter,generalize maxscale-kafka-adapter for generic kafka topics coming as avro,,Generic Kafka Data Adapter $end$ generalize maxscale-kafka-adapter for generic kafka topics coming as avro $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,18,,0,1,0,7,0,0,0,,0,850,1,0,0,2018-01-02 21:43:53,Generic Kafka Data Adapter,generalize maxscale-kafka-adapter for generic kafka topics coming as avro,,0,0,0,0,0.0,Generic Kafka Data Adapter $end$ generalize maxscale-kafka-adapter for generic kafka topics coming as avro $acceptance criteria:$,0,0,0,0,0,0,1,357.083,37,8,0.216216,3,0.0810811,3,0.0810811,1,0.027027,1,0.027027
40,MCOL-1142,Task,MCOL,2018-01-05 22:12:48,,0,support group install of AX,"In addition to the basic columnstore repo install can we additionally have an 'ax' group install. This will require adding the maxscale repo as well. This should include:
- columnstore
- maxscale
- data adapters (bulk data connectors and streaming data adapters)
- backup tool
- connectors (MariaDB client connectors for c, JDBC, ODBC)

While the package can be one, but group install of the each of the above group should be separate, as each of the above shall be installed on separate machine. i.e. CoulmnStore on UM, PM, MaxScale on separate proxy machine, data-adapters near data source and backup-tool on admin machine.

",,"support group install of AX $end$ In addition to the basic columnstore repo install can we additionally have an 'ax' group install. This will require adding the maxscale repo as well. This should include:
- columnstore
- maxscale
- data adapters (bulk data connectors and streaming data adapters)
- backup tool
- connectors (MariaDB client connectors for c, JDBC, ODBC)

While the package can be one, but group install of the each of the above group should be separate, as each of the above shall be installed on separate machine. i.e. CoulmnStore on UM, PM, MaxScale on separate proxy machine, data-adapters near data source and backup-tool on admin machine.

 $acceptance criteria:$",,David Thompson,David Thompson,Major,31,,0,9,1,22,0,2,0,,0,850,6,1,0,2018-01-12 19:48:58,support group install of AX,"In addition to the basic columnstore repo install can we additionally have an 'ax' group install. This will require adding the maxscale repo as well. This should include:
- columnstore
- maxscale
- data adapters (bulk data connectors and streaming data adapters)
- backup tool

While the package can be one, but group install of the each of the above group should be separate, as each of the above shall be installed on separate machine. i.e. CoulmnStore on UM, PM, MaxScale on separate proxy machine, data-adapters near data source and backup-tool on admin machine.

",,0,1,0,9,0.0882353,"support group install of AX $end$ In addition to the basic columnstore repo install can we additionally have an 'ax' group install. This will require adding the maxscale repo as well. This should include:
- columnstore
- maxscale
- data adapters (bulk data connectors and streaming data adapters)
- backup tool

While the package can be one, but group install of the each of the above group should be separate, as each of the above shall be installed on separate machine. i.e. CoulmnStore on UM, PM, MaxScale on separate proxy machine, data-adapters near data source and backup-tool on admin machine.

 $acceptance criteria:$",1,1,1,0,0,0,1,165.6,50,4,0.08,4,0.08,3,0.06,2,0.04,2,0.04
41,MCOL-1143,Task,MCOL,2018-01-05 22:15:25,,0,package build of mariadb-columnstore-tools,"So that it can be installed by repos, we'll need the tools install to also build packages in addition the binary install we have today. This probably also means we need to have a build loop to do this for all os distributions?",,"package build of mariadb-columnstore-tools $end$ So that it can be installed by repos, we'll need the tools install to also build packages in addition the binary install we have today. This probably also means we need to have a build loop to do this for all os distributions? $acceptance criteria:$",,David Thompson,David Thompson,Major,6,,0,5,1,2,0,0,0,,0,850,5,0,0,2018-01-12 17:44:34,package build of mariadb-columnstore-tools,"So that it can be installed by repos, we'll need the tools install to also build packages in addition the binary install we have today. This probably also means we need to have a build loop to do this for all os distributions?",,0,0,0,0,0.0,"package build of mariadb-columnstore-tools $end$ So that it can be installed by repos, we'll need the tools install to also build packages in addition the binary install we have today. This probably also means we need to have a build loop to do this for all os distributions? $acceptance criteria:$",0,0,0,0,0,0,1,163.483,51,5,0.0980392,5,0.0980392,3,0.0588235,2,0.0392157,2,0.0392157
42,MCOL-1145,Task,MCOL,2018-01-07 16:59:15,MCOL-1503,0,One step configuration of Single Server Node,"At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Single node install - that simply takes no parameter and then applies all the default parameters for postConfig automatically without user interaction in the background.",,"One step configuration of Single Server Node $end$ At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Single node install - that simply takes no parameter and then applies all the default parameters for postConfig automatically without user interaction in the background. $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,21,,0,8,0,4,0,1,0,,0,850,8,1,0,2018-06-19 15:35:00,One step configuration of Single Server Node,"At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Single node install - that simply takes no parameter and then applies all the default parameters for postConfig automatically without user interaction in the background.",,0,0,0,0,0.0,"One step configuration of Single Server Node $end$ At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Single node install - that simply takes no parameter and then applies all the default parameters for postConfig automatically without user interaction in the background. $acceptance criteria:$",0,0,0,0,0,0,1,3910.58,38,8,0.210526,3,0.0789474,3,0.0789474,1,0.0263158,1,0.0263158
43,MCOL-1146,Task,MCOL,2018-01-07 17:06:47,MCOL-1503,0,One step configuration of Multi Server Node,"At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Multi node install - that simply takes IP addresses of UM node and IP address of the PM node on command line and then applies all the default parameters for posConfig automatically without user interaction in the background.

Something like: multiNodeConfig UM=IP1,IP2,IP3...  PM=IP4,IP5,IP6

All the packages (rpm, deb, zypher or binaries) must have been installed on each of the UM and PM nodes before invoking this configuration utility.

",,"One step configuration of Multi Server Node $end$ At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Multi node install - that simply takes IP addresses of UM node and IP address of the PM node on command line and then applies all the default parameters for posConfig automatically without user interaction in the background.

Something like: multiNodeConfig UM=IP1,IP2,IP3...  PM=IP4,IP5,IP6

All the packages (rpm, deb, zypher or binaries) must have been installed on each of the UM and PM nodes before invoking this configuration utility.

 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,22,,0,15,0,4,0,4,0,,0,850,13,4,0,2018-06-19 15:34:46,One step configuration of Multi Server Node,"At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Multi node install - that simply takes IP addresses of UM node and IP address of the PM node on command line and then applies all the default parameters for posConfig automatically without user interaction in the background.

Something like: multiNodeConfig UM=IP1,IP2,IP3...  PM=IP4,IP5,IP6

All the packages (rpm, deb, zypher or binaries) must have been installed on each of the UM and PM nodes before invoking this configuration utility.

",,0,0,0,0,0.0,"One step configuration of Multi Server Node $end$ At present single node and multi-node install requires postConfig to be run after binary has been installed. postConfig is interactive configuration utility.

Please provide a utility for Multi node install - that simply takes IP addresses of UM node and IP address of the PM node on command line and then applies all the default parameters for posConfig automatically without user interaction in the background.

Something like: multiNodeConfig UM=IP1,IP2,IP3...  PM=IP4,IP5,IP6

All the packages (rpm, deb, zypher or binaries) must have been installed on each of the UM and PM nodes before invoking this configuration utility.

 $acceptance criteria:$",0,0,0,0,0,0,1,3910.45,39,8,0.205128,3,0.0769231,3,0.0769231,1,0.025641,1,0.025641
44,MCOL-1158,New Feature,MCOL,2018-01-12 20:58:32,,0,Support additional Python3 features using Swig's -py3 flag,Support Python3 capabilities by using Swig's -py3 flag when generating the wrapper code. Need separate directories for Python2 and Python3 as the generated wrapper code won't be equal any more.,,Support additional Python3 features using Swig's -py3 flag $end$ Support Python3 capabilities by using Swig's -py3 flag when generating the wrapper code. Need separate directories for Python2 and Python3 as the generated wrapper code won't be equal any more. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Minor,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-10-19 01:31:55,Support additional Python3 features using Swig's -py3 flag,Support Python3 capabilities by using Swig's -py3 flag when generating the wrapper code. Need separate directories for Python2 and Python3 as the generated wrapper code won't be equal any more.,,0,0,0,0,0.0,Support additional Python3 features using Swig's -py3 flag $end$ Support Python3 capabilities by using Swig's -py3 flag when generating the wrapper code. Need separate directories for Python2 and Python3 as the generated wrapper code won't be equal any more. $acceptance criteria:$,0,0,0,0,0,0,1,6700.55,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
45,MCOL-1159,Task,MCOL,2018-01-15 10:16:26,,0,Merge MariaDB 10.2.12,MariaDB 10.2.12 needs merging into develop-1.1,,Merge MariaDB 10.2.12 $end$ MariaDB 10.2.12 needs merging into develop-1.1 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,9,,0,2,0,1,0,0,0,,0,850,2,0,0,2018-01-15 10:16:26,Merge MariaDB 10.2.12,MariaDB 10.2.12 needs merging into develop-1.1,,0,0,0,0,0.0,Merge MariaDB 10.2.12 $end$ MariaDB 10.2.12 needs merging into develop-1.1 $acceptance criteria:$,0,0,0,0,0,0,0,0.0,45,2,0.0444444,2,0.0444444,1,0.0222222,1,0.0222222,1,0.0222222
46,MCOL-1171,New Feature,MCOL,2018-01-19 22:24:12,,0,Introduce benchmarks to test the performance with regards to jdbc,Benchmark the scala and python ColumnStoreExporter against jdbc and columnstore and jdbc and innodb.,,Introduce benchmarks to test the performance with regards to jdbc $end$ Benchmark the scala and python ColumnStoreExporter against jdbc and columnstore and jdbc and innodb. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Minor,6,,0,2,0,1,0,0,0,,0,850,1,0,0,2018-01-22 14:55:09,Introduce benchmarks to test the performance with regards to jdbc,Benchmark the scala and python ColumnStoreExporter against jdbc and columnstore and jdbc and innodb.,,0,0,0,0,0.0,Introduce benchmarks to test the performance with regards to jdbc $end$ Benchmark the scala and python ColumnStoreExporter against jdbc and columnstore and jdbc and innodb. $acceptance criteria:$,0,0,0,0,0,0,0,64.5,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
47,MCOL-1172,New Feature,MCOL,2018-01-19 22:30:44,,0,Create a consistent naming for scala and python spark exporter,Name is ColumnStoreExporter function is export,,Create a consistent naming for scala and python spark exporter $end$ Name is ColumnStoreExporter function is export $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Trivial,7,,0,3,0,1,0,0,0,,0,850,1,0,0,2018-02-02 10:17:59,Create a consistent naming for scala and python spark exporter,Name is ColumnStoreExporter function is export,,0,0,0,0,0.0,Create a consistent naming for scala and python spark exporter $end$ Name is ColumnStoreExporter function is export $acceptance criteria:$,0,0,0,0,0,0,0,323.783,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
48,MCOL-1175,New Feature,MCOL,2018-01-22 19:52:52,MCOL-3523,0,CrossEngineSupport stores pw in free text,"As per [MCS KB|https://mariadb.com/kb/en/library/configuring-columnstore-cross-engine-joins/] one must store a free text pw in the Columnstore.xml:

<CrossEngineSupport>
       <Host>127.0.0.1</Host>
       <Port>3306</Port>
       <User>mydbuser</User>
       {color:red}<Password>pwd</Password>{color}
</CrossEngineSupport>

Please determine how to hash or otherwise. 
Thanks!
Andy",,"CrossEngineSupport stores pw in free text $end$ As per [MCS KB|https://mariadb.com/kb/en/library/configuring-columnstore-cross-engine-joins/] one must store a free text pw in the Columnstore.xml:

<CrossEngineSupport>
       <Host>127.0.0.1</Host>
       <Port>3306</Port>
       <User>mydbuser</User>
       {color:red}<Password>pwd</Password>{color}
</CrossEngineSupport>

Please determine how to hash or otherwise. 
Thanks!
Andy $acceptance criteria:$",,Andy Allaway,Andy Allaway,Major,48,,2,5,3,9,0,0,0,,0,850,2,0,0,2021-01-12 18:39:58,CrossEngineSupport stores pw in free text,"As per [MCS KB|https://mariadb.com/kb/en/library/configuring-columnstore-cross-engine-joins/] one must store a free text pw in the Columnstore.xml:

<CrossEngineSupport>
       <Host>127.0.0.1</Host>
       <Port>3306</Port>
       <User>mydbuser</User>
       {color:red}<Password>pwd</Password>{color}
</CrossEngineSupport>

Please determine how to hash or otherwise. 
Thanks!
Andy",,0,0,0,0,0.0,"CrossEngineSupport stores pw in free text $end$ As per [MCS KB|https://mariadb.com/kb/en/library/configuring-columnstore-cross-engine-joins/] one must store a free text pw in the Columnstore.xml:

<CrossEngineSupport>
       <Host>127.0.0.1</Host>
       <Port>3306</Port>
       <User>mydbuser</User>
       {color:red}<Password>pwd</Password>{color}
</CrossEngineSupport>

Please determine how to hash or otherwise. 
Thanks!
Andy $acceptance criteria:$",0,0,0,0,0,0,1,26062.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
49,MCOL-1179,New Feature,MCOL,2018-01-25 19:27:56,,0,Pentaho Data Integration / Kettle - Bulk API Java Binding,Build an adapter for Pentaho/Kettle that lets you write data to columnstore using the bulk api java binding.,,Pentaho Data Integration / Kettle - Bulk API Java Binding $end$ Build an adapter for Pentaho/Kettle that lets you write data to columnstore using the bulk api java binding. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,19,,2,7,3,8,0,0,0,,0,850,3,0,0,2018-02-24 12:10:52,Pentaho Data Integration / Kettle - Bulk API Java Binding,Build an adapter for Pentaho/Kettle that lets you write data to columnstore using the bulk api java binding.,,0,0,0,0,0.0,Pentaho Data Integration / Kettle - Bulk API Java Binding $end$ Build an adapter for Pentaho/Kettle that lets you write data to columnstore using the bulk api java binding. $acceptance criteria:$,0,0,0,0,0,0,1,712.7,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
50,MCOL-1199,New Feature,MCOL,2018-02-05 17:49:44,,0,Forward Bulk Write API C++ Exceptions to Java ,"Occurring C++ exceptions aren't forwarded to Java. Therefore, in case a C++ exception is thrown, the Java program terminates.",,"Forward Bulk Write API C++ Exceptions to Java  $end$ Occurring C++ exceptions aren't forwarded to Java. Therefore, in case a C++ exception is thrown, the Java program terminates. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Critical,6,,0,3,0,1,0,0,0,,0,850,3,0,0,2018-02-05 18:04:52,Forward Bulk Write API C++ Exceptions to Java ,"Occurring C++ exceptions aren't forwarded to Java. Therefore, in case a C++ exception is thrown, the Java program terminates.",,0,0,0,0,0.0,"Forward Bulk Write API C++ Exceptions to Java  $end$ Occurring C++ exceptions aren't forwarded to Java. Therefore, in case a C++ exception is thrown, the Java program terminates. $acceptance criteria:$",0,0,0,0,0,0,0,0.25,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
51,MCOL-1200,New Feature,MCOL,2018-02-05 17:51:48,,0,Forward Bulk Write API C++ Exceptions to Python,Occurring C++ exceptions aren't forwarded to Python. In case a C++ exception occurs the Python program terminates.,,Forward Bulk Write API C++ Exceptions to Python $end$ Occurring C++ exceptions aren't forwarded to Python. In case a C++ exception occurs the Python program terminates. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,6,,0,4,0,1,0,0,0,,0,850,4,0,0,2018-02-05 18:05:32,Forward Bulk Write API C++ Exceptions to Python,Occurring C++ exceptions aren't forwarded to Python. In case a C++ exception occurs the Python program terminates.,,0,0,0,0,0.0,Forward Bulk Write API C++ Exceptions to Python $end$ Occurring C++ exceptions aren't forwarded to Python. In case a C++ exception occurs the Python program terminates. $acceptance criteria:$,0,0,0,0,0,0,0,0.216667,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
52,MCOL-1201,New Feature,MCOL,2018-02-05 18:06:20,,0,Allow UDAnF to have multiple parameters defined.,"UDAF in 1.1 allows for only one parameter to be defined. This JIRA is to allow for multiple parameters to be defined.

In addition, some of these parameters (only trailing ones) may be optional.",,"Allow UDAnF to have multiple parameters defined. $end$ UDAF in 1.1 allows for only one parameter to be defined. This JIRA is to allow for multiple parameters to be defined.

In addition, some of these parameters (only trailing ones) may be optional. $acceptance criteria:$",,David Hall,David Hall,Major,17,,0,4,1,11,0,0,0,,0,850,4,0,0,2018-04-09 14:37:30,Allow UDAnF to have multiple parameters defined.,"UDAF in 1.1 allows for only one parameter to be defined. This JIRA is to allow for multiple parameters to be defined.

In addition, some of these parameters (only trailing ones) may be optional.",,0,0,0,0,0.0,"Allow UDAnF to have multiple parameters defined. $end$ UDAF in 1.1 allows for only one parameter to be defined. This JIRA is to allow for multiple parameters to be defined.

In addition, some of these parameters (only trailing ones) may be optional. $acceptance criteria:$",0,0,0,0,0,0,1,1508.52,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
53,MCOL-1206,Task,MCOL,2018-02-06 22:14:52,,0,Merge MariaDB 10.1.31,MariaDB 10.1.31 has been released,,Merge MariaDB 10.1.31 $end$ MariaDB 10.1.31 has been released $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-02-06 22:14:52,Merge MariaDB 10.1.31,MariaDB 10.1.31 has been released,,0,0,0,0,0.0,Merge MariaDB 10.1.31 $end$ MariaDB 10.1.31 has been released $acceptance criteria:$,0,0,0,0,0,0,0,0.0,46,2,0.0434783,2,0.0434783,1,0.0217391,1,0.0217391,1,0.0217391
54,MCOL-1232,New Feature,MCOL,2018-02-23 17:29:37,,0,Spark connector - support different ColumnStore configurations,"Currently the SparkConnector only supports the default Columnstore.xml configuration in /usr/local/mariadb/columnstore/etc.

Individual configuration file locations should be supported in addition.",,"Spark connector - support different ColumnStore configurations $end$ Currently the SparkConnector only supports the default Columnstore.xml configuration in /usr/local/mariadb/columnstore/etc.

Individual configuration file locations should be supported in addition. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,13,,0,1,0,6,0,0,0,,0,850,1,0,0,2018-03-08 13:30:37,Spark connector - support different ColumnStore configurations,"Currently the SparkConnector only supports the default Columnstore.xml configuration in /usr/local/mariadb/columnstore/etc.

Individual configuration file locations should be supported in addition.",,0,0,0,0,0.0,"Spark connector - support different ColumnStore configurations $end$ Currently the SparkConnector only supports the default Columnstore.xml configuration in /usr/local/mariadb/columnstore/etc.

Individual configuration file locations should be supported in addition. $acceptance criteria:$",0,0,0,0,0,0,1,308.017,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
55,MCOL-1242,New Feature,MCOL,2018-03-05 23:52:47,,0,Remote CpImport,More input needed,,Remote CpImport $end$ More input needed $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,33,,2,9,8,3,0,0,0,,0,850,7,0,0,2018-09-11 21:31:53,Remote CpImport,More input needed,,0,0,0,0,0.0,Remote CpImport $end$ More input needed $acceptance criteria:$,0,0,0,0,0,0,1,4557.65,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
56,MCOL-1244,New Feature,MCOL,2018-03-06 15:15:49,MCOL-1503,0,make postConfigure default install non-distributed,"Currently postConfigure is defaulted to perform a distributed install, meaning the packages are pushed from pm1 to the other nodes.  There is a command line feature to enable non-distributed installs. 

This change is to reverse that functionality and make the default install non-distributed.

",,"make postConfigure default install non-distributed $end$ Currently postConfigure is defaulted to perform a distributed install, meaning the packages are pushed from pm1 to the other nodes.  There is a command line feature to enable non-distributed installs. 

This change is to reverse that functionality and make the default install non-distributed.

 $acceptance criteria:$",,David Hill,David Hill,Minor,15,,0,8,1,3,0,0,0,,0,850,7,0,0,2018-08-06 15:30:10,make postConfigure default install non-distributed,"Currently postConfigure is defaulted to perform a distributed install, meaning the packages are pushed from pm1 to the other nodes.  There is a command line feature to enable non-distributed installs. 

This change is to reverse that functionality and make the default install non-distributed.

",,0,0,0,0,0.0,"make postConfigure default install non-distributed $end$ Currently postConfigure is defaulted to perform a distributed install, meaning the packages are pushed from pm1 to the other nodes.  There is a command line feature to enable non-distributed installs. 

This change is to reverse that functionality and make the default install non-distributed.

 $acceptance criteria:$",0,0,0,0,0,0,1,3672.23,25,3,0.12,0,0.0,0,0.0,0,0.0,0,0.0
57,MCOL-1248,Task,MCOL,2018-03-07 14:56:39,,0,Installations instructions are not complete,"Installation instructions must be enriched:
1. Add compatibility version mapping between all parts involved in successful setup of the plugin.
As it was already explained in emailing:
OS: Ubuntu 16.04, Debian 9 and 8, CentOS 7 
BulkWrite SDK: 1.1.3
ColumnStore: 1.1.3
Java: 8, (9)
PDI: (7.0), 7.1, 8.1
MariaDB Database Client: (2.2.1), 2.2.2

2. Mark very clear what software module is required and what is optional and why. 
Example: MariaDB Database Client is responsible for executing the DDL and it is optional component.
Bulk Data SDK is required because there is a library needed for plugin to work.
",,"Installations instructions are not complete $end$ Installation instructions must be enriched:
1. Add compatibility version mapping between all parts involved in successful setup of the plugin.
As it was already explained in emailing:
OS: Ubuntu 16.04, Debian 9 and 8, CentOS 7 
BulkWrite SDK: 1.1.3
ColumnStore: 1.1.3
Java: 8, (9)
PDI: (7.0), 7.1, 8.1
MariaDB Database Client: (2.2.1), 2.2.2

2. Mark very clear what software module is required and what is optional and why. 
Example: MariaDB Database Client is responsible for executing the DDL and it is optional component.
Bulk Data SDK is required because there is a library needed for plugin to work.
 $acceptance criteria:$",,Elena Kotsinova,Elena Kotsinova,Minor,17,,0,5,0,2,0,0,0,,0,850,5,0,0,2018-03-07 17:53:43,Installations instructions are not complete,"Installation instructions must be enriched:
1. Add compatibility version mapping between all parts involved in successful setup of the plugin.
As it was already explained in emailing:
OS: Ubuntu 16.04, Debian 9 and 8, CentOS 7 
BulkWrite SDK: 1.1.3
ColumnStore: 1.1.3
Java: 8, (9)
PDI: (7.0), 7.1, 8.1
MariaDB Database Client: (2.2.1), 2.2.2

2. Mark very clear what software module is required and what is optional and why. 
Example: MariaDB Database Client is responsible for executing the DDL and it is optional component.
Bulk Data SDK is required because there is a library needed for plugin to work.
",,0,0,0,0,0.0,"Installations instructions are not complete $end$ Installation instructions must be enriched:
1. Add compatibility version mapping between all parts involved in successful setup of the plugin.
As it was already explained in emailing:
OS: Ubuntu 16.04, Debian 9 and 8, CentOS 7 
BulkWrite SDK: 1.1.3
ColumnStore: 1.1.3
Java: 8, (9)
PDI: (7.0), 7.1, 8.1
MariaDB Database Client: (2.2.1), 2.2.2

2. Mark very clear what software module is required and what is optional and why. 
Example: MariaDB Database Client is responsible for executing the DDL and it is optional component.
Bulk Data SDK is required because there is a library needed for plugin to work.
 $acceptance criteria:$",0,0,0,0,0,0,1,2.95,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
58,MCOL-1258,New Feature,MCOL,2018-03-12 14:02:09,,0,buildbot build the binary and rpm/deb tar.gz packages,Buildbot needs to be enhanced to build the the binary and rpm/deb tar.gz packages.,,buildbot build the binary and rpm/deb tar.gz packages $end$ Buildbot needs to be enhanced to build the the binary and rpm/deb tar.gz packages. $acceptance criteria:$,,David Hill,David Hill,Major,3,,0,1,0,1,0,1,0,,0,850,1,0,0,2018-03-12 14:02:09,builbot build the binary and rpm/deb tar.gz packages,Buildbot needs to be enhanced to build the the binary and rpm/deb tar.gz packages.,,1,0,0,2,0.04,builbot build the binary and rpm/deb tar.gz packages $end$ Buildbot needs to be enhanced to build the the binary and rpm/deb tar.gz packages. $acceptance criteria:$,1,1,0,0,0,0,0,0.0,26,3,0.115385,0,0.0,0,0.0,0,0.0,0,0.0
59,MCOL-1259,New Feature,MCOL,2018-03-12 14:13:04,,0,ColumnStore Data-Adapters needs a top level cmake for building all Data-Adapters,"Currently each ColumnStore Data Adapters is built separately on the build machines and buildbot. So any time a new adapter is added, the build scripts have to be updated. Need a top level cmake file that will build all data-adapters at once to simply the build process.",,"ColumnStore Data-Adapters needs a top level cmake for building all Data-Adapters $end$ Currently each ColumnStore Data Adapters is built separately on the build machines and buildbot. So any time a new adapter is added, the build scripts have to be updated. Need a top level cmake file that will build all data-adapters at once to simply the build process. $acceptance criteria:$",,David Hill,David Hill,Major,6,,0,4,0,2,0,0,0,,0,850,2,0,0,2018-05-18 23:10:35,ColumnStore Data-Adapters needs a top level cmake for building all Data-Adapters,"Currently each ColumnStore Data Adapters is built separately on the build machines and buildbot. So any time a new adapter is added, the build scripts have to be updated. Need a top level cmake file that will build all data-adapters at once to simply the build process.",,0,0,0,0,0.0,"ColumnStore Data-Adapters needs a top level cmake for building all Data-Adapters $end$ Currently each ColumnStore Data Adapters is built separately on the build machines and buildbot. So any time a new adapter is added, the build scripts have to be updated. Need a top level cmake file that will build all data-adapters at once to simply the build process. $acceptance criteria:$",0,0,0,0,0,0,1,1616.95,27,4,0.148148,0,0.0,0,0.0,0,0.0,0,0.0
60,MCOL-1263,Sub-Task,MCOL,2018-03-12 17:20:28,,0,Setup buildbot to use github pull requests to schedule builds,Get buildbot to schedule builds based on pull requests and not individual commits. Cleanup buildbot configurations to be more flexible. build projects out-of-source.,,Setup buildbot to use github pull requests to schedule builds $end$ Get buildbot to schedule builds based on pull requests and not individual commits. Cleanup buildbot configurations to be more flexible. build projects out-of-source. $acceptance criteria:$,,Ben Thompson,Ben Thompson,Minor,3,,0,0,0,29,0,0,0,,0,850,0,0,0,2018-03-12 17:20:28,Setup buildbot to use github pull requests to schedule builds,Get buildbot to schedule builds based on pull requests and not individual commits. Cleanup buildbot configurations to be more flexible. build projects out-of-source.,,0,0,0,0,0.0,Setup buildbot to use github pull requests to schedule builds $end$ Get buildbot to schedule builds based on pull requests and not individual commits. Cleanup buildbot configurations to be more flexible. build projects out-of-source. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
61,MCOL-1270,Sub-Task,MCOL,2018-03-14 18:37:47,,0,Test for Pull Requests,This is placeholder for creating pull request to test buildbot scheduling and reporting,,Test for Pull Requests $end$ This is placeholder for creating pull request to test buildbot scheduling and reporting $acceptance criteria:$,,Ben Thompson,Ben Thompson,Trivial,1,,0,1,0,29,0,0,0,,0,850,1,0,0,2018-03-14 18:37:47,Test for Pull Requests,This is placeholder for creating pull request to test buildbot scheduling and reporting,,0,0,0,0,0.0,Test for Pull Requests $end$ This is placeholder for creating pull request to test buildbot scheduling and reporting $acceptance criteria:$,0,0,0,0,0,0,1,0.0,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
62,MCOL-1271,Sub-Task,MCOL,2018-03-14 18:41:15,,0,Migrate buildbot to new AWS,All buildbot master and worker nodes will need to be moved to new account,,Migrate buildbot to new AWS $end$ All buildbot master and worker nodes will need to be moved to new account $acceptance criteria:$,,Ben Thompson,Ben Thompson,Minor,2,,0,1,0,29,0,0,0,,0,850,1,0,0,2018-03-14 18:41:15,Migrate buildbot to new AWS,All buildbot master and worker nodes will need to be moved to new account,,0,0,0,0,0.0,Migrate buildbot to new AWS $end$ All buildbot master and worker nodes will need to be moved to new account $acceptance criteria:$,0,0,0,0,0,0,1,0.0,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
63,MCOL-1272,Sub-Task,MCOL,2018-03-14 18:44:34,,0,Fix buildbot status reporting to github,Need to enable reporting to github pull requests with results from pull request scheduling of buildbot builds,,Fix buildbot status reporting to github $end$ Need to enable reporting to github pull requests with results from pull request scheduling of buildbot builds $acceptance criteria:$,,Ben Thompson,Ben Thompson,Minor,2,,0,0,0,29,0,0,0,,0,850,0,0,0,2018-03-14 18:44:34,Fix buildbot status reporting to github,Need to enable reporting to github pull requests with results from pull request scheduling of buildbot builds,,0,0,0,0,0.0,Fix buildbot status reporting to github $end$ Need to enable reporting to github pull requests with results from pull request scheduling of buildbot builds $acceptance criteria:$,0,0,0,0,0,0,1,0.0,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
64,MCOL-1273,Sub-Task,MCOL,2018-03-14 18:47:42,,0,enable manual builds on any branch from buildbot,buildbot force builds should be allowed to schedule builds from any branch on github.,,enable manual builds on any branch from buildbot $end$ buildbot force builds should be allowed to schedule builds from any branch on github. $acceptance criteria:$,,Ben Thompson,Ben Thompson,Minor,1,,0,1,0,29,0,0,0,,0,850,1,0,0,2018-03-14 18:47:42,enable manual builds on any branch from buildbot,buildbot force builds should be allowed to schedule builds from any branch on github.,,0,0,0,0,0.0,enable manual builds on any branch from buildbot $end$ buildbot force builds should be allowed to schedule builds from any branch on github. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
65,MCOL-1275,New Feature,MCOL,2018-03-15 16:31:42,,0,Usability Improvements: Settings tab must be first in the form and Field mapping must be second,"*MariaDB ColumnStore Bulk Loader form*
The tabs must switch their places as follows:
Settings must be first and selected by default.
Field mapping must be second tab  in the form.

Thus the expected user workflow will be achieved:
User setup connection parameters first and after this maps the fields.",,"Usability Improvements: Settings tab must be first in the form and Field mapping must be second $end$ *MariaDB ColumnStore Bulk Loader form*
The tabs must switch their places as follows:
Settings must be first and selected by default.
Field mapping must be second tab  in the form.

Thus the expected user workflow will be achieved:
User setup connection parameters first and after this maps the fields. $acceptance criteria:$",,Elena Kotsinova,Elena Kotsinova,Major,6,,0,3,0,1,0,0,0,,0,850,1,0,0,2018-03-19 16:32:15,Usability Improvements: Settings tab must be first in the form and Field mapping must be second,"*MariaDB ColumnStore Bulk Loader form*
The tabs must switch their places as follows:
Settings must be first and selected by default.
Field mapping must be second tab  in the form.

Thus the expected user workflow will be achieved:
User setup connection parameters first and after this maps the fields.",,0,0,0,0,0.0,"Usability Improvements: Settings tab must be first in the form and Field mapping must be second $end$ *MariaDB ColumnStore Bulk Loader form*
The tabs must switch their places as follows:
Settings must be first and selected by default.
Field mapping must be second tab  in the form.

Thus the expected user workflow will be achieved:
User setup connection parameters first and after this maps the fields. $acceptance criteria:$",0,0,0,0,0,0,0,96.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
66,MCOL-128,Sub-Task,MCOL,2016-06-12 00:23:39,,0,ALTER TABLE conversion to columnstore fails,"mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
",,"ALTER TABLE conversion to columnstore fails $end$ mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
 $acceptance criteria:$",,Justin Swanhart,Justin Swanhart,Major,24,,2,6,2,6,0,0,0,,0,850,6,0,0,2016-06-12 00:23:39,ALTER TABLE conversion to columnstore fails,"mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
",,0,0,0,0,0.0,"ALTER TABLE conversion to columnstore fails $end$ mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
67,MCOL-1281,New Feature,MCOL,2018-03-16 21:05:23,,0,mcsapi Windows support,"Next to Linux, Windows 10 should also be supported.",,"mcsapi Windows support $end$ Next to Linux, Windows 10 should also be supported. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,19,,0,3,5,6,0,0,0,,0,850,3,0,0,2018-07-11 20:43:37,mcsapi Windows support,"Next to Linux, Windows 10 should also be supported.",,0,0,0,0,0.0,"mcsapi Windows support $end$ Next to Linux, Windows 10 should also be supported. $acceptance criteria:$",0,0,0,0,0,0,1,2807.63,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
68,MCOL-1283,New Feature,MCOL,2018-03-16 21:40:08,,0,Try to pack/load the shared bulk write sdk library in/from jar to ease installation process,"If it's possible add the shared Bulk Write SDK library into the jar to ease the installation process.

If possible the spoon initialization script hasn't to be changed any more and the plugin can just be copied and pasted.",,"Try to pack/load the shared bulk write sdk library in/from jar to ease installation process $end$ If it's possible add the shared Bulk Write SDK library into the jar to ease the installation process.

If possible the spoon initialization script hasn't to be changed any more and the plugin can just be copied and pasted. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-03-19 18:41:58,Try to pack/load the shared bulk write sdk library in/from jar to ease installation process,"If it's possible add the shared Bulk Write SDK library into the jar to ease the installation process.

If possible the spoon initialization script hasn't to be changed any more and the plugin can just be copied and pasted.",,0,0,0,0,0.0,"Try to pack/load the shared bulk write sdk library in/from jar to ease installation process $end$ If it's possible add the shared Bulk Write SDK library into the jar to ease the installation process.

If possible the spoon initialization script hasn't to be changed any more and the plugin can just be copied and pasted. $acceptance criteria:$",0,0,0,0,0,0,1,69.0167,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
69,MCOL-1293,Task,MCOL,2018-03-22 15:05:20,,0,new package dependency - libnuma.so.1,"There is a new package dependency for the 1.1.4 installs... So the setup documnet, github server README and columnstoreClusterTester all need to be updated for all OS versions.

post-mysqld-install Successfully Completed
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock' (2)
Error running post-mysql-install, /tmp/post-mysql-install.log
Exiting...
[root@centos-7-yum-pm1 bin]# cat /tmp/post-mysqld-install.log 
/usr/local/mariadb/columnstore/mysql/bin/mysqld: error while loading shared libraries: libnuma.so.1: cannot open shared object file: No such file or directory

centos 7

---> Package numactl-libs.x86_64 0:2.0.9-6.el7_2 will be installed",,"new package dependency - libnuma.so.1 $end$ There is a new package dependency for the 1.1.4 installs... So the setup documnet, github server README and columnstoreClusterTester all need to be updated for all OS versions.

post-mysqld-install Successfully Completed
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock' (2)
Error running post-mysql-install, /tmp/post-mysql-install.log
Exiting...
[root@centos-7-yum-pm1 bin]# cat /tmp/post-mysqld-install.log 
/usr/local/mariadb/columnstore/mysql/bin/mysqld: error while loading shared libraries: libnuma.so.1: cannot open shared object file: No such file or directory

centos 7

---> Package numactl-libs.x86_64 0:2.0.9-6.el7_2 will be installed $acceptance criteria:$",,David Hill,David Hill,Trivial,8,,0,3,0,1,0,0,0,,0,850,3,0,0,2018-04-16 08:49:20,new package dependency - libnuma.so.1,"There is a new package dependency for the 1.1.4 installs... So the setup documnet, github server README and columnstoreClusterTester all need to be updated for all OS versions.

post-mysqld-install Successfully Completed
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock' (2)
Error running post-mysql-install, /tmp/post-mysql-install.log
Exiting...
[root@centos-7-yum-pm1 bin]# cat /tmp/post-mysqld-install.log 
/usr/local/mariadb/columnstore/mysql/bin/mysqld: error while loading shared libraries: libnuma.so.1: cannot open shared object file: No such file or directory

centos 7

---> Package numactl-libs.x86_64 0:2.0.9-6.el7_2 will be installed",,0,0,0,0,0.0,"new package dependency - libnuma.so.1 $end$ There is a new package dependency for the 1.1.4 installs... So the setup documnet, github server README and columnstoreClusterTester all need to be updated for all OS versions.

post-mysqld-install Successfully Completed
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock' (2)
Error running post-mysql-install, /tmp/post-mysql-install.log
Exiting...
[root@centos-7-yum-pm1 bin]# cat /tmp/post-mysqld-install.log 
/usr/local/mariadb/columnstore/mysql/bin/mysqld: error while loading shared libraries: libnuma.so.1: cannot open shared object file: No such file or directory

centos 7

---> Package numactl-libs.x86_64 0:2.0.9-6.el7_2 will be installed $acceptance criteria:$",0,0,0,0,0,0,0,593.733,28,4,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
70,MCOL-1296,New Feature,MCOL,2018-03-22 23:07:14,,0,Add debug output as an API option,"At the moment debug output is a compile time option, we should make it a run time option instead.",,"Add debug output as an API option $end$ At the moment debug output is a compile time option, we should make it a run time option instead. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,6,,0,2,0,2,0,0,0,,0,850,2,0,0,2018-03-22 23:07:14,Add debug output as an API option,"At the moment debug output is a compile time option, we should make it a run time option instead.",,0,0,0,0,0.0,"Add debug output as an API option $end$ At the moment debug output is a compile time option, we should make it a run time option instead. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,47,2,0.0425532,2,0.0425532,1,0.0212766,1,0.0212766,1,0.0212766
71,MCOL-1312,New Feature,MCOL,2018-03-28 16:47:43,,0,PDI version number - include git,Next to the main version number a sub version number showing the git tree should be added.,,PDI version number - include git $end$ Next to the main version number a sub version number showing the git tree should be added. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Minor,6,,0,3,0,1,0,0,0,,0,850,3,0,0,2018-03-28 16:47:43,PDI version number - include git,Next to the main version number a sub version number showing the git tree should be added.,,0,0,0,0,0.0,PDI version number - include git $end$ Next to the main version number a sub version number showing the git tree should be added. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,11,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
72,MCOL-1318,New Feature,MCOL,2018-04-02 11:07:32,,0,Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled ,"Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled 


it would be more appropriate to evaluate Warning instead of Failure
and run in additional checks  if Firewall Services or SELINUX are enabled 
in order to MDB Columnstore to be  functional/compatible  over/to  security configurations

currently :
although the   Firewall Services  are configured properly regarding the preparation requirement guide
[MariaDB ColumnStore port usage|https://mariadb.com/kb/en/library/preparing-for-columnstore-installation-11x/#mariadb-columnstore-port-usage]
so that  Columnstore is being operational , Columnstore Cluster Tester tool is evaluating Failure

how to repeat :
enable the Firewall Services with required ports and run the Columnstore Cluster Tester tool
{noformat}
*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : CentOS Linux 7 (Core)

** Run Ping access Test to remote nodes

172.20.2.205  Node Passed ping test
172.20.2.206  Node Passed ping test

** Run SSH Login access Test to remote nodes

172.20.2.205  Node Passed SSH login test using ssh-keys
172.20.2.206  Node Passed SSH login test using ssh-keys

** Run OS check - OS version needs to be the same on all nodes

Local Node OS Version : CentOS Linux 7 (Core)

172.20.2.205 Node OS Version : CentOS Linux 7 (Core)
172.20.2.206 Node OS Version : CentOS Linux 7 (Core)

** Run Locale check - Locale needs to be the same on all nodes

Local Node Locale : LANG=en_US.UTF-8
172.20.2.205 Node Locale : LANG=en_US.UTF-8
172.20.2.206 Node Locale : LANG=en_US.UTF-8

** Run SELINUX check - Setting should to be disabled on all nodes

Local Node SELINUX setting is Not Enabled
172.20.2.205 Node SELINUX setting is Not Enabled
172.20.2.206 Node SELINUX setting is Not Enabled

** Run Firewall Services check - Firewall Services should to be Inactive on all nodes

Local Node iptables service is Not Active
Local Node ufw service is Not Active
Local Node firewalld service is Not Active
Local Node firewall service is Not Active

172.20.2.205 Node iptables service is Not Enabled
172.20.2.205 Node ufw service is Not Enabled
172.20.2.205 Node firewalld service is Not Enabled
172.20.2.205 Node firewall service is Not Enabled

172.20.2.206 Node iptables service is Not Enabled
172.20.2.206 Node ufw service is Not Enabled
Failed, 172.20.2.206 Node firewalld service is Active, please disable
172.20.2.206 Node firewall service is Not Enabled


Failure occurred, do you want to continue? (y,n) > 
{noformat}


The problem is that all needed ports are enabled and  Columnstore service is operational
but  Columnstore Cluster Tester tool evaluates Failure
",,"Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled  $end$ Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled 


it would be more appropriate to evaluate Warning instead of Failure
and run in additional checks  if Firewall Services or SELINUX are enabled 
in order to MDB Columnstore to be  functional/compatible  over/to  security configurations

currently :
although the   Firewall Services  are configured properly regarding the preparation requirement guide
[MariaDB ColumnStore port usage|https://mariadb.com/kb/en/library/preparing-for-columnstore-installation-11x/#mariadb-columnstore-port-usage]
so that  Columnstore is being operational , Columnstore Cluster Tester tool is evaluating Failure

how to repeat :
enable the Firewall Services with required ports and run the Columnstore Cluster Tester tool
{noformat}
*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : CentOS Linux 7 (Core)

** Run Ping access Test to remote nodes

172.20.2.205  Node Passed ping test
172.20.2.206  Node Passed ping test

** Run SSH Login access Test to remote nodes

172.20.2.205  Node Passed SSH login test using ssh-keys
172.20.2.206  Node Passed SSH login test using ssh-keys

** Run OS check - OS version needs to be the same on all nodes

Local Node OS Version : CentOS Linux 7 (Core)

172.20.2.205 Node OS Version : CentOS Linux 7 (Core)
172.20.2.206 Node OS Version : CentOS Linux 7 (Core)

** Run Locale check - Locale needs to be the same on all nodes

Local Node Locale : LANG=en_US.UTF-8
172.20.2.205 Node Locale : LANG=en_US.UTF-8
172.20.2.206 Node Locale : LANG=en_US.UTF-8

** Run SELINUX check - Setting should to be disabled on all nodes

Local Node SELINUX setting is Not Enabled
172.20.2.205 Node SELINUX setting is Not Enabled
172.20.2.206 Node SELINUX setting is Not Enabled

** Run Firewall Services check - Firewall Services should to be Inactive on all nodes

Local Node iptables service is Not Active
Local Node ufw service is Not Active
Local Node firewalld service is Not Active
Local Node firewall service is Not Active

172.20.2.205 Node iptables service is Not Enabled
172.20.2.205 Node ufw service is Not Enabled
172.20.2.205 Node firewalld service is Not Enabled
172.20.2.205 Node firewall service is Not Enabled

172.20.2.206 Node iptables service is Not Enabled
172.20.2.206 Node ufw service is Not Enabled
Failed, 172.20.2.206 Node firewalld service is Active, please disable
172.20.2.206 Node firewall service is Not Enabled


Failure occurred, do you want to continue? (y,n) > 
{noformat}


The problem is that all needed ports are enabled and  Columnstore service is operational
but  Columnstore Cluster Tester tool evaluates Failure
 $acceptance criteria:$",,Zdravelina Sokolovska,Zdravelina Sokolovska,Major,8,,0,4,0,2,0,0,0,,0,850,4,0,0,2018-04-02 21:46:20,Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled ,"Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled 


it would be more appropriate to evaluate Warning instead of Failure
and run in additional checks  if Firewall Services or SELINUX are enabled 
in order to MDB Columnstore to be  functional/compatible  over/to  security configurations

currently :
although the   Firewall Services  are configured properly regarding the preparation requirement guide
[MariaDB ColumnStore port usage|https://mariadb.com/kb/en/library/preparing-for-columnstore-installation-11x/#mariadb-columnstore-port-usage]
so that  Columnstore is being operational , Columnstore Cluster Tester tool is evaluating Failure

how to repeat :
enable the Firewall Services with required ports and run the Columnstore Cluster Tester tool
{noformat}
*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : CentOS Linux 7 (Core)

** Run Ping access Test to remote nodes

172.20.2.205  Node Passed ping test
172.20.2.206  Node Passed ping test

** Run SSH Login access Test to remote nodes

172.20.2.205  Node Passed SSH login test using ssh-keys
172.20.2.206  Node Passed SSH login test using ssh-keys

** Run OS check - OS version needs to be the same on all nodes

Local Node OS Version : CentOS Linux 7 (Core)

172.20.2.205 Node OS Version : CentOS Linux 7 (Core)
172.20.2.206 Node OS Version : CentOS Linux 7 (Core)

** Run Locale check - Locale needs to be the same on all nodes

Local Node Locale : LANG=en_US.UTF-8
172.20.2.205 Node Locale : LANG=en_US.UTF-8
172.20.2.206 Node Locale : LANG=en_US.UTF-8

** Run SELINUX check - Setting should to be disabled on all nodes

Local Node SELINUX setting is Not Enabled
172.20.2.205 Node SELINUX setting is Not Enabled
172.20.2.206 Node SELINUX setting is Not Enabled

** Run Firewall Services check - Firewall Services should to be Inactive on all nodes

Local Node iptables service is Not Active
Local Node ufw service is Not Active
Local Node firewalld service is Not Active
Local Node firewall service is Not Active

172.20.2.205 Node iptables service is Not Enabled
172.20.2.205 Node ufw service is Not Enabled
172.20.2.205 Node firewalld service is Not Enabled
172.20.2.205 Node firewall service is Not Enabled

172.20.2.206 Node iptables service is Not Enabled
172.20.2.206 Node ufw service is Not Enabled
Failed, 172.20.2.206 Node firewalld service is Active, please disable
172.20.2.206 Node firewall service is Not Enabled


Failure occurred, do you want to continue? (y,n) > 
{noformat}


The problem is that all needed ports are enabled and  Columnstore service is operational
but  Columnstore Cluster Tester tool evaluates Failure
",,0,0,0,0,0.0,"Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled  $end$ Columnstore Cluster Tester tool  is evaluating Failure if Firewall Services or SELINUX are enabled 


it would be more appropriate to evaluate Warning instead of Failure
and run in additional checks  if Firewall Services or SELINUX are enabled 
in order to MDB Columnstore to be  functional/compatible  over/to  security configurations

currently :
although the   Firewall Services  are configured properly regarding the preparation requirement guide
[MariaDB ColumnStore port usage|https://mariadb.com/kb/en/library/preparing-for-columnstore-installation-11x/#mariadb-columnstore-port-usage]
so that  Columnstore is being operational , Columnstore Cluster Tester tool is evaluating Failure

how to repeat :
enable the Firewall Services with required ports and run the Columnstore Cluster Tester tool
{noformat}
*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : CentOS Linux 7 (Core)

** Run Ping access Test to remote nodes

172.20.2.205  Node Passed ping test
172.20.2.206  Node Passed ping test

** Run SSH Login access Test to remote nodes

172.20.2.205  Node Passed SSH login test using ssh-keys
172.20.2.206  Node Passed SSH login test using ssh-keys

** Run OS check - OS version needs to be the same on all nodes

Local Node OS Version : CentOS Linux 7 (Core)

172.20.2.205 Node OS Version : CentOS Linux 7 (Core)
172.20.2.206 Node OS Version : CentOS Linux 7 (Core)

** Run Locale check - Locale needs to be the same on all nodes

Local Node Locale : LANG=en_US.UTF-8
172.20.2.205 Node Locale : LANG=en_US.UTF-8
172.20.2.206 Node Locale : LANG=en_US.UTF-8

** Run SELINUX check - Setting should to be disabled on all nodes

Local Node SELINUX setting is Not Enabled
172.20.2.205 Node SELINUX setting is Not Enabled
172.20.2.206 Node SELINUX setting is Not Enabled

** Run Firewall Services check - Firewall Services should to be Inactive on all nodes

Local Node iptables service is Not Active
Local Node ufw service is Not Active
Local Node firewalld service is Not Active
Local Node firewall service is Not Active

172.20.2.205 Node iptables service is Not Enabled
172.20.2.205 Node ufw service is Not Enabled
172.20.2.205 Node firewalld service is Not Enabled
172.20.2.205 Node firewall service is Not Enabled

172.20.2.206 Node iptables service is Not Enabled
172.20.2.206 Node ufw service is Not Enabled
Failed, 172.20.2.206 Node firewalld service is Active, please disable
172.20.2.206 Node firewall service is Not Enabled


Failure occurred, do you want to continue? (y,n) > 
{noformat}


The problem is that all needed ports are enabled and  Columnstore service is operational
but  Columnstore Cluster Tester tool evaluates Failure
 $acceptance criteria:$",0,0,0,0,0,0,1,10.6333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
73,MCOL-1319,Task,MCOL,2018-04-03 08:43:42,,0,Merge MariaDB 10.2.14,MariaDB 10.2.14 has been released and needs merging into ColumnStore,,Merge MariaDB 10.2.14 $end$ MariaDB 10.2.14 has been released and needs merging into ColumnStore $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,2,0,2,0,0,0,,0,850,2,0,0,2018-04-03 08:43:42,Merge MariaDB 10.2.14,MariaDB 10.2.14 has been released and needs merging into ColumnStore,,0,0,0,0,0.0,Merge MariaDB 10.2.14 $end$ MariaDB 10.2.14 has been released and needs merging into ColumnStore $acceptance criteria:$,0,0,0,0,0,0,1,0.0,48,2,0.0416667,2,0.0416667,1,0.0208333,1,0.0208333,1,0.0208333
74,MCOL-1330,New Feature,MCOL,2018-04-09 15:34:41,,0,Make ColumnStore work under valgrind,Need an option in ExeMgr and other processes to bypass the results of setupResources() as this fails under valgrind causing immediate abort.,,Make ColumnStore work under valgrind $end$ Need an option in ExeMgr and other processes to bypass the results of setupResources() as this fails under valgrind causing immediate abort. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,2,2,2,1,0,0,0,,0,850,2,0,0,2018-06-25 13:02:24,Make ColumnStore work under valgrind,Need an option in ExeMgr and other processes to bypass the results of setupResources() as this fails under valgrind causing immediate abort.,,0,0,0,0,0.0,Make ColumnStore work under valgrind $end$ Need an option in ExeMgr and other processes to bypass the results of setupResources() as this fails under valgrind causing immediate abort. $acceptance criteria:$,0,0,0,0,0,0,0,1845.45,49,2,0.0408163,2,0.0408163,1,0.0204082,1,0.0204082,1,0.0204082
75,MCOL-1333,Task,MCOL,2018-04-11 22:53:14,,0,Document that .tar.gz of RPM files is needed for addModule command,"If a user tries to add a module and the .tar.gz containing the ColumnStore RPMs is not present, then they can see an error like this:

{noformat}
Apr 11 11:06:40 server1 ProcessManager[189417]: 40.293266 |0|0|0| E 17 CAL0000: line: 4546 addModule - ERROR: Package not found: /root/mariadb-columnstore*1.1.3-1*.rpm.tar.gz
{noformat}

This requirement does not seem to be documented here:

https://mariadb.com/kb/en/library/managing-columnstore-module-configurations/#adding-modules

We should probably document this.",,"Document that .tar.gz of RPM files is needed for addModule command $end$ If a user tries to add a module and the .tar.gz containing the ColumnStore RPMs is not present, then they can see an error like this:

{noformat}
Apr 11 11:06:40 server1 ProcessManager[189417]: 40.293266 |0|0|0| E 17 CAL0000: line: 4546 addModule - ERROR: Package not found: /root/mariadb-columnstore*1.1.3-1*.rpm.tar.gz
{noformat}

This requirement does not seem to be documented here:

https://mariadb.com/kb/en/library/managing-columnstore-module-configurations/#adding-modules

We should probably document this. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,7,,0,5,0,1,0,0,0,,0,850,4,0,0,2018-04-13 13:33:29,Document that .tar.gz of RPM files is needed for addModule command,"If a user tries to add a module and the .tar.gz containing the ColumnStore RPMs is not present, then they can see an error like this:

{noformat}
Apr 11 11:06:40 server1 ProcessManager[189417]: 40.293266 |0|0|0| E 17 CAL0000: line: 4546 addModule - ERROR: Package not found: /root/mariadb-columnstore*1.1.3-1*.rpm.tar.gz
{noformat}

This requirement does not seem to be documented here:

https://mariadb.com/kb/en/library/managing-columnstore-module-configurations/#adding-modules

We should probably document this.",,0,0,0,0,0.0,"Document that .tar.gz of RPM files is needed for addModule command $end$ If a user tries to add a module and the .tar.gz containing the ColumnStore RPMs is not present, then they can see an error like this:

{noformat}
Apr 11 11:06:40 server1 ProcessManager[189417]: 40.293266 |0|0|0| E 17 CAL0000: line: 4546 addModule - ERROR: Package not found: /root/mariadb-columnstore*1.1.3-1*.rpm.tar.gz
{noformat}

This requirement does not seem to be documented here:

https://mariadb.com/kb/en/library/managing-columnstore-module-configurations/#adding-modules

We should probably document this. $acceptance criteria:$",0,0,0,0,0,0,0,38.6667,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
76,MCOL-1337,New Feature,MCOL,2018-04-12 18:39:34,,0,Add the M18 ML example notebook to the docker image,"During the webinar I was asked to publish the M18 demo ML example Jupyter notebook.

Therefore, it should be added to the docker image on GitHub and DockerHub.",,"Add the M18 ML example notebook to the docker image $end$ During the webinar I was asked to publish the M18 demo ML example Jupyter notebook.

Therefore, it should be added to the docker image on GitHub and DockerHub. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Minor,5,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-05-01 17:00:28,Add the M18 ML example notebook to the docker image,"During the webinar I was asked to publish the M18 demo ML example Jupyter notebook.

Therefore, it should be added to the docker image on GitHub and DockerHub.",,0,0,0,0,0.0,"Add the M18 ML example notebook to the docker image $end$ During the webinar I was asked to publish the M18 demo ML example Jupyter notebook.

Therefore, it should be added to the docker image on GitHub and DockerHub. $acceptance criteria:$",0,0,0,0,0,0,1,454.333,12,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
77,MCOL-1344,New Feature,MCOL,2018-04-16 19:30:54,,0,CREATE table STATEMENT from Spark Dataframe structure,"Add a generateCreateTableSQL function for Spark DataFrames, which can be used to create the according CREATE TABLE statement to export a DataFrame.",,"CREATE table STATEMENT from Spark Dataframe structure $end$ Add a generateCreateTableSQL function for Spark DataFrames, which can be used to create the according CREATE TABLE statement to export a DataFrame. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,8,,0,1,0,3,0,0,0,,0,850,0,0,0,2018-04-20 06:26:26,CREATE table STATEMENT from Spark Dataframe structure,"Add a generateCreateTableSQL function for Spark DataFrames, which can be used to create the according CREATE TABLE statement to export a DataFrame.",,0,0,0,0,0.0,"CREATE table STATEMENT from Spark Dataframe structure $end$ Add a generateCreateTableSQL function for Spark DataFrames, which can be used to create the according CREATE TABLE statement to export a DataFrame. $acceptance criteria:$",0,0,0,0,0,0,1,82.9167,13,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
78,MCOL-1362,New Feature,MCOL,2018-04-24 00:58:43,,0,Add a export function that utilizes (sequential) write from Spark workers,"The current export function calls collect() on the DataFrame, and thereby writes it to memory in the Spark driver. This can lead to ridiculous amounts of memory usage (depending on the DF size). The current option only needs the bulk write SDK installed on Spark's driver and avoids concurrency problems, as the driver is the only process writing to CS.

Another option is to export each DataFrame's partition directly from the worker. This would result in less memory usage. On the downside every worker needs to have the CS bulk write API installed and we might run into concurrency problems if multiple processes want to write simultaneously to the same table.

This ticket covers the export from worker nodes and not the driver.

Depending on the concurrency problem, we might want to consider writing sequentially from each worker, or writing in parallel to different tables and joining them afterwards.",,"Add a export function that utilizes (sequential) write from Spark workers $end$ The current export function calls collect() on the DataFrame, and thereby writes it to memory in the Spark driver. This can lead to ridiculous amounts of memory usage (depending on the DF size). The current option only needs the bulk write SDK installed on Spark's driver and avoids concurrency problems, as the driver is the only process writing to CS.

Another option is to export each DataFrame's partition directly from the worker. This would result in less memory usage. On the downside every worker needs to have the CS bulk write API installed and we might run into concurrency problems if multiple processes want to write simultaneously to the same table.

This ticket covers the export from worker nodes and not the driver.

Depending on the concurrency problem, we might want to consider writing sequentially from each worker, or writing in parallel to different tables and joining them afterwards. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,15,,1,2,2,1,0,1,0,,0,850,0,1,0,2018-11-10 01:21:59,Add a export function that utilizes (sequential) write from Spark workers,"The current export function calls collect() on the DataFrame, and thereby writes it to memory in the Spark driver. This can lead to ridiculous amounts of memory usage (depending on the DF size). The current option only needs the bulk write SDK installed on Spark's driver and avoids concurrency problems, as the driver is the only process writing to CS.

Another option is to export each DataFrame's partition directly from the worker. This would result in less memory usage. On the downside every worker needs to have the CS bulk write API installed and we might run into concurrency problems if multiple processes want to write simultaneously to the same table.

This ticket covers the export from worker nodes and not the driver.

Depending on the concurrency problem, we might want to consider writing sequentially from each worker, or writing in parallel to different tables and joining them afterwards.",,0,0,0,0,0.0,"Add a export function that utilizes (sequential) write from Spark workers $end$ The current export function calls collect() on the DataFrame, and thereby writes it to memory in the Spark driver. This can lead to ridiculous amounts of memory usage (depending on the DF size). The current option only needs the bulk write SDK installed on Spark's driver and avoids concurrency problems, as the driver is the only process writing to CS.

Another option is to export each DataFrame's partition directly from the worker. This would result in less memory usage. On the downside every worker needs to have the CS bulk write API installed and we might run into concurrency problems if multiple processes want to write simultaneously to the same table.

This ticket covers the export from worker nodes and not the driver.

Depending on the concurrency problem, we might want to consider writing sequentially from each worker, or writing in parallel to different tables and joining them afterwards. $acceptance criteria:$",0,0,0,0,0,0,0,4800.38,14,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
79,MCOL-1364,New Feature,MCOL,2018-04-24 22:55:55,,0,Update mariadb-columnstore-api package names to amd64 in make file for Debian/Ubuntu,"Change Debian / Ubuntu package name to align with ColumStore's convention.

old: mariadb-columnstore-api-1.1.3-1-x86_64-stretch.deb
new: mariadb-columnstore-api-1.1.4-1-stretch.amd64.deb",,"Update mariadb-columnstore-api package names to amd64 in make file for Debian/Ubuntu $end$ Change Debian / Ubuntu package name to align with ColumStore's convention.

old: mariadb-columnstore-api-1.1.3-1-x86_64-stretch.deb
new: mariadb-columnstore-api-1.1.4-1-stretch.amd64.deb $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Trivial,9,,0,1,0,3,0,0,0,,0,850,0,0,0,2018-05-01 16:49:54,Update mariadb-columnstore-api package names to amd64 in make file for Debian/Ubuntu,"Change Debian / Ubuntu package name to align with ColumStore's convention.

old: mariadb-columnstore-api-1.1.3-1-x86_64-stretch.deb
new: mariadb-columnstore-api-1.1.4-1-stretch.amd64.deb",,0,0,0,0,0.0,"Update mariadb-columnstore-api package names to amd64 in make file for Debian/Ubuntu $end$ Change Debian / Ubuntu package name to align with ColumStore's convention.

old: mariadb-columnstore-api-1.1.3-1-x86_64-stretch.deb
new: mariadb-columnstore-api-1.1.4-1-stretch.amd64.deb $acceptance criteria:$",0,0,0,0,0,0,1,161.883,15,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
80,MCOL-1376,New Feature,MCOL,2018-05-02 08:16:22,,0,Support Ubuntu 18.04,Ubuntu 18.04 is released and ColumnStore won't compile in it... A few minor things need fixing.,,Support Ubuntu 18.04 $end$ Ubuntu 18.04 is released and ColumnStore won't compile in it... A few minor things need fixing. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,16,,0,7,0,7,0,0,0,,0,850,7,0,0,2018-05-02 08:16:22,Support Ubuntu 18.04,Ubuntu 18.04 is released and ColumnStore won't compile in it... A few minor things need fixing.,,0,0,0,0,0.0,Support Ubuntu 18.04 $end$ Ubuntu 18.04 is released and ColumnStore won't compile in it... A few minor things need fixing. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,50,2,0.04,2,0.04,1,0.02,1,0.02,1,0.02
81,MCOL-1378,New Feature,MCOL,2018-05-02 15:23:25,,0,Hardening Flags pt 2,"We added some hardening compile flags in 1.1, we should consider looking into a newer set for 1.2:

https://fedoraproject.org/wiki/Changes/HardeningFlags28

-D_GLIBCXX_ASSERTIONS interests me in particular for C++ vector/array bounds checking.",,"Hardening Flags pt 2 $end$ We added some hardening compile flags in 1.1, we should consider looking into a newer set for 1.2:

https://fedoraproject.org/wiki/Changes/HardeningFlags28

-D_GLIBCXX_ASSERTIONS interests me in particular for C++ vector/array bounds checking. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,19,,0,3,0,5,0,0,1,,0,850,3,0,0,2018-06-22 20:26:49,Hardening Flags pt 2,"We added some hardening compile flags in 1.1, we should consider looking into a newer set for 1.2:

https://fedoraproject.org/wiki/Changes/HardeningFlags28

-D_GLIBCXX_ASSERTIONS interests me in particular for C++ vector/array bounds checking.",,0,0,0,0,0.0,"Hardening Flags pt 2 $end$ We added some hardening compile flags in 1.1, we should consider looking into a newer set for 1.2:

https://fedoraproject.org/wiki/Changes/HardeningFlags28

-D_GLIBCXX_ASSERTIONS interests me in particular for C++ vector/array bounds checking. $acceptance criteria:$",0,0,0,0,0,0,1,1229.05,51,2,0.0392157,2,0.0392157,1,0.0196078,1,0.0196078,1,0.0196078
82,MCOL-138,Task,MCOL,2016-06-14 17:59:38,,0,postConfigure mentions MySQL,"postConfigure says, a bit into the processing:
Running the MariaDB Columnstore MySQL setup scripts
This should probably say
Running the MariaDB Columnstore setup scripts
",,"postConfigure mentions MySQL $end$ postConfigure says, a bit into the processing:
Running the MariaDB Columnstore MySQL setup scripts
This should probably say
Running the MariaDB Columnstore setup scripts
 $acceptance criteria:$",,Anders Karlsson,Anders Karlsson,Minor,12,,0,3,0,1,0,0,0,,0,850,1,0,0,2016-07-19 19:52:39,postConfigure mentions MySQL,"postConfigure says, a bit into the processing:
Running the MariaDB Columnstore MySQL setup scripts
This should probably say
Running the MariaDB Columnstore setup scripts
",,0,0,0,0,0.0,"postConfigure mentions MySQL $end$ postConfigure says, a bit into the processing:
Running the MariaDB Columnstore MySQL setup scripts
This should probably say
Running the MariaDB Columnstore setup scripts
 $acceptance criteria:$",0,0,0,0,0,0,0,841.883,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
83,MCOL-1385,New Feature,MCOL,2018-05-04 08:55:19,,0,Merge MariaDB 10.3,MariaDB ColumnStore 1.2 needs to use MariaDB 10.3,,Merge MariaDB 10.3 $end$ MariaDB ColumnStore 1.2 needs to use MariaDB 10.3 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,16,,1,2,4,8,0,0,0,,0,850,2,0,0,2018-05-05 05:44:19,Merge MariaDB 10.3,MariaDB ColumnStore 1.2 needs to use MariaDB 10.3,,0,0,0,0,0.0,Merge MariaDB 10.3 $end$ MariaDB ColumnStore 1.2 needs to use MariaDB 10.3 $acceptance criteria:$,0,0,0,0,0,0,1,20.8167,52,2,0.0384615,2,0.0384615,1,0.0192308,1,0.0192308,1,0.0192308
84,MCOL-1392,New Feature,MCOL,2018-05-04 22:40:21,,0,Add time field support for PDI plugin,As we introduce a new time field in CS 1.2 it should be supported by the PDI plugin as well.,,Add time field support for PDI plugin $end$ As we introduce a new time field in CS 1.2 it should be supported by the PDI plugin as well. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Minor,27,,0,3,0,8,0,0,0,,0,850,2,0,0,2018-05-29 23:11:20,Add time field support for PDI plugin,As we introduce a new time field in CS 1.2 it should be supported by the PDI plugin as well.,,0,0,0,0,0.0,Add time field support for PDI plugin $end$ As we introduce a new time field in CS 1.2 it should be supported by the PDI plugin as well. $acceptance criteria:$,0,0,0,0,0,0,1,600.5,16,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
85,MCOL-140,Task,MCOL,2016-06-14 18:20:49,,0,Concurrent Insert threads generate errors,"Inserting data into a simple table using multiple concurrent threads cause errors:
Table schema:
CREATE TABLE `facts` (
  `invoice_id` int(11) NOT NULL,
  `order_value` int(11) NOT NULL,
  `customer_id` int(11) NOT NULL
) ENGINE=Columnstore
Multiple concurrent inserts, 2 or more, using array inserts or not, eventually seems to generate this error:
2016-06-14 20:24:11 MySQL Error: (1815)
Internal error: CAL0001: Insert Failed:   a BRM VB entry error.",,"Concurrent Insert threads generate errors $end$ Inserting data into a simple table using multiple concurrent threads cause errors:
Table schema:
CREATE TABLE `facts` (
  `invoice_id` int(11) NOT NULL,
  `order_value` int(11) NOT NULL,
  `customer_id` int(11) NOT NULL
) ENGINE=Columnstore
Multiple concurrent inserts, 2 or more, using array inserts or not, eventually seems to generate this error:
2016-06-14 20:24:11 MySQL Error: (1815)
Internal error: CAL0001: Insert Failed:   a BRM VB entry error. $acceptance criteria:$",,Anders Karlsson,Anders Karlsson,Critical,13,,0,7,0,1,0,0,0,,0,850,6,0,0,2016-07-05 20:22:46,Concurrent Insert threads generate errors,"Inserting data into a simple table using multiple concurrent threads cause errors:
Table schema:
CREATE TABLE `facts` (
  `invoice_id` int(11) NOT NULL,
  `order_value` int(11) NOT NULL,
  `customer_id` int(11) NOT NULL
) ENGINE=Columnstore
Multiple concurrent inserts, 2 or more, using array inserts or not, eventually seems to generate this error:
2016-06-14 20:24:11 MySQL Error: (1815)
Internal error: CAL0001: Insert Failed:   a BRM VB entry error.",,0,0,0,0,0.0,"Concurrent Insert threads generate errors $end$ Inserting data into a simple table using multiple concurrent threads cause errors:
Table schema:
CREATE TABLE `facts` (
  `invoice_id` int(11) NOT NULL,
  `order_value` int(11) NOT NULL,
  `customer_id` int(11) NOT NULL
) ENGINE=Columnstore
Multiple concurrent inserts, 2 or more, using array inserts or not, eventually seems to generate this error:
2016-06-14 20:24:11 MySQL Error: (1815)
Internal error: CAL0001: Insert Failed:   a BRM VB entry error. $acceptance criteria:$",0,0,0,0,0,0,0,506.017,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
86,MCOL-1401,New Feature,MCOL,2018-05-08 20:23:25,,0,PDI plugin - continuous integration tests,"Currently there are only manual tests for the PDI plugin.

CI tests either through docker or native file system should be added where applicable.",,"PDI plugin - continuous integration tests $end$ Currently there are only manual tests for the PDI plugin.

CI tests either through docker or native file system should be added where applicable. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,33,,0,10,0,7,0,0,0,,0,850,10,0,0,2018-05-08 20:26:15,PDI plugin - continuous integration tests,"Currently there are only manual tests for the PDI plugin.

CI tests either through docker or native file system should be added where applicable.",,0,0,0,0,0.0,"PDI plugin - continuous integration tests $end$ Currently there are only manual tests for the PDI plugin.

CI tests either through docker or native file system should be added where applicable. $acceptance criteria:$",0,0,0,0,0,0,1,0.0333333,17,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
87,MCOL-1412,New Feature,MCOL,2018-05-14 21:02:05,,0,Backport Ubuntu 18.04 support to 1.1,Backport MCOL-1376,,Backport Ubuntu 18.04 support to 1.1 $end$ Backport MCOL-1376 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-05-14 21:02:05,Backport Ubuntu 18.04 support to 1.1,Backport MCOL-1376,,0,0,0,0,0.0,Backport Ubuntu 18.04 support to 1.1 $end$ Backport MCOL-1376 $acceptance criteria:$,0,0,0,0,0,0,1,0.0,53,2,0.0377358,2,0.0377358,1,0.0188679,1,0.0188679,1,0.0188679
88,MCOL-1417,Sub-Task,MCOL,2018-05-15 20:39:23,,0,TIME: cpimort saturates reserved NULL and empty indicator values incorrectly,"Both NULL and empty indicator values are saturated to -838:59:59.  They should be saturated to 00:00:00, as LDI does since these values are invalid TIME values.",,"TIME: cpimort saturates reserved NULL and empty indicator values incorrectly $end$ Both NULL and empty indicator values are saturated to -838:59:59.  They should be saturated to 00:00:00, as LDI does since these values are invalid TIME values. $acceptance criteria:$",,Daniel Lee,Daniel Lee,Major,14,,1,7,1,11,0,0,0,,0,850,7,0,0,2018-05-15 20:39:23,TIME: cpimort saturates reserved NULL and empty indicator values incorrectly,"Both NULL and empty indicator values are saturated to -838:59:59.  They should be saturated to 00:00:00, as LDI does since these values are invalid TIME values.",,0,0,0,0,0.0,"TIME: cpimort saturates reserved NULL and empty indicator values incorrectly $end$ Both NULL and empty indicator values are saturated to -838:59:59.  They should be saturated to 00:00:00, as LDI does since these values are invalid TIME values. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
89,MCOL-1418,Sub-Task,MCOL,2018-05-15 20:41:23,,0,TIME: LDI saturates out-of-range values incorrectly,LDI Saturated to max supported values (839:59:59) for both positive and negative out of range values. The negative out-of-range value should be saturated to -839:59:59.,,TIME: LDI saturates out-of-range values incorrectly $end$ LDI Saturated to max supported values (839:59:59) for both positive and negative out of range values. The negative out-of-range value should be saturated to -839:59:59. $acceptance criteria:$,,Daniel Lee,Daniel Lee,Major,9,,0,4,0,11,0,0,0,,0,850,4,0,0,2018-05-15 20:41:23,TIME: LDI saturates out-of-range values incorrectly,LDI Saturated to max supported values (839:59:59) for both positive and negative out of range values. The negative out-of-range value should be saturated to -839:59:59.,,0,0,0,0,0.0,TIME: LDI saturates out-of-range values incorrectly $end$ LDI Saturated to max supported values (839:59:59) for both positive and negative out of range values. The negative out-of-range value should be saturated to -839:59:59. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
90,MCOL-1419,Sub-Task,MCOL,2018-05-15 20:43:10,,0,TIME: Update saturates out-of-range values incorrectly,"Update saturates out-of-range values to ""00:00:00"".  It should saturate to min or max supported value instead.",,"TIME: Update saturates out-of-range values incorrectly $end$ Update saturates out-of-range values to ""00:00:00"".  It should saturate to min or max supported value instead. $acceptance criteria:$",,Daniel Lee,Daniel Lee,Major,9,,0,3,0,11,0,0,0,,0,850,3,0,0,2018-05-15 20:43:10,TIME: Update saturates out-of-range values incorrectly,"Update saturates out-of-range values to ""00:00:00"".  It should saturate to min or max supported value instead.",,0,0,0,0,0.0,"TIME: Update saturates out-of-range values incorrectly $end$ Update saturates out-of-range values to ""00:00:00"".  It should saturate to min or max supported value instead. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
91,MCOL-1427,Sub-Task,MCOL,2018-05-18 14:03:33,,0,Microsecond values are stored left-justified with trailing 0 padding,"MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.000123', '08:55:12.000123');
Query OK, 1 row affected (0.15 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+

| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+
1 row in set (0.05 sec)

MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.123000', '08:55:12.123000');
Query OK, 1 row affected (0.07 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+
| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+

Microsecond for the first insert should be 000123.  All functions with microsecond support are affected.",,"Microsecond values are stored left-justified with trailing 0 padding $end$ MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.000123', '08:55:12.000123');
Query OK, 1 row affected (0.15 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+

| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+
1 row in set (0.05 sec)

MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.123000', '08:55:12.123000');
Query OK, 1 row affected (0.07 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+
| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+

Microsecond for the first insert should be 000123.  All functions with microsecond support are affected. $acceptance criteria:$",,Daniel Lee,Daniel Lee,Critical,9,,0,4,0,11,0,1,0,,0,850,4,0,0,2018-05-18 14:03:33,Microsecond values are stored left-justified with trailing 0 padding,"MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.000123', '08:55:12.000123');
Query OK, 1 row affected (0.15 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+
| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+
1 row in set (0.05 sec)

MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.123000', '08:55:12.123000');
Query OK, 1 row affected (0.07 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+
| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+

Microsecond for the first insert should be 000123.  All functions with microsecond support are affected.",,0,1,0,0,0.0,"Microsecond values are stored left-justified with trailing 0 padding $end$ MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.000123', '08:55:12.000123');
Query OK, 1 row affected (0.15 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+
| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+
1 row in set (0.05 sec)

MariaDB [mytest]> insert t2 values ('2018-05-18 08:55:12.123000', '08:55:12.123000');
Query OK, 1 row affected (0.07 sec)

MariaDB [mytest]> select * from t2;
+----------------------------+-----------------+
| c1                         | c2              |
+----------------------------+-----------------+
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
| 2018-05-18 08:55:12.123000 | 08:55:12.123000 |
+----------------------------+-----------------+

Microsecond for the first insert should be 000123.  All functions with microsecond support are affected. $acceptance criteria:$",1,0,0,0,0,0,1,0.0,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
92,MCOL-1428,Sub-Task,MCOL,2018-05-18 16:21:49,,0,SUBTIME() as a WHERE condition on TIME data type caused primProc to hang,"MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (2.52 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.36 sec)

MariaDB [mytest]> 
MariaDB [mytest]> 
MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.04 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
| 13:00:00 |
+----------+
2 rows in set (0.11 sec)

MariaDB [mytest]> drop table t1;
Query OK, 0 rows affected (0.18 sec)

MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (0.21 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.10 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
+----------+
1 row in set (0.03 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where ADDTIME(CTIME,'1 01:24:59') =0;
Empty set (0.06 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where SUBTIME(CTIME,'1 01:24:59') =0;

It hung here

The same query with ""<0"" or "">0"" also hung.",,"SUBTIME() as a WHERE condition on TIME data type caused primProc to hang $end$ MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (2.52 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.36 sec)

MariaDB [mytest]> 
MariaDB [mytest]> 
MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.04 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
| 13:00:00 |
+----------+
2 rows in set (0.11 sec)

MariaDB [mytest]> drop table t1;
Query OK, 0 rows affected (0.18 sec)

MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (0.21 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.10 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
+----------+
1 row in set (0.03 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where ADDTIME(CTIME,'1 01:24:59') =0;
Empty set (0.06 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where SUBTIME(CTIME,'1 01:24:59') =0;

It hung here

The same query with ""<0"" or "">0"" also hung. $acceptance criteria:$",,Daniel Lee,Daniel Lee,Critical,11,,0,4,0,11,0,1,0,,0,850,4,0,0,2018-05-18 16:21:49,SUBTIME() on TIME data type caused primProc to hang,"MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (2.52 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.36 sec)

MariaDB [mytest]> 
MariaDB [mytest]> 
MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.04 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
| 13:00:00 |
+----------+
2 rows in set (0.11 sec)

MariaDB [mytest]> drop table t1;
Query OK, 0 rows affected (0.18 sec)

MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (0.21 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.10 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
+----------+
1 row in set (0.03 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where ADDTIME(CTIME,'1 01:24:59') =0;
Empty set (0.06 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where SUBTIME(CTIME,'1 01:24:59') =0;

It hung here

The same query with ""<0"" or "">0"" also hung.",,1,0,0,4,0.0216216,"SUBTIME() on TIME data type caused primProc to hang $end$ MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (2.52 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.36 sec)

MariaDB [mytest]> 
MariaDB [mytest]> 
MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.04 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
| 13:00:00 |
+----------+
2 rows in set (0.11 sec)

MariaDB [mytest]> drop table t1;
Query OK, 0 rows affected (0.18 sec)

MariaDB [mytest]> create table t1 (c1 time) engine=columnstore;
Query OK, 0 rows affected (0.21 sec)

MariaDB [mytest]> insert into t1 values ('13:00:00');
Query OK, 1 row affected (0.10 sec)

MariaDB [mytest]> select * from t1;
+----------+
| c1       |
+----------+
| 13:00:00 |
+----------+
1 row in set (0.03 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where ADDTIME(CTIME,'1 01:24:59') =0;
Empty set (0.06 sec)

MariaDB [mytest]> select cidx, CTIME,SUBTIME(CTIME,'1 01:24:59') from datatypetestm where SUBTIME(CTIME,'1 01:24:59') =0;

It hung here

The same query with ""<0"" or "">0"" also hung. $acceptance criteria:$",1,1,0,0,0,0,1,0.0,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
93,MCOL-1429,Sub-Task,MCOL,2018-05-18 17:09:29,,0,DAYNAME() and MONTHNAME() on TIME data  type columns caused primProc to restart,"The following query causes a primproc restart

select cidx, CTIME, DAYNAME(CTIME) from datatypetestm order by cidx;
",,"DAYNAME() and MONTHNAME() on TIME data  type columns caused primProc to restart $end$ The following query causes a primproc restart

select cidx, CTIME, DAYNAME(CTIME) from datatypetestm order by cidx;
 $acceptance criteria:$",,Daniel Lee,Daniel Lee,Critical,10,,0,5,0,11,0,1,0,,0,850,5,0,0,2018-05-18 17:09:29,DAYNAME() on TIME data  type columns caused primProc to restart,"The following query causes a primproc restart

select cidx, CTIME, DAYNAME(CTIME) from datatypetestm order by cidx;
",,1,0,0,2,0.0689655,"DAYNAME() on TIME data  type columns caused primProc to restart $end$ The following query causes a primproc restart

select cidx, CTIME, DAYNAME(CTIME) from datatypetestm order by cidx;
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,7,1,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
94,MCOL-1433,Sub-Task,MCOL,2018-05-21 15:38:02,,0,Some functions return non-matching results after data type TIME was added to the functions test suite,"The following distributed functions in ColumnStore failed functions test after TIME data type was added to the test suite.  I have checked some of these functions and found that they are not handling the TIME data type correctly.

The functions test suite is part of the Autopilot/features test.  It compared results returned by MariaDB server and ColumnStore.

CAST
CONVERT
DATE
DATE_FORMAT
DATEDIFF
DAY
DAYOFMONTH
DAYOFWEEK
DAYOFYEAR
MONTH
QUARTER
TIMEDIFF
TO_DAYS
WEEK
WEEK
WEEKOFYEAR
YEAR
DEGREES
HEX
INET_NTOA
LAST_DAY
MAKEDATE
MONTHNAME
NULLIF
PERIOD_ADD
STR_TO_DATE
TIMESTAMPDIFF
WEEKDAY
YEARWEEK
YEARWEEK
",,"Some functions return non-matching results after data type TIME was added to the functions test suite $end$ The following distributed functions in ColumnStore failed functions test after TIME data type was added to the test suite.  I have checked some of these functions and found that they are not handling the TIME data type correctly.

The functions test suite is part of the Autopilot/features test.  It compared results returned by MariaDB server and ColumnStore.

CAST
CONVERT
DATE
DATE_FORMAT
DATEDIFF
DAY
DAYOFMONTH
DAYOFWEEK
DAYOFYEAR
MONTH
QUARTER
TIMEDIFF
TO_DAYS
WEEK
WEEK
WEEKOFYEAR
YEAR
DEGREES
HEX
INET_NTOA
LAST_DAY
MAKEDATE
MONTHNAME
NULLIF
PERIOD_ADD
STR_TO_DATE
TIMESTAMPDIFF
WEEKDAY
YEARWEEK
YEARWEEK
 $acceptance criteria:$",,Daniel Lee,Daniel Lee,Critical,22,,2,11,2,11,0,0,0,,0,850,11,0,0,2018-05-21 15:38:02,Some functions return non-matching results after data type TIME was added to the functions test suite,"The following distributed functions in ColumnStore failed functions test after TIME data type was added to the test suite.  I have checked some of these functions and found that they are not handling the TIME data type correctly.

The functions test suite is part of the Autopilot/features test.  It compared results returned by MariaDB server and ColumnStore.

CAST
CONVERT
DATE
DATE_FORMAT
DATEDIFF
DAY
DAYOFMONTH
DAYOFWEEK
DAYOFYEAR
MONTH
QUARTER
TIMEDIFF
TO_DAYS
WEEK
WEEK
WEEKOFYEAR
YEAR
DEGREES
HEX
INET_NTOA
LAST_DAY
MAKEDATE
MONTHNAME
NULLIF
PERIOD_ADD
STR_TO_DATE
TIMESTAMPDIFF
WEEKDAY
YEARWEEK
YEARWEEK
",,0,0,0,0,0.0,"Some functions return non-matching results after data type TIME was added to the functions test suite $end$ The following distributed functions in ColumnStore failed functions test after TIME data type was added to the test suite.  I have checked some of these functions and found that they are not handling the TIME data type correctly.

The functions test suite is part of the Autopilot/features test.  It compared results returned by MariaDB server and ColumnStore.

CAST
CONVERT
DATE
DATE_FORMAT
DATEDIFF
DAY
DAYOFMONTH
DAYOFWEEK
DAYOFYEAR
MONTH
QUARTER
TIMEDIFF
TO_DAYS
WEEK
WEEK
WEEKOFYEAR
YEAR
DEGREES
HEX
INET_NTOA
LAST_DAY
MAKEDATE
MONTHNAME
NULLIF
PERIOD_ADD
STR_TO_DATE
TIMESTAMPDIFF
WEEKDAY
YEARWEEK
YEARWEEK
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,8,2,0.25,0,0.0,0,0.0,0,0.0,0,0.0
95,MCOL-1434,Task,MCOL,2018-05-21 17:00:54,,0,Merge MariaDB 10.1.33 into 1.0,New release of server needs merging,,Merge MariaDB 10.1.33 into 1.0 $end$ New release of server needs merging $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-05-21 17:00:54,Merge MariaDB 10.1.33 into 1.0,New release of server needs merging,,0,0,0,0,0.0,Merge MariaDB 10.1.33 into 1.0 $end$ New release of server needs merging $acceptance criteria:$,0,0,0,0,0,0,0,0.0,54,2,0.037037,2,0.037037,1,0.0185185,1,0.0185185,1,0.0185185
96,MCOL-1435,Task,MCOL,2018-05-21 17:02:03,,0,Merge MariaDB 10.2.15 into 1.1,New release of MariaDB needs merging,,Merge MariaDB 10.2.15 into 1.1 $end$ New release of MariaDB needs merging $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-05-21 17:02:03,Merge MariaDB 10.2.15 into 1.1,New release of MariaDB needs merging,,0,0,0,0,0.0,Merge MariaDB 10.2.15 into 1.1 $end$ New release of MariaDB needs merging $acceptance criteria:$,0,0,0,0,0,0,1,0.0,55,2,0.0363636,2,0.0363636,1,0.0181818,1,0.0181818,1,0.0181818
97,MCOL-1439,Task,MCOL,2018-05-22 16:28:47,,0,Documentation for Python API missing,"There seems that  documentation for the Python Data Ingestion API is largely missing. The C++ API is well documented, but Python and Java seems missing.",,"Documentation for Python API missing $end$ There seems that  documentation for the Python Data Ingestion API is largely missing. The C++ API is well documented, but Python and Java seems missing. $acceptance criteria:$",,Anders Karlsson,Anders Karlsson,Critical,8,,0,2,0,1,0,0,0,,0,850,1,0,0,2018-07-11 20:29:37,Documentation for Python API missing,"There seems that  documentation for the Python Data Ingestion API is largely missing. The C++ API is well documented, but Python and Java seems missing.",,0,0,0,0,0.0,"Documentation for Python API missing $end$ There seems that  documentation for the Python Data Ingestion API is largely missing. The C++ API is well documented, but Python and Java seems missing. $acceptance criteria:$",0,0,0,0,0,0,0,1204.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
98,MCOL-144,Task,MCOL,2016-06-15 20:24:53,,0,Function Test analysis from System Test,Analyze the function tests from Auto pilot for every failed fuctions,,Function Test analysis from System Test $end$ Analyze the function tests from Auto pilot for every failed fuctions $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-06-15 20:53:55,Function Test analysis from System Test,Analyze the function tests from Auto pilot for every failed fuctions,,0,0,0,0,0.0,Function Test analysis from System Test $end$ Analyze the function tests from Auto pilot for every failed fuctions $acceptance criteria:$,0,0,0,0,0,0,0,0.483333,24,6,0.25,1,0.0416667,1,0.0416667,1,0.0416667,1,0.0416667
99,MCOL-1446,New Feature,MCOL,2018-05-30 07:33:31,MCOL-1049,0,Change default internal sorting direction in CS,"Internal CS sorting code outputs its result in opposite direction contrasting the server.

{noformat}
MariaDB [tpch1c]> use cs;
Database changed
MariaDB [cs]> use test
Database changed
MariaDB [test]> create table cs1(i int) engine=columnstore;
insQuery OK, 0 rows affected (2.71 sec)

create table in1(i int) engine=innodb;

MariaDB [test]> insert into cs1 values (4),(15);
Query OK, 2 rows affected (0.59 sec)
Records: 2  Duplicates: 0  Warnings: 0

insert into in1 values (4),(15);

MariaDB [test]> select * from cs1 order by i desc; -- This one uses server's filesort
+------+
| i    |
+------+
|   15 |
|    4 |
+------+
2 rows in set (0.13 sec)

MariaDB [test]> select * from (select * from cs1 order by i desc) t1; -- This one uses internal CS sorting
+------+
| i    |
+------+
|    4 |
|   15 |
+------+
2 rows in set (0.02 sec)
{noformat}

Both outputs must be identical.",,"Change default internal sorting direction in CS $end$ Internal CS sorting code outputs its result in opposite direction contrasting the server.

{noformat}
MariaDB [tpch1c]> use cs;
Database changed
MariaDB [cs]> use test
Database changed
MariaDB [test]> create table cs1(i int) engine=columnstore;
insQuery OK, 0 rows affected (2.71 sec)

create table in1(i int) engine=innodb;

MariaDB [test]> insert into cs1 values (4),(15);
Query OK, 2 rows affected (0.59 sec)
Records: 2  Duplicates: 0  Warnings: 0

insert into in1 values (4),(15);

MariaDB [test]> select * from cs1 order by i desc; -- This one uses server's filesort
+------+
| i    |
+------+
|   15 |
|    4 |
+------+
2 rows in set (0.13 sec)

MariaDB [test]> select * from (select * from cs1 order by i desc) t1; -- This one uses internal CS sorting
+------+
| i    |
+------+
|    4 |
|   15 |
+------+
2 rows in set (0.02 sec)
{noformat}

Both outputs must be identical. $acceptance criteria:$",,Roman,Roman,Minor,20,,0,1,0,13,0,1,0,,0,850,1,0,0,2018-05-30 07:36:50,Change default internal sorting direction in CS,"Internal CS sorting code outputs its result in opposite direction contrasting the server.

MariaDB [tpch1c]> use cs;
Database changed
MariaDB [cs]> use test
Database changed
MariaDB [test]> create table cs1(i int) engine=columnstore;
insQuery OK, 0 rows affected (2.71 sec)

MariaDB [test]> insert into cs1 values (4),(15);
Query OK, 2 rows affected (0.59 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> select * from cs1 order by i desc;
+------+
| i    |
+------+
|   15 |
|    4 |
+------+
2 rows in set (0.13 sec)

MariaDB [test]> select * from (select * from cs1 order by i desc) t1;
+------+
| i    |
+------+
|    4 |
|   15 |
+------+
2 rows in set (0.02 sec)
",,0,1,0,30,0.232558,"Change default internal sorting direction in CS $end$ Internal CS sorting code outputs its result in opposite direction contrasting the server.

MariaDB [tpch1c]> use cs;
Database changed
MariaDB [cs]> use test
Database changed
MariaDB [test]> create table cs1(i int) engine=columnstore;
insQuery OK, 0 rows affected (2.71 sec)

MariaDB [test]> insert into cs1 values (4),(15);
Query OK, 2 rows affected (0.59 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> select * from cs1 order by i desc;
+------+
| i    |
+------+
|   15 |
|    4 |
+------+
2 rows in set (0.13 sec)

MariaDB [test]> select * from (select * from cs1 order by i desc) t1;
+------+
| i    |
+------+
|    4 |
|   15 |
+------+
2 rows in set (0.02 sec)
 $acceptance criteria:$",1,1,1,1,1,1,1,0.05,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
100,MCOL-145,Task,MCOL,2016-06-15 20:41:00,,0,Update Install process that uses new build process,,,Update Install process that uses new build process $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,8,,0,4,2,2,0,0,2,,0,850,4,0,0,2016-06-15 20:41:00,Update Install process that uses new build process,,,0,0,0,0,0.0,Update Install process that uses new build process $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,25,6,0.24,1,0.04,1,0.04,1,0.04,1,0.04
101,MCOL-146,Sub-Task,MCOL,2016-06-15 20:44:37,,0,build enterprise rpms from new build processes,,,build enterprise rpms from new build processes $end$ $acceptance criteria:$,,David Hill,David Hill,Minor,4,,0,3,0,2,0,1,0,,0,850,3,0,0,2016-06-15 20:44:37,build enterprise rpms from community build processes,,,1,0,0,2,0.1,build enterprise rpms from community build processes $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
102,MCOL-147,Task,MCOL,2016-06-15 20:51:31,,0,Validate  Schema Sync between InfiniDB and ColumnStore,,,Validate  Schema Sync between InfiniDB and ColumnStore $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-06-15 20:51:31,Validate  Schema Sync between InfiniDB and ColumnStore,,,0,0,0,0,0.0,Validate  Schema Sync between InfiniDB and ColumnStore $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,26,6,0.230769,1,0.0384615,1,0.0384615,1,0.0384615,1,0.0384615
103,MCOL-1475,New Feature,MCOL,2018-06-14 16:02:36,,0,Improve cross engine error handling,"Cross Engine Join handler has very rudimentary error handling. We need more details about what goes wrong in the MariaDB client/server, including proper error codes and messages where possible.",,"Improve cross engine error handling $end$ Cross Engine Join handler has very rudimentary error handling. We need more details about what goes wrong in the MariaDB client/server, including proper error codes and messages where possible. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,0,2,0,0,0,,0,850,2,0,0,2018-06-14 16:02:36,Improve cross engine error handling,"Cross Engine Join handler has very rudimentary error handling. We need more details about what goes wrong in the MariaDB client/server, including proper error codes and messages where possible.",,0,0,0,0,0.0,"Improve cross engine error handling $end$ Cross Engine Join handler has very rudimentary error handling. We need more details about what goes wrong in the MariaDB client/server, including proper error codes and messages where possible. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,56,2,0.0357143,2,0.0357143,1,0.0178571,1,0.0178571,1,0.0178571
104,MCOL-1482,New Feature,MCOL,2018-06-18 15:53:05,MCOL-1049,0,cross-engine update... join... not working when updating innodb tables,"It seems that an update statement cannot update innodb tables with columnstore content
(but an update statement can update columnstore tables with innodb content)

here is an example:

{code:sql}
-- create a columnstore and an innodb table
drop table if exists mcs; drop table if exists idb; 
create table mcs(a int, b int) engine=columnstore;
create table idb(a int, b int) engine=innodb;

-- insert some rows
insert into mcs(a,b) values(1,2),(2,3),(4,5);
insert into idb(a,b) values(1,2),(2,3),(4,5);

-- columnstore can be update with innodb
update mcs dest join idb src on dest.a=src.a set dest.b=src.b;

--- ERROR: innodb cannot be update with columnstore
update idb dest join mcs src on dest.a=src.a set dest.b=src.b;

=> Error Code: 1815. Internal error: IDB-2006: 'my_db.idb' does not exist in Columnstore.

{code}


a workaround is to create a temp innodb table, insert from columnstore into this table with an INSERT... SELECT..., then update from innodb to innodb, then drop the temp innodb table. But that's a bit complicated and lengthy
",,"cross-engine update... join... not working when updating innodb tables $end$ It seems that an update statement cannot update innodb tables with columnstore content
(but an update statement can update columnstore tables with innodb content)

here is an example:

{code:sql}
-- create a columnstore and an innodb table
drop table if exists mcs; drop table if exists idb; 
create table mcs(a int, b int) engine=columnstore;
create table idb(a int, b int) engine=innodb;

-- insert some rows
insert into mcs(a,b) values(1,2),(2,3),(4,5);
insert into idb(a,b) values(1,2),(2,3),(4,5);

-- columnstore can be update with innodb
update mcs dest join idb src on dest.a=src.a set dest.b=src.b;

--- ERROR: innodb cannot be update with columnstore
update idb dest join mcs src on dest.a=src.a set dest.b=src.b;

=> Error Code: 1815. Internal error: IDB-2006: 'my_db.idb' does not exist in Columnstore.

{code}


a workaround is to create a temp innodb table, insert from columnstore into this table with an INSERT... SELECT..., then update from innodb to innodb, then drop the temp innodb table. But that's a bit complicated and lengthy
 $acceptance criteria:$",,antoine,antoine,Blocker,47,,0,3,1,1,0,0,0,,0,850,1,0,0,2021-06-16 19:17:00,cross-engine update... join... not working when updating innodb tables,"It seems that an update statement cannot update innodb tables with columnstore content
(but an update statement can update columnstore tables with innodb content)

here is an example:

{code:sql}
-- create a columnstore and an innodb table
drop table if exists mcs; drop table if exists idb; 
create table mcs(a int, b int) engine=columnstore;
create table idb(a int, b int) engine=innodb;

-- insert some rows
insert into mcs(a,b) values(1,2),(2,3),(4,5);
insert into idb(a,b) values(1,2),(2,3),(4,5);

-- columnstore can be update with innodb
update mcs dest join idb src on dest.a=src.a set dest.b=src.b;

--- ERROR: innodb cannot be update with columnstore
update idb dest join mcs src on dest.a=src.a set dest.b=src.b;

=> Error Code: 1815. Internal error: IDB-2006: 'my_db.idb' does not exist in Columnstore.

{code}


a workaround is to create a temp innodb table, insert from columnstore into this table with an INSERT... SELECT..., then update from innodb to innodb, then drop the temp innodb table. But that's a bit complicated and lengthy
",,0,0,0,0,0.0,"cross-engine update... join... not working when updating innodb tables $end$ It seems that an update statement cannot update innodb tables with columnstore content
(but an update statement can update columnstore tables with innodb content)

here is an example:

{code:sql}
-- create a columnstore and an innodb table
drop table if exists mcs; drop table if exists idb; 
create table mcs(a int, b int) engine=columnstore;
create table idb(a int, b int) engine=innodb;

-- insert some rows
insert into mcs(a,b) values(1,2),(2,3),(4,5);
insert into idb(a,b) values(1,2),(2,3),(4,5);

-- columnstore can be update with innodb
update mcs dest join idb src on dest.a=src.a set dest.b=src.b;

--- ERROR: innodb cannot be update with columnstore
update idb dest join mcs src on dest.a=src.a set dest.b=src.b;

=> Error Code: 1815. Internal error: IDB-2006: 'my_db.idb' does not exist in Columnstore.

{code}


a workaround is to create a temp innodb table, insert from columnstore into this table with an INSERT... SELECT..., then update from innodb to innodb, then drop the temp innodb table. But that's a bit complicated and lengthy
 $acceptance criteria:$",0,0,0,0,0,0,0,26259.4,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
105,MCOL-1484,New Feature,MCOL,2018-06-19 14:08:39,,0,Use condition pushdown in I_S tables,"A sub-task for MCOL-1081 to be implemented in 1.1. Condition pushdowns in the I_S tables to accelerate the performance of the tables, especially when only a subset if information is needed.",,"Use condition pushdown in I_S tables $end$ A sub-task for MCOL-1081 to be implemented in 1.1. Condition pushdowns in the I_S tables to accelerate the performance of the tables, especially when only a subset if information is needed. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,1,1,0,0,0,,0,850,2,0,0,2018-06-19 14:08:39,Use condition pushdown in I_S tables,"A sub-task for MCOL-1081 to be implemented in 1.1. Condition pushdowns in the I_S tables to accelerate the performance of the tables, especially when only a subset if information is needed.",,0,0,0,0,0.0,"Use condition pushdown in I_S tables $end$ A sub-task for MCOL-1081 to be implemented in 1.1. Condition pushdowns in the I_S tables to accelerate the performance of the tables, especially when only a subset if information is needed. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,57,2,0.0350877,2,0.0350877,1,0.0175439,1,0.0175439,1,0.0175439
106,MCOL-1496,Sub-Task,MCOL,2018-06-23 10:14:08,,0,Joiner array boundary bug,New boundary check found an out of bounds bug in joiner code.,,Joiner array boundary bug $end$ New boundary check found an out of bounds bug in joiner code. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,1,0,5,0,0,0,,0,850,1,0,0,2018-06-23 10:14:08,Joiner array boundary bug,New boundary check found an out of bounds bug in joiner code.,,0,0,0,0,0.0,Joiner array boundary bug $end$ New boundary check found an out of bounds bug in joiner code. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,58,2,0.0344828,2,0.0344828,1,0.0172414,1,0.0172414,1,0.0172414
107,MCOL-1521,New Feature,MCOL,2018-06-29 23:23:24,,0,Java - mcsapi - introduce new function getJavaMcsapiVersion() ,"Add a function ""ColumnStoreDriver.getJavaMcsapiVersion()"" to return the javamcsapi version which not necessarily needs to be the same than the one of mcsapi.

This can help debugging if native mcsapi libraries and javamcsapi don't match.",,"Java - mcsapi - introduce new function getJavaMcsapiVersion()  $end$ Add a function ""ColumnStoreDriver.getJavaMcsapiVersion()"" to return the javamcsapi version which not necessarily needs to be the same than the one of mcsapi.

This can help debugging if native mcsapi libraries and javamcsapi don't match. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,10,,0,2,0,1,0,0,0,,0,850,1,0,0,2018-07-03 23:17:32,Java - mcsapi - introduce new function getJavaMcsapiVersion() ,"Add a function ""ColumnStoreDriver.getJavaMcsapiVersion()"" to return the javamcsapi version which not necessarily needs to be the same than the one of mcsapi.

This can help debugging if native mcsapi libraries and javamcsapi don't match.",,0,0,0,0,0.0,"Java - mcsapi - introduce new function getJavaMcsapiVersion()  $end$ Add a function ""ColumnStoreDriver.getJavaMcsapiVersion()"" to return the javamcsapi version which not necessarily needs to be the same than the one of mcsapi.

This can help debugging if native mcsapi libraries and javamcsapi don't match. $acceptance criteria:$",0,0,0,0,0,0,0,95.9,18,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
108,MCOL-1524,New Feature,MCOL,2018-07-03 17:40:07,,0,Backward / Forward compatibility test for javamcsapi and mcsapi,"With the decision to separate javamcsapi and mcsapi in the Informatica connector, scenarios are possible where the Informatica connector uses a newer version of the Java wrapper library javamcsapi that calls the function of an older version of mcsapi manually installed on the system.

It is also possible that a newer version of mcsapi installed on the system is called by an older version of javamcsapi.

As a result we need to ensure that the call of the essential functions of javamcsapi to mcsapi are backward and forward compatible within the minor version level. (e.g. 1.1.x)

This tests is shall be build to verify that.",,"Backward / Forward compatibility test for javamcsapi and mcsapi $end$ With the decision to separate javamcsapi and mcsapi in the Informatica connector, scenarios are possible where the Informatica connector uses a newer version of the Java wrapper library javamcsapi that calls the function of an older version of mcsapi manually installed on the system.

It is also possible that a newer version of mcsapi installed on the system is called by an older version of javamcsapi.

As a result we need to ensure that the call of the essential functions of javamcsapi to mcsapi are backward and forward compatible within the minor version level. (e.g. 1.1.x)

This tests is shall be build to verify that. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,15,,0,4,0,2,0,0,0,,0,850,4,0,0,2018-07-03 23:17:49,Backward / Forward compatibility test for javamcsapi and mcsapi,"With the decision to separate javamcsapi and mcsapi in the Informatica connector, scenarios are possible where the Informatica connector uses a newer version of the Java wrapper library javamcsapi that calls the function of an older version of mcsapi manually installed on the system.

It is also possible that a newer version of mcsapi installed on the system is called by an older version of javamcsapi.

As a result we need to ensure that the call of the essential functions of javamcsapi to mcsapi are backward and forward compatible within the minor version level. (e.g. 1.1.x)

This tests is shall be build to verify that.",,0,0,0,0,0.0,"Backward / Forward compatibility test for javamcsapi and mcsapi $end$ With the decision to separate javamcsapi and mcsapi in the Informatica connector, scenarios are possible where the Informatica connector uses a newer version of the Java wrapper library javamcsapi that calls the function of an older version of mcsapi manually installed on the system.

It is also possible that a newer version of mcsapi installed on the system is called by an older version of javamcsapi.

As a result we need to ensure that the call of the essential functions of javamcsapi to mcsapi are backward and forward compatible within the minor version level. (e.g. 1.1.x)

This tests is shall be build to verify that. $acceptance criteria:$",0,0,0,0,0,0,1,5.61667,19,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
109,MCOL-1532,New Feature,MCOL,2018-07-06 16:59:47,MCOL-1097,0,Run ColumnStore with vanilla MariaDB server,Run ColumnStore that at least supports statements with derived table using vanilla MariaDB server code. ,,Run ColumnStore with vanilla MariaDB server $end$ Run ColumnStore that at least supports statements with derived table using vanilla MariaDB server code.  $acceptance criteria:$,,Roman,Roman,Minor,9,,0,1,6,7,0,0,0,,0,850,1,0,0,2018-07-09 07:49:28,Run ColumnStore with vanilla MariaDB server,Run ColumnStore that at least supports statements with derived table using vanilla MariaDB server code. ,,0,0,0,0,0.0,Run ColumnStore with vanilla MariaDB server $end$ Run ColumnStore that at least supports statements with derived table using vanilla MariaDB server code.  $acceptance criteria:$,0,0,0,0,0,0,1,62.8167,1,1,1.0,1,1.0,1,1.0,1,1.0,1,1.0
110,MCOL-1547,Task,MCOL,2018-07-10 07:22:35,,0,Investigate renewed in 10.3  CASE implementation.,"There is an accessor sql/item_cmpfunc.h:Item_func_case::get_first_expr_num() in forked server code that is used to determine whether the CASE is simple or searched. This accessor should be removed if it is possible.
{code:cpp}
class Item_func_case :public Item_func_hybrid_field_type
...
get_first_expr_num() { return first_expr_num; }
{code}",,"Investigate renewed in 10.3  CASE implementation. $end$ There is an accessor sql/item_cmpfunc.h:Item_func_case::get_first_expr_num() in forked server code that is used to determine whether the CASE is simple or searched. This accessor should be removed if it is possible.
{code:cpp}
class Item_func_case :public Item_func_hybrid_field_type
...
get_first_expr_num() { return first_expr_num; }
{code} $acceptance criteria:$",,Roman,Roman,Minor,3,,0,0,1,1,0,0,0,,0,850,0,0,0,2018-07-10 07:32:53,Investigate renewed in 10.3  CASE implementation.,"There is an accessor sql/item_cmpfunc.h:Item_func_case::get_first_expr_num() in forked server code that is used to determine whether the CASE is simple or searched. This accessor should be removed if it is possible.
{code:cpp}
class Item_func_case :public Item_func_hybrid_field_type
...
get_first_expr_num() { return first_expr_num; }
{code}",,0,0,0,0,0.0,"Investigate renewed in 10.3  CASE implementation. $end$ There is an accessor sql/item_cmpfunc.h:Item_func_case::get_first_expr_num() in forked server code that is used to determine whether the CASE is simple or searched. This accessor should be removed if it is possible.
{code:cpp}
class Item_func_case :public Item_func_hybrid_field_type
...
get_first_expr_num() { return first_expr_num; }
{code} $acceptance criteria:$",0,0,0,0,0,0,0,0.166667,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
111,MCOL-1574,Task,MCOL,2018-07-20 13:18:19,,0,test oracle compatiblity in columnstore,"Once 10.3 is merged, test the major oracle compatibility features and validate what works / does not work and file tickets for what does not work - we will prioritze and fix low hanging fruit. Most stored procedure logic is likely problematic.

A good starting point for changes is this article:
https://mariadb.com/kb/en/library/sql_modeoracle-from-mariadb-103/",,"test oracle compatiblity in columnstore $end$ Once 10.3 is merged, test the major oracle compatibility features and validate what works / does not work and file tickets for what does not work - we will prioritze and fix low hanging fruit. Most stored procedure logic is likely problematic.

A good starting point for changes is this article:
https://mariadb.com/kb/en/library/sql_modeoracle-from-mariadb-103/ $acceptance criteria:$",,David Thompson,David Thompson,Major,8,,0,2,1,2,0,0,0,,0,850,2,0,0,2018-09-12 20:26:44,test oracle compatiblity in columnstore,"Once 10.3 is merged, test the major oracle compatibility features and validate what works / does not work and file tickets for what does not work - we will prioritze and fix low hanging fruit. Most stored procedure logic is likely problematic.

A good starting point for changes is this article:
https://mariadb.com/kb/en/library/sql_modeoracle-from-mariadb-103/",,0,0,0,0,0.0,"test oracle compatiblity in columnstore $end$ Once 10.3 is merged, test the major oracle compatibility features and validate what works / does not work and file tickets for what does not work - we will prioritze and fix low hanging fruit. Most stored procedure logic is likely problematic.

A good starting point for changes is this article:
https://mariadb.com/kb/en/library/sql_modeoracle-from-mariadb-103/ $acceptance criteria:$",0,0,0,0,0,0,1,1303.13,52,5,0.0961538,5,0.0961538,3,0.0576923,2,0.0384615,2,0.0384615
112,MCOL-1577,New Feature,MCOL,2018-07-20 22:23:22,,0,ColumnStore to allow CREATE TABLE table_name LIKE ... Syntax,"It would be nice if ColumnStore supported the CREATE TABLE table_name LIKE ... syntax.

For instance, if you try to execute this CREATE TABLE command:

{code}
CREATE TABLE table2 LIKE table1;
{code}

It throws the following error:

{code}
The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
{code}

Here is the CREATE TABLE for the existing table, table1:

{code}
CREATE TABLE `table1` (
`col1` int(10) NOT NULL,
`col2` varchar(255) DEFAULT NULL,
`col3` tinyint(127) DEFAULT NULL,
`col4` tinyint(1) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=utf8;
{code}",,"ColumnStore to allow CREATE TABLE table_name LIKE ... Syntax $end$ It would be nice if ColumnStore supported the CREATE TABLE table_name LIKE ... syntax.

For instance, if you try to execute this CREATE TABLE command:

{code}
CREATE TABLE table2 LIKE table1;
{code}

It throws the following error:

{code}
The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
{code}

Here is the CREATE TABLE for the existing table, table1:

{code}
CREATE TABLE `table1` (
`col1` int(10) NOT NULL,
`col2` varchar(255) DEFAULT NULL,
`col3` tinyint(127) DEFAULT NULL,
`col4` tinyint(1) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=utf8;
{code} $acceptance criteria:$",,Chris Calender,Chris Calender,Major,11,,0,2,0,3,0,0,0,,0,850,2,0,0,2018-07-25 18:33:32,ColumnStore to allow CREATE TABLE table_name LIKE ... Syntax,"It would be nice if ColumnStore supported the CREATE TABLE table_name LIKE ... syntax.

For instance, if you try to execute this CREATE TABLE command:

{code}
CREATE TABLE table2 LIKE table1;
{code}

It throws the following error:

{code}
The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
{code}

Here is the CREATE TABLE for the existing table, table1:

{code}
CREATE TABLE `table1` (
`col1` int(10) NOT NULL,
`col2` varchar(255) DEFAULT NULL,
`col3` tinyint(127) DEFAULT NULL,
`col4` tinyint(1) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=utf8;
{code}",,0,0,0,0,0.0,"ColumnStore to allow CREATE TABLE table_name LIKE ... Syntax $end$ It would be nice if ColumnStore supported the CREATE TABLE table_name LIKE ... syntax.

For instance, if you try to execute this CREATE TABLE command:

{code}
CREATE TABLE table2 LIKE table1;
{code}

It throws the following error:

{code}
The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
{code}

Here is the CREATE TABLE for the existing table, table1:

{code}
CREATE TABLE `table1` (
`col1` int(10) NOT NULL,
`col2` varchar(255) DEFAULT NULL,
`col3` tinyint(127) DEFAULT NULL,
`col4` tinyint(1) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=utf8;
{code} $acceptance criteria:$",0,0,0,0,0,0,1,116.167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
113,MCOL-1578,New Feature,MCOL,2018-07-23 04:57:52,,0,PDI Windows support,,,PDI Windows support $end$ $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,8,,0,2,2,4,0,0,0,,0,850,1,0,0,2018-07-28 00:51:35,PDI Windows support,,,0,0,0,0,0.0,PDI Windows support $end$ $acceptance criteria:$,0,0,0,0,0,0,1,115.883,20,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
114,MCOL-159,Sub-Task,MCOL,2016-06-21 19:34:32,,0,use cmake for columnstore-engine build,"current process is to use .configure, should be change to use .cmake like the server does",,"use cmake for columnstore-engine build $end$ current process is to use .configure, should be change to use .cmake like the server does $acceptance criteria:$",,David Hill,David Hill,Minor,7,,0,2,0,2,0,0,0,,0,850,2,0,0,2016-06-21 19:34:32,use cmake for columnstore-engine build,"current process is to use .configure, should be change to use .cmake like the server does",,0,0,0,0,0.0,"use cmake for columnstore-engine build $end$ current process is to use .configure, should be change to use .cmake like the server does $acceptance criteria:$",0,0,0,0,0,0,1,0.0,4,1,0.25,0,0.0,0,0.0,0,0.0,0,0.0
115,MCOL-1591,New Feature,MCOL,2018-07-25 01:27:55,,0,add UMASK check to ColumnStore Cluster Tester script,"Customer system failed to created table
from Master UM, module file permissions 
where incorrect because UMASK setting wasnt correct. was 027, should have been 0022.

this check needs to be added to the ColumnStore
Cluster Test script.
",,"add UMASK check to ColumnStore Cluster Tester script $end$ Customer system failed to created table
from Master UM, module file permissions 
where incorrect because UMASK setting wasnt correct. was 027, should have been 0022.

this check needs to be added to the ColumnStore
Cluster Test script.
 $acceptance criteria:$",,David Hill,David Hill,Minor,7,,0,3,0,2,0,0,0,,0,850,3,0,0,2018-08-01 03:44:43,add UMASK check to ColumnStore Cluster Tester script,"Customer system failed to created table
from Master UM, module file permissions 
where incorrect because UMASK setting wasnt correct. was 027, should have been 0022.

this check needs to be added to the ColumnStore
Cluster Test script.
",,0,0,0,0,0.0,"add UMASK check to ColumnStore Cluster Tester script $end$ Customer system failed to created table
from Master UM, module file permissions 
where incorrect because UMASK setting wasnt correct. was 027, should have been 0022.

this check needs to be added to the ColumnStore
Cluster Test script.
 $acceptance criteria:$",0,0,0,0,0,0,1,170.267,29,4,0.137931,0,0.0,0,0.0,0,0.0,0,0.0
116,MCOL-1596,New Feature,MCOL,2018-07-25 14:54:25,,0,mxs_adapter multi-table support,Running the adapter should be possible with a list of table names.,,mxs_adapter multi-table support $end$ Running the adapter should be possible with a list of table names. $acceptance criteria:$,,markus makela,markus makela,Major,8,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-08-03 12:46:07,mxs_adapter multi-table support,Running the adapter should be possible with a list of table names.,,0,0,0,0,0.0,mxs_adapter multi-table support $end$ Running the adapter should be possible with a list of table names. $acceptance criteria:$,0,0,0,0,0,0,1,213.85,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
117,MCOL-160,Task,MCOL,2016-06-21 19:35:28,,0,out-of-tree builds,"current build process doesn't support out-of-tree builds, needs to.",,"out-of-tree builds $end$ current build process doesn't support out-of-tree builds, needs to. $acceptance criteria:$",,David Hill,David Hill,Minor,24,,0,6,2,1,0,0,0,,0,850,5,0,0,2016-08-25 11:15:08,out-of-tree builds,"current build process doesn't support out-of-tree builds, needs to.",,0,0,0,0,0.0,"out-of-tree builds $end$ current build process doesn't support out-of-tree builds, needs to. $acceptance criteria:$",0,0,0,0,0,0,0,1551.65,5,1,0.2,0,0.0,0,0.0,0,0.0,0,0.0
118,MCOL-1615,Task,MCOL,2018-07-31 17:35:27,,0,Merge MariaDB 10.2.17,Need to merge the latest MariaDB,,Merge MariaDB 10.2.17 $end$ Need to merge the latest MariaDB $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,9,,0,1,0,2,0,1,0,,0,850,1,0,0,2018-07-31 17:35:27,Merge MariaDB 10.2.16,Need to merge the latest MariaDB,,1,0,0,2,0.0833333,Merge MariaDB 10.2.16 $end$ Need to merge the latest MariaDB $acceptance criteria:$,1,1,0,0,0,0,1,0.0,59,2,0.0338983,2,0.0338983,1,0.0169492,1,0.0169492,1,0.0169492
119,MCOL-1617,Task,MCOL,2018-08-01 18:19:25,,0,remove generated build doc from mcsapi in favor of its Readme.md in Github,"Currently we have two documents that document the build process of mcsapi. The build instructions in Github's Readme.md which is updated frequently and the build instructions in /docs that haven't been updated in 9 month.

It was decided that we remove the build instructions in /docs in favor of Readme.md to avoid confusion and double effort in maintaining both.

The usage_guides in the /docs folder are not affected by this MCOL and shall remain.",,"remove generated build doc from mcsapi in favor of its Readme.md in Github $end$ Currently we have two documents that document the build process of mcsapi. The build instructions in Github's Readme.md which is updated frequently and the build instructions in /docs that haven't been updated in 9 month.

It was decided that we remove the build instructions in /docs in favor of Readme.md to avoid confusion and double effort in maintaining both.

The usage_guides in the /docs folder are not affected by this MCOL and shall remain. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-08-01 18:19:57,remove generated build doc from mcsapi in favor of its Readme.md in Github,"Currently we have two documents that document the build process of mcsapi. The build instructions in Github's Readme.md which is updated frequently and the build instructions in /docs that haven't been updated in 9 month.

It was decided that we remove the build instructions in /docs in favor of Readme.md to avoid confusion and double effort in maintaining both.

The usage_guides in the /docs folder are not affected by this MCOL and shall remain.",,0,0,0,0,0.0,"remove generated build doc from mcsapi in favor of its Readme.md in Github $end$ Currently we have two documents that document the build process of mcsapi. The build instructions in Github's Readme.md which is updated frequently and the build instructions in /docs that haven't been updated in 9 month.

It was decided that we remove the build instructions in /docs in favor of Readme.md to avoid confusion and double effort in maintaining both.

The usage_guides in the /docs folder are not affected by this MCOL and shall remain. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,21,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
120,MCOL-1631,Task,MCOL,2018-08-07 17:39:15,,0,Merge MariaDB 10.1.35,MariaDB 10.1.35 has been release so it needs merging in,,Merge MariaDB 10.1.35 $end$ MariaDB 10.1.35 has been release so it needs merging in $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-08-07 17:39:15,Merge MariaDB 10.1.35,MariaDB 10.1.35 has been release so it needs merging in,,0,0,0,0,0.0,Merge MariaDB 10.1.35 $end$ MariaDB 10.1.35 has been release so it needs merging in $acceptance criteria:$,0,0,0,0,0,0,0,0.0,60,3,0.05,2,0.0333333,1,0.0166667,1,0.0166667,1,0.0166667
121,MCOL-1633,New Feature,MCOL,2018-08-08 21:23:38,,0,mcsapi Windows - add needed Windows Redistributables to installer,"To operate mcsapi on Windows 10, Microsoft Visual C++ Redistributables are needed.

These shall be added to the installer so that a customer doesn't need to obtain them in an additional step.

Needed Redistributables:
- Microsoft Visual C++ 2015 Redistributable (x64)
- -Microsoft Visual C++ 2012 Redistributable (x64)- were able to get rid of it in MCOL-1634 by manually compiling libiconv",,"mcsapi Windows - add needed Windows Redistributables to installer $end$ To operate mcsapi on Windows 10, Microsoft Visual C++ Redistributables are needed.

These shall be added to the installer so that a customer doesn't need to obtain them in an additional step.

Needed Redistributables:
- Microsoft Visual C++ 2015 Redistributable (x64)
- -Microsoft Visual C++ 2012 Redistributable (x64)- were able to get rid of it in MCOL-1634 by manually compiling libiconv $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,14,,0,4,0,3,0,4,0,,0,850,4,0,0,2018-08-08 21:23:38,Include Windows library build into mcsapi,"Currently one needs to manually build all dependent C++ libraries (libuv, snappy, boost, libxml2, libiconv, gtest etc.) and add them to their Visual Studio installation.

Efforts shall be made to automate the build of the dependent libraries for Windows and include them directly into the build process so that no manual dependency management is needed any more.

This could be done through Visual Studio's packet manager _nuget_, through git submodules of  needed libraries or a combination of both.",,1,3,0,134,0.885057,"Include Windows library build into mcsapi $end$ Currently one needs to manually build all dependent C++ libraries (libuv, snappy, boost, libxml2, libiconv, gtest etc.) and add them to their Visual Studio installation.

Efforts shall be made to automate the build of the dependent libraries for Windows and include them directly into the build process so that no manual dependency management is needed any more.

This could be done through Visual Studio's packet manager _nuget_, through git submodules of  needed libraries or a combination of both. $acceptance criteria:$",4,1,1,1,1,1,1,0.0,22,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
122,MCOL-1634,Task,MCOL,2018-08-08 21:29:43,,0,Include Windows library build into mcsapi,"Currently one needs to manually build all dependent C++ libraries (libuv, snappy, boost, libxml2, libiconv, gtest etc.) and add them to their Visual Studio installation.

Efforts shall be made to automate the build of the dependent libraries for Windows and include them directly into the build process so that no manual dependency management is needed any more.

This could be done through Visual Studio's packet manager _nuget_, through git submodules of  needed libraries or a combination of both.",,"Include Windows library build into mcsapi $end$ Currently one needs to manually build all dependent C++ libraries (libuv, snappy, boost, libxml2, libiconv, gtest etc.) and add them to their Visual Studio installation.

Efforts shall be made to automate the build of the dependent libraries for Windows and include them directly into the build process so that no manual dependency management is needed any more.

This could be done through Visual Studio's packet manager _nuget_, through git submodules of  needed libraries or a combination of both. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,14,,0,2,0,3,0,2,0,,0,850,2,0,0,2018-08-08 21:30:00,mcsapi Windows - add needed Windows Redistributables to installer,"To operate mcsapi on Windows 10, Microsoft Visual C++ Redistributables are needed.

These shall be added to the installer so that a customer doesn't need to obtain them in an additional step.

Needed Redistributables:
- Microsoft Visual C++ 2015 Redistributable (x64)
- Microsoft Visual C++ 2012 Redistributable (x64)",,1,1,0,123,1.28333,"mcsapi Windows - add needed Windows Redistributables to installer $end$ To operate mcsapi on Windows 10, Microsoft Visual C++ Redistributables are needed.

These shall be added to the installer so that a customer doesn't need to obtain them in an additional step.

Needed Redistributables:
- Microsoft Visual C++ 2015 Redistributable (x64)
- Microsoft Visual C++ 2012 Redistributable (x64) $acceptance criteria:$",2,1,1,1,1,1,1,0.0,23,1,0.0434783,1,0.0434783,1,0.0434783,1,0.0434783,1,0.0434783
123,MCOL-1642,New Feature,MCOL,2018-08-13 07:23:03,,0,Add SQL command that shows Primary Front-End MariaDB ColumnStore Module,"A command that shows whether a UM is the primary one is needed for MXS-1467. Adding a command (a function, variable or a table) allows users to know whether DDL statements can be executed on the UM.",,"Add SQL command that shows Primary Front-End MariaDB ColumnStore Module $end$ A command that shows whether a UM is the primary one is needed for MXS-1467. Adding a command (a function, variable or a table) allows users to know whether DDL statements can be executed on the UM. $acceptance criteria:$",,markus makela,markus makela,Major,14,,0,9,1,2,0,0,0,,0,850,5,0,0,2018-10-29 15:13:38,Add SQL command that shows Primary Front-End MariaDB ColumnStore Module,"A command that shows whether a UM is the primary one is needed for MXS-1467. Adding a command (a function, variable or a table) allows users to know whether DDL statements can be executed on the UM.",,0,0,0,0,0.0,"Add SQL command that shows Primary Front-End MariaDB ColumnStore Module $end$ A command that shows whether a UM is the primary one is needed for MXS-1467. Adding a command (a function, variable or a table) allows users to know whether DDL statements can be executed on the UM. $acceptance criteria:$",0,0,0,0,0,0,1,1855.83,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
124,MCOL-1644,New Feature,MCOL,2018-08-13 11:06:14,,0,PDI plugin CI tests - add check with PDI 8,"Currently test script downloads and tests only PDI version 7.1
Please add also testing flow with PDI version 8.
",,"PDI plugin CI tests - add check with PDI 8 $end$ Currently test script downloads and tests only PDI version 7.1
Please add also testing flow with PDI version 8.
 $acceptance criteria:$",,Elena Kotsinova,Elena Kotsinova,Minor,8,,1,2,1,3,0,0,0,,0,850,1,0,0,2018-08-22 21:19:17,PDI plugin CI tests - add check with PDI 8,"Currently test script downloads and tests only PDI version 7.1
Please add also testing flow with PDI version 8.
",,0,0,0,0,0.0,"PDI plugin CI tests - add check with PDI 8 $end$ Currently test script downloads and tests only PDI version 7.1
Please add also testing flow with PDI version 8.
 $acceptance criteria:$",0,0,0,0,0,0,1,226.217,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
125,MCOL-1646,Task,MCOL,2018-08-13 12:52:00,,0,Update AX docs for 18.04,Our AX install docs don't have instructions for Ubuntu 18.04. This should be rectified.,,Update AX docs for 18.04 $end$ Our AX install docs don't have instructions for Ubuntu 18.04. This should be rectified. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,2,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-08-13 12:52:00,Update AX docs for 18.04,Our AX install docs don't have instructions for Ubuntu 18.04. This should be rectified.,,0,0,0,0,0.0,Update AX docs for 18.04 $end$ Our AX install docs don't have instructions for Ubuntu 18.04. This should be rectified. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,61,3,0.0491803,2,0.0327869,1,0.0163934,1,0.0163934,1,0.0163934
126,MCOL-1661,New Feature,MCOL,2018-08-20 09:39:46,,0,Transform CDC events into UPDATE and DELETE statements,"Being able to transform the CDC events into events that update the ColumnStore instance allows replication of data from a remote MariaDB server to a ColumnStore setup.

As the ColumnStore API does not support updates or deletes, the adapter has to transform them into SQL statements to provide a current data stream instead of an append-only historical data stream.",,"Transform CDC events into UPDATE and DELETE statements $end$ Being able to transform the CDC events into events that update the ColumnStore instance allows replication of data from a remote MariaDB server to a ColumnStore setup.

As the ColumnStore API does not support updates or deletes, the adapter has to transform them into SQL statements to provide a current data stream instead of an append-only historical data stream. $acceptance criteria:$",,markus makela,markus makela,Major,13,,1,8,1,1,0,0,0,,0,850,5,0,0,2018-09-03 16:09:37,Transform CDC events into UPDATE and DELETE statements,"Being able to transform the CDC events into events that update the ColumnStore instance allows replication of data from a remote MariaDB server to a ColumnStore setup.

As the ColumnStore API does not support updates or deletes, the adapter has to transform them into SQL statements to provide a current data stream instead of an append-only historical data stream.",,0,0,0,0,0.0,"Transform CDC events into UPDATE and DELETE statements $end$ Being able to transform the CDC events into events that update the ColumnStore instance allows replication of data from a remote MariaDB server to a ColumnStore setup.

As the ColumnStore API does not support updates or deletes, the adapter has to transform them into SQL statements to provide a current data stream instead of an append-only historical data stream. $acceptance criteria:$",0,0,0,0,0,0,0,342.483,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
127,MCOL-1671,New Feature,MCOL,2018-08-22 21:45:05,,0,Windows mcsapi - add option to install libraries directly into Java,The mcsapi Windows installer should have an option to install javamcsapi directly into the Java JRE installation directory.,,Windows mcsapi - add option to install libraries directly into Java $end$ The mcsapi Windows installer should have an option to install javamcsapi directly into the Java JRE installation directory. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,15,,0,3,0,3,0,0,0,,0,850,2,0,0,2018-10-08 23:40:48,Windows mcsapi - add option to install libraries directly into Java,The mcsapi Windows installer should have an option to install javamcsapi directly into the Java JRE installation directory.,,0,0,0,0,0.0,Windows mcsapi - add option to install libraries directly into Java $end$ The mcsapi Windows installer should have an option to install javamcsapi directly into the Java JRE installation directory. $acceptance criteria:$,0,0,0,0,0,0,1,1129.92,24,2,0.0833333,2,0.0833333,2,0.0833333,2,0.0833333,2,0.0833333
128,MCOL-1682,Task,MCOL,2018-08-29 23:08:44,,0,PDI requested pull request changes,"To avoid merge conflicts, this ticket:

- If Win, else (Linux) in shared library detection
- Simplify directory concat to get cleaner code",,"PDI requested pull request changes $end$ To avoid merge conflicts, this ticket:

- If Win, else (Linux) in shared library detection
- Simplify directory concat to get cleaner code $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,9,,0,1,0,4,0,0,0,,0,850,0,0,0,2018-08-29 23:49:12,PDI requested pull request changes,"To avoid merge conflicts, this ticket:

- If Win, else (Linux) in shared library detection
- Simplify directory concat to get cleaner code",,0,0,0,0,0.0,"PDI requested pull request changes $end$ To avoid merge conflicts, this ticket:

- If Win, else (Linux) in shared library detection
- Simplify directory concat to get cleaner code $acceptance criteria:$",0,0,0,0,0,0,1,0.666667,25,2,0.08,2,0.08,2,0.08,2,0.08,2,0.08
129,MCOL-1688,New Feature,MCOL,2018-08-31 21:25:01,,0,Informatica PowerCenter Bulk Write Connector,"A connector for Informatica PowerCenter 10.2 shall be developed that supports bulk inserts through our Bulk Write SDK, and updates and deletes through JDBC.",,"Informatica PowerCenter Bulk Write Connector $end$ A connector for Informatica PowerCenter 10.2 shall be developed that supports bulk inserts through our Bulk Write SDK, and updates and deletes through JDBC. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,19,,1,0,3,8,0,0,0,,0,850,0,0,0,2018-08-31 21:25:01,Informatica PowerCenter Bulk Write Connector,"A connector for Informatica PowerCenter 10.2 shall be developed that supports bulk inserts through our Bulk Write SDK, and updates and deletes through JDBC.",,0,0,0,0,0.0,"Informatica PowerCenter Bulk Write Connector $end$ A connector for Informatica PowerCenter 10.2 shall be developed that supports bulk inserts through our Bulk Write SDK, and updates and deletes through JDBC. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,26,2,0.0769231,2,0.0769231,2,0.0769231,2,0.0769231,2,0.0769231
130,MCOL-1698,New Feature,MCOL,2018-09-06 15:08:32,,0,Add Distinct capability to specific UDAnF Window Functions,"Currently, there's no way to create a UDAnF that creates a DISTINCT result. The parser won't pass DISTINCT when an aggregate is used as an analytic function.

This ticket is to add a DISTINCT flag to the UDAnF context so that a UDAnF can be created that is always DISTINCT. It's also possible to create a multi-parm aggregate that could use one of the parameters to indicate distinctness.

This ticket does not cover the case of UDAF -- the use of a function as an aggregate, not a Window Function.

UDAnF created with this feature should have the UDAF_OVER_REQUIRED flag set in the runflags, as the distinctness would be ignored by aggregate use and would definitely confuse a user.",,"Add Distinct capability to specific UDAnF Window Functions $end$ Currently, there's no way to create a UDAnF that creates a DISTINCT result. The parser won't pass DISTINCT when an aggregate is used as an analytic function.

This ticket is to add a DISTINCT flag to the UDAnF context so that a UDAnF can be created that is always DISTINCT. It's also possible to create a multi-parm aggregate that could use one of the parameters to indicate distinctness.

This ticket does not cover the case of UDAF -- the use of a function as an aggregate, not a Window Function.

UDAnF created with this feature should have the UDAF_OVER_REQUIRED flag set in the runflags, as the distinctness would be ignored by aggregate use and would definitely confuse a user. $acceptance criteria:$",,David Hall,David Hall,Major,7,,0,2,1,5,0,0,0,,0,850,2,0,0,2018-09-06 15:11:39,Add Distinct capability to specific UDAnF Window Functions,"Currently, there's no way to create a UDAnF that creates a DISTINCT result. The parser won't pass DISTINCT when an aggregate is used as an analytic function.

This ticket is to add a DISTINCT flag to the UDAnF context so that a UDAnF can be created that is always DISTINCT. It's also possible to create a multi-parm aggregate that could use one of the parameters to indicate distinctness.

This ticket does not cover the case of UDAF -- the use of a function as an aggregate, not a Window Function.

UDAnF created with this feature should have the UDAF_OVER_REQUIRED flag set in the runflags, as the distinctness would be ignored by aggregate use and would definitely confuse a user.",,0,0,0,0,0.0,"Add Distinct capability to specific UDAnF Window Functions $end$ Currently, there's no way to create a UDAnF that creates a DISTINCT result. The parser won't pass DISTINCT when an aggregate is used as an analytic function.

This ticket is to add a DISTINCT flag to the UDAnF context so that a UDAnF can be created that is always DISTINCT. It's also possible to create a multi-parm aggregate that could use one of the parameters to indicate distinctness.

This ticket does not cover the case of UDAF -- the use of a function as an aggregate, not a Window Function.

UDAnF created with this feature should have the UDAF_OVER_REQUIRED flag set in the runflags, as the distinctness would be ignored by aggregate use and would definitely confuse a user. $acceptance criteria:$",0,0,0,0,0,0,1,0.05,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
131,MCOL-1707,Sub-Task,MCOL,2018-09-07 21:42:27,,0,slack reporting for buildbot builds,Build bot should report to slack with links to build test results and links to download failure logs,,slack reporting for buildbot builds $end$ Build bot should report to slack with links to build test results and links to download failure logs $acceptance criteria:$,,Ben Thompson,Ben Thompson,Minor,2,,0,0,0,29,0,0,0,,0,850,0,0,0,2018-09-07 21:42:27,slack reporting for buildbot builds,Build bot should report to slack with links to build test results and links to download failure logs,,0,0,0,0,0.0,slack reporting for buildbot builds $end$ Build bot should report to slack with links to build test results and links to download failure logs $acceptance criteria:$,0,0,0,0,0,0,1,0.0,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
132,MCOL-1713,Task,MCOL,2018-09-12 01:03:20,,0,Add Windows suffix to kettle data adapter,Windows suffix in the generated zip name is missing.,,Add Windows suffix to kettle data adapter $end$ Windows suffix in the generated zip name is missing. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Trivial,13,,0,3,0,2,0,0,0,,0,850,3,0,0,2018-09-12 01:03:20,Add Windows suffix to kettle data adapter,Windows suffix in the generated zip name is missing.,,0,0,0,0,0.0,Add Windows suffix to kettle data adapter $end$ Windows suffix in the generated zip name is missing. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,27,2,0.0740741,2,0.0740741,2,0.0740741,2,0.0740741,2,0.0740741
133,MCOL-1719,Task,MCOL,2018-09-13 06:58:17,,0,Rebase 1.2 on MariaDB 10.3.9,We need 1.2 to be based on 10.3.9,,Rebase 1.2 on MariaDB 10.3.9 $end$ We need 1.2 to be based on 10.3.9 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,12,,0,2,0,7,0,0,0,,0,850,2,0,0,2018-09-13 06:58:17,Rebase 1.2 on MariaDB 10.3.9,We need 1.2 to be based on 10.3.9,,0,0,0,0,0.0,Rebase 1.2 on MariaDB 10.3.9 $end$ We need 1.2 to be based on 10.3.9 $acceptance criteria:$,0,0,0,0,0,0,1,0.0,62,3,0.0483871,2,0.0322581,1,0.016129,1,0.016129,1,0.016129
134,MCOL-1731,New Feature,MCOL,2018-09-19 23:09:16,,0,informatica adapters should allow specification of custom jdbc properties,"As discussed, i think it would be helpful to allow someone to specify custom jdbc properties on the jdbc connect string just in case we need this to override certain behavior of our jdbc connector.   This should be entirely optional and 'advanced' mode if applicable, i.e most users should not need to do this.",,"informatica adapters should allow specification of custom jdbc properties $end$ As discussed, i think it would be helpful to allow someone to specify custom jdbc properties on the jdbc connect string just in case we need this to override certain behavior of our jdbc connector.   This should be entirely optional and 'advanced' mode if applicable, i.e most users should not need to do this. $acceptance criteria:$",,David Thompson,David Thompson,Major,8,,0,2,0,3,0,0,0,,0,850,2,0,0,2018-09-28 18:32:15,informatica adapters should allow specification of custom jdbc properties,"As discussed, i think it would be helpful to allow someone to specify custom jdbc properties on the jdbc connect string just in case we need this to override certain behavior of our jdbc connector.   This should be entirely optional and 'advanced' mode if applicable, i.e most users should not need to do this.",,0,0,0,0,0.0,"informatica adapters should allow specification of custom jdbc properties $end$ As discussed, i think it would be helpful to allow someone to specify custom jdbc properties on the jdbc connect string just in case we need this to override certain behavior of our jdbc connector.   This should be entirely optional and 'advanced' mode if applicable, i.e most users should not need to do this. $acceptance criteria:$",0,0,0,0,0,0,1,211.367,53,5,0.0943396,5,0.0943396,3,0.0566038,2,0.0377358,2,0.0377358
135,MCOL-1739,New Feature,MCOL,2018-09-21 18:30:20,,0,"Split mcsapi installation into different packages for C++, Java and Python","Currently we have one installation packet for mcsapi, javamcsapi, and pymcsapi.

Thus, a customer needs to meet all dependencies (Python 2.7, Python 3, Java ...) to install the packet even though he might just wants to use one component.

The same problem arises for the data-adapters build on top of mcsapi.

Therefore, it would make sense to split the installation from one packet into individual packets for mcsapi, javamcsapi, pymcsapi, Spark Scala connector and Spark Python connector.",,"Split mcsapi installation into different packages for C++, Java and Python $end$ Currently we have one installation packet for mcsapi, javamcsapi, and pymcsapi.

Thus, a customer needs to meet all dependencies (Python 2.7, Python 3, Java ...) to install the packet even though he might just wants to use one component.

The same problem arises for the data-adapters build on top of mcsapi.

Therefore, it would make sense to split the installation from one packet into individual packets for mcsapi, javamcsapi, pymcsapi, Spark Scala connector and Spark Python connector. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,17,,1,2,4,1,0,0,0,,0,850,1,0,0,2018-11-13 00:44:27,"Split mcsapi installation into different packages for C++, Java and Python","Currently we have one installation packet for mcsapi, javamcsapi, and pymcsapi.

Thus, a customer needs to meet all dependencies (Python 2.7, Python 3, Java ...) to install the packet even though he might just wants to use one component.

The same problem arises for the data-adapters build on top of mcsapi.

Therefore, it would make sense to split the installation from one packet into individual packets for mcsapi, javamcsapi, pymcsapi, Spark Scala connector and Spark Python connector.",,0,0,0,0,0.0,"Split mcsapi installation into different packages for C++, Java and Python $end$ Currently we have one installation packet for mcsapi, javamcsapi, and pymcsapi.

Thus, a customer needs to meet all dependencies (Python 2.7, Python 3, Java ...) to install the packet even though he might just wants to use one component.

The same problem arises for the data-adapters build on top of mcsapi.

Therefore, it would make sense to split the installation from one packet into individual packets for mcsapi, javamcsapi, pymcsapi, Spark Scala connector and Spark Python connector. $acceptance criteria:$",0,0,0,0,0,0,0,1254.23,28,2,0.0714286,2,0.0714286,2,0.0714286,2,0.0714286,2,0.0714286
136,MCOL-1740,New Feature,MCOL,2018-09-21 18:35:50,,0,mcsimport - depend on mcsapi only,Once the columnstore-api installation is split into distinct packages remote cpimport should just depend on the C++ part of mcsapi.,,mcsimport - depend on mcsapi only $end$ Once the columnstore-api installation is split into distinct packages remote cpimport should just depend on the C++ part of mcsapi. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,10,,0,5,2,1,0,0,0,,0,850,2,0,0,2018-11-15 11:53:52,mcsimport - depend on mcsapi only,Once the columnstore-api installation is split into distinct packages remote cpimport should just depend on the C++ part of mcsapi.,,0,0,0,0,0.0,mcsimport - depend on mcsapi only $end$ Once the columnstore-api installation is split into distinct packages remote cpimport should just depend on the C++ part of mcsapi. $acceptance criteria:$,0,0,0,0,0,0,0,1313.3,29,2,0.0689655,2,0.0689655,2,0.0689655,2,0.0689655,2,0.0689655
137,MCOL-1743,Task,MCOL,2018-09-22 00:49:17,,0,Documentation issue in repoinstallation of columnstore-kafka-adapter ,"We seem to have a general documentation issue in \[1\], package ""mariadb-columnstore-kafka-adapters"" can't be found and should be renamed to ""mariadb-columnstore-kafka-avro-adapters"".

\[1\] https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-package-repositories/",,"Documentation issue in repoinstallation of columnstore-kafka-adapter  $end$ We seem to have a general documentation issue in \[1\], package ""mariadb-columnstore-kafka-adapters"" can't be found and should be renamed to ""mariadb-columnstore-kafka-avro-adapters"".

\[1\] https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-package-repositories/ $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,12,,2,0,2,1,0,0,0,,0,850,0,0,0,2018-11-14 16:19:35,Documentation issue in repoinstallation of columnstore-kafka-adapter ,"We seem to have a general documentation issue in \[1\], package ""mariadb-columnstore-kafka-adapters"" can't be found and should be renamed to ""mariadb-columnstore-kafka-avro-adapters"".

\[1\] https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-package-repositories/",,0,0,0,0,0.0,"Documentation issue in repoinstallation of columnstore-kafka-adapter  $end$ We seem to have a general documentation issue in \[1\], package ""mariadb-columnstore-kafka-adapters"" can't be found and should be renamed to ""mariadb-columnstore-kafka-avro-adapters"".

\[1\] https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-package-repositories/ $acceptance criteria:$",0,0,0,0,0,0,0,1287.5,30,2,0.0666667,2,0.0666667,2,0.0666667,2,0.0666667,2,0.0666667
138,MCOL-1754,Task,MCOL,2018-09-28 20:11:32,,0,Change libmysql dependency for Windows api tests to libmariadb,"Currently the tests used the MySQL C connector to verify the results on Windows 10.
It should be switched to use the MariaDB C connector instead.",,"Change libmysql dependency for Windows api tests to libmariadb $end$ Currently the tests used the MySQL C connector to verify the results on Windows 10.
It should be switched to use the MariaDB C connector instead. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,9,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-09-28 20:11:32,Change libmysql dependency for Windows api tests to libmariadb,"Currently the tests used the MySQL C connector to verify the results on Windows 10.
It should be switched to use the MariaDB C connector instead.",,0,0,0,0,0.0,"Change libmysql dependency for Windows api tests to libmariadb $end$ Currently the tests used the MySQL C connector to verify the results on Windows 10.
It should be switched to use the MariaDB C connector instead. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,31,2,0.0645161,2,0.0645161,2,0.0645161,2,0.0645161,2,0.0645161
139,MCOL-1759,New Feature,MCOL,2018-10-01 22:22:38,,0,"Implement regr_sxx, regr_syy, regr_sxy and corr functions as UDAF","We have a requirement to implement the regr_xxx functions for Columnstore. The list of functions to implement did not include regr_sxx, reg_syy or regr_sxy. These functions seem to be part of most other database offerings and should be part of ours.

Also, the corr(x, y) function appears to be in everyone's library as well.

These functions are trivial to implement within the UDAF API. Writing test cases will take longer than implementation.

",,"Implement regr_sxx, regr_syy, regr_sxy and corr functions as UDAF $end$ We have a requirement to implement the regr_xxx functions for Columnstore. The list of functions to implement did not include regr_sxx, reg_syy or regr_sxy. These functions seem to be part of most other database offerings and should be part of ours.

Also, the corr(x, y) function appears to be in everyone's library as well.

These functions are trivial to implement within the UDAF API. Writing test cases will take longer than implementation.

 $acceptance criteria:$",,David Hall,David Hall,Major,10,,1,4,1,1,0,0,0,,0,850,4,0,0,2018-10-01 22:23:31,"Implement regr_sxx, regr_syy, regr_sxy and corr functions as UDAF","We have a requirement to implement the regr_xxx functions for Columnstore. The list of functions to implement did not include regr_sxx, reg_syy or regr_sxy. These functions seem to be part of most other database offerings and should be part of ours.

Also, the corr(x, y) function appears to be in everyone's library as well.

These functions are trivial to implement within the UDAF API. Writing test cases will take longer than implementation.

",,0,0,0,0,0.0,"Implement regr_sxx, regr_syy, regr_sxy and corr functions as UDAF $end$ We have a requirement to implement the regr_xxx functions for Columnstore. The list of functions to implement did not include regr_sxx, reg_syy or regr_sxy. These functions seem to be part of most other database offerings and should be part of ours.

Also, the corr(x, y) function appears to be in everyone's library as well.

These functions are trivial to implement within the UDAF API. Writing test cases will take longer than implementation.

 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
140,MCOL-1774,New Feature,MCOL,2018-10-05 20:59:49,,0,mcsimport - enclose by character support and escape character for enclose by char,"Add options for enclose by characters, like quotation marks for text fields, and escape characters for these enclose by characters.

Use the same command line parameters that are used in cpimport.",,"mcsimport - enclose by character support and escape character for enclose by char $end$ Add options for enclose by characters, like quotation marks for text fields, and escape characters for these enclose by characters.

Use the same command line parameters that are used in cpimport. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,7,,1,1,2,2,0,0,0,,0,850,0,0,0,2018-11-01 05:05:25,mcsimport - enclose by character support and escape character for enclose by char,"Add options for enclose by characters, like quotation marks for text fields, and escape characters for these enclose by characters.

Use the same command line parameters that are used in cpimport.",,0,0,0,0,0.0,"mcsimport - enclose by character support and escape character for enclose by char $end$ Add options for enclose by characters, like quotation marks for text fields, and escape characters for these enclose by characters.

Use the same command line parameters that are used in cpimport. $acceptance criteria:$",0,0,0,0,0,0,1,632.083,32,2,0.0625,2,0.0625,2,0.0625,2,0.0625,2,0.0625
141,MCOL-1790,Task,MCOL,2018-10-11 12:29:49,MCOL-1096,0,Implement new CASE item type detection,MDEV-16885 implements a new uniform way to detect case_simple / case_searched. We need to switch to it.,,Implement new CASE item type detection $end$ MDEV-16885 implements a new uniform way to detect case_simple / case_searched. We need to switch to it. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,1,1,0,0,0,,0,850,2,0,0,2018-11-19 19:05:32,Implement new CASE item type detection,MDEV-16885 implements a new uniform way to detect case_simple / case_searched. We need to switch to it.,,0,0,0,0,0.0,Implement new CASE item type detection $end$ MDEV-16885 implements a new uniform way to detect case_simple / case_searched. We need to switch to it. $acceptance criteria:$,0,0,0,0,0,0,0,942.583,63,3,0.047619,2,0.031746,1,0.015873,1,0.015873,1,0.015873
142,MCOL-1804,Task,MCOL,2018-10-15 09:35:13,,0,Rebase 1.2 on MariaDB 10.3.10,10.3.10 is out (announce email wasn't sent so I missed it). We need to rebase the develop server tree on this.,,Rebase 1.2 on MariaDB 10.3.10 $end$ 10.3.10 is out (announce email wasn't sent so I missed it). We need to rebase the develop server tree on this. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-11-05 13:09:00,Rebase 1.2 on MariaDB 10.3.10,10.3.10 is out (announce email wasn't sent so I missed it). We need to rebase the develop server tree on this.,,0,0,0,0,0.0,Rebase 1.2 on MariaDB 10.3.10 $end$ 10.3.10 is out (announce email wasn't sent so I missed it). We need to rebase the develop server tree on this. $acceptance criteria:$,0,0,0,0,0,0,0,507.55,64,3,0.046875,2,0.03125,1,0.015625,1,0.015625,1,0.015625
143,MCOL-1809,Task,MCOL,2018-10-16 08:59:41,,0,Document mysql_upgrade requirement as part of the upgrade process,"After upgrade to 1.1.6 the following error message occured in the mysql error log.


{code:java}
 [Warning] InnoDB: Table mysql/innodb_table_stats has length mismatch in the column name table_name. Please run mysql_upgrade
{code}

Root cause is a important change in MySQL 5.7.23


{code:java}
Important Change; Partitioning: After creating partitioned InnoDB tables with very long names, the table_name columns in the corresponding entries in the mysql.innodb_index_stats and mysql.innodb_table_stats system tables were truncated. To fix this issue, the length of the table_name column in each of these tables has been increased from 64 to 199 characters.
https://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-23.html
{code}

MCS 1.1.6 based on MariaDB 10.217

MariaDB 10.2.17 Release Notes
""InnoDB updated to 5.7.23 ""

So I suggest to add  an additional point 
mysql_upgrade 

for the
[upgrade description|https://mariadb.com/kb/en/library/mariadb-columnstore-software-upgrade-115-ga-to-116-ga/].


",,"Document mysql_upgrade requirement as part of the upgrade process $end$ After upgrade to 1.1.6 the following error message occured in the mysql error log.


{code:java}
 [Warning] InnoDB: Table mysql/innodb_table_stats has length mismatch in the column name table_name. Please run mysql_upgrade
{code}

Root cause is a important change in MySQL 5.7.23


{code:java}
Important Change; Partitioning: After creating partitioned InnoDB tables with very long names, the table_name columns in the corresponding entries in the mysql.innodb_index_stats and mysql.innodb_table_stats system tables were truncated. To fix this issue, the length of the table_name column in each of these tables has been increased from 64 to 199 characters.
https://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-23.html
{code}

MCS 1.1.6 based on MariaDB 10.217

MariaDB 10.2.17 Release Notes
""InnoDB updated to 5.7.23 ""

So I suggest to add  an additional point 
mysql_upgrade 

for the
[upgrade description|https://mariadb.com/kb/en/library/mariadb-columnstore-software-upgrade-115-ga-to-116-ga/].


 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,11,,2,4,2,1,0,1,0,,0,850,4,0,0,2018-11-09 07:58:10, mysql/innodb_table_stats has length mismatch after upgrade to 1.1.6,"After upgrade to 1.1.6 the following error message occured in the mysql error log.


{code:java}
 [Warning] InnoDB: Table mysql/innodb_table_stats has length mismatch in the column name table_name. Please run mysql_upgrade
{code}

Root cause is a important change in MySQL 5.7.23


{code:java}
Important Change; Partitioning: After creating partitioned InnoDB tables with very long names, the table_name columns in the corresponding entries in the mysql.innodb_index_stats and mysql.innodb_table_stats system tables were truncated. To fix this issue, the length of the table_name column in each of these tables has been increased from 64 to 199 characters.
https://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-23.html
{code}

MCS 1.1.6 based on MariaDB 10.217

MariaDB 10.2.17 Release Notes
""InnoDB updated to 5.7.23 ""

So I suggest to add  an additional point 
mysql_upgrade 

for the
[upgrade description|https://mariadb.com/kb/en/library/mariadb-columnstore-software-upgrade-115-ga-to-116-ga/].


",,1,0,0,15,0.0676692," mysql/innodb_table_stats has length mismatch after upgrade to 1.1.6 $end$ After upgrade to 1.1.6 the following error message occured in the mysql error log.


{code:java}
 [Warning] InnoDB: Table mysql/innodb_table_stats has length mismatch in the column name table_name. Please run mysql_upgrade
{code}

Root cause is a important change in MySQL 5.7.23


{code:java}
Important Change; Partitioning: After creating partitioned InnoDB tables with very long names, the table_name columns in the corresponding entries in the mysql.innodb_index_stats and mysql.innodb_table_stats system tables were truncated. To fix this issue, the length of the table_name column in each of these tables has been increased from 64 to 199 characters.
https://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-23.html
{code}

MCS 1.1.6 based on MariaDB 10.217

MariaDB 10.2.17 Release Notes
""InnoDB updated to 5.7.23 ""

So I suggest to add  an additional point 
mysql_upgrade 

for the
[upgrade description|https://mariadb.com/kb/en/library/mariadb-columnstore-software-upgrade-115-ga-to-116-ga/].


 $acceptance criteria:$",1,1,1,1,1,0,1,574.967,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
144,MCOL-1816,New Feature,MCOL,2018-10-17 21:27:32,,0,mcsapi - support bool data type,"With the new introduction of the boolean data type, mcsapi should support it as well.",,"mcsapi - support bool data type $end$ With the new introduction of the boolean data type, mcsapi should support it as well. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,11,,0,4,2,1,0,0,0,,0,850,2,0,0,2018-11-05 09:58:03,mcsapi - support bool data type,"With the new introduction of the boolean data type, mcsapi should support it as well.",,0,0,0,0,0.0,"mcsapi - support bool data type $end$ With the new introduction of the boolean data type, mcsapi should support it as well. $acceptance criteria:$",0,0,0,0,0,0,0,444.5,33,2,0.0606061,2,0.0606061,2,0.0606061,2,0.0606061,2,0.0606061
145,MCOL-1817,New Feature,MCOL,2018-10-17 21:29:05,,0,Pentaho support bool data type,"Once mcsapi supports boolean data types, our PDI plugin should support it too.",,"Pentaho support bool data type $end$ Once mcsapi supports boolean data types, our PDI plugin should support it too. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,7,,0,1,1,1,0,0,0,,0,850,0,0,0,2018-11-08 19:04:28,Pentaho support bool data type,"Once mcsapi supports boolean data types, our PDI plugin should support it too.",,0,0,0,0,0.0,"Pentaho support bool data type $end$ Once mcsapi supports boolean data types, our PDI plugin should support it too. $acceptance criteria:$",0,0,0,0,0,0,0,525.583,34,2,0.0588235,2,0.0588235,2,0.0588235,2,0.0588235,2,0.0588235
146,MCOL-1822,New Feature,MCOL,2018-10-18 18:34:00,,0,Change the default to use double when overflow occurs in SUM() and AVG(),"The SUM and AVG functions have logic to overflow from integer to double arithmetic when overflow occurs in int64. However, this logic is #defined out of the standard build.

The default logic should be to overflow into double if necessary, rather than aborting the query.",,"Change the default to use double when overflow occurs in SUM() and AVG() $end$ The SUM and AVG functions have logic to overflow from integer to double arithmetic when overflow occurs in int64. However, this logic is #defined out of the standard build.

The default logic should be to overflow into double if necessary, rather than aborting the query. $acceptance criteria:$",,David Hall,David Hall,Major,14,,3,5,3,5,0,0,0,,0,850,5,0,0,2018-11-19 19:06:20,Change the default to use double when overflow occurs in SUM() and AVG(),"The SUM and AVG functions have logic to overflow from integer to double arithmetic when overflow occurs in int64. However, this logic is #defined out of the standard build.

The default logic should be to overflow into double if necessary, rather than aborting the query.",,0,0,0,0,0.0,"Change the default to use double when overflow occurs in SUM() and AVG() $end$ The SUM and AVG functions have logic to overflow from integer to double arithmetic when overflow occurs in int64. However, this logic is #defined out of the standard build.

The default logic should be to overflow into double if necessary, rather than aborting the query. $acceptance criteria:$",0,0,0,0,0,0,1,768.533,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
147,MCOL-1844,New Feature,MCOL,2018-10-31 20:57:17,,0,Allow prior/custom changes made to myCnf-include-args.text be added to new myCnf-include-args.text after upgrading,Allow for prior/custom changes that have been made to [the original] myCnf-include-args.text be added/included in the new myCnf-include-args.text that is created after upgrading,,Allow prior/custom changes made to myCnf-include-args.text be added to new myCnf-include-args.text after upgrading $end$ Allow for prior/custom changes that have been made to [the original] myCnf-include-args.text be added/included in the new myCnf-include-args.text that is created after upgrading $acceptance criteria:$,,Chris Calender,Chris Calender,Minor,9,,0,5,1,1,0,0,0,,0,850,3,0,0,2018-11-20 15:33:11,Allow prior/custom changes made to myCnf-include-args.text be added to new myCnf-include-args.text after upgrading,Allow for prior/custom changes that have been made to [the original] myCnf-include-args.text be added/included in the new myCnf-include-args.text that is created after upgrading,,0,0,0,0,0.0,Allow prior/custom changes made to myCnf-include-args.text be added to new myCnf-include-args.text after upgrading $end$ Allow for prior/custom changes that have been made to [the original] myCnf-include-args.text be added/included in the new myCnf-include-args.text that is created after upgrading $acceptance criteria:$,0,0,0,0,0,0,0,474.583,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
148,MCOL-1866,Task,MCOL,2018-11-08 21:25:01,,0,"Change logo in mcsapi, PDI and mcsimport","Switch to the new logo in Windows installer, documentation, and within PDI",,"Change logo in mcsapi, PDI and mcsimport $end$ Switch to the new logo in Windows installer, documentation, and within PDI $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,7,,0,1,0,1,0,0,0,,0,850,0,0,0,2018-11-20 17:48:25,"Change logo in mcsapi, PDI and mcsimport","Switch to the new logo in Windows installer, documentation, and within PDI",,0,0,0,0,0.0,"Change logo in mcsapi, PDI and mcsimport $end$ Switch to the new logo in Windows installer, documentation, and within PDI $acceptance criteria:$",0,0,0,0,0,0,0,284.383,35,2,0.0571429,2,0.0571429,2,0.0571429,2,0.0571429,2,0.0571429
149,MCOL-1889,Task,MCOL,2018-11-14 17:04:58,,0,mcs 1.2.2 release notes,"1.2.2 release notes that everyone should use to add when something changes in 1.2.2 that need to be recorded, like:

1. package name change
2. new feature or package added
3. new KB doc added",,"mcs 1.2.2 release notes $end$ 1.2.2 release notes that everyone should use to add when something changes in 1.2.2 that need to be recorded, like:

1. package name change
2. new feature or package added
3. new KB doc added $acceptance criteria:$",,David Hill,David Hill,Trivial,4,,2,2,3,2,0,0,0,,0,850,1,0,0,2018-11-19 19:06:50,mcs 1.2.2 release notes,"1.2.2 release notes that everyone should use to add when something changes in 1.2.2 that need to be recorded, like:

1. package name change
2. new feature or package added
3. new KB doc added",,0,0,0,0,0.0,"mcs 1.2.2 release notes $end$ 1.2.2 release notes that everyone should use to add when something changes in 1.2.2 that need to be recorded, like:

1. package name change
2. new feature or package added
3. new KB doc added $acceptance criteria:$",0,0,0,0,0,0,1,122.017,30,4,0.133333,0,0.0,0,0.0,0,0.0,0,0.0
150,MCOL-1944,Task,MCOL,2018-11-15 18:43:06,,0,/var/log/mariadb/columnstore ownership set to 777 recursive,"In syslogSetup.sh:

{code}
chmod 777 -R /var/log/mariadb/columnstore
{code}

That is definitely the wrong thing to do.",,"/var/log/mariadb/columnstore ownership set to 777 recursive $end$ In syslogSetup.sh:

{code}
chmod 777 -R /var/log/mariadb/columnstore
{code}

That is definitely the wrong thing to do. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Blocker,8,,1,14,1,1,0,0,0,,0,850,13,0,0,2018-11-15 18:48:22,/var/log/mariadb/columnstore ownership set to 777 recursive,"In syslogSetup.sh:

{code}
chmod 777 -R /var/log/mariadb/columnstore
{code}

That is definitely the wrong thing to do.",,0,0,0,0,0.0,"/var/log/mariadb/columnstore ownership set to 777 recursive $end$ In syslogSetup.sh:

{code}
chmod 777 -R /var/log/mariadb/columnstore
{code}

That is definitely the wrong thing to do. $acceptance criteria:$",0,0,0,0,0,0,0,0.0833333,65,3,0.0461538,2,0.0307692,1,0.0153846,1,0.0153846,1,0.0153846
151,MCOL-1952,Task,MCOL,2018-11-20 15:38:21,,0,Rebase develop 10.3.11,MariaDB 10.3.11 has now been released.,,Rebase develop 10.3.11 $end$ MariaDB 10.3.11 has now been released. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,2,0,1,0,0,0,,0,850,2,0,0,2018-11-20 15:38:21,Rebase develop 10.3.11,MariaDB 10.3.11 has now been released.,,0,0,0,0,0.0,Rebase develop 10.3.11 $end$ MariaDB 10.3.11 has now been released. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,66,3,0.0454545,2,0.030303,1,0.0151515,1,0.0151515,1,0.0151515
152,MCOL-1961,New Feature,MCOL,2018-11-25 10:24:42,,0,"javamcsapi, pymcsapi known isTableLock and TableLockInfo limitations","- mcsapi's isTableLocked() function is effected by MCOL-1218 and only detects locks of tables that were existent when the ColumnStoreDriver was initialized
- javamcsapi and pymcsapi don't yet have methods to access the dbroot array returned by TableLockInfo's getDbrootList()
- javamcsapi and pymcsapi don't yet have methods to access the the creation time returned by TableLockInfo's getCreationTime()
- pymcsapi's isTableLocked() function only returns bool but not also the additional TableLockInfo object mcsapi returns",,"javamcsapi, pymcsapi known isTableLock and TableLockInfo limitations $end$ - mcsapi's isTableLocked() function is effected by MCOL-1218 and only detects locks of tables that were existent when the ColumnStoreDriver was initialized
- javamcsapi and pymcsapi don't yet have methods to access the dbroot array returned by TableLockInfo's getDbrootList()
- javamcsapi and pymcsapi don't yet have methods to access the the creation time returned by TableLockInfo's getCreationTime()
- pymcsapi's isTableLocked() function only returns bool but not also the additional TableLockInfo object mcsapi returns $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,10,,0,1,1,4,0,0,0,,0,850,1,0,0,2019-01-07 20:06:55,"javamcsapi, pymcsapi known isTableLock and TableLockInfo limitations","- mcsapi's isTableLocked() function is effected by MCOL-1218 and only detects locks of tables that were existent when the ColumnStoreDriver was initialized
- javamcsapi and pymcsapi don't yet have methods to access the dbroot array returned by TableLockInfo's getDbrootList()
- javamcsapi and pymcsapi don't yet have methods to access the the creation time returned by TableLockInfo's getCreationTime()
- pymcsapi's isTableLocked() function only returns bool but not also the additional TableLockInfo object mcsapi returns",,0,0,0,0,0.0,"javamcsapi, pymcsapi known isTableLock and TableLockInfo limitations $end$ - mcsapi's isTableLocked() function is effected by MCOL-1218 and only detects locks of tables that were existent when the ColumnStoreDriver was initialized
- javamcsapi and pymcsapi don't yet have methods to access the dbroot array returned by TableLockInfo's getDbrootList()
- javamcsapi and pymcsapi don't yet have methods to access the the creation time returned by TableLockInfo's getCreationTime()
- pymcsapi's isTableLocked() function only returns bool but not also the additional TableLockInfo object mcsapi returns $acceptance criteria:$",0,0,0,0,0,0,1,1041.7,36,2,0.0555556,2,0.0555556,2,0.0555556,2,0.0555556,2,0.0555556
153,MCOL-2,Task,MCOL,2016-05-02 16:39:54,,0,Update engine name to columnstore,Update engine name to columnstore,,Update engine name to columnstore $end$ Update engine name to columnstore $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,16,,0,6,0,1,0,0,0,,0,850,6,0,0,2016-05-03 21:21:55,Update engine name to columnstore,Update engine name to columnstore,,0,0,0,0,0.0,Update engine name to columnstore $end$ Update engine name to columnstore $acceptance criteria:$,0,0,0,0,0,0,0,28.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
154,MCOL-2005,Task,MCOL,2018-12-10 15:18:35,,0,Merge MariaDB 10.2.21 into develop-1.1,"MariaDB 10.2.19, 20 and 21 have been released",,"Merge MariaDB 10.2.21 into develop-1.1 $end$ MariaDB 10.2.19, 20 and 21 have been released $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,9,,1,1,1,2,0,2,0,,0,850,1,0,0,2018-12-10 15:18:35,Merge MariaDB 10.2.19 into develop-1.1,MariaDB 10.2.19 is released,,1,1,0,10,0.583333,Merge MariaDB 10.2.19 into develop-1.1 $end$ MariaDB 10.2.19 is released $acceptance criteria:$,2,1,1,1,0,0,1,0.0,67,3,0.0447761,2,0.0298507,1,0.0149254,1,0.0149254,1,0.0149254
155,MCOL-2013,New Feature,MCOL,2018-12-10 21:30:34,,0,API .NET support - Alpha,Review of external contribution https://github.com/mariadb-corporation/mariadb-columnstore-api/pull/142/files,,API .NET support - Alpha $end$ Review of external contribution https://github.com/mariadb-corporation/mariadb-columnstore-api/pull/142/files $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,20,,0,1,0,1,0,1,0,,0,850,1,0,0,2018-12-10 22:06:18,API .NET support,Review of external contribution https://github.com/mariadb-corporation/mariadb-columnstore-api/pull/142/files,,1,0,0,2,0.181818,API .NET support $end$ Review of external contribution https://github.com/mariadb-corporation/mariadb-columnstore-api/pull/142/files $acceptance criteria:$,1,1,0,0,0,0,0,0.583333,68,4,0.0588235,3,0.0441176,2,0.0294118,1,0.0147059,1,0.0147059
156,MCOL-2068,Task,MCOL,2019-01-07 16:58:07,,0,add support for using and defaulting memory based settings to docker image,"ColumnsStore now supports explicit memory settings for NumBlocksPct and TotalUmMemory however i believe there is not currently a way to force  / override these in postConfigure. 

The docker image should support 2 new env variables to allow overriding these and also default to some sensible small values like 1024m for NPB and 512m for TUM.  Assuming my analysis is correct we'll need a mechanism to allow postCfg to take and default values for these (either as parameters or utilizing the values already in ColumnStore.xml (there is some logic for this in the upgrade path).",,"add support for using and defaulting memory based settings to docker image $end$ ColumnsStore now supports explicit memory settings for NumBlocksPct and TotalUmMemory however i believe there is not currently a way to force  / override these in postConfigure. 

The docker image should support 2 new env variables to allow overriding these and also default to some sensible small values like 1024m for NPB and 512m for TUM.  Assuming my analysis is correct we'll need a mechanism to allow postCfg to take and default values for these (either as parameters or utilizing the values already in ColumnStore.xml (there is some logic for this in the upgrade path). $acceptance criteria:$",,David Thompson,David Thompson,Major,12,,1,2,1,4,0,0,0,,0,850,2,0,0,2019-01-07 16:58:07,add support for using and defaulting memory based settings to docker image,"ColumnsStore now supports explicit memory settings for NumBlocksPct and TotalUmMemory however i believe there is not currently a way to force  / override these in postConfigure. 

The docker image should support 2 new env variables to allow overriding these and also default to some sensible small values like 1024m for NPB and 512m for TUM.  Assuming my analysis is correct we'll need a mechanism to allow postCfg to take and default values for these (either as parameters or utilizing the values already in ColumnStore.xml (there is some logic for this in the upgrade path).",,0,0,0,0,0.0,"add support for using and defaulting memory based settings to docker image $end$ ColumnsStore now supports explicit memory settings for NumBlocksPct and TotalUmMemory however i believe there is not currently a way to force  / override these in postConfigure. 

The docker image should support 2 new env variables to allow overriding these and also default to some sensible small values like 1024m for NPB and 512m for TUM.  Assuming my analysis is correct we'll need a mechanism to allow postCfg to take and default values for these (either as parameters or utilizing the values already in ColumnStore.xml (there is some logic for this in the upgrade path). $acceptance criteria:$",0,0,0,0,0,0,1,0.0,54,5,0.0925926,5,0.0925926,3,0.0555556,2,0.037037,2,0.037037
157,MCOL-2082,Task,MCOL,2019-01-15 14:50:39,,0,Write Spark and PySpark documentation for mcsapi,"Similar to the [java|py]mcsapi documentations, usage and API documentations for the bulk write SDK's Spark and PySpark functions shall be written.",,"Write Spark and PySpark documentation for mcsapi $end$ Similar to the [java|py]mcsapi documentations, usage and API documentations for the bulk write SDK's Spark and PySpark functions shall be written. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,8,,0,2,0,3,0,0,0,,0,850,1,0,0,2019-01-28 11:04:05,Write Spark and PySpark documentation for mcsapi,"Similar to the [java|py]mcsapi documentations, usage and API documentations for the bulk write SDK's Spark and PySpark functions shall be written.",,0,0,0,0,0.0,"Write Spark and PySpark documentation for mcsapi $end$ Similar to the [java|py]mcsapi documentations, usage and API documentations for the bulk write SDK's Spark and PySpark functions shall be written. $acceptance criteria:$",0,0,0,0,0,0,1,308.217,37,2,0.0540541,2,0.0540541,2,0.0540541,2,0.0540541,2,0.0540541
158,MCOL-2084,Task,MCOL,2019-01-15 14:58:22,MCOL-2014,0,"Subclass IDBDataFile & IDBPolicy, & integrate with file IO system",Make subclasses of IDBDataFile and IDBPolicy that send IO requests to StorageManager.,,"Subclass IDBDataFile & IDBPolicy, & integrate with file IO system $end$ Make subclasses of IDBDataFile and IDBPolicy that send IO requests to StorageManager. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,4,,0,2,1,1,0,1,0,,0,850,2,0,0,2019-01-15 14:58:22,Subclass IDBDataFile & IDBPolicy,Make subclasses of IDBDataFile and IDBPolicy that send IO requests to StorageManager.,,1,0,0,8,0.368421,Subclass IDBDataFile & IDBPolicy $end$ Make subclasses of IDBDataFile and IDBPolicy that send IO requests to StorageManager. $acceptance criteria:$,1,1,1,0,0,0,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
159,MCOL-209,Task,MCOL,2016-06-28 18:48:05,MCOL-208,0,DBT3 Benchmark against InfiniDB,"ColumnStore vs InfiniDB TPCH performance benchamark using 1TB, 10TB and 100TB datasets

This should be on single node and multi-node configuration for both ColumnStore and InfiniDB.",,"DBT3 Benchmark against InfiniDB $end$ ColumnStore vs InfiniDB TPCH performance benchamark using 1TB, 10TB and 100TB datasets

This should be on single node and multi-node configuration for both ColumnStore and InfiniDB. $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,21,,0,2,2,1,0,2,0,,0,850,2,1,0,2016-07-05 20:30:56,DBT3 Benchmark against InfiniDB,"ColumnStore vs InfiniDB TPCH performance benchamark using 1TB, 10TB and 100TB datasets",,0,1,0,14,0.736842,"DBT3 Benchmark against InfiniDB $end$ ColumnStore vs InfiniDB TPCH performance benchamark using 1TB, 10TB and 100TB datasets $acceptance criteria:$",1,1,1,1,0,0,1,169.7,27,6,0.222222,1,0.037037,1,0.037037,1,0.037037,1,0.037037
160,MCOL-2097,Task,MCOL,2019-01-21 17:30:16,,0,Prepare PDI Windows 1.1.7 build for release testing,It will be put in the usual spot in our team drive.,,Prepare PDI Windows 1.1.7 build for release testing $end$ It will be put in the usual spot in our team drive. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Critical,2,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-01-21 17:30:16,Prepare PDI Windows 1.1.7 build for release testing,It will be put in the usual spot in our team drive.,,0,0,0,0,0.0,Prepare PDI Windows 1.1.7 build for release testing $end$ It will be put in the usual spot in our team drive. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,38,2,0.0526316,2,0.0526316,2,0.0526316,2,0.0526316,2,0.0526316
161,MCOL-210,Task,MCOL,2016-06-28 18:48:54,MCOL-208,0,DBT3 Benchmark against InnoDB,"ColumnStore benchmark against InnoDB using TPCH with 1TB, 10TB  datasets

ColumnStore on 1 UM, 3 PM
InnoDB on single node",,"DBT3 Benchmark against InnoDB $end$ ColumnStore benchmark against InnoDB using TPCH with 1TB, 10TB  datasets

ColumnStore on 1 UM, 3 PM
InnoDB on single node $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,23,,0,1,2,1,0,3,0,,0,850,1,1,0,2016-07-05 20:30:56,DBT3 Benchmark against InnoDB,"ColumnStore benchmark against InnoDB using TPCH with 1TB, 10TB  datasets",,0,2,0,10,0.588235,"DBT3 Benchmark against InnoDB $end$ ColumnStore benchmark against InnoDB using TPCH with 1TB, 10TB  datasets $acceptance criteria:$",2,1,1,1,0,0,1,169.7,28,7,0.25,2,0.0714286,2,0.0714286,1,0.0357143,1,0.0357143
162,MCOL-2101,New Feature,MCOL,2019-01-22 19:58:25,,0,setup tempfiles.d to prevent OS from flushing /tmp on reboot,"Customer request:

 If the OS flushes /tmp on reboot, Have the installer setup the proper tempfiles.d so the directory gets created when the system starts, to ensure the functions that use /tmp/<blah> will work when ColumnStore tries to use them.

An alternate method of having ColumnStore create the dir if it does not exist may not be sufficient as privileges must be granted for programs to be run by other than root.

Also add to docs that if running programs as other than root, one may need to add an entry to /etc/tempfiles.d/ to get the directory created with proper permissions for that user to successfully execute colxml, cpimport, etc..

Since you do not know what user a customer may run such programs as, this will likely continue to be a problem in the future.
",,"setup tempfiles.d to prevent OS from flushing /tmp on reboot $end$ Customer request:

 If the OS flushes /tmp on reboot, Have the installer setup the proper tempfiles.d so the directory gets created when the system starts, to ensure the functions that use /tmp/<blah> will work when ColumnStore tries to use them.

An alternate method of having ColumnStore create the dir if it does not exist may not be sufficient as privileges must be granted for programs to be run by other than root.

Also add to docs that if running programs as other than root, one may need to add an entry to /etc/tempfiles.d/ to get the directory created with proper permissions for that user to successfully execute colxml, cpimport, etc..

Since you do not know what user a customer may run such programs as, this will likely continue to be a problem in the future.
 $acceptance criteria:$",,David Hill,David Hill,Minor,27,,0,7,0,5,0,0,0,,0,850,5,0,0,2020-01-20 17:51:38,setup tempfiles.d to prevent OS from flushing /tmp on reboot,"Customer request:

 If the OS flushes /tmp on reboot, Have the installer setup the proper tempfiles.d so the directory gets created when the system starts, to ensure the functions that use /tmp/<blah> will work when ColumnStore tries to use them.

An alternate method of having ColumnStore create the dir if it does not exist may not be sufficient as privileges must be granted for programs to be run by other than root.

Also add to docs that if running programs as other than root, one may need to add an entry to /etc/tempfiles.d/ to get the directory created with proper permissions for that user to successfully execute colxml, cpimport, etc..

Since you do not know what user a customer may run such programs as, this will likely continue to be a problem in the future.
",,0,0,0,0,0.0,"setup tempfiles.d to prevent OS from flushing /tmp on reboot $end$ Customer request:

 If the OS flushes /tmp on reboot, Have the installer setup the proper tempfiles.d so the directory gets created when the system starts, to ensure the functions that use /tmp/<blah> will work when ColumnStore tries to use them.

An alternate method of having ColumnStore create the dir if it does not exist may not be sufficient as privileges must be granted for programs to be run by other than root.

Also add to docs that if running programs as other than root, one may need to add an entry to /etc/tempfiles.d/ to get the directory created with proper permissions for that user to successfully execute colxml, cpimport, etc..

Since you do not know what user a customer may run such programs as, this will likely continue to be a problem in the future.
 $acceptance criteria:$",0,0,0,0,0,0,1,8709.88,31,4,0.129032,0,0.0,0,0.0,0,0.0,0,0.0
163,MCOL-211,Task,MCOL,2016-06-28 18:50:33,MCOL-208,0,Resource Planning for DBT3 benchmarking,"Provide Resources needed for doing following TPCH benchmarks : using any one of the OS (CentOS7, Debian Jessie or Ubuntu Xenial)

(1) InfiniDB vs ColumnStore - 1UM , 3 PM  with 1TB, 10TB, 100 TB datasets
(2) InnoDB vs ColumnStore - 1 UM, 3 PM with 1TB, 10 TB datsets",,"Resource Planning for DBT3 benchmarking $end$ Provide Resources needed for doing following TPCH benchmarks : using any one of the OS (CentOS7, Debian Jessie or Ubuntu Xenial)

(1) InfiniDB vs ColumnStore - 1UM , 3 PM  with 1TB, 10TB, 100 TB datasets
(2) InnoDB vs ColumnStore - 1 UM, 3 PM with 1TB, 10 TB datsets $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,21,,0,1,3,2,0,2,0,,0,850,1,2,0,2016-07-05 20:30:56,Resource Planning for DBT3 benchmarking,"Provide Resources needed for doing following TPCH benchmarks : using any one of the OS (CentOS7, Debian Jessie or Ubuntu Xenial)

(1) InfiniDB vs ColumnStore - 1UM , 3 PM  with 1TB, 10TB, 100 TB datasets
(2) InnoDB vs ColumnStore - 1 UM, 3 PM with 1TB, 10 TB datsets",,0,0,0,0,0.0,"Resource Planning for DBT3 benchmarking $end$ Provide Resources needed for doing following TPCH benchmarks : using any one of the OS (CentOS7, Debian Jessie or Ubuntu Xenial)

(1) InfiniDB vs ColumnStore - 1UM , 3 PM  with 1TB, 10TB, 100 TB datasets
(2) InnoDB vs ColumnStore - 1 UM, 3 PM with 1TB, 10 TB datsets $acceptance criteria:$",0,0,0,0,0,0,1,169.667,29,8,0.275862,3,0.103448,3,0.103448,1,0.0344828,1,0.0344828
164,MCOL-2110,Task,MCOL,2019-01-25 10:50:21,,0,Cant build engine out-of-source,The server's README.md contains a description of out-of-source builds that doesn't work on the engine. This is a preferred method for testing different build configurations since they can be built independently without cleaning and rebuilding the whole project.,,Cant build engine out-of-source $end$ The server's README.md contains a description of out-of-source builds that doesn't work on the engine. This is a preferred method for testing different build configurations since they can be built independently without cleaning and rebuilding the whole project. $acceptance criteria:$,,David Mott,David Mott,Minor,9,,0,3,0,2,0,0,0,,0,850,1,0,0,2019-01-25 13:25:10,Cant build engine out-of-source,The server's README.md contains a description of out-of-source builds that doesn't work on the engine. This is a preferred method for testing different build configurations since they can be built independently without cleaning and rebuilding the whole project.,,0,0,0,0,0.0,Cant build engine out-of-source $end$ The server's README.md contains a description of out-of-source builds that doesn't work on the engine. This is a preferred method for testing different build configurations since they can be built independently without cleaning and rebuilding the whole project. $acceptance criteria:$,0,0,0,0,0,0,1,2.56667,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
165,MCOL-2115,Task,MCOL,2019-01-28 14:09:15,,0,Merge MariaDB 10.1.38 into server tree,merge this version when it is released.,,Merge MariaDB 10.1.38 into server tree $end$ merge this version when it is released. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-01-28 14:09:15,Merge MariaDB 10.1.38 into server tree,merge this version when it is released.,,0,0,0,0,0.0,Merge MariaDB 10.1.38 into server tree $end$ merge this version when it is released. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,69,5,0.0724638,3,0.0434783,2,0.0289855,1,0.0144928,1,0.0144928
166,MCOL-2120,Task,MCOL,2019-01-29 09:00:22,,0,Check NUMA devel package is installed on BuildBot instances,There is a report that the MariaDB server packages may not be detecting it and compiling with it.,,Check NUMA devel package is installed on BuildBot instances $end$ There is a report that the MariaDB server packages may not be detecting it and compiling with it. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,30,,0,4,0,2,0,0,0,,0,850,4,0,0,2019-01-29 09:00:22,Check NUMA devel package is installed on BuildBot instances,There is a report that the MariaDB server packages may not be detecting it and compiling with it.,,0,0,0,0,0.0,Check NUMA devel package is installed on BuildBot instances $end$ There is a report that the MariaDB server packages may not be detecting it and compiling with it. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,70,5,0.0714286,3,0.0428571,2,0.0285714,1,0.0142857,1,0.0142857
167,MCOL-2121,New Feature,MCOL,2019-01-29 10:26:36,MCOL-1097,0,Implement select_handler support,"As MDEV-17631 has been implemented recently it must be ported into the server to continue with convergence.
001 regression test`s queries must be supported by the handler.",,"Implement select_handler support $end$ As MDEV-17631 has been implemented recently it must be ported into the server to continue with convergence.
001 regression test`s queries must be supported by the handler. $acceptance criteria:$",,Roman,Roman,Major,12,,1,0,2,6,0,0,0,,0,850,0,0,0,2019-01-29 10:26:36,Implement select_handler support,"As MDEV-17631 has been implemented recently it must be ported into the server to continue with convergence.
001 regression test`s queries must be supported by the handler.",,0,0,0,0,0.0,"Implement select_handler support $end$ As MDEV-17631 has been implemented recently it must be ported into the server to continue with convergence.
001 regression test`s queries must be supported by the handler. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
168,MCOL-2129,New Feature,MCOL,2019-02-01 11:25:41,,0,Add a new postConfigure flag to resolve submitted hostnames to correct reverse dns names ,"With the new postConfigure option -x (MCOL-1607) to not resolve hostnames to IP addresses the replication of multi node UM setups fails, if hostnames are entered whose IP address doesn't resolve into the entered hostname via reverse dns lookup.

In this case the replication connection from the secondary um to the primary fails, as the secondary's um's wrong hostname is stated in mysql.user.

This ticket should solve the issue by introducing an optional flag to -x that translates the entered hostname in postConfigure into its reverse dns lookup hostname.",,"Add a new postConfigure flag to resolve submitted hostnames to correct reverse dns names  $end$ With the new postConfigure option -x (MCOL-1607) to not resolve hostnames to IP addresses the replication of multi node UM setups fails, if hostnames are entered whose IP address doesn't resolve into the entered hostname via reverse dns lookup.

In this case the replication connection from the secondary um to the primary fails, as the secondary's um's wrong hostname is stated in mysql.user.

This ticket should solve the issue by introducing an optional flag to -x that translates the entered hostname in postConfigure into its reverse dns lookup hostname. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,10,,2,2,2,3,0,0,0,,0,850,2,0,0,2019-02-01 15:20:58,Add a new postConfigure flag to resolve submitted hostnames to correct reverse dns names ,"With the new postConfigure option -x (MCOL-1607) to not resolve hostnames to IP addresses the replication of multi node UM setups fails, if hostnames are entered whose IP address doesn't resolve into the entered hostname via reverse dns lookup.

In this case the replication connection from the secondary um to the primary fails, as the secondary's um's wrong hostname is stated in mysql.user.

This ticket should solve the issue by introducing an optional flag to -x that translates the entered hostname in postConfigure into its reverse dns lookup hostname.",,0,0,0,0,0.0,"Add a new postConfigure flag to resolve submitted hostnames to correct reverse dns names  $end$ With the new postConfigure option -x (MCOL-1607) to not resolve hostnames to IP addresses the replication of multi node UM setups fails, if hostnames are entered whose IP address doesn't resolve into the entered hostname via reverse dns lookup.

In this case the replication connection from the secondary um to the primary fails, as the secondary's um's wrong hostname is stated in mysql.user.

This ticket should solve the issue by introducing an optional flag to -x that translates the entered hostname in postConfigure into its reverse dns lookup hostname. $acceptance criteria:$",0,0,0,0,0,0,1,3.91667,39,2,0.0512821,2,0.0512821,2,0.0512821,2,0.0512821,2,0.0512821
169,MCOL-213,Task,MCOL,2016-06-28 18:55:34,MCOL-212,0,Bulkload benchmarking against InfiniDB,"Bulkload Benchmarking against InfiniDB using 100G, 1TB, 10TB data size

(1) Using Cpimport
(2) LOAD DATA INFILE",,"Bulkload benchmarking against InfiniDB $end$ Bulkload Benchmarking against InfiniDB using 100G, 1TB, 10TB data size

(1) Using Cpimport
(2) LOAD DATA INFILE $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,21,,0,2,1,6,0,0,0,,0,850,2,0,0,2016-07-05 20:30:56,Bulkload benchmarking against InfiniDB,"Bulkload Benchmarking against InfiniDB using 100G, 1TB, 10TB data size

(1) Using Cpimport
(2) LOAD DATA INFILE",,0,0,0,0,0.0,"Bulkload benchmarking against InfiniDB $end$ Bulkload Benchmarking against InfiniDB using 100G, 1TB, 10TB data size

(1) Using Cpimport
(2) LOAD DATA INFILE $acceptance criteria:$",0,0,0,0,0,0,1,169.583,30,8,0.266667,3,0.1,3,0.1,1,0.0333333,1,0.0333333
170,MCOL-214,Task,MCOL,2016-06-28 18:56:54,MCOL-212,0,Bulkload benchmarking against InnoDB,"Bulkload Benchmarking against InnoDB using 100G, 1TB, 10TB data size
(1) cpimport vs LOAD DATA INFILE(InnoDB)
",,"Bulkload benchmarking against InnoDB $end$ Bulkload Benchmarking against InnoDB using 100G, 1TB, 10TB data size
(1) cpimport vs LOAD DATA INFILE(InnoDB)
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,22,,0,3,1,6,0,2,0,,0,850,3,2,0,2016-07-05 20:30:57,Bulkload benchmarking against InnoDB,"Bulkload Benchmarking against InnoDB using 100G, 1TB, 10TB data size
(1) cpimport vs LOAD DATA INFILE(InnoDB)
",,0,0,0,0,0.0,"Bulkload benchmarking against InnoDB $end$ Bulkload Benchmarking against InnoDB using 100G, 1TB, 10TB data size
(1) cpimport vs LOAD DATA INFILE(InnoDB)
 $acceptance criteria:$",0,0,0,0,0,0,1,169.567,31,8,0.258065,3,0.0967742,3,0.0967742,1,0.0322581,1,0.0322581
171,MCOL-2158,Task,MCOL,2019-02-11 18:09:51,,0,Merge MariaDB 10.2.22,MariaDB 10.2.22 has been released. This should be merged.,,Merge MariaDB 10.2.22 $end$ MariaDB 10.2.22 has been released. This should be merged. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-02-11 18:09:51,Merge MariaDB 10.2.22,MariaDB 10.2.22 has been released. This should be merged.,,0,0,0,0,0.0,Merge MariaDB 10.2.22 $end$ MariaDB 10.2.22 has been released. This should be merged. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,71,5,0.0704225,3,0.0422535,2,0.028169,1,0.0140845,1,0.0140845
172,MCOL-2178,New Feature,MCOL,2019-02-18 15:08:35,,0,Run CS with vanilla 10.4,"Run CS with vanila 10.4 using select_, derived_ and group_by_ pushdown handlers. 
 ",,"Run CS with vanilla 10.4 $end$ Run CS with vanila 10.4 using select_, derived_ and group_by_ pushdown handlers. 
  $acceptance criteria:$",,Roman,Roman,Major,12,,7,1,12,5,0,0,0,,0,850,1,0,0,2019-02-18 15:08:35,Run CS with vanilla 10.4,"Run CS with vanila 10.4 using select_, derived_ and group_by_ pushdown handlers. 
 ",,0,0,0,0,0.0,"Run CS with vanilla 10.4 $end$ Run CS with vanila 10.4 using select_, derived_ and group_by_ pushdown handlers. 
  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
173,MCOL-22,Task,MCOL,2016-05-03 05:40:44,,0,Build for CentOS 6.6,MariaDB ColumnStore build for CentOS 6.6 ,,Build for CentOS 6.6 $end$ MariaDB ColumnStore build for CentOS 6.6  $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,10,,0,3,0,1,0,0,0,,0,850,2,0,0,2016-05-10 19:26:02,Build for CentOS 6.6,MariaDB ColumnStore build for CentOS 6.6 ,,0,0,0,0,0.0,Build for CentOS 6.6 $end$ MariaDB ColumnStore build for CentOS 6.6  $acceptance criteria:$,0,0,0,0,0,0,0,181.75,7,3,0.428571,0,0.0,0,0.0,0,0.0,0,0.0
174,MCOL-220,Task,MCOL,2016-06-29 14:19:50,MCOL-208,0,Change Autopilot test suite to use DBT3 instead of TPCH,"Autopilot currently uses TPCH, which involves a use license, for test.  We should use DBT3 for testing instead.
",,"Change Autopilot test suite to use DBT3 instead of TPCH $end$ Autopilot currently uses TPCH, which involves a use license, for test.  We should use DBT3 for testing instead.
 $acceptance criteria:$",,Daniel Lee,Daniel Lee,Minor,7,,0,1,1,1,0,0,0,,0,850,1,0,0,2016-08-25 11:17:23,Change Autopilot test suite to use DBT3 instead of TPCH,"Autopilot currently uses TPCH, which involves a use license, for test.  We should use DBT3 for testing instead.
",,0,0,0,0,0.0,"Change Autopilot test suite to use DBT3 instead of TPCH $end$ Autopilot currently uses TPCH, which involves a use license, for test.  We should use DBT3 for testing instead.
 $acceptance criteria:$",0,0,0,0,0,0,0,1364.95,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
175,MCOL-2218,Task,MCOL,2019-03-07 06:58:07,,0,Rebase 1.2 on MariaDB 10.3.13,We need to rebase the 1.2 release on 10.3.13.,,Rebase 1.2 on MariaDB 10.3.13 $end$ We need to rebase the 1.2 release on 10.3.13. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-03-07 06:58:07,Rebase 1.2 on MariaDB 10.3.13,We need to rebase the 1.2 release on 10.3.13.,,0,0,0,0,0.0,Rebase 1.2 on MariaDB 10.3.13 $end$ We need to rebase the 1.2 release on 10.3.13. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,72,5,0.0694444,3,0.0416667,2,0.0277778,1,0.0138889,1,0.0138889
176,MCOL-23,Task,MCOL,2016-05-03 14:50:47,,0,001 Working Folder Test scripts fails,"Compare failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/misc/bug2954.sql
Compare failed - working_tpch1/misc/bug3783.sql
Compare failed - working_tpch1/misc/bug3881.sql
Compare failed - working_tpch1/misc/bug4827.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Compare failed - working_tpch1/view/view.sql
",,"001 Working Folder Test scripts fails $end$ Compare failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/misc/bug2954.sql
Compare failed - working_tpch1/misc/bug3783.sql
Compare failed - working_tpch1/misc/bug3881.sql
Compare failed - working_tpch1/misc/bug4827.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Compare failed - working_tpch1/view/view.sql
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,21,,0,7,1,1,0,1,0,,0,850,7,0,0,2016-05-10 22:51:57,001 Working Folder Test scripts fails,"Ref failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/group/q4.8.9.sql
Local failed - working_tpch1/misc/bug2954.sql
Local failed - working_tpch1/misc/bug3783.sql
Local failed - working_tpch1/misc/bug5267.sql
Local failed - working_tpch1/misc/insert_select.sql
Ref failed - working_tpch1/misc/bug2976.sql
Ref failed - working_tpch1/misc/bug3475.sql
Ref failed - working_tpch1/misc/bug3719.sql
Ref failed - working_tpch1/misc/bug3749.sql
Ref failed - working_tpch1/misc/bug3881.sql
Ref failed - working_tpch1/misc/bug3932.sql
Ref failed - working_tpch1/misc/bug3935.sql
Ref failed - working_tpch1/misc/bug3948.sql
Ref failed - working_tpch1/misc/bug4757.sql
Ref failed - working_tpch1/misc/bug4827.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/timestampdiff.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/addtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Local failed - working_tpch1/view/view.sql
Local failed - working_tpch1_calpontonly/misc/bug3513.sql
",,0,1,0,118,0.455026,"001 Working Folder Test scripts fails $end$ Ref failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/group/q4.8.9.sql
Local failed - working_tpch1/misc/bug2954.sql
Local failed - working_tpch1/misc/bug3783.sql
Local failed - working_tpch1/misc/bug5267.sql
Local failed - working_tpch1/misc/insert_select.sql
Ref failed - working_tpch1/misc/bug2976.sql
Ref failed - working_tpch1/misc/bug3475.sql
Ref failed - working_tpch1/misc/bug3719.sql
Ref failed - working_tpch1/misc/bug3749.sql
Ref failed - working_tpch1/misc/bug3881.sql
Ref failed - working_tpch1/misc/bug3932.sql
Ref failed - working_tpch1/misc/bug3935.sql
Ref failed - working_tpch1/misc/bug3948.sql
Ref failed - working_tpch1/misc/bug4757.sql
Ref failed - working_tpch1/misc/bug4827.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/timestampdiff.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/addtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Local failed - working_tpch1/view/view.sql
Local failed - working_tpch1_calpontonly/misc/bug3513.sql
 $acceptance criteria:$",1,1,1,1,1,1,1,176.017,8,3,0.375,0,0.0,0,0.0,0,0.0,0,0.0
177,MCOL-25,Task,MCOL,2016-05-03 15:33:45,,0,001 Working Folder UM Join Test Fails,"Working Folder UM Join Test Details:

Working Folder Test scripts that failed:
Ref failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/group/q4.8.9.sql
Local failed - working_tpch1/misc/bug2954.sql
Local failed - working_tpch1/misc/bug3783.sql
Local failed - working_tpch1/misc/bug5267.sql
Local failed - working_tpch1/misc/insert_select.sql
Ref failed - working_tpch1/misc/bug2976.sql
Ref failed - working_tpch1/misc/bug3475.sql
Ref failed - working_tpch1/misc/bug3719.sql
Ref failed - working_tpch1/misc/bug3749.sql
Ref failed - working_tpch1/misc/bug3881.sql
Ref failed - working_tpch1/misc/bug3932.sql
Ref failed - working_tpch1/misc/bug3935.sql
Ref failed - working_tpch1/misc/bug3948.sql
Ref failed - working_tpch1/misc/bug4757.sql
Ref failed - working_tpch1/misc/bug4827.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/timestampdiff.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/addtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Local failed - working_tpch1/view/view.sql
Local failed - working_tpch1_calpontonly/misc/bug3513.sql
",,"001 Working Folder UM Join Test Fails $end$ Working Folder UM Join Test Details:

Working Folder Test scripts that failed:
Ref failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/group/q4.8.9.sql
Local failed - working_tpch1/misc/bug2954.sql
Local failed - working_tpch1/misc/bug3783.sql
Local failed - working_tpch1/misc/bug5267.sql
Local failed - working_tpch1/misc/insert_select.sql
Ref failed - working_tpch1/misc/bug2976.sql
Ref failed - working_tpch1/misc/bug3475.sql
Ref failed - working_tpch1/misc/bug3719.sql
Ref failed - working_tpch1/misc/bug3749.sql
Ref failed - working_tpch1/misc/bug3881.sql
Ref failed - working_tpch1/misc/bug3932.sql
Ref failed - working_tpch1/misc/bug3935.sql
Ref failed - working_tpch1/misc/bug3948.sql
Ref failed - working_tpch1/misc/bug4757.sql
Ref failed - working_tpch1/misc/bug4827.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/timestampdiff.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/addtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Local failed - working_tpch1/view/view.sql
Local failed - working_tpch1_calpontonly/misc/bug3513.sql
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,17,,10,7,13,1,0,1,0,,0,850,7,0,0,2016-05-10 22:51:40,Working Folder UM Join Test Fails,"Working Folder UM Join Test Details:

Working Folder Test scripts that failed:
Ref failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/group/q4.8.9.sql
Local failed - working_tpch1/misc/bug2954.sql
Local failed - working_tpch1/misc/bug3783.sql
Local failed - working_tpch1/misc/bug5267.sql
Local failed - working_tpch1/misc/insert_select.sql
Ref failed - working_tpch1/misc/bug2976.sql
Ref failed - working_tpch1/misc/bug3475.sql
Ref failed - working_tpch1/misc/bug3719.sql
Ref failed - working_tpch1/misc/bug3749.sql
Ref failed - working_tpch1/misc/bug3881.sql
Ref failed - working_tpch1/misc/bug3932.sql
Ref failed - working_tpch1/misc/bug3935.sql
Ref failed - working_tpch1/misc/bug3948.sql
Ref failed - working_tpch1/misc/bug4757.sql
Ref failed - working_tpch1/misc/bug4827.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/timestampdiff.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/addtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Local failed - working_tpch1/view/view.sql
Local failed - working_tpch1_calpontonly/misc/bug3513.sql
",,1,0,0,1,0.00497512,"Working Folder UM Join Test Fails $end$ Working Folder UM Join Test Details:

Working Folder Test scripts that failed:
Ref failed - working_tpch1/aggregation/group_concat.sql
Ref failed - working_tpch1/group/q4.8.9.sql
Local failed - working_tpch1/misc/bug2954.sql
Local failed - working_tpch1/misc/bug3783.sql
Local failed - working_tpch1/misc/bug5267.sql
Local failed - working_tpch1/misc/insert_select.sql
Ref failed - working_tpch1/misc/bug2976.sql
Ref failed - working_tpch1/misc/bug3475.sql
Ref failed - working_tpch1/misc/bug3719.sql
Ref failed - working_tpch1/misc/bug3749.sql
Ref failed - working_tpch1/misc/bug3881.sql
Ref failed - working_tpch1/misc/bug3932.sql
Ref failed - working_tpch1/misc/bug3935.sql
Ref failed - working_tpch1/misc/bug3948.sql
Ref failed - working_tpch1/misc/bug4757.sql
Ref failed - working_tpch1/misc/bug4827.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE1.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/CASE2.SM.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/DAYNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql
Local failed - working_tpch1/qa_fe_cnxFunctions/timestampdiff.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/addtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_days.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/insert.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql
Ref failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql
Ref failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql
Local failed - working_tpch1/view/view.sql
Local failed - working_tpch1_calpontonly/misc/bug3513.sql
 $acceptance criteria:$",1,1,0,0,0,0,0,175.283,9,4,0.444444,1,0.111111,1,0.111111,1,0.111111,1,0.111111
178,MCOL-251,New Feature,MCOL,2016-07-07 16:56:11,,0,does columnstore need to issue snmp traps?,"The current version of MariaDB Columnstore issues snmptraps, which is used to inform remote control centers of an issue with the system. This is there because it was in InfiniDB. It was in InfiniDB when the software design was to be running on a dedicated appliance. 
When InfiniDB was converted to be a software only product, this functionality remained.

The question is, does this functionality of being able to generate snmptraps from columnstore need to be kept?

1 item of note, the issuing of local alarms is tied to issuing for a trap. So if we decide to remove the snmptrap functionality, the logic of sending alarms will need to be changed.

Also removing the snmptrap functionality altogether will simple our source tree and build process.",,"does columnstore need to issue snmp traps? $end$ The current version of MariaDB Columnstore issues snmptraps, which is used to inform remote control centers of an issue with the system. This is there because it was in InfiniDB. It was in InfiniDB when the software design was to be running on a dedicated appliance. 
When InfiniDB was converted to be a software only product, this functionality remained.

The question is, does this functionality of being able to generate snmptraps from columnstore need to be kept?

1 item of note, the issuing of local alarms is tied to issuing for a trap. So if we decide to remove the snmptrap functionality, the logic of sending alarms will need to be changed.

Also removing the snmptrap functionality altogether will simple our source tree and build process. $acceptance criteria:$",,David Hill,David Hill,Minor,17,,0,7,1,3,0,0,0,,0,850,6,0,0,2016-08-25 11:16:04,does columnstore need to issue snmp traps?,"The current version of MariaDB Columnstore issues snmptraps, which is used to inform remote control centers of an issue with the system. This is there because it was in InfiniDB. It was in InfiniDB when the software design was to be running on a dedicated appliance. 
When InfiniDB was converted to be a software only product, this functionality remained.

The question is, does this functionality of being able to generate snmptraps from columnstore need to be kept?

1 item of note, the issuing of local alarms is tied to issuing for a trap. So if we decide to remove the snmptrap functionality, the logic of sending alarms will need to be changed.

Also removing the snmptrap functionality altogether will simple our source tree and build process.",,0,0,0,0,0.0,"does columnstore need to issue snmp traps? $end$ The current version of MariaDB Columnstore issues snmptraps, which is used to inform remote control centers of an issue with the system. This is there because it was in InfiniDB. It was in InfiniDB when the software design was to be running on a dedicated appliance. 
When InfiniDB was converted to be a software only product, this functionality remained.

The question is, does this functionality of being able to generate snmptraps from columnstore need to be kept?

1 item of note, the issuing of local alarms is tied to issuing for a trap. So if we decide to remove the snmptrap functionality, the logic of sending alarms will need to be changed.

Also removing the snmptrap functionality altogether will simple our source tree and build process. $acceptance criteria:$",0,0,0,0,0,0,1,1170.32,6,1,0.166667,0,0.0,0,0.0,0,0.0,0,0.0
179,MCOL-262,Task,MCOL,2016-08-11 17:05:41,,0,cmake for building engine RPMs,still running build_rpm script should move to using cmake/cpack to build rpms similar to server build,,cmake for building engine RPMs $end$ still running build_rpm script should move to using cmake/cpack to build rpms similar to server build $acceptance criteria:$,,Ben Thompson,Ben Thompson,Minor,41,,0,7,2,3,0,0,0,,0,850,7,0,0,2016-08-25 11:15:18,cmake for building engine RPMs,still running build_rpm script should move to using cmake/cpack to build rpms similar to server build,,0,0,0,0,0.0,cmake for building engine RPMs $end$ still running build_rpm script should move to using cmake/cpack to build rpms similar to server build $acceptance criteria:$,0,0,0,0,0,0,1,330.15,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
180,MCOL-265,New Feature,MCOL,2016-08-18 12:05:15,MCOL-1049,0,TIMESTAMP data type not supported,DATETIME data type appears to work whereas any CREATE TABLE with a TIMESTAMP type throws an error,,TIMESTAMP data type not supported $end$ DATETIME data type appears to work whereas any CREATE TABLE with a TIMESTAMP type throws an error $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Minor,17,,2,3,3,3,0,0,0,,0,850,3,0,0,2019-04-25 10:20:06,TIMESTAMP data type not supported,DATETIME data type appears to work whereas any CREATE TABLE with a TIMESTAMP type throws an error,,0,0,0,0,0.0,TIMESTAMP data type not supported $end$ DATETIME data type appears to work whereas any CREATE TABLE with a TIMESTAMP type throws an error $acceptance criteria:$,0,0,0,0,0,0,1,23518.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
181,MCOL-266,New Feature,MCOL,2016-08-18 12:11:13,MCOL-1049,0,BOOLEAN data type not supported,BOOLEAN data type is not currently supported,,BOOLEAN data type not supported $end$ BOOLEAN data type is not currently supported $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Minor,17,,0,5,0,1,0,0,0,,0,850,4,0,0,2018-10-08 06:51:41,BOOLEAN data type not supported,BOOLEAN data type is not currently supported,,0,0,0,0,0.0,BOOLEAN data type not supported $end$ BOOLEAN data type is not currently supported $acceptance criteria:$,0,0,0,0,0,0,0,18738.7,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
182,MCOL-267,New Feature,MCOL,2016-08-18 12:17:47,,0,TEXT and BLOB data types are not supported,Columnstore throws an error for the TEXT data type.,,TEXT and BLOB data types are not supported $end$ Columnstore throws an error for the TEXT data type. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Minor,18,,9,5,14,8,0,1,0,,0,850,3,0,0,2017-01-23 23:26:02,TEXT data type not supported,Columnstore throws an error for the TEXT data type.,,1,0,0,5,0.235294,TEXT data type not supported $end$ Columnstore throws an error for the TEXT data type. $acceptance criteria:$,1,1,1,0,0,0,1,3803.13,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
183,MCOL-270,New Feature,MCOL,2016-08-18 12:34:17,MCOL-1049,0,MEDIUMINT data type not supported,Not supported in Columnstore,,MEDIUMINT data type not supported $end$ Not supported in Columnstore $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Minor,12,,0,2,0,1,0,0,0,,0,850,1,0,0,2019-08-16 12:58:26,MEDIUMINT data type not supported,Not supported in Columnstore,,0,0,0,0,0.0,MEDIUMINT data type not supported $end$ Not supported in Columnstore $acceptance criteria:$,0,0,0,0,0,0,0,26232.4,3,1,0.333333,1,0.333333,0,0.0,0,0.0,0,0.0
184,MCOL-271,New Feature,MCOL,2016-08-18 12:55:53,MCOL-1049,0,Improved support for NULL for Varchar/char/text/datetime/timestamp + ?,"I suspect this is related to MCOL-171.

MariaDB [sakila]> create table t2 (a varchar(255)) engine=columnstore;Query OK, 0 rows affected (0.61 sec)

MariaDB [sakila]> insert into t2 values ('');
Query OK, 1 row affected (0.26 sec)

MariaDB [sakila]> select * from t2;
+------+                                           
| a    |
+------+
| NULL |
+------+
1 row in set (0.10 sec)


MariaDB [sakila]> create table t1 (a varchar(255) not null) engine=columnstore;
Query OK, 0 rows affected (0.53 sec)

MariaDB [sakila]> insert into t1 values ('');
ERROR 1815 (HY000): Internal error: CAL0001: Insert Failed:  IDB-4015: Column 'a' cannot be null.  

--------------------------
external effects on NULL where clause  used in appications

https://docs.google.com/document/d/1dZ6jlp3_tZ0NPQp9LXG0sFVvrMvJOKGTNwwVy80SCws/edit

",,"Improved support for NULL for Varchar/char/text/datetime/timestamp + ? $end$ I suspect this is related to MCOL-171.

MariaDB [sakila]> create table t2 (a varchar(255)) engine=columnstore;Query OK, 0 rows affected (0.61 sec)

MariaDB [sakila]> insert into t2 values ('');
Query OK, 1 row affected (0.26 sec)

MariaDB [sakila]> select * from t2;
+------+                                           
| a    |
+------+
| NULL |
+------+
1 row in set (0.10 sec)


MariaDB [sakila]> create table t1 (a varchar(255) not null) engine=columnstore;
Query OK, 0 rows affected (0.53 sec)

MariaDB [sakila]> insert into t1 values ('');
ERROR 1815 (HY000): Internal error: CAL0001: Insert Failed:  IDB-4015: Column 'a' cannot be null.  

--------------------------
external effects on NULL where clause  used in appications

https://docs.google.com/document/d/1dZ6jlp3_tZ0NPQp9LXG0sFVvrMvJOKGTNwwVy80SCws/edit

 $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Blocker,64,,6,14,8,6,0,4,0,,0,850,10,0,0,2016-08-25 11:15:31,Empty varchar text gets set to NULL,"I suspect this is related to MCOL-171.

MariaDB [sakila]> create table t2 (a varchar(255)) engine=columnstore;Query OK, 0 rows affected (0.61 sec)

MariaDB [sakila]> insert into t2 values ('');
Query OK, 1 row affected (0.26 sec)

MariaDB [sakila]> select * from t2;
+------+                                           
| a    |
+------+
| NULL |
+------+
1 row in set (0.10 sec)


MariaDB [sakila]> create table t1 (a varchar(255) not null) engine=columnstore;
Query OK, 0 rows affected (0.53 sec)

MariaDB [sakila]> insert into t1 values ('');
ERROR 1815 (HY000): Internal error: CAL0001: Insert Failed:  IDB-4015: Column 'a' cannot be null.  

",,3,1,0,24,0.182692,"Empty varchar text gets set to NULL $end$ I suspect this is related to MCOL-171.

MariaDB [sakila]> create table t2 (a varchar(255)) engine=columnstore;Query OK, 0 rows affected (0.61 sec)

MariaDB [sakila]> insert into t2 values ('');
Query OK, 1 row affected (0.26 sec)

MariaDB [sakila]> select * from t2;
+------+                                           
| a    |
+------+
| NULL |
+------+
1 row in set (0.10 sec)


MariaDB [sakila]> create table t1 (a varchar(255) not null) engine=columnstore;
Query OK, 0 rows affected (0.53 sec)

MariaDB [sakila]> insert into t1 values ('');
ERROR 1815 (HY000): Internal error: CAL0001: Insert Failed:  IDB-4015: Column 'a' cannot be null.  

 $acceptance criteria:$",4,1,1,1,1,1,1,166.317,4,1,0.25,1,0.25,0,0.0,0,0.0,0,0.0
185,MCOL-272,New Feature,MCOL,2016-08-18 14:30:18,,0,generate debian packages,"for 1.0.2, we generated binary packages for ubuntu 16.04. We need to be able to generate debian packages...",,"generate debian packages $end$ for 1.0.2, we generated binary packages for ubuntu 16.04. We need to be able to generate debian packages... $acceptance criteria:$",,David Hill,David Hill,Critical,18,,0,5,0,4,0,1,0,,0,850,5,0,0,2016-10-25 18:19:12,generate ubuntu 16.04 debian packages,"for 1.0.2, we generated binary packages for ubuntu 16.04. We need to be able to generate debian packages...",,1,0,0,2,0.0769231,"generate ubuntu 16.04 debian packages $end$ for 1.0.2, we generated binary packages for ubuntu 16.04. We need to be able to generate debian packages... $acceptance criteria:$",1,1,0,0,0,0,1,1635.8,7,1,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
186,MCOL-274,New Feature,MCOL,2016-08-25 07:31:14,,0,DATETIME field doesn't match with MySQL standard,"This isn't a dealbreaker, by any means -- but I'm curious why ColumnStore restricts the range by 400 years?

ColumnStore supports:
{quote}
A date and time combination. Supported range is 1400-01-01 00:00:00 to 9999-12-31 23:59:59.
{quote}


That differs from MySQL 5.5+ which supports
{quote}
The DATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in 'YYYY-MM-DD HH:MM:SS' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'.
{quote}


ColumnStore reports (when attempting a value out of range):
{code}
ERROR 1815 (HY000) at line 306: Internal error: The default value is out of range for the specified data type.
{code}
",,"DATETIME field doesn't match with MySQL standard $end$ This isn't a dealbreaker, by any means -- but I'm curious why ColumnStore restricts the range by 400 years?

ColumnStore supports:
{quote}
A date and time combination. Supported range is 1400-01-01 00:00:00 to 9999-12-31 23:59:59.
{quote}


That differs from MySQL 5.5+ which supports
{quote}
The DATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in 'YYYY-MM-DD HH:MM:SS' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'.
{quote}


ColumnStore reports (when attempting a value out of range):
{code}
ERROR 1815 (HY000) at line 306: Internal error: The default value is out of range for the specified data type.
{code}
 $acceptance criteria:$",,Andrew Ernst,Andrew Ernst,Minor,8,,1,8,2,1,0,0,0,,0,850,7,0,0,2016-08-25 16:23:13,DATETIME field doesn't match with MySQL standard,"This isn't a dealbreaker, by any means -- but I'm curious why ColumnStore restricts the range by 400 years?

ColumnStore supports:
{quote}
A date and time combination. Supported range is 1400-01-01 00:00:00 to 9999-12-31 23:59:59.
{quote}


That differs from MySQL 5.5+ which supports
{quote}
The DATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in 'YYYY-MM-DD HH:MM:SS' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'.
{quote}


ColumnStore reports (when attempting a value out of range):
{code}
ERROR 1815 (HY000) at line 306: Internal error: The default value is out of range for the specified data type.
{code}
",,0,0,0,0,0.0,"DATETIME field doesn't match with MySQL standard $end$ This isn't a dealbreaker, by any means -- but I'm curious why ColumnStore restricts the range by 400 years?

ColumnStore supports:
{quote}
A date and time combination. Supported range is 1400-01-01 00:00:00 to 9999-12-31 23:59:59.
{quote}


That differs from MySQL 5.5+ which supports
{quote}
The DATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in 'YYYY-MM-DD HH:MM:SS' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'.
{quote}


ColumnStore reports (when attempting a value out of range):
{code}
ERROR 1815 (HY000) at line 306: Internal error: The default value is out of range for the specified data type.
{code}
 $acceptance criteria:$",0,0,0,0,0,0,0,8.85,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
187,MCOL-28,Task,MCOL,2016-05-03 15:36:41,,0,006 Count while loading fails,"006 Count while loading:              Failed (loads=100, rowsPerLoad=25000, queries=4394, errors=0, incorrectQueries=1277)",,"006 Count while loading fails $end$ 006 Count while loading:              Failed (loads=100, rowsPerLoad=25000, queries=4394, errors=0, incorrectQueries=1277) $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-05-10 20:21:13,006 Count while loading fails,"006 Count while loading:              Failed (loads=100, rowsPerLoad=25000, queries=4394, errors=0, incorrectQueries=1277)",,0,0,0,0,0.0,"006 Count while loading fails $end$ 006 Count while loading:              Failed (loads=100, rowsPerLoad=25000, queries=4394, errors=0, incorrectQueries=1277) $acceptance criteria:$",0,0,0,0,0,0,0,172.733,10,5,0.5,1,0.1,1,0.1,1,0.1,1,0.1
188,MCOL-29,Task,MCOL,2016-05-03 15:37:11,,0,011 cpimport feature test fails,"011 cpimport Features Test:           Failed (check /root/genii/mysql/queries/nightly/alltest/test011.log)
",,"011 cpimport feature test fails $end$ 011 cpimport Features Test:           Failed (check /root/genii/mysql/queries/nightly/alltest/test011.log)
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,4,,0,5,0,1,0,0,0,,0,850,0,0,0,2016-05-23 07:37:30,011 cpimport feature test fails,"011 cpimport Features Test:           Failed (check /root/genii/mysql/queries/nightly/alltest/test011.log)
",,0,0,0,0,0.0,"011 cpimport feature test fails $end$ 011 cpimport Features Test:           Failed (check /root/genii/mysql/queries/nightly/alltest/test011.log)
 $acceptance criteria:$",0,0,0,0,0,0,0,472.0,11,5,0.454545,1,0.0909091,1,0.0909091,1,0.0909091,1,0.0909091
189,MCOL-294,New Feature,MCOL,2016-09-13 12:36:32,,0,Fix jemalloc implementation,"We should do the following:

1. Update the jemalloc directory to have the source of the latest version (remove the tarball and .so file from there)
2. Update the build scripts to link jemalloc against all the binaries to replace the default allocator

This should improve some of the performance issues we have seen with 1.0.2 onwards.",,"Fix jemalloc implementation $end$ We should do the following:

1. Update the jemalloc directory to have the source of the latest version (remove the tarball and .so file from there)
2. Update the build scripts to link jemalloc against all the binaries to replace the default allocator

This should improve some of the performance issues we have seen with 1.0.2 onwards. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Critical,24,,0,8,1,3,0,0,0,,0,850,8,0,0,2016-09-13 19:11:19,Fix jemalloc implementation,"We should do the following:

1. Update the jemalloc directory to have the source of the latest version (remove the tarball and .so file from there)
2. Update the build scripts to link jemalloc against all the binaries to replace the default allocator

This should improve some of the performance issues we have seen with 1.0.2 onwards.",,0,0,0,0,0.0,"Fix jemalloc implementation $end$ We should do the following:

1. Update the jemalloc directory to have the source of the latest version (remove the tarball and .so file from there)
2. Update the build scripts to link jemalloc against all the binaries to replace the default allocator

This should improve some of the performance issues we have seen with 1.0.2 onwards. $acceptance criteria:$",0,0,0,0,0,0,1,6.56667,5,2,0.4,2,0.4,1,0.2,1,0.2,1,0.2
190,MCOL-297,Task,MCOL,2016-09-14 18:06:56,,0,CHARACTER_LENGTH(datetime) returns wrong length,"A date time returns a string similar to ""1997-01-01 00:00:00"", which has 19 characters, yet the function CHARACTER_LENGTH when passed a date-time object returns 7 fewer. This is because of the following specific code in the function:

			//adjust for microseconds not counted
			return (int64_t)date.size() - 7;

Not sure why this is there as microseconds have nothing to do with it.",,"CHARACTER_LENGTH(datetime) returns wrong length $end$ A date time returns a string similar to ""1997-01-01 00:00:00"", which has 19 characters, yet the function CHARACTER_LENGTH when passed a date-time object returns 7 fewer. This is because of the following specific code in the function:

			//adjust for microseconds not counted
			return (int64_t)date.size() - 7;

Not sure why this is there as microseconds have nothing to do with it. $acceptance criteria:$",,David Hall,David Hall,Trivial,14,,0,2,1,1,0,0,0,,0,850,1,0,0,2016-09-21 16:03:12,CHARACTER_LENGTH(datetime) returns wrong length,"A date time returns a string similar to ""1997-01-01 00:00:00"", which has 19 characters, yet the function CHARACTER_LENGTH when passed a date-time object returns 7 fewer. This is because of the following specific code in the function:

			//adjust for microseconds not counted
			return (int64_t)date.size() - 7;

Not sure why this is there as microseconds have nothing to do with it.",,0,0,0,0,0.0,"CHARACTER_LENGTH(datetime) returns wrong length $end$ A date time returns a string similar to ""1997-01-01 00:00:00"", which has 19 characters, yet the function CHARACTER_LENGTH when passed a date-time object returns 7 fewer. This is because of the following specific code in the function:

			//adjust for microseconds not counted
			return (int64_t)date.size() - 7;

Not sure why this is there as microseconds have nothing to do with it. $acceptance criteria:$",0,0,0,0,0,0,0,165.933,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
191,MCOL-3,Task,MCOL,2016-05-02 16:42:28,,0,Change directories and paths to use new mariadb columnstore ,"(1) Where ever we are using calpont in directory paths for data, install , library, scripts, config files - it needs to be changed to  mariadb/columnstore

(2) Whereever we are using infinidb in directory paths for data, install , library, scripts, config files - it needs to be changed to columnstore",,"Change directories and paths to use new mariadb columnstore  $end$ (1) Where ever we are using calpont in directory paths for data, install , library, scripts, config files - it needs to be changed to  mariadb/columnstore

(2) Whereever we are using infinidb in directory paths for data, install , library, scripts, config files - it needs to be changed to columnstore $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,19,,0,6,0,1,0,2,0,,0,850,6,1,0,2016-05-04 00:43:26,Change directories and paths to use new mariadb columnstore ,"(1) Where ever we are using calpont in directory paths for data, install , library, scripts, config files - it needs to be changed to  mariadb-columnstore or mdbcolumn

(2) Whereever we are using infinidb in directory paths for data, install , library, scripts, config files - it needs to be changed to columnstore",,0,1,0,4,0.0461538,"Change directories and paths to use new mariadb columnstore  $end$ (1) Where ever we are using calpont in directory paths for data, install , library, scripts, config files - it needs to be changed to  mariadb-columnstore or mdbcolumn

(2) Whereever we are using infinidb in directory paths for data, install , library, scripts, config files - it needs to be changed to columnstore $acceptance criteria:$",1,1,0,0,0,0,0,32.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
192,MCOL-30,Task,MCOL,2016-05-03 15:37:23,,0,012 Varbinary test fails,012 Varbinary Test:                   Failed (Check logs/diff.txt for diffs),,012 Varbinary test fails $end$ 012 Varbinary Test:                   Failed (Check logs/diff.txt for diffs) $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,5,,0,4,0,1,0,0,0,,0,850,4,0,0,2016-05-10 22:54:47,012 Varbinary test fails,012 Varbinary Test:                   Failed (Check logs/diff.txt for diffs),,0,0,0,0,0.0,012 Varbinary test fails $end$ 012 Varbinary Test:                   Failed (Check logs/diff.txt for diffs) $acceptance criteria:$,0,0,0,0,0,0,0,175.283,12,5,0.416667,1,0.0833333,1,0.0833333,1,0.0833333,1,0.0833333
193,MCOL-304,Task,MCOL,2016-09-20 21:35:04,,0,MariaDB ColumnStore Package Repository,"Create appropriate yum, apt, zypper repository.
 Provide instructions to configure the repository access and installation from repository

See Example of how it is done for enterprise pacakge: https://mariadb.com/my_portal/download/10.1",,"MariaDB ColumnStore Package Repository $end$ Create appropriate yum, apt, zypper repository.
 Provide instructions to configure the repository access and installation from repository

See Example of how it is done for enterprise pacakge: https://mariadb.com/my_portal/download/10.1 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,49,,2,21,2,32,0,1,0,,0,850,21,1,0,2016-11-22 19:34:12,MariaDB ColumnStore Package Repository,"Create appropriate yum, apt, zypper repository.
 Provide instructions to configure the repository access and installation from repository

See Example of how it is done for enterprise pacakge: https://mariadb.com/my_portal/download/10.1",,0,0,0,0,0.0,"MariaDB ColumnStore Package Repository $end$ Create appropriate yum, apt, zypper repository.
 Provide instructions to configure the repository access and installation from repository

See Example of how it is done for enterprise pacakge: https://mariadb.com/my_portal/download/10.1 $acceptance criteria:$",0,0,0,0,0,0,1,1509.98,32,8,0.25,3,0.09375,3,0.09375,1,0.03125,1,0.03125
194,MCOL-305,Task,MCOL,2016-09-21 15:59:47,,0,MariaDB Server 10.1.18 Merge,,,MariaDB Server 10.1.18 Merge $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,9,,0,3,1,1,0,0,0,,0,850,3,0,0,2016-09-21 16:00:48,MariaDB Server 10.1.18 Merge,,,0,0,0,0,0.0,MariaDB Server 10.1.18 Merge $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0166667,33,8,0.242424,3,0.0909091,3,0.0909091,1,0.030303,1,0.030303
195,MCOL-307,New Feature,MCOL,2016-09-22 19:05:47,,0,implement redistribution logic,A data rebalancing function is needed to support redistribution of data between nodes. This should be surfaced as a command in mcsadmin.  This should check for writes being disabled before proceeding. Suspend write can be run to perform this.,,implement redistribution logic $end$ A data rebalancing function is needed to support redistribution of data between nodes. This should be surfaced as a command in mcsadmin.  This should check for writes being disabled before proceeding. Suspend write can be run to perform this. $acceptance criteria:$,,David Thompson,David Thompson,Critical,29,,0,13,0,4,0,0,0,,0,850,13,0,0,2016-10-25 18:16:09,implement redistribution logic,A data rebalancing function is needed to support redistribution of data between nodes. This should be surfaced as a command in mcsadmin.  This should check for writes being disabled before proceeding. Suspend write can be run to perform this.,,0,0,0,0,0.0,implement redistribution logic $end$ A data rebalancing function is needed to support redistribution of data between nodes. This should be surfaced as a command in mcsadmin.  This should check for writes being disabled before proceeding. Suspend write can be run to perform this. $acceptance criteria:$,0,0,0,0,0,0,1,791.167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
196,MCOL-309,New Feature,MCOL,2016-09-23 16:46:44,,0,Support method to report on data set size,"There should be a means to determine storage actually used by columnstore by:
- all
- schema
- schema and table 

This should report by:
- schema
- tablename,
- columnName 
- dataType
- columnWidth
- dbroot
- partition
- segment
- filename
 -size (in GB compressed).
- Uncompressed size would be desirable if possible too.


Information_Schema tables (Table and Column etc) likely would be the best and standard ways to support this, but not all of this information may be representable in that form. What can be populated in infoschema should be done and then we can review an approach for most easily capturing the remaining attributes as either a proc, script, or mcsadmin command.",,"Support method to report on data set size $end$ There should be a means to determine storage actually used by columnstore by:
- all
- schema
- schema and table 

This should report by:
- schema
- tablename,
- columnName 
- dataType
- columnWidth
- dbroot
- partition
- segment
- filename
 -size (in GB compressed).
- Uncompressed size would be desirable if possible too.


Information_Schema tables (Table and Column etc) likely would be the best and standard ways to support this, but not all of this information may be representable in that form. What can be populated in infoschema should be done and then we can review an approach for most easily capturing the remaining attributes as either a proc, script, or mcsadmin command. $acceptance criteria:$",,David Thompson,David Thompson,Critical,16,,0,5,1,3,0,0,0,,0,850,5,0,0,2016-10-25 18:16:05,Support method to report on data set size,"There should be a means to determine storage actually used by columnstore by:
- all
- schema
- schema and table 

This should report by:
- schema
- tablename,
- columnName 
- dataType
- columnWidth
- dbroot
- partition
- segment
- filename
 -size (in GB compressed).
- Uncompressed size would be desirable if possible too.


Information_Schema tables (Table and Column etc) likely would be the best and standard ways to support this, but not all of this information may be representable in that form. What can be populated in infoschema should be done and then we can review an approach for most easily capturing the remaining attributes as either a proc, script, or mcsadmin command.",,0,0,0,0,0.0,"Support method to report on data set size $end$ There should be a means to determine storage actually used by columnstore by:
- all
- schema
- schema and table 

This should report by:
- schema
- tablename,
- columnName 
- dataType
- columnWidth
- dbroot
- partition
- segment
- filename
 -size (in GB compressed).
- Uncompressed size would be desirable if possible too.


Information_Schema tables (Table and Column etc) likely would be the best and standard ways to support this, but not all of this information may be representable in that form. What can be populated in infoschema should be done and then we can review an approach for most easily capturing the remaining attributes as either a proc, script, or mcsadmin command. $acceptance criteria:$",0,0,0,0,0,0,1,769.483,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
197,MCOL-311,New Feature,MCOL,2016-09-23 17:03:29,,0,utility for finding objects file,"A utility should provide a means of determing the directory for the first file of a given object. This is useful for troubleshooting.

Arguments:
- oid / object id
- verbose flag for more details",,"utility for finding objects file $end$ A utility should provide a means of determing the directory for the first file of a given object. This is useful for troubleshooting.

Arguments:
- oid / object id
- verbose flag for more details $acceptance criteria:$",,David Thompson,David Thompson,Critical,33,,0,14,0,4,0,0,0,,0,850,14,0,0,2016-10-25 18:16:02,utility for finding objects file,"A utility should provide a means of determing the directory for the first file of a given object. This is useful for troubleshooting.

Arguments:
- oid / object id
- verbose flag for more details",,0,0,0,0,0.0,"utility for finding objects file $end$ A utility should provide a means of determing the directory for the first file of a given object. This is useful for troubleshooting.

Arguments:
- oid / object id
- verbose flag for more details $acceptance criteria:$",0,0,0,0,0,0,1,769.2,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
198,MCOL-312,New Feature,MCOL,2016-09-23 17:16:20,MCOL-3521,0,utility to rebuild extent maps,"A utility should provide the means to rebuild the extent map data for a column from the data. This would be useful should there be bugs in this logic and would avoid the need for rebuilding the table from scratch.

The utility should check for and fail if writes are not suspended. It should provide options to rebuild extent maps for a single column and also for multiple (table, etc).",,"utility to rebuild extent maps $end$ A utility should provide the means to rebuild the extent map data for a column from the data. This would be useful should there be bugs in this logic and would avoid the need for rebuilding the table from scratch.

The utility should check for and fail if writes are not suspended. It should provide options to rebuild extent maps for a single column and also for multiple (table, etc). $acceptance criteria:$",,David Thompson,David Thompson,Critical,39,,0,7,1,5,0,0,0,,0,850,5,0,0,2021-02-05 20:43:23,utility to rebuild extent maps,"A utility should provide the means to rebuild the extent map data for a column from the data. This would be useful should there be bugs in this logic and would avoid the need for rebuilding the table from scratch.

The utility should check for and fail if writes are not suspended. It should provide options to rebuild extent maps for a single column and also for multiple (table, etc).",,0,0,0,0,0.0,"utility to rebuild extent maps $end$ A utility should provide the means to rebuild the extent map data for a column from the data. This would be useful should there be bugs in this logic and would avoid the need for rebuilding the table from scratch.

The utility should check for and fail if writes are not suspended. It should provide options to rebuild extent maps for a single column and also for multiple (table, etc). $acceptance criteria:$",0,0,0,0,0,0,1,38307.4,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
199,MCOL-3242,Task,MCOL,2019-04-09 21:57:22,,0,Improve load/save in BRM structs,"Hacking up the BRM structs to make them use IDBDataFile consistently, I noticed that the EM, VBBM, and VSS are still using my v 0.1 prototype code for loading and saving.  That code, written like 12 years ago, loads and saves elements one by one.  That won't scale, and it'll cause noticeable pauses across the system while it does that.  This is another one of those things I had to fix at Tune (saving a 600MB extent map ~50 bytes at a time = pain).

This task is for making them save consecutive groups of elements at a time.  Much much quicker.

One other thing I noticed is that it's taking a full snapshot of all of the structs on commit.  That's expensive, esp with the current impl, and it doesn't need to do that.",,"Improve load/save in BRM structs $end$ Hacking up the BRM structs to make them use IDBDataFile consistently, I noticed that the EM, VBBM, and VSS are still using my v 0.1 prototype code for loading and saving.  That code, written like 12 years ago, loads and saves elements one by one.  That won't scale, and it'll cause noticeable pauses across the system while it does that.  This is another one of those things I had to fix at Tune (saving a 600MB extent map ~50 bytes at a time = pain).

This task is for making them save consecutive groups of elements at a time.  Much much quicker.

One other thing I noticed is that it's taking a full snapshot of all of the structs on commit.  That's expensive, esp with the current impl, and it doesn't need to do that. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Minor,13,,0,6,0,1,0,0,0,,0,850,3,0,0,2019-06-17 13:28:25,Improve load/save in BRM structs,"Hacking up the BRM structs to make them use IDBDataFile consistently, I noticed that the EM, VBBM, and VSS are still using my v 0.1 prototype code for loading and saving.  That code, written like 12 years ago, loads and saves elements one by one.  That won't scale, and it'll cause noticeable pauses across the system while it does that.  This is another one of those things I had to fix at Tune (saving a 600MB extent map ~50 bytes at a time = pain).

This task is for making them save consecutive groups of elements at a time.  Much much quicker.

One other thing I noticed is that it's taking a full snapshot of all of the structs on commit.  That's expensive, esp with the current impl, and it doesn't need to do that.",,0,0,0,0,0.0,"Improve load/save in BRM structs $end$ Hacking up the BRM structs to make them use IDBDataFile consistently, I noticed that the EM, VBBM, and VSS are still using my v 0.1 prototype code for loading and saving.  That code, written like 12 years ago, loads and saves elements one by one.  That won't scale, and it'll cause noticeable pauses across the system while it does that.  This is another one of those things I had to fix at Tune (saving a 600MB extent map ~50 bytes at a time = pain).

This task is for making them save consecutive groups of elements at a time.  Much much quicker.

One other thing I noticed is that it's taking a full snapshot of all of the structs on commit.  That's expensive, esp with the current impl, and it doesn't need to do that. $acceptance criteria:$",0,0,0,0,0,0,0,1647.52,1,1,1.0,1,1.0,0,0.0,0,0.0,0,0.0
200,MCOL-325,New Feature,MCOL,2016-09-26 19:18:03,,0,WEEK() handling needs to match MySQL's,"Causing many working_tpch1 failures:
InfiniDB's week() function (and related functions) have different start day of week rules to MariaDB by default, this is documented as such in the code. It is causing several test failures in 001. It should be changed to be in-line with MariaDB's default.",,"WEEK() handling needs to match MySQL's $end$ Causing many working_tpch1 failures:
InfiniDB's week() function (and related functions) have different start day of week rules to MariaDB by default, this is documented as such in the code. It is causing several test failures in 001. It should be changed to be in-line with MariaDB's default. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,11,,0,2,1,1,0,0,0,,0,850,2,0,0,2016-09-26 20:09:25,WEEK() handling needs to match MySQL's,"Causing many working_tpch1 failures:
InfiniDB's week() function (and related functions) have different start day of week rules to MariaDB by default, this is documented as such in the code. It is causing several test failures in 001. It should be changed to be in-line with MariaDB's default.",,0,0,0,0,0.0,"WEEK() handling needs to match MySQL's $end$ Causing many working_tpch1 failures:
InfiniDB's week() function (and related functions) have different start day of week rules to MariaDB by default, this is documented as such in the code. It is causing several test failures in 001. It should be changed to be in-line with MariaDB's default. $acceptance criteria:$",0,0,0,0,0,0,0,0.85,6,2,0.333333,2,0.333333,1,0.166667,1,0.166667,1,0.166667
201,MCOL-3267,New Feature,MCOL,2019-04-17 10:43:33,,0,Support ORDER BY within UNION subqueries,"We do not support ORDER BY within subqueries that are unioned, for example:

{code:sql}
SELECT * FROM
((SELECT a, b FROM t1 ORDER BY b LIMIT 2) sq1
UNION ALL
(SELECT a, b FROM t2 ORDER BY b LIMIT 2) sq2)
ORDER BY 1,2;
{code}
",,"Support ORDER BY within UNION subqueries $end$ We do not support ORDER BY within subqueries that are unioned, for example:

{code:sql}
SELECT * FROM
((SELECT a, b FROM t1 ORDER BY b LIMIT 2) sq1
UNION ALL
(SELECT a, b FROM t2 ORDER BY b LIMIT 2) sq2)
ORDER BY 1,2;
{code}
 $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,8,,0,5,0,1,0,1,0,,0,850,2,0,0,2019-04-23 20:34:01,Support ORDER BY within UNION subqueries,"We do not support ORDER BY within subqueries that are unioned, for example:

{code:sql}
SELECT * FROM
((SELECT a, b FROM t1 ORDER BY b LIMIT 100) sq1
UNION ALL
(SELECT a, b, FROM t2 ORDER BY b LIMIT 100) sq2)
ORDER BY 1,2;
{code}
",,0,1,0,6,0.0555556,"Support ORDER BY within UNION subqueries $end$ We do not support ORDER BY within subqueries that are unioned, for example:

{code:sql}
SELECT * FROM
((SELECT a, b FROM t1 ORDER BY b LIMIT 100) sq1
UNION ALL
(SELECT a, b, FROM t2 ORDER BY b LIMIT 100) sq2)
ORDER BY 1,2;
{code}
 $acceptance criteria:$",1,1,1,0,0,0,1,153.833,73,5,0.0684932,3,0.0410959,2,0.0273973,1,0.0136986,1,0.0136986
202,MCOL-3270,New Feature,MCOL,2019-04-18 18:55:12,,0,Improve cpimport ingest speed into Dictionary columns,"Given 800 000 000 records with a couple Dictionary columns with lots of equal length strings in the data set. It took 4 167 seconds to ingest the data set into CS.
After the patch it takes only 467 seconds.

There were two main sources of latency:
- Dctnry::getTokenFromArray represented de-dup buffer as array and called memcpy for any equal-sized string
- COND_WAIT_SECONDS was 3 seconds per default
",,"Improve cpimport ingest speed into Dictionary columns $end$ Given 800 000 000 records with a couple Dictionary columns with lots of equal length strings in the data set. It took 4 167 seconds to ingest the data set into CS.
After the patch it takes only 467 seconds.

There were two main sources of latency:
- Dctnry::getTokenFromArray represented de-dup buffer as array and called memcpy for any equal-sized string
- COND_WAIT_SECONDS was 3 seconds per default
 $acceptance criteria:$",,Roman,Roman,Major,6,,1,2,1,1,0,0,0,,0,850,2,0,0,2019-04-18 18:55:12,Improve cpimport ingest speed into Dictionary columns,"Given 800 000 000 records with a couple Dictionary columns with lots of equal length strings in the data set. It took 4 167 seconds to ingest the data set into CS.
After the patch it takes only 467 seconds.

There were two main sources of latency:
- Dctnry::getTokenFromArray represented de-dup buffer as array and called memcpy for any equal-sized string
- COND_WAIT_SECONDS was 3 seconds per default
",,0,0,0,0,0.0,"Improve cpimport ingest speed into Dictionary columns $end$ Given 800 000 000 records with a couple Dictionary columns with lots of equal length strings in the data set. It took 4 167 seconds to ingest the data set into CS.
After the patch it takes only 467 seconds.

There were two main sources of latency:
- Dctnry::getTokenFromArray represented de-dup buffer as array and called memcpy for any equal-sized string
- COND_WAIT_SECONDS was 3 seconds per default
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
203,MCOL-33,Task,MCOL,2016-05-03 15:46:36,,0,200: Monitor UM Memory Failed,"200 Monitor TotalUmMemory:            Failed (check test200/diff.txt)

TEST200 : df -h output:
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1             185G   90G   86G  52% /
tmpfs                 3.9G   15M  3.9G   1% /dev/shm
",,"200: Monitor UM Memory Failed $end$ 200 Monitor TotalUmMemory:            Failed (check test200/diff.txt)

TEST200 : df -h output:
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1             185G   90G   86G  52% /
tmpfs                 3.9G   15M  3.9G   1% /dev/shm
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,4,,0,4,0,1,0,0,0,,0,850,0,0,0,2016-05-23 07:36:44,200: Monitor UM Memory Failed,"200 Monitor TotalUmMemory:            Failed (check test200/diff.txt)

TEST200 : df -h output:
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1             185G   90G   86G  52% /
tmpfs                 3.9G   15M  3.9G   1% /dev/shm
",,0,0,0,0,0.0,"200: Monitor UM Memory Failed $end$ 200 Monitor TotalUmMemory:            Failed (check test200/diff.txt)

TEST200 : df -h output:
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1             185G   90G   86G  52% /
tmpfs                 3.9G   15M  3.9G   1% /dev/shm
 $acceptance criteria:$",0,0,0,0,0,0,0,471.833,13,5,0.384615,1,0.0769231,1,0.0769231,1,0.0769231,1,0.0769231
204,MCOL-3315,Task,MCOL,2019-05-16 19:18:59,,0,Rebase on MariaDB 10.3.15,Update base MariaDB version,,Rebase on MariaDB 10.3.15 $end$ Update base MariaDB version $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Critical,7,,0,2,0,1,0,0,0,,0,850,2,0,0,2019-05-16 19:18:59,Rebase on MariaDB 10.3.15,Update base MariaDB version,,0,0,0,0,0.0,Rebase on MariaDB 10.3.15 $end$ Update base MariaDB version $acceptance criteria:$,0,0,0,0,0,0,0,0.0,74,6,0.0810811,4,0.0540541,2,0.027027,1,0.0135135,1,0.0135135
205,MCOL-3321,Task,MCOL,2019-05-21 18:26:38,,0,some regr_*** function tests need order by to make them deterministic,,,some regr_*** function tests need order by to make them deterministic $end$ $acceptance criteria:$,,David Hall,David Hall,Trivial,9,,0,2,0,2,0,0,0,,0,850,2,0,0,2019-05-21 18:27:28,some regr_*** function tests need order by to make them deterministic,,,0,0,0,0,0.0,some regr_*** function tests need order by to make them deterministic $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
206,MCOL-3331,New Feature,MCOL,2019-05-24 14:00:47,,0,Switch pymcsapi's Python 3 requirement from Python 3.4 to Python 3.6 on CentOS 7,Since Python 3.4 got end-of-life at 2019-03-19 we should switch to a newer Python 3 version on CentOS 7 as requirement for pymcsapi and the libraries dependent on it.,,Switch pymcsapi's Python 3 requirement from Python 3.4 to Python 3.6 on CentOS 7 $end$ Since Python 3.4 got end-of-life at 2019-03-19 we should switch to a newer Python 3 version on CentOS 7 as requirement for pymcsapi and the libraries dependent on it. $acceptance criteria:$,,Jens Röwekamp,Jens Röwekamp,Major,13,,0,3,1,2,0,0,0,,0,850,3,0,0,2019-05-24 14:00:47,Switch pymcsapi's Python 3 requirement from Python 3.4 to Python 3.6 on CentOS 7,Since Python 3.4 got end-of-life at 2019-03-19 we should switch to a newer Python 3 version on CentOS 7 as requirement for pymcsapi and the libraries dependent on it.,,0,0,0,0,0.0,Switch pymcsapi's Python 3 requirement from Python 3.4 to Python 3.6 on CentOS 7 $end$ Since Python 3.4 got end-of-life at 2019-03-19 we should switch to a newer Python 3 version on CentOS 7 as requirement for pymcsapi and the libraries dependent on it. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,40,2,0.05,2,0.05,2,0.05,2,0.05,2,0.05
207,MCOL-3343,Task,MCOL,2019-05-30 15:34:49,,0,Window Functions don't work with arithmetic operators or other functions,"Discovered during QA:

create table tb1 (d1 decimal(7,2), i1 int)engine=columnstore;
insert into tb1 values (7.12, 1), (12.12, 1);

select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from tb1 group by i1;

This results in a random output in 1.2, and a 0 output in 1.1. The answer is 100 as can be seen when using innodb:
MariaDB [dhall]> select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from inno_tb1 group by i1;
+---------------------------------------------------------+
| (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) |
+---------------------------------------------------------+
|                                                     100 |
+---------------------------------------------------------+
1 row in set (0.008 sec)

Using the divide operator '/' instead of the div function may result in a different set of random numbers.
",,"Window Functions don't work with arithmetic operators or other functions $end$ Discovered during QA:

create table tb1 (d1 decimal(7,2), i1 int)engine=columnstore;
insert into tb1 values (7.12, 1), (12.12, 1);

select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from tb1 group by i1;

This results in a random output in 1.2, and a 0 output in 1.1. The answer is 100 as can be seen when using innodb:
MariaDB [dhall]> select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from inno_tb1 group by i1;
+---------------------------------------------------------+
| (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) |
+---------------------------------------------------------+
|                                                     100 |
+---------------------------------------------------------+
1 row in set (0.008 sec)

Using the divide operator '/' instead of the div function may result in a different set of random numbers.
 $acceptance criteria:$",,David Hall,David Hall,Major,11,,2,5,2,2,0,1,0,,0,850,5,0,0,2019-05-30 15:34:49,Incorrect results in specific case,"Discovered during QA:

create table tb1 (d1 decimal(7,2), i1 int)engine=columnstore;
insert into tb1 values (7.12, 1), (12.12, 1);

select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from tb1 group by i1;

This results in a random output in 1.2, and a 0 output in 1.1. The answer is 100 as can be seen when using innodb:
MariaDB [dhall]> select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from inno_tb1 group by i1;
+---------------------------------------------------------+
| (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) |
+---------------------------------------------------------+
|                                                     100 |
+---------------------------------------------------------+
1 row in set (0.008 sec)

Using the divide operator '/' instead of the div function may result in a different set of random numbers.
",,1,0,0,15,0.0840336,"Incorrect results in specific case $end$ Discovered during QA:

create table tb1 (d1 decimal(7,2), i1 int)engine=columnstore;
insert into tb1 values (7.12, 1), (12.12, 1);

select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from tb1 group by i1;

This results in a random output in 1.2, and a 0 output in 1.1. The answer is 100 as can be seen when using innodb:
MariaDB [dhall]> select (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) from inno_tb1 group by i1;
+---------------------------------------------------------+
| (sum(d1)*100.0) div sum(sum(d1)) over (partition by i1) |
+---------------------------------------------------------+
|                                                     100 |
+---------------------------------------------------------+
1 row in set (0.008 sec)

Using the divide operator '/' instead of the div function may result in a different set of random numbers.
 $acceptance criteria:$",1,1,1,1,1,0,1,0.0,11,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
208,MCOL-3349,Sub-Task,MCOL,2019-05-31 20:33:53,,0,CTAS (Create Table as Select),"Add capability to create table in ColumnStore as a select.

CREATE TABLE test2 ENGINE=columnstore AS
SELECT *
FROM test;",,"CTAS (Create Table as Select) $end$ Add capability to create table in ColumnStore as a select.

CREATE TABLE test2 ENGINE=columnstore AS
SELECT *
FROM test; $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,15,,0,2,0,6,0,0,0,,0,850,2,0,0,2019-05-31 20:33:53,CTAS (Create Table as Select),"Add capability to create table in ColumnStore as a select.

CREATE TABLE test2 ENGINE=columnstore AS
SELECT *
FROM test;",,0,0,0,0,0.0,"CTAS (Create Table as Select) $end$ Add capability to create table in ColumnStore as a select.

CREATE TABLE test2 ENGINE=columnstore AS
SELECT *
FROM test; $acceptance criteria:$",0,0,0,0,0,0,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
209,MCOL-3388,Task,MCOL,2019-06-19 16:47:56,,0,Merge up 1.1 -> 1.2,Issues that are merged up will be tagged in this ticket for QA.,,Merge up 1.1 -> 1.2 $end$ Issues that are merged up will be tagged in this ticket for QA. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,3,3,3,1,0,0,0,,0,850,3,0,0,2019-06-19 16:47:56,Merge up 1.1 -> 1.2,Issues that are merged up will be tagged in this ticket for QA.,,0,0,0,0,0.0,Merge up 1.1 -> 1.2 $end$ Issues that are merged up will be tagged in this ticket for QA. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,75,6,0.08,4,0.0533333,2,0.0266667,1,0.0133333,1,0.0133333
210,MCOL-3389,Task,MCOL,2019-06-21 16:31:04,,0,Backport MCOL-1495 to 1.1,There have been requests to backport MCOL-1495 to 1.1. This ticket is to track that.,,Backport MCOL-1495 to 1.1 $end$ There have been requests to backport MCOL-1495 to 1.1. This ticket is to track that. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,9,,1,1,1,1,0,0,0,,0,850,1,0,0,2019-06-21 16:31:04,Backport MCOL-1495 to 1.1,There have been requests to backport MCOL-1495 to 1.1. This ticket is to track that.,,0,0,0,0,0.0,Backport MCOL-1495 to 1.1 $end$ There have been requests to backport MCOL-1495 to 1.1. This ticket is to track that. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,76,6,0.0789474,4,0.0526316,2,0.0263158,1,0.0131579,1,0.0131579
211,MCOL-3398,Task,MCOL,2019-07-03 18:24:15,,0,Rebase 1.2 on MariaDB 10.3.16,ColumnStore needs to be rebased on MariaDB 10.3.16,,Rebase 1.2 on MariaDB 10.3.16 $end$ ColumnStore needs to be rebased on MariaDB 10.3.16 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,0,1,0,0,0,,0,850,2,0,0,2019-07-03 18:24:15,Rebase 1.2 on MariaDB 10.3.16,ColumnStore needs to be rebased on MariaDB 10.3.16,,0,0,0,0,0.0,Rebase 1.2 on MariaDB 10.3.16 $end$ ColumnStore needs to be rebased on MariaDB 10.3.16 $acceptance criteria:$,0,0,0,0,0,0,0,0.0,77,6,0.0779221,4,0.0519481,2,0.025974,1,0.012987,1,0.012987
212,MCOL-3416,New Feature,MCOL,2019-08-08 12:29:18,,0,Add support for COND_EQUAL conditions set,MDB now sends down a set of equi-join conditions in JOIN::cond_equal in form of COND_EQUAL instance. This set must be replaced with ParserTree node that combines all equi-conditions.,,Add support for COND_EQUAL conditions set $end$ MDB now sends down a set of equi-join conditions in JOIN::cond_equal in form of COND_EQUAL instance. This set must be replaced with ParserTree node that combines all equi-conditions. $acceptance criteria:$,,Roman,Roman,Major,6,,0,3,0,1,0,0,0,,0,850,2,0,0,2019-09-26 09:02:41,Add support for COND_EQUAL conditions set,MDB now sends down a set of equi-join conditions in JOIN::cond_equal in form of COND_EQUAL instance. This set must be replaced with ParserTree node that combines all equi-conditions.,,0,0,0,0,0.0,Add support for COND_EQUAL conditions set $end$ MDB now sends down a set of equi-join conditions in JOIN::cond_equal in form of COND_EQUAL instance. This set must be replaced with ParserTree node that combines all equi-conditions. $acceptance criteria:$,0,0,0,0,0,0,0,1172.55,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
213,MCOL-3419,Task,MCOL,2019-08-08 21:08:45,,0,Changing systemlang corrupts extentmap and system catalog,"Start up a CS single server system. Shut it down. Change the systemlang to something viable, say ja_JP. Start the system back up. EM and system catalog will be unusable.",,"Changing systemlang corrupts extentmap and system catalog $end$ Start up a CS single server system. Shut it down. Change the systemlang to something viable, say ja_JP. Start the system back up. EM and system catalog will be unusable. $acceptance criteria:$",,David Hall,David Hall,Major,10,,1,5,2,1,0,0,0,,0,850,4,0,0,2019-08-09 22:45:00,Changing systemlang corrupts extentmap and system catalog,"Start up a CS single server system. Shut it down. Change the systemlang to something viable, say ja_JP. Start the system back up. EM and system catalog will be unusable.",,0,0,0,0,0.0,"Changing systemlang corrupts extentmap and system catalog $end$ Start up a CS single server system. Shut it down. Change the systemlang to something viable, say ja_JP. Start the system back up. EM and system catalog will be unusable. $acceptance criteria:$",0,0,0,0,0,0,0,25.6,12,1,0.0833333,1,0.0833333,1,0.0833333,1,0.0833333,0,0.0
214,MCOL-3420,Task,MCOL,2019-08-08 21:15:30,,0,Changing systemlang to an unrecognized string causes CS to not start up.,,,Changing systemlang to an unrecognized string causes CS to not start up. $end$ $acceptance criteria:$,,David Hall,David Hall,Major,16,,0,3,1,2,0,0,0,,0,850,3,0,0,2020-03-02 18:54:33,Changing systemlang to an unrecognized string causes CS to not start up.,,,0,0,0,0,0.0,Changing systemlang to an unrecognized string causes CS to not start up. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,4965.65,13,1,0.0769231,1,0.0769231,1,0.0769231,1,0.0769231,0,0.0
215,MCOL-3483,Task,MCOL,2019-09-05 15:50:59,,0,Port cloud functionality to CS version 1.2.5,"To give DBS a final deliverable, I need to port our cloud functionality to 1.2.5.",,"Port cloud functionality to CS version 1.2.5 $end$ To give DBS a final deliverable, I need to port our cloud functionality to 1.2.5. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,23,,0,3,0,8,0,0,0,,0,850,0,0,0,2019-09-26 09:01:35,Port cloud functionality to CS version 1.2.5,"To give DBS a final deliverable, I need to port our cloud functionality to 1.2.5.",,0,0,0,0,0.0,"Port cloud functionality to CS version 1.2.5 $end$ To give DBS a final deliverable, I need to port our cloud functionality to 1.2.5. $acceptance criteria:$",0,0,0,0,0,0,1,497.167,2,1,0.5,1,0.5,0,0.0,0,0.0,0,0.0
216,MCOL-35,Task,MCOL,2016-05-03 15:52:18,,0,211 Concurrent Transactions Test failed,"211 Concurrent Transactions Test:     Failed (seconds=1800, thr=15, rowFactor=10000, batches=0, rows=0, bad count=0, errors=75, hung threads=0)",,"211 Concurrent Transactions Test failed $end$ 211 Concurrent Transactions Test:     Failed (seconds=1800, thr=15, rowFactor=10000, batches=0, rows=0, bad count=0, errors=75, hung threads=0) $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,11,,3,6,3,1,0,0,0,,0,850,2,0,0,2016-06-15 20:28:39,211 Concurrent Transactions Test failed,"211 Concurrent Transactions Test:     Failed (seconds=1800, thr=15, rowFactor=10000, batches=0, rows=0, bad count=0, errors=75, hung threads=0)",,0,0,0,0,0.0,"211 Concurrent Transactions Test failed $end$ 211 Concurrent Transactions Test:     Failed (seconds=1800, thr=15, rowFactor=10000, batches=0, rows=0, bad count=0, errors=75, hung threads=0) $acceptance criteria:$",0,0,0,0,0,0,0,1036.6,14,5,0.357143,1,0.0714286,1,0.0714286,1,0.0714286,1,0.0714286
217,MCOL-3503,New Feature,MCOL,2019-09-16 14:35:04,MCOL-1049,0,Create MODA function for columnstore,"A new aggregate function MODA which takes the mode of the input numeric field and determines the mode. Tie breakers are: Closest to the AVG, then smallest absolute value. ",,"Create MODA function for columnstore $end$ A new aggregate function MODA which takes the mode of the input numeric field and determines the mode. Tie breakers are: Closest to the AVG, then smallest absolute value.  $acceptance criteria:$",,David Hall,David Hall,Major,31,,0,10,0,1,0,0,0,,0,850,10,0,0,2019-09-25 15:45:59,Create MODA function for columnstore,"A new aggregate function MODA which takes the mode of the input numeric field and determines the mode. Tie breakers are: Closest to the AVG, then smallest absolute value. ",,0,0,0,0,0.0,"Create MODA function for columnstore $end$ A new aggregate function MODA which takes the mode of the input numeric field and determines the mode. Tie breakers are: Closest to the AVG, then smallest absolute value.  $acceptance criteria:$",0,0,0,0,0,0,0,217.167,14,1,0.0714286,1,0.0714286,1,0.0714286,1,0.0714286,0,0.0
218,MCOL-3514,New Feature,MCOL,2019-09-24 09:31:03,,0,Make cpimport read from data in S3 buckets,cpimport needs new options to allow it to read a source file from an Amazon S3 bucket.,,Make cpimport read from data in S3 buckets $end$ cpimport needs new options to allow it to read a source file from an Amazon S3 bucket. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,15,,0,3,0,3,0,0,2,,0,850,2,0,0,2019-09-24 09:55:18,Make cpimport read from data in S3 buckets,cpimport needs new options to allow it to read a source file from an Amazon S3 bucket.,,0,0,0,0,0.0,Make cpimport read from data in S3 buckets $end$ cpimport needs new options to allow it to read a source file from an Amazon S3 bucket. $acceptance criteria:$,0,0,0,0,0,0,1,0.4,78,6,0.0769231,4,0.0512821,2,0.025641,1,0.0128205,1,0.0128205
219,MCOL-3515,New Feature,MCOL,2019-09-24 11:01:58,,0,Make non-distributed install the only install,We need to remove the option of distributed / non-distributed install. non-distributed should be the only install mechanism.,,Make non-distributed install the only install $end$ We need to remove the option of distributed / non-distributed install. non-distributed should be the only install mechanism. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-09-24 11:01:58,Make non-distributed install the only install,We need to remove the option of distributed / non-distributed install. non-distributed should be the only install mechanism.,,0,0,0,0,0.0,Make non-distributed install the only install $end$ We need to remove the option of distributed / non-distributed install. non-distributed should be the only install mechanism. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,79,6,0.0759494,4,0.0506329,2,0.0253165,1,0.0126582,1,0.0126582
220,MCOL-3520,Sub-Task,MCOL,2019-09-27 18:42:39,,0,Fix cpimport S3 multi-PM usage,"Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too.",,"Fix cpimport S3 multi-PM usage $end$ Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Blocker,13,,1,3,1,3,0,1,0,,0,850,3,0,0,2019-09-27 18:42:39,Fix multi-PM usage,"Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too.",,1,0,0,2,0.105263,"Fix multi-PM usage $end$ Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too. $acceptance criteria:$",1,1,0,0,0,0,1,0.0,80,6,0.075,4,0.05,2,0.025,1,0.0125,1,0.0125
221,MCOL-3542,Task,MCOL,2019-10-03 20:13:48,MCOL-3548,0,Add option to not verify an SSL certificate,"Low hanging fruit.

We gave the 1.2.5 + S3 package to Patrice @ ABS (iirc?) to play with.  It is not working for him b/c the S3 boxes he's using (some on-prem WD boxes that impl S3 protocol) have SSL certs that can't be verified.

Using the libmarias3 lib directly and setting S3NOVERIFY=1, he can interact with it.  SM doesn't currently have the option to do that, but should have one.  Just needs to know to init the S3 lib with that var or not, don't have to implement anything substantial.",,"Add option to not verify an SSL certificate $end$ Low hanging fruit.

We gave the 1.2.5 + S3 package to Patrice @ ABS (iirc?) to play with.  It is not working for him b/c the S3 boxes he's using (some on-prem WD boxes that impl S3 protocol) have SSL certs that can't be verified.

Using the libmarias3 lib directly and setting S3NOVERIFY=1, he can interact with it.  SM doesn't currently have the option to do that, but should have one.  Just needs to know to init the S3 lib with that var or not, don't have to implement anything substantial. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,14,,0,3,0,3,0,0,0,,0,850,1,0,0,2021-03-29 21:42:14,Add option to not verify an SSL certificate,"Low hanging fruit.

We gave the 1.2.5 + S3 package to Patrice @ ABS (iirc?) to play with.  It is not working for him b/c the S3 boxes he's using (some on-prem WD boxes that impl S3 protocol) have SSL certs that can't be verified.

Using the libmarias3 lib directly and setting S3NOVERIFY=1, he can interact with it.  SM doesn't currently have the option to do that, but should have one.  Just needs to know to init the S3 lib with that var or not, don't have to implement anything substantial.",,0,0,0,0,0.0,"Add option to not verify an SSL certificate $end$ Low hanging fruit.

We gave the 1.2.5 + S3 package to Patrice @ ABS (iirc?) to play with.  It is not working for him b/c the S3 boxes he's using (some on-prem WD boxes that impl S3 protocol) have SSL certs that can't be verified.

Using the libmarias3 lib directly and setting S3NOVERIFY=1, he can interact with it.  SM doesn't currently have the option to do that, but should have one.  Just needs to know to init the S3 lib with that var or not, don't have to implement anything substantial. $acceptance criteria:$",0,0,0,0,0,0,1,13033.5,3,1,0.333333,1,0.333333,0,0.0,0,0.0,0,0.0
222,MCOL-3551,New Feature,MCOL,2019-10-11 10:23:23,,0,MariaDB should be installed in generic paths,MariaDB server not longer needs to be installed in a custom path. As part of convergence we should remove that requirement and make ColumnStore work with the generic binary paths.,,MariaDB should be installed in generic paths $end$ MariaDB server not longer needs to be installed in a custom path. As part of convergence we should remove that requirement and make ColumnStore work with the generic binary paths. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,1,2,3,1,0,0,0,,0,850,2,0,0,2019-10-11 10:23:23,MariaDB should be installed in generic paths,MariaDB server not longer needs to be installed in a custom path. As part of convergence we should remove that requirement and make ColumnStore work with the generic binary paths.,,0,0,0,0,0.0,MariaDB should be installed in generic paths $end$ MariaDB server not longer needs to be installed in a custom path. As part of convergence we should remove that requirement and make ColumnStore work with the generic binary paths. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,81,7,0.0864198,4,0.0493827,2,0.0246914,1,0.0123457,1,0.0123457
223,MCOL-3552,New Feature,MCOL,2019-10-11 11:18:59,,0,Make plugins use --plugin-load-add,Load using columnstore.cnf and --plugin-load-add instead of the install we do at the moment.,,Make plugins use --plugin-load-add $end$ Load using columnstore.cnf and --plugin-load-add instead of the install we do at the moment. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,1,1,1,0,0,0,,0,850,1,0,0,2019-10-11 11:18:59,Make plugins use --plugin-load-add,Load using columnstore.cnf and --plugin-load-add instead of the install we do at the moment.,,0,0,0,0,0.0,Make plugins use --plugin-load-add $end$ Load using columnstore.cnf and --plugin-load-add instead of the install we do at the moment. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,82,7,0.0853659,4,0.0487805,2,0.0243902,1,0.0121951,1,0.0121951
224,MCOL-3554,New Feature,MCOL,2019-10-11 15:13:41,,0,mcsapi - port specification through Columnstore.xml,"Currently mcsapi uses the ColumnStore default ports to connect to.
This patch should allow mcsapi to use non-default ports as specified in Columnstore.xml

It should further add an optional Columnstore.xml entry to skip the Columnstore version check against the ProcMon port. This is useful as we need to expose as less ports as possible for SkySQL.
As SkySQL will use stable ColumnStore versions only, the version check could be skipped and the ProcMon port not exposed to our customers.",,"mcsapi - port specification through Columnstore.xml $end$ Currently mcsapi uses the ColumnStore default ports to connect to.
This patch should allow mcsapi to use non-default ports as specified in Columnstore.xml

It should further add an optional Columnstore.xml entry to skip the Columnstore version check against the ProcMon port. This is useful as we need to expose as less ports as possible for SkySQL.
As SkySQL will use stable ColumnStore versions only, the version check could be skipped and the ProcMon port not exposed to our customers. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,13,,0,3,0,1,0,0,0,,0,850,1,0,0,2019-10-16 08:25:57,mcsapi - port specification through Columnstore.xml,"Currently mcsapi uses the ColumnStore default ports to connect to.
This patch should allow mcsapi to use non-default ports as specified in Columnstore.xml

It should further add an optional Columnstore.xml entry to skip the Columnstore version check against the ProcMon port. This is useful as we need to expose as less ports as possible for SkySQL.
As SkySQL will use stable ColumnStore versions only, the version check could be skipped and the ProcMon port not exposed to our customers.",,0,0,0,0,0.0,"mcsapi - port specification through Columnstore.xml $end$ Currently mcsapi uses the ColumnStore default ports to connect to.
This patch should allow mcsapi to use non-default ports as specified in Columnstore.xml

It should further add an optional Columnstore.xml entry to skip the Columnstore version check against the ProcMon port. This is useful as we need to expose as less ports as possible for SkySQL.
As SkySQL will use stable ColumnStore versions only, the version check could be skipped and the ProcMon port not exposed to our customers. $acceptance criteria:$",0,0,0,0,0,0,0,113.2,41,2,0.0487805,2,0.0487805,2,0.0487805,2,0.0487805,2,0.0487805
225,MCOL-3556,New Feature,MCOL,2019-10-14 10:50:26,,0,Switch to per-UM setting for replication slave,"ColumnStore needs to set whether an individual UM should replay slave events on a ColumnStore table.

This will be done with a new MariaDB variable called ""columnstore_replication_slave"" which is off by default and can only be changed in my.cnf, not at runtime.

It cannot be changed at runtime because it would be a global variable that would need to be re-read, which requires the replication thread to end and restart. This overly complicates things and therefore a MariaDB restart is easier.",,"Switch to per-UM setting for replication slave $end$ ColumnStore needs to set whether an individual UM should replay slave events on a ColumnStore table.

This will be done with a new MariaDB variable called ""columnstore_replication_slave"" which is off by default and can only be changed in my.cnf, not at runtime.

It cannot be changed at runtime because it would be a global variable that would need to be re-read, which requires the replication thread to end and restart. This overly complicates things and therefore a MariaDB restart is easier. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,6,,0,3,0,1,0,0,0,,0,850,3,0,0,2019-10-14 10:56:06,Switch to per-UM setting for replication slave,"ColumnStore needs to set whether an individual UM should replay slave events on a ColumnStore table.

This will be done with a new MariaDB variable called ""columnstore_replication_slave"" which is off by default and can only be changed in my.cnf, not at runtime.

It cannot be changed at runtime because it would be a global variable that would need to be re-read, which requires the replication thread to end and restart. This overly complicates things and therefore a MariaDB restart is easier.",,0,0,0,0,0.0,"Switch to per-UM setting for replication slave $end$ ColumnStore needs to set whether an individual UM should replay slave events on a ColumnStore table.

This will be done with a new MariaDB variable called ""columnstore_replication_slave"" which is off by default and can only be changed in my.cnf, not at runtime.

It cannot be changed at runtime because it would be a global variable that would need to be re-read, which requires the replication thread to end and restart. This overly complicates things and therefore a MariaDB restart is easier. $acceptance criteria:$",0,0,0,0,0,0,0,0.0833333,83,7,0.0843373,4,0.0481928,2,0.0240964,1,0.0120482,1,0.0120482
226,MCOL-3559,New Feature,MCOL,2019-10-14 13:42:18,MCOL-1503,0,Don't allow postConfigure to run if MariaDB has already started,It is possible a user will want to add ColumnStore to MariaDB after MariaDB is already running. We should protect for this scenario.,,Don't allow postConfigure to run if MariaDB has already started $end$ It is possible a user will want to add ColumnStore to MariaDB after MariaDB is already running. We should protect for this scenario. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,9,,0,2,0,1,0,2,0,,0,850,2,0,0,2019-10-14 13:42:18,Allow postConfigure to run after MariaDB has already started,"It is possible a user will want to add ColumnStore to MariaDB after MariaDB is already running. We should allow for this scenario.

Best way I can think of doing this is to have an additional postConfigure option as to whether or not to setup mysqld and additional flags to postConfigure to specify root user credentials.

Still some things I haven't figured out such as getting the UDFs and stored procs to install.",,1,1,0,57,0.635294,"Allow postConfigure to run after MariaDB has already started $end$ It is possible a user will want to add ColumnStore to MariaDB after MariaDB is already running. We should allow for this scenario.

Best way I can think of doing this is to have an additional postConfigure option as to whether or not to setup mysqld and additional flags to postConfigure to specify root user credentials.

Still some things I haven't figured out such as getting the UDFs and stored procs to install. $acceptance criteria:$",2,1,1,1,1,1,1,0.0,84,7,0.0833333,4,0.047619,2,0.0238095,1,0.0119048,1,0.0119048
227,MCOL-3563,Task,MCOL,2019-10-16 20:30:15,,0,cleanup compiler warnings / type errors in *Task classes,"Got a lot of legitimate compiler warnings building develop.  PosixTask::read() returns an int, is assigned to a bool, and then there's a comparison 'if (bool < 0)', which is always false.  So, there's currently no error handling for those read() calls.",,"cleanup compiler warnings / type errors in *Task classes $end$ Got a lot of legitimate compiler warnings building develop.  PosixTask::read() returns an int, is assigned to a bool, and then there's a comparison 'if (bool < 0)', which is always false.  So, there's currently no error handling for those read() calls. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,9,,0,2,0,1,0,0,0,,0,850,1,0,0,2019-11-19 16:16:30,cleanup compiler warnings / type errors in *Task classes,"Got a lot of legitimate compiler warnings building develop.  PosixTask::read() returns an int, is assigned to a bool, and then there's a comparison 'if (bool < 0)', which is always false.  So, there's currently no error handling for those read() calls.",,0,0,0,0,0.0,"cleanup compiler warnings / type errors in *Task classes $end$ Got a lot of legitimate compiler warnings building develop.  PosixTask::read() returns an int, is assigned to a bool, and then there's a comparison 'if (bool < 0)', which is always false.  So, there's currently no error handling for those read() calls. $acceptance criteria:$",0,0,0,0,0,0,0,811.767,4,1,0.25,1,0.25,0,0.0,0,0.0,0,0.0
228,MCOL-3577,New Feature,MCOL,2019-10-28 18:11:58,,0,Storagemanager should set a flag to indicate successful flush after mcsadmin suspenddatabasewrites,"After setting ColumnStore into read only mode through ""mcsadmin suspenddatabasewrites"" the storage manager should set a flag once all cached data has been written to S3 or local disk.

This flag is important for the integrity of disk based snapshots.",,"Storagemanager should set a flag to indicate successful flush after mcsadmin suspenddatabasewrites $end$ After setting ColumnStore into read only mode through ""mcsadmin suspenddatabasewrites"" the storage manager should set a flag once all cached data has been written to S3 or local disk.

This flag is important for the integrity of disk based snapshots. $acceptance criteria:$",,Jens Röwekamp,Jens Röwekamp,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-11-22 13:43:15,Storagemanager should set a flag to indicate successful flush after mcsadmin suspenddatabasewrites,"After setting ColumnStore into read only mode through ""mcsadmin suspenddatabasewrites"" the storage manager should set a flag once all cached data has been written to S3 or local disk.

This flag is important for the integrity of disk based snapshots.",,0,0,0,0,0.0,"Storagemanager should set a flag to indicate successful flush after mcsadmin suspenddatabasewrites $end$ After setting ColumnStore into read only mode through ""mcsadmin suspenddatabasewrites"" the storage manager should set a flag once all cached data has been written to S3 or local disk.

This flag is important for the integrity of disk based snapshots. $acceptance criteria:$",0,0,0,0,0,0,0,595.517,42,2,0.047619,2,0.047619,2,0.047619,2,0.047619,2,0.047619
229,MCOL-3581,Sub-Task,MCOL,2019-10-31 16:26:54,,0,Multi-threaded order-by for 1.2,"This ticket is to track multi-threaded order-by added into 1.2. This is for use cases where ColumnStore is executing the order by, such as subqueries.",,"Multi-threaded order-by for 1.2 $end$ This ticket is to track multi-threaded order-by added into 1.2. This is for use cases where ColumnStore is executing the order by, such as subqueries. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,5,,0,3,0,1,0,0,0,,0,850,3,0,0,2019-10-31 16:26:54,Multi-threaded order-by for 1.2,"This ticket is to track multi-threaded order-by added into 1.2. This is for use cases where ColumnStore is executing the order by, such as subqueries.",,0,0,0,0,0.0,"Multi-threaded order-by for 1.2 $end$ This ticket is to track multi-threaded order-by added into 1.2. This is for use cases where ColumnStore is executing the order by, such as subqueries. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,85,8,0.0941176,5,0.0588235,3,0.0352941,2,0.0235294,2,0.0235294
230,MCOL-3585,Task,MCOL,2019-11-04 16:21:24,,0,update some defaults in config file,"Some of the defaults are more appropriate to the machines ~ 10-15 years ago.  Time to update...

1) PM join threshold.  With ssds it makes sense to take some mem from cache and use it for larger pm joins.  I would suggest a min of 1GB.  The key is named PmMaxMemorySmallSide.
2) Number of aggregation threads & buckets.  For some reason this is 4.  Would be good to automate this setting, but if time is short, let's just set them higher.  As a rough guideline, make the # of buckets 4 X num threads.  Keys of interest are RowAggrThreads and RowAggrBuckets.
3) degree of parallelism.  This would also be a good candidate for automation.  MaxOutstandingRequests is how many extents worth of jobs can be in flight at once.  With higher core count, bigger mem, and potential for scale-out, should probably be increased.  Automation would be a function of core count across all PMs, ProcessorThreadsPerScan, etc.

There also isn't much reason to have large buffers between jobsteps, so I would suggest trimming FifoSizeLargeSide, and FifoSize.  Benchmark a little to confirm.
",,"update some defaults in config file $end$ Some of the defaults are more appropriate to the machines ~ 10-15 years ago.  Time to update...

1) PM join threshold.  With ssds it makes sense to take some mem from cache and use it for larger pm joins.  I would suggest a min of 1GB.  The key is named PmMaxMemorySmallSide.
2) Number of aggregation threads & buckets.  For some reason this is 4.  Would be good to automate this setting, but if time is short, let's just set them higher.  As a rough guideline, make the # of buckets 4 X num threads.  Keys of interest are RowAggrThreads and RowAggrBuckets.
3) degree of parallelism.  This would also be a good candidate for automation.  MaxOutstandingRequests is how many extents worth of jobs can be in flight at once.  With higher core count, bigger mem, and potential for scale-out, should probably be increased.  Automation would be a function of core count across all PMs, ProcessorThreadsPerScan, etc.

There also isn't much reason to have large buffers between jobsteps, so I would suggest trimming FifoSizeLargeSide, and FifoSize.  Benchmark a little to confirm.
 $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,7,,0,2,0,1,0,0,0,,0,850,1,0,0,2019-11-22 13:43:03,update some defaults in config file,"Some of the defaults are more appropriate to the machines ~ 10-15 years ago.  Time to update...

1) PM join threshold.  With ssds it makes sense to take some mem from cache and use it for larger pm joins.  I would suggest a min of 1GB.  The key is named PmMaxMemorySmallSide.
2) Number of aggregation threads & buckets.  For some reason this is 4.  Would be good to automate this setting, but if time is short, let's just set them higher.  As a rough guideline, make the # of buckets 4 X num threads.  Keys of interest are RowAggrThreads and RowAggrBuckets.
3) degree of parallelism.  This would also be a good candidate for automation.  MaxOutstandingRequests is how many extents worth of jobs can be in flight at once.  With higher core count, bigger mem, and potential for scale-out, should probably be increased.  Automation would be a function of core count across all PMs, ProcessorThreadsPerScan, etc.

There also isn't much reason to have large buffers between jobsteps, so I would suggest trimming FifoSizeLargeSide, and FifoSize.  Benchmark a little to confirm.
",,0,0,0,0,0.0,"update some defaults in config file $end$ Some of the defaults are more appropriate to the machines ~ 10-15 years ago.  Time to update...

1) PM join threshold.  With ssds it makes sense to take some mem from cache and use it for larger pm joins.  I would suggest a min of 1GB.  The key is named PmMaxMemorySmallSide.
2) Number of aggregation threads & buckets.  For some reason this is 4.  Would be good to automate this setting, but if time is short, let's just set them higher.  As a rough guideline, make the # of buckets 4 X num threads.  Keys of interest are RowAggrThreads and RowAggrBuckets.
3) degree of parallelism.  This would also be a good candidate for automation.  MaxOutstandingRequests is how many extents worth of jobs can be in flight at once.  With higher core count, bigger mem, and potential for scale-out, should probably be increased.  Automation would be a function of core count across all PMs, ProcessorThreadsPerScan, etc.

There also isn't much reason to have large buffers between jobsteps, so I would suggest trimming FifoSizeLargeSide, and FifoSize.  Benchmark a little to confirm.
 $acceptance criteria:$",0,0,0,0,0,0,0,429.35,5,1,0.2,1,0.2,0,0.0,0,0.0,0,0.0
231,MCOL-3597,Sub-Task,MCOL,2019-11-08 09:52:55,,0,TIMEDIFF() returns NULL instead of expected value,"Queries from both working_tpch1/qa_fe_cnxFunctions/TIMEDIFF.DM.sql and
working_tpch1/qa_fe_postProcessedFunctions/TIMEDIFF.DM.sql are exampes to reproduce the issue.",,"TIMEDIFF() returns NULL instead of expected value $end$ Queries from both working_tpch1/qa_fe_cnxFunctions/TIMEDIFF.DM.sql and
working_tpch1/qa_fe_postProcessedFunctions/TIMEDIFF.DM.sql are exampes to reproduce the issue. $acceptance criteria:$",,Roman,Roman,Minor,7,,0,2,0,7,0,0,0,,0,850,2,0,0,2019-11-08 09:52:55,TIMEDIFF() returns NULL instead of expected value,"Queries from both working_tpch1/qa_fe_cnxFunctions/TIMEDIFF.DM.sql and
working_tpch1/qa_fe_postProcessedFunctions/TIMEDIFF.DM.sql are exampes to reproduce the issue.",,0,0,0,0,0.0,"TIMEDIFF() returns NULL instead of expected value $end$ Queries from both working_tpch1/qa_fe_cnxFunctions/TIMEDIFF.DM.sql and
working_tpch1/qa_fe_postProcessedFunctions/TIMEDIFF.DM.sql are exampes to reproduce the issue. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,7,1,0.142857,1,0.142857,1,0.142857,1,0.142857,1,0.142857
232,MCOL-3598,Sub-Task,MCOL,2019-11-08 09:54:44,,0,ORDER BY over negative SEC_TO_TIME() results produce an incorrect order,"The query:
select cidx, CSMALLINT,SEC_TO_TIME(CSMALLINT) from datatypetestm order by SEC_TO_TIME(CSMALLINT), cidx;
taken from working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql is the way to reproduce the issue.",,"ORDER BY over negative SEC_TO_TIME() results produce an incorrect order $end$ The query:
select cidx, CSMALLINT,SEC_TO_TIME(CSMALLINT) from datatypetestm order by SEC_TO_TIME(CSMALLINT), cidx;
taken from working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql is the way to reproduce the issue. $acceptance criteria:$",,Roman,Roman,Minor,5,,0,2,0,7,0,0,0,,0,850,2,0,0,2019-11-08 09:54:44,ORDER BY over negative SEC_TO_TIME() results produce an incorrect order,"The query:
select cidx, CSMALLINT,SEC_TO_TIME(CSMALLINT) from datatypetestm order by SEC_TO_TIME(CSMALLINT), cidx;
taken from working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql is the way to reproduce the issue.",,0,0,0,0,0.0,"ORDER BY over negative SEC_TO_TIME() results produce an incorrect order $end$ The query:
select cidx, CSMALLINT,SEC_TO_TIME(CSMALLINT) from datatypetestm order by SEC_TO_TIME(CSMALLINT), cidx;
taken from working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql is the way to reproduce the issue. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,8,1,0.125,1,0.125,1,0.125,1,0.125,1,0.125
233,MCOL-3599,Sub-Task,MCOL,2019-11-08 09:57:06,,0,UDaFs moda() and regr_ are not created in 1.4,The mentioned UDaFs are not created after we started to change paths for CS. There are two tests directories in our regression that fails having no moda() and regr_ UDaFs: working_tpch1/windowFunctions/MCOL-3503.sql and working_tpch1_compareLogOnly/regr/*,,UDaFs moda() and regr_ are not created in 1.4 $end$ The mentioned UDaFs are not created after we started to change paths for CS. There are two tests directories in our regression that fails having no moda() and regr_ UDaFs: working_tpch1/windowFunctions/MCOL-3503.sql and working_tpch1_compareLogOnly/regr/* $acceptance criteria:$,,Roman,Roman,Minor,6,,0,2,0,7,0,0,0,,0,850,2,0,0,2019-11-08 09:57:06,UDaFs moda() and regr_ are not created in 1.4,The mentioned UDaFs are not created after we started to change paths for CS. There are two tests directories in our regression that fails having no moda() and regr_ UDaFs: working_tpch1/windowFunctions/MCOL-3503.sql and working_tpch1_compareLogOnly/regr/*,,0,0,0,0,0.0,UDaFs moda() and regr_ are not created in 1.4 $end$ The mentioned UDaFs are not created after we started to change paths for CS. There are two tests directories in our regression that fails having no moda() and regr_ UDaFs: working_tpch1/windowFunctions/MCOL-3503.sql and working_tpch1_compareLogOnly/regr/* $acceptance criteria:$,0,0,0,0,0,0,1,0.0,9,1,0.111111,1,0.111111,1,0.111111,1,0.111111,1,0.111111
234,MCOL-3601,Sub-Task,MCOL,2019-11-08 10:03:13,,0,OUTER JOIN doesn't work,"These are a list of tests from the regression to test this issue: working_tpch1/misc/bug3759.sql
working_tpch1_compareLogOnly/fnJoin/bug5289.sql
working_tpch1_compareLogOnly/fnJoin/tpch05b.mod.sql
working_tpch1_compareLogOnly/fnJoin/tpch16b.sql
working_tpch1_compareLogOnly/fnJoin/tpch16.sql
working_tpch1_compareLogOnly/fnJoin/tpch18.sql
working_tpch1_compareLogOnly/fnJoin/tpch20b.sql
working_tpch1_compareLogOnly/misc/simple_outer.sql
",,"OUTER JOIN doesn't work $end$ These are a list of tests from the regression to test this issue: working_tpch1/misc/bug3759.sql
working_tpch1_compareLogOnly/fnJoin/bug5289.sql
working_tpch1_compareLogOnly/fnJoin/tpch05b.mod.sql
working_tpch1_compareLogOnly/fnJoin/tpch16b.sql
working_tpch1_compareLogOnly/fnJoin/tpch16.sql
working_tpch1_compareLogOnly/fnJoin/tpch18.sql
working_tpch1_compareLogOnly/fnJoin/tpch20b.sql
working_tpch1_compareLogOnly/misc/simple_outer.sql
 $acceptance criteria:$",,Roman,Roman,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-11-08 10:03:13,OUTER JOIN doesn't work,"These are a list of tests from the regression to test this issue: working_tpch1/misc/bug3759.sql
working_tpch1_compareLogOnly/fnJoin/bug5289.sql
working_tpch1_compareLogOnly/fnJoin/tpch05b.mod.sql
working_tpch1_compareLogOnly/fnJoin/tpch16b.sql
working_tpch1_compareLogOnly/fnJoin/tpch16.sql
working_tpch1_compareLogOnly/fnJoin/tpch18.sql
working_tpch1_compareLogOnly/fnJoin/tpch20b.sql
working_tpch1_compareLogOnly/misc/simple_outer.sql
",,0,0,0,0,0.0,"OUTER JOIN doesn't work $end$ These are a list of tests from the regression to test this issue: working_tpch1/misc/bug3759.sql
working_tpch1_compareLogOnly/fnJoin/bug5289.sql
working_tpch1_compareLogOnly/fnJoin/tpch05b.mod.sql
working_tpch1_compareLogOnly/fnJoin/tpch16b.sql
working_tpch1_compareLogOnly/fnJoin/tpch16.sql
working_tpch1_compareLogOnly/fnJoin/tpch18.sql
working_tpch1_compareLogOnly/fnJoin/tpch20b.sql
working_tpch1_compareLogOnly/misc/simple_outer.sql
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,10,1,0.1,1,0.1,1,0.1,1,0.1,1,0.1
235,MCOL-3602,Sub-Task,MCOL,2019-11-08 10:05:40,,0,IN + correlated subquery,"Here is the test to check:
working_tpch1/misc/bug4827.sql",,"IN + correlated subquery $end$ Here is the test to check:
working_tpch1/misc/bug4827.sql $acceptance criteria:$",,Roman,Roman,Major,9,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-11-08 10:05:40,IN + correlated subquery,"Here is the test to check:
working_tpch1/misc/bug4827.sql",,0,0,0,0,0.0,"IN + correlated subquery $end$ Here is the test to check:
working_tpch1/misc/bug4827.sql $acceptance criteria:$",0,0,0,0,0,0,0,0.0,11,1,0.0909091,1,0.0909091,1,0.0909091,1,0.0909091,1,0.0909091
236,MCOL-3603,Sub-Task,MCOL,2019-11-08 10:06:41,,0,IN + noncorrelated subquery,"Here is the list to test:
working_tpch1/misc/bug3911.sql
working_tpch1/misc/bug3932.sql
working_tpch1/misc/bug4215.sql
working_tpch1/misc/bug4757.sql
working_tpch1/misc/fluffycat.sql",,"IN + noncorrelated subquery $end$ Here is the list to test:
working_tpch1/misc/bug3911.sql
working_tpch1/misc/bug3932.sql
working_tpch1/misc/bug4215.sql
working_tpch1/misc/bug4757.sql
working_tpch1/misc/fluffycat.sql $acceptance criteria:$",,Roman,Roman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-11-08 10:06:41,IN + noncorrelated subquery,"Here is the list to test:
working_tpch1/misc/bug3911.sql
working_tpch1/misc/bug3932.sql
working_tpch1/misc/bug4215.sql
working_tpch1/misc/bug4757.sql
working_tpch1/misc/fluffycat.sql",,0,0,0,0,0.0,"IN + noncorrelated subquery $end$ Here is the list to test:
working_tpch1/misc/bug3911.sql
working_tpch1/misc/bug3932.sql
working_tpch1/misc/bug4215.sql
working_tpch1/misc/bug4757.sql
working_tpch1/misc/fluffycat.sql $acceptance criteria:$",0,0,0,0,0,0,0,0.0,12,1,0.0833333,1,0.0833333,1,0.0833333,1,0.0833333,1,0.0833333
237,MCOL-3606,New Feature,MCOL,2019-11-09 11:20:29,,0,ColumnStore should install in generic paths,"Binaries need to go into standard binary directories, some will need to be renamed so as not to conflict. Libraries will either need to go into the standard library directory or MariaDB's library directory. ColumnStore data should go in /var/lib/columnstore.",,"ColumnStore should install in generic paths $end$ Binaries need to go into standard binary directories, some will need to be renamed so as not to conflict. Libraries will either need to go into the standard library directory or MariaDB's library directory. ColumnStore data should go in /var/lib/columnstore. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-11-09 11:20:29,ColumnStore should install in generic paths,"Binaries need to go into standard binary directories, some will need to be renamed so as not to conflict. Libraries will either need to go into the standard library directory or MariaDB's library directory. ColumnStore data should go in /var/lib/columnstore.",,0,0,0,0,0.0,"ColumnStore should install in generic paths $end$ Binaries need to go into standard binary directories, some will need to be renamed so as not to conflict. Libraries will either need to go into the standard library directory or MariaDB's library directory. ColumnStore data should go in /var/lib/columnstore. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,86,8,0.0930233,5,0.0581395,3,0.0348837,2,0.0232558,2,0.0232558
238,MCOL-3607,Task,MCOL,2019-11-12 11:19:26,,0,ERROR 1815 with log10 and aggregation functions ,"To reproduce


{code:java}
create table log10test (a double) engine=columnstore;
insert into log10test (a) values(123);
select log10(sum(a)) from log10test;
ERROR 1815 (HY000): Internal error: log10: datatype of long double
{code}
",,"ERROR 1815 with log10 and aggregation functions  $end$ To reproduce


{code:java}
create table log10test (a double) engine=columnstore;
insert into log10test (a) values(123);
select log10(sum(a)) from log10test;
ERROR 1815 (HY000): Internal error: log10: datatype of long double
{code}
 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,13,,0,5,1,1,0,0,0,,0,850,4,0,0,2019-11-12 17:26:40,ERROR 1815 with log10 and aggregation functions ,"To reproduce


{code:java}
create table log10test (a double) engine=columnstore;
insert into log10test (a) values(123);
select log10(sum(a)) from log10test;
ERROR 1815 (HY000): Internal error: log10: datatype of long double
{code}
",,0,0,0,0,0.0,"ERROR 1815 with log10 and aggregation functions  $end$ To reproduce


{code:java}
create table log10test (a double) engine=columnstore;
insert into log10test (a) values(123);
select log10(sum(a)) from log10test;
ERROR 1815 (HY000): Internal error: log10: datatype of long double
{code}
 $acceptance criteria:$",0,0,0,0,0,0,0,6.11667,2,1,0.5,1,0.5,1,0.5,1,0.5,0,0.0
239,MCOL-3609,Sub-Task,MCOL,2019-11-12 14:30:39,,1,Create a framework for wide data types.,Implement a framework to support data types wider then 8 bytes.,,Create a framework for wide data types. $end$ Implement a framework to support data types wider then 8 bytes. $acceptance criteria:$,10.0,Roman,Roman,Minor,3,,0,2,0,16,3,0,0,,3,850,2,0,0,2019-11-12 14:30:39,Create a framework for wide data types.,Implement a framework to support data types wider then 8 bytes.,,0,0,0,0,0.0,Create a framework for wide data types. $end$ Implement a framework to support data types wider then 8 bytes. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,13,1,0.0769231,1,0.0769231,1,0.0769231,1,0.0769231,1,0.0769231
240,MCOL-3612,Task,MCOL,2019-11-13 21:16:42,,0,Merge develop-1.2 into develop,The linked tickets will be merged up from 1.2 -> develop.,,Merge develop-1.2 into develop $end$ The linked tickets will be merged up from 1.2 -> develop. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,3,2,3,1,0,0,0,,0,850,2,0,0,2019-11-13 21:31:53,Merge develop-1.2 into develop,The linked tickets will be merged up from 1.2 -> develop.,,0,0,0,0,0.0,Merge develop-1.2 into develop $end$ The linked tickets will be merged up from 1.2 -> develop. $acceptance criteria:$,0,0,0,0,0,0,0,0.25,87,8,0.091954,5,0.0574713,3,0.0344828,2,0.0229885,2,0.0229885
241,MCOL-3624,Task,MCOL,2019-11-25 08:10:21,MENT-124,0,Move jemalloc to LD_PRELOAD,"We won't be compiling MariaDB Server any more so we can't link jemalloc at build time as this would cause issues with our plugin. We need to switch to LD_PRELOAD for the ColumnStore binaries (particularly ExeMgr and PrimProc).

As a side-effect this should also fix issues with jemalloc 5.x used in CentOS 8 and other newer platforms.",,"Move jemalloc to LD_PRELOAD $end$ We won't be compiling MariaDB Server any more so we can't link jemalloc at build time as this would cause issues with our plugin. We need to switch to LD_PRELOAD for the ColumnStore binaries (particularly ExeMgr and PrimProc).

As a side-effect this should also fix issues with jemalloc 5.x used in CentOS 8 and other newer platforms. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,11,,1,2,1,1,0,0,0,,0,850,2,0,0,2019-12-05 14:22:32,Move jemalloc to LD_PRELOAD,"We won't be compiling MariaDB Server any more so we can't link jemalloc at build time as this would cause issues with our plugin. We need to switch to LD_PRELOAD for the ColumnStore binaries (particularly ExeMgr and PrimProc).

As a side-effect this should also fix issues with jemalloc 5.x used in CentOS 8 and other newer platforms.",,0,0,0,0,0.0,"Move jemalloc to LD_PRELOAD $end$ We won't be compiling MariaDB Server any more so we can't link jemalloc at build time as this would cause issues with our plugin. We need to switch to LD_PRELOAD for the ColumnStore binaries (particularly ExeMgr and PrimProc).

As a side-effect this should also fix issues with jemalloc 5.x used in CentOS 8 and other newer platforms. $acceptance criteria:$",0,0,0,0,0,0,0,246.2,88,8,0.0909091,5,0.0568182,3,0.0340909,2,0.0227273,2,0.0227273
242,MCOL-3625,Task,MCOL,2019-11-25 09:19:55,MENT-124,0,Rename packages,Package names should align with MariaDB's when built. The information to do this can probably be obtained from the parent CMake.,,Rename packages $end$ Package names should align with MariaDB's when built. The information to do this can probably be obtained from the parent CMake. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-12-05 14:22:49,Rename packages,Package names should align with MariaDB's when built. The information to do this can probably be obtained from the parent CMake.,,0,0,0,0,0.0,Rename packages $end$ Package names should align with MariaDB's when built. The information to do this can probably be obtained from the parent CMake. $acceptance criteria:$,0,0,0,0,0,0,0,245.033,89,8,0.0898876,5,0.0561798,3,0.0337079,2,0.0224719,2,0.0224719
243,MCOL-3626,Task,MCOL,2019-11-25 09:21:52,MENT-124,0,Kill mcsmysql prompt,"postConfigure echos:

""Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console""

MariaDB want us to remove this alias.",,"Kill mcsmysql prompt $end$ postConfigure echos:

""Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console""

MariaDB want us to remove this alias. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-12-05 14:23:04,Kill mcsmysql prompt,"postConfigure echos:

""Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console""

MariaDB want us to remove this alias.",,0,0,0,0,0.0,"Kill mcsmysql prompt $end$ postConfigure echos:

""Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console""

MariaDB want us to remove this alias. $acceptance criteria:$",0,0,0,0,0,0,0,245.017,90,8,0.0888889,5,0.0555556,3,0.0333333,2,0.0222222,2,0.0222222
244,MCOL-3627,Task,MCOL,2019-11-25 09:25:51,MENT-124,0,Rename library binary,Instead of libcalmysql.so it should be something like ha_columnstore.so.,,Rename library binary $end$ Instead of libcalmysql.so it should be something like ha_columnstore.so. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-12-05 14:23:21,Rename library binary,Instead of libcalmysql.so it should be something like ha_columnstore.so.,,0,0,0,0,0.0,Rename library binary $end$ Instead of libcalmysql.so it should be something like ha_columnstore.so. $acceptance criteria:$,0,0,0,0,0,0,0,244.95,91,8,0.0879121,5,0.0549451,3,0.032967,2,0.021978,2,0.021978
245,MCOL-3628,Task,MCOL,2019-11-25 11:57:27,MENT-124,0,Unifi plugins information_schema plugins into handler plugin,The information_schema plugins should be unified in the handler plugin to create one library.,,Unifi plugins information_schema plugins into handler plugin $end$ The information_schema plugins should be unified in the handler plugin to create one library. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-12-05 14:23:35,Unifi plugins information_schema plugins into handler plugin,The information_schema plugins should be unified in the handler plugin to create one library.,,0,0,0,0,0.0,Unifi plugins information_schema plugins into handler plugin $end$ The information_schema plugins should be unified in the handler plugin to create one library. $acceptance criteria:$,0,0,0,0,0,0,0,242.433,92,8,0.0869565,5,0.0543478,3,0.0326087,2,0.0217391,2,0.0217391
246,MCOL-3630,Task,MCOL,2019-11-27 09:40:56,,0,"Remove ""columnstore start"" for non PM1 message",This is now an unneeded step,,"Remove ""columnstore start"" for non PM1 message $end$ This is now an unneeded step $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,0,1,0,0,0,,0,850,2,0,0,2019-12-05 16:51:59,"Remove ""columnstore start"" for non PM1 message",This is now an unneeded step,,0,0,0,0,0.0,"Remove ""columnstore start"" for non PM1 message $end$ This is now an unneeded step $acceptance criteria:$",0,0,0,0,0,0,0,199.183,93,8,0.0860215,5,0.0537634,3,0.0322581,2,0.0215054,2,0.0215054
247,MCOL-3631,Sub-Task,MCOL,2019-11-27 11:31:40,,0,regr_ tests returns doubles with higher precision in 1.4 then in 1.2,All current failures in regr_ tests happen b/c CS now returns higher precion double. Need to explain why and replace the tests results if needed.,,regr_ tests returns doubles with higher precision in 1.4 then in 1.2 $end$ All current failures in regr_ tests happen b/c CS now returns higher precion double. Need to explain why and replace the tests results if needed. $acceptance criteria:$,,Roman,Roman,Minor,7,,1,4,1,7,0,0,0,,0,850,4,0,0,2019-11-27 11:31:40,regr_ tests returns doubles with higher precision in 1.4 then in 1.2,All current failures in regr_ tests happen b/c CS now returns higher precion double. Need to explain why and replace the tests results if needed.,,0,0,0,0,0.0,regr_ tests returns doubles with higher precision in 1.4 then in 1.2 $end$ All current failures in regr_ tests happen b/c CS now returns higher precion double. Need to explain why and replace the tests results if needed. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,14,1,0.0714286,1,0.0714286,1,0.0714286,1,0.0714286,1,0.0714286
248,MCOL-3632,Sub-Task,MCOL,2019-11-27 11:34:00,,0,windowFunctions failures,"17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO.",,"windowFunctions failures $end$ 17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO. $acceptance criteria:$",,Roman,Roman,Major,21,,0,5,0,7,0,0,0,,0,850,5,0,0,2019-11-27 11:34:00,windowFunctions failures,"17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO.",,0,0,0,0,0.0,"windowFunctions failures $end$ 17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,15,1,0.0666667,1,0.0666667,1,0.0666667,1,0.0666667,1,0.0666667
249,MCOL-3647,Task,MCOL,2019-12-04 14:57:18,,0,Merge develop-1.2 into develop,The linked tickets will be merged up from 1.2 -> develop,,Merge develop-1.2 into develop $end$ The linked tickets will be merged up from 1.2 -> develop $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,8,,3,2,4,1,0,0,0,,0,850,2,0,0,2019-12-04 15:10:48,Merge develop-1.2 into develop,The linked tickets will be merged up from 1.2 -> develop,,0,0,0,0,0.0,Merge develop-1.2 into develop $end$ The linked tickets will be merged up from 1.2 -> develop $acceptance criteria:$,0,0,0,0,0,0,0,0.216667,94,8,0.0851064,5,0.0531915,3,0.0319149,2,0.0212766,2,0.0212766
250,MCOL-366,Task,MCOL,2016-10-17 22:12:51,,0,regression test improvements,"The following fixes would make using the regression test less painful for first time users:
- the checked in file for referenceSystem should contain none rather than srvhill01
- test001.sh should use a single append in the queryTester run for working_tpch1
- REGRESSION_TEST_INSTALL_DIR should not need to be set, all scripts can infer this where needed.",,"regression test improvements $end$ The following fixes would make using the regression test less painful for first time users:
- the checked in file for referenceSystem should contain none rather than srvhill01
- test001.sh should use a single append in the queryTester run for working_tpch1
- REGRESSION_TEST_INSTALL_DIR should not need to be set, all scripts can infer this where needed. $acceptance criteria:$",,David Thompson,David Thompson,Major,7,,0,4,0,2,0,0,0,,0,850,2,0,0,2016-10-19 16:26:59,regression test improvements,"The following fixes would make using the regression test less painful for first time users:
- the checked in file for referenceSystem should contain none rather than srvhill01
- test001.sh should use a single append in the queryTester run for working_tpch1
- REGRESSION_TEST_INSTALL_DIR should not need to be set, all scripts can infer this where needed.",,0,0,0,0,0.0,"regression test improvements $end$ The following fixes would make using the regression test less painful for first time users:
- the checked in file for referenceSystem should contain none rather than srvhill01
- test001.sh should use a single append in the queryTester run for working_tpch1
- REGRESSION_TEST_INSTALL_DIR should not need to be set, all scripts can infer this where needed. $acceptance criteria:$",0,0,0,0,0,0,1,42.2333,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
251,MCOL-3686,Task,MCOL,2019-12-23 17:58:43,,0,Improve installation procedure and reduce SQL limitations,"unconditional part of post-install and postConfigure should be run automatically by package manager

uninstallation should shutdown the ""system"" and stop all columnstore processes

purge should delete all data

should use systemd/sysv as the mariadb server

ideally should work like a normal plugin

ideally shouldn't ssh into other hosts for cluster management

From the meeting in Frankfurt:
- ALTER TABLE ENGINE=COLUMNSTORE
- CREATE TABLE AS SELECT
- COLLATION should not result in an error, a warning should be added
- remove auto_increment implementation for ColumnStore
-- Test SEQUENCES to replace this",,"Improve installation procedure and reduce SQL limitations $end$ unconditional part of post-install and postConfigure should be run automatically by package manager

uninstallation should shutdown the ""system"" and stop all columnstore processes

purge should delete all data

should use systemd/sysv as the mariadb server

ideally should work like a normal plugin

ideally shouldn't ssh into other hosts for cluster management

From the meeting in Frankfurt:
- ALTER TABLE ENGINE=COLUMNSTORE
- CREATE TABLE AS SELECT
- COLLATION should not result in an error, a warning should be added
- remove auto_increment implementation for ColumnStore
-- Test SEQUENCES to replace this $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,26,,0,4,0,6,0,3,10,,0,850,2,3,0,2020-01-20 17:35:50,Improve installation procedure and reduce SQL limitations,"unconditional part of post-install and postConfigure should be run automatically by package manager

uninstallation should shutdown the ""system"" and stop all columnstore processes

purge should delete all data

should use systemd/sysv as the mariadb server

ideally should work like a normal plugin

ideally shouldn't ssh into other hosts for cluster management

From the meeting in Frankfurt:
- ALTER TABLE ENGINE=COLUMNSTORE
- CREATE TABLE AS SELECT
- COLLATION should not result in an error, a warning should be added
- remove auto_increment implementation for ColumnStore
-- Test SEQUENCES to replace this",,0,0,0,0,0.0,"Improve installation procedure and reduce SQL limitations $end$ unconditional part of post-install and postConfigure should be run automatically by package manager

uninstallation should shutdown the ""system"" and stop all columnstore processes

purge should delete all data

should use systemd/sysv as the mariadb server

ideally should work like a normal plugin

ideally shouldn't ssh into other hosts for cluster management

From the meeting in Frankfurt:
- ALTER TABLE ENGINE=COLUMNSTORE
- CREATE TABLE AS SELECT
- COLLATION should not result in an error, a warning should be added
- remove auto_increment implementation for ColumnStore
-- Test SEQUENCES to replace this $acceptance criteria:$",0,0,0,0,0,0,1,671.617,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
252,MCOL-3707,Sub-Task,MCOL,2020-01-03 16:54:06,,0,Run columnstore-post-install on combined RPM/DEB install,The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds.,,Run columnstore-post-install on combined RPM/DEB install $end$ The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,2,0,6,0,0,0,,0,850,2,0,0,2020-01-03 16:54:06,Run columnstore-post-install on combined RPM/DEB install,The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds.,,0,0,0,0,0.0,Run columnstore-post-install on combined RPM/DEB install $end$ The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,95,8,0.0842105,5,0.0526316,3,0.0315789,2,0.0210526,2,0.0210526
253,MCOL-3708,Sub-Task,MCOL,2020-01-03 16:55:59,,0,RPM/DEB removal should run mcsadmin shutdown,When the RPM/DEB packages for ColumnStore are removed they should execute a cluster shutdown.,,RPM/DEB removal should run mcsadmin shutdown $end$ When the RPM/DEB packages for ColumnStore are removed they should execute a cluster shutdown. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,12,,0,4,0,6,0,0,0,,0,850,4,0,0,2020-01-03 16:55:59,RPM/DEB removal should run mcsadmin shutdown,When the RPM/DEB packages for ColumnStore are removed they should execute a cluster shutdown.,,0,0,0,0,0.0,RPM/DEB removal should run mcsadmin shutdown $end$ When the RPM/DEB packages for ColumnStore are removed they should execute a cluster shutdown. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,96,8,0.0833333,5,0.0520833,3,0.03125,2,0.0208333,2,0.0208333
254,MCOL-3709,Sub-Task,MCOL,2020-01-03 16:57:32,,0,Package purge should remove /var/lib/columnstore,APT Purge is designed to remove all data for the package. The debian scripts in the server should be updated to do this for ColumnStore.,,Package purge should remove /var/lib/columnstore $end$ APT Purge is designed to remove all data for the package. The debian scripts in the server should be updated to do this for ColumnStore. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Critical,7,,0,0,0,6,0,0,0,,0,850,0,0,0,2020-01-03 16:57:32,Package purge should remove /var/lib/columnstore,APT Purge is designed to remove all data for the package. The debian scripts in the server should be updated to do this for ColumnStore.,,0,0,0,0,0.0,Package purge should remove /var/lib/columnstore $end$ APT Purge is designed to remove all data for the package. The debian scripts in the server should be updated to do this for ColumnStore. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,97,8,0.0824742,5,0.0515464,3,0.0309278,2,0.0206186,2,0.0206186
255,MCOL-3718,Sub-Task,MCOL,2020-01-10 08:59:14,,0,Use MariaDB system instead of mysql-Columnstore,When ColumnStore is running and MariaDB's systemd stop is used mysql-Columnstore will restart it. We should use MariaDB's systemd for start/stop instead.,,Use MariaDB system instead of mysql-Columnstore $end$ When ColumnStore is running and MariaDB's systemd stop is used mysql-Columnstore will restart it. We should use MariaDB's systemd for start/stop instead. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,19,,1,4,1,6,0,2,0,,0,850,4,0,0,2020-01-10 08:59:14,Use MariaDB system instead of columnstore_run,When ColumnStore is running and MariaDB's systemd stop is used columnstore_run will restart it. We should use MariaDB's systemd for start/stop instead.,,1,1,0,4,0.0645161,Use MariaDB system instead of columnstore_run $end$ When ColumnStore is running and MariaDB's systemd stop is used columnstore_run will restart it. We should use MariaDB's systemd for start/stop instead. $acceptance criteria:$,2,1,0,0,0,0,1,0.0,98,8,0.0816327,5,0.0510204,3,0.0306122,2,0.0204082,2,0.0204082
256,MCOL-3721,Sub-Task,MCOL,2020-01-10 11:53:17,,0,COLLATE used in DDL and ORDER BY should be allowed,When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings.,,COLLATE used in DDL and ORDER BY should be allowed $end$ When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,14,,0,3,0,6,0,0,0,,0,850,3,0,0,2020-01-10 11:53:17,COLLATE used in DDL and ORDER BY should be allowed,When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings.,,0,0,0,0,0.0,COLLATE used in DDL and ORDER BY should be allowed $end$ When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,99,9,0.0909091,5,0.050505,3,0.030303,2,0.020202,2,0.020202
257,MCOL-3731,Task,MCOL,2020-01-20 16:48:11,,1,Create OAM-less Docker Image,Take ownership of SkySQL Docker image and add scripts/patches to eliminate OAM.,,Create OAM-less Docker Image $end$ Take ownership of SkySQL Docker image and add scripts/patches to eliminate OAM. $acceptance criteria:$,2.0,Todd Stoffel,Todd Stoffel,Critical,9,,0,3,1,2,1,0,0,,1,850,3,0,0,2020-01-20 17:30:54,Create OAM-less Docker Image,Take ownership of SkySQL Docker image and add scripts/patches to eliminate OAM.,,0,0,0,0,0.0,Create OAM-less Docker Image $end$ Take ownership of SkySQL Docker image and add scripts/patches to eliminate OAM. $acceptance criteria:$,0,0,0,0,0,0,1,0.7,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
258,MCOL-3732,Task,MCOL,2020-01-20 16:50:36,,0,Remove All Outdated ColumnStore Docker Images,There are some lingering experiments and obsolete Docker projects related to ColumnStore in GitHub and DockerHub. We need to remove those and get control of the way ColumnStore is presented publicly.,,Remove All Outdated ColumnStore Docker Images $end$ There are some lingering experiments and obsolete Docker projects related to ColumnStore in GitHub and DockerHub. We need to remove those and get control of the way ColumnStore is presented publicly. $acceptance criteria:$,,Todd Stoffel,Todd Stoffel,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-01-20 18:18:05,Remove All Outdated ColumnStore Docker Images,There are some lingering experiments and obsolete Docker projects related to ColumnStore in GitHub and DockerHub. We need to remove those and get control of the way ColumnStore is presented publicly.,,0,0,0,0,0.0,Remove All Outdated ColumnStore Docker Images $end$ There are some lingering experiments and obsolete Docker projects related to ColumnStore in GitHub and DockerHub. We need to remove those and get control of the way ColumnStore is presented publicly. $acceptance criteria:$,0,0,0,0,0,0,0,1.45,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
259,MCOL-3733,Task,MCOL,2020-01-20 16:52:10,,0,Create Official DockerHub Image for ColumnStore,"This is our public image for use with production systems, demos, etc.  This needs to be owned and built by the ColumnStore engineering team.",,"Create Official DockerHub Image for ColumnStore $end$ This is our public image for use with production systems, demos, etc.  This needs to be owned and built by the ColumnStore engineering team. $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,9,,0,3,1,2,0,0,0,,0,850,3,0,0,2020-01-20 18:17:44,Create Official DockerHub Image for ColumnStore,"This is our public image for use with production systems, demos, etc.  This needs to be owned and built by the ColumnStore engineering team.",,0,0,0,0,0.0,"Create Official DockerHub Image for ColumnStore $end$ This is our public image for use with production systems, demos, etc.  This needs to be owned and built by the ColumnStore engineering team. $acceptance criteria:$",0,0,0,0,0,0,1,1.41667,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
260,MCOL-3741,Task,MCOL,2020-01-23 21:07:10,MCOL-4561,0,Change the error numbers from IDB-nnnn to MCS-nnnn,"As part of the branding effort, change the error messages to say MCS-nnnn rather than IDB-nnnn. None of the numbers need change",,"Change the error numbers from IDB-nnnn to MCS-nnnn $end$ As part of the branding effort, change the error messages to say MCS-nnnn rather than IDB-nnnn. None of the numbers need change $acceptance criteria:$",,David Hall,David Hall,Minor,27,,0,2,1,6,0,0,0,,0,850,2,0,0,2021-08-06 21:23:02,Change the error numbers from IDB-nnnn to MCS-nnnn,"As part of the branding effort, change the error messages to say MCS-nnnn rather than IDB-nnnn. None of the numbers need change",,0,0,0,0,0.0,"Change the error numbers from IDB-nnnn to MCS-nnnn $end$ As part of the branding effort, change the error messages to say MCS-nnnn rather than IDB-nnnn. None of the numbers need change $acceptance criteria:$",0,0,0,0,0,0,1,13464.2,15,1,0.0666667,1,0.0666667,1,0.0666667,1,0.0666667,0,0.0
261,MCOL-3743,New Feature,MCOL,2020-01-24 16:27:58,,0,postConfigure quick support of S3 storagemanager,"We need to add quick install flags for S3 storage to facilitate use with automation.

example:


{code:java}
postConfigure -qs --sm_endpoint=… --sm_bucket=… --sm_id=… --sm_key=… --sm_region=...
{code}
",,"postConfigure quick support of S3 storagemanager $end$ We need to add quick install flags for S3 storage to facilitate use with automation.

example:


{code:java}
postConfigure -qs --sm_endpoint=… --sm_bucket=… --sm_id=… --sm_key=… --sm_region=...
{code}
 $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Minor,24,,0,3,0,6,0,1,0,,0,850,3,0,0,2020-02-03 15:24:43,postConfigure quick support of S3 storagemanager,"We need to add quick install flags for S3 storage to facilitate use with automation.

example:


{code:java}
postConfigure -qs --sm_endpoint=… --sm_bucket=… --sm_id=… --sm_key=…
{code}
",,0,1,0,1,0.030303,"postConfigure quick support of S3 storagemanager $end$ We need to add quick install flags for S3 storage to facilitate use with automation.

example:


{code:java}
postConfigure -qs --sm_endpoint=… --sm_bucket=… --sm_id=… --sm_key=…
{code}
 $acceptance criteria:$",1,1,0,0,0,0,1,238.933,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
262,MCOL-3745,Task,MCOL,2020-01-27 22:09:10,,0,Investigate slow-down in DDL after a restart,"Todd noticed during testing that if he killed either controllernode or DMLProc, and let it restart, drop table would be occasionally be slow.  

Ben reproduced the problem and saw correlation with upstream bandwidth usage.  When upstream traffic stops, that's when the drop table stmt would return. 
 Presumably, SM is sending a lot of data to the cloud, which is blocking the ops for dropping a table.

Ben also couldn't reproduce it when using the 'fake cloud' module.

Need to investigate.",,"Investigate slow-down in DDL after a restart $end$ Todd noticed during testing that if he killed either controllernode or DMLProc, and let it restart, drop table would be occasionally be slow.  

Ben reproduced the problem and saw correlation with upstream bandwidth usage.  When upstream traffic stops, that's when the drop table stmt would return. 
 Presumably, SM is sending a lot of data to the cloud, which is blocking the ops for dropping a table.

Ben also couldn't reproduce it when using the 'fake cloud' module.

Need to investigate. $acceptance criteria:$",,Patrick LeBlanc,Todd Stoffel,Major,13,,0,3,1,3,0,1,0,,0,850,3,0,0,2020-01-27 22:09:10,Investigate slow-down in DDL after a restart,"Todd noticed during testing that if he killed either controllernode or DMLProc, and let it restart, drop table would be occasionally be slow.  

Ben reproduced the problem and saw correlation with upstream bandwidth usage.  Presumably, SM is sending a lot of data to the cloud presumably.  When upstream traffic stops, that's when the drop table stmt would return.

Ben also couldn't reproduce it when using the 'fake cloud' module.

Need to investigate.",,0,1,0,32,0.304878,"Investigate slow-down in DDL after a restart $end$ Todd noticed during testing that if he killed either controllernode or DMLProc, and let it restart, drop table would be occasionally be slow.  

Ben reproduced the problem and saw correlation with upstream bandwidth usage.  Presumably, SM is sending a lot of data to the cloud presumably.  When upstream traffic stops, that's when the drop table stmt would return.

Ben also couldn't reproduce it when using the 'fake cloud' module.

Need to investigate. $acceptance criteria:$",1,1,1,1,1,1,1,0.0,6,1,0.166667,1,0.166667,0,0.0,0,0.0,0,0.0
263,MCOL-3747,Task,MCOL,2020-01-28 18:18:41,,0,Regression in 1.4 working_ssb_compareLogOnly/sub/order_limit_sub,"working_ssb_compareLogOnly/sub/order_limit_sub, the second query gives the wrong answer:

select * from 
    (select lo_orderkey, count(*) 
     from lineorder where lo_orderkey in 
        (select * 
         from (select lo_orderkey 
	       from lineorder 
               join dateinfo 
               on lo_orderdate = d_datekey 
               where lo_orderkey <= 900
               group by lo_orderkey 
               order by sum(lo_ordtotalprice), lo_orderkey desc
               limit 5
               ) alias1 
         ) 
     group by lo_orderkey 
     order by 1, 2 asc 
     limit 4 
     ) alias2 
order by 2, 1 desc;

+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|           4 |        1 |
|           2 |        1 |
|           3 |        6 |
|           1 |        6 |
+-------------+----------+

In 1.2 we get the correct answer:
+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|         421 |        1 |
|         389 |        1 |
|         228 |        1 |
|         162 |        1 |
+-------------+----------+

I ran the interior queries separately and got correct answers. It doesn't seem to break until the final wrapper.
",,"Regression in 1.4 working_ssb_compareLogOnly/sub/order_limit_sub $end$ working_ssb_compareLogOnly/sub/order_limit_sub, the second query gives the wrong answer:

select * from 
    (select lo_orderkey, count(*) 
     from lineorder where lo_orderkey in 
        (select * 
         from (select lo_orderkey 
	       from lineorder 
               join dateinfo 
               on lo_orderdate = d_datekey 
               where lo_orderkey <= 900
               group by lo_orderkey 
               order by sum(lo_ordtotalprice), lo_orderkey desc
               limit 5
               ) alias1 
         ) 
     group by lo_orderkey 
     order by 1, 2 asc 
     limit 4 
     ) alias2 
order by 2, 1 desc;

+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|           4 |        1 |
|           2 |        1 |
|           3 |        6 |
|           1 |        6 |
+-------------+----------+

In 1.2 we get the correct answer:
+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|         421 |        1 |
|         389 |        1 |
|         228 |        1 |
|         162 |        1 |
+-------------+----------+

I ran the interior queries separately and got correct answers. It doesn't seem to break until the final wrapper.
 $acceptance criteria:$",,David Hall,David Hall,Blocker,15,,5,8,5,1,0,0,0,,0,850,5,0,0,2020-02-03 15:26:22,Regression in 1.4 working_ssb_compareLogOnly/sub/order_limit_sub,"working_ssb_compareLogOnly/sub/order_limit_sub, the second query gives the wrong answer:

select * from 
    (select lo_orderkey, count(*) 
     from lineorder where lo_orderkey in 
        (select * 
         from (select lo_orderkey 
	       from lineorder 
               join dateinfo 
               on lo_orderdate = d_datekey 
               where lo_orderkey <= 900
               group by lo_orderkey 
               order by sum(lo_ordtotalprice), lo_orderkey desc
               limit 5
               ) alias1 
         ) 
     group by lo_orderkey 
     order by 1, 2 asc 
     limit 4 
     ) alias2 
order by 2, 1 desc;

+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|           4 |        1 |
|           2 |        1 |
|           3 |        6 |
|           1 |        6 |
+-------------+----------+

In 1.2 we get the correct answer:
+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|         421 |        1 |
|         389 |        1 |
|         228 |        1 |
|         162 |        1 |
+-------------+----------+

I ran the interior queries separately and got correct answers. It doesn't seem to break until the final wrapper.
",,0,0,0,0,0.0,"Regression in 1.4 working_ssb_compareLogOnly/sub/order_limit_sub $end$ working_ssb_compareLogOnly/sub/order_limit_sub, the second query gives the wrong answer:

select * from 
    (select lo_orderkey, count(*) 
     from lineorder where lo_orderkey in 
        (select * 
         from (select lo_orderkey 
	       from lineorder 
               join dateinfo 
               on lo_orderdate = d_datekey 
               where lo_orderkey <= 900
               group by lo_orderkey 
               order by sum(lo_ordtotalprice), lo_orderkey desc
               limit 5
               ) alias1 
         ) 
     group by lo_orderkey 
     order by 1, 2 asc 
     limit 4 
     ) alias2 
order by 2, 1 desc;

+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|           4 |        1 |
|           2 |        1 |
|           3 |        6 |
|           1 |        6 |
+-------------+----------+

In 1.2 we get the correct answer:
+-------------+----------+
| lo_orderkey | count(*) |
+-------------+----------+
|         421 |        1 |
|         389 |        1 |
|         228 |        1 |
|         162 |        1 |
+-------------+----------+

I ran the interior queries separately and got correct answers. It doesn't seem to break until the final wrapper.
 $acceptance criteria:$",0,0,0,0,0,0,0,141.117,16,1,0.0625,1,0.0625,1,0.0625,1,0.0625,0,0.0
264,MCOL-3750,Task,MCOL,2020-01-29 18:25:21,,0,Make buildbot send the CS team the results from the nightly tests,"Right now BB is putting test results in the -build channel.  With the current process, the engineer has to remember to check it, then find the build of interest in a big list of builds that includes PR builds and nightly builds for every platform, then navigate BB's interface, etc etc etc.  It's all pointless if nobody remembers to check in the first place, but if even they do it's a pita.

BB will be more useful if it tells us how the nightly tests went.  To start, I'd like the C7 nightly builder for the develop branch to mail the CS mailing list its go.log file when it finishes.  If there is no go.log file, meaning it didn't even make it to that point, we need to know that as well.

The msg summary should be ""BB <distro> <branch> nightly results for <date>"".  If there's a go.log file, the msg body should be the go.log file.  If there's no go.log file, the msg body should say ""Test failed prior to running go.sh"".

We'll start there.  If this turns out to be useful, we can think about improvements.  If not we can scrap it.
",,"Make buildbot send the CS team the results from the nightly tests $end$ Right now BB is putting test results in the -build channel.  With the current process, the engineer has to remember to check it, then find the build of interest in a big list of builds that includes PR builds and nightly builds for every platform, then navigate BB's interface, etc etc etc.  It's all pointless if nobody remembers to check in the first place, but if even they do it's a pita.

BB will be more useful if it tells us how the nightly tests went.  To start, I'd like the C7 nightly builder for the develop branch to mail the CS mailing list its go.log file when it finishes.  If there is no go.log file, meaning it didn't even make it to that point, we need to know that as well.

The msg summary should be ""BB <distro> <branch> nightly results for <date>"".  If there's a go.log file, the msg body should be the go.log file.  If there's no go.log file, the msg body should say ""Test failed prior to running go.sh"".

We'll start there.  If this turns out to be useful, we can think about improvements.  If not we can scrap it.
 $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,15,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-02-18 17:13:47,Make buildbot send the CS team the results from the nightly tests,"Right now BB is putting test results in the -build channel.  With the current process, the engineer has to remember to check it, then find the build of interest in a big list of builds that includes PR builds and nightly builds for every platform, then navigate BB's interface, etc etc etc.  It's all pointless if nobody remembers to check in the first place, but if even they do it's a pita.

BB will be more useful if it tells us how the nightly tests went.  To start, I'd like the C7 nightly builder for the develop branch to mail the CS mailing list its go.log file when it finishes.  If there is no go.log file, meaning it didn't even make it to that point, we need to know that as well.

The msg summary should be ""BB <distro> <branch> nightly results for <date>"".  If there's a go.log file, the msg body should be the go.log file.  If there's no go.log file, the msg body should say ""Test failed prior to running go.sh"".

We'll start there.  If this turns out to be useful, we can think about improvements.  If not we can scrap it.
",,0,0,0,0,0.0,"Make buildbot send the CS team the results from the nightly tests $end$ Right now BB is putting test results in the -build channel.  With the current process, the engineer has to remember to check it, then find the build of interest in a big list of builds that includes PR builds and nightly builds for every platform, then navigate BB's interface, etc etc etc.  It's all pointless if nobody remembers to check in the first place, but if even they do it's a pita.

BB will be more useful if it tells us how the nightly tests went.  To start, I'd like the C7 nightly builder for the develop branch to mail the CS mailing list its go.log file when it finishes.  If there is no go.log file, meaning it didn't even make it to that point, we need to know that as well.

The msg summary should be ""BB <distro> <branch> nightly results for <date>"".  If there's a go.log file, the msg body should be the go.log file.  If there's no go.log file, the msg body should say ""Test failed prior to running go.sh"".

We'll start there.  If this turns out to be useful, we can think about improvements.  If not we can scrap it.
 $acceptance criteria:$",0,0,0,0,0,0,1,478.8,7,2,0.285714,2,0.285714,1,0.142857,1,0.142857,1,0.142857
265,MCOL-3753,Sub-Task,MCOL,2020-02-03 17:56:56,,0,Full Decimal Research,,,Full Decimal Research $end$ $acceptance criteria:$,,David Hall,David Hall,Major,3,,0,0,0,16,0,0,0,,0,850,0,0,0,2020-02-03 17:56:56,Full Decimal Research,,,0,0,0,0,0.0,Full Decimal Research $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,17,1,0.0588235,1,0.0588235,1,0.0588235,1,0.0588235,0,0.0
266,MCOL-3756,Sub-Task,MCOL,2020-02-04 10:22:55,,0,Implement ISTRUE()  function,"The test working_tpch1_compareLogOnly/view/mts_view.60.sql fails with
{noformat}
MariaDB [test]> insert into table_24532 values (0, 0, 0, 0);
Query OK, 1 row affected (0.540 sec)

MariaDB [test]> select * from view_24532_a;
 ERROR 1178 (42000): The storage engine for the table doesn't support IDB-1001: Function 'istrue' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function.
{noformat}",,"Implement ISTRUE()  function $end$ The test working_tpch1_compareLogOnly/view/mts_view.60.sql fails with
{noformat}
MariaDB [test]> insert into table_24532 values (0, 0, 0, 0);
Query OK, 1 row affected (0.540 sec)

MariaDB [test]> select * from view_24532_a;
 ERROR 1178 (42000): The storage engine for the table doesn't support IDB-1001: Function 'istrue' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function.
{noformat} $acceptance criteria:$",,Roman,Roman,Major,10,,0,2,0,7,0,0,0,,0,850,2,0,0,2020-02-04 10:22:55,Implement ISTRUE()  function,"The test working_tpch1_compareLogOnly/view/mts_view.60.sql fails with
{noformat}
MariaDB [test]> insert into table_24532 values (0, 0, 0, 0);
Query OK, 1 row affected (0.540 sec)

MariaDB [test]> select * from view_24532_a;
 ERROR 1178 (42000): The storage engine for the table doesn't support IDB-1001: Function 'istrue' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function.
{noformat}",,0,0,0,0,0.0,"Implement ISTRUE()  function $end$ The test working_tpch1_compareLogOnly/view/mts_view.60.sql fails with
{noformat}
MariaDB [test]> insert into table_24532 values (0, 0, 0, 0);
Query OK, 1 row affected (0.540 sec)

MariaDB [test]> select * from view_24532_a;
 ERROR 1178 (42000): The storage engine for the table doesn't support IDB-1001: Function 'istrue' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function.
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,0.0,16,1,0.0625,1,0.0625,1,0.0625,1,0.0625,1,0.0625
267,MCOL-3757,Sub-Task,MCOL,2020-02-04 10:30:54,,0,"CS does not support LIMIT in correlated subqueries, but gives erroneous results","CS previously returned an error for some queries that failed previously:
{noformat}
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;

CREATE VIEW v1 AS
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;
SELECT * FROM v1;

-ERROR 1815 (HY000) at line 96: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
-ERROR 1815 (HY000) at line 110: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
{noformat}

Now these queries work. We need to investigate the scope of this unexpected free feature.

",,"CS does not support LIMIT in correlated subqueries, but gives erroneous results $end$ CS previously returned an error for some queries that failed previously:
{noformat}
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;

CREATE VIEW v1 AS
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;
SELECT * FROM v1;

-ERROR 1815 (HY000) at line 96: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
-ERROR 1815 (HY000) at line 110: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
{noformat}

Now these queries work. We need to investigate the scope of this unexpected free feature.

 $acceptance criteria:$",,Roman,Roman,Minor,12,,0,6,0,7,0,1,0,,0,850,6,0,0,2020-02-04 10:30:54,Confirm that CS now supports LIMIT in correlated subqueries.,"CS previously returned an error for some queries that failed previously:
{noformat}
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;

CREATE VIEW v1 AS
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;
SELECT * FROM v1;

-ERROR 1815 (HY000) at line 96: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
-ERROR 1815 (HY000) at line 110: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
{noformat}

Now these queries work. We need to investigate the scope of this unexpected free feature.

",,1,0,0,13,0.0704225,"Confirm that CS now supports LIMIT in correlated subqueries. $end$ CS previously returned an error for some queries that failed previously:
{noformat}
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;

CREATE VIEW v1 AS
SELECT DISTINCT t2.gid AS lgid,
                (SELECT t1.name FROM t1, t2
                   WHERE t1.lid  = t2.lid AND t2.gid = lgid
                     ORDER BY t2.dt DESC LIMIT 1
                ) as clid
  FROM t2;
SELECT * FROM v1;

-ERROR 1815 (HY000) at line 96: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
-ERROR 1815 (HY000) at line 110: Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
{noformat}

Now these queries work. We need to investigate the scope of this unexpected free feature.

 $acceptance criteria:$",1,1,1,1,0,0,1,0.0,17,1,0.0588235,1,0.0588235,1,0.0588235,1,0.0588235,1,0.0588235
268,MCOL-3784,Task,MCOL,2020-02-11 20:05:19,,0,Add include <boost/scoped_ptr.hpp> to rowaggregation.h,"Somehow, Ubuntu suddenly doesn't include boost/scoped_ptr.hpp implicitly like it had. Need to add it everywhere.",,"Add include <boost/scoped_ptr.hpp> to rowaggregation.h $end$ Somehow, Ubuntu suddenly doesn't include boost/scoped_ptr.hpp implicitly like it had. Need to add it everywhere. $acceptance criteria:$",,David Hall,David Hall,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-02-11 20:05:49,Add include <boost/scoped_ptr.hpp> to rowaggregation.h,"Somehow, Ubuntu suddenly doesn't include boost/scoped_ptr.hpp implicitly like it had. Need to add it everywhere.",,0,0,0,0,0.0,"Add include <boost/scoped_ptr.hpp> to rowaggregation.h $end$ Somehow, Ubuntu suddenly doesn't include boost/scoped_ptr.hpp implicitly like it had. Need to add it everywhere. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,18,1,0.0555556,1,0.0555556,1,0.0555556,1,0.0555556,0,0.0
269,MCOL-3821,Sub-Task,MCOL,2020-02-20 21:52:22,,0,CLONE - windowFunctions failures,"17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO.",,"CLONE - windowFunctions failures $end$ 17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,7,,0,1,0,7,0,0,0,,0,850,1,0,0,2020-02-20 21:52:22,CLONE - windowFunctions failures,"17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO.",,0,0,0,0,0.0,"CLONE - windowFunctions failures $end$ 17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,8,2,0.25,2,0.25,1,0.125,1,0.125,1,0.125
270,MCOL-3836,Epic,MCOL,2020-02-24 21:35:03,,0,Columnstore OAM replacement,An epic to hold tasks for OAM replacement,,Columnstore OAM replacement $end$ An epic to hold tasks for OAM replacement $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,43,,8,1,16,19,0,2,0,,0,850,1,0,0,2020-03-16 18:43:15,OAM replacement,This ticket includes subtasks for replacing OAM.,,1,1,0,14,0.666667,OAM replacement $end$ This ticket includes subtasks for replacing OAM. $acceptance criteria:$,2,1,1,1,0,0,1,501.133,9,2,0.222222,2,0.222222,1,0.111111,1,0.111111,1,0.111111
271,MCOL-385,Task,MCOL,2016-11-01 14:45:35,,0,10.1.19 Merge,10.1.19 is planned to come out on November 3rd. Merge once it comes out.,,10.1.19 Merge $end$ 10.1.19 is planned to come out on November 3rd. Merge once it comes out. $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,11,,0,3,0,3,0,0,0,,0,850,3,0,0,2016-11-07 19:08:24,10.1.19 Merge,10.1.19 is planned to come out on November 3rd. Merge once it comes out.,,0,0,0,0,0.0,10.1.19 Merge $end$ 10.1.19 is planned to come out on November 3rd. Merge once it comes out. $acceptance criteria:$,0,0,0,0,0,0,1,148.367,34,8,0.235294,3,0.0882353,3,0.0882353,1,0.0294118,1,0.0294118
272,MCOL-3852,Sub-Task,MCOL,2020-03-02 16:42:52,,0,CLONE - ALTER TABLE conversion to columnstore fails,"mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
",,"CLONE - ALTER TABLE conversion to columnstore fails $end$ mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
 $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,9,,1,2,1,6,0,0,0,,0,850,2,0,0,2020-03-02 16:42:52,CLONE - ALTER TABLE conversion to columnstore fails,"mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
",,0,0,0,0,0.0,"CLONE - ALTER TABLE conversion to columnstore fails $end$ mysql> create table t2(c1 int);
Query OK, 0 rows affected (0.01 sec)

mysql> alter table t2 engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,10,3,0.3,3,0.3,2,0.2,1,0.1,1,0.1
273,MCOL-3854,Task,MCOL,2020-03-02 16:47:32,,0,CLONE - Run columnstore-post-install on combined RPM/DEB install,The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds.,,CLONE - Run columnstore-post-install on combined RPM/DEB install $end$ The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds. $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,12,,1,2,1,3,0,0,0,,0,850,2,0,0,2020-03-02 16:47:32,CLONE - Run columnstore-post-install on combined RPM/DEB install,The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds.,,0,0,0,0,0.0,CLONE - Run columnstore-post-install on combined RPM/DEB install $end$ The RPM build in storage/columnstore and the DEB build in debian directory of the server tree need updating to trigger columnstore-post-install as they do in the separated package builds. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,11,3,0.272727,3,0.272727,2,0.181818,1,0.0909091,1,0.0909091
274,MCOL-3856,Sub-Task,MCOL,2020-03-02 16:50:17,,0,CLONE - windowFunctions failures,"17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO.",,"CLONE - windowFunctions failures $end$ 17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Major,8,,0,2,0,7,0,0,0,,0,850,2,0,0,2020-03-02 16:50:17,CLONE - windowFunctions failures,"17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO.",,0,0,0,0,0.0,"CLONE - windowFunctions failures $end$ 17/19 failures happen b/c CS 1.4 returns the result. 1.2 returns errors though. Need to compare results with external database and correct the reference files.
The query working_tpch1_compareLogOnly/windowFunctions/q0035.sql need additional research. getSelectPlan doesn't handle the query properly IMHO. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,12,3,0.25,3,0.25,2,0.166667,1,0.0833333,1,0.0833333
275,MCOL-386,New Feature,MCOL,2016-11-02 11:23:35,,0,postConfigure should show cause of write error.,"in helpers.cpp the writeConfig try...catch block ignores the error message and just returns when there is an error. An error here could be for many different reasons including XML issues. We should change this to something like:

{noformat}
catch(const std::exception &exc)
{
    std::cerr << exc.what() << std::endl;
}
{noformat}
",,"postConfigure should show cause of write error. $end$ in helpers.cpp the writeConfig try...catch block ignores the error message and just returns when there is an error. An error here could be for many different reasons including XML issues. We should change this to something like:

{noformat}
catch(const std::exception &exc)
{
    std::cerr << exc.what() << std::endl;
}
{noformat}
 $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,12,,0,2,0,3,0,0,0,,0,850,2,0,0,2016-11-02 18:09:49,postConfigure should show cause of write error.,"in helpers.cpp the writeConfig try...catch block ignores the error message and just returns when there is an error. An error here could be for many different reasons including XML issues. We should change this to something like:

{noformat}
catch(const std::exception &exc)
{
    std::cerr << exc.what() << std::endl;
}
{noformat}
",,0,0,0,0,0.0,"postConfigure should show cause of write error. $end$ in helpers.cpp the writeConfig try...catch block ignores the error message and just returns when there is an error. An error here could be for many different reasons including XML issues. We should change this to something like:

{noformat}
catch(const std::exception &exc)
{
    std::cerr << exc.what() << std::endl;
}
{noformat}
 $acceptance criteria:$",0,0,0,0,0,0,1,6.76667,7,2,0.285714,2,0.285714,1,0.142857,1,0.142857,1,0.142857
276,MCOL-3861,Sub-Task,MCOL,2020-03-02 18:09:58,,0,CLONE - Fix cpimport S3 multi-PM usage,"Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too.",,"CLONE - Fix cpimport S3 multi-PM usage $end$ Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too. $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Blocker,9,,1,2,1,3,0,0,0,,0,850,2,0,0,2020-03-02 18:09:58,CLONE - Fix cpimport S3 multi-PM usage,"Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too.",,0,0,0,0,0.0,"CLONE - Fix cpimport S3 multi-PM usage $end$ Splitter has a different codepath for multi-PM (WEFileReadThread::openInFile()), need to support this too. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,13,3,0.230769,3,0.230769,2,0.153846,1,0.0769231,1,0.0769231
277,MCOL-3862,Sub-Task,MCOL,2020-03-02 18:14:11,,0,CLONE - COLLATE used in DDL and ORDER BY should be allowed,When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings.,,CLONE - COLLATE used in DDL and ORDER BY should be allowed $end$ When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings. $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,9,,0,2,0,6,0,0,0,,0,850,2,0,0,2020-03-02 18:14:11,CLONE - COLLATE used in DDL and ORDER BY should be allowed,When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings.,,0,0,0,0,0.0,CLONE - COLLATE used in DDL and ORDER BY should be allowed $end$ When COLLATE is used in DDL it should be allowed to pass with warnings. Additionally if it doesn't in ORDER BY it should pass in that too with warnings. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,14,3,0.214286,3,0.214286,2,0.142857,1,0.0714286,1,0.0714286
278,MCOL-3886,Sub-Task,MCOL,2020-03-16 14:32:31,,0,Failover design work,Failover was split into its own task.  This ticket is for tracking work toward its design & planned interaction with the main management code.,,Failover design work $end$ Failover was split into its own task.  This ticket is for tracking work toward its design & planned interaction with the main management code. $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-16 14:32:31,Failover design work,Failover was split into its own task.  This ticket is for tracking work toward its design & planned interaction with the main management code.,,0,0,0,0,0.0,Failover design work $end$ Failover was split into its own task.  This ticket is for tracking work toward its design & planned interaction with the main management code. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,15,3,0.2,3,0.2,2,0.133333,1,0.0666667,1,0.0666667
279,MCOL-3909,New Feature,MCOL,2020-03-30 18:32:53,MCOL-3524,0,Standalone Python scripting for Rest API for cluster management and failover,This will be the parent task for implementing the new failover mechanism.,,Standalone Python scripting for Rest API for cluster management and failover $end$ This will be the parent task for implementing the new failover mechanism. $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,19,,0,0,0,1,0,1,4,,0,850,0,1,0,2020-07-10 15:09:50,Standalone Python scripting for Rest API for cluster management and failover,This will be the parent task for implementing the new failover mechanism.,,0,0,0,0,0.0,Standalone Python scripting for Rest API for cluster management and failover $end$ This will be the parent task for implementing the new failover mechanism. $acceptance criteria:$,0,0,0,0,0,0,0,2444.6,16,3,0.1875,3,0.1875,2,0.125,1,0.0625,1,0.0625
280,MCOL-3910,Sub-Task,MCOL,2020-03-30 18:35:29,,0,Initial coding of the design,Initial coding of the failover design + unit tests.,,Initial coding of the design $end$ Initial coding of the failover design + unit tests. $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-03-30 18:35:29,Initial coding of the design,Initial coding of the failover design + unit tests.,,0,0,0,0,0.0,Initial coding of the design $end$ Initial coding of the failover design + unit tests. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,17,3,0.176471,3,0.176471,2,0.117647,1,0.0588235,1,0.0588235
281,MCOL-392,New Feature,MCOL,2016-11-07 21:41:07,MCOL-1049,0,TIME datatype is not supported,"The time datatype is not currently supported requiring use of alternative datatypes (e.g. int, string, datetime).

In addition microsecond support should be implemented for both DATETIME and TIME
",,"TIME datatype is not supported $end$ The time datatype is not currently supported requiring use of alternative datatypes (e.g. int, string, datetime).

In addition microsecond support should be implemented for both DATETIME and TIME
 $acceptance criteria:$",,David Thompson,David Thompson,Major,25,,1,7,2,11,0,2,7,,0,850,6,2,0,2018-04-19 17:57:56,TIME datatype is not supported,"The time datatype is not currently supported requiring use of alternative datatypes (e.g. int, string, datetime).

In addition microsecond support should be implemented for both DATETIME and TIME
",,0,0,0,0,0.0,"TIME datatype is not supported $end$ The time datatype is not currently supported requiring use of alternative datatypes (e.g. int, string, datetime).

In addition microsecond support should be implemented for both DATETIME and TIME
 $acceptance criteria:$",0,0,0,0,0,0,1,12668.3,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
282,MCOL-3964,Sub-Task,MCOL,2020-04-27 13:12:09,,0,Replace CSV engine with MCS in the initial version.,,,Replace CSV engine with MCS in the initial version. $end$ $acceptance criteria:$,,Roman,Roman,Major,2,,0,1,0,1,0,1,0,,0,850,1,0,0,2020-04-27 13:12:09,Replace CVS engine with MCS in the initial version.,,,1,0,0,2,0.0833333,Replace CVS engine with MCS in the initial version. $end$ $acceptance criteria:$,1,1,0,0,0,0,0,0.0,18,2,0.111111,2,0.111111,2,0.111111,1,0.0555556,1,0.0555556
283,MCOL-3976,New Feature,MCOL,2020-05-01 14:34:29,,0,Amazon S3 needs to support use of IAM roles,"Columnstore Amazon for 1.2 releases and early support both Access Keys and IAM roles. Customer used IAM roles in their production and they request that S3 storage support IAm role setup.

Currently only support Access keys via the configuration file.

From customer

It must be adapted to use an IAM instance profile or we can not use S3 storage.
",,"Amazon S3 needs to support use of IAM roles $end$ Columnstore Amazon for 1.2 releases and early support both Access Keys and IAM roles. Customer used IAM roles in their production and they request that S3 storage support IAm role setup.

Currently only support Access keys via the configuration file.

From customer

It must be adapted to use an IAM instance profile or we can not use S3 storage.
 $acceptance criteria:$",,David Hill,David Hill,Critical,30,,1,5,1,1,0,0,0,,0,850,3,0,0,2020-09-08 21:59:12,Amazon S3 needs to support use of IAM roles,"Columnstore Amazon for 1.2 releases and early support both Access Keys and IAM roles. Customer used IAM roles in their production and they request that S3 storage support IAm role setup.

Currently only support Access keys via the configuration file.

From customer

It must be adapted to use an IAM instance profile or we can not use S3 storage.
",,0,0,0,0,0.0,"Amazon S3 needs to support use of IAM roles $end$ Columnstore Amazon for 1.2 releases and early support both Access Keys and IAM roles. Customer used IAM roles in their production and they request that S3 storage support IAm role setup.

Currently only support Access keys via the configuration file.

From customer

It must be adapted to use an IAM instance profile or we can not use S3 storage.
 $acceptance criteria:$",0,0,0,0,0,0,0,3127.4,32,4,0.125,0,0.0,0,0.0,0,0.0,0,0.0
284,MCOL-4,Task,MCOL,2016-05-02 16:45:07,,0,Build for CentOS 7,,,Build for CentOS 7 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,17,,0,5,1,1,0,0,0,,0,850,5,0,0,2016-05-10 19:25:49,Build for CentOS 7,,,0,0,0,0,0.0,Build for CentOS 7 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,194.667,2,1,0.5,0,0.0,0,0.0,0,0.0,0,0.0
285,MCOL-4000,Task,MCOL,2020-05-12 21:13:35,,0,Add an option to allow using cpimport for LDI and INSERT..SELECT in a transaction,"CS has a session variable, columnstore_use_import_for_batchinsert, that has 2 values, ON (default) and OFF. This allows a user to set the option of using cpimport (ON) or the regular DML (OFF) for LDI and INSERT SELECT statements.

If a user performs these queries in a transaction (such as by setting autocommit=0 or using the START TRANSACTION statement), however, CS fallsback to using DML for the operation. This behaviour is because of the current limitation of cpimport to handle rollbacks.

We want to add a third option to columnstore_use_import_for_batchinsert: ALWAYS. This value will always use cpimport for LDI and INSERT SELECT queries, irrespective of whether the query runs in a transaction or not. This gives the user the ability to use a faster import method, with the caveat that if a user issues a rollback of the transaction, it will have no effect as the data would have already been committed to actual database files by cpimport.",,"Add an option to allow using cpimport for LDI and INSERT..SELECT in a transaction $end$ CS has a session variable, columnstore_use_import_for_batchinsert, that has 2 values, ON (default) and OFF. This allows a user to set the option of using cpimport (ON) or the regular DML (OFF) for LDI and INSERT SELECT statements.

If a user performs these queries in a transaction (such as by setting autocommit=0 or using the START TRANSACTION statement), however, CS fallsback to using DML for the operation. This behaviour is because of the current limitation of cpimport to handle rollbacks.

We want to add a third option to columnstore_use_import_for_batchinsert: ALWAYS. This value will always use cpimport for LDI and INSERT SELECT queries, irrespective of whether the query runs in a transaction or not. This gives the user the ability to use a faster import method, with the caveat that if a user issues a rollback of the transaction, it will have no effect as the data would have already been committed to actual database files by cpimport. $acceptance criteria:$",,Gagan Goel,Gagan Goel,Major,12,,1,4,1,1,0,0,0,,0,850,4,0,0,2020-05-12 21:13:35,Add an option to allow using cpimport for LDI and INSERT..SELECT in a transaction,"CS has a session variable, columnstore_use_import_for_batchinsert, that has 2 values, ON (default) and OFF. This allows a user to set the option of using cpimport (ON) or the regular DML (OFF) for LDI and INSERT SELECT statements.

If a user performs these queries in a transaction (such as by setting autocommit=0 or using the START TRANSACTION statement), however, CS fallsback to using DML for the operation. This behaviour is because of the current limitation of cpimport to handle rollbacks.

We want to add a third option to columnstore_use_import_for_batchinsert: ALWAYS. This value will always use cpimport for LDI and INSERT SELECT queries, irrespective of whether the query runs in a transaction or not. This gives the user the ability to use a faster import method, with the caveat that if a user issues a rollback of the transaction, it will have no effect as the data would have already been committed to actual database files by cpimport.",,0,0,0,0,0.0,"Add an option to allow using cpimport for LDI and INSERT..SELECT in a transaction $end$ CS has a session variable, columnstore_use_import_for_batchinsert, that has 2 values, ON (default) and OFF. This allows a user to set the option of using cpimport (ON) or the regular DML (OFF) for LDI and INSERT SELECT statements.

If a user performs these queries in a transaction (such as by setting autocommit=0 or using the START TRANSACTION statement), however, CS fallsback to using DML for the operation. This behaviour is because of the current limitation of cpimport to handle rollbacks.

We want to add a third option to columnstore_use_import_for_batchinsert: ALWAYS. This value will always use cpimport for LDI and INSERT SELECT queries, irrespective of whether the query runs in a transaction or not. This gives the user the ability to use a faster import method, with the caveat that if a user issues a rollback of the transaction, it will have no effect as the data would have already been committed to actual database files by cpimport. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
286,MCOL-4041,Task,MCOL,2020-06-05 10:04:31,,0,Test backup package with OAM-less MCS.,,,Test backup package with OAM-less MCS. $end$ $acceptance criteria:$,,Roman,Roman,Major,21,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-04-04 21:14:39,Test backup package with OAM-less MCS.,,,0,0,0,0,0.0,Test backup package with OAM-less MCS. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,7283.17,19,3,0.157895,2,0.105263,2,0.105263,1,0.0526316,1,0.0526316
287,MCOL-405,Task,MCOL,2016-11-14 16:38:03,,0,MariaDB ColumnStore Mult-UM Guide,Documentation to provide how to setup multi-um configuration.,,MariaDB ColumnStore Mult-UM Guide $end$ Documentation to provide how to setup multi-um configuration. $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,20,,0,1,0,11,0,0,0,,0,850,1,0,0,2016-11-22 19:34:12,MariaDB ColumnStore Mult-UM Guide,Documentation to provide how to setup multi-um configuration.,,0,0,0,0,0.0,MariaDB ColumnStore Mult-UM Guide $end$ Documentation to provide how to setup multi-um configuration. $acceptance criteria:$,0,0,0,0,0,0,1,194.933,35,8,0.228571,3,0.0857143,3,0.0857143,1,0.0285714,1,0.0285714
288,MCOL-4055,Sub-Task,MCOL,2020-06-10 21:56:59,,0,Integrate with the cluster management code,,,Integrate with the cluster management code $end$ $acceptance criteria:$,,Patrick LeBlanc,Patrick LeBlanc,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-06-10 21:56:59,Integrate with the cluster management code,,,0,0,0,0,0.0,Integrate with the cluster management code $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,18,3,0.166667,3,0.166667,2,0.111111,1,0.0555556,1,0.0555556
289,MCOL-406,New Feature,MCOL,2016-11-15 11:21:56,,0,Stored procedures required for I_S tables,"Stored procedures are required for the new I_S tables created in MCOL-309. At the very least:

1. a full disk size report
2. a usage report per table

We need somewhere to put these as I_S cannot have stored procedures. I recommend something new such as columnstoresys so that it doesn't have any of the legacy names our other schemas use.",,"Stored procedures required for I_S tables $end$ Stored procedures are required for the new I_S tables created in MCOL-309. At the very least:

1. a full disk size report
2. a usage report per table

We need somewhere to put these as I_S cannot have stored procedures. I recommend something new such as columnstoresys so that it doesn't have any of the legacy names our other schemas use. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,29,,0,16,1,3,0,0,0,,0,850,16,0,0,2016-11-15 11:21:56,Stored procedures required for I_S tables,"Stored procedures are required for the new I_S tables created in MCOL-309. At the very least:

1. a full disk size report
2. a usage report per table

We need somewhere to put these as I_S cannot have stored procedures. I recommend something new such as columnstoresys so that it doesn't have any of the legacy names our other schemas use.",,0,0,0,0,0.0,"Stored procedures required for I_S tables $end$ Stored procedures are required for the new I_S tables created in MCOL-309. At the very least:

1. a full disk size report
2. a usage report per table

We need somewhere to put these as I_S cannot have stored procedures. I recommend something new such as columnstoresys so that it doesn't have any of the legacy names our other schemas use. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,8,2,0.25,2,0.25,1,0.125,1,0.125,1,0.125
290,MCOL-4155,Task,MCOL,2020-07-08 20:40:29,,0,Reduce redundancy & confusion around having two columnstore.cnf files,"Serg correctly observed that after merging one of his PRs into develop, we are now deploying both a x-columnstore.cnf and a columnstore.cnf file in both community and enterprise, b/c we don't have a separate branch or repo for each.

For community, the x-columnstore.cnf is unnecessary, and once we're gamma-level, the plugin-maturity option can be removed (b/c gamma is the default there).

His suggestion to avoid having both is to package only columnstore.cnf, and have post-install generate x-columnstore.cnf IF it sees the enterprise-specific .cnf file in /etc/my.cnf.d

Other option is to mod cmake to detect the INSTALL_LAYOUT var, and if enterprise, install x-columnstore.cnf.  Else, don't.

Multiple easy options.  It should be done before the next release.



",,"Reduce redundancy & confusion around having two columnstore.cnf files $end$ Serg correctly observed that after merging one of his PRs into develop, we are now deploying both a x-columnstore.cnf and a columnstore.cnf file in both community and enterprise, b/c we don't have a separate branch or repo for each.

For community, the x-columnstore.cnf is unnecessary, and once we're gamma-level, the plugin-maturity option can be removed (b/c gamma is the default there).

His suggestion to avoid having both is to package only columnstore.cnf, and have post-install generate x-columnstore.cnf IF it sees the enterprise-specific .cnf file in /etc/my.cnf.d

Other option is to mod cmake to detect the INSTALL_LAYOUT var, and if enterprise, install x-columnstore.cnf.  Else, don't.

Multiple easy options.  It should be done before the next release.



 $acceptance criteria:$",,Patrick LeBlanc,Patrick LeBlanc,Minor,22,,0,3,0,1,0,0,0,,0,850,3,0,0,2020-07-10 15:19:33,Reduce redundancy & confusion around having two columnstore.cnf files,"Serg correctly observed that after merging one of his PRs into develop, we are now deploying both a x-columnstore.cnf and a columnstore.cnf file in both community and enterprise, b/c we don't have a separate branch or repo for each.

For community, the x-columnstore.cnf is unnecessary, and once we're gamma-level, the plugin-maturity option can be removed (b/c gamma is the default there).

His suggestion to avoid having both is to package only columnstore.cnf, and have post-install generate x-columnstore.cnf IF it sees the enterprise-specific .cnf file in /etc/my.cnf.d

Other option is to mod cmake to detect the INSTALL_LAYOUT var, and if enterprise, install x-columnstore.cnf.  Else, don't.

Multiple easy options.  It should be done before the next release.



",,0,0,0,0,0.0,"Reduce redundancy & confusion around having two columnstore.cnf files $end$ Serg correctly observed that after merging one of his PRs into develop, we are now deploying both a x-columnstore.cnf and a columnstore.cnf file in both community and enterprise, b/c we don't have a separate branch or repo for each.

For community, the x-columnstore.cnf is unnecessary, and once we're gamma-level, the plugin-maturity option can be removed (b/c gamma is the default there).

His suggestion to avoid having both is to package only columnstore.cnf, and have post-install generate x-columnstore.cnf IF it sees the enterprise-specific .cnf file in /etc/my.cnf.d

Other option is to mod cmake to detect the INSTALL_LAYOUT var, and if enterprise, install x-columnstore.cnf.  Else, don't.

Multiple easy options.  It should be done before the next release.



 $acceptance criteria:$",0,0,0,0,0,0,0,42.65,19,3,0.157895,3,0.157895,2,0.105263,1,0.0526316,1,0.0526316
291,MCOL-4167,Sub-Task,MCOL,2020-07-13 15:00:01,,0,Add support for wide-DECIMAL into FuncExp framework.,,,Add support for wide-DECIMAL into FuncExp framework. $end$ $acceptance criteria:$,,Roman,Roman,Major,5,,0,1,0,16,0,1,0,,0,850,1,0,0,2020-07-13 15:00:01,Get wide DECIMAL into FuncExp framework.,,,1,0,0,7,0.444444,Get wide DECIMAL into FuncExp framework. $end$ $acceptance criteria:$,1,1,1,0,0,0,1,0.0,20,3,0.15,2,0.1,2,0.1,1,0.05,1,0.05
292,MCOL-4171,Sub-Task,MCOL,2020-07-13 15:16:05,,0,Add support for wide-DECIMAL into Window Functions.,,,Add support for wide-DECIMAL into Window Functions. $end$ $acceptance criteria:$,,Roman,Roman,Major,6,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-07-13 15:16:05,Add support for wide-DECIMAL into Window Functions.,,,0,0,0,0,0.0,Add support for wide-DECIMAL into Window Functions. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,21,4,0.190476,3,0.142857,2,0.0952381,1,0.047619,1,0.047619
293,MCOL-4172,Sub-Task,MCOL,2020-07-13 15:17:51,,0,Add support for wide-DECIMAL into statistical aggregate and regr_ UDAF functions,,,Add support for wide-DECIMAL into statistical aggregate and regr_ UDAF functions $end$ $acceptance criteria:$,,Roman,Roman,Major,8,,0,3,0,16,0,1,0,,0,850,3,0,0,2020-07-13 15:17:51,"Add support for wide-DECIMAL into statistical aggregate functions, e.g. stddev, variance",,,1,0,0,8,0.285714,"Add support for wide-DECIMAL into statistical aggregate functions, e.g. stddev, variance $end$ $acceptance criteria:$",1,1,1,0,0,0,1,0.0,22,4,0.181818,3,0.136364,2,0.0909091,1,0.0454545,1,0.0454545
294,MCOL-4174,Sub-Task,MCOL,2020-07-13 15:29:59,,0,Review/refactor frontend/connector code.,,,Review/refactor frontend/connector code. $end$ $acceptance criteria:$,,Roman,Roman,Major,6,,0,2,0,16,0,0,0,,0,850,2,0,0,2020-07-13 15:29:59,Review/refactor frontend/connector code.,,,0,0,0,0,0.0,Review/refactor frontend/connector code. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,23,5,0.217391,4,0.173913,2,0.0869565,1,0.0434783,1,0.0434783
295,MCOL-4175,Sub-Task,MCOL,2020-07-13 15:30:14,,0,Review/refactor primitives code,,,Review/refactor primitives code $end$ $acceptance criteria:$,,Roman,Roman,Major,5,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-07-13 15:30:14,Review/refactor primitives code,,,0,0,0,0,0.0,Review/refactor primitives code $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,24,5,0.208333,4,0.166667,2,0.0833333,1,0.0416667,1,0.0416667
296,MCOL-4177,Sub-Task,MCOL,2020-07-13 15:30:43,,0,Review/refactor bulk insertion paths and code.,,,Review/refactor bulk insertion paths and code. $end$ $acceptance criteria:$,,Roman,Roman,Major,7,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-07-13 15:30:43,Review/refactor bulk insertion paths and code.,,,0,0,0,0,0.0,Review/refactor bulk insertion paths and code. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,25,5,0.2,4,0.16,2,0.08,1,0.04,1,0.04
297,MCOL-4178,Sub-Task,MCOL,2020-07-13 15:31:01,,0,Review/refactor DBRM and Casual Partitioning code.,,,Review/refactor DBRM and Casual Partitioning code. $end$ $acceptance criteria:$,,Roman,Roman,Major,6,,0,2,0,16,0,1,0,,0,850,2,0,0,2020-07-13 15:31:01,Review/refactor DBRM code.,,,1,0,0,3,0.5,Review/refactor DBRM code. $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,26,5,0.192308,4,0.153846,2,0.0769231,1,0.0384615,1,0.0384615
298,MCOL-4180,Sub-Task,MCOL,2020-07-14 16:52:59,,0,Review wide-DECIMAL code in dbcon/execplan classes.,,,Review wide-DECIMAL code in dbcon/execplan classes. $end$ $acceptance criteria:$,,Gagan Goel,Gagan Goel,Major,6,,0,2,0,16,0,1,0,,0,850,2,0,0,2020-07-14 16:52:59,Add support for wide-DECIMAL into dbcon/execplan classes.,,,1,0,0,7,0.5,Add support for wide-DECIMAL into dbcon/execplan classes. $end$ $acceptance criteria:$,1,1,1,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
299,MCOL-4188,Task,MCOL,2020-07-17 10:06:08,MCOL-1049,0,Verify the whole Full DECIMAL patch against regression test suite and fix issues found.,,,Verify the whole Full DECIMAL patch against regression test suite and fix issues found. $end$ $acceptance criteria:$,,Roman,Roman,Major,16,,2,3,2,2,0,1,0,,0,850,0,1,0,2021-01-29 15:01:39,Verify the whole Full DECIMAL patch against regression test suite and fix issues found.,,,0,0,0,0,0.0,Verify the whole Full DECIMAL patch against regression test suite and fix issues found. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,4708.92,27,6,0.222222,4,0.148148,2,0.0740741,1,0.037037,1,0.037037
300,MCOL-420,Task,MCOL,2016-11-23 20:59:07,,0,Add alias for cpimport in  columnstoreAlias,Please add alias for cpimport in columnstoreAlias - so the user does not have to type full path all the time,,Add alias for cpimport in  columnstoreAlias $end$ Please add alias for cpimport in columnstoreAlias - so the user does not have to type full path all the time $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,2,0,1,0,0,0,,0,850,2,0,0,2016-11-23 21:05:31,Add alias for cpimport in  columnstoreAlias,Please add alias for cpimport in columnstoreAlias - so the user does not have to type full path all the time,,0,0,0,0,0.0,Add alias for cpimport in  columnstoreAlias $end$ Please add alias for cpimport in columnstoreAlias - so the user does not have to type full path all the time $acceptance criteria:$,0,0,0,0,0,0,0,0.1,36,8,0.222222,3,0.0833333,3,0.0833333,1,0.0277778,1,0.0277778
301,MCOL-4285,Sub-Task,MCOL,2020-08-31 16:52:54,,0,Implement Stage 1 of the ColumnStore Cache,,,Implement Stage 1 of the ColumnStore Cache $end$ $acceptance criteria:$,,Gagan Goel,Gagan Goel,Major,16,,0,3,0,1,0,0,0,,0,850,3,0,0,2020-08-31 16:52:54,Implement Stage 1 of the ColumnStore Cache,,,0,0,0,0,0.0,Implement Stage 1 of the ColumnStore Cache $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,2,1,0.5,1,0.5,0,0.0,0,0.0,0,0.0
302,MCOL-4313,Sub-Task,MCOL,2020-09-17 13:52:20,,0,Release builds contains ASM that either crashes processes or affects query results,"This only affects decimal(38)

There are two known issues caused by -OX flags of gcc. 
GCC <= 9.2 with -O2 tends to use movdqa instruction for int128_t asignment operations that fails on unaligned memory addresses. [Here|https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92904] is some information.

There is an issue with count(distinct d1) expressions used in projections. Repetitions of the query produces different results for the same data set.
{noformat}
SELECT count(distinct d1) FROM cs1;
{noformat}",,"Release builds contains ASM that either crashes processes or affects query results $end$ This only affects decimal(38)

There are two known issues caused by -OX flags of gcc. 
GCC <= 9.2 with -O2 tends to use movdqa instruction for int128_t asignment operations that fails on unaligned memory addresses. [Here|https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92904] is some information.

There is an issue with count(distinct d1) expressions used in projections. Repetitions of the query produces different results for the same data set.
{noformat}
SELECT count(distinct d1) FROM cs1;
{noformat} $acceptance criteria:$",,Roman,Roman,Minor,13,,0,2,0,16,0,3,0,,0,850,2,0,0,2020-09-17 13:52:20,Release builds contains ASM that either crashes processes or affects query results,"There are two know issues caused by -OX flags of gcc. 
GCC <= 9.2 with -O2 tends to use movdqa instruction for int128_t asignment operations that fails on unaligned memory addresses. [Here|https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92904] is some information and potential fix for gcc >= 9.2.4

There is an issue with count(distinct d1) expressions used in projections. Repetitions of the query produces different results for the same data set.
{noformat}
SELECT count(distinct d1) FROM cs1;
{noformat}",,0,3,0,15,0.149425,"Release builds contains ASM that either crashes processes or affects query results $end$ There are two know issues caused by -OX flags of gcc. 
GCC <= 9.2 with -O2 tends to use movdqa instruction for int128_t asignment operations that fails on unaligned memory addresses. [Here|https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92904] is some information and potential fix for gcc >= 9.2.4

There is an issue with count(distinct d1) expressions used in projections. Repetitions of the query produces different results for the same data set.
{noformat}
SELECT count(distinct d1) FROM cs1;
{noformat} $acceptance criteria:$",3,1,1,1,1,0,1,0.0,28,6,0.214286,4,0.142857,2,0.0714286,1,0.0357143,1,0.0357143
303,MCOL-4319,New Feature,MCOL,2020-09-21 23:21:19,,0,Expand logrotate script for Columnstore to include cpimport logs,"Maybe we should add a section like this to /etc/logrotate.d/columnstore:

{code:java}
/var/lib/columnstore/data/bulk/log/* {
    daily
    rotate 2
    compress
    compresscmd xz
}
{code}
",,"Expand logrotate script for Columnstore to include cpimport logs $end$ Maybe we should add a section like this to /etc/logrotate.d/columnstore:

{code:java}
/var/lib/columnstore/data/bulk/log/* {
    daily
    rotate 2
    compress
    compresscmd xz
}
{code}
 $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,23,,0,2,0,2,0,0,0,,0,850,1,0,0,2021-01-22 17:26:15,Expand logrotate script for Columnstore to include cpimport logs,"Maybe we should add a section like this to /etc/logrotate.d/columnstore:

{code:java}
/var/lib/columnstore/data/bulk/log/* {
    daily
    rotate 2
    compress
    compresscmd xz
}
{code}
",,0,0,0,0,0.0,"Expand logrotate script for Columnstore to include cpimport logs $end$ Maybe we should add a section like this to /etc/logrotate.d/columnstore:

{code:java}
/var/lib/columnstore/data/bulk/log/* {
    daily
    rotate 2
    compress
    compresscmd xz
}
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,2946.07,5,1,0.2,0,0.0,0,0.0,0,0.0,0,0.0
304,MCOL-4337,New Feature,MCOL,2020-10-07 09:50:46,,0,Controllernode must establish connection with his workernodes on its startup,Controllernode now establish DBRM_Worker connections lazily so it doesn't wait for them to come up. This calls for additional startup check conditions in a cluster setups. Controllernode must wait for all WNs to come up before CN is ready to process requests.,,Controllernode must establish connection with his workernodes on its startup $end$ Controllernode now establish DBRM_Worker connections lazily so it doesn't wait for them to come up. This calls for additional startup check conditions in a cluster setups. Controllernode must wait for all WNs to come up before CN is ready to process requests. $acceptance criteria:$,,Roman,Roman,Minor,19,,0,10,2,1,0,0,0,,0,850,10,0,0,2020-10-07 09:50:46,Controllernode must establish connection with his workernodes on its startup,Controllernode now establish DBRM_Worker connections lazily so it doesn't wait for them to come up. This calls for additional startup check conditions in a cluster setups. Controllernode must wait for all WNs to come up before CN is ready to process requests.,,0,0,0,0,0.0,Controllernode must establish connection with his workernodes on its startup $end$ Controllernode now establish DBRM_Worker connections lazily so it doesn't wait for them to come up. This calls for additional startup check conditions in a cluster setups. Controllernode must wait for all WNs to come up before CN is ready to process requests. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,29,7,0.241379,5,0.172414,3,0.103448,2,0.0689655,1,0.0344828
305,MCOL-4361,Task,MCOL,2020-10-15 11:26:34,MCOL-1049,0,"Replace pow(10.0, (double)scale) expressions with a static dictionary lookup.","We need to find and replace expressions like pow(10.0, (double)scale) related to DECIMALS(<19) processing and replace them with a static dictionary lookup.",,"Replace pow(10.0, (double)scale) expressions with a static dictionary lookup. $end$ We need to find and replace expressions like pow(10.0, (double)scale) related to DECIMALS(<19) processing and replace them with a static dictionary lookup. $acceptance criteria:$",,Roman,Roman,Minor,25,,36,0,37,2,0,0,0,,0,850,0,0,0,2021-03-22 13:22:07,"Replace pow(10.0, (double)scale) expressions with a static dictionary lookup.","We need to find and replace expressions like pow(10.0, (double)scale) related to DECIMALS(<19) processing and replace them with a static dictionary lookup.",,0,0,0,0,0.0,"Replace pow(10.0, (double)scale) expressions with a static dictionary lookup. $end$ We need to find and replace expressions like pow(10.0, (double)scale) related to DECIMALS(<19) processing and replace them with a static dictionary lookup. $acceptance criteria:$",0,0,0,0,0,0,1,3793.92,30,7,0.233333,5,0.166667,3,0.1,2,0.0666667,1,0.0333333
306,MCOL-4362,Sub-Task,MCOL,2020-10-15 17:10:49,,0,"funcexp::Func_ceil::getIntVal fails with Thread 20 ""PPBatchPrimProc"" received signal SIGFPE, Arithmetic exception","The changes made to the method doesn't expect narrow decimal with the scale > 18. 
Can be reproduced with working_tpch1/qa_fe_cnxFunctions/bug3334_overflow_error.negative.sql",,"funcexp::Func_ceil::getIntVal fails with Thread 20 ""PPBatchPrimProc"" received signal SIGFPE, Arithmetic exception $end$ The changes made to the method doesn't expect narrow decimal with the scale > 18. 
Can be reproduced with working_tpch1/qa_fe_cnxFunctions/bug3334_overflow_error.negative.sql $acceptance criteria:$",,Roman,Roman,Minor,1,,0,0,0,16,0,0,0,,0,850,0,0,0,2020-10-15 17:10:49,"funcexp::Func_ceil::getIntVal fails with Thread 20 ""PPBatchPrimProc"" received signal SIGFPE, Arithmetic exception","The changes made to the method doesn't expect narrow decimal with the scale > 18. 
Can be reproduced with working_tpch1/qa_fe_cnxFunctions/bug3334_overflow_error.negative.sql",,0,0,0,0,0.0,"funcexp::Func_ceil::getIntVal fails with Thread 20 ""PPBatchPrimProc"" received signal SIGFPE, Arithmetic exception $end$ The changes made to the method doesn't expect narrow decimal with the scale > 18. 
Can be reproduced with working_tpch1/qa_fe_cnxFunctions/bug3334_overflow_error.negative.sql $acceptance criteria:$",0,0,0,0,0,0,1,0.0,31,7,0.225806,5,0.16129,3,0.0967742,2,0.0645161,1,0.0322581
307,MCOL-4363,Task,MCOL,2020-10-15 19:21:55,MCOL-3836,0,Push storagemanager.cnf to newly added nodes in a cluster.,"Currently, to use S3, user needs to modify storagemanager.cnf on every node.  Add functionality to cmapi to push storagemanager.cnf to newly added nodes.",,"Push storagemanager.cnf to newly added nodes in a cluster. $end$ Currently, to use S3, user needs to modify storagemanager.cnf on every node.  Add functionality to cmapi to push storagemanager.cnf to newly added nodes. $acceptance criteria:$",,Jose Rojas,Jose Rojas,Minor,24,,0,4,0,6,0,0,0,,0,850,4,0,0,2021-01-25 18:24:17,Push storagemanager.cnf to newly added nodes in a cluster.,"Currently, to use S3, user needs to modify storagemanager.cnf on every node.  Add functionality to cmapi to push storagemanager.cnf to newly added nodes.",,0,0,0,0,0.0,"Push storagemanager.cnf to newly added nodes in a cluster. $end$ Currently, to use S3, user needs to modify storagemanager.cnf on every node.  Add functionality to cmapi to push storagemanager.cnf to newly added nodes. $acceptance criteria:$",0,0,0,0,0,0,1,2447.03,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
308,MCOL-4387,Sub-Task,MCOL,2020-11-11 11:14:21,,0,Convert dataconvert::decimalToString() into VDecimal:: and TSInt128 classes methods.,There are static dataconvert::decimalToString() functions for int128 and int64 values. We need to convert these functions into a set of VDecimal:: and TSInt128:: methods to  logical SQL data types API for DECIMAL. This will also reduce a size of libdataconvert.so file.,,Convert dataconvert::decimalToString() into VDecimal:: and TSInt128 classes methods. $end$ There are static dataconvert::decimalToString() functions for int128 and int64 values. We need to convert these functions into a set of VDecimal:: and TSInt128:: methods to  logical SQL data types API for DECIMAL. This will also reduce a size of libdataconvert.so file. $acceptance criteria:$,,Roman,Roman,Major,3,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-11-11 11:14:21,Convert dataconvert::decimalToString() into VDecimal:: and TSInt128 classes methods.,There are static dataconvert::decimalToString() functions for int128 and int64 values. We need to convert these functions into a set of VDecimal:: and TSInt128:: methods to  logical SQL data types API for DECIMAL. This will also reduce a size of libdataconvert.so file.,,0,0,0,0,0.0,Convert dataconvert::decimalToString() into VDecimal:: and TSInt128 classes methods. $end$ There are static dataconvert::decimalToString() functions for int128 and int64 values. We need to convert these functions into a set of VDecimal:: and TSInt128:: methods to  logical SQL data types API for DECIMAL. This will also reduce a size of libdataconvert.so file. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,32,7,0.21875,5,0.15625,3,0.09375,2,0.0625,1,0.03125
309,MCOL-4389,Sub-Task,MCOL,2020-11-12 14:03:35,,0,Add support for wide-DECIMAL into UNION processing,,,Add support for wide-DECIMAL into UNION processing $end$ $acceptance criteria:$,,Roman,Roman,Major,3,,0,0,0,16,0,0,0,,0,850,0,0,0,2020-11-12 14:03:35,Add support for wide-DECIMAL into UNION processing,,,0,0,0,0,0.0,Add support for wide-DECIMAL into UNION processing $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,33,7,0.212121,5,0.151515,3,0.0909091,2,0.0606061,1,0.030303
310,MCOL-4390,Sub-Task,MCOL,2020-11-13 11:49:59,,0,Overflow checks fails in decimal unit tests,"{noformat}
root@drrtuy-devel-1:/data/mdb-server/storage/columnstore/columnstore# ./bin/mcs_decimal_tests 
Running main() from gtest_main.cc
[==========] Running 13 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 13 tests from Decimal
[ RUN      ] Decimal.compareCheck
[       OK ] Decimal.compareCheck (0 ms)
[ RUN      ] Decimal.additionNoOverflowCheck
[       OK ] Decimal.additionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionNoOverflowCheck
[       OK ] Decimal.divisionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionWithOverflowCheck
[       OK ] Decimal.divisionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.additionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:512: Failure
Expected: doAdd(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.additionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionNoOverflowCheck
[       OK ] Decimal.subtractionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:687: Failure
Expected: doSubtract(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.subtractionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationNoOverflowCheck
[       OK ] Decimal.multiplicationNoOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:853: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:869: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.multiplicationWithOverflowCheck (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale0
[       OK ] Decimal.DecimalToStringCheckScale0 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale10
[       OK ] Decimal.DecimalToStringCheckScale10 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale38
[       OK ] Decimal.DecimalToStringCheckScale38 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale37
[       OK ] Decimal.DecimalToStringCheckScale37 (0 ms)
[----------] 13 tests from Decimal (0 ms total)

[----------] Global test environment tear-down
[==========] 13 tests from 1 test case ran. (1 ms total)
[  PASSED  ] 10 tests.
[  FAILED  ] 3 tests, listed below:
[  FAILED  ] Decimal.additionWithOverflowCheck
[  FAILED  ] Decimal.subtractionWithOverflowCheck
[  FAILED  ] Decimal.multiplicationWithOverflowCheck

 3 FAILED TESTS
{noformat}",,"Overflow checks fails in decimal unit tests $end$ {noformat}
root@drrtuy-devel-1:/data/mdb-server/storage/columnstore/columnstore# ./bin/mcs_decimal_tests 
Running main() from gtest_main.cc
[==========] Running 13 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 13 tests from Decimal
[ RUN      ] Decimal.compareCheck
[       OK ] Decimal.compareCheck (0 ms)
[ RUN      ] Decimal.additionNoOverflowCheck
[       OK ] Decimal.additionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionNoOverflowCheck
[       OK ] Decimal.divisionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionWithOverflowCheck
[       OK ] Decimal.divisionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.additionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:512: Failure
Expected: doAdd(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.additionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionNoOverflowCheck
[       OK ] Decimal.subtractionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:687: Failure
Expected: doSubtract(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.subtractionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationNoOverflowCheck
[       OK ] Decimal.multiplicationNoOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:853: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:869: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.multiplicationWithOverflowCheck (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale0
[       OK ] Decimal.DecimalToStringCheckScale0 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale10
[       OK ] Decimal.DecimalToStringCheckScale10 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale38
[       OK ] Decimal.DecimalToStringCheckScale38 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale37
[       OK ] Decimal.DecimalToStringCheckScale37 (0 ms)
[----------] 13 tests from Decimal (0 ms total)

[----------] Global test environment tear-down
[==========] 13 tests from 1 test case ran. (1 ms total)
[  PASSED  ] 10 tests.
[  FAILED  ] 3 tests, listed below:
[  FAILED  ] Decimal.additionWithOverflowCheck
[  FAILED  ] Decimal.subtractionWithOverflowCheck
[  FAILED  ] Decimal.multiplicationWithOverflowCheck

 3 FAILED TESTS
{noformat} $acceptance criteria:$",,Roman,Roman,Major,4,,0,0,0,16,0,0,0,,0,850,0,0,0,2020-11-13 11:49:59,Overflow checks fails in decimal unit tests,"{noformat}
root@drrtuy-devel-1:/data/mdb-server/storage/columnstore/columnstore# ./bin/mcs_decimal_tests 
Running main() from gtest_main.cc
[==========] Running 13 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 13 tests from Decimal
[ RUN      ] Decimal.compareCheck
[       OK ] Decimal.compareCheck (0 ms)
[ RUN      ] Decimal.additionNoOverflowCheck
[       OK ] Decimal.additionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionNoOverflowCheck
[       OK ] Decimal.divisionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionWithOverflowCheck
[       OK ] Decimal.divisionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.additionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:512: Failure
Expected: doAdd(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.additionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionNoOverflowCheck
[       OK ] Decimal.subtractionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:687: Failure
Expected: doSubtract(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.subtractionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationNoOverflowCheck
[       OK ] Decimal.multiplicationNoOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:853: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:869: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.multiplicationWithOverflowCheck (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale0
[       OK ] Decimal.DecimalToStringCheckScale0 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale10
[       OK ] Decimal.DecimalToStringCheckScale10 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale38
[       OK ] Decimal.DecimalToStringCheckScale38 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale37
[       OK ] Decimal.DecimalToStringCheckScale37 (0 ms)
[----------] 13 tests from Decimal (0 ms total)

[----------] Global test environment tear-down
[==========] 13 tests from 1 test case ran. (1 ms total)
[  PASSED  ] 10 tests.
[  FAILED  ] 3 tests, listed below:
[  FAILED  ] Decimal.additionWithOverflowCheck
[  FAILED  ] Decimal.subtractionWithOverflowCheck
[  FAILED  ] Decimal.multiplicationWithOverflowCheck

 3 FAILED TESTS
{noformat}",,0,0,0,0,0.0,"Overflow checks fails in decimal unit tests $end$ {noformat}
root@drrtuy-devel-1:/data/mdb-server/storage/columnstore/columnstore# ./bin/mcs_decimal_tests 
Running main() from gtest_main.cc
[==========] Running 13 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 13 tests from Decimal
[ RUN      ] Decimal.compareCheck
[       OK ] Decimal.compareCheck (0 ms)
[ RUN      ] Decimal.additionNoOverflowCheck
[       OK ] Decimal.additionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionNoOverflowCheck
[       OK ] Decimal.divisionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.divisionWithOverflowCheck
[       OK ] Decimal.divisionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.additionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:512: Failure
Expected: doAdd(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.additionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionNoOverflowCheck
[       OK ] Decimal.subtractionNoOverflowCheck (0 ms)
[ RUN      ] Decimal.subtractionWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:687: Failure
Expected: doSubtract(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.subtractionWithOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationNoOverflowCheck
[       OK ] Decimal.multiplicationNoOverflowCheck (0 ms)
[ RUN      ] Decimal.multiplicationWithOverflowCheck
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:853: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
/data/mdb-server/storage/columnstore/columnstore/tests/mcs_decimal-tests.cpp:869: Failure
Expected: doMultiply(l, r, result) throws an exception of type logging::OperationOverflowExcept.
  Actual: it throws nothing.
[  FAILED  ] Decimal.multiplicationWithOverflowCheck (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale0
[       OK ] Decimal.DecimalToStringCheckScale0 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale10
[       OK ] Decimal.DecimalToStringCheckScale10 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale38
[       OK ] Decimal.DecimalToStringCheckScale38 (0 ms)
[ RUN      ] Decimal.DecimalToStringCheckScale37
[       OK ] Decimal.DecimalToStringCheckScale37 (0 ms)
[----------] 13 tests from Decimal (0 ms total)

[----------] Global test environment tear-down
[==========] 13 tests from 1 test case ran. (1 ms total)
[  PASSED  ] 10 tests.
[  FAILED  ] 3 tests, listed below:
[  FAILED  ] Decimal.additionWithOverflowCheck
[  FAILED  ] Decimal.subtractionWithOverflowCheck
[  FAILED  ] Decimal.multiplicationWithOverflowCheck

 3 FAILED TESTS
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,0.0,34,7,0.205882,5,0.147059,3,0.0882353,2,0.0588235,1,0.0294118
311,MCOL-4394,Sub-Task,MCOL,2020-11-16 11:15:27,,0,Move __float128 related code into a separate header file.,,,Move __float128 related code into a separate header file. $end$ $acceptance criteria:$,,Roman,Roman,Minor,4,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-11-16 11:15:27,Move __float128 related code into a separate header file.,,,0,0,0,0,0.0,Move __float128 related code into a separate header file. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,35,7,0.2,5,0.142857,3,0.0857143,2,0.0571429,1,0.0285714
312,MCOL-4398,Sub-Task,MCOL,2020-11-18 23:45:30,,0,Add DDL test cases for wide decimal columns,"Expand on the existing DDL test cases to also include ""Alter table"" tests.",,"Add DDL test cases for wide decimal columns $end$ Expand on the existing DDL test cases to also include ""Alter table"" tests. $acceptance criteria:$",,Gagan Goel,Gagan Goel,Major,3,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-11-18 23:45:30,Add DDL test cases for wide decimal columns,"Expand on the existing DDL test cases to also include ""Alter table"" tests.",,0,0,0,0,0.0,"Add DDL test cases for wide decimal columns $end$ Expand on the existing DDL test cases to also include ""Alter table"" tests. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,3,1,0.333333,1,0.333333,0,0.0,0,0.0,0,0.0
313,MCOL-4406,Task,MCOL,2020-11-20 10:08:00,,0,Add unit test step into develop pipelines,There are couple unit tests under ./tests that must be run by CI pipelines. Some of the tests are written in CPPUnitTest and some in GTest so both are prerequisites.,,Add unit test step into develop pipelines $end$ There are couple unit tests under ./tests that must be run by CI pipelines. Some of the tests are written in CPPUnitTest and some in GTest so both are prerequisites. $acceptance criteria:$,,Roman,Roman,Major,23,,0,1,2,10,0,0,0,,0,850,1,0,0,2021-01-28 15:41:55,Add unit test step into develop pipelines,There are couple unit tests under ./tests that must be run by CI pipelines. Some of the tests are written in CPPUnitTest and some in GTest so both are prerequisites.,,0,0,0,0,0.0,Add unit test step into develop pipelines $end$ There are couple unit tests under ./tests that must be run by CI pipelines. Some of the tests are written in CPPUnitTest and some in GTest so both are prerequisites. $acceptance criteria:$,0,0,0,0,0,0,1,1661.55,36,7,0.194444,5,0.138889,3,0.0833333,2,0.0555556,1,0.0277778
314,MCOL-4409,Sub-Task,MCOL,2020-11-23 16:00:54,,0,Rename VDecimal into Decimal and make IDB_Decimal an alias from Decimal,We need to rename VDecimal into Decimal and move all static methods of the current datatypes::Decimal into the new class.,,Rename VDecimal into Decimal and make IDB_Decimal an alias from Decimal $end$ We need to rename VDecimal into Decimal and move all static methods of the current datatypes::Decimal into the new class. $acceptance criteria:$,,Roman,Roman,Major,5,,0,1,0,16,0,1,0,,0,850,1,0,0,2020-11-23 16:00:54,Rename VDecimal into Decimal,We need to rename VDecimal into Decimal and move all static methods of the current datatypes::Decimal into the new class.,,1,0,0,7,0.259259,Rename VDecimal into Decimal $end$ We need to rename VDecimal into Decimal and move all static methods of the current datatypes::Decimal into the new class. $acceptance criteria:$,1,1,1,0,0,0,1,0.0,37,7,0.189189,5,0.135135,3,0.0810811,2,0.0540541,1,0.027027
315,MCOL-444,New Feature,MCOL,2016-12-06 18:25:41,,0,split character import issue,"select name from flights where id=250908;
+----------------------------------------------------------------------------------------------------+
| name |
+----------------------------------------------------------------------------------------------------+
| Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords |
+----------------------------------------------------------------------------------------------------+
1 row in set, 1 warning (0.03 sec)

imysql-rea> select substr(name,1,100) from flights where id=250908;
+--------------------+
| substr(name,1,100) |
+--------------------+
| |
+--------------------+
1 row in set (0.02 sec)

show warnings;
+---------+------+---------------------------------------------------------------+
| Level | Code | Message |
+---------+------+---------------------------------------------------------------+
| Warning | 1366 | Incorrect string value: '\xE2\x80' for column 'name' at row 0 |
+---------+------+---------------------------------------------------------------+
1 row in set (0.00 sec)

select length(name), char_length(name) from flights where id=250908;
+--------------+-------------------+
| length(name) | char_length(name) |
+--------------+-------------------+
| 100 | 0 |
+--------------+-------------------+


select hex(name) from flights where id=250908;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hex(name) |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E280 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

In MySql db:
Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords - ""ocean"", ""water"", ""view"" or ""esplanade""

The source database is running MySQL 5.6.22
The data is dumped using mysqldump, sent to the InfiniDB server and imported with cpimport, which we allow to truncate strings that are too long.
The use of the substr function in the query should be superfluous as far as InfiniDB is concerned, as the column is 100 bytes, it makes the query results compatible with the MySQL servers. 
That doesn't explain the char_length function results above or why InfiniDB is returning an empty string for the substr function.

I took the hex value and played with it on https://sites.google.com/site/nathanlexwww/tools/utf8-convert
The last bit, E280, appear to be a truncated multi-byte character from the original data. The '-' character.
the - code is E28093

Here is the hex of the original data from MySQL db:
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E2809320E2809C6F6365616EE2809D2C20E2809C7761746572E2809D2C20E2809C76696577E2809D206F7220E2809C6573706C616E61646522 |

So, cpimport is truncating bytes disregarding the number of bytes in the UTF-8 character. This is bad, need a way to fix these.
The substr and char_length functions appears to be choking if it is missing all the bits in a multi-byte character, at the end of the string anyway.

Was not sure how to categorize this, Character Set, Table Corruption or General Usage.

Making the export process of this table do the substr function up front, is probably the correct way to fix this properly. But that means doing it for all tables with varchar fields and would preclude using mysqldump, making the process much more tedious and take a lot longer.
The best solution for us is for cpimport to work better with multi-byte character sets (utf-8 for us).

A few thoughts.
1. Could be fixed in the InfiniDB engine to ignore or deal with incomplete multi-byte characters.
2. Add a switch to the cpimport command to put it in a multi-byte character safe mode.
3. If 2 slows down cpimport very much make a cpimport that is multi-byte character safe, cpimportMBS, to be used when one is importing data in a multi-byte character set. OR even auto detect it based on the data (more accurate), or based on the table or column definition (least accurate).

In our case, multi-byte characters only appear in the dimension tables, which are much smaller than the fact tables. The fact tables having only a date, int and bigint columns, where import speed is important.

I'm sure you are all well versed in UTF-8, but thought I would send this along anyway.

https://en.wikipedia.org/wiki/UTF-8

So you don't have to examine the whole string to determine if the last character is valid (complete).

",,"split character import issue $end$ select name from flights where id=250908;
+----------------------------------------------------------------------------------------------------+
| name |
+----------------------------------------------------------------------------------------------------+
| Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords |
+----------------------------------------------------------------------------------------------------+
1 row in set, 1 warning (0.03 sec)

imysql-rea> select substr(name,1,100) from flights where id=250908;
+--------------------+
| substr(name,1,100) |
+--------------------+
| |
+--------------------+
1 row in set (0.02 sec)

show warnings;
+---------+------+---------------------------------------------------------------+
| Level | Code | Message |
+---------+------+---------------------------------------------------------------+
| Warning | 1366 | Incorrect string value: '\xE2\x80' for column 'name' at row 0 |
+---------+------+---------------------------------------------------------------+
1 row in set (0.00 sec)

select length(name), char_length(name) from flights where id=250908;
+--------------+-------------------+
| length(name) | char_length(name) |
+--------------+-------------------+
| 100 | 0 |
+--------------+-------------------+


select hex(name) from flights where id=250908;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hex(name) |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E280 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

In MySql db:
Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords - ""ocean"", ""water"", ""view"" or ""esplanade""

The source database is running MySQL 5.6.22
The data is dumped using mysqldump, sent to the InfiniDB server and imported with cpimport, which we allow to truncate strings that are too long.
The use of the substr function in the query should be superfluous as far as InfiniDB is concerned, as the column is 100 bytes, it makes the query results compatible with the MySQL servers. 
That doesn't explain the char_length function results above or why InfiniDB is returning an empty string for the substr function.

I took the hex value and played with it on https://sites.google.com/site/nathanlexwww/tools/utf8-convert
The last bit, E280, appear to be a truncated multi-byte character from the original data. The '-' character.
the - code is E28093

Here is the hex of the original data from MySQL db:
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E2809320E2809C6F6365616EE2809D2C20E2809C7761746572E2809D2C20E2809C76696577E2809D206F7220E2809C6573706C616E61646522 |

So, cpimport is truncating bytes disregarding the number of bytes in the UTF-8 character. This is bad, need a way to fix these.
The substr and char_length functions appears to be choking if it is missing all the bits in a multi-byte character, at the end of the string anyway.

Was not sure how to categorize this, Character Set, Table Corruption or General Usage.

Making the export process of this table do the substr function up front, is probably the correct way to fix this properly. But that means doing it for all tables with varchar fields and would preclude using mysqldump, making the process much more tedious and take a lot longer.
The best solution for us is for cpimport to work better with multi-byte character sets (utf-8 for us).

A few thoughts.
1. Could be fixed in the InfiniDB engine to ignore or deal with incomplete multi-byte characters.
2. Add a switch to the cpimport command to put it in a multi-byte character safe mode.
3. If 2 slows down cpimport very much make a cpimport that is multi-byte character safe, cpimportMBS, to be used when one is importing data in a multi-byte character set. OR even auto detect it based on the data (more accurate), or based on the table or column definition (least accurate).

In our case, multi-byte characters only appear in the dimension tables, which are much smaller than the fact tables. The fact tables having only a date, int and bigint columns, where import speed is important.

I'm sure you are all well versed in UTF-8, but thought I would send this along anyway.

https://en.wikipedia.org/wiki/UTF-8

So you don't have to examine the whole string to determine if the last character is valid (complete).

 $acceptance criteria:$",,David Hill,David Hill,Minor,8,,0,5,0,1,0,0,0,,0,850,3,0,0,2017-11-27 04:37:36,split character import issue,"select name from flights where id=250908;
+----------------------------------------------------------------------------------------------------+
| name |
+----------------------------------------------------------------------------------------------------+
| Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords |
+----------------------------------------------------------------------------------------------------+
1 row in set, 1 warning (0.03 sec)

imysql-rea> select substr(name,1,100) from flights where id=250908;
+--------------------+
| substr(name,1,100) |
+--------------------+
| |
+--------------------+
1 row in set (0.02 sec)

show warnings;
+---------+------+---------------------------------------------------------------+
| Level | Code | Message |
+---------+------+---------------------------------------------------------------+
| Warning | 1366 | Incorrect string value: '\xE2\x80' for column 'name' at row 0 |
+---------+------+---------------------------------------------------------------+
1 row in set (0.00 sec)

select length(name), char_length(name) from flights where id=250908;
+--------------+-------------------+
| length(name) | char_length(name) |
+--------------+-------------------+
| 100 | 0 |
+--------------+-------------------+


select hex(name) from flights where id=250908;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hex(name) |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E280 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

In MySql db:
Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords - ""ocean"", ""water"", ""view"" or ""esplanade""

The source database is running MySQL 5.6.22
The data is dumped using mysqldump, sent to the InfiniDB server and imported with cpimport, which we allow to truncate strings that are too long.
The use of the substr function in the query should be superfluous as far as InfiniDB is concerned, as the column is 100 bytes, it makes the query results compatible with the MySQL servers. 
That doesn't explain the char_length function results above or why InfiniDB is returning an empty string for the substr function.

I took the hex value and played with it on https://sites.google.com/site/nathanlexwww/tools/utf8-convert
The last bit, E280, appear to be a truncated multi-byte character from the original data. The '-' character.
the - code is E28093

Here is the hex of the original data from MySQL db:
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E2809320E2809C6F6365616EE2809D2C20E2809C7761746572E2809D2C20E2809C76696577E2809D206F7220E2809C6573706C616E61646522 |

So, cpimport is truncating bytes disregarding the number of bytes in the UTF-8 character. This is bad, need a way to fix these.
The substr and char_length functions appears to be choking if it is missing all the bits in a multi-byte character, at the end of the string anyway.

Was not sure how to categorize this, Character Set, Table Corruption or General Usage.

Making the export process of this table do the substr function up front, is probably the correct way to fix this properly. But that means doing it for all tables with varchar fields and would preclude using mysqldump, making the process much more tedious and take a lot longer.
The best solution for us is for cpimport to work better with multi-byte character sets (utf-8 for us).

A few thoughts.
1. Could be fixed in the InfiniDB engine to ignore or deal with incomplete multi-byte characters.
2. Add a switch to the cpimport command to put it in a multi-byte character safe mode.
3. If 2 slows down cpimport very much make a cpimport that is multi-byte character safe, cpimportMBS, to be used when one is importing data in a multi-byte character set. OR even auto detect it based on the data (more accurate), or based on the table or column definition (least accurate).

In our case, multi-byte characters only appear in the dimension tables, which are much smaller than the fact tables. The fact tables having only a date, int and bigint columns, where import speed is important.

I'm sure you are all well versed in UTF-8, but thought I would send this along anyway.

https://en.wikipedia.org/wiki/UTF-8

So you don't have to examine the whole string to determine if the last character is valid (complete).

",,0,0,0,0,0.0,"split character import issue $end$ select name from flights where id=250908;
+----------------------------------------------------------------------------------------------------+
| name |
+----------------------------------------------------------------------------------------------------+
| Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords |
+----------------------------------------------------------------------------------------------------+
1 row in set, 1 warning (0.03 sec)

imysql-rea> select substr(name,1,100) from flights where id=250908;
+--------------------+
| substr(name,1,100) |
+--------------------+
| |
+--------------------+
1 row in set (0.02 sec)

show warnings;
+---------+------+---------------------------------------------------------------+
| Level | Code | Message |
+---------+------+---------------------------------------------------------------+
| Warning | 1366 | Incorrect string value: '\xE2\x80' for column 'name' at row 0 |
+---------+------+---------------------------------------------------------------+
1 row in set (0.00 sec)

select length(name), char_length(name) from flights where id=250908;
+--------------+-------------------+
| length(name) | char_length(name) |
+--------------+-------------------+
| 100 | 0 |
+--------------+-------------------+


select hex(name) from flights where id=250908;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hex(name) |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E280 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

In MySql db:
Retargeting - Internal Run of Site - Searches that contain at least one of the following keywords - ""ocean"", ""water"", ""view"" or ""esplanade""

The source database is running MySQL 5.6.22
The data is dumped using mysqldump, sent to the InfiniDB server and imported with cpimport, which we allow to truncate strings that are too long.
The use of the substr function in the query should be superfluous as far as InfiniDB is concerned, as the column is 100 bytes, it makes the query results compatible with the MySQL servers. 
That doesn't explain the char_length function results above or why InfiniDB is returning an empty string for the substr function.

I took the hex value and played with it on https://sites.google.com/site/nathanlexwww/tools/utf8-convert
The last bit, E280, appear to be a truncated multi-byte character from the original data. The '-' character.
the - code is E28093

Here is the hex of the original data from MySQL db:
| 5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E2809320E2809C6F6365616EE2809D2C20E2809C7761746572E2809D2C20E2809C76696577E2809D206F7220E2809C6573706C616E61646522 |

So, cpimport is truncating bytes disregarding the number of bytes in the UTF-8 character. This is bad, need a way to fix these.
The substr and char_length functions appears to be choking if it is missing all the bits in a multi-byte character, at the end of the string anyway.

Was not sure how to categorize this, Character Set, Table Corruption or General Usage.

Making the export process of this table do the substr function up front, is probably the correct way to fix this properly. But that means doing it for all tables with varchar fields and would preclude using mysqldump, making the process much more tedious and take a lot longer.
The best solution for us is for cpimport to work better with multi-byte character sets (utf-8 for us).

A few thoughts.
1. Could be fixed in the InfiniDB engine to ignore or deal with incomplete multi-byte characters.
2. Add a switch to the cpimport command to put it in a multi-byte character safe mode.
3. If 2 slows down cpimport very much make a cpimport that is multi-byte character safe, cpimportMBS, to be used when one is importing data in a multi-byte character set. OR even auto detect it based on the data (more accurate), or based on the table or column definition (least accurate).

In our case, multi-byte characters only appear in the dimension tables, which are much smaller than the fact tables. The fact tables having only a date, int and bigint columns, where import speed is important.

I'm sure you are all well versed in UTF-8, but thought I would send this along anyway.

https://en.wikipedia.org/wiki/UTF-8

So you don't have to examine the whole string to determine if the last character is valid (complete).

 $acceptance criteria:$",0,0,0,0,0,0,0,8530.18,8,2,0.25,0,0.0,0,0.0,0,0.0,0,0.0
316,MCOL-445,New Feature,MCOL,2016-12-06 18:28:49,,0,configxml.sh should be case in-sensitive.,"configxml.sh works case sensitive.
Sure would be nice if when doing a getconfig it would return matching items regardless of case, So the user could see how it is supposed to be.
Then when doing a setconfig, either work non-case sensitive or modify the existing non-case sensitive match or error if the case does not match, instead of creating a lower case version of an existing setting.

Not sure how that would impact the software when the server is started.

I think this is why i resisted using it the last few years preferring to hand edit the file.",,"configxml.sh should be case in-sensitive. $end$ configxml.sh works case sensitive.
Sure would be nice if when doing a getconfig it would return matching items regardless of case, So the user could see how it is supposed to be.
Then when doing a setconfig, either work non-case sensitive or modify the existing non-case sensitive match or error if the case does not match, instead of creating a lower case version of an existing setting.

Not sure how that would impact the software when the server is started.

I think this is why i resisted using it the last few years preferring to hand edit the file. $acceptance criteria:$",,David Hill,David Hill,Minor,8,,0,4,0,2,0,0,0,,0,850,3,0,0,2017-11-27 17:50:17,configxml.sh should be case in-sensitive.,"configxml.sh works case sensitive.
Sure would be nice if when doing a getconfig it would return matching items regardless of case, So the user could see how it is supposed to be.
Then when doing a setconfig, either work non-case sensitive or modify the existing non-case sensitive match or error if the case does not match, instead of creating a lower case version of an existing setting.

Not sure how that would impact the software when the server is started.

I think this is why i resisted using it the last few years preferring to hand edit the file.",,0,0,0,0,0.0,"configxml.sh should be case in-sensitive. $end$ configxml.sh works case sensitive.
Sure would be nice if when doing a getconfig it would return matching items regardless of case, So the user could see how it is supposed to be.
Then when doing a setconfig, either work non-case sensitive or modify the existing non-case sensitive match or error if the case does not match, instead of creating a lower case version of an existing setting.

Not sure how that would impact the software when the server is started.

I think this is why i resisted using it the last few years preferring to hand edit the file. $acceptance criteria:$",0,0,0,0,0,0,1,8543.35,9,2,0.222222,0,0.0,0,0.0,0,0.0,0,0.0
317,MCOL-4452,Sub-Task,MCOL,2020-12-15 18:22:29,,0,UDAF processing might return wrong results duplicating one of the previous group's results,"Consider the example:
{noformat}
MariaDB [test]> select CCHAR9, regr_avgx(CDECIMAL9_2, length(CCHAR9))  from datatypetestm1 where CCHAR9 in ('zzzzzzzzz','iiii') group by CCHAR9;
+-----------+----------------------------------------+
| CCHAR9    | regr_avgx(CDECIMAL9_2, length(CCHAR9)) |
+-----------+----------------------------------------+
| iiii      |                               9.000000 |
| zzzzzzzzz |                               9.000000 |
+-----------+----------------------------------------+
2 rows in set (0.021 sec)


{noformat}

The expected output must be this:
{noformat}
MariaDB [test]> select CCHAR9, regr_avgx(CDECIMAL9_2, length(CCHAR9))  from datatypetestm1 where CCHAR9 in ('zzzzzzzzz','iiii') group by CCHAR9;
+-----------+----------------------------------------+
| CCHAR9    | regr_avgx(CDECIMAL9_2, length(CCHAR9)) |
+-----------+----------------------------------------+
| iiii      |                               4.000000 |
| zzzzzzzzz |                               9.000000 |
+-----------+----------------------------------------+
2 rows in set (0.021 sec)
{noformat}",,"UDAF processing might return wrong results duplicating one of the previous group's results $end$ Consider the example:
{noformat}
MariaDB [test]> select CCHAR9, regr_avgx(CDECIMAL9_2, length(CCHAR9))  from datatypetestm1 where CCHAR9 in ('zzzzzzzzz','iiii') group by CCHAR9;
+-----------+----------------------------------------+
| CCHAR9    | regr_avgx(CDECIMAL9_2, length(CCHAR9)) |
+-----------+----------------------------------------+
| iiii      |                               9.000000 |
| zzzzzzzzz |                               9.000000 |
+-----------+----------------------------------------+
2 rows in set (0.021 sec)


{noformat}

The expected output must be this:
{noformat}
MariaDB [test]> select CCHAR9, regr_avgx(CDECIMAL9_2, length(CCHAR9))  from datatypetestm1 where CCHAR9 in ('zzzzzzzzz','iiii') group by CCHAR9;
+-----------+----------------------------------------+
| CCHAR9    | regr_avgx(CDECIMAL9_2, length(CCHAR9)) |
+-----------+----------------------------------------+
| iiii      |                               4.000000 |
| zzzzzzzzz |                               9.000000 |
+-----------+----------------------------------------+
2 rows in set (0.021 sec)
{noformat} $acceptance criteria:$",,Roman,Roman,Major,8,,0,1,0,16,0,3,0,,0,850,1,0,0,2020-12-15 18:22:29,Look into regr_ tests failing in test001,"Consider the example:
{noformat}
select  regr_avgx(CDECIMAL9_2, length(CCHAR9))  from datatypetestm1 where CCHAR9 in ('zzzzzzzzz','iiii') group by CCHAR9;

{noformat}",,1,2,0,96,3.2963,"Look into regr_ tests failing in test001 $end$ Consider the example:
{noformat}
select  regr_avgx(CDECIMAL9_2, length(CCHAR9))  from datatypetestm1 where CCHAR9 in ('zzzzzzzzz','iiii') group by CCHAR9;

{noformat} $acceptance criteria:$",3,1,1,1,1,1,1,0.0,38,8,0.210526,6,0.157895,3,0.0789474,2,0.0526316,1,0.0263158
318,MCOL-4457,Sub-Task,MCOL,2020-12-17 17:08:33,,0,Research current aggregation implementation,,,Research current aggregation implementation $end$ $acceptance criteria:$,,Roman,Roman,Major,2,,0,0,0,9,0,0,0,,0,850,0,0,0,2020-12-17 17:08:33,Research current aggregation implementation,,,0,0,0,0,0.0,Research current aggregation implementation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,39,9,0.230769,7,0.179487,4,0.102564,3,0.0769231,2,0.0512821
319,MCOL-4458,Sub-Task,MCOL,2020-12-17 17:08:55,,0,Create design plan,,,Create design plan $end$ $acceptance criteria:$,,Roman,Roman,Major,2,,0,0,0,9,0,0,0,,0,850,0,0,0,2020-12-17 17:08:55,Create design plan,,,0,0,0,0,0.0,Create design plan $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,40,9,0.225,7,0.175,4,0.1,3,0.075,2,0.05
320,MCOL-446,New Feature,MCOL,2016-12-06 18:31:27,,0,mycnf config change request,"Please consider adding these, at least the character set related items and max_length_for_sort_data to the file /usr/local/Calpont/bin/myCnf-include-args.text so that if a customer has them defined they get preserved during an upgrade. (values included for reference, as the file only contains the setting name)
The general and slow log items are not as important as the rest.

max_length_for_sort_data= 8388608
tmpdir = /db_storage/mysql_tmp/
log-error = /db_storage/log/mysqld.log
general_log_file = /db_storage/log/query.log
slow_query_log_file = /db_storage/log/slow_query.log
general-log = 0
slow-query-log = 0
character-set-server=utf8
collation-server=utf8_general_ci
init-connect='SET NAMES utf8'
binlog_format = ROW

please add this one to the list.
secure-auth

it defaults to off, allowing less secure password encryption to be used and stored in the mysql.users table.

http://dev.mysql.com/doc/refman/5.1/en/server-options.html#option_mysqld_secure-auth",,"mycnf config change request $end$ Please consider adding these, at least the character set related items and max_length_for_sort_data to the file /usr/local/Calpont/bin/myCnf-include-args.text so that if a customer has them defined they get preserved during an upgrade. (values included for reference, as the file only contains the setting name)
The general and slow log items are not as important as the rest.

max_length_for_sort_data= 8388608
tmpdir = /db_storage/mysql_tmp/
log-error = /db_storage/log/mysqld.log
general_log_file = /db_storage/log/query.log
slow_query_log_file = /db_storage/log/slow_query.log
general-log = 0
slow-query-log = 0
character-set-server=utf8
collation-server=utf8_general_ci
init-connect='SET NAMES utf8'
binlog_format = ROW

please add this one to the list.
secure-auth

it defaults to off, allowing less secure password encryption to be used and stored in the mysql.users table.

http://dev.mysql.com/doc/refman/5.1/en/server-options.html#option_mysqld_secure-auth $acceptance criteria:$",,David Hill,David Hill,Minor,9,,0,4,0,1,0,0,0,,0,850,3,0,0,2017-11-27 17:48:11,mycnf config change request,"Please consider adding these, at least the character set related items and max_length_for_sort_data to the file /usr/local/Calpont/bin/myCnf-include-args.text so that if a customer has them defined they get preserved during an upgrade. (values included for reference, as the file only contains the setting name)
The general and slow log items are not as important as the rest.

max_length_for_sort_data= 8388608
tmpdir = /db_storage/mysql_tmp/
log-error = /db_storage/log/mysqld.log
general_log_file = /db_storage/log/query.log
slow_query_log_file = /db_storage/log/slow_query.log
general-log = 0
slow-query-log = 0
character-set-server=utf8
collation-server=utf8_general_ci
init-connect='SET NAMES utf8'
binlog_format = ROW

please add this one to the list.
secure-auth

it defaults to off, allowing less secure password encryption to be used and stored in the mysql.users table.

http://dev.mysql.com/doc/refman/5.1/en/server-options.html#option_mysqld_secure-auth",,0,0,0,0,0.0,"mycnf config change request $end$ Please consider adding these, at least the character set related items and max_length_for_sort_data to the file /usr/local/Calpont/bin/myCnf-include-args.text so that if a customer has them defined they get preserved during an upgrade. (values included for reference, as the file only contains the setting name)
The general and slow log items are not as important as the rest.

max_length_for_sort_data= 8388608
tmpdir = /db_storage/mysql_tmp/
log-error = /db_storage/log/mysqld.log
general_log_file = /db_storage/log/query.log
slow_query_log_file = /db_storage/log/slow_query.log
general-log = 0
slow-query-log = 0
character-set-server=utf8
collation-server=utf8_general_ci
init-connect='SET NAMES utf8'
binlog_format = ROW

please add this one to the list.
secure-auth

it defaults to off, allowing less secure password encryption to be used and stored in the mysql.users table.

http://dev.mysql.com/doc/refman/5.1/en/server-options.html#option_mysqld_secure-auth $acceptance criteria:$",0,0,0,0,0,0,0,8543.27,10,2,0.2,0,0.0,0,0.0,0,0.0,0,0.0
321,MCOL-4463,Sub-Task,MCOL,2020-12-18 14:57:01,,0,Fix between for DECIMAL columns with certain literal values in projection.,"With the code at cc6c51b5 the test fails:
{noformat}
DROP TABLE cs1;
CREATE TABLE cs1 (d1 DECIMAL(38))engine=columnstore;
INSERT INTO cs1 VALUES (123456);
SELECT ""between_test"", d1 BETWEEN 123455 AND 123457 f1 FROM cs1;
{noformat}

This happens b/c of the error introduced by the recent decimal comparison refactoring. 
The patch also addresses the fact that ConstantColumn() creates fResult.decimalVal as a narrow decimal for literals 123455 and 123457 b/c MDB sends the literals as an INTs and not wide-DECIMALs.",,"Fix between for DECIMAL columns with certain literal values in projection. $end$ With the code at cc6c51b5 the test fails:
{noformat}
DROP TABLE cs1;
CREATE TABLE cs1 (d1 DECIMAL(38))engine=columnstore;
INSERT INTO cs1 VALUES (123456);
SELECT ""between_test"", d1 BETWEEN 123455 AND 123457 f1 FROM cs1;
{noformat}

This happens b/c of the error introduced by the recent decimal comparison refactoring. 
The patch also addresses the fact that ConstantColumn() creates fResult.decimalVal as a narrow decimal for literals 123455 and 123457 b/c MDB sends the literals as an INTs and not wide-DECIMALs. $acceptance criteria:$",,Roman,Roman,Major,4,,0,0,0,16,0,0,0,,0,850,0,0,0,2020-12-18 14:57:01,Fix between for DECIMAL columns with certain literal values in projection.,"With the code at cc6c51b5 the test fails:
{noformat}
DROP TABLE cs1;
CREATE TABLE cs1 (d1 DECIMAL(38))engine=columnstore;
INSERT INTO cs1 VALUES (123456);
SELECT ""between_test"", d1 BETWEEN 123455 AND 123457 f1 FROM cs1;
{noformat}

This happens b/c of the error introduced by the recent decimal comparison refactoring. 
The patch also addresses the fact that ConstantColumn() creates fResult.decimalVal as a narrow decimal for literals 123455 and 123457 b/c MDB sends the literals as an INTs and not wide-DECIMALs.",,0,0,0,0,0.0,"Fix between for DECIMAL columns with certain literal values in projection. $end$ With the code at cc6c51b5 the test fails:
{noformat}
DROP TABLE cs1;
CREATE TABLE cs1 (d1 DECIMAL(38))engine=columnstore;
INSERT INTO cs1 VALUES (123456);
SELECT ""between_test"", d1 BETWEEN 123455 AND 123457 f1 FROM cs1;
{noformat}

This happens b/c of the error introduced by the recent decimal comparison refactoring. 
The patch also addresses the fact that ConstantColumn() creates fResult.decimalVal as a narrow decimal for literals 123455 and 123457 b/c MDB sends the literals as an INTs and not wide-DECIMALs. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,41,9,0.219512,7,0.170732,4,0.097561,3,0.0731707,2,0.0487805
322,MCOL-4478,Sub-Task,MCOL,2020-12-30 10:19:48,,0,avg() on wide DECIMAL doesn't round the last digit properly,"Consider the scenario:
{noformat}
CREATE TABLE cs5(d DECIMAL(25,19)) ENGINE=columnstore;
INSERT INTO cs5 VALUES (1),(0.1),(0.05),(0.05),(0.05),(0.05),(0.2),(0.154),(0.1),(0.05),(0.05);
SELECT avg(d) FROM cs5;
{noformat}

The expected result is:
{noformat}
avg(d)
0.16854545454545454545455
{noformat}
however MCS doesn't round the last digit properly
{noformat}
avg(d)
0.16854545454545454545454
{noformat}
",,"avg() on wide DECIMAL doesn't round the last digit properly $end$ Consider the scenario:
{noformat}
CREATE TABLE cs5(d DECIMAL(25,19)) ENGINE=columnstore;
INSERT INTO cs5 VALUES (1),(0.1),(0.05),(0.05),(0.05),(0.05),(0.2),(0.154),(0.1),(0.05),(0.05);
SELECT avg(d) FROM cs5;
{noformat}

The expected result is:
{noformat}
avg(d)
0.16854545454545454545455
{noformat}
however MCS doesn't round the last digit properly
{noformat}
avg(d)
0.16854545454545454545454
{noformat}
 $acceptance criteria:$",,Roman,Roman,Minor,5,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-12-30 10:19:48,avg() on wide DECIMAL doesn't round the last digit properly,"Consider the scenario:
{noformat}
CREATE TABLE cs5(d DECIMAL(25,19)) ENGINE=columnstore;
INSERT INTO cs5 VALUES (1),(0.1),(0.05),(0.05),(0.05),(0.05),(0.2),(0.154),(0.1),(0.05),(0.05);
SELECT avg(d) FROM cs5;
{noformat}

The expected result is:
{noformat}
avg(d)
0.16854545454545454545455
{noformat}
however MCS doesn't round the last digit properly
{noformat}
avg(d)
0.16854545454545454545454
{noformat}
",,0,0,0,0,0.0,"avg() on wide DECIMAL doesn't round the last digit properly $end$ Consider the scenario:
{noformat}
CREATE TABLE cs5(d DECIMAL(25,19)) ENGINE=columnstore;
INSERT INTO cs5 VALUES (1),(0.1),(0.05),(0.05),(0.05),(0.05),(0.2),(0.154),(0.1),(0.05),(0.05);
SELECT avg(d) FROM cs5;
{noformat}

The expected result is:
{noformat}
avg(d)
0.16854545454545454545455
{noformat}
however MCS doesn't round the last digit properly
{noformat}
avg(d)
0.16854545454545454545454
{noformat}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,42,9,0.214286,7,0.166667,4,0.0952381,3,0.0714286,2,0.047619
323,MCOL-4479,Sub-Task,MCOL,2020-12-30 10:29:58,,0,Remove libquadmath as the dependency,"We now use libquadmath functions in ./utils/funcexp/func_cast.cpp and ./utils/funcexp/func_mod.cpp. This is the dependency we should avoid b/c:
- MCS doesn't need the full libquadmath only two functions out of it
- the dependency adds extra complexity when it gets down to prerequisite packages installation",,"Remove libquadmath as the dependency $end$ We now use libquadmath functions in ./utils/funcexp/func_cast.cpp and ./utils/funcexp/func_mod.cpp. This is the dependency we should avoid b/c:
- MCS doesn't need the full libquadmath only two functions out of it
- the dependency adds extra complexity when it gets down to prerequisite packages installation $acceptance criteria:$",,Roman,Roman,Minor,6,,0,1,0,16,0,0,0,,0,850,1,0,0,2020-12-30 10:29:58,Remove libquadmath as the dependency,"We now use libquadmath functions in ./utils/funcexp/func_cast.cpp and ./utils/funcexp/func_mod.cpp. This is the dependency we should avoid b/c:
- MCS doesn't need the full libquadmath only two functions out of it
- the dependency adds extra complexity when it gets down to prerequisite packages installation",,0,0,0,0,0.0,"Remove libquadmath as the dependency $end$ We now use libquadmath functions in ./utils/funcexp/func_cast.cpp and ./utils/funcexp/func_mod.cpp. This is the dependency we should avoid b/c:
- MCS doesn't need the full libquadmath only two functions out of it
- the dependency adds extra complexity when it gets down to prerequisite packages installation $acceptance criteria:$",0,0,0,0,0,0,1,0.0,43,9,0.209302,7,0.162791,4,0.0930233,3,0.0697674,2,0.0465116
324,MCOL-4513,Task,MCOL,2021-01-22 16:39:56,,0,"Investigate the original reasons why ColumnStore is flagging ""circular joins"" as an error, and provide guidance as to how to stop it from happening.","SELECT * FROM a,b,c WHERE a.fk=b.PK and b.FK=c.PK and a.FOO = c.FOO

For historical reasons (nolonger known) ColumnStore is unable to do what every other relational databases do - break the loop in the join graph, and treat one of cross-table clauses as a filter as opposed to join condition (we are talking about b.FK=c.PK vs a.FOO=c.FOO - one of them has to be dropped from the join graph and become a post join filter).

This task is to figure out why it has not been done (if possible), and figure out how to solve it in the safest manner.",,"Investigate the original reasons why ColumnStore is flagging ""circular joins"" as an error, and provide guidance as to how to stop it from happening. $end$ SELECT * FROM a,b,c WHERE a.fk=b.PK and b.FK=c.PK and a.FOO = c.FOO

For historical reasons (nolonger known) ColumnStore is unable to do what every other relational databases do - break the loop in the join graph, and treat one of cross-table clauses as a filter as opposed to join condition (we are talking about b.FK=c.PK vs a.FOO=c.FOO - one of them has to be dropped from the join graph and become a post join filter).

This task is to figure out why it has not been done (if possible), and figure out how to solve it in the safest manner. $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Major,11,,1,1,1,2,0,0,0,,0,850,1,0,0,2021-04-09 13:34:51,"Investigate the original reasons why ColumnStore is flagging ""circular joins"" as an error, and provide guidance as to how to stop it from happening.","SELECT * FROM a,b,c WHERE a.fk=b.PK and b.FK=c.PK and a.FOO = c.FOO

For historical reasons (nolonger known) ColumnStore is unable to do what every other relational databases do - break the loop in the join graph, and treat one of cross-table clauses as a filter as opposed to join condition (we are talking about b.FK=c.PK vs a.FOO=c.FOO - one of them has to be dropped from the join graph and become a post join filter).

This task is to figure out why it has not been done (if possible), and figure out how to solve it in the safest manner.",,0,0,0,0,0.0,"Investigate the original reasons why ColumnStore is flagging ""circular joins"" as an error, and provide guidance as to how to stop it from happening. $end$ SELECT * FROM a,b,c WHERE a.fk=b.PK and b.FK=c.PK and a.FOO = c.FOO

For historical reasons (nolonger known) ColumnStore is unable to do what every other relational databases do - break the loop in the join graph, and treat one of cross-table clauses as a filter as opposed to join condition (we are talking about b.FK=c.PK vs a.FOO=c.FOO - one of them has to be dropped from the join graph and become a post join filter).

This task is to figure out why it has not been done (if possible), and figure out how to solve it in the safest manner. $acceptance criteria:$",0,0,0,0,0,0,1,1844.9,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
325,MCOL-4520,Task,MCOL,2021-01-28 15:58:12,,0,Move mtr suites to columnstore engine repo,,,Move mtr suites to columnstore engine repo $end$ $acceptance criteria:$,,Roman Navrotskiy,Roman Navrotskiy,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-01-28 20:27:58,Move mtr suites to columnstore engine repo,,,0,0,0,0,0.0,Move mtr suites to columnstore engine repo $end$ $acceptance criteria:$,0,0,0,0,0,0,0,4.48333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
326,MCOL-4525,Task,MCOL,2021-02-01 15:37:17,,0,Implement select_handler=AUTO,"In 1.2 time Columnstore supported set infinidb_vtable_mode = 2.

This meant ""auto-switch mode"": ColumnStore will attempt to process the query internally, if it cannot, it will automatically switch the query to run in row-by-row mode.

The use case described the CS0142347 is a non-equi join.

There are certainly other cases where this was happening in 1.2, but now fails with an error message. We should attempt to restore the behavior to be as close to what it was in 1.2 as possible. 

Best if we could also produce a standard MariaDB warning if the query ends up being done in ROW mode. If this is too hard or impossible - a message in debug log would be needed.",,"Implement select_handler=AUTO $end$ In 1.2 time Columnstore supported set infinidb_vtable_mode = 2.

This meant ""auto-switch mode"": ColumnStore will attempt to process the query internally, if it cannot, it will automatically switch the query to run in row-by-row mode.

The use case described the CS0142347 is a non-equi join.

There are certainly other cases where this was happening in 1.2, but now fails with an error message. We should attempt to restore the behavior to be as close to what it was in 1.2 as possible. 

Best if we could also produce a standard MariaDB warning if the query ends up being done in ROW mode. If this is too hard or impossible - a message in debug log would be needed. $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Blocker,41,,0,3,1,8,0,4,2,,0,850,3,0,0,2021-02-01 15:39:06,Implement select_handler=AUTO,"In 1.2 time Columnstore supported set infinidb_vtable_mode = 2.

This meant ""auto-switch mode"": ColumnStore will attempt to process the query internally, if it cannot, it will automatically switch the query to run in row-by-row mode.

One use case is represented by CS0142347 (Qberg), with their reporduction below. This happened to be non equi join. There are certainly other cases where this was happening in 1.2, but now fails with an error message. We should attempt to restore the behavior to be as close to what it was in 1.2 as possible. 

Best if we could also produce a standard MariaDB warning if the query ends up being done in ROW mode. If this is too hard or impossible - a message in debug log would be needed.

Qberg's case:

SET columnstore_select_handler=1;
 
SELECT COUNT(DISTINCT cwc_id) npres,COUNT(DISTINCT cwc_cw_id) ncw,cwc_in_id insegna
FROM (qcommon.giorno,dmc_dg_ita_19.campagna_web_codice)
WHERE cwc_fa_id IN (23) AND giorno BETWEEN cwc_datainizio AND cwc_datafine AND mese IN (202009)
GROUP BY insegna;
 ERROR 1815 (HY000): Internal error: IDB-1000: 'giorno' and 'campagna_web_codice' are not joined.

    Table: giorno
CREATE TABLE `giorno` (
 `giorno` date NOT NULL DEFAULT '0000-00-00',
 `giornoa` mediumint(7) DEFAULT NULL,
 `mese` mediumint(6) DEFAULT NULL,
 `settimana` mediumint(6) DEFAULT NULL,
 `quindicina` mediumint(6) DEFAULT NULL,
 `trimestre` smallint(5) DEFAULT NULL,
 `quadrimestre` smallint(5) DEFAULT NULL,
 `semestre` smallint(5) DEFAULT NULL,
 `bimestre` smallint(5) DEFAULT NULL,
 `anno` smallint(4) DEFAULT NULL,
 PRIMARY KEY (`giorno`),
 KEY `mese` (`mese`),
 KEY `quindicina` (`quindicina`),
 KEY `settimana` (`settimana`),
 KEY `trimestre` (`trimestre`),
 KEY `semestre` (`semestre`),
 KEY `bimestre` (`bimestre`),
 KEY `quadrimestre` (`quadrimestre`),
 KEY `giornoa` (`giornoa`)
 ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='Giorni'

 Table: campagna_web_codice
CREATE TABLE `campagna_web_codice` (
 `cwc_id` int(11) DEFAULT NULL,
 `cwc_cw_id` int(11) DEFAULT NULL,
 `cwc_cd_id` int(11) DEFAULT NULL,
 `cwc_codice` varchar(30) DEFAULT NULL,
 `cwc_va_id` int(11) DEFAULT NULL,
 `cwc_prezzo` decimal(12,2) DEFAULT NULL,
 `cwc_prezzoListino` decimal(12,2) DEFAULT NULL,
 `cwc_prezzoSpedizione` decimal(8,2) DEFAULT NULL,
 `cwc_in_id` int(11) DEFAULT NULL,
 `cwc_na_id` int(11) DEFAULT NULL,
 `cwc_desc` varchar(100) DEFAULT NULL,
 `cwc_datainizio` date DEFAULT NULL,
 `cwc_datafine` date DEFAULT NULL,
 `cwc_mo_id` int(11) DEFAULT NULL,
 `cwc_ma_id` int(11) DEFAULT NULL,
 `cwc_pd_id` int(11) DEFAULT NULL,
 `cwc_set_id` int(11) DEFAULT NULL,
 `cwc_mr_id` int(11) DEFAULT NULL,
 `cwc_ca_id` int(11) DEFAULT NULL,
 `cwc_fa_id` int(11) DEFAULT NULL,
 `cwc_ti_id` int(11) DEFAULT NULL,
 `cwc_azws` varchar(255) DEFAULT NULL
 ) ENGINE=Columnstore DEFAULT CHARSET=utf8",,0,4,0,241,0.667614,"Implement select_handler=AUTO $end$ In 1.2 time Columnstore supported set infinidb_vtable_mode = 2.

This meant ""auto-switch mode"": ColumnStore will attempt to process the query internally, if it cannot, it will automatically switch the query to run in row-by-row mode.

One use case is represented by CS0142347 (Qberg), with their reporduction below. This happened to be non equi join. There are certainly other cases where this was happening in 1.2, but now fails with an error message. We should attempt to restore the behavior to be as close to what it was in 1.2 as possible. 

Best if we could also produce a standard MariaDB warning if the query ends up being done in ROW mode. If this is too hard or impossible - a message in debug log would be needed.

Qberg's case:

SET columnstore_select_handler=1;
 
SELECT COUNT(DISTINCT cwc_id) npres,COUNT(DISTINCT cwc_cw_id) ncw,cwc_in_id insegna
FROM (qcommon.giorno,dmc_dg_ita_19.campagna_web_codice)
WHERE cwc_fa_id IN (23) AND giorno BETWEEN cwc_datainizio AND cwc_datafine AND mese IN (202009)
GROUP BY insegna;
 ERROR 1815 (HY000): Internal error: IDB-1000: 'giorno' and 'campagna_web_codice' are not joined.

    Table: giorno
CREATE TABLE `giorno` (
 `giorno` date NOT NULL DEFAULT '0000-00-00',
 `giornoa` mediumint(7) DEFAULT NULL,
 `mese` mediumint(6) DEFAULT NULL,
 `settimana` mediumint(6) DEFAULT NULL,
 `quindicina` mediumint(6) DEFAULT NULL,
 `trimestre` smallint(5) DEFAULT NULL,
 `quadrimestre` smallint(5) DEFAULT NULL,
 `semestre` smallint(5) DEFAULT NULL,
 `bimestre` smallint(5) DEFAULT NULL,
 `anno` smallint(4) DEFAULT NULL,
 PRIMARY KEY (`giorno`),
 KEY `mese` (`mese`),
 KEY `quindicina` (`quindicina`),
 KEY `settimana` (`settimana`),
 KEY `trimestre` (`trimestre`),
 KEY `semestre` (`semestre`),
 KEY `bimestre` (`bimestre`),
 KEY `quadrimestre` (`quadrimestre`),
 KEY `giornoa` (`giornoa`)
 ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='Giorni'

 Table: campagna_web_codice
CREATE TABLE `campagna_web_codice` (
 `cwc_id` int(11) DEFAULT NULL,
 `cwc_cw_id` int(11) DEFAULT NULL,
 `cwc_cd_id` int(11) DEFAULT NULL,
 `cwc_codice` varchar(30) DEFAULT NULL,
 `cwc_va_id` int(11) DEFAULT NULL,
 `cwc_prezzo` decimal(12,2) DEFAULT NULL,
 `cwc_prezzoListino` decimal(12,2) DEFAULT NULL,
 `cwc_prezzoSpedizione` decimal(8,2) DEFAULT NULL,
 `cwc_in_id` int(11) DEFAULT NULL,
 `cwc_na_id` int(11) DEFAULT NULL,
 `cwc_desc` varchar(100) DEFAULT NULL,
 `cwc_datainizio` date DEFAULT NULL,
 `cwc_datafine` date DEFAULT NULL,
 `cwc_mo_id` int(11) DEFAULT NULL,
 `cwc_ma_id` int(11) DEFAULT NULL,
 `cwc_pd_id` int(11) DEFAULT NULL,
 `cwc_set_id` int(11) DEFAULT NULL,
 `cwc_mr_id` int(11) DEFAULT NULL,
 `cwc_ca_id` int(11) DEFAULT NULL,
 `cwc_fa_id` int(11) DEFAULT NULL,
 `cwc_ti_id` int(11) DEFAULT NULL,
 `cwc_azws` varchar(255) DEFAULT NULL
 ) ENGINE=Columnstore DEFAULT CHARSET=utf8 $acceptance criteria:$",4,1,1,1,1,1,1,0.0166667,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
327,MCOL-4528,Sub-Task,MCOL,2021-02-02 17:01:21,,0,Analyze and explain the reasons for the difference between 5.4 and 5.5,"Basic facts as known today:
* Available traces show 2x elapsed time between 5.4 and 5.5
* They also show 50% difference in LIO
* This is almost certainly NOT extent elimination problem. The reason for this statement is that when this query was run using utf8, it did not do extent elimination, and the difference was close to 4x on elapsed and 3x on LIO.
* Personal opinions point to CPU overhead of collations. But before this is positively determined - the difference in LIO needs to be explaind.
REPEAT: this task is RESEARCH and diagnosis only. No solutions are to be proposed at this time.

",,"Analyze and explain the reasons for the difference between 5.4 and 5.5 $end$ Basic facts as known today:
* Available traces show 2x elapsed time between 5.4 and 5.5
* They also show 50% difference in LIO
* This is almost certainly NOT extent elimination problem. The reason for this statement is that when this query was run using utf8, it did not do extent elimination, and the difference was close to 4x on elapsed and 3x on LIO.
* Personal opinions point to CPU overhead of collations. But before this is positively determined - the difference in LIO needs to be explaind.
REPEAT: this task is RESEARCH and diagnosis only. No solutions are to be proposed at this time.

 $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Major,3,,0,1,0,2,0,0,0,,0,850,1,0,0,2021-02-02 17:01:21,Analyze and explain the reasons for the difference between 5.4 and 5.5,"Basic facts as known today:
* Available traces show 2x elapsed time between 5.4 and 5.5
* They also show 50% difference in LIO
* This is almost certainly NOT extent elimination problem. The reason for this statement is that when this query was run using utf8, it did not do extent elimination, and the difference was close to 4x on elapsed and 3x on LIO.
* Personal opinions point to CPU overhead of collations. But before this is positively determined - the difference in LIO needs to be explaind.
REPEAT: this task is RESEARCH and diagnosis only. No solutions are to be proposed at this time.

",,0,0,0,0,0.0,"Analyze and explain the reasons for the difference between 5.4 and 5.5 $end$ Basic facts as known today:
* Available traces show 2x elapsed time between 5.4 and 5.5
* They also show 50% difference in LIO
* This is almost certainly NOT extent elimination problem. The reason for this statement is that when this query was run using utf8, it did not do extent elimination, and the difference was close to 4x on elapsed and 3x on LIO.
* Personal opinions point to CPU overhead of collations. But before this is positively determined - the difference in LIO needs to be explaind.
REPEAT: this task is RESEARCH and diagnosis only. No solutions are to be proposed at this time.

 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
328,MCOL-4529,Task,MCOL,2021-02-02 17:42:03,MCOL-3525,0,Design the proper way of doing extent elimination on varchars with utf8 or utf8mb4,"Overall - the top ticket (4522) is a massive regression. 

Starting in 5.5, the true length of varchar and char conforms to SQL standard, and incorporates byte counts for character sets such as utf8, and utf8mb4. Up to and including 5.4, it was wrong, and only reserved the bytes equal to the number of characters defined in the column DDL. The side effect of it is exemplified in the customer complaint: a varchar(5) under olf utf8 used to fit, and filters on it were subject to extent elimination. With 5.5, it no longer fits, which causes columnstore to essentially ignore an otherwise tight filter, and to proceed with the full scan.

",,"Design the proper way of doing extent elimination on varchars with utf8 or utf8mb4 $end$ Overall - the top ticket (4522) is a massive regression. 

Starting in 5.5, the true length of varchar and char conforms to SQL standard, and incorporates byte counts for character sets such as utf8, and utf8mb4. Up to and including 5.4, it was wrong, and only reserved the bytes equal to the number of characters defined in the column DDL. The side effect of it is exemplified in the customer complaint: a varchar(5) under olf utf8 used to fit, and filters on it were subject to extent elimination. With 5.5, it no longer fits, which causes columnstore to essentially ignore an otherwise tight filter, and to proceed with the full scan.

 $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Critical,18,,1,10,3,4,0,3,0,,0,850,10,0,0,2021-02-02 17:59:03,Design proper way of doing extent elimination on varchars with utf8 or utf8mb4,"Overall - the top ticket (4522) is a massive regression. But, as the additional description paragraph shows, the problem has two parts. 

Part 1. Starting in 5.5, the true length of varchar and char conforms to SQL standard, and incorporates byte counts for character sets such as utf8, and utf8mb4. Up to and including 5.4, it was wrong, and only reserved the bytes equal to the number of characters defined in the column DDL. The side effect of it is exemplified in the customer complaint: a varchar(5) under olf utf8 used to fit, and filters on it were subject to extent elimination. With 5.5, it no longer fits, which causes columnstore to essentially ignore an otherwise tight filter, and to proceed with the full scan.

This subtask is limited ONLY to a design. No coding should be done until that design is reviewed and blessed for execution - at which point there will be another sub-task.

Part 2. It seems that even if the charset is defined as latin1, we have lost performance to the tune of 2x. This subtask DOES NOT involve this problem - there is a separate subordinate ticket  to deal with that.",,2,1,0,86,0.40566,"Design proper way of doing extent elimination on varchars with utf8 or utf8mb4 $end$ Overall - the top ticket (4522) is a massive regression. But, as the additional description paragraph shows, the problem has two parts. 

Part 1. Starting in 5.5, the true length of varchar and char conforms to SQL standard, and incorporates byte counts for character sets such as utf8, and utf8mb4. Up to and including 5.4, it was wrong, and only reserved the bytes equal to the number of characters defined in the column DDL. The side effect of it is exemplified in the customer complaint: a varchar(5) under olf utf8 used to fit, and filters on it were subject to extent elimination. With 5.5, it no longer fits, which causes columnstore to essentially ignore an otherwise tight filter, and to proceed with the full scan.

This subtask is limited ONLY to a design. No coding should be done until that design is reviewed and blessed for execution - at which point there will be another sub-task.

Part 2. It seems that even if the charset is defined as latin1, we have lost performance to the tune of 2x. This subtask DOES NOT involve this problem - there is a separate subordinate ticket  to deal with that. $acceptance criteria:$",3,1,1,1,1,1,1,0.283333,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
329,MCOL-453,New Feature,MCOL,2016-12-08 15:08:28,,0,change mcsadmin to only allow the module add/remove/disable commands from active PM Module,"Change the following commands to only allow them to be run from the Active OAM Parent PM module, which generally is PM1. All change and update type commands should only be run from this module.

Commands changed:

addModule
removeModule
alterSystem-disableModule
alterSystem-enableModule
",,"change mcsadmin to only allow the module add/remove/disable commands from active PM Module $end$ Change the following commands to only allow them to be run from the Active OAM Parent PM module, which generally is PM1. All change and update type commands should only be run from this module.

Commands changed:

addModule
removeModule
alterSystem-disableModule
alterSystem-enableModule
 $acceptance criteria:$",,David Hill,David Hill,Minor,27,,0,10,0,4,0,0,0,,0,850,7,0,0,2016-12-09 15:46:52,change mcsadmin to only allow the module add/remove/disable commands from active PM Module,"Change the following commands to only allow them to be run from the Active OAM Parent PM module, which generally is PM1. All change and update type commands should only be run from this module.

Commands changed:

addModule
removeModule
alterSystem-disableModule
alterSystem-enableModule
",,0,0,0,0,0.0,"change mcsadmin to only allow the module add/remove/disable commands from active PM Module $end$ Change the following commands to only allow them to be run from the Active OAM Parent PM module, which generally is PM1. All change and update type commands should only be run from this module.

Commands changed:

addModule
removeModule
alterSystem-disableModule
alterSystem-enableModule
 $acceptance criteria:$",0,0,0,0,0,0,1,24.6333,11,2,0.181818,0,0.0,0,0.0,0,0.0,0,0.0
330,MCOL-4560,Task,MCOL,2021-02-25 05:58:22,MCOL-4561,0,Remove All Legacy Variables From Columnstore.xml,Many of the variables in Columnstore.xml are deprecated or relate to OAM (which has been removed). We should clean this up to alleviate confusion.,,Remove All Legacy Variables From Columnstore.xml $end$ Many of the variables in Columnstore.xml are deprecated or relate to OAM (which has been removed). We should clean this up to alleviate confusion. $acceptance criteria:$,,Todd Stoffel,Todd Stoffel,Major,26,,0,4,1,8,0,0,0,,0,850,4,0,0,2021-08-06 21:23:30,Remove All Legacy Variables From Columnstore.xml,Many of the variables in Columnstore.xml are deprecated or relate to OAM (which has been removed). We should clean this up to alleviate confusion.,,0,0,0,0,0.0,Remove All Legacy Variables From Columnstore.xml $end$ Many of the variables in Columnstore.xml are deprecated or relate to OAM (which has been removed). We should clean this up to alleviate confusion. $acceptance criteria:$,0,0,0,0,0,0,1,3903.42,6,1,0.166667,0,0.0,0,0.0,0,0.0,0,0.0
331,MCOL-4566,New Feature,MCOL,2021-03-01 14:33:13,,0,rebuildEM utility must support compressed segment files,"There are certain situations when it is impossible to restore metadata information that either was completely or partially lost. Metadata in this case includes Extent Map and auxilary database OID counter used to allocate OIDs for database object to be created.
This issue describes the part that manages with Extent Map partial or comple loss. There is a tool under tool/rebuildEM called rebuildEM which algo creates an Extent Map using the existing columnar data files. Its algorithm is very simple it counts a number of blocks and creates corresponding number of extents in the new in-memory Extent Map. In the end it writes an image of the Extent Map to disk.
The main problem with the tool is that it doesn't support compressed(MCS as of 6.1.1 supports Snappy compression) files. The suggested approach is to change the tool's algorithm so that it uses compressed files to produce Extent Map. A compressed file contains a structure CompressedDBFileHeader as the header. The   fBlockCount attribute points to a number of blocks in the file.
The important assumption for the tool is that all dbroots must be available at the node where rebuildEM is run so the cluster must either have a shared or S3 storage.
To be noted that we are going to add LZ4 as a compression method.
One must remove extent map file to test how rebuildEM works with  compressed files.
",,"rebuildEM utility must support compressed segment files $end$ There are certain situations when it is impossible to restore metadata information that either was completely or partially lost. Metadata in this case includes Extent Map and auxilary database OID counter used to allocate OIDs for database object to be created.
This issue describes the part that manages with Extent Map partial or comple loss. There is a tool under tool/rebuildEM called rebuildEM which algo creates an Extent Map using the existing columnar data files. Its algorithm is very simple it counts a number of blocks and creates corresponding number of extents in the new in-memory Extent Map. In the end it writes an image of the Extent Map to disk.
The main problem with the tool is that it doesn't support compressed(MCS as of 6.1.1 supports Snappy compression) files. The suggested approach is to change the tool's algorithm so that it uses compressed files to produce Extent Map. A compressed file contains a structure CompressedDBFileHeader as the header. The   fBlockCount attribute points to a number of blocks in the file.
The important assumption for the tool is that all dbroots must be available at the node where rebuildEM is run so the cluster must either have a shared or S3 storage.
To be noted that we are going to add LZ4 as a compression method.
One must remove extent map file to test how rebuildEM works with  compressed files.
 $acceptance criteria:$",,Roman,Roman,Minor,64,,0,24,4,9,0,2,2,,0,850,22,1,0,2021-03-05 12:46:10,rebuildEM utility must support comressed segment files,"There are certain situations when it is impossible to restore metadata information that either was completely or partially lost. Metadata in this case includes Extent Map and auxilary database OID counter used to allocate OIDs for database object to be created.
This issue describes the part that manages with Extent Map partial or comple loss. There is a tool under tool/rebuildEM called rebuildEM which algo creates an Extent Map using the existing columnar data files. Its algorithm is very simple it counts a number of blocks and creates corresponding number of extents in the new in-memory Extent Map. In the end it writes an image of the Extent Map to disk.
The main problem with the tool is that it doesn't support compressed(MCS as of 6.1.1 supports Snappy compression) files. The suggested approach is to change the tool's algorithm so that it uses compressed files to produce Extent Map. A compressed file contains a structure CompressedDBFileHeader as the header. The   fBlockCount attribute points to a number of blocks in the file.
The important assumption for the tool is that all dbroots must be available at the node where rebuildEM is run so the cluster must either have a shared or S3 storage.
To be noted that we are going to add LZ4 as a compression method.
One must remove extent map file to test how rebuildEM works with  compressed files.
",,1,0,0,2,0.00416667,"rebuildEM utility must support comressed segment files $end$ There are certain situations when it is impossible to restore metadata information that either was completely or partially lost. Metadata in this case includes Extent Map and auxilary database OID counter used to allocate OIDs for database object to be created.
This issue describes the part that manages with Extent Map partial or comple loss. There is a tool under tool/rebuildEM called rebuildEM which algo creates an Extent Map using the existing columnar data files. Its algorithm is very simple it counts a number of blocks and creates corresponding number of extents in the new in-memory Extent Map. In the end it writes an image of the Extent Map to disk.
The main problem with the tool is that it doesn't support compressed(MCS as of 6.1.1 supports Snappy compression) files. The suggested approach is to change the tool's algorithm so that it uses compressed files to produce Extent Map. A compressed file contains a structure CompressedDBFileHeader as the header. The   fBlockCount attribute points to a number of blocks in the file.
The important assumption for the tool is that all dbroots must be available at the node where rebuildEM is run so the cluster must either have a shared or S3 storage.
To be noted that we are going to add LZ4 as a compression method.
One must remove extent map file to test how rebuildEM works with  compressed files.
 $acceptance criteria:$",1,1,0,0,0,0,1,94.2,44,9,0.204545,7,0.159091,4,0.0909091,3,0.0681818,2,0.0454545
332,MCOL-4585,Task,MCOL,2021-03-05 16:56:49,,0,group_concat followed by regr_avgx gives wrong results,"The following query gives bad results:
select l_orderkey, group_concat(l_partkey, ""TF"" order by l_partkey) partkeys, regr_avgx(l_tax, l_extendedprice) avg_price from lineitem where l_orderkey < 10 group by l_orderkey order by l_orderkey;
+------------+--------------------------------------------------------------+--------------+
| l_orderkey | partkeys                                                     | avg_price    |
+------------+--------------------------------------------------------------+--------------+
|          1 | 2132TF,15635TF,24027TF,63700TF,67310TF,155190TF              | 30310.211667 |
|          2 | 106170TF                                                     |     0.000000 |
|          3 | 4297TF,19036TF,29380TF,62143TF,128449TF,183095TF             |     0.000000 |
|          4 | 88035TF                                                      |     0.000000 |
|          5 | 37531TF,108570TF,123927TF                                    | 49276.323333 |
|          6 | 139636TF                                                     | 61998.310000 |
|          7 | 79251TF,94780TF,145243TF,151894TF,157238TF,163073TF,182052TF | 37447.331429 |
+------------+--------------------------------------------------------------+--------------+
7 rows in set (0.154 sec)

There should be no 0.00000 in there",,"group_concat followed by regr_avgx gives wrong results $end$ The following query gives bad results:
select l_orderkey, group_concat(l_partkey, ""TF"" order by l_partkey) partkeys, regr_avgx(l_tax, l_extendedprice) avg_price from lineitem where l_orderkey < 10 group by l_orderkey order by l_orderkey;
+------------+--------------------------------------------------------------+--------------+
| l_orderkey | partkeys                                                     | avg_price    |
+------------+--------------------------------------------------------------+--------------+
|          1 | 2132TF,15635TF,24027TF,63700TF,67310TF,155190TF              | 30310.211667 |
|          2 | 106170TF                                                     |     0.000000 |
|          3 | 4297TF,19036TF,29380TF,62143TF,128449TF,183095TF             |     0.000000 |
|          4 | 88035TF                                                      |     0.000000 |
|          5 | 37531TF,108570TF,123927TF                                    | 49276.323333 |
|          6 | 139636TF                                                     | 61998.310000 |
|          7 | 79251TF,94780TF,145243TF,151894TF,157238TF,163073TF,182052TF | 37447.331429 |
+------------+--------------------------------------------------------------+--------------+
7 rows in set (0.154 sec)

There should be no 0.00000 in there $acceptance criteria:$",,David Hall,David Hall,Blocker,9,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-03-05 17:11:17,group_concat followed by regr_avgx gives wrong results,"The following query gives bad results:
select l_orderkey, group_concat(l_partkey, ""TF"" order by l_partkey) partkeys, regr_avgx(l_tax, l_extendedprice) avg_price from lineitem where l_orderkey < 10 group by l_orderkey order by l_orderkey;
+------------+--------------------------------------------------------------+--------------+
| l_orderkey | partkeys                                                     | avg_price    |
+------------+--------------------------------------------------------------+--------------+
|          1 | 2132TF,15635TF,24027TF,63700TF,67310TF,155190TF              | 30310.211667 |
|          2 | 106170TF                                                     |     0.000000 |
|          3 | 4297TF,19036TF,29380TF,62143TF,128449TF,183095TF             |     0.000000 |
|          4 | 88035TF                                                      |     0.000000 |
|          5 | 37531TF,108570TF,123927TF                                    | 49276.323333 |
|          6 | 139636TF                                                     | 61998.310000 |
|          7 | 79251TF,94780TF,145243TF,151894TF,157238TF,163073TF,182052TF | 37447.331429 |
+------------+--------------------------------------------------------------+--------------+
7 rows in set (0.154 sec)

There should be no 0.00000 in there",,0,0,0,0,0.0,"group_concat followed by regr_avgx gives wrong results $end$ The following query gives bad results:
select l_orderkey, group_concat(l_partkey, ""TF"" order by l_partkey) partkeys, regr_avgx(l_tax, l_extendedprice) avg_price from lineitem where l_orderkey < 10 group by l_orderkey order by l_orderkey;
+------------+--------------------------------------------------------------+--------------+
| l_orderkey | partkeys                                                     | avg_price    |
+------------+--------------------------------------------------------------+--------------+
|          1 | 2132TF,15635TF,24027TF,63700TF,67310TF,155190TF              | 30310.211667 |
|          2 | 106170TF                                                     |     0.000000 |
|          3 | 4297TF,19036TF,29380TF,62143TF,128449TF,183095TF             |     0.000000 |
|          4 | 88035TF                                                      |     0.000000 |
|          5 | 37531TF,108570TF,123927TF                                    | 49276.323333 |
|          6 | 139636TF                                                     | 61998.310000 |
|          7 | 79251TF,94780TF,145243TF,151894TF,157238TF,163073TF,182052TF | 37447.331429 |
+------------+--------------------------------------------------------------+--------------+
7 rows in set (0.154 sec)

There should be no 0.00000 in there $acceptance criteria:$",0,0,0,0,0,0,0,0.233333,19,1,0.0526316,1,0.0526316,1,0.0526316,1,0.0526316,0,0.0
333,MCOL-4589,Sub-Task,MCOL,2021-03-08 06:51:52,,0,Optimize out columns in a subquery involving a UNION which are not referenced in the outer select,"This task is similar to MCOL-4543, but for subqueries involving a UNION. I.e., If query Q1 is of the form:
{code:sql}
SELECT count(c2) FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t1)q;
{code}
Assuming t1 here contains 10 columns c1, c2, ... , c10. We build an ineffective RowGroup in ExeMgr of the form (1, c2_value1, 1, 1, 1, 1, 1, 1, 1, 1). The objective here is to remove all non-referenced columns from the end, until the first referenced column is encountered, i.e. trim down the RowGroup to (1, c2_value1).",,"Optimize out columns in a subquery involving a UNION which are not referenced in the outer select $end$ This task is similar to MCOL-4543, but for subqueries involving a UNION. I.e., If query Q1 is of the form:
{code:sql}
SELECT count(c2) FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t1)q;
{code}
Assuming t1 here contains 10 columns c1, c2, ... , c10. We build an ineffective RowGroup in ExeMgr of the form (1, c2_value1, 1, 1, 1, 1, 1, 1, 1, 1). The objective here is to remove all non-referenced columns from the end, until the first referenced column is encountered, i.e. trim down the RowGroup to (1, c2_value1). $acceptance criteria:$",,Gagan Goel,Gagan Goel,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-03-08 06:51:52,Optimize out columns in a subquery involving a UNION which are not referenced in the outer select,"This task is similar to MCOL-4543, but for subqueries involving a UNION. I.e., If query Q1 is of the form:
{code:sql}
SELECT count(c2) FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t1)q;
{code}
Assuming t1 here contains 10 columns c1, c2, ... , c10. We build an ineffective RowGroup in ExeMgr of the form (1, c2_value1, 1, 1, 1, 1, 1, 1, 1, 1). The objective here is to remove all non-referenced columns from the end, until the first referenced column is encountered, i.e. trim down the RowGroup to (1, c2_value1).",,0,0,0,0,0.0,"Optimize out columns in a subquery involving a UNION which are not referenced in the outer select $end$ This task is similar to MCOL-4543, but for subqueries involving a UNION. I.e., If query Q1 is of the form:
{code:sql}
SELECT count(c2) FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t1)q;
{code}
Assuming t1 here contains 10 columns c1, c2, ... , c10. We build an ineffective RowGroup in ExeMgr of the form (1, c2_value1, 1, 1, 1, 1, 1, 1, 1, 1). The objective here is to remove all non-referenced columns from the end, until the first referenced column is encountered, i.e. trim down the RowGroup to (1, c2_value1). $acceptance criteria:$",0,0,0,0,0,0,0,0.0,4,1,0.25,1,0.25,0,0.0,0,0.0,0,0.0
334,MCOL-4590,Task,MCOL,2021-03-08 07:21:09,MCOL-3525,0,Normalization process in TupleUnion step is slow,"Assume query Q1 is:
{code:sql}
select count(c1) from (select * from t1 union all select * from t1)q;
{code} 
And assume query Q2 is:
{code:sql}
select count(c1) from (select * from t1)q;
{code}
After the optimization performed in MCOL-4589, Q1's execution time is still higher than 2x compared to Q2 (for a table with 100 columns and 1million records, Q1 takes 0.180s while Q2 takes 0.065s, i.e. Q1 is 2.77x compared to Q2. Ideally, it should be close to 2x). This is primarily due to the TupleUnion::normalize() call that runs in ExeMgr which has an inefficient implementation. The objective of this task is to come up with an effective implementation of TupleUnion::normalize().",,"Normalization process in TupleUnion step is slow $end$ Assume query Q1 is:
{code:sql}
select count(c1) from (select * from t1 union all select * from t1)q;
{code} 
And assume query Q2 is:
{code:sql}
select count(c1) from (select * from t1)q;
{code}
After the optimization performed in MCOL-4589, Q1's execution time is still higher than 2x compared to Q2 (for a table with 100 columns and 1million records, Q1 takes 0.180s while Q2 takes 0.065s, i.e. Q1 is 2.77x compared to Q2. Ideally, it should be close to 2x). This is primarily due to the TupleUnion::normalize() call that runs in ExeMgr which has an inefficient implementation. The objective of this task is to come up with an effective implementation of TupleUnion::normalize(). $acceptance criteria:$",,Gagan Goel,Gagan Goel,Major,25,,0,6,0,4,0,1,0,,0,850,4,1,0,2022-07-08 16:07:41,Normalization process in TupleUnion step is slow,"Assume query Q1 is:
{code:sql}
select count(c1) from (select * from t1 union all select * from t1)q;
{code} 
And assume query Q2 is:
{code:sql}
select count(c1) from (select * from t1)q;
{code}
After the optimization performed in MCOL-4589, Q1's execution time is still higher than 2x compared to Q2 (for a table with 100 columns and 1million records, Q1 takes 0.180s while Q2 takes 0.065s, i.e. Q1 is 2.77x compared to Q2. Ideally, it should be close to 2x). This is primarily due to the TupleUnion::normalize() call that runs in ExeMgr which has an inefficient implementation. The objective of this task is to come up with an effective implementation of TupleUnion::normalize().",,0,0,0,0,0.0,"Normalization process in TupleUnion step is slow $end$ Assume query Q1 is:
{code:sql}
select count(c1) from (select * from t1 union all select * from t1)q;
{code} 
And assume query Q2 is:
{code:sql}
select count(c1) from (select * from t1)q;
{code}
After the optimization performed in MCOL-4589, Q1's execution time is still higher than 2x compared to Q2 (for a table with 100 columns and 1million records, Q1 takes 0.180s while Q2 takes 0.065s, i.e. Q1 is 2.77x compared to Q2. Ideally, it should be close to 2x). This is primarily due to the TupleUnion::normalize() call that runs in ExeMgr which has an inefficient implementation. The objective of this task is to come up with an effective implementation of TupleUnion::normalize(). $acceptance criteria:$",0,0,0,0,0,0,1,11696.8,5,1,0.2,1,0.2,0,0.0,0,0.0,0,0.0
335,MCOL-4597,Task,MCOL,2021-03-09 18:51:01,,0,/etc/my.cnf.d/ '.cnf' Files Not Getting Moved Back Into Place During Upgrade,"Columnstore in place upgrades are not moving the original config files back into place after install. This could have a lot of down stream impact on things like:

# Custom log locations
# Server IDs
# Custom settings
# Default collations

Just by changing server IDs alone (to their default value of 1) causes all replication to break.

Let's restore the old postConfigure functionality that handled this. ",,"/etc/my.cnf.d/ '.cnf' Files Not Getting Moved Back Into Place During Upgrade $end$ Columnstore in place upgrades are not moving the original config files back into place after install. This could have a lot of down stream impact on things like:

# Custom log locations
# Server IDs
# Custom settings
# Default collations

Just by changing server IDs alone (to their default value of 1) causes all replication to break.

Let's restore the old postConfigure functionality that handled this.  $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Critical,14,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-09 18:51:29,/etc/my.cnf.d/ '.cnf' Files Not Getting Moved Back Into Place During Upgrade,"Columnstore in place upgrades are not moving the original config files back into place after install. This could have a lot of down stream impact on things like:

# Custom log locations
# Server IDs
# Custom settings
# Default collations

Just by changing server IDs alone (to their default value of 1) causes all replication to break.

Let's restore the old postConfigure functionality that handled this. ",,0,0,0,0,0.0,"/etc/my.cnf.d/ '.cnf' Files Not Getting Moved Back Into Place During Upgrade $end$ Columnstore in place upgrades are not moving the original config files back into place after install. This could have a lot of down stream impact on things like:

# Custom log locations
# Server IDs
# Custom settings
# Default collations

Just by changing server IDs alone (to their default value of 1) causes all replication to break.

Let's restore the old postConfigure functionality that handled this.  $acceptance criteria:$",0,0,0,0,0,0,0,0.0,7,1,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
336,MCOL-4603,New Feature,MCOL,2021-03-12 15:59:27,,0,Replace long double with wide/narrow-decimal for avg() and sum() result type for all numerical datatypesdecimal result,"As of now MCS uses long double as an internal data for results of avg() and sum() for numerical data types columns except wide-decimal. 
This approach imposes a number of limitations, e.g. we can not safely join on the aggregates results being a float number. Presumably long double math operations are slower comparing to int128.
This gets us to the point of replacing long double with wide-decimal as a result of avg() and sum() for all integer data types. This causes a wide consequences that covers lots of facilities of the code. The full list can be obtained by search for LONGDOUBLE in the repo.",,"Replace long double with wide/narrow-decimal for avg() and sum() result type for all numerical datatypesdecimal result $end$ As of now MCS uses long double as an internal data for results of avg() and sum() for numerical data types columns except wide-decimal. 
This approach imposes a number of limitations, e.g. we can not safely join on the aggregates results being a float number. Presumably long double math operations are slower comparing to int128.
This gets us to the point of replacing long double with wide-decimal as a result of avg() and sum() for all integer data types. This causes a wide consequences that covers lots of facilities of the code. The full list can be obtained by search for LONGDOUBLE in the repo. $acceptance criteria:$",,Roman,Roman,Major,26,,3,4,4,4,0,0,0,,0,850,4,0,0,2021-03-12 15:59:27,Replace long double with wide/narrow-decimal for avg() and sum() result type for all numerical datatypesdecimal result,"As of now MCS uses long double as an internal data for results of avg() and sum() for numerical data types columns except wide-decimal. 
This approach imposes a number of limitations, e.g. we can not safely join on the aggregates results being a float number. Presumably long double math operations are slower comparing to int128.
This gets us to the point of replacing long double with wide-decimal as a result of avg() and sum() for all integer data types. This causes a wide consequences that covers lots of facilities of the code. The full list can be obtained by search for LONGDOUBLE in the repo.",,0,0,0,0,0.0,"Replace long double with wide/narrow-decimal for avg() and sum() result type for all numerical datatypesdecimal result $end$ As of now MCS uses long double as an internal data for results of avg() and sum() for numerical data types columns except wide-decimal. 
This approach imposes a number of limitations, e.g. we can not safely join on the aggregates results being a float number. Presumably long double math operations are slower comparing to int128.
This gets us to the point of replacing long double with wide-decimal as a result of avg() and sum() for all integer data types. This causes a wide consequences that covers lots of facilities of the code. The full list can be obtained by search for LONGDOUBLE in the repo. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,45,10,0.222222,7,0.155556,4,0.0888889,3,0.0666667,2,0.0444444
337,MCOL-461,New Feature,MCOL,2016-12-12 16:28:34,,0,remove the option of 'mp' in postConfigure,"With the document use of .my.cnf for the mysql root user password, the option of '-mp' is no longer needed and should be removed from postConfigure",,"remove the option of 'mp' in postConfigure $end$ With the document use of .my.cnf for the mysql root user password, the option of '-mp' is no longer needed and should be removed from postConfigure $acceptance criteria:$",,David Hill,David Hill,Trivial,9,,0,4,0,2,0,0,0,,0,850,2,0,0,2017-01-12 17:19:25,remove the option of 'mp' in postConfigure,"With the document use of .my.cnf for the mysql root user password, the option of '-mp' is no longer needed and should be removed from postConfigure",,0,0,0,0,0.0,"remove the option of 'mp' in postConfigure $end$ With the document use of .my.cnf for the mysql root user password, the option of '-mp' is no longer needed and should be removed from postConfigure $acceptance criteria:$",0,0,0,0,0,0,1,744.833,12,2,0.166667,0,0.0,0,0.0,0,0.0,0,0.0
338,MCOL-4617,Sub-Task,MCOL,2021-03-17 09:17:47,,0,Move in-to-exists predicate creation and injection into the engine,"We currently leverage the existing server functionality provided by Item_in_subselect::create_in_to_exists_cond and Item_in_subselect::inject_in_to_exists_cond to create and inject in-to-exists predicate into an IN subquery's JOIN struct. Aim of this task is to move this predicate creation and injection into the engine. That is, the objective is to leave the subquery's JOIN unaltered and instead directly create and inject this predicate into ColumnStore's select execution plan.",,"Move in-to-exists predicate creation and injection into the engine $end$ We currently leverage the existing server functionality provided by Item_in_subselect::create_in_to_exists_cond and Item_in_subselect::inject_in_to_exists_cond to create and inject in-to-exists predicate into an IN subquery's JOIN struct. Aim of this task is to move this predicate creation and injection into the engine. That is, the objective is to leave the subquery's JOIN unaltered and instead directly create and inject this predicate into ColumnStore's select execution plan. $acceptance criteria:$",,Gagan Goel,Gagan Goel,Major,7,,1,0,1,8,0,0,0,,0,850,0,0,0,2021-03-17 09:17:47,Move in-to-exists predicate creation and injection into the engine,"We currently leverage the existing server functionality provided by Item_in_subselect::create_in_to_exists_cond and Item_in_subselect::inject_in_to_exists_cond to create and inject in-to-exists predicate into an IN subquery's JOIN struct. Aim of this task is to move this predicate creation and injection into the engine. That is, the objective is to leave the subquery's JOIN unaltered and instead directly create and inject this predicate into ColumnStore's select execution plan.",,0,0,0,0,0.0,"Move in-to-exists predicate creation and injection into the engine $end$ We currently leverage the existing server functionality provided by Item_in_subselect::create_in_to_exists_cond and Item_in_subselect::inject_in_to_exists_cond to create and inject in-to-exists predicate into an IN subquery's JOIN struct. Aim of this task is to move this predicate creation and injection into the engine. That is, the objective is to leave the subquery's JOIN unaltered and instead directly create and inject this predicate into ColumnStore's select execution plan. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,6,1,0.166667,1,0.166667,0,0.0,0,0.0,0,0.0
339,MCOL-462,New Feature,MCOL,2016-12-12 21:52:04,,0,Amazon ColumnStore AMI support of IAM role with certificates,"The current version of the AMI requires that a user have the acess/secret key certificates in files on the Instance them's. The Amazon IAM role is there to allow the user to define these types of certificates which would allow the runing process to access them without have to have them locally in a file on the system..

So the logic in postConfigure and the MCS cloud scripts need to change to allow the use of ENV variables, which is what would be passed from the AIM setup.

",,"Amazon ColumnStore AMI support of IAM role with certificates $end$ The current version of the AMI requires that a user have the acess/secret key certificates in files on the Instance them's. The Amazon IAM role is there to allow the user to define these types of certificates which would allow the runing process to access them without have to have them locally in a file on the system..

So the logic in postConfigure and the MCS cloud scripts need to change to allow the use of ENV variables, which is what would be passed from the AIM setup.

 $acceptance criteria:$",,David Hill,David Hill,Minor,12,,0,8,0,4,0,0,0,,0,850,7,0,0,2016-12-14 21:49:59,Amazon ColumnStore AMI support of IAM role with certificates,"The current version of the AMI requires that a user have the acess/secret key certificates in files on the Instance them's. The Amazon IAM role is there to allow the user to define these types of certificates which would allow the runing process to access them without have to have them locally in a file on the system..

So the logic in postConfigure and the MCS cloud scripts need to change to allow the use of ENV variables, which is what would be passed from the AIM setup.

",,0,0,0,0,0.0,"Amazon ColumnStore AMI support of IAM role with certificates $end$ The current version of the AMI requires that a user have the acess/secret key certificates in files on the Instance them's. The Amazon IAM role is there to allow the user to define these types of certificates which would allow the runing process to access them without have to have them locally in a file on the system..

So the logic in postConfigure and the MCS cloud scripts need to change to allow the use of ENV variables, which is what would be passed from the AIM setup.

 $acceptance criteria:$",0,0,0,0,0,0,1,47.95,13,2,0.153846,0,0.0,0,0.0,0,0.0,0,0.0
340,MCOL-4624,Sub-Task,MCOL,2021-03-20 19:44:52,,0,Implement proper calculation for HWM from segment file.,"To proper restore individual extent we have to calculate HWM from the segment file.
At the current version, segment file has a field `blockCount` which specifies an amount of preallocated blocks inside segment file, but HWM specifies the the last filled block - 1. So based on block count, we can make a binary search for range (0, blockCount) and find the first block with the empty value. Since those file are compressed we have to use header ptrs part.
Note: The HWM is important part, adding this will solve the problem `cpiinfo` as well, the bulk is using `batch` insertion, so it aligns rows to block size (even if we insert 1 row it will increment HWM by one).",,"Implement proper calculation for HWM from segment file. $end$ To proper restore individual extent we have to calculate HWM from the segment file.
At the current version, segment file has a field `blockCount` which specifies an amount of preallocated blocks inside segment file, but HWM specifies the the last filled block - 1. So based on block count, we can make a binary search for range (0, blockCount) and find the first block with the empty value. Since those file are compressed we have to use header ptrs part.
Note: The HWM is important part, adding this will solve the problem `cpiinfo` as well, the bulk is using `batch` insertion, so it aligns rows to block size (even if we insert 1 row it will increment HWM by one). $acceptance criteria:$",,Denis Khalikov,Denis Khalikov,Minor,5,,0,1,1,9,0,0,0,,0,850,1,0,0,2021-03-20 19:44:52,Implement proper calculation for HWM from segment file.,"To proper restore individual extent we have to calculate HWM from the segment file.
At the current version, segment file has a field `blockCount` which specifies an amount of preallocated blocks inside segment file, but HWM specifies the the last filled block - 1. So based on block count, we can make a binary search for range (0, blockCount) and find the first block with the empty value. Since those file are compressed we have to use header ptrs part.
Note: The HWM is important part, adding this will solve the problem `cpiinfo` as well, the bulk is using `batch` insertion, so it aligns rows to block size (even if we insert 1 row it will increment HWM by one).",,0,0,0,0,0.0,"Implement proper calculation for HWM from segment file. $end$ To proper restore individual extent we have to calculate HWM from the segment file.
At the current version, segment file has a field `blockCount` which specifies an amount of preallocated blocks inside segment file, but HWM specifies the the last filled block - 1. So based on block count, we can make a binary search for range (0, blockCount) and find the first block with the empty value. Since those file are compressed we have to use header ptrs part.
Note: The HWM is important part, adding this will solve the problem `cpiinfo` as well, the bulk is using `batch` insertion, so it aligns rows to block size (even if we insert 1 row it will increment HWM by one). $acceptance criteria:$",0,0,0,0,0,0,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
341,MCOL-4635,Sub-Task,MCOL,2021-03-23 12:45:19,,0,Properly insert LBID into segment file header for bulk insertion.,Properly insert LBID into segment file header for bulk insertion.,,Properly insert LBID into segment file header for bulk insertion. $end$ Properly insert LBID into segment file header for bulk insertion. $acceptance criteria:$,,Denis Khalikov,Denis Khalikov,Minor,2,,0,1,0,9,0,0,0,,0,850,1,0,0,2021-03-23 12:45:19,Properly insert LBID into segment file header for bulk insertion.,Properly insert LBID into segment file header for bulk insertion.,,0,0,0,0,0.0,Properly insert LBID into segment file header for bulk insertion. $end$ Properly insert LBID into segment file header for bulk insertion. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
342,MCOL-4654,Sub-Task,MCOL,2021-03-31 12:56:25,,0,LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition`,"LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition`.
1. `getUncompressedSize`: currently `CompressedInetStreamSocket` and `JoinPartition` require a function `getUncompressedSize`, so we can allocate appropriate size for uncompressed buffer. LZ4 does not have by default this function, reading the `snappy` documentation I found out that the cost of this function is O(1) so it probably stores the input size in the compressed block, so I think we can do the same way for those two specific parts.

2. Currently `CompressedInetStreamSocket` and `JoinPartition`  use config to enable/disable compression, for sockets and for temporal files. Probably we should add a one more field in config to specify the compression type.",,"LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition` $end$ LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition`.
1. `getUncompressedSize`: currently `CompressedInetStreamSocket` and `JoinPartition` require a function `getUncompressedSize`, so we can allocate appropriate size for uncompressed buffer. LZ4 does not have by default this function, reading the `snappy` documentation I found out that the cost of this function is O(1) so it probably stores the input size in the compressed block, so I think we can do the same way for those two specific parts.

2. Currently `CompressedInetStreamSocket` and `JoinPartition`  use config to enable/disable compression, for sockets and for temporal files. Probably we should add a one more field in config to specify the compression type. $acceptance criteria:$",,Denis Khalikov,Denis Khalikov,Major,2,,0,1,0,5,0,0,0,,0,850,1,0,0,2021-03-31 12:56:25,LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition`,"LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition`.
1. `getUncompressedSize`: currently `CompressedInetStreamSocket` and `JoinPartition` require a function `getUncompressedSize`, so we can allocate appropriate size for uncompressed buffer. LZ4 does not have by default this function, reading the `snappy` documentation I found out that the cost of this function is O(1) so it probably stores the input size in the compressed block, so I think we can do the same way for those two specific parts.

2. Currently `CompressedInetStreamSocket` and `JoinPartition`  use config to enable/disable compression, for sockets and for temporal files. Probably we should add a one more field in config to specify the compression type.",,0,0,0,0,0.0,"LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition` $end$ LZ4 compression support for `CompressedInetStreamSocket` and `JoinPartition`.
1. `getUncompressedSize`: currently `CompressedInetStreamSocket` and `JoinPartition` require a function `getUncompressedSize`, so we can allocate appropriate size for uncompressed buffer. LZ4 does not have by default this function, reading the `snappy` documentation I found out that the cost of this function is O(1) so it probably stores the input size in the compressed block, so I think we can do the same way for those two specific parts.

2. Currently `CompressedInetStreamSocket` and `JoinPartition`  use config to enable/disable compression, for sockets and for temporal files. Probably we should add a one more field in config to specify the compression type. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
343,MCOL-4665,Sub-Task,MCOL,2021-04-05 12:46:24,,0,Move outer join to inner join conversion into the engine.,The server performs the outer join to inner join optimisation in sql_select.cc::simplify_joins(). The objective of this task is to move this conversion into the ColumnStore execution plan creation in the plugin code.,,Move outer join to inner join conversion into the engine. $end$ The server performs the outer join to inner join optimisation in sql_select.cc::simplify_joins(). The objective of this task is to move this conversion into the ColumnStore execution plan creation in the plugin code. $acceptance criteria:$,,Gagan Goel,Gagan Goel,Major,4,,0,0,0,8,0,0,0,,0,850,0,0,0,2021-04-05 12:46:24,Move outer join to inner join conversion into the engine.,The server performs the outer join to inner join optimisation in sql_select.cc::simplify_joins(). The objective of this task is to move this conversion into the ColumnStore execution plan creation in the plugin code.,,0,0,0,0,0.0,Move outer join to inner join conversion into the engine. $end$ The server performs the outer join to inner join optimisation in sql_select.cc::simplify_joins(). The objective of this task is to move this conversion into the ColumnStore execution plan creation in the plugin code. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,7,1,0.142857,1,0.142857,0,0.0,0,0.0,0,0.0
344,MCOL-4679,New Feature,MCOL,2021-04-16 09:04:34,,0,Simplify configuration of EM to PM connections,"To describe a connection from UM to PM one needs to add a section into Columnstore.xml like this for every connection
{noformat}
<PMS1>
       <IPAddr>127.0.0.1</IPAddr>
       <Port>8620</Port>
</PMS1>
{noformat}
If the number of connections is significant it might be tedious to describe them and wasteful to store them in the config.

The suggested way to describe now is to add a single section per PM exists in the cluster and set PrimitiveServers.ConnectionsPerPrimProc to describe how many PM-EM connections must be created by DEC::Setup() method.

It is worth to note the issue will also cover CMAPI part that manipulates the XML configuration adding or removing a node from the cluster.
",,"Simplify configuration of EM to PM connections $end$ To describe a connection from UM to PM one needs to add a section into Columnstore.xml like this for every connection
{noformat}
<PMS1>
       <IPAddr>127.0.0.1</IPAddr>
       <Port>8620</Port>
</PMS1>
{noformat}
If the number of connections is significant it might be tedious to describe them and wasteful to store them in the config.

The suggested way to describe now is to add a single section per PM exists in the cluster and set PrimitiveServers.ConnectionsPerPrimProc to describe how many PM-EM connections must be created by DEC::Setup() method.

It is worth to note the issue will also cover CMAPI part that manipulates the XML configuration adding or removing a node from the cluster.
 $acceptance criteria:$",,Roman,Roman,Minor,32,,1,11,1,2,0,0,0,,0,850,8,0,0,2021-06-14 17:51:29,Simplify configuration of EM to PM connections,"To describe a connection from UM to PM one needs to add a section into Columnstore.xml like this for every connection
{noformat}
<PMS1>
       <IPAddr>127.0.0.1</IPAddr>
       <Port>8620</Port>
</PMS1>
{noformat}
If the number of connections is significant it might be tedious to describe them and wasteful to store them in the config.

The suggested way to describe now is to add a single section per PM exists in the cluster and set PrimitiveServers.ConnectionsPerPrimProc to describe how many PM-EM connections must be created by DEC::Setup() method.

It is worth to note the issue will also cover CMAPI part that manipulates the XML configuration adding or removing a node from the cluster.
",,0,0,0,0,0.0,"Simplify configuration of EM to PM connections $end$ To describe a connection from UM to PM one needs to add a section into Columnstore.xml like this for every connection
{noformat}
<PMS1>
       <IPAddr>127.0.0.1</IPAddr>
       <Port>8620</Port>
</PMS1>
{noformat}
If the number of connections is significant it might be tedious to describe them and wasteful to store them in the config.

The suggested way to describe now is to add a single section per PM exists in the cluster and set PrimitiveServers.ConnectionsPerPrimProc to describe how many PM-EM connections must be created by DEC::Setup() method.

It is worth to note the issue will also cover CMAPI part that manipulates the XML configuration adding or removing a node from the cluster.
 $acceptance criteria:$",0,0,0,0,0,0,1,1424.77,46,10,0.217391,7,0.152174,4,0.0869565,3,0.0652174,2,0.0434783
345,MCOL-4681,Task,MCOL,2021-04-16 13:11:33,,0,Fix install_mcs_mysql.sh.in to do CREATE FUNCTION instead of INSERT INTO mysql.func,"install_mcs_mysql.sh.in currently uses direct INSERTs to populate mysql.func, e.g.:
{code:sql}
...
INSERT INTO mysql.func VALUES ('mcssystemready',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemreadonly',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemprimary',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('regr_avgx',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_avgy',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_count',2,'libregr_mysql.so','aggregate');
...
{code}
This is a not good way to install UDFs and UDAFs:
- There is no a guaratee that the structure of mysql.func will always be the same.
- This approach needs to restart the MariaDB server to make the functions actually available. This makes ColumnStore post-installation step unnecessarily more complex.


The correct way to install UDFs and UDAFs it to use CREATE FUNCTION and CREATE AGGREGATE FUNCTION statements.
{code:sql}
...
CREATE OR REPLACE FUNCTION mcssystemready RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemreadonly RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemprimary RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgx RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgy RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_count RETURNS INTEGER SONAME 'libregr_mysql.so';
...
{code}

- It's safe about mysql.func structure changes
- It does not need the server restart. Functions become immediately available.
",,"Fix install_mcs_mysql.sh.in to do CREATE FUNCTION instead of INSERT INTO mysql.func $end$ install_mcs_mysql.sh.in currently uses direct INSERTs to populate mysql.func, e.g.:
{code:sql}
...
INSERT INTO mysql.func VALUES ('mcssystemready',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemreadonly',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemprimary',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('regr_avgx',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_avgy',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_count',2,'libregr_mysql.so','aggregate');
...
{code}
This is a not good way to install UDFs and UDAFs:
- There is no a guaratee that the structure of mysql.func will always be the same.
- This approach needs to restart the MariaDB server to make the functions actually available. This makes ColumnStore post-installation step unnecessarily more complex.


The correct way to install UDFs and UDAFs it to use CREATE FUNCTION and CREATE AGGREGATE FUNCTION statements.
{code:sql}
...
CREATE OR REPLACE FUNCTION mcssystemready RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemreadonly RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemprimary RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgx RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgy RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_count RETURNS INTEGER SONAME 'libregr_mysql.so';
...
{code}

- It's safe about mysql.func structure changes
- It does not need the server restart. Functions become immediately available.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,21,,2,3,2,1,0,0,0,,0,850,2,0,0,2021-06-21 14:45:35,Fix install_mcs_mysql.sh.in to do CREATE FUNCTION instead of INSERT INTO mysql.func,"install_mcs_mysql.sh.in currently uses direct INSERTs to populate mysql.func, e.g.:
{code:sql}
...
INSERT INTO mysql.func VALUES ('mcssystemready',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemreadonly',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemprimary',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('regr_avgx',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_avgy',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_count',2,'libregr_mysql.so','aggregate');
...
{code}
This is a not good way to install UDFs and UDAFs:
- There is no a guaratee that the structure of mysql.func will always be the same.
- This approach needs to restart the MariaDB server to make the functions actually available. This makes ColumnStore post-installation step unnecessarily more complex.


The correct way to install UDFs and UDAFs it to use CREATE FUNCTION and CREATE AGGREGATE FUNCTION statements.
{code:sql}
...
CREATE OR REPLACE FUNCTION mcssystemready RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemreadonly RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemprimary RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgx RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgy RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_count RETURNS INTEGER SONAME 'libregr_mysql.so';
...
{code}

- It's safe about mysql.func structure changes
- It does not need the server restart. Functions become immediately available.
",,0,0,0,0,0.0,"Fix install_mcs_mysql.sh.in to do CREATE FUNCTION instead of INSERT INTO mysql.func $end$ install_mcs_mysql.sh.in currently uses direct INSERTs to populate mysql.func, e.g.:
{code:sql}
...
INSERT INTO mysql.func VALUES ('mcssystemready',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemreadonly',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('mcssystemprimary',2,'ha_columnstore.so','function');
INSERT INTO mysql.func VALUES ('regr_avgx',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_avgy',1,'libregr_mysql.so','aggregate');
INSERT INTO mysql.func VALUES ('regr_count',2,'libregr_mysql.so','aggregate');
...
{code}
This is a not good way to install UDFs and UDAFs:
- There is no a guaratee that the structure of mysql.func will always be the same.
- This approach needs to restart the MariaDB server to make the functions actually available. This makes ColumnStore post-installation step unnecessarily more complex.


The correct way to install UDFs and UDAFs it to use CREATE FUNCTION and CREATE AGGREGATE FUNCTION statements.
{code:sql}
...
CREATE OR REPLACE FUNCTION mcssystemready RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemreadonly RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE FUNCTION mcssystemprimary RETURNS INTEGER SONAME 'ha_columnstore.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgx RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_avgy RETURNS REAL SONAME 'libregr_mysql.so';
CREATE OR REPLACE AGGREGATE FUNCTION regr_count RETURNS INTEGER SONAME 'libregr_mysql.so';
...
{code}

- It's safe about mysql.func structure changes
- It does not need the server restart. Functions become immediately available.
 $acceptance criteria:$",0,0,0,0,0,0,0,1585.57,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
346,MCOL-4685,Task,MCOL,2021-04-20 13:57:25,,0,Eliminate some irrelevant settings (uncompressed data and extents per file),"1. remove the option to declare uncompressed columns (set columnstore_compression_type = 0). 
2. ignore [COMMENT '[compression=0] option at table or column level (no error messages, just disregard);
3. remove the option to set more than 2 extents per file (ExtentsPreSegmentFile)",,"Eliminate some irrelevant settings (uncompressed data and extents per file) $end$ 1. remove the option to declare uncompressed columns (set columnstore_compression_type = 0). 
2. ignore [COMMENT '[compression=0] option at table or column level (no error messages, just disregard);
3. remove the option to set more than 2 extents per file (ExtentsPreSegmentFile) $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Minor,16,,0,7,1,3,0,1,0,,0,850,7,0,0,2021-04-28 14:51:13,Eliminate some irrelevant settings (uncompressed data and extents per file),"1. remove the option to declare uncompressed columns (set infinidb_compression_type = 0). 
2. ignore [COMMENT '[compression=0] option at table or column level (no error messages, just disregard);
3. remove the option to set more than 2 extents per file (ExtentsPreSegmentFile)",,0,1,0,2,0.0188679,"Eliminate some irrelevant settings (uncompressed data and extents per file) $end$ 1. remove the option to declare uncompressed columns (set infinidb_compression_type = 0). 
2. ignore [COMMENT '[compression=0] option at table or column level (no error messages, just disregard);
3. remove the option to set more than 2 extents per file (ExtentsPreSegmentFile) $acceptance criteria:$",1,1,0,0,0,0,1,192.883,4,2,0.5,2,0.5,2,0.5,2,0.5,2,0.5
347,MCOL-4692,Task,MCOL,2021-04-22 12:33:36,,0,Remove global mutex lock in OAMCache facility.,"oam/oamcpp/oamcache.cpp has a global mutex that can slow down execution b/c of lock contention, namely boost::mutex cacheLock.
The existing implementation of OAMCache can also re-read configuration file holding the lock from either disk or OS buffercache in certain conditions. I also remove this functionality to reduce the lock contention.",,"Remove global mutex lock in OAMCache facility. $end$ oam/oamcpp/oamcache.cpp has a global mutex that can slow down execution b/c of lock contention, namely boost::mutex cacheLock.
The existing implementation of OAMCache can also re-read configuration file holding the lock from either disk or OS buffercache in certain conditions. I also remove this functionality to reduce the lock contention. $acceptance criteria:$",,Roman,Roman,Major,6,,2,1,2,1,0,1,0,,0,850,1,1,0,2021-04-22 18:02:15,Remove global mutex lock in OAMCache facility.,"oam/oamcpp/oamcache.cpp has a global mutex that can slow down execution b/c of lock contention, namely boost::mutex cacheLock.
The existing implementation of OAMCache can also re-read configuration file holding the lock from either disk or OS buffercache in certain conditions. I also remove this functionality to reduce the lock contention.",,0,0,0,0,0.0,"Remove global mutex lock in OAMCache facility. $end$ oam/oamcpp/oamcache.cpp has a global mutex that can slow down execution b/c of lock contention, namely boost::mutex cacheLock.
The existing implementation of OAMCache can also re-read configuration file holding the lock from either disk or OS buffercache in certain conditions. I also remove this functionality to reduce the lock contention. $acceptance criteria:$",0,0,0,0,0,0,0,5.46667,47,10,0.212766,7,0.148936,4,0.0851064,3,0.0638298,2,0.0425532
348,MCOL-4694,Task,MCOL,2021-04-22 18:02:07,,0,Distribute primitive jobs messages b/w TCP connections provided by DEC,"As of now the meta primitive messages:
* BATCH_PRIMITIVE_DESTROY
* BATCH_PRIMITIVE_ADD_JOINER
* BATCH_PRIMITIVE_END_JOINER
* BATCH_PRIMITIVE_ABORT
* DICT_CREATE_EQUALITY_FILTER
* DICT_DESTROY_EQUALITY_FILTER 
 are always sent of over the first DEC connections to PMs. This fact results in a contention given that BATCH_PRIMITIVE_ADD_JOINER streams a lot of small side table data with it.
Low level scanning/filtering primitives:
* BATCH_PRIMITIVE_RUN
* DICT_TOKEN_BY_SCAN_COMPARE
Are effectively distributed b/w first min(N, M) connections where N is a number of connections to every PP and M is a number of primitive jobs sent(effectively produced by TupleBPS::makeJobs) 

MCS needs to distribute workload b/w connections using round-robin algorithm.
Future investigation might point to a finer-grained distribution algo.",,"Distribute primitive jobs messages b/w TCP connections provided by DEC $end$ As of now the meta primitive messages:
* BATCH_PRIMITIVE_DESTROY
* BATCH_PRIMITIVE_ADD_JOINER
* BATCH_PRIMITIVE_END_JOINER
* BATCH_PRIMITIVE_ABORT
* DICT_CREATE_EQUALITY_FILTER
* DICT_DESTROY_EQUALITY_FILTER 
 are always sent of over the first DEC connections to PMs. This fact results in a contention given that BATCH_PRIMITIVE_ADD_JOINER streams a lot of small side table data with it.
Low level scanning/filtering primitives:
* BATCH_PRIMITIVE_RUN
* DICT_TOKEN_BY_SCAN_COMPARE
Are effectively distributed b/w first min(N, M) connections where N is a number of connections to every PP and M is a number of primitive jobs sent(effectively produced by TupleBPS::makeJobs) 

MCS needs to distribute workload b/w connections using round-robin algorithm.
Future investigation might point to a finer-grained distribution algo. $acceptance criteria:$",,Roman,Roman,Major,10,,1,3,1,3,0,0,0,,0,850,3,0,0,2021-04-22 18:02:07,Distribute primitive jobs messages b/w TCP connections provided by DEC,"As of now the meta primitive messages:
* BATCH_PRIMITIVE_DESTROY
* BATCH_PRIMITIVE_ADD_JOINER
* BATCH_PRIMITIVE_END_JOINER
* BATCH_PRIMITIVE_ABORT
* DICT_CREATE_EQUALITY_FILTER
* DICT_DESTROY_EQUALITY_FILTER 
 are always sent of over the first DEC connections to PMs. This fact results in a contention given that BATCH_PRIMITIVE_ADD_JOINER streams a lot of small side table data with it.
Low level scanning/filtering primitives:
* BATCH_PRIMITIVE_RUN
* DICT_TOKEN_BY_SCAN_COMPARE
Are effectively distributed b/w first min(N, M) connections where N is a number of connections to every PP and M is a number of primitive jobs sent(effectively produced by TupleBPS::makeJobs) 

MCS needs to distribute workload b/w connections using round-robin algorithm.
Future investigation might point to a finer-grained distribution algo.",,0,0,0,0,0.0,"Distribute primitive jobs messages b/w TCP connections provided by DEC $end$ As of now the meta primitive messages:
* BATCH_PRIMITIVE_DESTROY
* BATCH_PRIMITIVE_ADD_JOINER
* BATCH_PRIMITIVE_END_JOINER
* BATCH_PRIMITIVE_ABORT
* DICT_CREATE_EQUALITY_FILTER
* DICT_DESTROY_EQUALITY_FILTER 
 are always sent of over the first DEC connections to PMs. This fact results in a contention given that BATCH_PRIMITIVE_ADD_JOINER streams a lot of small side table data with it.
Low level scanning/filtering primitives:
* BATCH_PRIMITIVE_RUN
* DICT_TOKEN_BY_SCAN_COMPARE
Are effectively distributed b/w first min(N, M) connections where N is a number of connections to every PP and M is a number of primitive jobs sent(effectively produced by TupleBPS::makeJobs) 

MCS needs to distribute workload b/w connections using round-robin algorithm.
Future investigation might point to a finer-grained distribution algo. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,48,10,0.208333,7,0.145833,4,0.0833333,3,0.0625,2,0.0416667
349,MCOL-4699,New Feature,MCOL,2021-04-28 14:41:42,MCOL-4343,0,support queries with circular OUTER joins,This is a continuation of MCOL-1205. The description in that ticket is the same as here - the only difference is that MCOL-1205 is now limited to dealing with INNER joins only.,,support queries with circular OUTER joins $end$ This is a continuation of MCOL-1205. The description in that ticket is the same as here - the only difference is that MCOL-1205 is now limited to dealing with INNER joins only. $acceptance criteria:$,,Gregory Dorman,Gregory Dorman,Major,52,,2,5,2,11,0,0,0,,0,850,5,0,0,2021-04-28 14:44:25,support queries with circular OUTER joins,This is a continuation of MCOL-1205. The description in that ticket is the same as here - the only difference is that MCOL-1205 is now limited to dealing with INNER joins only.,,0,0,0,0,0.0,support queries with circular OUTER joins $end$ This is a continuation of MCOL-1205. The description in that ticket is the same as here - the only difference is that MCOL-1205 is now limited to dealing with INNER joins only. $acceptance criteria:$,0,0,0,0,0,0,1,0.0333333,5,3,0.6,2,0.4,2,0.4,2,0.4,2,0.4
350,MCOL-470,Task,MCOL,2016-12-15 19:45:51,,0,merge server 10.1.20 code,10.1.20 maintenance release was published today.,,merge server 10.1.20 code $end$ 10.1.20 maintenance release was published today. $acceptance criteria:$,,David Thompson,David Thompson,Major,11,,0,3,0,4,0,0,0,,0,850,3,0,0,2016-12-16 21:58:18,merge server 10.1.20 code,10.1.20 maintenance release was published today.,,0,0,0,0,0.0,merge server 10.1.20 code $end$ 10.1.20 maintenance release was published today. $acceptance criteria:$,0,0,0,0,0,0,1,26.2,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
351,MCOL-4704,Task,MCOL,2021-04-30 16:54:03,,0,Retest MCOL-4687 (Insert from view) regression in 6.1.1 when ready,,,Retest MCOL-4687 (Insert from view) regression in 6.1.1 when ready $end$ $acceptance criteria:$,,Gregory Dorman,Gregory Dorman,Blocker,9,,1,1,1,1,0,1,0,,0,850,1,1,0,2021-05-17 16:22:48,Retest MCOL-4687 (Insert from view) regression in 6.1.1 when ready,,,0,0,0,0,0.0,Retest MCOL-4687 (Insert from view) regression in 6.1.1 when ready $end$ $acceptance criteria:$,0,0,0,0,0,0,0,407.467,6,3,0.5,2,0.333333,2,0.333333,2,0.333333,2,0.333333
352,MCOL-4705,Task,MCOL,2021-04-30 17:47:28,,0,Retest MCOL-4613 in 6.1.1 given that the blocking fix for MCOL-4612 is merged into develop branch,There seems to be no PR or merge into 6.1.1 as of now. We need to make sure this does not reappear when we make it.,,Retest MCOL-4613 in 6.1.1 given that the blocking fix for MCOL-4612 is merged into develop branch $end$ There seems to be no PR or merge into 6.1.1 as of now. We need to make sure this does not reappear when we make it. $acceptance criteria:$,,Gregory Dorman,Gregory Dorman,Blocker,9,,1,1,1,1,0,1,0,,0,850,1,1,0,2021-05-17 16:22:54,Retest MCOL-4613 in 6.1.1 given that the blocking fix for MCOL-4612 is merged into develop branch,There seems to be no PR or merge into 6.1.1 as of now. We need to make sure this does not reappear when we make it.,,0,0,0,0,0.0,Retest MCOL-4613 in 6.1.1 given that the blocking fix for MCOL-4612 is merged into develop branch $end$ There seems to be no PR or merge into 6.1.1 as of now. We need to make sure this does not reappear when we make it. $acceptance criteria:$,0,0,0,0,0,0,0,406.583,7,3,0.428571,2,0.285714,2,0.285714,2,0.285714,2,0.285714
353,MCOL-4706,Sub-Task,MCOL,2021-04-30 18:18:07,,0,Additional test needed,"This is very unusual. MCOL-4613 was fixed in develop-5 - but for develop the code changes are going to be filed for both 4612 and 4613.

This subtask is a reminder that you need to test 4613 once the code for this one is merged into 6.1.1.",,"Additional test needed $end$ This is very unusual. MCOL-4613 was fixed in develop-5 - but for develop the code changes are going to be filed for both 4612 and 4613.

This subtask is a reminder that you need to test 4613 once the code for this one is merged into 6.1.1. $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Blocker,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-04-30 18:18:07,Additional test needed,"This is very unusual. MCOL-4613 was fixed in develop-5 - but for develop the code changes are going to be filed for both 4612 and 4613.

This subtask is a reminder that you need to test 4613 once the code for this one is merged into 6.1.1.",,0,0,0,0,0.0,"Additional test needed $end$ This is very unusual. MCOL-4613 was fixed in develop-5 - but for develop the code changes are going to be filed for both 4612 and 4613.

This subtask is a reminder that you need to test 4613 once the code for this one is merged into 6.1.1. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,8,3,0.375,2,0.25,2,0.25,2,0.25,2,0.25
354,MCOL-4709,New Feature,MCOL,2021-05-07 16:01:08,MCOL-4343,0,Merge Disk-based  aggregation to 6.1.1,This is a clone of MCOL-563 Implement Disk-based aggregation. This Jira is to merge the feature into the develop branch,,Merge Disk-based  aggregation to 6.1.1 $end$ This is a clone of MCOL-563 Implement Disk-based aggregation. This Jira is to merge the feature into the develop branch $acceptance criteria:$,,David Hall,David Hall,Blocker,21,,0,1,2,3,0,0,0,,0,850,1,0,0,2021-05-07 16:05:13,Merge Disk-based  aggregation to 6.1.1,This is a clone of MCOL-563 Implement Disk-based aggregation. This Jira is to merge the feature into the develop branch,,0,0,0,0,0.0,Merge Disk-based  aggregation to 6.1.1 $end$ This is a clone of MCOL-563 Implement Disk-based aggregation. This Jira is to merge the feature into the develop branch $acceptance criteria:$,0,0,0,0,0,0,1,0.0666667,20,1,0.05,1,0.05,1,0.05,1,0.05,0,0.0
355,MCOL-4713,New Feature,MCOL,2021-05-08 13:18:56,,0,Optimizer statistics,"Optimizer statistics

Most engines feeds their query optimizers with different types of statistics, e.g histograms of most common values, fractions of NULL values, Number of Distinct Values, relation cardinality. The statistics allows to make educated guesses regarding cardinality of intermediate and final result of a query, e.g. optimizer finds the optimal join order using this statistics. MCS now has a rudimentary statistics that consist of a relation cardinality only.

The goal

We need to design and build facilities that allows MCS to make educated guesses about cardinality of intermediate and final result of a query and use this knowledge to produce an optimal plan.

What is statistics

The simplest version of statistics must have the list of attributes listed earlier. The improved version should have an extensible format to add attributes(e.g. correlated or dependant histograms on functionaly dependant columns) w/o lots of changes.

What triggers stats collection

There must be a user knob in the form of ANALYZE SQL statement. Bulk insert operation inserting more then N values must also trigger ANALYZE to regresh the stats.

Consumers

ExeMgr or whatever the optimizer goes into is the most important consumer. The fact that there might be more then one EM in the cluster must be taken into account.

Where to put stats and how to save it b/w restarts

EMs in the cluster must have direct access to the stats so stats in-memory representation must reside in the RAM of the primary EM. Other EMs must have a synchronized version of the stats.

Producers

EM must initiate statistics collection. The statistics collection process must not differ much from SELECT * FROM query. The sampling method isn't yet choosen.

Please see the design doc for more details.",,"Optimizer statistics $end$ Optimizer statistics

Most engines feeds their query optimizers with different types of statistics, e.g histograms of most common values, fractions of NULL values, Number of Distinct Values, relation cardinality. The statistics allows to make educated guesses regarding cardinality of intermediate and final result of a query, e.g. optimizer finds the optimal join order using this statistics. MCS now has a rudimentary statistics that consist of a relation cardinality only.

The goal

We need to design and build facilities that allows MCS to make educated guesses about cardinality of intermediate and final result of a query and use this knowledge to produce an optimal plan.

What is statistics

The simplest version of statistics must have the list of attributes listed earlier. The improved version should have an extensible format to add attributes(e.g. correlated or dependant histograms on functionaly dependant columns) w/o lots of changes.

What triggers stats collection

There must be a user knob in the form of ANALYZE SQL statement. Bulk insert operation inserting more then N values must also trigger ANALYZE to regresh the stats.

Consumers

ExeMgr or whatever the optimizer goes into is the most important consumer. The fact that there might be more then one EM in the cluster must be taken into account.

Where to put stats and how to save it b/w restarts

EMs in the cluster must have direct access to the stats so stats in-memory representation must reside in the RAM of the primary EM. Other EMs must have a synchronized version of the stats.

Producers

EM must initiate statistics collection. The statistics collection process must not differ much from SELECT * FROM query. The sampling method isn't yet choosen.

Please see the design doc for more details. $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Blocker,15,,3,2,4,3,0,3,0,,0,850,2,0,0,2021-05-12 17:40:27,Create mechanism for keeping and interpreting column statistics,"From the beginning of time ColumnStore does not keep any meta information required for logical optimization of queries. This task intends to resolve it, and enable a large set of very high impact optimization projects. Primary examples:

1. Even though CS data is always taken from OLTP systems which have primary keys and unique indexes, this information is not recorded. As a result, it is not possible to estimate the impact of any pushed predicate on row size of the a filtering job step - instrumental in deciding on a best join order, or perform a proper join elimination in case of circular joins, among others.
2. Nor is there any column cardinality estimation mechanism (a viable, and likely a more generally applicable alternative to additional table metadata).
3. There is no distribution information either.

There are multiple possibilities.
1. A more difficult yet more impactful would be to collect column statistics - cardinalities and distributions, and use them at the planning time. 

Knowing that a given column is predominantly UNIUE would provide tremendous help in estimating the size of a rowset resulting from application of an equi predicate to a scalar (or of a ""short"" IN list of scalar values), hence in deciding on the proper join order. 

Inversely, knowing that a column has low cardinality relative to the cardinality of the table would enable the detection of a cartesian explosion risks inherent in joins which use such columns on both sides (e.g. Query 5 of TPC-H) - so called join elimination.

2. In shorter term, it should be easy enough to enable syntactical constructs like PRIMARY KEY and UNIQUE INDEX. There is no need to build the additional data structures beyond traditional table metadata - these would be merely PROXY PRIMARY KEY, or a PROXY UNIQUE INDEX, helping the planner to make proper guesses about cardinalities of various job steps leading to proper estimation of their costs, and ending in a proper decision making as regards join ordering or join elimination.",,1,2,0,588,0.94186,"Create mechanism for keeping and interpreting column statistics $end$ From the beginning of time ColumnStore does not keep any meta information required for logical optimization of queries. This task intends to resolve it, and enable a large set of very high impact optimization projects. Primary examples:

1. Even though CS data is always taken from OLTP systems which have primary keys and unique indexes, this information is not recorded. As a result, it is not possible to estimate the impact of any pushed predicate on row size of the a filtering job step - instrumental in deciding on a best join order, or perform a proper join elimination in case of circular joins, among others.
2. Nor is there any column cardinality estimation mechanism (a viable, and likely a more generally applicable alternative to additional table metadata).
3. There is no distribution information either.

There are multiple possibilities.
1. A more difficult yet more impactful would be to collect column statistics - cardinalities and distributions, and use them at the planning time. 

Knowing that a given column is predominantly UNIUE would provide tremendous help in estimating the size of a rowset resulting from application of an equi predicate to a scalar (or of a ""short"" IN list of scalar values), hence in deciding on the proper join order. 

Inversely, knowing that a column has low cardinality relative to the cardinality of the table would enable the detection of a cartesian explosion risks inherent in joins which use such columns on both sides (e.g. Query 5 of TPC-H) - so called join elimination.

2. In shorter term, it should be easy enough to enable syntactical constructs like PRIMARY KEY and UNIQUE INDEX. There is no need to build the additional data structures beyond traditional table metadata - these would be merely PROXY PRIMARY KEY, or a PROXY UNIQUE INDEX, helping the planner to make proper guesses about cardinalities of various job steps leading to proper estimation of their costs, and ending in a proper decision making as regards join ordering or join elimination. $acceptance criteria:$",3,1,1,1,1,1,1,100.35,9,3,0.333333,2,0.222222,2,0.222222,2,0.222222,2,0.222222
356,MCOL-473,Task,MCOL,2016-12-19 18:32:44,,0,use version file for columnstore engine version,To avoid past issues and have zero maintenance builds can we switch to using a VERSION file and avoid having to configure and pass these into cmake / build. We can follow the convention / mechanism followed by server.,,use version file for columnstore engine version $end$ To avoid past issues and have zero maintenance builds can we switch to using a VERSION file and avoid having to configure and pass these into cmake / build. We can follow the convention / mechanism followed by server. $acceptance criteria:$,,David Thompson,David Thompson,Major,14,,0,2,0,4,0,0,0,,0,850,2,0,0,2016-12-19 18:33:06,use version file for columnstore engine version,To avoid past issues and have zero maintenance builds can we switch to using a VERSION file and avoid having to configure and pass these into cmake / build. We can follow the convention / mechanism followed by server.,,0,0,0,0,0.0,use version file for columnstore engine version $end$ To avoid past issues and have zero maintenance builds can we switch to using a VERSION file and avoid having to configure and pass these into cmake / build. We can follow the convention / mechanism followed by server. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
357,MCOL-4739,Task,MCOL,2021-06-01 13:34:56,,0,Test 5.6.1 first candidate in all configuraions,"Take https://cspkg.s3.amazonaws.com/index.html?prefix=custom/2469/amd64/
This is supposed to be 10.5.10-7 + 5.6.1 as in current develop-5 (created on my request Roman Navrotsky).

Follow installation and upgrade published instructions. Make sure the following permutations all work - once for fresh install and once for upgrade from 5.5.2:

1. Single node hard disks
1a. Single node S3
2. Three nodes hard disks (no shared storage) with MaxScale
3. Three nodes S3 shared storage and MaxScale
3a. Test failover

Set it up so that this can be repeated aouple more times as we get close to the final release candidate any day now.
",,"Test 5.6.1 first candidate in all configuraions $end$ Take https://cspkg.s3.amazonaws.com/index.html?prefix=custom/2469/amd64/
This is supposed to be 10.5.10-7 + 5.6.1 as in current develop-5 (created on my request Roman Navrotsky).

Follow installation and upgrade published instructions. Make sure the following permutations all work - once for fresh install and once for upgrade from 5.5.2:

1. Single node hard disks
1a. Single node S3
2. Three nodes hard disks (no shared storage) with MaxScale
3. Three nodes S3 shared storage and MaxScale
3a. Test failover

Set it up so that this can be repeated aouple more times as we get close to the final release candidate any day now.
 $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Blocker,6,,0,2,0,1,0,1,0,,0,850,2,0,0,2021-06-01 13:35:11,Test 5.6.1 first candidate in all configuraions,"Take https://cspkg.s3.amazonaws.com/index.html?prefix=custom/2469/amd64/
This is supposed to be 10.5.10-7 + 5.6.1 as in current develop-5 (created on my request Roman Navrotsky).

Follow installation and upgrade published instructions. Make sure the following permutations all work:

1. Single node hard disks
1a. Single node S3
2. Three nodes hard disks (no shared storage) with MaxScale
3. Three nodes S3 shared storage and MaxScale
3a. Test failover

Set it up so that this can be repeated aouple more times as we get close to the final release candidate any day now.
",,0,1,0,13,0.123711,"Test 5.6.1 first candidate in all configuraions $end$ Take https://cspkg.s3.amazonaws.com/index.html?prefix=custom/2469/amd64/
This is supposed to be 10.5.10-7 + 5.6.1 as in current develop-5 (created on my request Roman Navrotsky).

Follow installation and upgrade published instructions. Make sure the following permutations all work:

1. Single node hard disks
1a. Single node S3
2. Three nodes hard disks (no shared storage) with MaxScale
3. Three nodes S3 shared storage and MaxScale
3a. Test failover

Set it up so that this can be repeated aouple more times as we get close to the final release candidate any day now.
 $acceptance criteria:$",1,1,1,1,0,0,1,0.0,10,4,0.4,3,0.3,3,0.3,3,0.3,3,0.3
358,MCOL-4742,Task,MCOL,2021-06-01 06:59:45,,0,cmapi should respect dbroot path from columnstore.xml -post-install document needed,"It seems, that dbroot paths are handled hardcoded in cmapi.

from node_manipulation.py

{code:java}
    etree.SubElement(sysconf_node, f""DBRoot{next_dbroot_id}"").text =\
      f""/var/lib/columnstore/data{next_dbroot_id}""
    current_dbroot_id = next_dbroot_id
{code}


",,"cmapi should respect dbroot path from columnstore.xml -post-install document needed $end$ It seems, that dbroot paths are handled hardcoded in cmapi.

from node_manipulation.py

{code:java}
    etree.SubElement(sysconf_node, f""DBRoot{next_dbroot_id}"").text =\
      f""/var/lib/columnstore/data{next_dbroot_id}""
    current_dbroot_id = next_dbroot_id
{code}


 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,35,,0,3,0,2,0,3,0,,0,850,3,1,0,2022-03-02 23:07:22,cmapi should respect dbroot path from columnstore.xml,"It seems, that dbroot pathes are handled hardcoded in cmapi.

from node_manipulation.py

{code:java}
    etree.SubElement(sysconf_node, f""DBRoot{next_dbroot_id}"").text =\
      f""/var/lib/columnstore/data{next_dbroot_id}""
    current_dbroot_id = next_dbroot_id
{code}


",,1,1,0,5,0.129032,"cmapi should respect dbroot path from columnstore.xml $end$ It seems, that dbroot pathes are handled hardcoded in cmapi.

from node_manipulation.py

{code:java}
    etree.SubElement(sysconf_node, f""DBRoot{next_dbroot_id}"").text =\
      f""/var/lib/columnstore/data{next_dbroot_id}""
    current_dbroot_id = next_dbroot_id
{code}


 $acceptance criteria:$",2,1,1,0,0,0,1,6592.12,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,0,0.0
359,MCOL-4761,Task,MCOL,2021-06-15 23:43:59,,0,CMAPI Packages Should Follow The Server/Engine Naming Convention,"RPM packages for CMAPI should be stylized with camel case.

*MariaDB-columnstore-cmapi-1.4.rpm*

!Screen Shot 2021-06-15 at 4.41.33 PM.png|example!

DEB packages for CMAPI should be all lower case:

*mariadb-columnstore-cmapi_1.4_amd64.deb*

Also, let's unify the use of architecture suffixes. As you can see from this example. DEB packages include those and RPMs do not.

This could break wild card type upgrades:

{code:xml}
yum -y upgrade MariaDB-*
{code}",,"CMAPI Packages Should Follow The Server/Engine Naming Convention $end$ RPM packages for CMAPI should be stylized with camel case.

*MariaDB-columnstore-cmapi-1.4.rpm*

!Screen Shot 2021-06-15 at 4.41.33 PM.png|example!

DEB packages for CMAPI should be all lower case:

*mariadb-columnstore-cmapi_1.4_amd64.deb*

Also, let's unify the use of architecture suffixes. As you can see from this example. DEB packages include those and RPMs do not.

This could break wild card type upgrades:

{code:xml}
yum -y upgrade MariaDB-*
{code} $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,18,,0,2,0,1,0,0,0,,0,850,1,0,0,2021-06-25 21:07:32,CMAPI Packages Should Follow The Server/Engine Naming Convention,"RPM packages for CMAPI should be stylized with camel case.

*MariaDB-columnstore-cmapi-1.4.rpm*

!Screen Shot 2021-06-15 at 4.41.33 PM.png|example!

DEB packages for CMAPI should be all lower case:

*mariadb-columnstore-cmapi_1.4_amd64.deb*

Also, let's unify the use of architecture suffixes. As you can see from this example. DEB packages include those and RPMs do not.

This could break wild card type upgrades:

{code:xml}
yum -y upgrade MariaDB-*
{code}",,0,0,0,0,0.0,"CMAPI Packages Should Follow The Server/Engine Naming Convention $end$ RPM packages for CMAPI should be stylized with camel case.

*MariaDB-columnstore-cmapi-1.4.rpm*

!Screen Shot 2021-06-15 at 4.41.33 PM.png|example!

DEB packages for CMAPI should be all lower case:

*mariadb-columnstore-cmapi_1.4_amd64.deb*

Also, let's unify the use of architecture suffixes. As you can see from this example. DEB packages include those and RPMs do not.

This could break wild card type upgrades:

{code:xml}
yum -y upgrade MariaDB-*
{code} $acceptance criteria:$",0,0,0,0,0,0,0,237.383,8,1,0.125,0,0.0,0,0.0,0,0.0,0,0.0
360,MCOL-4769,New Feature,MCOL,2021-06-22 13:23:20,,0,Complete ColumnStore Insert Cache Part 1,"h3. The objective of this task is to advance ColumnStore write cache - as defined in MCOL-3875. 

First phase of it was limited to supporting HTAP replication only. This task is extending it into more general status: INSERTS and LDI.

Known problems to be solved under priority:

1. Once the feature is activated in configs, all previous columnstore tables can no longer be accessed. Once de-activated, they come back.
-2. Once the feature is activated, and a table is created and populated, it can be dropped, but then cannot be re-created.- (this is confirmed as not happening anymore, not a problem).
3. In case of multi-node CS cluster set up for DDL replication from primary to the rest, when the cache is flushed, it appears that the data replication occurs also (in addition to DDL). It is supposed to be suppressed in a normal CS cluster operation.
4. Under heavier concurrent bombardment of LDIs, some sessions go into ""waiting for table lock"" state, which in some cases clears after a few hours (unclear why, perhaps some sort of timeout), and some remain in that state forever. Waiting for the lock is understandable - cache is being locked for the duration of a flush. But even if flush takes a few seconds, why would some sessions lock out for hours, and some forever? (workaround - MCOL-4790, use {{columnstore_cache_use_import}})

h3. Basics of the algorithms (full description in 3875):
* inserts are accumulated in cache, transactions serialized by cache locks
* Rollbacks will roll back cache
* Accumulated inserts are flushed into columnstore on various events (all non inserts, including SELECT - and settable size threshold defaulted to 500,000 rows in the cache). 
-- we may want to consider relaxing the above rule and letting SELECTS run as ""dirty reads""

h3. Testing instructions

*Part 1. Positive tests.*

* AUTOCOMMIT=ON, do a lot of individual INSERTs. With cache_insert and cpimport_cache_insert the speed should be orders of magnitude greater.
* Do same inserts but this time within multiple transactions - have something like 3000-4000 inserts in each transaction.
* Do lots of LDIs (also 3000-4000 rows each) in rapid loop (see test scripts in MCOL-4790 as an example). Should be a huge difference in speed.
* Do above from multiple concurrent sessions - the speed improvements should be even greater.

*Part 2.  Adversarial tests.*

* While running AUTOCOMMIT=ON and inserts, also do occasional updates, deletes, and even selects. These reduce the effectiveness of the feature (cache has to be flushed every time these happen). The feature is not useful for workflows if they do not have a predominance of uninterrupted inserts or LDIs.
* Put some updates, deletes or selects inside transactions which do inserts (say, one of those for each 500 inserts). compare the speed.
* Run transactions with inserts from multiple sessions, but rollback some and not others. Make sure the end result data is correct. ",,"Complete ColumnStore Insert Cache Part 1 $end$ h3. The objective of this task is to advance ColumnStore write cache - as defined in MCOL-3875. 

First phase of it was limited to supporting HTAP replication only. This task is extending it into more general status: INSERTS and LDI.

Known problems to be solved under priority:

1. Once the feature is activated in configs, all previous columnstore tables can no longer be accessed. Once de-activated, they come back.
-2. Once the feature is activated, and a table is created and populated, it can be dropped, but then cannot be re-created.- (this is confirmed as not happening anymore, not a problem).
3. In case of multi-node CS cluster set up for DDL replication from primary to the rest, when the cache is flushed, it appears that the data replication occurs also (in addition to DDL). It is supposed to be suppressed in a normal CS cluster operation.
4. Under heavier concurrent bombardment of LDIs, some sessions go into ""waiting for table lock"" state, which in some cases clears after a few hours (unclear why, perhaps some sort of timeout), and some remain in that state forever. Waiting for the lock is understandable - cache is being locked for the duration of a flush. But even if flush takes a few seconds, why would some sessions lock out for hours, and some forever? (workaround - MCOL-4790, use {{columnstore_cache_use_import}})

h3. Basics of the algorithms (full description in 3875):
* inserts are accumulated in cache, transactions serialized by cache locks
* Rollbacks will roll back cache
* Accumulated inserts are flushed into columnstore on various events (all non inserts, including SELECT - and settable size threshold defaulted to 500,000 rows in the cache). 
-- we may want to consider relaxing the above rule and letting SELECTS run as ""dirty reads""

h3. Testing instructions

*Part 1. Positive tests.*

* AUTOCOMMIT=ON, do a lot of individual INSERTs. With cache_insert and cpimport_cache_insert the speed should be orders of magnitude greater.
* Do same inserts but this time within multiple transactions - have something like 3000-4000 inserts in each transaction.
* Do lots of LDIs (also 3000-4000 rows each) in rapid loop (see test scripts in MCOL-4790 as an example). Should be a huge difference in speed.
* Do above from multiple concurrent sessions - the speed improvements should be even greater.

*Part 2.  Adversarial tests.*

* While running AUTOCOMMIT=ON and inserts, also do occasional updates, deletes, and even selects. These reduce the effectiveness of the feature (cache has to be flushed every time these happen). The feature is not useful for workflows if they do not have a predominance of uninterrupted inserts or LDIs.
* Put some updates, deletes or selects inside transactions which do inserts (say, one of those for each 500 inserts). compare the speed.
* Run transactions with inserts from multiple sessions, but rollback some and not others. Make sure the end result data is correct.  $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Critical,49,,3,3,5,9,0,14,0,,0,850,3,1,0,2021-06-23 12:53:37,Complete ColumnStore Insert Cache ,"The objective of this task is to advance ColumnStore write cache - as defined in MCOL-3875. 

First phase of it was limited to supporting HTAP replication only. This task is extending it into more general status: INSERTS and LDI.

Known problems to be solved under priority:
1. Once the feature is activated in configs, all previous columnstore tables can no longer be accessed. Once de-activated, they come back.
2. Once the feature is activated, and a table is created and populated, it can be dropped, but then cannot be re-created.

Basics of the algorithms (full description in 3875):
* inserts are accumulated in cache, transactions serialized by cache locks
* Rollbacks will roll back cache
* Accumulated inserts are flushed into columnstore on various events (all non inserts, including SELECT - and settable size threshold defaulted to 500,000 rows in the cache). 
-- we may want to consider relaxing the above rule and letting SELECTS run as ""dirty reads""
",,1,12,0,330,1.9759,"Complete ColumnStore Insert Cache  $end$ The objective of this task is to advance ColumnStore write cache - as defined in MCOL-3875. 

First phase of it was limited to supporting HTAP replication only. This task is extending it into more general status: INSERTS and LDI.

Known problems to be solved under priority:
1. Once the feature is activated in configs, all previous columnstore tables can no longer be accessed. Once de-activated, they come back.
2. Once the feature is activated, and a table is created and populated, it can be dropped, but then cannot be re-created.

Basics of the algorithms (full description in 3875):
* inserts are accumulated in cache, transactions serialized by cache locks
* Rollbacks will roll back cache
* Accumulated inserts are flushed into columnstore on various events (all non inserts, including SELECT - and settable size threshold defaulted to 500,000 rows in the cache). 
-- we may want to consider relaxing the above rule and letting SELECTS run as ""dirty reads""
 $acceptance criteria:$",13,1,1,1,1,1,1,23.5,11,5,0.454545,4,0.363636,4,0.363636,3,0.272727,3,0.272727
361,MCOL-4771,Task,MCOL,2021-06-22 17:54:22,,0,Rand() Killing PrimProc Under Certain Circumstances,"* It occurs on some tables, but no others (flights, but not airlines)
* Sometimes it blows up on limit1, sometimes on limit 2, sometimes on 10, sometimes on 10000
* Errors vary. In case of dockers the cluster is hosed and requires recycling (on prem systemd repairs it).
* Verified in 5.5.2 and 5.6.1

To reproduce:

Starting with the flights sample data (bts):
https://github.com/mariadb-corporation/mariadb-columnstore-samples

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: st: 0 TupleBPS::receiveMultiPrimitiveMessages() IDB-2035: An internal error occurred.  Check the error log file & contact support

MariaDB [gjd]> use bts
Database changed
MariaDB [bts]> select rand() from flights limit 1;
+---------------------+
| rand()              |
+---------------------+
| 0.06580742547829396 |
+---------------------+
1 row in set (0.118 sec)

MariaDB [bts]> select rand() from flights limit 2;
ERROR 1815 (HY000): Internal error: TupleBPS::run() caught DistributedEngineComm::write: Broken Pipe error
MariaDB [bts]>
{code}

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
{code}

",,"Rand() Killing PrimProc Under Certain Circumstances $end$ * It occurs on some tables, but no others (flights, but not airlines)
* Sometimes it blows up on limit1, sometimes on limit 2, sometimes on 10, sometimes on 10000
* Errors vary. In case of dockers the cluster is hosed and requires recycling (on prem systemd repairs it).
* Verified in 5.5.2 and 5.6.1

To reproduce:

Starting with the flights sample data (bts):
https://github.com/mariadb-corporation/mariadb-columnstore-samples

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: st: 0 TupleBPS::receiveMultiPrimitiveMessages() IDB-2035: An internal error occurred.  Check the error log file & contact support

MariaDB [gjd]> use bts
Database changed
MariaDB [bts]> select rand() from flights limit 1;
+---------------------+
| rand()              |
+---------------------+
| 0.06580742547829396 |
+---------------------+
1 row in set (0.118 sec)

MariaDB [bts]> select rand() from flights limit 2;
ERROR 1815 (HY000): Internal error: TupleBPS::run() caught DistributedEngineComm::write: Broken Pipe error
MariaDB [bts]>
{code}

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
{code}

 $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,14,,1,2,1,2,0,0,0,,0,850,2,0,0,2021-07-23 16:26:48,Rand() Killing PrimProc Under Certain Circumstances,"* It occurs on some tables, but no others (flights, but not airlines)
* Sometimes it blows up on limit1, sometimes on limit 2, sometimes on 10, sometimes on 10000
* Errors vary. In case of dockers the cluster is hosed and requires recycling (on prem systemd repairs it).
* Verified in 5.5.2 and 5.6.1

To reproduce:

Starting with the flights sample data (bts):
https://github.com/mariadb-corporation/mariadb-columnstore-samples

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: st: 0 TupleBPS::receiveMultiPrimitiveMessages() IDB-2035: An internal error occurred.  Check the error log file & contact support

MariaDB [gjd]> use bts
Database changed
MariaDB [bts]> select rand() from flights limit 1;
+---------------------+
| rand()              |
+---------------------+
| 0.06580742547829396 |
+---------------------+
1 row in set (0.118 sec)

MariaDB [bts]> select rand() from flights limit 2;
ERROR 1815 (HY000): Internal error: TupleBPS::run() caught DistributedEngineComm::write: Broken Pipe error
MariaDB [bts]>
{code}

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
{code}

",,0,0,0,0,0.0,"Rand() Killing PrimProc Under Certain Circumstances $end$ * It occurs on some tables, but no others (flights, but not airlines)
* Sometimes it blows up on limit1, sometimes on limit 2, sometimes on 10, sometimes on 10000
* Errors vary. In case of dockers the cluster is hosed and requires recycling (on prem systemd repairs it).
* Verified in 5.5.2 and 5.6.1

To reproduce:

Starting with the flights sample data (bts):
https://github.com/mariadb-corporation/mariadb-columnstore-samples

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: st: 0 TupleBPS::receiveMultiPrimitiveMessages() IDB-2035: An internal error occurred.  Check the error log file & contact support

MariaDB [gjd]> use bts
Database changed
MariaDB [bts]> select rand() from flights limit 1;
+---------------------+
| rand()              |
+---------------------+
| 0.06580742547829396 |
+---------------------+
1 row in set (0.118 sec)

MariaDB [bts]> select rand() from flights limit 2;
ERROR 1815 (HY000): Internal error: TupleBPS::run() caught DistributedEngineComm::write: Broken Pipe error
MariaDB [bts]>
{code}

{code:java}
MariaDB [bts]> create table test as select *, md5(rand(1)) from flights;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
{code}

 $acceptance criteria:$",0,0,0,0,0,0,1,742.533,9,1,0.111111,0,0.0,0,0.0,0,0.0,0,0.0
362,MCOL-4809,New Feature,MCOL,2021-07-09 19:03:56,MCOL-3525,0,Vectorize column scanning/filtering,"As of now there is no way to vectorize the loops of the scanning/filtering code that resides in primitives/linux-port/column.* 

The basic logic is that for the column the mentioned code traverses the block of values:
- skiping empty values
- filtering the values using related filters from SQL statement
- saving the values that satisfies into the output buffer
The code optionally traverses the column block and touch only those values with specific RIDs sent from upper layers. 

The data processing is scalar here with lots of conditions that slows down execution. 
The suggested way is to refactor the code to leverage data prefetch and batch processing using SIMD instructions. The available CPU command set should be detected in runtime on PP startup or at least once per column block.",,"Vectorize column scanning/filtering $end$ As of now there is no way to vectorize the loops of the scanning/filtering code that resides in primitives/linux-port/column.* 

The basic logic is that for the column the mentioned code traverses the block of values:
- skiping empty values
- filtering the values using related filters from SQL statement
- saving the values that satisfies into the output buffer
The code optionally traverses the column block and touch only those values with specific RIDs sent from upper layers. 

The data processing is scalar here with lots of conditions that slows down execution. 
The suggested way is to refactor the code to leverage data prefetch and batch processing using SIMD instructions. The available CPU command set should be detected in runtime on PP startup or at least once per column block. $acceptance criteria:$",,Roman,Roman,Critical,24,,0,3,3,9,0,0,0,,0,850,3,0,0,2021-07-09 19:03:56,Vectorize column scanning/filtering,"As of now there is no way to vectorize the loops of the scanning/filtering code that resides in primitives/linux-port/column.* 

The basic logic is that for the column the mentioned code traverses the block of values:
- skiping empty values
- filtering the values using related filters from SQL statement
- saving the values that satisfies into the output buffer
The code optionally traverses the column block and touch only those values with specific RIDs sent from upper layers. 

The data processing is scalar here with lots of conditions that slows down execution. 
The suggested way is to refactor the code to leverage data prefetch and batch processing using SIMD instructions. The available CPU command set should be detected in runtime on PP startup or at least once per column block.",,0,0,0,0,0.0,"Vectorize column scanning/filtering $end$ As of now there is no way to vectorize the loops of the scanning/filtering code that resides in primitives/linux-port/column.* 

The basic logic is that for the column the mentioned code traverses the block of values:
- skiping empty values
- filtering the values using related filters from SQL statement
- saving the values that satisfies into the output buffer
The code optionally traverses the column block and touch only those values with specific RIDs sent from upper layers. 

The data processing is scalar here with lots of conditions that slows down execution. 
The suggested way is to refactor the code to leverage data prefetch and batch processing using SIMD instructions. The available CPU command set should be detected in runtime on PP startup or at least once per column block. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,49,10,0.204082,7,0.142857,4,0.0816327,3,0.0612245,2,0.0408163
363,MCOL-4810,New Feature,MCOL,2021-07-12 09:04:14,MCOL-3525,0,Redundant copying and wasting memory in PrimProc,"There is a class called ByteStream that represents ... a byte stream. BS allocates and uses another arbitrary sized chunk of RAM storing RGData with long strings. BS also runs a memcpy to move the data.
This behavior is too wasteful resource-wise. The current implementation of BS can't mix together the stream from its own buffer and data from a smart pointer backed buffer. 
The suggested approach is to replace BS'es internal buffer in certain cases(when it serializes RGData to send another portion of records over the network) with a stack of smart pointers that contains actual data buffers. This saves RAM that is shared b/w RGData and ByteStream and time b/c runtime skips memcpy calls.   ",,"Redundant copying and wasting memory in PrimProc $end$ There is a class called ByteStream that represents ... a byte stream. BS allocates and uses another arbitrary sized chunk of RAM storing RGData with long strings. BS also runs a memcpy to move the data.
This behavior is too wasteful resource-wise. The current implementation of BS can't mix together the stream from its own buffer and data from a smart pointer backed buffer. 
The suggested approach is to replace BS'es internal buffer in certain cases(when it serializes RGData to send another portion of records over the network) with a stack of smart pointers that contains actual data buffers. This saves RAM that is shared b/w RGData and ByteStream and time b/c runtime skips memcpy calls.    $acceptance criteria:$",,Roman,Roman,Critical,13,,1,1,2,3,0,1,0,,0,850,1,0,0,2021-07-25 22:36:29,ByteStream makes another copy of immutable data,"There is a class called ByteStream that represents ... a byte stream. BS allocates and uses another arbitrary sized chunk of RAM storing RGData with long strings. BS also runs a memcpy to move the data.
This behavior is too wasteful resource-wise. The current implementation of BS can't mix together the stream from its own buffer and data from a smart pointer backed buffer. 
The suggested approach is to replace BS'es internal buffer in certain cases(when it serializes RGData to send another portion of records over the network) with a stack of smart pointers that contains actual data buffers. This saves RAM that is shared b/w RGData and ByteStream and time b/c runtime skips memcpy calls.   ",,1,0,0,14,0.0555556,"ByteStream makes another copy of immutable data $end$ There is a class called ByteStream that represents ... a byte stream. BS allocates and uses another arbitrary sized chunk of RAM storing RGData with long strings. BS also runs a memcpy to move the data.
This behavior is too wasteful resource-wise. The current implementation of BS can't mix together the stream from its own buffer and data from a smart pointer backed buffer. 
The suggested approach is to replace BS'es internal buffer in certain cases(when it serializes RGData to send another portion of records over the network) with a stack of smart pointers that contains actual data buffers. This saves RAM that is shared b/w RGData and ByteStream and time b/c runtime skips memcpy calls.    $acceptance criteria:$",1,1,1,1,0,0,1,325.533,50,10,0.2,7,0.14,4,0.08,3,0.06,2,0.04
364,MCOL-4814,Task,MCOL,2021-07-15 17:22:17,,0,Enable CS build with LZ4 compression only by build flag,,,Enable CS build with LZ4 compression only by build flag $end$ $acceptance criteria:$,,Denis Khalikov,Denis Khalikov,Blocker,9,,0,3,0,1,0,0,0,,0,850,2,0,0,2021-07-15 19:31:56,Enable CS build with LZ4 compression only by build flag,,,0,0,0,0,0.0,Enable CS build with LZ4 compression only by build flag $end$ $acceptance criteria:$,0,0,0,0,0,0,0,2.15,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
365,MCOL-4815,New Feature,MCOL,2021-07-16 11:45:46,,0,Refactor ColumnCommand to have multiple derived classes specified by column width ,"There are number of if/switch blocks in a number of ColumnCommand methods. Some of them are inside heavy processing loops. 
The current CC method implementation doesn't allow to separate out a low level scanning/filtering loops, e.g. go over the column block and compare the values against empty/null magic. This prevents the code from being parallelized using SIMD.
The suggested approach is to turn most of the related CC methods into templates to leverage this in the future pathes.  ",,"Refactor ColumnCommand to have multiple derived classes specified by column width  $end$ There are number of if/switch blocks in a number of ColumnCommand methods. Some of them are inside heavy processing loops. 
The current CC method implementation doesn't allow to separate out a low level scanning/filtering loops, e.g. go over the column block and compare the values against empty/null magic. This prevents the code from being parallelized using SIMD.
The suggested approach is to turn most of the related CC methods into templates to leverage this in the future pathes.   $acceptance criteria:$",,Roman,Roman,Major,4,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-07-16 11:45:46,Refactor ColumnCommand to have multiple derived classes specified by column width ,"There are number of if/switch blocks in a number of ColumnCommand methods. Some of them are inside heavy processing loops. 
The current CC method implementation doesn't allow to separate out a low level scanning/filtering loops, e.g. go over the column block and compare the values against empty/null magic. This prevents the code from being parallelized using SIMD.
The suggested approach is to turn most of the related CC methods into templates to leverage this in the future pathes.  ",,0,0,0,0,0.0,"Refactor ColumnCommand to have multiple derived classes specified by column width  $end$ There are number of if/switch blocks in a number of ColumnCommand methods. Some of them are inside heavy processing loops. 
The current CC method implementation doesn't allow to separate out a low level scanning/filtering loops, e.g. go over the column block and compare the values against empty/null magic. This prevents the code from being parallelized using SIMD.
The suggested approach is to turn most of the related CC methods into templates to leverage this in the future pathes.   $acceptance criteria:$",0,0,0,0,0,0,0,0.0,51,11,0.215686,8,0.156863,5,0.0980392,3,0.0588235,2,0.0392157
366,MCOL-4849,New Feature,MCOL,2021-08-30 12:34:14,MCOL-3525,0,Optimize ExeMgr to reduce a number of context switches ,"MCS generates enourmous amount of context switches that degrades performance a lot. The screenshoot cs.png demonstates this, namely cs number raises from ~80 to 21k once I run a single query(select l_orderkey, count(l_orderkey) from lineitem group by l_orderkey limit 10) in an infinite loop.
According with the perf tool observations(collected with perf record --call-graph dwarf -e context-switches -p $(pidof ExeMgr) command) there are number of candidates for the optimization:
- messageqcpp::InetStreamSocket::readToMagic() that both uses poll and reads a byte at a time increasing the number of cs needed.
- joblist::TupleBPS::receiveMultiPrimitiveMessages() that contains a loop that processes intermediate RGData sent by PP to EM as results of Primitive requests. There are both mutex and conditional_variables widely used in TupleBPS methods's code.
- joblist::FIFO<rowgroup::RGData>::swapBuffers() that leverages mutexes to save crit section of this RGData queue from TBPS to TAS.
Plz take a look at perf.png for some details.
The goal is to reduce a number of context switches produced by ExeMgr code. It is worth to have in mind that othere categories of queries might produce a different pattern. However the three parts mentioned affect every query b/c the are substantial to query processing.",,"Optimize ExeMgr to reduce a number of context switches  $end$ MCS generates enourmous amount of context switches that degrades performance a lot. The screenshoot cs.png demonstates this, namely cs number raises from ~80 to 21k once I run a single query(select l_orderkey, count(l_orderkey) from lineitem group by l_orderkey limit 10) in an infinite loop.
According with the perf tool observations(collected with perf record --call-graph dwarf -e context-switches -p $(pidof ExeMgr) command) there are number of candidates for the optimization:
- messageqcpp::InetStreamSocket::readToMagic() that both uses poll and reads a byte at a time increasing the number of cs needed.
- joblist::TupleBPS::receiveMultiPrimitiveMessages() that contains a loop that processes intermediate RGData sent by PP to EM as results of Primitive requests. There are both mutex and conditional_variables widely used in TupleBPS methods's code.
- joblist::FIFO<rowgroup::RGData>::swapBuffers() that leverages mutexes to save crit section of this RGData queue from TBPS to TAS.
Plz take a look at perf.png for some details.
The goal is to reduce a number of context switches produced by ExeMgr code. It is worth to have in mind that othere categories of queries might produce a different pattern. However the three parts mentioned affect every query b/c the are substantial to query processing. $acceptance criteria:$",,Roman,Roman,Major,16,,1,1,1,6,0,1,0,,0,850,1,0,0,2021-08-30 12:34:14,Optimize ExeMgr to reduce a number of context switches ,"MCS generates enourmous amount of context switches that degradates performance a lot. The screenshoot cs.png demonstates this, namely cs number raises from ~80 to 21k once I run a single query(select l_orderkey, count(l_orderkey) from lineitem group by l_orderkey limit 10) in an infinite loop.
According with the perf tool observations(collected with perf record --call-graph dwarf -e context-switches -p $(pidof ExeMgr) command) there are number of candidates for the optimization:
- messageqcpp::InetStreamSocket::readToMagic() that both uses poll and reads a byte at a time increasing the number of cs needed.
- joblist::TupleBPS::receiveMultiPrimitiveMessages() that contains a loop that processes intermediate RGData sent by PP to EM as results of Primitive requests. There are both mutex and conditional_variables widely used in TupleBPS methods's code.
- joblist::FIFO<rowgroup::RGData>::swapBuffers() that leverages mutexes to save crit section of this RGData queue from TBPS to TAS.
Plz take a look at perf.png for some details.
The goal is to reduce a number of context switches produced by ExeMgr code. It is worth to have in mind that othere categories of queries might produce a different pattern. However the three parts mentioned affect every query b/c the are substantial to query processing.",,0,1,0,2,0.00490196,"Optimize ExeMgr to reduce a number of context switches  $end$ MCS generates enourmous amount of context switches that degradates performance a lot. The screenshoot cs.png demonstates this, namely cs number raises from ~80 to 21k once I run a single query(select l_orderkey, count(l_orderkey) from lineitem group by l_orderkey limit 10) in an infinite loop.
According with the perf tool observations(collected with perf record --call-graph dwarf -e context-switches -p $(pidof ExeMgr) command) there are number of candidates for the optimization:
- messageqcpp::InetStreamSocket::readToMagic() that both uses poll and reads a byte at a time increasing the number of cs needed.
- joblist::TupleBPS::receiveMultiPrimitiveMessages() that contains a loop that processes intermediate RGData sent by PP to EM as results of Primitive requests. There are both mutex and conditional_variables widely used in TupleBPS methods's code.
- joblist::FIFO<rowgroup::RGData>::swapBuffers() that leverages mutexes to save crit section of this RGData queue from TBPS to TAS.
Plz take a look at perf.png for some details.
The goal is to reduce a number of context switches produced by ExeMgr code. It is worth to have in mind that othere categories of queries might produce a different pattern. However the three parts mentioned affect every query b/c the are substantial to query processing. $acceptance criteria:$",1,1,0,0,0,0,1,0.0,52,11,0.211538,8,0.153846,5,0.0961538,3,0.0576923,2,0.0384615
367,MCOL-4851,Task,MCOL,2021-08-30 17:50:25,,0,Fix CMAPI tests and related things.,"* Improve and fix cmapi tests (exclude failover) to stabilize output.
* fix code style in places when code changed to meet PEP-8
* add and improve docstrings
* fix methods, classes and functions related to broken tests
* fix founded bugs in related code:
** passing arguments while calling get_current_config_root in NodeConfig.apply_config
** method of getting a dispatcher_name in NodeConfig.apply_config
** in code imports in NodeConfig.apply_config
** file open using context manager in NodeConfig.apply_config
** copy default cmapi config if system config doesn't exist
** unused argument in cmapi_server.helpers.get_version function
** DBRM.__send_command naming to DBRM._send_command
** DBRM.__send_command to parse GETREADONLY response in a right way
** MySocket.myreceive_to_magic to read response one byte at a time
** unused open() in endpoints.ClusterController
** dbrm.DBRM._send_command logic
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** rem useless mtehod set_cluster_mode from NodeConfig in mcs_node_control/models/node_config.py",,"Fix CMAPI tests and related things. $end$ * Improve and fix cmapi tests (exclude failover) to stabilize output.
* fix code style in places when code changed to meet PEP-8
* add and improve docstrings
* fix methods, classes and functions related to broken tests
* fix founded bugs in related code:
** passing arguments while calling get_current_config_root in NodeConfig.apply_config
** method of getting a dispatcher_name in NodeConfig.apply_config
** in code imports in NodeConfig.apply_config
** file open using context manager in NodeConfig.apply_config
** copy default cmapi config if system config doesn't exist
** unused argument in cmapi_server.helpers.get_version function
** DBRM.__send_command naming to DBRM._send_command
** DBRM.__send_command to parse GETREADONLY response in a right way
** MySocket.myreceive_to_magic to read response one byte at a time
** unused open() in endpoints.ClusterController
** dbrm.DBRM._send_command logic
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** rem useless mtehod set_cluster_mode from NodeConfig in mcs_node_control/models/node_config.py $acceptance criteria:$",,Alan Mologorsky,Alan Mologorsky,Major,32,,0,0,1,4,0,3,0,,0,850,0,3,0,2021-10-18 17:44:09,Fix CMAPI tests and related things.,"* Improve and fix cmapi tests (exclude failover) to stabilize output.
* fix code style in places when code changed to meet PEP-8
* add and improve docstrings
* fix methods, classes and functions related to broken tests
* fix founded bugs in related code:
** passing arguments while calling get_current_config_root in NodeConfig.apply_config
** method of getting a dispatcher_name in NodeConfig.apply_config
** in code imports in NodeConfig.apply_config
** file open using context manager in NodeConfig.apply_config
** copy default cmapi config if system config doesn't exist
** unused argument in cmapi_server.helpers.get_version function
** DBRM.__send_command naming to DBRM._send_command
** DBRM.__send_command to parse GETREADONLY response in a right way
** MySocket.myreceive_to_magic to read response one byte at a time
** unused open() in endpoints.ClusterController
** dbrm.DBRM._send_command logic
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** rem useless mtehod set_cluster_mode from NodeConfig in mcs_node_control/models/node_config.py",,0,0,0,0,0.0,"Fix CMAPI tests and related things. $end$ * Improve and fix cmapi tests (exclude failover) to stabilize output.
* fix code style in places when code changed to meet PEP-8
* add and improve docstrings
* fix methods, classes and functions related to broken tests
* fix founded bugs in related code:
** passing arguments while calling get_current_config_root in NodeConfig.apply_config
** method of getting a dispatcher_name in NodeConfig.apply_config
** in code imports in NodeConfig.apply_config
** file open using context manager in NodeConfig.apply_config
** copy default cmapi config if system config doesn't exist
** unused argument in cmapi_server.helpers.get_version function
** DBRM.__send_command naming to DBRM._send_command
** DBRM.__send_command to parse GETREADONLY response in a right way
** MySocket.myreceive_to_magic to read response one byte at a time
** unused open() in endpoints.ClusterController
** dbrm.DBRM._send_command logic
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** add CMAPI_DEFAULT_CONF const to cmapi_server.__main__
** rem useless mtehod set_cluster_mode from NodeConfig in mcs_node_control/models/node_config.py $acceptance criteria:$",0,0,0,0,0,0,1,1175.88,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
368,MCOL-4860,Task,MCOL,2021-09-06 17:38:52,,0,Disable multiple ExeMgrs in the cluster,Very unfortunately but if a node has both EM and PP it will have a huge system overhead in terms of CPU b/c these two compete for CPU cores. The converged topology with EM/PP at every node is impractical untill we converge processes. The converge will allow to account/limit the workload EM and PP produce their resource consumption. ,,Disable multiple ExeMgrs in the cluster $end$ Very unfortunately but if a node has both EM and PP it will have a huge system overhead in terms of CPU b/c these two compete for CPU cores. The converged topology with EM/PP at every node is impractical untill we converge processes. The converge will allow to account/limit the workload EM and PP produce their resource consumption.  $acceptance criteria:$,,Roman,Roman,Critical,6,,1,0,1,4,0,0,0,,0,850,0,0,0,2021-09-06 17:38:52,Disable multiple ExeMgrs in the cluster,Very unfortunately but if a node has both EM and PP it will have a huge system overhead in terms of CPU b/c these two compete for CPU cores. The converged topology with EM/PP at every node is impractical untill we converge processes. The converge will allow to account/limit the workload EM and PP produce their resource consumption. ,,0,0,0,0,0.0,Disable multiple ExeMgrs in the cluster $end$ Very unfortunately but if a node has both EM and PP it will have a huge system overhead in terms of CPU b/c these two compete for CPU cores. The converged topology with EM/PP at every node is impractical untill we converge processes. The converge will allow to account/limit the workload EM and PP produce their resource consumption.  $acceptance criteria:$,0,0,0,0,0,0,1,0.0,53,12,0.226415,8,0.150943,5,0.0943396,3,0.0566038,2,0.0377358
369,MCOL-4876,Task,MCOL,2021-09-24 15:34:00,,0,Separate values and RID vectors sent b/w filtering and PrimitiveProcessor runtimes ,"There are three modes of projection in the filtering code: OT_BOTH, OT_RID, OT_DATAVALUE. Filtering code sends back values in form of
* OT_RID  Array uint16_t rids
* OT_DATAVALUE Array T values
* OT_BOTH  Array { uint16_t rid, T value }
This form makes vectorized filtering processing slower so the suggestion is to separate two vectors to make writes in filterting code faster.
The buffer for NewColResultHeader must be aligned by the sizeof(biggest integral data type available). It is 16 bytes now.
This issue also addresses the fact that some buffers either aren't aligned at all(e.g. BPP::blockData) or aligned but are used by structures that effectively breaks the alignment(e.g. BPP::outputMsg).",,"Separate values and RID vectors sent b/w filtering and PrimitiveProcessor runtimes  $end$ There are three modes of projection in the filtering code: OT_BOTH, OT_RID, OT_DATAVALUE. Filtering code sends back values in form of
* OT_RID  Array uint16_t rids
* OT_DATAVALUE Array T values
* OT_BOTH  Array { uint16_t rid, T value }
This form makes vectorized filtering processing slower so the suggestion is to separate two vectors to make writes in filterting code faster.
The buffer for NewColResultHeader must be aligned by the sizeof(biggest integral data type available). It is 16 bytes now.
This issue also addresses the fact that some buffers either aren't aligned at all(e.g. BPP::blockData) or aligned but are used by structures that effectively breaks the alignment(e.g. BPP::outputMsg). $acceptance criteria:$",,Roman,Roman,Major,9,,0,1,1,2,0,1,0,,0,850,1,0,0,2021-09-24 15:34:00,Separate values and RID vectors sent b/w filtering and PrimitiveProcessor runtimes ,"There are three modes of projection in the filtering code: OT_BOTH, OT_RID, OT_DATAVALUE. Filtering code sends back values in form of
* OT_RID  Array uint16_t rids
* OT_DATAVALUE Array T values
* OT_BOTH  Array { uint16_t rid, T value }
This form makes vectorized filtering processing slower so the suggestion is to separate two vectors to make writes in filterting code faster.
The buffer for NewColResultHeader must be aligned by the sizeof(biggest integral data type available). It is 16 bytes now.",,0,1,0,28,0.294737,"Separate values and RID vectors sent b/w filtering and PrimitiveProcessor runtimes  $end$ There are three modes of projection in the filtering code: OT_BOTH, OT_RID, OT_DATAVALUE. Filtering code sends back values in form of
* OT_RID  Array uint16_t rids
* OT_DATAVALUE Array T values
* OT_BOTH  Array { uint16_t rid, T value }
This form makes vectorized filtering processing slower so the suggestion is to separate two vectors to make writes in filterting code faster.
The buffer for NewColResultHeader must be aligned by the sizeof(biggest integral data type available). It is 16 bytes now. $acceptance criteria:$",1,1,1,1,1,1,1,0.0,54,12,0.222222,8,0.148148,5,0.0925926,3,0.0555556,2,0.037037
370,MCOL-488,New Feature,MCOL,2016-12-28 17:40:45,,0,create AlarmConfig.installSave file,"In the cases where AlarmConfig.xml is zero'd out, if this happens on a single server deployment you do not have a working copy to use from another node. The only solution is to do another install to get the file. A simpler solution would be to create a copy of AlarmConfig.xml as AlarmConfig.xml.installSave like we do for ColumnStore.xml. It's unlikely someone would change this so copying the installSave would help anyone recover from the case where AlarmConfig.xml get's zero'd out. Even when that bug gets fixed it's probably still helpful to have an installl time reference file",,"create AlarmConfig.installSave file $end$ In the cases where AlarmConfig.xml is zero'd out, if this happens on a single server deployment you do not have a working copy to use from another node. The only solution is to do another install to get the file. A simpler solution would be to create a copy of AlarmConfig.xml as AlarmConfig.xml.installSave like we do for ColumnStore.xml. It's unlikely someone would change this so copying the installSave would help anyone recover from the case where AlarmConfig.xml get's zero'd out. Even when that bug gets fixed it's probably still helpful to have an installl time reference file $acceptance criteria:$",,David Thompson,David Thompson,Major,8,,0,5,0,2,0,0,0,,0,850,5,0,0,2017-01-03 16:13:12,create AlarmConfig.installSave file,"In the cases where AlarmConfig.xml is zero'd out, if this happens on a single server deployment you do not have a working copy to use from another node. The only solution is to do another install to get the file. A simpler solution would be to create a copy of AlarmConfig.xml as AlarmConfig.xml.installSave like we do for ColumnStore.xml. It's unlikely someone would change this so copying the installSave would help anyone recover from the case where AlarmConfig.xml get's zero'd out. Even when that bug gets fixed it's probably still helpful to have an installl time reference file",,0,0,0,0,0.0,"create AlarmConfig.installSave file $end$ In the cases where AlarmConfig.xml is zero'd out, if this happens on a single server deployment you do not have a working copy to use from another node. The only solution is to do another install to get the file. A simpler solution would be to create a copy of AlarmConfig.xml as AlarmConfig.xml.installSave like we do for ColumnStore.xml. It's unlikely someone would change this so copying the installSave would help anyone recover from the case where AlarmConfig.xml get's zero'd out. Even when that bug gets fixed it's probably still helpful to have an installl time reference file $acceptance criteria:$",0,0,0,0,0,0,1,142.533,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
371,MCOL-4898,New Feature,MCOL,2021-10-15 12:28:59,,0,cmapi not respect custom mariadb port for crossenginesupport from Columnstore.xml,"
Given the following config

{code:java}

*******************************
<CrossEngineSupport>
<Host>127.0.0.1</Host>
<Port>3307</Port>
<User>cross_engine</User>
<Password>xxxxxxxx</Password>
**************************

{code}


The sourcecode from cmapi reveals:

{code:java}

node_config = NodeConfig()
    root = node_config.get_current_config_root()
    ces_node = root.find(""./CrossEngineSupport"")
    username = ces_node.find(""./User"").text
    password = ces_node.find(""./Password"").text


 cmd = (f""/usr/bin/mariadb -h 127.0.0.1 -u '{username}' --password='{password}' -sN -e \
            \""SELECT COUNT(1) AS slave_threads FROM information_schema.PROCESSLIST WHERE USER = 'system user' AND COMMAND LIKE 'Slave%';\"""")

{code}

So Port will neither not be read from Columnstore.xml nor used in the mariadb commandline ",,"cmapi not respect custom mariadb port for crossenginesupport from Columnstore.xml $end$ 
Given the following config

{code:java}

*******************************
<CrossEngineSupport>
<Host>127.0.0.1</Host>
<Port>3307</Port>
<User>cross_engine</User>
<Password>xxxxxxxx</Password>
**************************

{code}


The sourcecode from cmapi reveals:

{code:java}

node_config = NodeConfig()
    root = node_config.get_current_config_root()
    ces_node = root.find(""./CrossEngineSupport"")
    username = ces_node.find(""./User"").text
    password = ces_node.find(""./Password"").text


 cmd = (f""/usr/bin/mariadb -h 127.0.0.1 -u '{username}' --password='{password}' -sN -e \
            \""SELECT COUNT(1) AS slave_threads FROM information_schema.PROCESSLIST WHERE USER = 'system user' AND COMMAND LIKE 'Slave%';\"""")

{code}

So Port will neither not be read from Columnstore.xml nor used in the mariadb commandline  $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,37,,0,1,1,1,0,0,0,,0,850,0,0,0,2022-02-15 17:15:20,cmapi not respect custom mariadb port for crossenginesupport from Columnstore.xml,"
Given the following config

{code:java}

*******************************
<CrossEngineSupport>
<Host>127.0.0.1</Host>
<Port>3307</Port>
<User>cross_engine</User>
<Password>xxxxxxxx</Password>
**************************

{code}


The sourcecode from cmapi reveals:

{code:java}

node_config = NodeConfig()
    root = node_config.get_current_config_root()
    ces_node = root.find(""./CrossEngineSupport"")
    username = ces_node.find(""./User"").text
    password = ces_node.find(""./Password"").text


 cmd = (f""/usr/bin/mariadb -h 127.0.0.1 -u '{username}' --password='{password}' -sN -e \
            \""SELECT COUNT(1) AS slave_threads FROM information_schema.PROCESSLIST WHERE USER = 'system user' AND COMMAND LIKE 'Slave%';\"""")

{code}

So Port will neither not be read from Columnstore.xml nor used in the mariadb commandline ",,0,0,0,0,0.0,"cmapi not respect custom mariadb port for crossenginesupport from Columnstore.xml $end$ 
Given the following config

{code:java}

*******************************
<CrossEngineSupport>
<Host>127.0.0.1</Host>
<Port>3307</Port>
<User>cross_engine</User>
<Password>xxxxxxxx</Password>
**************************

{code}


The sourcecode from cmapi reveals:

{code:java}

node_config = NodeConfig()
    root = node_config.get_current_config_root()
    ces_node = root.find(""./CrossEngineSupport"")
    username = ces_node.find(""./User"").text
    password = ces_node.find(""./Password"").text


 cmd = (f""/usr/bin/mariadb -h 127.0.0.1 -u '{username}' --password='{password}' -sN -e \
            \""SELECT COUNT(1) AS slave_threads FROM information_schema.PROCESSLIST WHERE USER = 'system user' AND COMMAND LIKE 'Slave%';\"""")

{code}

So Port will neither not be read from Columnstore.xml nor used in the mariadb commandline  $acceptance criteria:$",0,0,0,0,0,0,0,2956.77,4,2,0.5,2,0.5,1,0.25,1,0.25,0,0.0
372,MCOL-4907,Task,MCOL,2021-10-28 11:37:48,,0,Fix logging format and logic inside cmapi,"logging inside cmapi configured in a weird way.
Reconfigure logging and fix all places with it's init.
Use only one init point at application startup.

UPD:
Because of relative hardcoded logging configuration file path it blocks normal working of CLI tool and causes stdout messages.",,"Fix logging format and logic inside cmapi $end$ logging inside cmapi configured in a weird way.
Reconfigure logging and fix all places with it's init.
Use only one init point at application startup.

UPD:
Because of relative hardcoded logging configuration file path it blocks normal working of CLI tool and causes stdout messages. $acceptance criteria:$",,Alan Mologorsky,Alan Mologorsky,Blocker,16,,0,0,2,1,0,1,0,,0,850,0,0,0,2022-05-17 15:58:58,Fix logging format and logic inside cmapi,"logging inside cmapi configured in a weird way.
Reconfigure logging and fix all places with it's init.
Use only one init point at application startup.",,0,1,0,20,0.571429,"Fix logging format and logic inside cmapi $end$ logging inside cmapi configured in a weird way.
Reconfigure logging and fix all places with it's init.
Use only one init point at application startup. $acceptance criteria:$",1,1,1,1,1,1,1,4828.35,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
373,MCOL-4910,New Feature,MCOL,2021-10-29 13:52:07,,0,C++ 20,"We have performance issues and using async code is superior advance in this direction. Only C++ 20 can let us write async code in sync terms, and maybe using modern Seaside framework.
Coroutines introduced in c++20 are killer feature and we should use modern technologies
",,"C++ 20 $end$ We have performance issues and using async code is superior advance in this direction. Only C++ 20 can let us write async code in sync terms, and maybe using modern Seaside framework.
Coroutines introduced in c++20 are killer feature and we should use modern technologies
 $acceptance criteria:$",,Leonid Fedorov,Leonid Fedorov,Major,15,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-10-29 14:01:46,C++ 20,"We have performance issues and using async code is superior advance in this direction. Only C++ 20 can let us write async code in sync terms, and maybe using modern Seaside framework.
Coroutines introduced in c++20 are killer feature and we should use modern technologies
",,0,0,0,0,0.0,"C++ 20 $end$ We have performance issues and using async code is superior advance in this direction. Only C++ 20 can let us write async code in sync terms, and maybe using modern Seaside framework.
Coroutines introduced in c++20 are killer feature and we should use modern technologies
 $acceptance criteria:$",0,0,0,0,0,0,1,0.15,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
374,MCOL-4917,New Feature,MCOL,2021-11-08 14:32:58,,0,Certain Extent Map operations doesn't scale well.,There is a major deficiency in parts of EM code that makes the operations slower. We need to profile and introduce lookup structures to speed up read/write ops in EM. ExtentMap::getDbRootHWMInfo() must be overloaded to serve multiple oids at a time. ExtentMap::setLocalHWM is called multiple times compacting journal into EM on MCS shutdown(save_brm presumably). setLocalHWM is utterly slow that forces systemd to kill save_brm on timeout.,,Certain Extent Map operations doesn't scale well. $end$ There is a major deficiency in parts of EM code that makes the operations slower. We need to profile and introduce lookup structures to speed up read/write ops in EM. ExtentMap::getDbRootHWMInfo() must be overloaded to serve multiple oids at a time. ExtentMap::setLocalHWM is called multiple times compacting journal into EM on MCS shutdown(save_brm presumably). setLocalHWM is utterly slow that forces systemd to kill save_brm on timeout. $acceptance criteria:$,,Roman,Roman,Critical,20,,1,1,5,4,0,0,0,,0,850,1,0,0,2021-11-08 14:32:58,Certain Extent Map operations doesn't scale well.,There is a major deficiency in parts of EM code that makes the operations slower. We need to profile and introduce lookup structures to speed up read/write ops in EM. ExtentMap::getDbRootHWMInfo() must be overloaded to serve multiple oids at a time. ExtentMap::setLocalHWM is called multiple times compacting journal into EM on MCS shutdown(save_brm presumably). setLocalHWM is utterly slow that forces systemd to kill save_brm on timeout.,,0,0,0,0,0.0,Certain Extent Map operations doesn't scale well. $end$ There is a major deficiency in parts of EM code that makes the operations slower. We need to profile and introduce lookup structures to speed up read/write ops in EM. ExtentMap::getDbRootHWMInfo() must be overloaded to serve multiple oids at a time. ExtentMap::setLocalHWM is called multiple times compacting journal into EM on MCS shutdown(save_brm presumably). setLocalHWM is utterly slow that forces systemd to kill save_brm on timeout. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,55,13,0.236364,9,0.163636,6,0.109091,4,0.0727273,3,0.0545455
375,MCOL-4923,New Feature,MCOL,2021-11-15 18:36:27,,0,Benchmark factoring out the constant string in dictionary filtering.,"All but two predicates for filtering operations over dictionaries' rows are implemented using strnncollsp function from MariaDB server code. Here's [a link|https://github.com/MariaDB/server/blob/10.8/strings/ctype-uca.ic#L186] to an include fiile which implements such function (it is included in several *.c files and is more or less duplicated).

Note that we initialize two scanners and perform two scanner_next functions in the loop. One such scanner_next function is implemented above in the same file and it also sports a loop and pretty heavy logic inside and outside of that loop.

Usually, these scanning operations are performed until there is a difference or an end of both of strings.

But, for one contrived example, the TPCH benchmark suite sports customer table with c_name column which contains ""names"" such as 'customer#000000010' - e.g., one cn expect that some 10-11 first characters will be different. Also, c_name column has a case-insensitive collation which is lightweight yet still requires some processing. And, last but not least, queries in TPCH appear to use a lot of filtering over concrete customer names.

It appears that it is worthwhile to have scanning performed only once for a string that is constant in the filtering operation.

Right now there's no such functionality in the server. Alexander Barkov said that it may be beneficial for server too.

The goal of this task is to implement that functionalitty for some (not all - for two or three collations, some simple like latin1_swedish_ci and some complex like utf8_ci) and perform benchmarks. I think it is possible to just copy out some code from strings directory of the server just to demonstrate what can be achieved.",,"Benchmark factoring out the constant string in dictionary filtering. $end$ All but two predicates for filtering operations over dictionaries' rows are implemented using strnncollsp function from MariaDB server code. Here's [a link|https://github.com/MariaDB/server/blob/10.8/strings/ctype-uca.ic#L186] to an include fiile which implements such function (it is included in several *.c files and is more or less duplicated).

Note that we initialize two scanners and perform two scanner_next functions in the loop. One such scanner_next function is implemented above in the same file and it also sports a loop and pretty heavy logic inside and outside of that loop.

Usually, these scanning operations are performed until there is a difference or an end of both of strings.

But, for one contrived example, the TPCH benchmark suite sports customer table with c_name column which contains ""names"" such as 'customer#000000010' - e.g., one cn expect that some 10-11 first characters will be different. Also, c_name column has a case-insensitive collation which is lightweight yet still requires some processing. And, last but not least, queries in TPCH appear to use a lot of filtering over concrete customer names.

It appears that it is worthwhile to have scanning performed only once for a string that is constant in the filtering operation.

Right now there's no such functionality in the server. Alexander Barkov said that it may be beneficial for server too.

The goal of this task is to implement that functionalitty for some (not all - for two or three collations, some simple like latin1_swedish_ci and some complex like utf8_ci) and perform benchmarks. I think it is possible to just copy out some code from strings directory of the server just to demonstrate what can be achieved. $acceptance criteria:$",,Sergey Zefirov,Sergey Zefirov,Major,1,,0,6,0,1,0,0,0,,0,850,6,0,0,2021-11-16 12:47:13,Benchmark factoring out the constant string in dictionary filtering.,"All but two predicates for filtering operations over dictionaries' rows are implemented using strnncollsp function from MariaDB server code. Here's [a link|https://github.com/MariaDB/server/blob/10.8/strings/ctype-uca.ic#L186] to an include fiile which implements such function (it is included in several *.c files and is more or less duplicated).

Note that we initialize two scanners and perform two scanner_next functions in the loop. One such scanner_next function is implemented above in the same file and it also sports a loop and pretty heavy logic inside and outside of that loop.

Usually, these scanning operations are performed until there is a difference or an end of both of strings.

But, for one contrived example, the TPCH benchmark suite sports customer table with c_name column which contains ""names"" such as 'customer#000000010' - e.g., one cn expect that some 10-11 first characters will be different. Also, c_name column has a case-insensitive collation which is lightweight yet still requires some processing. And, last but not least, queries in TPCH appear to use a lot of filtering over concrete customer names.

It appears that it is worthwhile to have scanning performed only once for a string that is constant in the filtering operation.

Right now there's no such functionality in the server. Alexander Barkov said that it may be beneficial for server too.

The goal of this task is to implement that functionalitty for some (not all - for two or three collations, some simple like latin1_swedish_ci and some complex like utf8_ci) and perform benchmarks. I think it is possible to just copy out some code from strings directory of the server just to demonstrate what can be achieved.",,0,0,0,0,0.0,"Benchmark factoring out the constant string in dictionary filtering. $end$ All but two predicates for filtering operations over dictionaries' rows are implemented using strnncollsp function from MariaDB server code. Here's [a link|https://github.com/MariaDB/server/blob/10.8/strings/ctype-uca.ic#L186] to an include fiile which implements such function (it is included in several *.c files and is more or less duplicated).

Note that we initialize two scanners and perform two scanner_next functions in the loop. One such scanner_next function is implemented above in the same file and it also sports a loop and pretty heavy logic inside and outside of that loop.

Usually, these scanning operations are performed until there is a difference or an end of both of strings.

But, for one contrived example, the TPCH benchmark suite sports customer table with c_name column which contains ""names"" such as 'customer#000000010' - e.g., one cn expect that some 10-11 first characters will be different. Also, c_name column has a case-insensitive collation which is lightweight yet still requires some processing. And, last but not least, queries in TPCH appear to use a lot of filtering over concrete customer names.

It appears that it is worthwhile to have scanning performed only once for a string that is constant in the filtering operation.

Right now there's no such functionality in the server. Alexander Barkov said that it may be beneficial for server too.

The goal of this task is to implement that functionalitty for some (not all - for two or three collations, some simple like latin1_swedish_ci and some complex like utf8_ci) and perform benchmarks. I think it is possible to just copy out some code from strings directory of the server just to demonstrate what can be achieved. $acceptance criteria:$",0,0,0,0,0,0,0,18.1667,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
376,MCOL-4936,Task,MCOL,2021-12-03 02:43:43,,0,Disable bingloging for MCS tables running DML/bulk insertion(LDIF)  ,LOAD DATA IN FILE and DML are the only ways to ingest data into MCS tables in the cloud. Both LDIF and DML produces binlog entries for MCS tables. The binlogs themselves doesn't make any  sense in MCS cluster b/c of its distributed storage and also binlogs can take a lot of disk space in case of LDIF. The immediate solution is to disable binlogs for DML/bulk insertion for MCS tables.,,Disable bingloging for MCS tables running DML/bulk insertion(LDIF)   $end$ LOAD DATA IN FILE and DML are the only ways to ingest data into MCS tables in the cloud. Both LDIF and DML produces binlog entries for MCS tables. The binlogs themselves doesn't make any  sense in MCS cluster b/c of its distributed storage and also binlogs can take a lot of disk space in case of LDIF. The immediate solution is to disable binlogs for DML/bulk insertion for MCS tables. $acceptance criteria:$,,Roman,Roman,Blocker,21,,1,8,1,2,0,0,0,,0,850,8,0,0,2021-12-21 17:13:34,Disable bingloging for MCS tables running DML/bulk insertion(LDIF)  ,LOAD DATA IN FILE and DML are the only ways to ingest data into MCS tables in the cloud. Both LDIF and DML produces binlog entries for MCS tables. The binlogs themselves doesn't make any  sense in MCS cluster b/c of its distributed storage and also binlogs can take a lot of disk space in case of LDIF. The immediate solution is to disable binlogs for DML/bulk insertion for MCS tables.,,0,0,0,0,0.0,Disable bingloging for MCS tables running DML/bulk insertion(LDIF)   $end$ LOAD DATA IN FILE and DML are the only ways to ingest data into MCS tables in the cloud. Both LDIF and DML produces binlog entries for MCS tables. The binlogs themselves doesn't make any  sense in MCS cluster b/c of its distributed storage and also binlogs can take a lot of disk space in case of LDIF. The immediate solution is to disable binlogs for DML/bulk insertion for MCS tables. $acceptance criteria:$,0,0,0,0,0,0,1,446.483,56,13,0.232143,9,0.160714,6,0.107143,4,0.0714286,3,0.0535714
377,MCOL-4938,Task,MCOL,2021-12-06 20:22:11,,0,Disable/enable mariadb-columnstore systemd unit installing/removing CMAPI package,"One can start MCS directly calling systemctl start mariadb-columnstore or via CMAPI. When CMAPI is installed one must not start MCS directly using mariadb-columnstore unit b/c what it does interferes with CMAPI starting procedures. This might bring cluster into unusable state. Here is the simplest example, mariadb-columnstore unit starts mcs-workernode@1, mcs-dmlproc and mcs-ddlproc units on a secondary node where they don't belong.

CMAPI package must effectively disable mariadb-columnstore systemd unit when it is installed and re-enabled if needed when it is removed from the system. CMAPI/MCS upgrade procedures must be taken into account. ",,"Disable/enable mariadb-columnstore systemd unit installing/removing CMAPI package $end$ One can start MCS directly calling systemctl start mariadb-columnstore or via CMAPI. When CMAPI is installed one must not start MCS directly using mariadb-columnstore unit b/c what it does interferes with CMAPI starting procedures. This might bring cluster into unusable state. Here is the simplest example, mariadb-columnstore unit starts mcs-workernode@1, mcs-dmlproc and mcs-ddlproc units on a secondary node where they don't belong.

CMAPI package must effectively disable mariadb-columnstore systemd unit when it is installed and re-enabled if needed when it is removed from the system. CMAPI/MCS upgrade procedures must be taken into account.  $acceptance criteria:$",,Roman,Roman,Minor,5,,0,2,0,2,0,0,0,,0,850,2,0,0,2021-12-06 20:22:54,Disable/enable mariadb-columnstore systemd unit installing/removing CMAPI package,"One can start MCS directly calling systemctl start mariadb-columnstore or via CMAPI. When CMAPI is installed one must not start MCS directly using mariadb-columnstore unit b/c what it does interferes with CMAPI starting procedures. This might bring cluster into unusable state. Here is the simplest example, mariadb-columnstore unit starts mcs-workernode@1, mcs-dmlproc and mcs-ddlproc units on a secondary node where they don't belong.

CMAPI package must effectively disable mariadb-columnstore systemd unit when it is installed and re-enabled if needed when it is removed from the system. CMAPI/MCS upgrade procedures must be taken into account. ",,0,0,0,0,0.0,"Disable/enable mariadb-columnstore systemd unit installing/removing CMAPI package $end$ One can start MCS directly calling systemctl start mariadb-columnstore or via CMAPI. When CMAPI is installed one must not start MCS directly using mariadb-columnstore unit b/c what it does interferes with CMAPI starting procedures. This might bring cluster into unusable state. Here is the simplest example, mariadb-columnstore unit starts mcs-workernode@1, mcs-dmlproc and mcs-ddlproc units on a secondary node where they don't belong.

CMAPI package must effectively disable mariadb-columnstore systemd unit when it is installed and re-enabled if needed when it is removed from the system. CMAPI/MCS upgrade procedures must be taken into account.  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,57,13,0.22807,9,0.157895,6,0.105263,4,0.0701754,3,0.0526316
378,MCOL-4939,New Feature,MCOL,2021-12-06 20:29:04,,0,Add a method to disable failover facility in CMAPI. ,"There are known customer installations that don't use shared storage so failover mechanism might break such clusters.
There must be a knob in cmapi configuration file to disable failover facility if needed.

New 

Changes has been made:
* add application section with auto_failover = False parameter to default cmapi_server.conf
* failover now is *turned off by default* even if there are no ""application"" section or no auto_failover parameter exist in cmapi_server.conf
* failover has now three different logical states:
** turned off - no failover thread started. To turn it on set auto_failover=True in application section of cmapi_server.conf file of each node and restart cmapi.
** turned on and inactive - there are failover thread but it doesn't work. It becomes active automatically if nodes count >= 3
** turned on and active - there are an active failover thread and it is activated. Can be deactivated automatically if nodes_count < 3",,"Add a method to disable failover facility in CMAPI.  $end$ There are known customer installations that don't use shared storage so failover mechanism might break such clusters.
There must be a knob in cmapi configuration file to disable failover facility if needed.

New 

Changes has been made:
* add application section with auto_failover = False parameter to default cmapi_server.conf
* failover now is *turned off by default* even if there are no ""application"" section or no auto_failover parameter exist in cmapi_server.conf
* failover has now three different logical states:
** turned off - no failover thread started. To turn it on set auto_failover=True in application section of cmapi_server.conf file of each node and restart cmapi.
** turned on and inactive - there are failover thread but it doesn't work. It becomes active automatically if nodes count >= 3
** turned on and active - there are an active failover thread and it is activated. Can be deactivated automatically if nodes_count < 3 $acceptance criteria:$",,Roman,Roman,Major,43,,2,25,3,3,0,3,0,,0,850,25,0,0,2021-12-06 20:29:04,Add a method to disable failover facility in CMAPI. ,"There are known customer installations that don't use shared storage so failover mechanism might break such clusters.
There must be a knob in cmapi configuration file to disable failover facility if needed.",,0,3,0,120,2.72727,"Add a method to disable failover facility in CMAPI.  $end$ There are known customer installations that don't use shared storage so failover mechanism might break such clusters.
There must be a knob in cmapi configuration file to disable failover facility if needed. $acceptance criteria:$",3,1,1,1,1,1,1,0.0,58,13,0.224138,9,0.155172,6,0.103448,4,0.0689655,3,0.0517241
379,MCOL-4954,Task,MCOL,2021-12-27 23:42:33,,0,Add support for new DBRM socket bytestream format.,"DBRM socket bytestream protocol has been changed since  MCS 6.2.1 release.
Add support for both new and old protocols.
In new version add 4 extra bytes for long strings counting purpose to the header.
 ",,"Add support for new DBRM socket bytestream format. $end$ DBRM socket bytestream protocol has been changed since  MCS 6.2.1 release.
Add support for both new and old protocols.
In new version add 4 extra bytes for long strings counting purpose to the header.
  $acceptance criteria:$",,Alan Mologorsky,Alan Mologorsky,Blocker,23,,3,4,7,2,0,0,0,,0,850,4,0,0,2022-01-21 18:44:39,Add support for new DBRM socket bytestream format.,"DBRM socket bytestream protocol has been changed since  MCS 6.2.1 release.
Add support for both new and old protocols.
In new version add 4 extra bytes for long strings counting purpose to the header.
 ",,0,0,0,0,0.0,"Add support for new DBRM socket bytestream format. $end$ DBRM socket bytestream protocol has been changed since  MCS 6.2.1 release.
Add support for both new and old protocols.
In new version add 4 extra bytes for long strings counting purpose to the header.
  $acceptance criteria:$",0,0,0,0,0,0,1,595.033,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
380,MCOL-497,New Feature,MCOL,2017-01-10 21:08:46,,0,support ssl connection in cross engine joins,"Since we promote the use of ssl connections, we should also add a configuration to support ssl connections in cross engine joins. This might be useful if the remote server is truly remote.",,"support ssl connection in cross engine joins $end$ Since we promote the use of ssl connections, we should also add a configuration to support ssl connections in cross engine joins. This might be useful if the remote server is truly remote. $acceptance criteria:$",,David Thompson,David Thompson,Major,25,,0,5,0,18,0,0,0,,0,850,4,0,0,2018-01-02 14:27:51,support ssl connection in cross engine joins,"Since we promote the use of ssl connections, we should also add a configuration to support ssl connections in cross engine joins. This might be useful if the remote server is truly remote.",,0,0,0,0,0.0,"support ssl connection in cross engine joins $end$ Since we promote the use of ssl connections, we should also add a configuration to support ssl connections in cross engine joins. This might be useful if the remote server is truly remote. $acceptance criteria:$",0,0,0,0,0,0,1,8561.32,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
381,MCOL-4978,Task,MCOL,2022-01-31 13:05:36,,0,Add support of encrypted CEJ passwords to CMAPI,"In MCS 6.2.3 added feature for CEJ password encryption.
Need to support both versions: encrypted and raw CEJ passwords.",,"Add support of encrypted CEJ passwords to CMAPI $end$ In MCS 6.2.3 added feature for CEJ password encryption.
Need to support both versions: encrypted and raw CEJ passwords. $acceptance criteria:$",,Alan Mologorsky,Alan Mologorsky,Blocker,19,,0,2,0,1,0,2,0,,0,850,2,0,0,2022-02-15 17:12:53,Add support of encrypted CES passwords to CMAPI,"In MCS 6.2.3 added feature for CES password encryption.
Need to support both versions: encrypted and raw CES passwords.",,1,1,0,6,0.1,"Add support of encrypted CES passwords to CMAPI $end$ In MCS 6.2.3 added feature for CES password encryption.
Need to support both versions: encrypted and raw CES passwords. $acceptance criteria:$",2,1,1,0,0,0,1,364.117,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
382,MCOL-498,New Feature,MCOL,2017-01-10 21:11:56,,0,provide option to not preallocate disk on extents,A configuration should be provided to allow for not preallocating disk storage on extent creation. This would be useful for ssd storage where contiguous allocation is not important and this would also reduce wear slightly.,,provide option to not preallocate disk on extents $end$ A configuration should be provided to allow for not preallocating disk storage on extent creation. This would be useful for ssd storage where contiguous allocation is not important and this would also reduce wear slightly. $acceptance criteria:$,,David Thompson,David Thompson,Major,48,,1,14,1,23,0,0,0,,0,850,12,0,0,2018-03-05 17:28:27,provide option to not preallocate disk on extents,A configuration should be provided to allow for not preallocating disk storage on extent creation. This would be useful for ssd storage where contiguous allocation is not important and this would also reduce wear slightly.,,0,0,0,0,0.0,provide option to not preallocate disk on extents $end$ A configuration should be provided to allow for not preallocating disk storage on extent creation. This would be useful for ssd storage where contiguous allocation is not important and this would also reduce wear slightly. $acceptance criteria:$,0,0,0,0,0,0,1,10052.3,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
383,MCOL-5,Task,MCOL,2016-05-02 16:45:21,,0,Build for Ubuntu 16.04,,,Build for Ubuntu 16.04 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,23,,0,10,1,1,0,1,0,,0,850,10,0,0,2016-05-10 19:26:19,Build for Debian 16.04,,,1,0,0,2,0.142857,Build for Debian 16.04 $end$ $acceptance criteria:$,1,1,0,0,0,0,0,194.667,3,1,0.333333,0,0.0,0,0.0,0,0.0,0,0.0
384,MCOL-5001,New Feature,MCOL,2022-02-25 18:15:20,,0,Combine PrimProc and ExeMgr runtimes,"The eventual goals are:
- avoid network hop sending data from PP to a local EM
- combine PP and joblist thread pools in a single runtime to synchronously manage both pools(prerequisite for workload scheduler)

The phases are:
- clean-up EM runtime
- combine PP and EM starting EM thread in PP startup procedures
- clean-up systemd/CMAPI startup and maintenance code ",,"Combine PrimProc and ExeMgr runtimes $end$ The eventual goals are:
- avoid network hop sending data from PP to a local EM
- combine PP and joblist thread pools in a single runtime to synchronously manage both pools(prerequisite for workload scheduler)

The phases are:
- clean-up EM runtime
- combine PP and EM starting EM thread in PP startup procedures
- clean-up systemd/CMAPI startup and maintenance code  $acceptance criteria:$",,Roman,Roman,Major,9,,1,0,5,2,0,0,0,,0,850,0,0,0,2022-02-25 18:15:20,Combine PrimProc and ExeMgr runtimes,"The eventual goals are:
- avoid network hop sending data from PP to a local EM
- combine PP and joblist thread pools in a single runtime to synchronously manage both pools(prerequisite for workload scheduler)

The phases are:
- clean-up EM runtime
- combine PP and EM starting EM thread in PP startup procedures
- clean-up systemd/CMAPI startup and maintenance code ",,0,0,0,0,0.0,"Combine PrimProc and ExeMgr runtimes $end$ The eventual goals are:
- avoid network hop sending data from PP to a local EM
- combine PP and joblist thread pools in a single runtime to synchronously manage both pools(prerequisite for workload scheduler)

The phases are:
- clean-up EM runtime
- combine PP and EM starting EM thread in PP startup procedures
- clean-up systemd/CMAPI startup and maintenance code  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,59,14,0.237288,10,0.169492,7,0.118644,5,0.0847458,4,0.0677966
385,MCOL-5012,Task,MCOL,2022-03-09 13:26:52,,0,Drone - Docker build integration,"The suggestion is to provide a single  build pipeline where
- each dev  build contains latest of  CS code proper, CMAPI , dockerfile and other components
- similar situation for production builds
- the resulting dockerhub tags should be logically similar (""parallel"") to folders containing on prem zip files
 
We should review GIT repos  we use.

We could probably review how MaridDB  ES server docker is built and try a common approach

The sky group is working on  the way to create the Sky database from a shell script on any account and using any docker tags as parameters.
We could use this to run some of our validation  QA on the Sky system.
",,"Drone - Docker build integration $end$ The suggestion is to provide a single  build pipeline where
- each dev  build contains latest of  CS code proper, CMAPI , dockerfile and other components
- similar situation for production builds
- the resulting dockerhub tags should be logically similar (""parallel"") to folders containing on prem zip files
 
We should review GIT repos  we use.

We could probably review how MaridDB  ES server docker is built and try a common approach

The sky group is working on  the way to create the Sky database from a shell script on any account and using any docker tags as parameters.
We could use this to run some of our validation  QA on the Sky system.
 $acceptance criteria:$",,alexey vorovich,alexey vorovich,Major,15,,0,4,0,1,0,1,0,,0,850,0,0,0,2022-05-12 19:53:36,Docker build integration,"The suggestion is to provide a single  build pipeline where
- each dev  build contains latest of  CS code proper, CMAPI , dockerfile and other components
- similar situation for production builds
- the resulting dockerhub tags should be logically similar (""parallel"") to folders containing on prem zip files
 
We should review GIT repos  we use.

We could probably review how MaridDB  ES server docker is built and try a common approach

The sky group is working on  the way to create the Sky database from a shell script on any account and using any docker tags as parameters.
We could use this to run some of our validation  QA on the Sky system.
",,1,0,0,2,0.0166667,"Docker build integration $end$ The suggestion is to provide a single  build pipeline where
- each dev  build contains latest of  CS code proper, CMAPI , dockerfile and other components
- similar situation for production builds
- the resulting dockerhub tags should be logically similar (""parallel"") to folders containing on prem zip files
 
We should review GIT repos  we use.

We could probably review how MaridDB  ES server docker is built and try a common approach

The sky group is working on  the way to create the Sky database from a shell script on any account and using any docker tags as parameters.
We could use this to run some of our validation  QA on the Sky system.
 $acceptance criteria:$",1,1,0,0,0,0,0,1542.43,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
386,MCOL-5013,New Feature,MCOL,2022-03-09 21:12:26,,0,Support Load data from AWS  S3 :  UDF : columnstore_info.load_from_s3,"Columnstore should support a plain SQL syntax for loading data directly from s3 buckets.

Current implementation for 2208 is 

usage:

AWS
{code:sql}
set columnstore_s3_key='<s3_key>';
set columnstore_s3_secret='<s3_secret>';
set columnstore_s3_region='region';

CALL columnstore_info.load_from_s3(""<bucket>"", ""<file_name>"", ""<db_name>"", ""<table_name>"", ""<terminated_by>"",  ""<enclosed_by>"", ""<escaped_by>"");

EXAMPLE

CALL columnstore_info.load_from_s3(""s3://columnstore-test"", ""data1.csv"", ""d1"", ""t1"", "","", """", """" )
{code}

or for Google Storage
{code}
set columnstore_s3_key='GOOGXXXxxxx';
set columnstore_s3_secret='XXXXXXXXXX;
CALL columnstore_info.load_from_s3(""gs://columnstore-test"", ""test.tbl"", ""test"", ""gg"", ""|"", """", """");
{code}



last three params are the same as cpimport  -s , -E , -C
{code} terminated_by:  {code}is the delimiter between column values. mandatory
{code} enclosed_by:{code} Enclosed by character if field values are enclosed. optional . can be """" empty string
{code} escaped_by: {code} Escape character used in conjunction with 'enclosed by'.optional . can be """" empty string
                        character, or as part of NULL escape sequence ('\N');
                        default is '\'
enclosed_by and escaped_by can be set blank to use defaults

EXAMPLE

CALL columnstore_info.load_from_s3(""s3://avorovich2"", ""data1.csv"", ""d1"", ""t1"", "","", """", """" )

--------
future options maybe (not implemented in 2208)

{code:sql}
mariadb> LOAD DATA S3 FROM 's3://blah.blah/mydata.dat' into table xyz FIELDS TERMINATED BY '|';
{code}

This command should invoke the backend columnstore api and an s3 client to stream data directly from a remote source into cpimport.

This gives us a rapid data ingestion technique that is compatible with SkySQL and provides UAC _(Unlike our previous utility - mcsimport)_.",,"Support Load data from AWS  S3 :  UDF : columnstore_info.load_from_s3 $end$ Columnstore should support a plain SQL syntax for loading data directly from s3 buckets.

Current implementation for 2208 is 

usage:

AWS
{code:sql}
set columnstore_s3_key='<s3_key>';
set columnstore_s3_secret='<s3_secret>';
set columnstore_s3_region='region';

CALL columnstore_info.load_from_s3(""<bucket>"", ""<file_name>"", ""<db_name>"", ""<table_name>"", ""<terminated_by>"",  ""<enclosed_by>"", ""<escaped_by>"");

EXAMPLE

CALL columnstore_info.load_from_s3(""s3://columnstore-test"", ""data1.csv"", ""d1"", ""t1"", "","", """", """" )
{code}

or for Google Storage
{code}
set columnstore_s3_key='GOOGXXXxxxx';
set columnstore_s3_secret='XXXXXXXXXX;
CALL columnstore_info.load_from_s3(""gs://columnstore-test"", ""test.tbl"", ""test"", ""gg"", ""|"", """", """");
{code}



last three params are the same as cpimport  -s , -E , -C
{code} terminated_by:  {code}is the delimiter between column values. mandatory
{code} enclosed_by:{code} Enclosed by character if field values are enclosed. optional . can be """" empty string
{code} escaped_by: {code} Escape character used in conjunction with 'enclosed by'.optional . can be """" empty string
                        character, or as part of NULL escape sequence ('\N');
                        default is '\'
enclosed_by and escaped_by can be set blank to use defaults

EXAMPLE

CALL columnstore_info.load_from_s3(""s3://avorovich2"", ""data1.csv"", ""d1"", ""t1"", "","", """", """" )

--------
future options maybe (not implemented in 2208)

{code:sql}
mariadb> LOAD DATA S3 FROM 's3://blah.blah/mydata.dat' into table xyz FIELDS TERMINATED BY '|';
{code}

This command should invoke the backend columnstore api and an s3 client to stream data directly from a remote source into cpimport.

This gives us a rapid data ingestion technique that is compatible with SkySQL and provides UAC _(Unlike our previous utility - mcsimport)_. $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,75,,6,23,11,2,0,33,1,,0,850,19,5,0,2022-04-08 18:15:05,ColumnStore Streaming Import Feature,"Columnstore should support a plain SQL syntax for loading data directly from s3 buckets.

Something like:

{code:sql}
mariadb> LOAD DATA STREAM FROM 's3://blah.blah/mydata.dat' into table xyz FIELDS TERMINATED BY '|';
{code}

This command should invoke the backend columnstore api and an s3 client to stream data directly from a remote source into cpimport.

This gives us a rapid data ingestion technique that is compatible with SkySQL and provides UAC _(Unlike our previous utility - mcsimport)_.",,9,19,0,165,1.92683,"ColumnStore Streaming Import Feature $end$ Columnstore should support a plain SQL syntax for loading data directly from s3 buckets.

Something like:

{code:sql}
mariadb> LOAD DATA STREAM FROM 's3://blah.blah/mydata.dat' into table xyz FIELDS TERMINATED BY '|';
{code}

This command should invoke the backend columnstore api and an s3 client to stream data directly from a remote source into cpimport.

This gives us a rapid data ingestion technique that is compatible with SkySQL and provides UAC _(Unlike our previous utility - mcsimport)_. $acceptance criteria:$",28,1,1,1,1,1,1,717.033,10,1,0.1,0,0.0,0,0.0,0,0.0,0,0.0
387,MCOL-5021,New Feature,MCOL,2022-03-21 17:39:33,,0,Implement an auxiliary (hidden) column to improve DELETE performance.,"This task is to implement an auxiliary (AUX) column in ColumnStore to improve the performance of DELETE operations.

This will be accomplished by implementing a hidden 1-byte boolean column with 0 indicating the column value is disabled (deleted) and 1 indicating the column value is enabled (active). Idea is to improve the performance of DELETE operation by simply toggling the AUX column value from 1 to 0, instead of the current implementation which writes empty magic values for all columns in the table for all the impacted rows. Empty magics are the current method in ColumnStore to detect if a given row is deleted or not. So for a wide table, the current method would lead to significant performance slowdown when performing DELETEs.

Points to note:
-New tables created in the CS release with this feature are subject to DELETE optimization.
-Existing table w/o the aux column is not affected (no DELETE optimization). MCOL-5122 is created as an enhancement of this ticket to allow a user to create the AUX column for an existing table which was created with an older version of ColumnStore without the AUX column feature.

*Design Document for the feature:*

https://docs.google.com/document/d/1OAWYxlsfgMwPoeDtY6wZ9JcoCubFyodQ63cCZ9NjLCI/edit?usp=sharing",,"Implement an auxiliary (hidden) column to improve DELETE performance. $end$ This task is to implement an auxiliary (AUX) column in ColumnStore to improve the performance of DELETE operations.

This will be accomplished by implementing a hidden 1-byte boolean column with 0 indicating the column value is disabled (deleted) and 1 indicating the column value is enabled (active). Idea is to improve the performance of DELETE operation by simply toggling the AUX column value from 1 to 0, instead of the current implementation which writes empty magic values for all columns in the table for all the impacted rows. Empty magics are the current method in ColumnStore to detect if a given row is deleted or not. So for a wide table, the current method would lead to significant performance slowdown when performing DELETEs.

Points to note:
-New tables created in the CS release with this feature are subject to DELETE optimization.
-Existing table w/o the aux column is not affected (no DELETE optimization). MCOL-5122 is created as an enhancement of this ticket to allow a user to create the AUX column for an existing table which was created with an older version of ColumnStore without the AUX column feature.

*Design Document for the feature:*

https://docs.google.com/document/d/1OAWYxlsfgMwPoeDtY6wZ9JcoCubFyodQ63cCZ9NjLCI/edit?usp=sharing $acceptance criteria:$",,Gagan Goel,Gagan Goel,Major,26,,1,8,3,1,0,10,0,,0,850,8,0,0,2022-03-22 17:51:23,Implement an auxiliary (hidden) column to improve DELETE performance.,"This task is to implement an auxiliary (AUX) column in ColumnStore to improve the performance of DELETE operations.

This will be accomplished by implementing a hidden 1-byte boolean column with 0 indicating the column value is disabled (deleted) and 1 indicating the column value is enabled (active). Idea is to improve the performance of DELETE operation by simply toggling the AUX column value from 1 to 0, instead of the current implementation which writes empty magic values for all columns in the table for all the impacted rows. Empty magics are the current method in ColumnStore to detect if a given row is deleted or not. So for a wide table, the current method would lead to significant performance slowdown when performing DELETEs.",,0,10,0,72,0.533333,"Implement an auxiliary (hidden) column to improve DELETE performance. $end$ This task is to implement an auxiliary (AUX) column in ColumnStore to improve the performance of DELETE operations.

This will be accomplished by implementing a hidden 1-byte boolean column with 0 indicating the column value is disabled (deleted) and 1 indicating the column value is enabled (active). Idea is to improve the performance of DELETE operation by simply toggling the AUX column value from 1 to 0, instead of the current implementation which writes empty magic values for all columns in the table for all the impacted rows. Empty magics are the current method in ColumnStore to detect if a given row is deleted or not. So for a wide table, the current method would lead to significant performance slowdown when performing DELETEs. $acceptance criteria:$",10,1,1,1,1,1,1,24.1833,8,1,0.125,1,0.125,0,0.0,0,0.0,0,0.0
388,MCOL-5037,New Feature,MCOL,2022-03-31 10:24:15,,0,Up-merge EM Index into develop-6,This is up-merge for MCOL-4912,,Up-merge EM Index into develop-6 $end$ This is up-merge for MCOL-4912 $acceptance criteria:$,,Roman,Roman,Blocker,9,,1,1,1,1,0,1,0,,0,850,1,0,0,2022-03-31 10:24:57,Up-merge EM Index into develop and develop-6,This is up-merge for MCOL-4912,,1,0,0,2,0.133333,Up-merge EM Index into develop and develop-6 $end$ This is up-merge for MCOL-4912 $acceptance criteria:$,1,1,0,0,0,0,0,0.0,60,14,0.233333,10,0.166667,7,0.116667,5,0.0833333,4,0.0666667
389,MCOL-5044,New Feature,MCOL,2022-04-06 12:00:52,,0,Improve PP thread pool with a fair scheduler,"PP now uses PriorityThreadPool(PTP) that has 3 priority queues and supports Job re-scheduling(can happen if there is no space to service a primitive job request from EM).
PTP doesn't equally distribute computation resources b/w primitive jobs that belongs to different queries so that the variance of query timings is linear to number of simultaneous queries.
The suggestion is to implement a scheduling component that will try to fairly process mentioned primitive jobs.     ",,"Improve PP thread pool with a fair scheduler $end$ PP now uses PriorityThreadPool(PTP) that has 3 priority queues and supports Job re-scheduling(can happen if there is no space to service a primitive job request from EM).
PTP doesn't equally distribute computation resources b/w primitive jobs that belongs to different queries so that the variance of query timings is linear to number of simultaneous queries.
The suggestion is to implement a scheduling component that will try to fairly process mentioned primitive jobs.      $acceptance criteria:$",,Roman,Roman,Major,18,,2,5,4,1,0,0,0,,0,850,3,0,0,2022-05-27 16:42:31,Improve PP thread pool with a fair scheduler,"PP now uses PriorityThreadPool(PTP) that has 3 priority queues and supports Job re-scheduling(can happen if there is no space to service a primitive job request from EM).
PTP doesn't equally distribute computation resources b/w primitive jobs that belongs to different queries so that the variance of query timings is linear to number of simultaneous queries.
The suggestion is to implement a scheduling component that will try to fairly process mentioned primitive jobs.     ",,0,0,0,0,0.0,"Improve PP thread pool with a fair scheduler $end$ PP now uses PriorityThreadPool(PTP) that has 3 priority queues and supports Job re-scheduling(can happen if there is no space to service a primitive job request from EM).
PTP doesn't equally distribute computation resources b/w primitive jobs that belongs to different queries so that the variance of query timings is linear to number of simultaneous queries.
The suggestion is to implement a scheduling component that will try to fairly process mentioned primitive jobs.      $acceptance criteria:$",0,0,0,0,0,0,0,1228.68,61,15,0.245902,10,0.163934,7,0.114754,5,0.0819672,4,0.0655738
390,MCOL-505,New Feature,MCOL,2017-01-12 16:49:44,,0,Performance improvements to ExeMgr,This ticket is to track the performance improvement work I have made to ExeMgr,,Performance improvements to ExeMgr $end$ This ticket is to track the performance improvement work I have made to ExeMgr $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,2,1,1,0,0,0,,0,850,2,0,0,2017-01-12 16:49:44,Performance improvements to ExeMgr,This ticket is to track the performance improvement work I have made to ExeMgr,,0,0,0,0,0.0,Performance improvements to ExeMgr $end$ This ticket is to track the performance improvement work I have made to ExeMgr $acceptance criteria:$,0,0,0,0,0,0,0,0.0,9,2,0.222222,2,0.222222,1,0.111111,1,0.111111,1,0.111111
391,MCOL-506,Task,MCOL,2017-01-12 19:15:39,,0,merge server 10.1.21 release,,,merge server 10.1.21 release $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,10,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-01-12 19:15:39,merge server 10.1.21 release,,,0,0,0,0,0.0,merge server 10.1.21 release $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,11,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
392,MCOL-507,New Feature,MCOL,2017-01-16 12:32:59,,0,More performance improvements to ExeMgr,This ticket is to track the further performance improvement work I have made to ExeMgr,,More performance improvements to ExeMgr $end$ This ticket is to track the further performance improvement work I have made to ExeMgr $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Minor,5,,0,2,1,1,0,0,0,,0,850,2,0,0,2017-01-16 12:32:59,More performance improvements to ExeMgr,This ticket is to track the further performance improvement work I have made to ExeMgr,,0,0,0,0,0.0,More performance improvements to ExeMgr $end$ This ticket is to track the further performance improvement work I have made to ExeMgr $acceptance criteria:$,0,0,0,0,0,0,0,0.0,10,2,0.2,2,0.2,1,0.1,1,0.1,1,0.1
393,MCOL-5072,New Feature,MCOL,2022-05-05 19:01:35,MCOL-5111,0,dockerfile : Merge 3 GIT repo for skysql /enterprise/comminuty  ,"1. Would be good to have a single repo producing 3dockers (sky and enterprise/community)
",,"dockerfile : Merge 3 GIT repo for skysql /enterprise/comminuty   $end$ 1. Would be good to have a single repo producing 3dockers (sky and enterprise/community)
 $acceptance criteria:$",,alexey vorovich,alexey vorovich,Major,12,,0,0,0,4,0,2,0,,0,850,0,0,0,2022-05-13 14:02:38,dockerfile : review  if we can combine GIT repo for skysql and enterprise  ,"1. Would be good to have a single repo producing 2 dockers (sky and entreprise)
2. as step 2 would be even better to produce 1 docker ",,1,1,0,27,0.511628,"dockerfile : review  if we can combine GIT repo for skysql and enterprise   $end$ 1. Would be good to have a single repo producing 2 dockers (sky and entreprise)
2. as step 2 would be even better to produce 1 docker  $acceptance criteria:$",2,1,1,1,1,1,1,187.017,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
394,MCOL-5085,New Feature,MCOL,2022-05-13 18:21:15,MCOL-5075,0,dockerfile: parameterize CMAPI package location,,,dockerfile: parameterize CMAPI package location $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,1,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-05-13 18:21:15,dockerfile: parameterize CMAPI package location,,,0,0,0,0,0.0,dockerfile: parameterize CMAPI package location $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,2,2,1.0,1,0.5,1,0.5,1,0.5,1,0.5
395,MCOL-5089,New Feature,MCOL,2022-05-16 08:20:39,,0,Merge RBTree-based Extent Map with EM Index to remove scaleability slow-downs -develop5,"This is a final merge of the two features introduced to reduce ExtentMap operations slowdowns caused by a significant Extent Map.
In the end MCS operations, e.g. INSERT, bulk ingestion, SELECT must operate almost as fast as they did with an empty ExtentMap.

4 QA There are two aspects of this project:
- functionality(there must be no regression) so you need to test overal MCS functionality(pay attention to bulk ingestion running in different modes and rollbacks).
 - overall speedup when used with Extent Maps(> 50MB). There is compressed extent map image(BRM_saved_em.tar.bz2) from one of customers I will share that should be used for testing. One needs to replace empty EM file with the image and compare timings for a variety of operations: INSERT, bulk ingestion, rollback, deletes. Plz note that the previous stable MCS 5 version already contains the subset of the functionality. There is another important detail, namely with the current code Extent Map takes about 5 times more then previously in shared memory, e.g. 300 MB image will now takes 1.2GB for Extent Map + 400 MB for index. We will address the extent map size after this patch. ",,"Merge RBTree-based Extent Map with EM Index to remove scaleability slow-downs -develop5 $end$ This is a final merge of the two features introduced to reduce ExtentMap operations slowdowns caused by a significant Extent Map.
In the end MCS operations, e.g. INSERT, bulk ingestion, SELECT must operate almost as fast as they did with an empty ExtentMap.

4 QA There are two aspects of this project:
- functionality(there must be no regression) so you need to test overal MCS functionality(pay attention to bulk ingestion running in different modes and rollbacks).
 - overall speedup when used with Extent Maps(> 50MB). There is compressed extent map image(BRM_saved_em.tar.bz2) from one of customers I will share that should be used for testing. One needs to replace empty EM file with the image and compare timings for a variety of operations: INSERT, bulk ingestion, rollback, deletes. Plz note that the previous stable MCS 5 version already contains the subset of the functionality. There is another important detail, namely with the current code Extent Map takes about 5 times more then previously in shared memory, e.g. 300 MB image will now takes 1.2GB for Extent Map + 400 MB for index. We will address the extent map size after this patch.  $acceptance criteria:$",,Roman,Roman,Major,24,,1,6,4,1,0,3,0,,0,850,6,0,0,2022-05-16 08:20:39,Merge RBTree-based Extent Map with EM Index to remove scaleability slow-downs,"This is a final merge of the two features introduced to reduce ExtentMap operations slowdowns caused by a significant Extent Map.
In the end MCS operations, e.g. INSERT, bulk ingestion, SELECT must operate almost as fast as they did with an empty ExtentMap.",,1,2,0,149,2.61404,"Merge RBTree-based Extent Map with EM Index to remove scaleability slow-downs $end$ This is a final merge of the two features introduced to reduce ExtentMap operations slowdowns caused by a significant Extent Map.
In the end MCS operations, e.g. INSERT, bulk ingestion, SELECT must operate almost as fast as they did with an empty ExtentMap. $acceptance criteria:$",3,1,1,1,1,1,1,0.0,62,15,0.241935,10,0.16129,7,0.112903,5,0.0806452,4,0.0645161
396,MCOL-509,Task,MCOL,2017-01-17 23:13:37,,0,deploy buildbot for build and regression test automation,buildbot is the corporate standard for build automation within mariadb. We should migrate the builds to an instance of this and trigger builds from github. This should perform both build and regression tests for pull requests as well as merges.,,deploy buildbot for build and regression test automation $end$ buildbot is the corporate standard for build automation within mariadb. We should migrate the builds to an instance of this and trigger builds from github. This should perform both build and regression tests for pull requests as well as merges. $acceptance criteria:$,,David Thompson,David Thompson,Major,14,,0,5,1,13,0,0,0,,0,850,5,0,0,2017-01-17 23:14:13,deploy buildbot for build and regression test automation,buildbot is the corporate standard for build automation within mariadb. We should migrate the builds to an instance of this and trigger builds from github. This should perform both build and regression tests for pull requests as well as merges.,,0,0,0,0,0.0,deploy buildbot for build and regression test automation $end$ buildbot is the corporate standard for build automation within mariadb. We should migrate the builds to an instance of this and trigger builds from github. This should perform both build and regression tests for pull requests as well as merges. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,12,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
397,MCOL-5091,Task,MCOL,2022-05-16 08:23:20,,0,Up-merge RBTree-based EM into develop,,,Up-merge RBTree-based EM into develop $end$ $acceptance criteria:$,,Roman,Roman,Major,14,,1,0,3,1,0,1,0,,0,850,0,0,0,2022-05-16 08:23:20,Up-merge EMIndex + RBTree-based EM into develop,,,1,0,0,2,0.2,Up-merge EMIndex + RBTree-based EM into develop $end$ $acceptance criteria:$,1,1,0,0,0,0,0,0.0,63,16,0.253968,11,0.174603,8,0.126984,6,0.0952381,5,0.0793651
398,MCOL-5096,New Feature,MCOL,2022-05-20 17:30:31,,0,Drone - new build grid to support multiple server versions (CS +ES),,,Drone - new build grid to support multiple server versions (CS +ES) $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-05-20 17:30:31,Drone - new build grid to support multiple server versions (CS +ES),,,0,0,0,0,0.0,Drone - new build grid to support multiple server versions (CS +ES) $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,3,2,0.666667,1,0.333333,1,0.333333,1,0.333333,1,0.333333
399,MCOL-51,Task,MCOL,2016-05-11 17:17:37,,0,bug3783 Failure,"For bug3783, we have:
drop table if exists bug3783;
set max_length_for_sort_data = 4096;
create table bug3783 (id int, name varchar(1000))engine=infinidb;
insert into bug3783 values (1,'yionfsdayfeiwajg'),(2,'gretsuyhejkstj jkete');
select id, hex(name) from bug3783 order by 1,2;

The query fails for us but it didn't used to. However, the artificial setting of max_length_for_sort_data is triggering the problem. Nobody should do that. ",,"bug3783 Failure $end$ For bug3783, we have:
drop table if exists bug3783;
set max_length_for_sort_data = 4096;
create table bug3783 (id int, name varchar(1000))engine=infinidb;
insert into bug3783 values (1,'yionfsdayfeiwajg'),(2,'gretsuyhejkstj jkete');
select id, hex(name) from bug3783 order by 1,2;

The query fails for us but it didn't used to. However, the artificial setting of max_length_for_sort_data is triggering the problem. Nobody should do that.  $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,15,,0,13,0,1,0,0,0,,0,850,11,0,0,2016-05-11 17:21:30,bug3783 Failure,"For bug3783, we have:
drop table if exists bug3783;
set max_length_for_sort_data = 4096;
create table bug3783 (id int, name varchar(1000))engine=infinidb;
insert into bug3783 values (1,'yionfsdayfeiwajg'),(2,'gretsuyhejkstj jkete');
select id, hex(name) from bug3783 order by 1,2;

The query fails for us but it didn't used to. However, the artificial setting of max_length_for_sort_data is triggering the problem. Nobody should do that. ",,0,0,0,0,0.0,"bug3783 Failure $end$ For bug3783, we have:
drop table if exists bug3783;
set max_length_for_sort_data = 4096;
create table bug3783 (id int, name varchar(1000))engine=infinidb;
insert into bug3783 values (1,'yionfsdayfeiwajg'),(2,'gretsuyhejkstj jkete');
select id, hex(name) from bug3783 order by 1,2;

The query fails for us but it didn't used to. However, the artificial setting of max_length_for_sort_data is triggering the problem. Nobody should do that.  $acceptance criteria:$",0,0,0,0,0,0,0,0.05,15,5,0.333333,1,0.0666667,1,0.0666667,1,0.0666667,1,0.0666667
400,MCOL-510,Task,MCOL,2017-01-17 23:16:15,,0,create build for SUSE 12,,,create build for SUSE 12 $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,7,,0,4,1,3,0,0,0,,0,850,4,0,0,2017-01-17 23:16:15,create build for SUSE 12,,,0,0,0,0,0.0,create build for SUSE 12 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,13,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
401,MCOL-5104,New Feature,MCOL,2022-05-27 15:10:15,,0,"Statistic Functions improvement: STDDEV_SAMP, STDDEV_POP, VAR_POP, VAP_SAMP","Columnstore statistic functions {{STDDEV_SAMP, STDDEV_POP, VAR_POP, VAP_SAMP}}
use naive algorithm, that looses precision dramatically
see example

{code}
MariaDB [test]> create table t(a bigint) engine columnstore;
Query OK, 0 rows affected (0.344 sec)

MariaDB [test]> insert into t value (80000000001);
Query OK, 1 row affected (0.144 sec)

MariaDB [test]> insert into t value (80000000003);
Query OK, 1 row affected (0.076 sec)

MariaDB [test]> select std(a) from t;
+--------+
| std(a) |
+--------+
| 0.0000 |
+--------+
1 row in set (0.059 sec)

MariaDB [test]> create table tm(a bigint);
Query OK, 0 rows affected (0.010 sec)

MariaDB [test]> insert into tm value (80000000003);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> insert into tm value (80000000001);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> select std(a) from tm;
+--------+
| std(a) |
+--------+
| 1.0000 |
+--------+
1 row in set (0.001 sec)}}
{code}
Same problem we have in twin window functions. 
Because of this problem we have different behavior on ARM and x86, and broken tests also.
We should replace naive algorithm  with more stable like server does

Also Columnstore functions work improperly with decimals
{code}
MariaDB [test]> select * from tdec;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.045 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdec;
+----+------+
| q1 | std  |
+----+------+
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
+----+------+
10 rows in set (0.010 sec)
{code}

compare with server

{code}
MariaDB [test]> select * from tdecserv;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.000 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdecserv;
+----+------+
| q1 | std  |
+----+------+
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
+----+------+
10 rows in set (0.001 sec)
{code}

All of this should be fixed

 See
[Algorythms|https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance]",,"Statistic Functions improvement: STDDEV_SAMP, STDDEV_POP, VAR_POP, VAP_SAMP $end$ Columnstore statistic functions {{STDDEV_SAMP, STDDEV_POP, VAR_POP, VAP_SAMP}}
use naive algorithm, that looses precision dramatically
see example

{code}
MariaDB [test]> create table t(a bigint) engine columnstore;
Query OK, 0 rows affected (0.344 sec)

MariaDB [test]> insert into t value (80000000001);
Query OK, 1 row affected (0.144 sec)

MariaDB [test]> insert into t value (80000000003);
Query OK, 1 row affected (0.076 sec)

MariaDB [test]> select std(a) from t;
+--------+
| std(a) |
+--------+
| 0.0000 |
+--------+
1 row in set (0.059 sec)

MariaDB [test]> create table tm(a bigint);
Query OK, 0 rows affected (0.010 sec)

MariaDB [test]> insert into tm value (80000000003);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> insert into tm value (80000000001);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> select std(a) from tm;
+--------+
| std(a) |
+--------+
| 1.0000 |
+--------+
1 row in set (0.001 sec)}}
{code}
Same problem we have in twin window functions. 
Because of this problem we have different behavior on ARM and x86, and broken tests also.
We should replace naive algorithm  with more stable like server does

Also Columnstore functions work improperly with decimals
{code}
MariaDB [test]> select * from tdec;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.045 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdec;
+----+------+
| q1 | std  |
+----+------+
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
+----+------+
10 rows in set (0.010 sec)
{code}

compare with server

{code}
MariaDB [test]> select * from tdecserv;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.000 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdecserv;
+----+------+
| q1 | std  |
+----+------+
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
+----+------+
10 rows in set (0.001 sec)
{code}

All of this should be fixed

 See
[Algorythms|https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance] $acceptance criteria:$",,Leonid Fedorov,Leonid Fedorov,Major,19,,1,0,1,1,0,3,0,,0,850,0,0,0,2022-05-27 16:32:39,Statistic Functions improve,"Columnstore statistic functions {{STDDEV_SAMP, STDDEV_POP, VAR_POP, VAP_SAMP}}
use naive algorithm, that looses precision dramatically
see example

{code}
MariaDB [test]> create table t(a bigint) engine columnstore;
Query OK, 0 rows affected (0.344 sec)

MariaDB [test]> insert into t value (80000000001);
Query OK, 1 row affected (0.144 sec)

MariaDB [test]> insert into t value (80000000003);
Query OK, 1 row affected (0.076 sec)

MariaDB [test]> select std(a) from t;
+--------+
| std(a) |
+--------+
| 0.0000 |
+--------+
1 row in set (0.059 sec)

MariaDB [test]> create table tm(a bigint);
Query OK, 0 rows affected (0.010 sec)

MariaDB [test]> insert into tm value (80000000003);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> insert into tm value (80000000001);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> select std(a) from tm;
+--------+
| std(a) |
+--------+
| 1.0000 |
+--------+
1 row in set (0.001 sec)}}
{code}
Same problem we have in twin window functions. 
Because of this problem we have different behavior on ARM and x86, and broken tests also.
We should replace naive algorithm  with more stable like server does

Also Columnstore functions work improperly with decimals
{code}
MariaDB [test]> select * from tdec;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.045 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdec;
+----+------+
| q1 | std  |
+----+------+
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
+----+------+
10 rows in set (0.010 sec)
{code}

compare with server

{code}
MariaDB [test]> select * from tdecserv;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.000 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdecserv;
+----+------+
| q1 | std  |
+----+------+
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
+----+------+
10 rows in set (0.001 sec)
{code}

All of this should be fixed

 See
[Algorythms|https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance]",,3,0,0,6,0.0110375,"Statistic Functions improve $end$ Columnstore statistic functions {{STDDEV_SAMP, STDDEV_POP, VAR_POP, VAP_SAMP}}
use naive algorithm, that looses precision dramatically
see example

{code}
MariaDB [test]> create table t(a bigint) engine columnstore;
Query OK, 0 rows affected (0.344 sec)

MariaDB [test]> insert into t value (80000000001);
Query OK, 1 row affected (0.144 sec)

MariaDB [test]> insert into t value (80000000003);
Query OK, 1 row affected (0.076 sec)

MariaDB [test]> select std(a) from t;
+--------+
| std(a) |
+--------+
| 0.0000 |
+--------+
1 row in set (0.059 sec)

MariaDB [test]> create table tm(a bigint);
Query OK, 0 rows affected (0.010 sec)

MariaDB [test]> insert into tm value (80000000003);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> insert into tm value (80000000001);
Query OK, 1 row affected (0.001 sec)

MariaDB [test]> select std(a) from tm;
+--------+
| std(a) |
+--------+
| 1.0000 |
+--------+
1 row in set (0.001 sec)}}
{code}
Same problem we have in twin window functions. 
Because of this problem we have different behavior on ARM and x86, and broken tests also.
We should replace naive algorithm  with more stable like server does

Also Columnstore functions work improperly with decimals
{code}
MariaDB [test]> select * from tdec;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.045 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdec;
+----+------+
| q1 | std  |
+----+------+
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
| q1 | NULL |
+----+------+
10 rows in set (0.010 sec)
{code}

compare with server

{code}
MariaDB [test]> select * from tdecserv;
+--------------------------------------+
| col1                                 |
+--------------------------------------+
| 800000000000000000000000000000000001 |
| 800000000000000000000000000000000003 |
| 800000000000000000000000000000000005 |
| 800000000000000000000000000000000007 |
| 800000000000000000000000000000000011 |
| 800000000000000000000000000000000013 |
| 800000000000000000000000000000000015 |
| 800000000000000000000000000000000017 |
| 800000000000000000000000000000000019 |
| 800000000000000000000000000000000021 |
+--------------------------------------+
10 rows in set (0.000 sec)

MariaDB [test]> SELECT 'q1', floor(STD(col1) OVER ()) AS std FROM tdecserv;
+----+------+
| q1 | std  |
+----+------+
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
| q1 |    0 |
+----+------+
10 rows in set (0.001 sec)
{code}

All of this should be fixed

 See
[Algorythms|https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance] $acceptance criteria:$",3,1,1,0,0,0,1,1.36667,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
402,MCOL-5106,New Feature,MCOL,2022-05-27 17:11:28,,0,mcsRebuild EM - support multi-node + S3,"`mcsRebuildEM` supports 2 types of storage `shared local storage` and `S3`.
`shared local storage`.

*Shared local storage use case.*
_1. setup a cluster._
_2. login into mcs1 node._
docker exec -it mcs1 bash
_3. create a database, table and insert a row._

{code:sql}
[root@mcs1 /]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 17
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> create database temp;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use temp;
Database changed
MariaDB [temp]> create table t1 (a int, b varchar(200)) engine=columnstore;
Query OK, 0 rows affected (0.599 sec)

MariaDB [temp]> insert into t1 values (1, ""abcdf"");
Query OK, 1 row affected (0.151 sec)

MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | abcdf |
+------+-------+
1 row in set (0.033 sec)

MariaDB [temp]> exit
Bye
{code}

_4. Shutdown cluster._
{code:text}
[root@mcs1 /]# mcsShutdown 
{
  ""timestamp"": ""2022-06-13 09:17:17.040974"",
  ""mcs1"": {
    ""timestamp"": ""2022-06-13 09:17:26.433336""
  },
  ""mcs2"": {
    ""timestamp"": ""2022-06-13 09:17:29.753189""
  },
  ""mcs3"": {
    ""timestamp"": ""2022-06-13 09:17:33.071669""
  }
}
{code}

_5. Remove BRM_saves_em file. (Backup file of extentmap)_
{code:text}
[root@mcs1 /]# rm /var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em 
rm: remove regular file '/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em'? y
{code}

_6. Restore extent map from data files._

{code:text}
[root@mcs1 /]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Do you want to continue Y/N? y
Initialize system extents from the initial state
Collect extents for the DBRoot /var/lib/columnstore/data1
Collect extents for the DBRoot /var/lib/columnstore/data2
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/188.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3004, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/187.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3003, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 128
HWM is: 0
FileId is collected [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/189.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3005, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Collect extents for the DBRoot /var/lib/columnstore/data3
Build extent map with size 3
Extent is created, allocated size 4096 actual LBID 234496
For [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 238592
For [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 246784
For [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Setting a HWM for [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Completed.
{code}

_7. Start cluster._
{code:text}
[root@mcs1 /]# mcsStart 
{
  ""timestamp"": ""2022-06-13 09:22:42.826742""
}
{code}

_8. Check CS does work._
{code:text}
[root@mcs1 /]# mysql 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> use temp;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | abcdf |
+------+-------+
1 row in set (0.082 sec)

MariaDB [temp]> exit
Bye
{code}


*S3 use case.*
1, 2, 3, 4 steps are the same as for `shared local storage`.
5. Start `StorageManager` (access to S3)
{code:text}
[root@mcs1 ~]# StorageManager 
StorageManager[2634]: Using the config file found at /etc/columnstore/storagemanager.cnf
StorageManager[2634]: Cache/cache_size = 4294967296
StorageManager[2634]: S3Storage: S3 connectivity & permissions are OK
StorageManager[2634]: max_concurrent_downloads = 20
StorageManager[2634]: max_concurrent_downloads = 21
StorageManager[2634]: max_concurrent_uploads = 20
StorageManager[2634]: max_concurrent_uploads = 21
StorageManager[2634]: StorageManager started.
StorageManager[2634]: SessionManager waiting for sockets.
StorageManager[2633]: StorageManager main process has started
{code}

5. Remove `BRM_saves_em`, you can do it by hand on S3 bucket or by running tool, it will ask you to delete this file if this file exists.
6. Run `mcsRebuildEm`

{code:text}
[root@mcs1 ~]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Do you want to continue Y/N? y
/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em file exists. 
Do you want to delete this file Y/N? y
StorageManager[2634]: Ownership: taking ownership of data1
Initialize system extents from the initial state
Collect extents for the DBRoot /var/lib/columnstore/data1
Collect extents for the DBRoot /var/lib/columnstore/data2
StorageManager[2634]: Ownership: taking ownership of data2
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/188.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3004, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/187.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3003, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 128
HWM is: 0
FileId is collected [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/189.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3005, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Collect extents for the DBRoot /var/lib/columnstore/data3
Build extent map with size 3
Extent is created, allocated size 4096 actual LBID 234496
For [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 238592
For [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 246784
For [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Setting a HWM for [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Completed.
{code}

7. Shutdown `StorageManager`.

{code:text}
[root@mcs1 ~]# ps -aux 
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.0   4376   840 ?        Ss   09:35   0:00 /usr/bin/tini -- docker-entrypoint.sh start-se
root           7  0.0  0.0  14852  3276 ?        S    09:35   0:00 /bin/bash /usr/bin/docker-entrypoint.sh start-
root          12  0.0  0.0 197260  2368 ?        Ssl  09:35   0:00 rsyslogd
root          14  0.0  0.0  14852  3248 ?        S    09:35   0:00 bash /usr/bin/start-services
root         930  0.4  0.1 1503624 59888 ?       Sl   09:36   0:02 /usr/share/columnstore/cmapi/python/bin/python
root        1014  0.0  0.0  15116  3472 ?        S    09:36   0:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/li
mysql       1178  0.1  0.3 1692432 108224 ?      Sl   09:36   0:00 /usr/sbin/mariadbd --basedir=/usr --datadir=/v
root        1226  0.0  0.0 111688  5496 ?        Sl   09:36   0:00 monit -I
root        2410  0.0  0.0  15116  3764 pts/0    Ss   09:38   0:00 bash
root        2634  0.1  0.0 2863456 28436 pts/0   Sl   09:42   0:00 StorageManager
root        2720  0.0  0.0  50544  3788 pts/0    R+   09:45   0:00 ps -aux
[root@mcs1 ~]# kill -s 15 2634
[root@mcs1 ~]# StorageManager[2634]: SessionManager Caught Signal 15
StorageManager[2634]: Shutdown StorageManager...
StorageManager[2634]: Ownership: releasing ownership of data1
StorageManager[2634]: Ownership: releasing ownership of data2
StorageManager[2634]: StorageManager Shutdown Complete.
{code}

8. Start cluster.

{code:text}
[root@mcs1 ~]# mcsStart 
{
  ""timestamp"": ""2022-06-13 09:47:28.612874""
}
{code}

9. Check that CS works.

{code:text}
[root@mcs1 ~]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> use temp;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | fsddf |
+------+-------+
1 row in set (0.071 sec)

{code}

",,"mcsRebuild EM - support multi-node + S3 $end$ `mcsRebuildEM` supports 2 types of storage `shared local storage` and `S3`.
`shared local storage`.

*Shared local storage use case.*
_1. setup a cluster._
_2. login into mcs1 node._
docker exec -it mcs1 bash
_3. create a database, table and insert a row._

{code:sql}
[root@mcs1 /]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 17
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> create database temp;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use temp;
Database changed
MariaDB [temp]> create table t1 (a int, b varchar(200)) engine=columnstore;
Query OK, 0 rows affected (0.599 sec)

MariaDB [temp]> insert into t1 values (1, ""abcdf"");
Query OK, 1 row affected (0.151 sec)

MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | abcdf |
+------+-------+
1 row in set (0.033 sec)

MariaDB [temp]> exit
Bye
{code}

_4. Shutdown cluster._
{code:text}
[root@mcs1 /]# mcsShutdown 
{
  ""timestamp"": ""2022-06-13 09:17:17.040974"",
  ""mcs1"": {
    ""timestamp"": ""2022-06-13 09:17:26.433336""
  },
  ""mcs2"": {
    ""timestamp"": ""2022-06-13 09:17:29.753189""
  },
  ""mcs3"": {
    ""timestamp"": ""2022-06-13 09:17:33.071669""
  }
}
{code}

_5. Remove BRM_saves_em file. (Backup file of extentmap)_
{code:text}
[root@mcs1 /]# rm /var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em 
rm: remove regular file '/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em'? y
{code}

_6. Restore extent map from data files._

{code:text}
[root@mcs1 /]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Do you want to continue Y/N? y
Initialize system extents from the initial state
Collect extents for the DBRoot /var/lib/columnstore/data1
Collect extents for the DBRoot /var/lib/columnstore/data2
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/188.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3004, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/187.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3003, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 128
HWM is: 0
FileId is collected [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/189.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3005, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Collect extents for the DBRoot /var/lib/columnstore/data3
Build extent map with size 3
Extent is created, allocated size 4096 actual LBID 234496
For [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 238592
For [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 246784
For [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Setting a HWM for [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Completed.
{code}

_7. Start cluster._
{code:text}
[root@mcs1 /]# mcsStart 
{
  ""timestamp"": ""2022-06-13 09:22:42.826742""
}
{code}

_8. Check CS does work._
{code:text}
[root@mcs1 /]# mysql 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> use temp;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | abcdf |
+------+-------+
1 row in set (0.082 sec)

MariaDB [temp]> exit
Bye
{code}


*S3 use case.*
1, 2, 3, 4 steps are the same as for `shared local storage`.
5. Start `StorageManager` (access to S3)
{code:text}
[root@mcs1 ~]# StorageManager 
StorageManager[2634]: Using the config file found at /etc/columnstore/storagemanager.cnf
StorageManager[2634]: Cache/cache_size = 4294967296
StorageManager[2634]: S3Storage: S3 connectivity & permissions are OK
StorageManager[2634]: max_concurrent_downloads = 20
StorageManager[2634]: max_concurrent_downloads = 21
StorageManager[2634]: max_concurrent_uploads = 20
StorageManager[2634]: max_concurrent_uploads = 21
StorageManager[2634]: StorageManager started.
StorageManager[2634]: SessionManager waiting for sockets.
StorageManager[2633]: StorageManager main process has started
{code}

5. Remove `BRM_saves_em`, you can do it by hand on S3 bucket or by running tool, it will ask you to delete this file if this file exists.
6. Run `mcsRebuildEm`

{code:text}
[root@mcs1 ~]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Do you want to continue Y/N? y
/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em file exists. 
Do you want to delete this file Y/N? y
StorageManager[2634]: Ownership: taking ownership of data1
Initialize system extents from the initial state
Collect extents for the DBRoot /var/lib/columnstore/data1
Collect extents for the DBRoot /var/lib/columnstore/data2
StorageManager[2634]: Ownership: taking ownership of data2
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/188.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3004, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/187.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3003, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 128
HWM is: 0
FileId is collected [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Processing file: /var/lib/columnstore/data2/000.dir/000.dir/011.dir/189.dir/000.dir/FILE000.cdf
fileName2Oid:  [OID: 3005, partition: 0, segment: 0] 
Searching for HWM... 
Block count: 256
HWM is: 0
FileId is collected [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Collect extents for the DBRoot /var/lib/columnstore/data3
Build extent map with size 3
Extent is created, allocated size 4096 actual LBID 234496
For [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3003, partition: 0, segment: 0, col width: 4, lbid:234496, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 238592
For [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Setting a HWM for [OID: 3004, partition: 0, segment: 0, col width: 8, lbid:238592, hwm: 0, isDict: 0]
Extent is created, allocated size 8192 actual LBID 246784
For [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Setting a HWM for [OID: 3005, partition: 0, segment: 0, col width: 8, lbid:246784, hwm: 0, isDict: 1]
Completed.
{code}

7. Shutdown `StorageManager`.

{code:text}
[root@mcs1 ~]# ps -aux 
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.0   4376   840 ?        Ss   09:35   0:00 /usr/bin/tini -- docker-entrypoint.sh start-se
root           7  0.0  0.0  14852  3276 ?        S    09:35   0:00 /bin/bash /usr/bin/docker-entrypoint.sh start-
root          12  0.0  0.0 197260  2368 ?        Ssl  09:35   0:00 rsyslogd
root          14  0.0  0.0  14852  3248 ?        S    09:35   0:00 bash /usr/bin/start-services
root         930  0.4  0.1 1503624 59888 ?       Sl   09:36   0:02 /usr/share/columnstore/cmapi/python/bin/python
root        1014  0.0  0.0  15116  3472 ?        S    09:36   0:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/li
mysql       1178  0.1  0.3 1692432 108224 ?      Sl   09:36   0:00 /usr/sbin/mariadbd --basedir=/usr --datadir=/v
root        1226  0.0  0.0 111688  5496 ?        Sl   09:36   0:00 monit -I
root        2410  0.0  0.0  15116  3764 pts/0    Ss   09:38   0:00 bash
root        2634  0.1  0.0 2863456 28436 pts/0   Sl   09:42   0:00 StorageManager
root        2720  0.0  0.0  50544  3788 pts/0    R+   09:45   0:00 ps -aux
[root@mcs1 ~]# kill -s 15 2634
[root@mcs1 ~]# StorageManager[2634]: SessionManager Caught Signal 15
StorageManager[2634]: Shutdown StorageManager...
StorageManager[2634]: Ownership: releasing ownership of data1
StorageManager[2634]: Ownership: releasing ownership of data2
StorageManager[2634]: StorageManager Shutdown Complete.
{code}

8. Start cluster.

{code:text}
[root@mcs1 ~]# mcsStart 
{
  ""timestamp"": ""2022-06-13 09:47:28.612874""
}
{code}

9. Check that CS works.

{code:text}
[root@mcs1 ~]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> use temp;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | fsddf |
+------+-------+
1 row in set (0.071 sec)

{code}

 $acceptance criteria:$",,alexey vorovich,Denis Khalikov,Major,9,,0,3,1,1,0,2,0,,0,850,3,0,0,2022-05-31 15:40:07,mcsRebuild EM - support multi-node + S3,"`mcsRebuildEM` supports 2 types of storage `shared local storage` and `S3`.
`shared local storage`.

*Shared local storage use case.*
1. setup a cluster.
2. login into mcs1 node.
docker exec -it mcs1 bash
3. create a database, table and insert a row.
[root@mcs1 /]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 17
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> create database temp;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use temp;
Database changed
MariaDB [temp]> create table t1 (a int, b varchar(200)) engine=columnstore;
Query OK, 0 rows affected (0.599 sec)

MariaDB [temp]> insert into t1 values (1, ""abcdf"");
Query OK, 1 row affected (0.151 sec)

MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | abcdf |
+------+-------+
1 row in set (0.033 sec)

MariaDB [temp]> exit
Bye
",,0,2,0,1219,6.73889,"mcsRebuild EM - support multi-node + S3 $end$ `mcsRebuildEM` supports 2 types of storage `shared local storage` and `S3`.
`shared local storage`.

*Shared local storage use case.*
1. setup a cluster.
2. login into mcs1 node.
docker exec -it mcs1 bash
3. create a database, table and insert a row.
[root@mcs1 /]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 17
Server version: 10.6.8-4-MariaDB-log MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> create database temp;
Query OK, 1 row affected (0.000 sec)

MariaDB [(none)]> use temp;
Database changed
MariaDB [temp]> create table t1 (a int, b varchar(200)) engine=columnstore;
Query OK, 0 rows affected (0.599 sec)

MariaDB [temp]> insert into t1 values (1, ""abcdf"");
Query OK, 1 row affected (0.151 sec)

MariaDB [temp]> select * from t1;
+------+-------+
| a    | b     |
+------+-------+
|    1 | abcdf |
+------+-------+
1 row in set (0.033 sec)

MariaDB [temp]> exit
Bye
 $acceptance criteria:$",2,1,1,1,1,1,1,94.4667,4,2,0.5,1,0.25,1,0.25,1,0.25,1,0.25
403,MCOL-5109,Task,MCOL,2022-05-31 15:27:00,,0,Make ServicePrimProc a singleton available in ExeMgr facilities,"There is a class called ServicePrimProc(primitiveserver.cpp).
This class should contain a pointer to thread pools used by PrimProc, namely two PriorityThreadPools: procPoolPtr and OOBPool to facilitate the current workload calculation.
To enable information exchange b/w EM and PP that co-exists in a common runtime one needs to make a global pointer to the ServicePrimProc instances used.",,"Make ServicePrimProc a singleton available in ExeMgr facilities $end$ There is a class called ServicePrimProc(primitiveserver.cpp).
This class should contain a pointer to thread pools used by PrimProc, namely two PriorityThreadPools: procPoolPtr and OOBPool to facilitate the current workload calculation.
To enable information exchange b/w EM and PP that co-exists in a common runtime one needs to make a global pointer to the ServicePrimProc instances used. $acceptance criteria:$",,Roman,Roman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-05-31 15:27:00,Make ServicePrimProc a singleton available in ExeMgr facilities,"There is a class called ServicePrimProc(primitiveserver.cpp).
This class should contain a pointer to thread pools used by PrimProc, namely two PriorityThreadPools: procPoolPtr and OOBPool to facilitate the current workload calculation.
To enable information exchange b/w EM and PP that co-exists in a common runtime one needs to make a global pointer to the ServicePrimProc instances used.",,0,0,0,0,0.0,"Make ServicePrimProc a singleton available in ExeMgr facilities $end$ There is a class called ServicePrimProc(primitiveserver.cpp).
This class should contain a pointer to thread pools used by PrimProc, namely two PriorityThreadPools: procPoolPtr and OOBPool to facilitate the current workload calculation.
To enable information exchange b/w EM and PP that co-exists in a common runtime one needs to make a global pointer to the ServicePrimProc instances used. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,64,17,0.265625,11,0.171875,8,0.125,6,0.09375,5,0.078125
404,MCOL-511,New Feature,MCOL,2017-01-18 01:18:32,,0,native write data api,Support invocation of bulk data insert / cpimport via an API that can be invoked using multiple programming languages and ETL tools. This avoids the need to write a local file and invoke cpimport on a pm server.,,native write data api $end$ Support invocation of bulk data insert / cpimport via an API that can be invoked using multiple programming languages and ETL tools. This avoids the need to write a local file and invoke cpimport on a pm server. $acceptance criteria:$,,David Thompson,David Thompson,Major,21,,0,0,0,11,0,0,6,,0,850,0,0,0,2017-01-18 01:18:32,native write data api,Support invocation of bulk data insert / cpimport via an API that can be invoked using multiple programming languages and ETL tools. This avoids the need to write a local file and invoke cpimport on a pm server.,,0,0,0,0,0.0,native write data api $end$ Support invocation of bulk data insert / cpimport via an API that can be invoked using multiple programming languages and ETL tools. This avoids the need to write a local file and invoke cpimport on a pm server. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,14,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
405,MCOL-5112,Task,MCOL,2022-06-02 21:17:34,MCOL-5111,0,docker:review the need for mcs_process and monit. possibly remove,,,docker:review the need for mcs_process and monit. possibly remove $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-06-02 21:18:41,docker:review the need for mcs_process and monit. possibly remove,,,0,0,0,0,0.0,docker:review the need for mcs_process and monit. possibly remove $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,5,3,0.6,2,0.4,2,0.4,2,0.4,2,0.4
406,MCOL-5113,Task,MCOL,2022-06-02 21:20:02,MCOL-5111,0,docker: make provision  standard  for both docker-compose and sky,"We do have reports of sporadic readonly cluster with CS in Sky. I suggest we review startup

1. for docker-compose 
*docker-compose up -d && docker exec -it mcs1 provision*
here we wait for all nodes to start and then execute cluster demo on mcs1
this is pretty simple   https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/master/scripts/provision

2. for sky this is very different .  look at the logic https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/12de7a737b47dc14c09321401c996fdce3c01ebe/scripts/skysql-specific-startup.sh#L80
it seems that SKY  is using quite a complex logic and the timing is not clear.

Could we switch to the provision model where provision  script allows to pass in the array of hosts

-operator start  N nodes
-each node completes starting  server and CMAPI  server
-operator calls cluster_demo on node 1  and  passes array of nodes . The cluster demo completes the startup

the pod shutdown should be handled unirformly. Ideally a new deprovision script that Todd provides and operator invokes at end 
",,"docker: make provision  standard  for both docker-compose and sky $end$ We do have reports of sporadic readonly cluster with CS in Sky. I suggest we review startup

1. for docker-compose 
*docker-compose up -d && docker exec -it mcs1 provision*
here we wait for all nodes to start and then execute cluster demo on mcs1
this is pretty simple   https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/master/scripts/provision

2. for sky this is very different .  look at the logic https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/12de7a737b47dc14c09321401c996fdce3c01ebe/scripts/skysql-specific-startup.sh#L80
it seems that SKY  is using quite a complex logic and the timing is not clear.

Could we switch to the provision model where provision  script allows to pass in the array of hosts

-operator start  N nodes
-each node completes starting  server and CMAPI  server
-operator calls cluster_demo on node 1  and  passes array of nodes . The cluster demo completes the startup

the pod shutdown should be handled unirformly. Ideally a new deprovision script that Todd provides and operator invokes at end 
 $acceptance criteria:$",,alexey vorovich,alexey vorovich,Minor,14,,0,1,0,2,0,6,0,,0,850,1,0,0,2022-06-02 21:20:02,docker:review cluster_demo VS skysql-specific-startup.sh. ,"review timing 
1. for docker-compose 
docker-compose up -d && docker exec -it mcs1 cluster-demo
here we wait for all nodes to start and then execute cluster demo on mcs1
this is pretty simple   https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/master/scripts/cluster-demo

2. for sky this is very different .  look at the logic https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/12de7a737b47dc14c09321401c996fdce3c01ebe/scripts/skysql-specific-startup.sh#L80
it seems that SKY  is using quite a complex logic and the timing is not clear.

Could we switch to the cluster-demo model.

-cluster_demo script allows to p[ass in the array of hosts
-operator start  N nodes and calls cluster_demo on node 1  and  passes array of nodes 
",,3,3,0,80,0.666667,"docker:review cluster_demo VS skysql-specific-startup.sh.  $end$ review timing 
1. for docker-compose 
docker-compose up -d && docker exec -it mcs1 cluster-demo
here we wait for all nodes to start and then execute cluster demo on mcs1
this is pretty simple   https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/master/scripts/cluster-demo

2. for sky this is very different .  look at the logic https://github.com/mariadb-corporation/mariadb-skysql-columnstore-docker/blob/12de7a737b47dc14c09321401c996fdce3c01ebe/scripts/skysql-specific-startup.sh#L80
it seems that SKY  is using quite a complex logic and the timing is not clear.

Could we switch to the cluster-demo model.

-cluster_demo script allows to p[ass in the array of hosts
-operator start  N nodes and calls cluster_demo on node 1  and  passes array of nodes 
 $acceptance criteria:$",6,1,1,1,1,1,1,0.0,6,3,0.5,2,0.333333,2,0.333333,2,0.333333,2,0.333333
407,MCOL-5126,New Feature,MCOL,2022-06-09 15:47:17,,0,docker: columnstore-init should skip exemgr in develop,,,docker: columnstore-init should skip exemgr in develop $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Blocker,6,,0,2,0,1,0,2,0,,0,850,2,0,0,2022-06-09 15:47:17,columnstore-init should skip execmngr in develop,,,2,0,0,3,0.222222,columnstore-init should skip execmngr in develop $end$ $acceptance criteria:$,2,1,0,0,0,0,0,0.0,7,4,0.571429,3,0.428571,3,0.428571,3,0.428571,3,0.428571
408,MCOL-513,Task,MCOL,2017-01-18 01:23:00,,0,analyze and implement thread pools and memory buffers for performance optimization,Analyze code and implement thread pools and memory buffers where this will benefit query or write performance.,,analyze and implement thread pools and memory buffers for performance optimization $end$ Analyze code and implement thread pools and memory buffers where this will benefit query or write performance. $acceptance criteria:$,,David Thompson,David Thompson,Major,15,,0,4,0,9,0,0,0,,0,850,4,0,0,2017-01-18 01:23:00,analyze and implement thread pools and memory buffers for performance optimization,Analyze code and implement thread pools and memory buffers where this will benefit query or write performance.,,0,0,0,0,0.0,analyze and implement thread pools and memory buffers for performance optimization $end$ Analyze code and implement thread pools and memory buffers where this will benefit query or write performance. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,15,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
409,MCOL-5131,New Feature,MCOL,2022-06-13 09:54:10,,0,mcsRebuildEM - add support to calculate HWM for system cat. files.,"System cat. files are not compressed for some reason, that means, those files do not have a special header with `lbid`, `colwidth` and so on. We restore extents for this files from binary blob (initial state) and HWM is not calculating for them because `ChunkManager` does not have an API to read not compressed files block by block.
For big data bases - system files will grow as well and HWM will be greater than initial state, this should be supported.",,"mcsRebuildEM - add support to calculate HWM for system cat. files. $end$ System cat. files are not compressed for some reason, that means, those files do not have a special header with `lbid`, `colwidth` and so on. We restore extents for this files from binary blob (initial state) and HWM is not calculating for them because `ChunkManager` does not have an API to read not compressed files block by block.
For big data bases - system files will grow as well and HWM will be greater than initial state, this should be supported. $acceptance criteria:$",,Denis Khalikov,Denis Khalikov,Minor,16,,0,2,1,1,0,0,0,,0,850,2,0,0,2022-06-13 09:59:36,mcsRebuildEM - add support to calculate HWM for system cat. files.,"System cat. files are not compressed for some reason, that means, those files do not have a special header with `lbid`, `colwidth` and so on. We restore extents for this files from binary blob (initial state) and HWM is not calculating for them because `ChunkManager` does not have an API to read not compressed files block by block.
For big data bases - system files will grow as well and HWM will be greater than initial state, this should be supported.",,0,0,0,0,0.0,"mcsRebuildEM - add support to calculate HWM for system cat. files. $end$ System cat. files are not compressed for some reason, that means, those files do not have a special header with `lbid`, `colwidth` and so on. We restore extents for this files from binary blob (initial state) and HWM is not calculating for them because `ChunkManager` does not have an API to read not compressed files block by block.
For big data bases - system files will grow as well and HWM will be greater than initial state, this should be supported. $acceptance criteria:$",0,0,0,0,0,0,0,0.0833333,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
410,MCOL-5138,New Feature,MCOL,2022-06-15 15:10:15,,0,Cmapi should skip exemgr for engine builds from develop.,ExeMgr will be removed from develop. So proper mcs processes handling shoud be implemented.,,Cmapi should skip exemgr for engine builds from develop. $end$ ExeMgr will be removed from develop. So proper mcs processes handling shoud be implemented. $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,19,,1,3,1,1,0,2,0,,0,850,3,1,0,2022-06-17 15:57:59,Cmapi should skip exemgr for engine builds from develop (15 files ),ExeMgr will be removed from develop. So proper mcs processes handling shoud be implemented.,,1,0,0,5,0.137931,Cmapi should skip exemgr for engine builds from develop (15 files ) $end$ ExeMgr will be removed from develop. So proper mcs processes handling shoud be implemented. $acceptance criteria:$,1,1,1,0,0,0,1,48.7833,8,5,0.625,3,0.375,3,0.375,3,0.375,3,0.375
411,MCOL-514,Task,MCOL,2017-01-18 01:25:39,,0,analyze and create task list for 10.2 server merge / window function consolidation,,,analyze and create task list for 10.2 server merge / window function consolidation $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,11,,0,1,1,8,0,0,0,,0,850,1,0,0,2017-01-18 01:25:39,analyze and create task list for 10.2 server merge / window function consolidation,,,0,0,0,0,0.0,analyze and create task list for 10.2 server merge / window function consolidation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,16,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
412,MCOL-5140,New Feature,MCOL,2022-06-16 14:25:52,,0,Improve min/max to use vectorized updates (AMD+ ARM),"The code in primitives/linux-port/column.cpp:vectorizedFiltering suffers calculating min/max values for a block in a scalar manner that takes place in vectWriteRIDValues/vectWriteColValues. According with my observations we can reduce the timings 20% down removing this deficiency. 
Tested with 10**9 int64 records and a simple query 'SELECT c1 FROM t1 WHERE c1 = 5 OR c1 = 10;  ",,"Improve min/max to use vectorized updates (AMD+ ARM) $end$ The code in primitives/linux-port/column.cpp:vectorizedFiltering suffers calculating min/max values for a block in a scalar manner that takes place in vectWriteRIDValues/vectWriteColValues. According with my observations we can reduce the timings 20% down removing this deficiency. 
Tested with 10**9 int64 records and a simple query 'SELECT c1 FROM t1 WHERE c1 = 5 OR c1 = 10;   $acceptance criteria:$",,Roman,Roman,Major,13,,0,3,0,1,0,2,0,,0,850,3,0,0,2022-06-21 15:43:32,SIMD code suffers from scalar MinMax calculation,"The code in primitives/linux-port/column.cpp:vectorizedFiltering suffers calculating min/max values for a block in a scalar manner that takes place in vectWriteRIDValues/vectWriteColValues. According with my observations we can reduce the timings 20% down removing this deficiency. 
Tested with 10**9 int64 records and a simple query 'SELECT c1 FROM t1 WHERE c1 = 5 OR c1 = 10;  ",,2,0,0,15,0.123077,"SIMD code suffers from scalar MinMax calculation $end$ The code in primitives/linux-port/column.cpp:vectorizedFiltering suffers calculating min/max values for a block in a scalar manner that takes place in vectWriteRIDValues/vectWriteColValues. According with my observations we can reduce the timings 20% down removing this deficiency. 
Tested with 10**9 int64 records and a simple query 'SELECT c1 FROM t1 WHERE c1 = 5 OR c1 = 10;   $acceptance criteria:$",2,1,1,1,1,0,1,121.283,65,17,0.261538,11,0.169231,8,0.123077,6,0.0923077,5,0.0769231
413,MCOL-5152,New Feature,MCOL,2022-06-30 16:39:39,,0,PP now puts data directly into EM input queue bypassing network hop ,"The local interaction b/w PP and EM now goes over a network socket and in case of a single-node installation the socket is not even compressed. This imposes extra latency working with a single-node.
This feature bypasses network hop for ByteStreams generated by BATCH_PRIMITIVE_RUN primitive jobs.
",,"PP now puts data directly into EM input queue bypassing network hop  $end$ The local interaction b/w PP and EM now goes over a network socket and in case of a single-node installation the socket is not even compressed. This imposes extra latency working with a single-node.
This feature bypasses network hop for ByteStreams generated by BATCH_PRIMITIVE_RUN primitive jobs.
 $acceptance criteria:$",,Roman,Roman,Major,13,,0,2,1,1,0,0,0,,0,850,2,0,0,2022-06-30 16:39:39,PP now puts data directly into EM input queue bypassing network hop ,"The local interaction b/w PP and EM now goes over a network socket and in case of a single-node installation the socket is not even compressed. This imposes extra latency working with a single-node.
This feature bypasses network hop for ByteStreams generated by BATCH_PRIMITIVE_RUN primitive jobs.
",,0,0,0,0,0.0,"PP now puts data directly into EM input queue bypassing network hop  $end$ The local interaction b/w PP and EM now goes over a network socket and in case of a single-node installation the socket is not even compressed. This imposes extra latency working with a single-node.
This feature bypasses network hop for ByteStreams generated by BATCH_PRIMITIVE_RUN primitive jobs.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,66,18,0.272727,12,0.181818,9,0.136364,7,0.106061,5,0.0757576
414,MCOL-5155,Sub-Task,MCOL,2022-07-08 01:46:47,,0,CMAPI : support load data from s3 ,,,CMAPI : support load data from s3  $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-07-08 01:46:47,CMAPI : support load data from s3 ,,,0,0,0,0,0.0,CMAPI : support load data from s3  $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,9,6,0.666667,4,0.444444,3,0.333333,3,0.333333,3,0.333333
415,MCOL-5160,Task,MCOL,2022-07-15 15:58:33,,0,CMAPI ARM build + new drone tree structure,"new package tree 

https://mariadbcorp.atlassian.net/wiki/spaces/~695761903/pages/1695482244/CS+package+tree",,"CMAPI ARM build + new drone tree structure $end$ new package tree 

https://mariadbcorp.atlassian.net/wiki/spaces/~695761903/pages/1695482244/CS+package+tree $acceptance criteria:$",,alexey vorovich,Roman Navrotskiy,Major,7,,0,2,1,1,0,1,0,,0,850,2,0,0,2022-07-15 15:59:08,CMAPI ARM build,"new package tree 

https://mariadbcorp.atlassian.net/wiki/spaces/~695761903/pages/1695482244/CS+package+tree",,1,0,0,5,0.5,"CMAPI ARM build $end$ new package tree 

https://mariadbcorp.atlassian.net/wiki/spaces/~695761903/pages/1695482244/CS+package+tree $acceptance criteria:$",1,1,1,0,0,0,1,0.0,10,6,0.6,4,0.4,3,0.3,3,0.3,3,0.3
416,MCOL-5162,Task,MCOL,2022-07-18 21:35:43,,0,dbbuilder will support syscat upgrades,"There are two features in 22.08 that needs to upgrade an existing syscat with multiple syscat columns. There is no existing facility that can do this now.
After some consideration dbbuilder update should be used to run syscat upgrades from now on.",,"dbbuilder will support syscat upgrades $end$ There are two features in 22.08 that needs to upgrade an existing syscat with multiple syscat columns. There is no existing facility that can do this now.
After some consideration dbbuilder update should be used to run syscat upgrades from now on. $acceptance criteria:$",,Roman,Roman,Major,12,,0,4,0,1,0,0,0,,0,850,3,0,0,2022-07-22 15:41:45,dbbuilder will support syscat upgrades,"There are two features in 22.08 that needs to upgrade an existing syscat with multiple syscat columns. There is no existing facility that can do this now.
After some consideration dbbuilder update should be used to run syscat upgrades from now on.",,0,0,0,0,0.0,"dbbuilder will support syscat upgrades $end$ There are two features in 22.08 that needs to upgrade an existing syscat with multiple syscat columns. There is no existing facility that can do this now.
After some consideration dbbuilder update should be used to run syscat upgrades from now on. $acceptance criteria:$",0,0,0,0,0,0,0,90.1,67,18,0.268657,12,0.179104,9,0.134328,7,0.104478,5,0.0746269
417,MCOL-5163,New Feature,MCOL,2022-07-19 16:22:34,,0,"Increase the stability of writing processes: WriteEngineServer, DMLProc, DDLProc","As of 6.4.1 MCS uses systemd to handle processing start and stop routine. It also handles the processes hierarchy restarts, e.g. if mcs-primproc(contains both EM and PP) crashes systemd restarts mcs-writeengineserver unit. If WE is in the middle of a write operation this potentially left the system in an unusable state. If  mcs-writeengineserver is restarted systemd restarts both mcs-dmlproc and mcs-ddlproc so if DMLProc was doing some changes it left failed transactions behind. It is tedious to clean these stuck txns from the cluster.
The suggested approach is to decouple pairs: mcs-primproc, mcs-writeengineserver and mcs-writeengineserver and mcs-dmlproc, mcs-writeengineserver and mcs-ddlproc systemd-wise.",,"Increase the stability of writing processes: WriteEngineServer, DMLProc, DDLProc $end$ As of 6.4.1 MCS uses systemd to handle processing start and stop routine. It also handles the processes hierarchy restarts, e.g. if mcs-primproc(contains both EM and PP) crashes systemd restarts mcs-writeengineserver unit. If WE is in the middle of a write operation this potentially left the system in an unusable state. If  mcs-writeengineserver is restarted systemd restarts both mcs-dmlproc and mcs-ddlproc so if DMLProc was doing some changes it left failed transactions behind. It is tedious to clean these stuck txns from the cluster.
The suggested approach is to decouple pairs: mcs-primproc, mcs-writeengineserver and mcs-writeengineserver and mcs-dmlproc, mcs-writeengineserver and mcs-ddlproc systemd-wise. $acceptance criteria:$",,Roman,Roman,Major,8,,2,2,2,1,0,0,0,,0,850,2,0,0,2022-07-19 16:22:34,"Increase the stability of writing processes: WriteEngineServer, DMLProc, DDLProc","As of 6.4.1 MCS uses systemd to handle processing start and stop routine. It also handles the processes hierarchy restarts, e.g. if mcs-primproc(contains both EM and PP) crashes systemd restarts mcs-writeengineserver unit. If WE is in the middle of a write operation this potentially left the system in an unusable state. If  mcs-writeengineserver is restarted systemd restarts both mcs-dmlproc and mcs-ddlproc so if DMLProc was doing some changes it left failed transactions behind. It is tedious to clean these stuck txns from the cluster.
The suggested approach is to decouple pairs: mcs-primproc, mcs-writeengineserver and mcs-writeengineserver and mcs-dmlproc, mcs-writeengineserver and mcs-ddlproc systemd-wise.",,0,0,0,0,0.0,"Increase the stability of writing processes: WriteEngineServer, DMLProc, DDLProc $end$ As of 6.4.1 MCS uses systemd to handle processing start and stop routine. It also handles the processes hierarchy restarts, e.g. if mcs-primproc(contains both EM and PP) crashes systemd restarts mcs-writeengineserver unit. If WE is in the middle of a write operation this potentially left the system in an unusable state. If  mcs-writeengineserver is restarted systemd restarts both mcs-dmlproc and mcs-ddlproc so if DMLProc was doing some changes it left failed transactions behind. It is tedious to clean these stuck txns from the cluster.
The suggested approach is to decouple pairs: mcs-primproc, mcs-writeengineserver and mcs-writeengineserver and mcs-dmlproc, mcs-writeengineserver and mcs-ddlproc systemd-wise. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,68,18,0.264706,12,0.176471,9,0.132353,7,0.102941,5,0.0735294
418,MCOL-5166,New Feature,MCOL,2022-07-22 17:28:50,,0,EM to PP facility communication speedup  for a single-node ,"After MCOL-5001 there is a possibility to use in-memory exchange b/w EM and PP bypassing local network hop thus reducing time spent in kernel moving data from EM to PP. 
The suggested approach is to replace network communication with a queue in DEC class that can be used to send primitive messages(sometimes with data, e.g. for JOINs) from EM to PP.
",,"EM to PP facility communication speedup  for a single-node  $end$ After MCOL-5001 there is a possibility to use in-memory exchange b/w EM and PP bypassing local network hop thus reducing time spent in kernel moving data from EM to PP. 
The suggested approach is to replace network communication with a queue in DEC class that can be used to send primitive messages(sometimes with data, e.g. for JOINs) from EM to PP.
 $acceptance criteria:$",,Roman,Roman,Major,9,,1,1,2,1,0,0,0,,0,850,1,0,0,2022-07-22 17:30:15,EM to PP facility communication speedup  for a single-node ,"After MCOL-5001 there is a possibility to use in-memory exchange b/w EM and PP bypassing local network hop thus reducing time spent in kernel moving data from EM to PP. 
The suggested approach is to replace network communication with a queue in DEC class that can be used to send primitive messages(sometimes with data, e.g. for JOINs) from EM to PP.
",,0,0,0,0,0.0,"EM to PP facility communication speedup  for a single-node  $end$ After MCOL-5001 there is a possibility to use in-memory exchange b/w EM and PP bypassing local network hop thus reducing time spent in kernel moving data from EM to PP. 
The suggested approach is to replace network communication with a queue in DEC class that can be used to send primitive messages(sometimes with data, e.g. for JOINs) from EM to PP.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0166667,69,18,0.26087,12,0.173913,9,0.130435,7,0.101449,5,0.0724638
419,MCOL-5167,New Feature,MCOL,2022-07-25 14:36:25,,0,JOIN: support on clause filter for table which is not involved in a join,,,JOIN: support on clause filter for table which is not involved in a join $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,9,,1,1,1,1,0,0,0,,0,850,1,0,0,2022-07-25 14:36:25,JOIN: support on clause filter for table which is not involved in a join,,,0,0,0,0,0.0,JOIN: support on clause filter for table which is not involved in a join $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,11,7,0.636364,5,0.454545,3,0.272727,3,0.272727,3,0.272727
420,MCOL-5172,Task,MCOL,2022-07-27 15:33:13,,0,EM rebuild : support new catalog entries,,,EM rebuild : support new catalog entries $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,9,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-07-27 15:33:13,EM rebuild : support new catalog entries,,,0,0,0,0,0.0,EM rebuild : support new catalog entries $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,12,7,0.583333,5,0.416667,3,0.25,3,0.25,3,0.25
421,MCOL-5174,Task,MCOL,2022-08-02 15:38:36,,0,Create bash script to restart processes in a loop.,"We need for a universal way for starting/restarting processes in docker container.
The best way to solve that is small and simple bash script. (loop_process_starter.sh)

* start bash script (non binary directly) with arguments
* script starts process (w\o daemonising it) with logging to file
* if process fails script restarting it again with delay",,"Create bash script to restart processes in a loop. $end$ We need for a universal way for starting/restarting processes in docker container.
The best way to solve that is small and simple bash script. (loop_process_starter.sh)

* start bash script (non binary directly) with arguments
* script starts process (w\o daemonising it) with logging to file
* if process fails script restarting it again with delay $acceptance criteria:$",,Alan Mologorsky,Alan Mologorsky,Major,14,,0,0,0,1,0,4,0,,0,850,0,0,0,2022-08-02 15:38:36,Create .sh script to restart processes in a loop.,"We need for a universal way for starting/restarting processes in docker container.
The best way to solve that is small and simple bash script.

* start bash script (non binary directly) with arguments
* script starts process (w\o daemonising it) with logging to file
* if process fails script restarting it again with delay",,3,1,0,3,0.030303,"Create .sh script to restart processes in a loop. $end$ We need for a universal way for starting/restarting processes in docker container.
The best way to solve that is small and simple bash script.

* start bash script (non binary directly) with arguments
* script starts process (w\o daemonising it) with logging to file
* if process fails script restarting it again with delay $acceptance criteria:$",4,1,0,0,0,0,0,0.0,4,2,0.5,2,0.5,1,0.25,1,0.25,1,0.25
422,MCOL-518,New Feature,MCOL,2017-01-18 22:34:07,,0,backup (cold) and restore tool,"Create a backup and restore utility to automate and simplify the current cold back up process: https://mariadb.com/kb/en/mariadb/columnstore-backup-and-recovery/

Requirements: https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.201q742yqs77",,"backup (cold) and restore tool $end$ Create a backup and restore utility to automate and simplify the current cold back up process: https://mariadb.com/kb/en/mariadb/columnstore-backup-and-recovery/

Requirements: https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.201q742yqs77 $acceptance criteria:$",,David Thompson,David Thompson,Major,11,,0,1,0,9,0,0,0,,0,850,1,0,0,2017-01-18 22:34:07,backup (cold) and restore tool,"Create a backup and restore utility to automate and simplify the current cold back up process: https://mariadb.com/kb/en/mariadb/columnstore-backup-and-recovery/

Requirements: https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.201q742yqs77",,0,0,0,0,0.0,"backup (cold) and restore tool $end$ Create a backup and restore utility to automate and simplify the current cold back up process: https://mariadb.com/kb/en/mariadb/columnstore-backup-and-recovery/

Requirements: https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.201q742yqs77 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,17,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
423,MCOL-5184,Task,MCOL,2022-08-09 14:24:07,,0,ColumnStore 6.4.4-1 (RC) Testing Status,For Dompe . Not GA.,,ColumnStore 6.4.4-1 (RC) Testing Status $end$ For Dompe . Not GA. $acceptance criteria:$,,alexey vorovich,alexey vorovich,Major,7,,1,1,1,1,0,1,0,,0,850,1,0,0,2022-08-09 14:26:00,ColumnStore 6.4.1-1 (RC) Testing Status,For Dompe . Not GA.,,1,0,0,2,0.0769231,ColumnStore 6.4.1-1 (RC) Testing Status $end$ For Dompe . Not GA. $acceptance criteria:$,1,1,0,0,0,0,0,0.0166667,13,7,0.538462,5,0.384615,3,0.230769,3,0.230769,3,0.230769
424,MCOL-5188,Task,MCOL,2022-08-11 20:08:11,,0,regr funtions have numeric error when numers are very small,"Specifically regr_r2 and regr_slope, but there could be a similar behavior in others.

When we calculate the final result, often the amount of rounding error in the accumulators is such that we can get different results depending on the order of the data read. This makes our test suite spuriously fail.

There are techniques found in the literature that can help compensate for these numerical issues.
See
https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
https://www.johndcook.com/blog/standard_deviation/
For ideas. I'm sure there are other references  that can be found.",,"regr funtions have numeric error when numers are very small $end$ Specifically regr_r2 and regr_slope, but there could be a similar behavior in others.

When we calculate the final result, often the amount of rounding error in the accumulators is such that we can get different results depending on the order of the data read. This makes our test suite spuriously fail.

There are techniques found in the literature that can help compensate for these numerical issues.
See
https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
https://www.johndcook.com/blog/standard_deviation/
For ideas. I'm sure there are other references  that can be found. $acceptance criteria:$",,David Hall,David Hall,Major,11,,1,0,1,1,0,0,0,,0,850,0,0,0,2022-09-06 15:54:04,regr funtions have numeric error when numers are very small,"Specifically regr_r2 and regr_slope, but there could be a similar behavior in others.

When we calculate the final result, often the amount of rounding error in the accumulators is such that we can get different results depending on the order of the data read. This makes our test suite spuriously fail.

There are techniques found in the literature that can help compensate for these numerical issues.
See
https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
https://www.johndcook.com/blog/standard_deviation/
For ideas. I'm sure there are other references  that can be found.",,0,0,0,0,0.0,"regr funtions have numeric error when numers are very small $end$ Specifically regr_r2 and regr_slope, but there could be a similar behavior in others.

When we calculate the final result, often the amount of rounding error in the accumulators is such that we can get different results depending on the order of the data read. This makes our test suite spuriously fail.

There are techniques found in the literature that can help compensate for these numerical issues.
See
https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
https://www.johndcook.com/blog/standard_deviation/
For ideas. I'm sure there are other references  that can be found. $acceptance criteria:$",0,0,0,0,0,0,0,619.75,21,1,0.047619,1,0.047619,1,0.047619,1,0.047619,0,0.0
425,MCOL-519,New Feature,MCOL,2017-01-18 22:39:00,,0,productize glusterfs support and add tools to automate,"There currently is a glusterfs storage install option in the installer if it detects gluster running on the server. Currently this fails installation, however glusterfs works if setup as external storage.  The plan is to stablize this functionality and to develop seperate tools to help manage this.

requirements:https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.g5o9qj5ukq6f",,"productize glusterfs support and add tools to automate $end$ There currently is a glusterfs storage install option in the installer if it detects gluster running on the server. Currently this fails installation, however glusterfs works if setup as external storage.  The plan is to stablize this functionality and to develop seperate tools to help manage this.

requirements:https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.g5o9qj5ukq6f $acceptance criteria:$",,David Thompson,David Thompson,Major,21,,2,2,2,13,0,0,4,,0,850,2,0,0,2017-03-28 04:15:28,productize glusterfs support and add tools to automate,"There currently is a glusterfs storage install option in the installer if it detects gluster running on the server. Currently this fails installation, however glusterfs works if setup as external storage.  The plan is to stablize this functionality and to develop seperate tools to help manage this.

requirements:https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.g5o9qj5ukq6f",,0,0,0,0,0.0,"productize glusterfs support and add tools to automate $end$ There currently is a glusterfs storage install option in the installer if it detects gluster running on the server. Currently this fails installation, however glusterfs works if setup as external storage.  The plan is to stablize this functionality and to develop seperate tools to help manage this.

requirements:https://docs.google.com/document/d/1Z-WmZc-WNW9TlGeM5RKScQ8vhEq_WBLOlMlmwrfkh9Q/edit?pli=1#heading=h.g5o9qj5ukq6f $acceptance criteria:$",0,0,0,0,0,0,1,1637.6,18,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
426,MCOL-5191,Task,MCOL,2022-08-15 13:11:36,,0, Join Optimizer: histogram statistics/ needed for cost-based  optimization,"The previous step delivers a statistics that allows to detect uniqueness of a column to be able to cut lops in a JOIN graph the most effective way.
The current step will deliver a histogram collected for columns of tables. The strained goal is to collect a Number of Dictinct Values statistics also.
In the scope of this project the mentioned statistics collection is a manual operation and run by ANALYZE TABLE command as previously.",," Join Optimizer: histogram statistics/ needed for cost-based  optimization $end$ The previous step delivers a statistics that allows to detect uniqueness of a column to be able to cut lops in a JOIN graph the most effective way.
The current step will deliver a histogram collected for columns of tables. The strained goal is to collect a Number of Dictinct Values statistics also.
In the scope of this project the mentioned statistics collection is a manual operation and run by ANALYZE TABLE command as previously. $acceptance criteria:$",,Roman,Roman,Major,16,,0,1,1,2,0,4,0,,0,850,1,2,0,2022-09-06 15:52:08,Join Optimizer: histogram statistics/ needed for join optimizer,"The previous step delivers a statistics that allows to detect uniqueness of a column to be able to cut lops in a JOIN graph the most effective way.
The current step will deliver a histogram collected for columns of tables. The strained goal is to collect a Number of Dictinct Values statistics also.
In the scope of this project the mentioned statistics collection is a manual operation and run by ANALYZE TABLE command as previously.",,2,0,0,4,0.0232558,"Join Optimizer: histogram statistics/ needed for join optimizer $end$ The previous step delivers a statistics that allows to detect uniqueness of a column to be able to cut lops in a JOIN graph the most effective way.
The current step will deliver a histogram collected for columns of tables. The strained goal is to collect a Number of Dictinct Values statistics also.
In the scope of this project the mentioned statistics collection is a manual operation and run by ANALYZE TABLE command as previously. $acceptance criteria:$",2,1,0,0,0,0,1,530.667,70,18,0.257143,12,0.171429,9,0.128571,7,0.1,5,0.0714286
427,MCOL-5195,Task,MCOL,2022-08-16 14:20:28,,0,"Correlated subquery for equi/non-equi scalar filter and join condition (TPC-H q2, q17)","This issue covers the feature MCS lacks to pass query #2 from TPC-H test suit.

Here are some preliminary info.
Join in MCS can be run in two modes:
- centralized at UM
- distributed at PP
The next paragraph describes distributed processing at PP. The current Join processing model has two types of Join sides: small-side table and large-side table(it doesn't correlate with the cardinality of the tables). Small side tables are broadcasted to all PP to build a hashmap/-s and large side is processed row by row using small side hash maps.

Consider the query  
{noformat}
SELECT c1, c2, c3 FROM f, dim1, dim2 WHERE
f.c1 = dim1.c1 AND dim1.c2 = dim2.c2 and dim1.c3 = (SELECT max(dim1.c3) FROM dim1, dim2 WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2);
{noformat}

This query's last WHERE expression filters on the maximum dim1.c3 value for the equi-JOIN condition f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. The semantics of the the query is a equi-JOIN result where every record has dim1.c3 is the maximum for every combination f1.c1 = dim1.c1.

Let's consider more generic variant where max(dim1.c3) can be an expression of a single aggregate function of one of JOINED tables, e.g. min(dim1.c3)+1 or (if avg(dim1.c3) == 5 then X ELSE Y). Let's say conditions of a Correlated SubQuery are fully correlated thus they are the same as at one level above this CSQ, namely WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. To find a correlated maximum of dim1.c3 for a given f.c1 = dim1.c1 MCS needs to have a GROUP BY operator results. Let's consider GROUP BY properties in the context of MCS code. GROUP BY works with dim1 contents that satisfies simple filters of the query(there is no such in this example). It is important to note that PP will have a full dim1 available at this point b/c all small side tables are broadcasted to all PP. The key column for this GROUP BY is dim1.c1 and the result of this aggregation is max(dim1.c3). In the context of MCS needs to construct UMRowAggregation for input RowGroup of dim1.c1 and dim1.c3. RowAggregation output RowGroup has a single column for max(dim1.c3). This RowAggregation can be constructed on the fly in parallel with PP constructing the hashmap for the top-level JOIN.
If PP has such UMRowAggregation instance available it will be able to look into UMRowAggregation using f1.c1 as a GROUP BY key value traversing large side table and applying an expression on top of UMRowAggregation result.

The comments will describe state of art implementations from the open source databases.",,"Correlated subquery for equi/non-equi scalar filter and join condition (TPC-H q2, q17) $end$ This issue covers the feature MCS lacks to pass query #2 from TPC-H test suit.

Here are some preliminary info.
Join in MCS can be run in two modes:
- centralized at UM
- distributed at PP
The next paragraph describes distributed processing at PP. The current Join processing model has two types of Join sides: small-side table and large-side table(it doesn't correlate with the cardinality of the tables). Small side tables are broadcasted to all PP to build a hashmap/-s and large side is processed row by row using small side hash maps.

Consider the query  
{noformat}
SELECT c1, c2, c3 FROM f, dim1, dim2 WHERE
f.c1 = dim1.c1 AND dim1.c2 = dim2.c2 and dim1.c3 = (SELECT max(dim1.c3) FROM dim1, dim2 WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2);
{noformat}

This query's last WHERE expression filters on the maximum dim1.c3 value for the equi-JOIN condition f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. The semantics of the the query is a equi-JOIN result where every record has dim1.c3 is the maximum for every combination f1.c1 = dim1.c1.

Let's consider more generic variant where max(dim1.c3) can be an expression of a single aggregate function of one of JOINED tables, e.g. min(dim1.c3)+1 or (if avg(dim1.c3) == 5 then X ELSE Y). Let's say conditions of a Correlated SubQuery are fully correlated thus they are the same as at one level above this CSQ, namely WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. To find a correlated maximum of dim1.c3 for a given f.c1 = dim1.c1 MCS needs to have a GROUP BY operator results. Let's consider GROUP BY properties in the context of MCS code. GROUP BY works with dim1 contents that satisfies simple filters of the query(there is no such in this example). It is important to note that PP will have a full dim1 available at this point b/c all small side tables are broadcasted to all PP. The key column for this GROUP BY is dim1.c1 and the result of this aggregation is max(dim1.c3). In the context of MCS needs to construct UMRowAggregation for input RowGroup of dim1.c1 and dim1.c3. RowAggregation output RowGroup has a single column for max(dim1.c3). This RowAggregation can be constructed on the fly in parallel with PP constructing the hashmap for the top-level JOIN.
If PP has such UMRowAggregation instance available it will be able to look into UMRowAggregation using f1.c1 as a GROUP BY key value traversing large side table and applying an expression on top of UMRowAggregation result.

The comments will describe state of art implementations from the open source databases. $acceptance criteria:$",,Roman,Roman,Major,16,,2,3,2,1,0,2,0,,0,850,1,2,0,2023-01-19 10:36:58,"Correlated subquery for equi/non-equi scalar filter and join condition (TPC-H q2, q17)","This issue covers the feature MCS lacks to pass query #2 from TPC-H test suit.

Here are some preliminary info.
Join in MCS can be run in two modes:
- centralized at UM
- distributed at PP
The next paragraph describes distributed processing at PP. The current Join processing model has two types of Join sides: small-side table and large-side table(it doesn't correlate with the cardinality of the tables). Small side tables are broadcasted to all PP to build a hashmap/-s and large side is processed row by row using small side hash maps.

Consider the query  
{noformat}
SELECT c1, c2, c3 FROM f, dim1, dim2 WHERE
f.c1 = dim1.c1 AND dim1.c2 = dim2.c2 and dim1.c3 = (SELECT max(dim1.c3) FROM dim1, dim2 WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2);
{noformat}

This query's last WHERE expression filters on the maximum dim1.c3 value for the equi-JOIN condition f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. The semantics of the the query is a equi-JOIN result where every record has dim1.c3 is the maximum for every combination f1.c1 = dim1.c1.

Let's consider more generic variant where max(dim1.c3) can be an expression of a single aggregate function of one of JOINED tables, e.g. min(dim1.c3)+1 or (if avg(dim1.c3) == 5 then X ELSE Y). Let's say conditions of a Correlated SubQuery are fully correlated thus they are the same as at one level above this CSQ, namely WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. To find a correlated maximum of dim1.c3 for a given f.c1 = dim1.c1 MCS needs to have a GROUP BY operator results. Let's consider GROUP BY properties in the context of MCS code. GROUP BY works with dim1 contents that satisfies simple filters of the query(there is no such in this example). It is important to note that PP will have a full dim1 available at this point b/c all small side tables are broadcasted to all PP. The key column for this GROUP BY is dim1.c1 and the result of this aggregation is max(dim1.c3). In the context of MCS needs to construct UMRowAggregation for input RowGroup of dim1.c1 and dim1.c3. RowAggregation output RowGroup has a single column for max(dim1.c3). This RowAggregation can be constructed on the fly in parallel with PP constructing the hashmap for the top-level JOIN.
If PP has such UMRowAggregation instance available it will be able to look into UMRowAggregation using f1.c1 as a GROUP BY key value traversing large side table and applying an expression on top of UMRowAggregation result.

The comments will describe state of art implementations from the open source databases.",,0,0,0,0,0.0,"Correlated subquery for equi/non-equi scalar filter and join condition (TPC-H q2, q17) $end$ This issue covers the feature MCS lacks to pass query #2 from TPC-H test suit.

Here are some preliminary info.
Join in MCS can be run in two modes:
- centralized at UM
- distributed at PP
The next paragraph describes distributed processing at PP. The current Join processing model has two types of Join sides: small-side table and large-side table(it doesn't correlate with the cardinality of the tables). Small side tables are broadcasted to all PP to build a hashmap/-s and large side is processed row by row using small side hash maps.

Consider the query  
{noformat}
SELECT c1, c2, c3 FROM f, dim1, dim2 WHERE
f.c1 = dim1.c1 AND dim1.c2 = dim2.c2 and dim1.c3 = (SELECT max(dim1.c3) FROM dim1, dim2 WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2);
{noformat}

This query's last WHERE expression filters on the maximum dim1.c3 value for the equi-JOIN condition f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. The semantics of the the query is a equi-JOIN result where every record has dim1.c3 is the maximum for every combination f1.c1 = dim1.c1.

Let's consider more generic variant where max(dim1.c3) can be an expression of a single aggregate function of one of JOINED tables, e.g. min(dim1.c3)+1 or (if avg(dim1.c3) == 5 then X ELSE Y). Let's say conditions of a Correlated SubQuery are fully correlated thus they are the same as at one level above this CSQ, namely WHERE f.c1 = dim1.c1 AND dim1.c2 = dim2.c2. To find a correlated maximum of dim1.c3 for a given f.c1 = dim1.c1 MCS needs to have a GROUP BY operator results. Let's consider GROUP BY properties in the context of MCS code. GROUP BY works with dim1 contents that satisfies simple filters of the query(there is no such in this example). It is important to note that PP will have a full dim1 available at this point b/c all small side tables are broadcasted to all PP. The key column for this GROUP BY is dim1.c1 and the result of this aggregation is max(dim1.c3). In the context of MCS needs to construct UMRowAggregation for input RowGroup of dim1.c1 and dim1.c3. RowAggregation output RowGroup has a single column for max(dim1.c3). This RowAggregation can be constructed on the fly in parallel with PP constructing the hashmap for the top-level JOIN.
If PP has such UMRowAggregation instance available it will be able to look into UMRowAggregation using f1.c1 as a GROUP BY key value traversing large side table and applying an expression on top of UMRowAggregation result.

The comments will describe state of art implementations from the open source databases. $acceptance criteria:$",0,0,0,0,0,0,0,3740.27,71,19,0.267606,12,0.169014,9,0.126761,7,0.0985916,5,0.0704225
428,MCOL-52,Task,MCOL,2016-05-11 18:32:30,MCOL-10,0,MariaDB ColumnStore - How to Build,"Right now there is build instructions for InfiniDB at https://github.com/mariadb-corporation/InfiniDB#this-is-infinidb-462 for a user to download source code and build from it. But this would not work for MariaDB ColumnStore.

This task is for us to create equivalent build instructions for MariaDB ColumnStore that community users can use.",,"MariaDB ColumnStore - How to Build $end$ Right now there is build instructions for InfiniDB at https://github.com/mariadb-corporation/InfiniDB#this-is-infinidb-462 for a user to download source code and build from it. But this would not work for MariaDB ColumnStore.

This task is for us to create equivalent build instructions for MariaDB ColumnStore that community users can use. $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,14,,0,4,0,2,0,2,0,,0,850,4,2,0,2016-05-17 19:56:49,MariaDB ColumnStore - How to Build,"Right now there is build instructions for InfiniDB at https://github.com/mariadb-corporation/InfiniDB#this-is-infinidb-462 for a user to download source code and build from it. But this would not work for MariaDB ColumnStore.

This task is for us to create equivalent build instructions for MariaDB ColumnStore that community users can use.",,0,0,0,0,0.0,"MariaDB ColumnStore - How to Build $end$ Right now there is build instructions for InfiniDB at https://github.com/mariadb-corporation/InfiniDB#this-is-infinidb-462 for a user to download source code and build from it. But this would not work for MariaDB ColumnStore.

This task is for us to create equivalent build instructions for MariaDB ColumnStore that community users can use. $acceptance criteria:$",0,0,0,0,0,0,1,145.4,16,5,0.3125,1,0.0625,1,0.0625,1,0.0625,1,0.0625
429,MCOL-520,New Feature,MCOL,2017-01-18 22:53:07,MCOL-1503,0,true non root install phase 1,"The non root install currently requires that you enable passwordless sudo access for the account which is not as desirable. For an initial phase of improvement here, we should allow all normal ongoing operational use cases (query, dml, cpimport, shutdown, startup, etc) to work with no sudo access. Installation and system mainteance tasks such as addModule could still depend on sudo access but the more we can simplify the better.

After initial review, the decision was made to move towards a model where we more mirror the core mariadb server which will be needed for convergence anyways:
- Move away from the dedicated split of root and non root installs.
- Officially support only os package installs (i.e. rpm deb).  
- Binary install will still be needed unofficially for dev setups or supporting non standard linux distros.
- The os package install should install the columnstore components and run under the mysql user (like server). 
- Any required root steps then can be done as package scripts which are run as root.
- Migration paths will need to be developed / tested for upgrade.",,"true non root install phase 1 $end$ The non root install currently requires that you enable passwordless sudo access for the account which is not as desirable. For an initial phase of improvement here, we should allow all normal ongoing operational use cases (query, dml, cpimport, shutdown, startup, etc) to work with no sudo access. Installation and system mainteance tasks such as addModule could still depend on sudo access but the more we can simplify the better.

After initial review, the decision was made to move towards a model where we more mirror the core mariadb server which will be needed for convergence anyways:
- Move away from the dedicated split of root and non root installs.
- Officially support only os package installs (i.e. rpm deb).  
- Binary install will still be needed unofficially for dev setups or supporting non standard linux distros.
- The os package install should install the columnstore components and run under the mysql user (like server). 
- Any required root steps then can be done as package scripts which are run as root.
- Migration paths will need to be developed / tested for upgrade. $acceptance criteria:$",,David Thompson,David Thompson,Critical,39,,0,25,1,12,0,1,0,,0,850,25,0,0,2017-06-08 14:23:04,true non root install phase 1,"The non root install currently requires that you enable passwordless sudo access for the account which is not as desirable. For an initial phase of improvement here, we should allow all normal ongoing operational use cases (query, dml, cpimport, shutdown, startup, etc) to work with no sudo access. Installation and system mainteance tasks such as addModule could still depend on sudo access but the more we can simplify the better.",,0,1,0,114,1.44304,"true non root install phase 1 $end$ The non root install currently requires that you enable passwordless sudo access for the account which is not as desirable. For an initial phase of improvement here, we should allow all normal ongoing operational use cases (query, dml, cpimport, shutdown, startup, etc) to work with no sudo access. Installation and system mainteance tasks such as addModule could still depend on sudo access but the more we can simplify the better. $acceptance criteria:$",1,1,1,1,1,1,1,3375.48,19,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
430,MCOL-5206,Task,MCOL,2022-08-24 15:29:08,,0,Merge RBTree + EMIndex combination into develop [phase 3],,,Merge RBTree + EMIndex combination into develop [phase 3] $end$ $acceptance criteria:$,,Roman,Roman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-08-24 15:29:08,Merge RBTree + EMIndex combination into develop [phase 3],,,0,0,0,0,0.0,Merge RBTree + EMIndex combination into develop [phase 3] $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,72,19,0.263889,12,0.166667,9,0.125,7,0.0972222,5,0.0694444
431,MCOL-521,New Feature,MCOL,2017-01-18 23:03:33,,0,add distributed regression aggregate and window functions,add support for the regr_* functions as aggregate and window functions.,,add distributed regression aggregate and window functions $end$ add support for the regr_* functions as aggregate and window functions. $acceptance criteria:$,,David Thompson,David Thompson,Major,28,,2,11,4,7,0,0,0,,0,850,10,0,0,2018-06-07 15:15:43,add distributed regression aggregate and window functions,add support for the regr_* functions as aggregate and window functions.,,0,0,0,0,0.0,add distributed regression aggregate and window functions $end$ add support for the regr_* functions as aggregate and window functions. $acceptance criteria:$,0,0,0,0,0,0,1,12112.2,20,1,0.05,1,0.05,1,0.05,1,0.05,1,0.05
432,MCOL-522,New Feature,MCOL,2017-01-18 23:13:45,,0,support pre-installed software in postConfigure and addModule - phase I,"PostConfigure and addModule (etc) should support a mode of operation where they do not perform the actual software install and instead assume / verify that the user has already done this (either rpm, deb, or binary + post-install) and purely do the system configuration aspect. 

This likely will also help reduce the number of sudo use cases for non root in addition. It will also make it more straightforward to use third party orchestration software such as docker for multi node.

Phase I - user is required to preinstall the software. postConfigure will still communicate with new server for setup.",,"support pre-installed software in postConfigure and addModule - phase I $end$ PostConfigure and addModule (etc) should support a mode of operation where they do not perform the actual software install and instead assume / verify that the user has already done this (either rpm, deb, or binary + post-install) and purely do the system configuration aspect. 

This likely will also help reduce the number of sudo use cases for non root in addition. It will also make it more straightforward to use third party orchestration software such as docker for multi node.

Phase I - user is required to preinstall the software. postConfigure will still communicate with new server for setup. $acceptance criteria:$",,David Thompson,David Thompson,Major,13,,0,11,1,5,0,2,0,,0,850,11,0,0,2017-04-24 21:23:12,support pre-installed software in postConfigure and addModule,"PostConfigure and addModule (etc) should support a mode of operation where they do not perform the actual software install and instead assume / verify that the user has already done this (either rpm, deb, or binary + post-install) and purely do the system configuration aspect. 

This likely will also help reduce the number of sudo use cases for non root in addition. It will also make it more straightforward to use third party orchestration software such as docker for multi node.",,1,1,0,22,0.241758,"support pre-installed software in postConfigure and addModule $end$ PostConfigure and addModule (etc) should support a mode of operation where they do not perform the actual software install and instead assume / verify that the user has already done this (either rpm, deb, or binary + post-install) and purely do the system configuration aspect. 

This likely will also help reduce the number of sudo use cases for non root in addition. It will also make it more straightforward to use third party orchestration software such as docker for multi node. $acceptance criteria:$",2,1,1,1,1,1,1,2302.15,21,1,0.047619,1,0.047619,1,0.047619,1,0.047619,1,0.047619
433,MCOL-5224,New Feature,MCOL,2022-09-19 11:11:54,,0,JSON_ARRAYAGG join failing,"Hi

customer complain and report that this query fail:


{code:java}




MariaDB [dmc_dg_ita_199]> SELECT
-> ma_id, ma_desc, JSON_ARRAYAGG(DISTINCT cwc_na_id) ma_nas, JSON_ARRAYAGG(DISTINCT cwc_pd_id) ma_pds FROM
-> (SELECT DISTINCT cwc_ma_id, cwc_na_id, cwc_pd_id FROM dmc_dg_ita_199.campagna_web_codice) CWC JOIN qcommon.marca ON ma_id = cwc_ma_id
-> GROUP BY ma_id, ma_desc;

{code}

with the follow errors:

ERROR 1178 (42000): The storage engine for the table doesn't support Non supported aggregate type on the select clause",,"JSON_ARRAYAGG join failing $end$ Hi

customer complain and report that this query fail:


{code:java}




MariaDB [dmc_dg_ita_199]> SELECT
-> ma_id, ma_desc, JSON_ARRAYAGG(DISTINCT cwc_na_id) ma_nas, JSON_ARRAYAGG(DISTINCT cwc_pd_id) ma_pds FROM
-> (SELECT DISTINCT cwc_ma_id, cwc_na_id, cwc_pd_id FROM dmc_dg_ita_199.campagna_web_codice) CWC JOIN qcommon.marca ON ma_id = cwc_ma_id
-> GROUP BY ma_id, ma_desc;

{code}

with the follow errors:

ERROR 1178 (42000): The storage engine for the table doesn't support Non supported aggregate type on the select clause $acceptance criteria:$",,Massimo,Massimo,Major,8,,0,0,2,2,0,0,0,,0,850,0,0,0,2022-09-26 09:54:17,JSON_ARRAYAGG join failing,"Hi

customer complain and report that this query fail:


{code:java}




MariaDB [dmc_dg_ita_199]> SELECT
-> ma_id, ma_desc, JSON_ARRAYAGG(DISTINCT cwc_na_id) ma_nas, JSON_ARRAYAGG(DISTINCT cwc_pd_id) ma_pds FROM
-> (SELECT DISTINCT cwc_ma_id, cwc_na_id, cwc_pd_id FROM dmc_dg_ita_199.campagna_web_codice) CWC JOIN qcommon.marca ON ma_id = cwc_ma_id
-> GROUP BY ma_id, ma_desc;

{code}

with the follow errors:

ERROR 1178 (42000): The storage engine for the table doesn't support Non supported aggregate type on the select clause",,0,0,0,0,0.0,"JSON_ARRAYAGG join failing $end$ Hi

customer complain and report that this query fail:


{code:java}




MariaDB [dmc_dg_ita_199]> SELECT
-> ma_id, ma_desc, JSON_ARRAYAGG(DISTINCT cwc_na_id) ma_nas, JSON_ARRAYAGG(DISTINCT cwc_pd_id) ma_pds FROM
-> (SELECT DISTINCT cwc_ma_id, cwc_na_id, cwc_pd_id FROM dmc_dg_ita_199.campagna_web_codice) CWC JOIN qcommon.marca ON ma_id = cwc_ma_id
-> GROUP BY ma_id, ma_desc;

{code}

with the follow errors:

ERROR 1178 (42000): The storage engine for the table doesn't support Non supported aggregate type on the select clause $acceptance criteria:$",0,0,0,0,0,0,1,166.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
434,MCOL-523,New Feature,MCOL,2017-01-19 18:59:43,,0,support user defined aggregate functions,,,support user defined aggregate functions $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,26,,0,7,1,11,0,0,9,,0,850,7,0,0,2017-04-12 02:44:26,support user defined aggregate functions,,,0,0,0,0,0.0,support user defined aggregate functions $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1975.73,22,2,0.0909091,2,0.0909091,2,0.0909091,2,0.0909091,2,0.0909091
435,MCOL-528,New Feature,MCOL,2017-01-20 22:04:00,,0,Local query would not work if user chose not to enable schema sync,"Build tested: 1.0.7-1

For 1.0.7-1, we added a prompt in postConfigure to allow user to enable or disable schema sync.  If the user chose not to enable schema sync and enable local query in the next prompt, local query will not work since it requires schema sync enabled.  We need to do one of the following:

1) Prompt for local query first, if the answer is yes, then turn on schema sync without prompt the user.  This would be the preferred method

2) If the user answered n to schema sync, and y to local query, we still enable schema sync, overriding the user's answer (n to the schema sync question)

----

===== Setup System Module Type Configuration =====
==> pm1: 
==> pm1: There are 2 options when configuring the System Module Type: separate and combined
==> pm1: 
==> pm1:   'separate' - User and Performance functionality on separate servers.
==> pm1: 
==> pm1:   'combined' - User and Performance functionality on the same server
==> pm1: Select the type of System Module Install [1=separate, 2=combined] (2) > 1
==> pm1: 
==> pm1: NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
==> pm1:       schemas and InnoDB tables across the User Module nodes. This feature can be enabled
==> pm1:       or disabled, for example, if you wish to configure your own replication post installation.
==> pm1: MariaDB ColumnStore Schema Sync feature, do you want to enable? [y,n] (y) > n
==> pm1: Seperate Server Installation will be performed.
==> pm1: 
==> pm1: NOTE: Local Query Feature allows the ability to query data from a single Performance
==> pm1:       Module. Check MariaDB ColumnStore Admin Guide for additional information.
==> pm1: Enable Local Query feature? [y,n] (n) > y
==> pm1: 
==> pm1: NOTE: Local Query Feature is enabled
",,"Local query would not work if user chose not to enable schema sync $end$ Build tested: 1.0.7-1

For 1.0.7-1, we added a prompt in postConfigure to allow user to enable or disable schema sync.  If the user chose not to enable schema sync and enable local query in the next prompt, local query will not work since it requires schema sync enabled.  We need to do one of the following:

1) Prompt for local query first, if the answer is yes, then turn on schema sync without prompt the user.  This would be the preferred method

2) If the user answered n to schema sync, and y to local query, we still enable schema sync, overriding the user's answer (n to the schema sync question)

----

===== Setup System Module Type Configuration =====
==> pm1: 
==> pm1: There are 2 options when configuring the System Module Type: separate and combined
==> pm1: 
==> pm1:   'separate' - User and Performance functionality on separate servers.
==> pm1: 
==> pm1:   'combined' - User and Performance functionality on the same server
==> pm1: Select the type of System Module Install [1=separate, 2=combined] (2) > 1
==> pm1: 
==> pm1: NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
==> pm1:       schemas and InnoDB tables across the User Module nodes. This feature can be enabled
==> pm1:       or disabled, for example, if you wish to configure your own replication post installation.
==> pm1: MariaDB ColumnStore Schema Sync feature, do you want to enable? [y,n] (y) > n
==> pm1: Seperate Server Installation will be performed.
==> pm1: 
==> pm1: NOTE: Local Query Feature allows the ability to query data from a single Performance
==> pm1:       Module. Check MariaDB ColumnStore Admin Guide for additional information.
==> pm1: Enable Local Query feature? [y,n] (n) > y
==> pm1: 
==> pm1: NOTE: Local Query Feature is enabled
 $acceptance criteria:$",,Daniel Lee,Daniel Lee,Minor,8,,0,3,0,1,0,0,0,,0,850,3,0,0,2017-01-20 23:34:40,Local query would not work if user chose not to enable schema sync,"Build tested: 1.0.7-1

For 1.0.7-1, we added a prompt in postConfigure to allow user to enable or disable schema sync.  If the user chose not to enable schema sync and enable local query in the next prompt, local query will not work since it requires schema sync enabled.  We need to do one of the following:

1) Prompt for local query first, if the answer is yes, then turn on schema sync without prompt the user.  This would be the preferred method

2) If the user answered n to schema sync, and y to local query, we still enable schema sync, overriding the user's answer (n to the schema sync question)

----

===== Setup System Module Type Configuration =====
==> pm1: 
==> pm1: There are 2 options when configuring the System Module Type: separate and combined
==> pm1: 
==> pm1:   'separate' - User and Performance functionality on separate servers.
==> pm1: 
==> pm1:   'combined' - User and Performance functionality on the same server
==> pm1: Select the type of System Module Install [1=separate, 2=combined] (2) > 1
==> pm1: 
==> pm1: NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
==> pm1:       schemas and InnoDB tables across the User Module nodes. This feature can be enabled
==> pm1:       or disabled, for example, if you wish to configure your own replication post installation.
==> pm1: MariaDB ColumnStore Schema Sync feature, do you want to enable? [y,n] (y) > n
==> pm1: Seperate Server Installation will be performed.
==> pm1: 
==> pm1: NOTE: Local Query Feature allows the ability to query data from a single Performance
==> pm1:       Module. Check MariaDB ColumnStore Admin Guide for additional information.
==> pm1: Enable Local Query feature? [y,n] (n) > y
==> pm1: 
==> pm1: NOTE: Local Query Feature is enabled
",,0,0,0,0,0.0,"Local query would not work if user chose not to enable schema sync $end$ Build tested: 1.0.7-1

For 1.0.7-1, we added a prompt in postConfigure to allow user to enable or disable schema sync.  If the user chose not to enable schema sync and enable local query in the next prompt, local query will not work since it requires schema sync enabled.  We need to do one of the following:

1) Prompt for local query first, if the answer is yes, then turn on schema sync without prompt the user.  This would be the preferred method

2) If the user answered n to schema sync, and y to local query, we still enable schema sync, overriding the user's answer (n to the schema sync question)

----

===== Setup System Module Type Configuration =====
==> pm1: 
==> pm1: There are 2 options when configuring the System Module Type: separate and combined
==> pm1: 
==> pm1:   'separate' - User and Performance functionality on separate servers.
==> pm1: 
==> pm1:   'combined' - User and Performance functionality on the same server
==> pm1: Select the type of System Module Install [1=separate, 2=combined] (2) > 1
==> pm1: 
==> pm1: NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
==> pm1:       schemas and InnoDB tables across the User Module nodes. This feature can be enabled
==> pm1:       or disabled, for example, if you wish to configure your own replication post installation.
==> pm1: MariaDB ColumnStore Schema Sync feature, do you want to enable? [y,n] (y) > n
==> pm1: Seperate Server Installation will be performed.
==> pm1: 
==> pm1: NOTE: Local Query Feature allows the ability to query data from a single Performance
==> pm1:       Module. Check MariaDB ColumnStore Admin Guide for additional information.
==> pm1: Enable Local Query feature? [y,n] (n) > y
==> pm1: 
==> pm1: NOTE: Local Query Feature is enabled
 $acceptance criteria:$",0,0,0,0,0,0,0,1.5,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
436,MCOL-529,New Feature,MCOL,2017-01-23 17:20:38,,0,DBRM message queue clients need to be pooled,"When running test007 a second time we get the following error many times towards the end:

ERROR 1836 (HY000) at line 1: Running in read-only mode

The error log at the time is attached.",,"DBRM message queue clients need to be pooled $end$ When running test007 a second time we get the following error many times towards the end:

ERROR 1836 (HY000) at line 1: Running in read-only mode

The error log at the time is attached. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Critical,16,,0,6,0,4,0,1,0,,0,850,3,1,0,2017-04-03 15:35:11,DBRM message queue clients need to be pooled,"When running test007 a second time we get the following error many times towards the end:

ERROR 1836 (HY000) at line 1: Running in read-only mode

The error log at the time is attached.",,0,0,0,0,0.0,"DBRM message queue clients need to be pooled $end$ When running test007 a second time we get the following error many times towards the end:

ERROR 1836 (HY000) at line 1: Running in read-only mode

The error log at the time is attached. $acceptance criteria:$",0,0,0,0,0,0,1,1678.23,11,2,0.181818,2,0.181818,1,0.0909091,1,0.0909091,1,0.0909091
437,MCOL-53,Task,MCOL,2016-05-11 19:54:27,,0,Analyse Nightly Regression Failures,,,Analyse Nightly Regression Failures $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-05-11 19:54:47,Analyse Nightly Regression Failures,,,0,0,0,0,0.0,Analyse Nightly Regression Failures $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,17,5,0.294118,1,0.0588235,1,0.0588235,1,0.0588235,1,0.0588235
438,MCOL-5313,Task,MCOL,2022-11-18 10:48:12,,0,Re-test new Exttent Map implementation,"MCS 22.08 and onwards has a new ExtentMap in-memory representation leverages RBTree over shared memory. There were certain issues with MCS 5 where the EM format was first coined, namely some undefined behavior. There is a presumption that MCS 5 suffers from the fact that all systemd units in MCS 5 restart at once when one the units(e.g. ExeMgr) fails.
  ",,"Re-test new Exttent Map implementation $end$ MCS 22.08 and onwards has a new ExtentMap in-memory representation leverages RBTree over shared memory. There were certain issues with MCS 5 where the EM format was first coined, namely some undefined behavior. There is a presumption that MCS 5 suffers from the fact that all systemd units in MCS 5 restart at once when one the units(e.g. ExeMgr) fails.
   $acceptance criteria:$",,Roman,Roman,Major,3,,1,0,2,1,0,0,0,,0,850,0,0,0,2022-11-18 10:48:12,Re-test new Exttent Map implementation,"MCS 22.08 and onwards has a new ExtentMap in-memory representation leverages RBTree over shared memory. There were certain issues with MCS 5 where the EM format was first coined, namely some undefined behavior. There is a presumption that MCS 5 suffers from the fact that all systemd units in MCS 5 restart at once when one the units(e.g. ExeMgr) fails.
  ",,0,0,0,0,0.0,"Re-test new Exttent Map implementation $end$ MCS 22.08 and onwards has a new ExtentMap in-memory representation leverages RBTree over shared memory. There were certain issues with MCS 5 where the EM format was first coined, namely some undefined behavior. There is a presumption that MCS 5 suffers from the fact that all systemd units in MCS 5 restart at once when one the units(e.g. ExeMgr) fails.
   $acceptance criteria:$",0,0,0,0,0,0,0,0.0,73,19,0.260274,12,0.164384,9,0.123288,7,0.0958904,5,0.0684932
439,MCOL-5329,Task,MCOL,2022-12-04 14:42:31,,0,"Downmerge MTR changes from develop, develop-6 to develop-5",ES 10.5 finally got the patch that adds encoding/collation information into SHOW CREATE TABLE output. [~kirill.perov@mariadb.com] could you plz downmerge your patch?,,"Downmerge MTR changes from develop, develop-6 to develop-5 $end$ ES 10.5 finally got the patch that adds encoding/collation information into SHOW CREATE TABLE output. [~kirill.perov@mariadb.com] could you plz downmerge your patch? $acceptance criteria:$",,Roman,Roman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-12-04 14:42:42,"Downmerge MTR changes from develop, develop-6 to develop-5",ES 10.5 finally got the patch that adds encoding/collation information into SHOW CREATE TABLE output. [~kirill.perov@mariadb.com] could you plz downmerge your patch?,,0,0,0,0,0.0,"Downmerge MTR changes from develop, develop-6 to develop-5 $end$ ES 10.5 finally got the patch that adds encoding/collation information into SHOW CREATE TABLE output. [~kirill.perov@mariadb.com] could you plz downmerge your patch? $acceptance criteria:$",0,0,0,0,0,0,0,0.0,74,19,0.256757,12,0.162162,9,0.121622,7,0.0945946,5,0.0675676
440,MCOL-5361,Task,MCOL,2022-12-16 16:38:57,,0,prepare/execute : accomodate latest changes in server 10.6.11,,,prepare/execute : accomodate latest changes in server 10.6.11 $end$ $acceptance criteria:$,,alexey vorovich,alexey vorovich,Blocker,14,,1,3,1,1,0,0,0,,0,850,2,0,0,2022-12-19 18:24:02,prepare/execute : accomodate latest changes in server 10.6.11,,,0,0,0,0,0.0,prepare/execute : accomodate latest changes in server 10.6.11 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,73.75,14,8,0.571429,5,0.357143,3,0.214286,3,0.214286,3,0.214286
441,MCOL-5400,Task,MCOL,2023-01-26 12:08:40,,0,Disable GROUP BY Pushdown by default,"The mentioned GROUP BY pushdown proved to be error-prone, e.g. its execution starts after optimize phase so MDB optimizer might has rewritten JOIN conditions dropping some of them. The optimized conditions forces the query to error-out. Moreover this pushdown method are rarely called b/c it is the last in the precedence of pushdown methods available:
- select handler
- derived handler
- group by handler
- table scan with conditions
Deactivation of GBH might implicitly affects the variety of queries that MCS can process so at least full test001 should be run with the patch.  ",,"Disable GROUP BY Pushdown by default $end$ The mentioned GROUP BY pushdown proved to be error-prone, e.g. its execution starts after optimize phase so MDB optimizer might has rewritten JOIN conditions dropping some of them. The optimized conditions forces the query to error-out. Moreover this pushdown method are rarely called b/c it is the last in the precedence of pushdown methods available:
- select handler
- derived handler
- group by handler
- table scan with conditions
Deactivation of GBH might implicitly affects the variety of queries that MCS can process so at least full test001 should be run with the patch.   $acceptance criteria:$",,Roman,Roman,Major,7,,0,2,0,1,0,0,0,,0,850,2,0,0,2023-01-26 12:08:40,Disable GROUP BY Pushdown by default,"The mentioned GROUP BY pushdown proved to be error-prone, e.g. its execution starts after optimize phase so MDB optimizer might has rewritten JOIN conditions dropping some of them. The optimized conditions forces the query to error-out. Moreover this pushdown method are rarely called b/c it is the last in the precedence of pushdown methods available:
- select handler
- derived handler
- group by handler
- table scan with conditions
Deactivation of GBH might implicitly affects the variety of queries that MCS can process so at least full test001 should be run with the patch.  ",,0,0,0,0,0.0,"Disable GROUP BY Pushdown by default $end$ The mentioned GROUP BY pushdown proved to be error-prone, e.g. its execution starts after optimize phase so MDB optimizer might has rewritten JOIN conditions dropping some of them. The optimized conditions forces the query to error-out. Moreover this pushdown method are rarely called b/c it is the last in the precedence of pushdown methods available:
- select handler
- derived handler
- group by handler
- table scan with conditions
Deactivation of GBH might implicitly affects the variety of queries that MCS can process so at least full test001 should be run with the patch.   $acceptance criteria:$",0,0,0,0,0,0,0,0.0,75,19,0.253333,12,0.16,9,0.12,7,0.0933333,5,0.0666667
442,MCOL-5426,Task,MCOL,2023-02-17 19:36:11,,0,Move CMAPI to Open Source License & Include In Community Release,"Remove references to BSL in CMAPI license files and switch to GPLv2.  Also start packaging and shipping CMAPI with community releases.

This is an interim step until CMAPI is fully merged into the engine.",,"Move CMAPI to Open Source License & Include In Community Release $end$ Remove references to BSL in CMAPI license files and switch to GPLv2.  Also start packaging and shipping CMAPI with community releases.

This is an interim step until CMAPI is fully merged into the engine. $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,12,,0,0,0,3,0,0,0,,0,850,0,0,0,2023-02-17 20:51:12,Move CMAPI to Open Source License & Include In Community Release,"Remove references to BSL in CMAPI license files and switch to GPLv2.  Also start packaging and shipping CMAPI with community releases.

This is an interim step until CMAPI is fully merged into the engine.",,0,0,0,0,0.0,"Move CMAPI to Open Source License & Include In Community Release $end$ Remove references to BSL in CMAPI license files and switch to GPLv2.  Also start packaging and shipping CMAPI with community releases.

This is an interim step until CMAPI is fully merged into the engine. $acceptance criteria:$",0,0,0,0,0,0,1,1.25,11,2,0.181818,1,0.0909091,1,0.0909091,1,0.0909091,1,0.0909091
443,MCOL-548,New Feature,MCOL,2017-02-07 15:49:22,,0,clean up engine repo cmake warnings,"These errors get reported on Ubuntu and SuSE builds when the engine cmake is run

Cloning into 'mariadb-columnstore-engine'...
remote: Counting objects: 22901, done.
remote: Compressing objects: 100% (160/160), done.
remote: Total 22901 (delta 72), reused 0 (delta 0), pack-reused 22741
Receiving objects: 100% (22901/22901), 103.89 MiB | 5.33 MiB/s, done.
Resolving deltas: 100% (12660/12660), done.
Checking connectivity... done.
error: pathspec 'devel_1.0' did not match any file(s) known to git.
SERVER_BUILD_INCLUDE_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/../include
SERVER_SOURCE_ROOT_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/..
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-gram.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-scan.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-gram.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-scan.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.",,"clean up engine repo cmake warnings $end$ These errors get reported on Ubuntu and SuSE builds when the engine cmake is run

Cloning into 'mariadb-columnstore-engine'...
remote: Counting objects: 22901, done.
remote: Compressing objects: 100% (160/160), done.
remote: Total 22901 (delta 72), reused 0 (delta 0), pack-reused 22741
Receiving objects: 100% (22901/22901), 103.89 MiB | 5.33 MiB/s, done.
Resolving deltas: 100% (12660/12660), done.
Checking connectivity... done.
error: pathspec 'devel_1.0' did not match any file(s) known to git.
SERVER_BUILD_INCLUDE_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/../include
SERVER_SOURCE_ROOT_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/..
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-gram.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-scan.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-gram.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-scan.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it. $acceptance criteria:$",,David Hill,David Hill,Trivial,7,,0,4,0,2,0,0,0,,0,850,2,0,0,2018-01-24 11:14:17,clean up engine repo cmake warnings,"These errors get reported on Ubuntu and SuSE builds when the engine cmake is run

Cloning into 'mariadb-columnstore-engine'...
remote: Counting objects: 22901, done.
remote: Compressing objects: 100% (160/160), done.
remote: Total 22901 (delta 72), reused 0 (delta 0), pack-reused 22741
Receiving objects: 100% (22901/22901), 103.89 MiB | 5.33 MiB/s, done.
Resolving deltas: 100% (12660/12660), done.
Checking connectivity... done.
error: pathspec 'devel_1.0' did not match any file(s) known to git.
SERVER_BUILD_INCLUDE_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/../include
SERVER_SOURCE_ROOT_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/..
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-gram.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-scan.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-gram.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-scan.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.",,0,0,0,0,0.0,"clean up engine repo cmake warnings $end$ These errors get reported on Ubuntu and SuSE builds when the engine cmake is run

Cloning into 'mariadb-columnstore-engine'...
remote: Counting objects: 22901, done.
remote: Compressing objects: 100% (160/160), done.
remote: Total 22901 (delta 72), reused 0 (delta 0), pack-reused 22741
Receiving objects: 100% (22901/22901), 103.89 MiB | 5.33 MiB/s, done.
Resolving deltas: 100% (12660/12660), done.
Checking connectivity... done.
error: pathspec 'devel_1.0' did not match any file(s) known to git.
SERVER_BUILD_INCLUDE_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/../include
SERVER_SOURCE_ROOT_DIR = /home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/..
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-gram.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/ddlpackage/ddl-scan.cpp""
  of target ""ddlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-gram.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
  Policy CMP0046 is not set: Error on non-existent dependency in
  add_dependencies.  Run ""cmake --help-policy CMP0046"" for policy details.
  Use the cmake_policy command to set the policy and suppress this warning.

  The dependency target
  ""/home/builder/mariadb-columnstore-server/mariadb-columnstore-engine/dbcon/dmlpackage/dml-scan.cpp""
  of target ""dmlpackage"" does not exist.
This warning is for project developers.  Use -Wno-dev to suppress it. $acceptance criteria:$",0,0,0,0,0,0,1,8419.4,14,2,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
444,MCOL-55,Task,MCOL,2016-05-13 22:29:58,,0,Update copyright headers,"All the source code file refer to InfiniDB copyright in the header section right now. This task is define the  header section with MariaDB copy right added and update all files with defined header section


",,"Update copyright headers $end$ All the source code file refer to InfiniDB copyright in the header section right now. This task is define the  header section with MariaDB copy right added and update all files with defined header section


 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,10,,0,0,0,1,0,0,2,,0,850,0,0,0,2016-05-13 22:33:08,Update copyright headers,"All the source code file refer to InfiniDB copyright in the header section right now. This task is define the  header section with MariaDB copy right added and update all files with defined header section


",,0,0,0,0,0.0,"Update copyright headers $end$ All the source code file refer to InfiniDB copyright in the header section right now. This task is define the  header section with MariaDB copy right added and update all files with defined header section


 $acceptance criteria:$",0,0,0,0,0,0,0,0.05,18,5,0.277778,1,0.0555556,1,0.0555556,1,0.0555556,1,0.0555556
445,MCOL-552,New Feature,MCOL,2017-02-07 20:54:13,,0,cleanup postConfigure install for Suse,"these 2 systemctl messages to printed to the screen during suse install in postConfigure
Just messages, not any errors.. install does g on to work

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
redirecting to systemctl start .service
redirecting to systemctl stop .service
post-mysql-install Successfully Completed
",,"cleanup postConfigure install for Suse $end$ these 2 systemctl messages to printed to the screen during suse install in postConfigure
Just messages, not any errors.. install does g on to work

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
redirecting to systemctl start .service
redirecting to systemctl stop .service
post-mysql-install Successfully Completed
 $acceptance criteria:$",,David Hill,David Hill,Minor,11,,0,4,0,3,0,0,0,,0,850,4,0,0,2017-02-28 16:48:19,cleanup postConfigure install for Suse,"these 2 systemctl messages to printed to the screen during suse install in postConfigure
Just messages, not any errors.. install does g on to work

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
redirecting to systemctl start .service
redirecting to systemctl stop .service
post-mysql-install Successfully Completed
",,0,0,0,0,0.0,"cleanup postConfigure install for Suse $end$ these 2 systemctl messages to printed to the screen during suse install in postConfigure
Just messages, not any errors.. install does g on to work

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
redirecting to systemctl start .service
redirecting to systemctl stop .service
post-mysql-install Successfully Completed
 $acceptance criteria:$",0,0,0,0,0,0,1,499.9,15,2,0.133333,0,0.0,0,0.0,0,0.0,0,0.0
446,MCOL-56,Sub-Task,MCOL,2016-05-13 22:31:16,,0,Define header section text with MariaDB Copyright added,Update the header section update that include MariaDB Copyright in addition to existing InfiniDB copyright,,Define header section text with MariaDB Copyright added $end$ Update the header section update that include MariaDB Copyright in addition to existing InfiniDB copyright $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,9,,0,5,0,1,0,1,0,,0,850,5,0,0,2016-05-13 22:31:16,Define header section text with MariaDB Copyright added,Define the header section update that include MariaDB Copyright in addition to existing InfiniDB copyright,,0,1,0,2,0.0384615,Define header section text with MariaDB Copyright added $end$ Define the header section update that include MariaDB Copyright in addition to existing InfiniDB copyright $acceptance criteria:$,1,1,0,0,0,0,0,0.0,19,5,0.263158,1,0.0526316,1,0.0526316,1,0.0526316,1,0.0526316
447,MCOL-563,New Feature,MCOL,2017-02-10 15:20:59,MCOL-4343,0,Implement Disk-based  aggregation,"Hi,

I read in the knowledge base that Disk-based  aggregation are not implemented.

Have you some ETA/Roadmap on this feature or is it out of the scope?

It could be quite helpfull to avoid the process to be killed when he asks for too much memory on  aggregations, especially since it's working for joins. 

https://mariadb.com/kb/en/mariadb/columnstore-disk-based-joins/

Regards

Mathieu",,"Implement Disk-based  aggregation $end$ Hi,

I read in the knowledge base that Disk-based  aggregation are not implemented.

Have you some ETA/Roadmap on this feature or is it out of the scope?

It could be quite helpfull to avoid the process to be killed when he asks for too much memory on  aggregations, especially since it's working for joins. 

https://mariadb.com/kb/en/mariadb/columnstore-disk-based-joins/

Regards

Mathieu $acceptance criteria:$",,mathieu raillard,mathieu raillard,Blocker,92,,2,20,4,9,0,2,3,,0,850,20,2,0,2021-01-12 14:12:07,Implement Disk-based  aggregation,"Hi,

I read in the knowledge base that Disk-based  aggregation are not implemented.

Have you some ETA/Roadmap on this feature or is it out of the scope?

It could be quite helpfull to avoid the process to be killed when he asks for too much memory on  aggregations, especially since it's working for joins. 

https://mariadb.com/kb/en/mariadb/columnstore-disk-based-joins/

Regards

Mathieu",,0,0,0,0,0.0,"Implement Disk-based  aggregation $end$ Hi,

I read in the knowledge base that Disk-based  aggregation are not implemented.

Have you some ETA/Roadmap on this feature or is it out of the scope?

It could be quite helpfull to avoid the process to be killed when he asks for too much memory on  aggregations, especially since it's working for joins. 

https://mariadb.com/kb/en/mariadb/columnstore-disk-based-joins/

Regards

Mathieu $acceptance criteria:$",0,0,0,0,0,0,1,34366.9,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
448,MCOL-569,New Feature,MCOL,2017-02-15 18:04:47,MCOL-1049,0,CS does not support reserved words as table names,"create table `user`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.

create table `datetime`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
",,"CS does not support reserved words as table names $end$ create table `user`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.

create table `datetime`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
 $acceptance criteria:$",,Justin Swanhart,Justin Swanhart,Major,9,,2,4,2,2,0,0,0,,0,850,2,0,0,2018-01-24 11:08:44,CS does not support reserved words as table names,"create table `user`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.

create table `datetime`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
",,0,0,0,0,0.0,"CS does not support reserved words as table names $end$ create table `user`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.

create table `datetime`(id bigint) engine=columnstore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.
 $acceptance criteria:$",0,0,0,0,0,0,1,8225.05,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
449,MCOL-57,Sub-Task,MCOL,2016-05-13 22:32:09,,0,Update source code file header section,This task is to use the header section text defined by MCOL-56 and use it in all of the source code file,,Update source code file header section $end$ This task is to use the header section text defined by MCOL-56 and use it in all of the source code file $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-05-13 22:32:09,Update source code file header section,This task is to use the header section text defined by MCOL-56 and use it in all of the source code file,,0,0,0,0,0.0,Update source code file header section $end$ This task is to use the header section text defined by MCOL-56 and use it in all of the source code file $acceptance criteria:$,0,0,0,0,0,0,0,0.0,20,6,0.3,1,0.05,1,0.05,1,0.05,1,0.05
450,MCOL-574,New Feature,MCOL,2017-02-17 16:28:00,,0,Cross Engine step tries to use a bad UDS path for localhost,The MariaDB client library by default tries to use a UDS socket by default for 'localhost' instead of TCP. We need to force it to use TCP for now to keep the same behaviour as libdrizzle had.,,Cross Engine step tries to use a bad UDS path for localhost $end$ The MariaDB client library by default tries to use a UDS socket by default for 'localhost' instead of TCP. We need to force it to use TCP for now to keep the same behaviour as libdrizzle had. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Critical,8,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-02-17 16:28:00,Cross Engine step tries to use a bad UDS path for localhost,The MariaDB client library by default tries to use a UDS socket by default for 'localhost' instead of TCP. We need to force it to use TCP for now to keep the same behaviour as libdrizzle had.,,0,0,0,0,0.0,Cross Engine step tries to use a bad UDS path for localhost $end$ The MariaDB client library by default tries to use a UDS socket by default for 'localhost' instead of TCP. We need to force it to use TCP for now to keep the same behaviour as libdrizzle had. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,12,2,0.166667,2,0.166667,1,0.0833333,1,0.0833333,1,0.0833333
451,MCOL-579,Task,MCOL,2017-02-18 17:05:59,,0,Enabled harderning compile flags,"For security we should add the harderning flags used in MariaDB. The section in their CMake is:

{noformat}
# enable security hardening features, like most distributions do
# in our benchmarks that costs about ~1% of performance, depending on the load
IF(CMAKE_C_COMPILER_VERSION VERSION_LESS ""4.6"")
  SET(security_default OFF)
ELSE()
  SET(security_default ON)
ENDIF()
OPTION(SECURITY_HARDENED ""Use security-enhancing compiler features (stack protector, relro, etc)"" ${security_default})
IF(SECURITY_HARDENED)
  # security-enhancing flags
  MY_CHECK_AND_SET_COMPILER_FLAG(""-pie -fPIC"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-Wl,-z,relro,-z,now"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-fstack-protector --param=ssp-buffer-size=4"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-D_FORTIFY_SOURCE=2"" RELEASE RELWITHDEBINFO)
ENDIF()
{noformat}",,"Enabled harderning compile flags $end$ For security we should add the harderning flags used in MariaDB. The section in their CMake is:

{noformat}
# enable security hardening features, like most distributions do
# in our benchmarks that costs about ~1% of performance, depending on the load
IF(CMAKE_C_COMPILER_VERSION VERSION_LESS ""4.6"")
  SET(security_default OFF)
ELSE()
  SET(security_default ON)
ENDIF()
OPTION(SECURITY_HARDENED ""Use security-enhancing compiler features (stack protector, relro, etc)"" ${security_default})
IF(SECURITY_HARDENED)
  # security-enhancing flags
  MY_CHECK_AND_SET_COMPILER_FLAG(""-pie -fPIC"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-Wl,-z,relro,-z,now"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-fstack-protector --param=ssp-buffer-size=4"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-D_FORTIFY_SOURCE=2"" RELEASE RELWITHDEBINFO)
ENDIF()
{noformat} $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,12,,0,2,0,4,0,0,0,,0,850,2,0,0,2017-07-31 16:17:41,Enabled harderning compile flags,"For security we should add the harderning flags used in MariaDB. The section in their CMake is:

{noformat}
# enable security hardening features, like most distributions do
# in our benchmarks that costs about ~1% of performance, depending on the load
IF(CMAKE_C_COMPILER_VERSION VERSION_LESS ""4.6"")
  SET(security_default OFF)
ELSE()
  SET(security_default ON)
ENDIF()
OPTION(SECURITY_HARDENED ""Use security-enhancing compiler features (stack protector, relro, etc)"" ${security_default})
IF(SECURITY_HARDENED)
  # security-enhancing flags
  MY_CHECK_AND_SET_COMPILER_FLAG(""-pie -fPIC"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-Wl,-z,relro,-z,now"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-fstack-protector --param=ssp-buffer-size=4"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-D_FORTIFY_SOURCE=2"" RELEASE RELWITHDEBINFO)
ENDIF()
{noformat}",,0,0,0,0,0.0,"Enabled harderning compile flags $end$ For security we should add the harderning flags used in MariaDB. The section in their CMake is:

{noformat}
# enable security hardening features, like most distributions do
# in our benchmarks that costs about ~1% of performance, depending on the load
IF(CMAKE_C_COMPILER_VERSION VERSION_LESS ""4.6"")
  SET(security_default OFF)
ELSE()
  SET(security_default ON)
ENDIF()
OPTION(SECURITY_HARDENED ""Use security-enhancing compiler features (stack protector, relro, etc)"" ${security_default})
IF(SECURITY_HARDENED)
  # security-enhancing flags
  MY_CHECK_AND_SET_COMPILER_FLAG(""-pie -fPIC"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-Wl,-z,relro,-z,now"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-fstack-protector --param=ssp-buffer-size=4"")
  MY_CHECK_AND_SET_COMPILER_FLAG(""-D_FORTIFY_SOURCE=2"" RELEASE RELWITHDEBINFO)
ENDIF()
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,3911.18,13,2,0.153846,2,0.153846,1,0.0769231,1,0.0769231,1,0.0769231
452,MCOL-59,Task,MCOL,2016-05-15 03:24:07,,0,name change of Calpont.xml files,"Calpont.xml and Calpont.xml.singleserver files need to be changed to Columnstore.xml.

Plus all the code that references the file Calpont.xml need to be changed.",,"name change of Calpont.xml files $end$ Calpont.xml and Calpont.xml.singleserver files need to be changed to Columnstore.xml.

Plus all the code that references the file Calpont.xml need to be changed. $acceptance criteria:$",,David Hill,David Hill,Minor,7,,0,4,0,1,0,0,0,,0,850,2,0,0,2016-06-15 20:35:19,name change of Calpont.xml files,"Calpont.xml and Calpont.xml.singleserver files need to be changed to Columnstore.xml.

Plus all the code that references the file Calpont.xml need to be changed.",,0,0,0,0,0.0,"name change of Calpont.xml files $end$ Calpont.xml and Calpont.xml.singleserver files need to be changed to Columnstore.xml.

Plus all the code that references the file Calpont.xml need to be changed. $acceptance criteria:$",0,0,0,0,0,0,0,761.183,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
453,MCOL-593,New Feature,MCOL,2017-02-27 16:19:33,,0,support columnstore tables as slaves to innodb master tables,"Setting up a columnstore table in  a slave and replicating to it currently does not work and silently fails.  I believe this is because we use master / slave replication in a multi um setup to ensure synchronization of non columnstore lookup tables and columnstore table definitions where we obviously don't want the slave to process any dml since the data is already available.

Another challenge is that columnstore does not have a mode to ignore non supported constructs and so most innodb ddl will fail. This could be worked around by precreating the schema.  ",,"support columnstore tables as slaves to innodb master tables $end$ Setting up a columnstore table in  a slave and replicating to it currently does not work and silently fails.  I believe this is because we use master / slave replication in a multi um setup to ensure synchronization of non columnstore lookup tables and columnstore table definitions where we obviously don't want the slave to process any dml since the data is already available.

Another challenge is that columnstore does not have a mode to ignore non supported constructs and so most innodb ddl will fail. This could be worked around by precreating the schema.   $acceptance criteria:$",,David Thompson,David Thompson,Major,18,,0,3,1,1,0,0,0,,0,850,3,0,0,2019-04-15 13:45:22,support columnstore tables as slaves to innodb master tables,"Setting up a columnstore table in  a slave and replicating to it currently does not work and silently fails.  I believe this is because we use master / slave replication in a multi um setup to ensure synchronization of non columnstore lookup tables and columnstore table definitions where we obviously don't want the slave to process any dml since the data is already available.

Another challenge is that columnstore does not have a mode to ignore non supported constructs and so most innodb ddl will fail. This could be worked around by precreating the schema.  ",,0,0,0,0,0.0,"support columnstore tables as slaves to innodb master tables $end$ Setting up a columnstore table in  a slave and replicating to it currently does not work and silently fails.  I believe this is because we use master / slave replication in a multi um setup to ensure synchronization of non columnstore lookup tables and columnstore table definitions where we obviously don't want the slave to process any dml since the data is already available.

Another challenge is that columnstore does not have a mode to ignore non supported constructs and so most innodb ddl will fail. This could be worked around by precreating the schema.   $acceptance criteria:$",0,0,0,0,0,0,0,18645.4,23,2,0.0869565,2,0.0869565,2,0.0869565,2,0.0869565,2,0.0869565
454,MCOL-597,Task,MCOL,2017-02-28 21:08:35,,0,Merge with MariaDB 10.2,"Merge the Columnstore changes for MariaDB to Server version 10.2. This includes all the changes for Columnstore in the mariadb-columnstore-server directory as well as all the changes needed in the connector to use the MariaDB generated Windows Functions objects.

New functionality (Like CTE) in Server 10.2 will be stubbed out and issue a ""not supported"" error. These functionalities will be added by other MCOL.

There is some risk that other incompatibilities may be found and will need to be addressed.

See the document ""Merging Columnstore to MariaDB Server 10.2"" for more information.",,"Merge with MariaDB 10.2 $end$ Merge the Columnstore changes for MariaDB to Server version 10.2. This includes all the changes for Columnstore in the mariadb-columnstore-server directory as well as all the changes needed in the connector to use the MariaDB generated Windows Functions objects.

New functionality (Like CTE) in Server 10.2 will be stubbed out and issue a ""not supported"" error. These functionalities will be added by other MCOL.

There is some risk that other incompatibilities may be found and will need to be addressed.

See the document ""Merging Columnstore to MariaDB Server 10.2"" for more information. $acceptance criteria:$",,David Hall,David Hall,Major,11,,1,3,3,6,0,0,0,,0,850,3,0,0,2017-02-28 21:28:13,Merge with MariaDB 10.2,"Merge the Columnstore changes for MariaDB to Server version 10.2. This includes all the changes for Columnstore in the mariadb-columnstore-server directory as well as all the changes needed in the connector to use the MariaDB generated Windows Functions objects.

New functionality (Like CTE) in Server 10.2 will be stubbed out and issue a ""not supported"" error. These functionalities will be added by other MCOL.

There is some risk that other incompatibilities may be found and will need to be addressed.

See the document ""Merging Columnstore to MariaDB Server 10.2"" for more information.",,0,0,0,0,0.0,"Merge with MariaDB 10.2 $end$ Merge the Columnstore changes for MariaDB to Server version 10.2. This includes all the changes for Columnstore in the mariadb-columnstore-server directory as well as all the changes needed in the connector to use the MariaDB generated Windows Functions objects.

New functionality (Like CTE) in Server 10.2 will be stubbed out and issue a ""not supported"" error. These functionalities will be added by other MCOL.

There is some risk that other incompatibilities may be found and will need to be addressed.

See the document ""Merging Columnstore to MariaDB Server 10.2"" for more information. $acceptance criteria:$",0,0,0,0,0,0,1,0.316667,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
455,MCOL-598,New Feature,MCOL,2017-02-28 21:13:54,,0,Add CTE functionality to Columnstore,"CTE functionality is new to MariaDB 10.2. This is a complex feature and is compounded by the distributed nature of Columnstore. Changes will have to be effected in almost all processes. The full scope has to be determined and a design document created.

Sergey Petrunia is a resource to be consulted.",,"Add CTE functionality to Columnstore $end$ CTE functionality is new to MariaDB 10.2. This is a complex feature and is compounded by the distributed nature of Columnstore. Changes will have to be effected in almost all processes. The full scope has to be determined and a design document created.

Sergey Petrunia is a resource to be consulted. $acceptance criteria:$",,David Hall,David Hall,Major,7,,2,5,3,2,0,0,0,,0,850,5,0,0,2017-06-12 17:34:29,Add CTE functionality to Columnstore,"CTE functionality is new to MariaDB 10.2. This is a complex feature and is compounded by the distributed nature of Columnstore. Changes will have to be effected in almost all processes. The full scope has to be determined and a design document created.

Sergey Petrunia is a resource to be consulted.",,0,0,0,0,0.0,"Add CTE functionality to Columnstore $end$ CTE functionality is new to MariaDB 10.2. This is a complex feature and is compounded by the distributed nature of Columnstore. Changes will have to be effected in almost all processes. The full scope has to be determined and a design document created.

Sergey Petrunia is a resource to be consulted. $acceptance criteria:$",0,0,0,0,0,0,1,2492.33,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
456,MCOL-599,New Feature,MCOL,2017-02-28 21:16:30,,0,JSON Functions,"MariaDB 10.2 has added JSON Functions to its arsenal. Columnstore should also support these functions. Distributed JSON functions would be preferred, but perhaps a way could be found to shorten the cycle and not distribute them (Just a guess). ",,"JSON Functions $end$ MariaDB 10.2 has added JSON Functions to its arsenal. Columnstore should also support these functions. Distributed JSON functions would be preferred, but perhaps a way could be found to shorten the cycle and not distribute them (Just a guess).  $acceptance criteria:$",,David Hall,David Hall,Major,7,,2,5,3,2,0,0,0,,0,850,5,0,0,2017-06-12 17:40:16,JSON Functions,"MariaDB 10.2 has added JSON Functions to its arsenal. Columnstore should also support these functions. Distributed JSON functions would be preferred, but perhaps a way could be found to shorten the cycle and not distribute them (Just a guess). ",,0,0,0,0,0.0,"JSON Functions $end$ MariaDB 10.2 has added JSON Functions to its arsenal. Columnstore should also support these functions. Distributed JSON functions would be preferred, but perhaps a way could be found to shorten the cycle and not distribute them (Just a guess).  $acceptance criteria:$",0,0,0,0,0,0,1,2492.38,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
457,MCOL-6,Task,MCOL,2016-05-02 16:46:06,MCOL-10,0,MariaDB ColumnStore QuickStart Guide,"(1) Update QuickStart Guide that refers to new directory structure and engine name-  David Hill

(2) Once doc/pdf version is ready from David Hill - add it as content in KB - Ian
",,"MariaDB ColumnStore QuickStart Guide $end$ (1) Update QuickStart Guide that refers to new directory structure and engine name-  David Hill

(2) Once doc/pdf version is ready from David Hill - add it as content in KB - Ian
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,24,,0,5,0,1,0,4,0,,0,850,1,4,0,2016-05-20 04:20:17,MariaDB ColumnStore QuickStart Guide,"(1) Update QuickStart Guide that refers to new directory structure and engine name-  David Hill

(2) Once doc/pdf version is ready from David Hill - add it as content in KB - Ian
",,0,0,0,0,0.0,"MariaDB ColumnStore QuickStart Guide $end$ (1) Update QuickStart Guide that refers to new directory structure and engine name-  David Hill

(2) Once doc/pdf version is ready from David Hill - add it as content in KB - Ian
 $acceptance criteria:$",0,0,0,0,0,0,0,419.567,4,2,0.5,0,0.0,0,0.0,0,0.0,0,0.0
458,MCOL-601,New Feature,MCOL,2017-02-28 21:27:16,,0,New CREATE USER functions,"MariaDB Server 10.2 added the following functions which need to be implemented in Columnstore:

The SHOW CREATE USER statement was introduced.
New CREATE USER options for limiting resource usage and tls/ssl.
New ALTER USER statement",,"New CREATE USER functions $end$ MariaDB Server 10.2 added the following functions which need to be implemented in Columnstore:

The SHOW CREATE USER statement was introduced.
New CREATE USER options for limiting resource usage and tls/ssl.
New ALTER USER statement $acceptance criteria:$",,David Hall,David Hall,Major,3,,0,3,0,2,0,0,0,,0,850,3,0,0,2017-06-12 17:35:45,New CREATE USER functions,"MariaDB Server 10.2 added the following functions which need to be implemented in Columnstore:

The SHOW CREATE USER statement was introduced.
New CREATE USER options for limiting resource usage and tls/ssl.
New ALTER USER statement",,0,0,0,0,0.0,"New CREATE USER functions $end$ MariaDB Server 10.2 added the following functions which need to be implemented in Columnstore:

The SHOW CREATE USER statement was introduced.
New CREATE USER options for limiting resource usage and tls/ssl.
New ALTER USER statement $acceptance criteria:$",0,0,0,0,0,0,1,2492.13,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
459,MCOL-609,Task,MCOL,2017-03-07 17:42:24,,0,merge server 10.1.22 release,When the 10.1.22 server release is available (eta week of 3/14) should merge down and prepare for 1.0.8 release.,,merge server 10.1.22 release $end$ When the 10.1.22 server release is available (eta week of 3/14) should merge down and prepare for 1.0.8 release. $acceptance criteria:$,,David Thompson,David Thompson,Major,7,,0,2,0,1,0,0,0,,0,850,2,0,0,2017-03-13 17:49:54,merge server 10.1.22 release,When the 10.1.22 server release is available (eta week of 3/14) should merge down and prepare for 1.0.8 release.,,0,0,0,0,0.0,merge server 10.1.22 release $end$ When the 10.1.22 server release is available (eta week of 3/14) should merge down and prepare for 1.0.8 release. $acceptance criteria:$,0,0,0,0,0,0,0,144.117,24,2,0.0833333,2,0.0833333,2,0.0833333,2,0.0833333,2,0.0833333
460,MCOL-61,Task,MCOL,2016-05-19 14:26:18,,0,Create AMI for MariaDB ColumnStore,"Create AMI for MariaDB ColumnStore - make sure that the README does not refer to Calpont.

e.g. instead of /usr/local/Calpont - it should refer to /usr/local/MariaDB/Columnstore",,"Create AMI for MariaDB ColumnStore $end$ Create AMI for MariaDB ColumnStore - make sure that the README does not refer to Calpont.

e.g. instead of /usr/local/Calpont - it should refer to /usr/local/MariaDB/Columnstore $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,10,,0,9,0,3,0,0,0,,0,850,6,0,0,2016-10-25 20:25:32,Create AMI for MariaDB ColumnStore,"Create AMI for MariaDB ColumnStore - make sure that the README does not refer to Calpont.

e.g. instead of /usr/local/Calpont - it should refer to /usr/local/MariaDB/Columnstore",,0,0,0,0,0.0,"Create AMI for MariaDB ColumnStore $end$ Create AMI for MariaDB ColumnStore - make sure that the README does not refer to Calpont.

e.g. instead of /usr/local/Calpont - it should refer to /usr/local/MariaDB/Columnstore $acceptance criteria:$",0,0,0,0,0,0,1,3821.98,21,6,0.285714,1,0.047619,1,0.047619,1,0.047619,1,0.047619
461,MCOL-610,Task,MCOL,2017-03-07 21:24:14,,0,1.08 backport MCOL-480 warning error reported after upgrade to 1.0.6,,,1.08 backport MCOL-480 warning error reported after upgrade to 1.0.6 $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,13,,1,3,1,2,0,1,0,,0,850,3,0,0,2017-03-07 21:24:14,backport MCOL-480 to 1.0.8,,,1,0,0,8,1.0,backport MCOL-480 to 1.0.8 $end$ $acceptance criteria:$,1,1,1,0,0,0,1,0.0,25,2,0.08,2,0.08,2,0.08,2,0.08,2,0.08
462,MCOL-611,Task,MCOL,2017-03-07 21:25:44,,0,"1.0.8 backport MCOL-353 with 6 PMs, cpimport has an long lag time",,,"1.0.8 backport MCOL-353 with 6 PMs, cpimport has an long lag time $end$ $acceptance criteria:$",,David Thompson,David Thompson,Major,8,,1,2,1,2,0,2,0,,0,850,2,0,0,2017-03-07 21:25:44,backport MCOL-353 to 1.0.8,,,2,0,0,12,1.42857,backport MCOL-353 to 1.0.8 $end$ $acceptance criteria:$,2,1,1,1,0,0,1,0.0,26,3,0.115385,3,0.115385,2,0.0769231,2,0.0769231,2,0.0769231
463,MCOL-613,Sub-Task,MCOL,2017-03-08 16:02:11,,0,enforce limitation of postCfg needing to be run on pm1 - fix for 1.0.8,,,enforce limitation of postCfg needing to be run on pm1 - fix for 1.0.8 $end$ $acceptance criteria:$,,David Hill,David Hill,Major,13,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-03-08 16:02:11,enforce limitation of postCfg needing to be run on pm1 - fix for 1.0.8,,,0,0,0,0,0.0,enforce limitation of postCfg needing to be run on pm1 - fix for 1.0.8 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,16,2,0.125,0,0.0,0,0.0,0,0.0,0,0.0
464,MCOL-62,Task,MCOL,2016-05-20 14:44:05,,0,112 Varbinary test fails,"112 Varbinary Test:                   Failed (Check logs/diff.txt for diffs)

 cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log
diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
—
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB",,"112 Varbinary test fails $end$ 112 Varbinary Test:                   Failed (Check logs/diff.txt for diffs)

 cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log
diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
—
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,6,,0,2,0,1,0,0,0,,0,850,2,0,0,2016-05-20 14:44:05,112 Varbinary test fails,"112 Varbinary Test:                   Failed (Check logs/diff.txt for diffs)

 cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log
diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
—
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB",,0,0,0,0,0.0,"112 Varbinary test fails $end$ 112 Varbinary Test:                   Failed (Check logs/diff.txt for diffs)

 cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log
diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
—
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB $acceptance criteria:$",0,0,0,0,0,0,0,0.0,22,6,0.272727,1,0.0454545,1,0.0454545,1,0.0454545,1,0.0454545
465,MCOL-636,New Feature,MCOL,2017-03-24 14:44:15,,0,Performance improvement with string handling,With the blob feature I left a marker in ha_calpont_impl.cpp:470 to note that we are doing string copies where just using the pointer will do. There may be other places where we might be doing this too.,,Performance improvement with string handling $end$ With the blob feature I left a marker in ha_calpont_impl.cpp:470 to note that we are doing string copies where just using the pointer will do. There may be other places where we might be doing this too. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Minor,10,,0,2,0,5,0,0,0,,0,850,2,0,0,2017-07-18 13:54:20,Performance improvement with string handling,With the blob feature I left a marker in ha_calpont_impl.cpp:470 to note that we are doing string copies where just using the pointer will do. There may be other places where we might be doing this too.,,0,0,0,0,0.0,Performance improvement with string handling $end$ With the blob feature I left a marker in ha_calpont_impl.cpp:470 to note that we are doing string copies where just using the pointer will do. There may be other places where we might be doing this too. $acceptance criteria:$,0,0,0,0,0,0,1,2783.17,14,2,0.142857,2,0.142857,1,0.0714286,1,0.0714286,1,0.0714286
466,MCOL-638,Task,MCOL,2017-03-24 22:26:12,,0,create a boost install for centos 6,"For centos6, it would provide a much simpler setup experience if we could provide a pre-built rpm of the necessary boost libraries so that customers don't need to go through a compilation process especially on production servers.",,"create a boost install for centos 6 $end$ For centos6, it would provide a much simpler setup experience if we could provide a pre-built rpm of the necessary boost libraries so that customers don't need to go through a compilation process especially on production servers. $acceptance criteria:$",,David Thompson,David Thompson,Major,4,,0,3,0,3,0,0,0,,0,850,3,0,0,2017-03-24 22:26:12,create a boost install for centos 6,"For centos6, it would provide a much simpler setup experience if we could provide a pre-built rpm of the necessary boost libraries so that customers don't need to go through a compilation process especially on production servers.",,0,0,0,0,0.0,"create a boost install for centos 6 $end$ For centos6, it would provide a much simpler setup experience if we could provide a pre-built rpm of the necessary boost libraries so that customers don't need to go through a compilation process especially on production servers. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,27,4,0.148148,4,0.148148,3,0.111111,2,0.0740741,2,0.0740741
467,MCOL-641,New Feature,MCOL,2017-03-27 13:39:33,MCOL-1049,0,Full DECIMAL support in ColumnStore,"MariaDB ColumnStore supports DECIMAL with some limitations:

1. We do not support the full DECIMAL range that is in MariaDB

2. In several places in the code we convert the DECIMAL to DOUBLE during execution therefore losing precision

Implementing this will likely require the following:

* Implementation of methods to handle MariaDB's DECIMAL format
* Support for a longer than 8-byte numeric column type (there is an InfiniDB tree with work for this already)
* Modification of the primitives processor for the math
* Modification of the function expression processor to handle the new type
* Version upgrade support for DECIMAL from the current form to the new form",,"Full DECIMAL support in ColumnStore $end$ MariaDB ColumnStore supports DECIMAL with some limitations:

1. We do not support the full DECIMAL range that is in MariaDB

2. In several places in the code we convert the DECIMAL to DOUBLE during execution therefore losing precision

Implementing this will likely require the following:

* Implementation of methods to handle MariaDB's DECIMAL format
* Support for a longer than 8-byte numeric column type (there is an InfiniDB tree with work for this already)
* Modification of the primitives processor for the math
* Modification of the function expression processor to handle the new type
* Version upgrade support for DECIMAL from the current form to the new form $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,66,,46,4,51,16,0,0,27,,0,850,1,0,0,2020-01-20 18:14:30,Full DECIMAL support in ColumnStore,"MariaDB ColumnStore supports DECIMAL with some limitations:

1. We do not support the full DECIMAL range that is in MariaDB

2. In several places in the code we convert the DECIMAL to DOUBLE during execution therefore losing precision

Implementing this will likely require the following:

* Implementation of methods to handle MariaDB's DECIMAL format
* Support for a longer than 8-byte numeric column type (there is an InfiniDB tree with work for this already)
* Modification of the primitives processor for the math
* Modification of the function expression processor to handle the new type
* Version upgrade support for DECIMAL from the current form to the new form",,0,0,0,0,0.0,"Full DECIMAL support in ColumnStore $end$ MariaDB ColumnStore supports DECIMAL with some limitations:

1. We do not support the full DECIMAL range that is in MariaDB

2. In several places in the code we convert the DECIMAL to DOUBLE during execution therefore losing precision

Implementing this will likely require the following:

* Implementation of methods to handle MariaDB's DECIMAL format
* Support for a longer than 8-byte numeric column type (there is an InfiniDB tree with work for this already)
* Modification of the primitives processor for the math
* Modification of the function expression processor to handle the new type
* Version upgrade support for DECIMAL from the current form to the new form $acceptance criteria:$",0,0,0,0,0,0,1,24700.6,15,2,0.133333,2,0.133333,1,0.0666667,1,0.0666667,1,0.0666667
468,MCOL-642,New Feature,MCOL,2017-03-27 16:18:11,,0,Add BLOB/TEXT detection,"We need to detect the difference between BLOB and TEXT so that cpimport can use string data for text and hex data for blob.

This can be done by having two distinct types in our DDL instead of making it all BLOB.",,"Add BLOB/TEXT detection $end$ We need to detect the difference between BLOB and TEXT so that cpimport can use string data for text and hex data for blob.

This can be done by having two distinct types in our DDL instead of making it all BLOB. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,8,,0,2,1,4,0,0,0,,0,850,1,0,0,2017-03-31 16:30:14,Add BLOB/TEXT detection,"We need to detect the difference between BLOB and TEXT so that cpimport can use string data for text and hex data for blob.

This can be done by having two distinct types in our DDL instead of making it all BLOB.",,0,0,0,0,0.0,"Add BLOB/TEXT detection $end$ We need to detect the difference between BLOB and TEXT so that cpimport can use string data for text and hex data for blob.

This can be done by having two distinct types in our DDL instead of making it all BLOB. $acceptance criteria:$",0,0,0,0,0,0,1,96.2,16,2,0.125,2,0.125,1,0.0625,1,0.0625,1,0.0625
469,MCOL-657,New Feature,MCOL,2017-04-06 10:48:39,,0,Support NULL safe equals (<=>),"The NULL safe equals operator (<=>) gives a not supported error ""Error Code: 122. IDB-1001: Function '<=>' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function."".

This should be implemented. We could implement it by expanding it to:
{noformat}
a = b OR (a IS NULL AND b IS NULL)
{noformat}

As outlined in: https://mariadb.com/kb/en/mariadb/null-safe-equal/",,"Support NULL safe equals (<=>) $end$ The NULL safe equals operator (<=>) gives a not supported error ""Error Code: 122. IDB-1001: Function '<=>' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function."".

This should be implemented. We could implement it by expanding it to:
{noformat}
a = b OR (a IS NULL AND b IS NULL)
{noformat}

As outlined in: https://mariadb.com/kb/en/mariadb/null-safe-equal/ $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,7,,1,4,1,2,0,0,0,,0,850,3,0,0,2017-05-08 18:31:49,Support NULL safe equals (<=>),"The NULL safe equals operator (<=>) gives a not supported error ""Error Code: 122. IDB-1001: Function '<=>' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function."".

This should be implemented. We could implement it by expanding it to:
{noformat}
a = b OR (a IS NULL AND b IS NULL)
{noformat}

As outlined in: https://mariadb.com/kb/en/mariadb/null-safe-equal/",,0,0,0,0,0.0,"Support NULL safe equals (<=>) $end$ The NULL safe equals operator (<=>) gives a not supported error ""Error Code: 122. IDB-1001: Function '<=>' can only be used in the outermost select or order by clause and cannot be used in conjunction with an aggregate function."".

This should be implemented. We could implement it by expanding it to:
{noformat}
a = b OR (a IS NULL AND b IS NULL)
{noformat}

As outlined in: https://mariadb.com/kb/en/mariadb/null-safe-equal/ $acceptance criteria:$",0,0,0,0,0,0,1,775.717,17,2,0.117647,2,0.117647,1,0.0588235,1,0.0588235,1,0.0588235
470,MCOL-664,New Feature,MCOL,2017-04-15 05:20:50,,0,TEXT columns need to support the same functions as VARCHAR,At the moment we disable them in the same way we do with BLOB,,TEXT columns need to support the same functions as VARCHAR $end$ At the moment we disable them in the same way we do with BLOB $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,1,1,3,0,0,0,,0,850,1,0,0,2017-04-15 05:20:50,TEXT columns need to support the same functions as VARCHAR,At the moment we disable them in the same way we do with BLOB,,0,0,0,0,0.0,TEXT columns need to support the same functions as VARCHAR $end$ At the moment we disable them in the same way we do with BLOB $acceptance criteria:$,0,0,0,0,0,0,1,0.0,18,2,0.111111,2,0.111111,1,0.0555556,1,0.0555556,1,0.0555556
471,MCOL-704,Task,MCOL,2017-05-08 16:50:36,,0,backport to 1.0.9: MCOL-686 Using BETWEEN together with date functions in WHERE clause 100x slower than InfiniDB,,,backport to 1.0.9: MCOL-686 Using BETWEEN together with date functions in WHERE clause 100x slower than InfiniDB $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,6,,1,2,1,2,0,0,0,,0,850,2,0,0,2017-05-08 18:32:38,backport to 1.0.9: MCOL-686 Using BETWEEN together with date functions in WHERE clause 100x slower than InfiniDB,,,0,0,0,0,0.0,backport to 1.0.9: MCOL-686 Using BETWEEN together with date functions in WHERE clause 100x slower than InfiniDB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1.7,28,4,0.142857,4,0.142857,3,0.107143,2,0.0714286,2,0.0714286
472,MCOL-706,Task,MCOL,2017-05-08 18:33:46,,0,Merge 10.1.23 into 1.0.9,We need to merge 10.1.23 and cherry pick MDEV-12673,,Merge 10.1.23 into 1.0.9 $end$ We need to merge 10.1.23 and cherry pick MDEV-12673 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,11,,0,4,0,2,0,0,0,,0,850,4,0,0,2017-05-08 18:33:46,Merge 10.1.23 into 1.0.9,We need to merge 10.1.23 and cherry pick MDEV-12673,,0,0,0,0,0.0,Merge 10.1.23 into 1.0.9 $end$ We need to merge 10.1.23 and cherry pick MDEV-12673 $acceptance criteria:$,0,0,0,0,0,0,1,0.0,19,2,0.105263,2,0.105263,1,0.0526316,1,0.0526316,1,0.0526316
473,MCOL-716,New Feature,MCOL,2017-05-11 19:36:37,MCOL-1049,0,support non alphanumeric characters for table and column names,"Currently columnstore restricts object names to be alphanumeric and underscore, i.e. ""A-Z a-z 0-9 _"".  This precludes use of accented characters for europeans and most non roman languages from using a more natural name.

Created from: https://mariadb.com/kb/en/mariadb/object-name-naming-rule/",,"support non alphanumeric characters for table and column names $end$ Currently columnstore restricts object names to be alphanumeric and underscore, i.e. ""A-Z a-z 0-9 _"".  This precludes use of accented characters for europeans and most non roman languages from using a more natural name.

Created from: https://mariadb.com/kb/en/mariadb/object-name-naming-rule/ $acceptance criteria:$",,David Thompson,David Thompson,Major,7,,0,3,0,2,0,0,0,,0,850,2,0,0,2018-01-29 12:36:59,support non alphanumeric characters for table and column names,"Currently columnstore restricts object names to be alphanumeric and underscore, i.e. ""A-Z a-z 0-9 _"".  This precludes use of accented characters for europeans and most non roman languages from using a more natural name.

Created from: https://mariadb.com/kb/en/mariadb/object-name-naming-rule/",,0,0,0,0,0.0,"support non alphanumeric characters for table and column names $end$ Currently columnstore restricts object names to be alphanumeric and underscore, i.e. ""A-Z a-z 0-9 _"".  This precludes use of accented characters for europeans and most non roman languages from using a more natural name.

Created from: https://mariadb.com/kb/en/mariadb/object-name-naming-rule/ $acceptance criteria:$",0,0,0,0,0,0,1,6305.0,29,4,0.137931,4,0.137931,3,0.103448,2,0.0689655,2,0.0689655
474,MCOL-723,New Feature,MCOL,2017-05-20 22:44:36,,0,MariabDB ColumnStore Cluster tester tool,"create a script that will test the following items on a cluster (set of servers) where ColumnStore is being installed.

Node Ping test
	Node SSH test
	ColumnStore Port test
	OS version
	Locale settings
	Firewall settings
     Date/time settings
	Dependent packages installed
     For non-root user install - test permissions on /tmp and /dev/shm",,"MariabDB ColumnStore Cluster tester tool $end$ create a script that will test the following items on a cluster (set of servers) where ColumnStore is being installed.

Node Ping test
	Node SSH test
	ColumnStore Port test
	OS version
	Locale settings
	Firewall settings
     Date/time settings
	Dependent packages installed
     For non-root user install - test permissions on /tmp and /dev/shm $acceptance criteria:$",,David Hill,David Hill,Minor,4,,0,4,0,1,0,0,0,,0,850,2,0,0,2017-05-22 17:03:59,MariabDB ColumnStore Cluster tester tool,"create a script that will test the following items on a cluster (set of servers) where ColumnStore is being installed.

Node Ping test
	Node SSH test
	ColumnStore Port test
	OS version
	Locale settings
	Firewall settings
     Date/time settings
	Dependent packages installed
     For non-root user install - test permissions on /tmp and /dev/shm",,0,0,0,0,0.0,"MariabDB ColumnStore Cluster tester tool $end$ create a script that will test the following items on a cluster (set of servers) where ColumnStore is being installed.

Node Ping test
	Node SSH test
	ColumnStore Port test
	OS version
	Locale settings
	Firewall settings
     Date/time settings
	Dependent packages installed
     For non-root user install - test permissions on /tmp and /dev/shm $acceptance criteria:$",0,0,0,0,0,0,0,42.3167,17,2,0.117647,0,0.0,0,0.0,0,0.0,0,0.0
475,MCOL-724,Task,MCOL,2017-05-22 16:56:49,,0,merge down 10.2.6,I believe the final commit for 10.2.6 is ca7cf69cb1328 but we may as well just pull down the latest as it's highly unlikely we'd release before a 10.2.7. ,,merge down 10.2.6 $end$ I believe the final commit for 10.2.6 is ca7cf69cb1328 but we may as well just pull down the latest as it's highly unlikely we'd release before a 10.2.7.  $acceptance criteria:$,,David Thompson,David Thompson,Major,8,,0,3,0,2,0,0,0,,0,850,3,0,0,2017-05-22 16:56:53,merge down 10.2.6,I believe the final commit for 10.2.6 is ca7cf69cb1328 but we may as well just pull down the latest as it's highly unlikely we'd release before a 10.2.7. ,,0,0,0,0,0.0,merge down 10.2.6 $end$ I believe the final commit for 10.2.6 is ca7cf69cb1328 but we may as well just pull down the latest as it's highly unlikely we'd release before a 10.2.7.  $acceptance criteria:$,0,0,0,0,0,0,1,0.0,30,4,0.133333,4,0.133333,3,0.1,2,0.0666667,2,0.0666667
476,MCOL-732,Task,MCOL,2017-05-31 20:02:41,,0,Merge Server 10.1.24,,,Merge Server 10.1.24 $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,6,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-05-31 20:02:41,Merge Server 10.1.24,,,0,0,0,0,0.0,Merge Server 10.1.24 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,31,4,0.129032,4,0.129032,3,0.0967742,2,0.0645161,2,0.0645161
477,MCOL-739,Sub-Task,MCOL,2017-06-05 14:01:14,,0,Add system catalog network call,We need a call to get the system catalog over a network. Probably either via DBRM or WriteEngine process.,,Add system catalog network call $end$ We need a call to get the system catalog over a network. Probably either via DBRM or WriteEngine process. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,2,0,11,0,0,0,,0,850,2,0,0,2017-06-05 14:01:14,Add system catalog network call,We need a call to get the system catalog over a network. Probably either via DBRM or WriteEngine process.,,0,0,0,0,0.0,Add system catalog network call $end$ We need a call to get the system catalog over a network. Probably either via DBRM or WriteEngine process. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,20,2,0.1,2,0.1,1,0.05,1,0.05,1,0.05
478,MCOL-740,Sub-Task,MCOL,2017-06-05 14:07:38,,0,Create bulk write API,We need the library to be able to have a bulk write API to act similar to cpimport,,Create bulk write API $end$ We need the library to be able to have a bulk write API to act similar to cpimport $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,1,0,11,0,0,0,,0,850,1,0,0,2017-06-05 14:07:38,Create bulk write API,We need the library to be able to have a bulk write API to act similar to cpimport,,0,0,0,0,0.0,Create bulk write API $end$ We need the library to be able to have a bulk write API to act similar to cpimport $acceptance criteria:$,0,0,0,0,0,0,1,0.0,21,2,0.0952381,2,0.0952381,1,0.047619,1,0.047619,1,0.047619
479,MCOL-741,New Feature,MCOL,2017-06-05 14:08:21,,0,Create Avro API,We need an Avro API to stream Avro writes to ColumnStore,,Create Avro API $end$ We need an Avro API to stream Avro writes to ColumnStore $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,16,,0,2,0,2,0,0,0,,0,850,1,0,0,2017-09-25 21:46:08,Create Avro API,We need an Avro API to stream Avro writes to ColumnStore,,0,0,0,0,0.0,Create Avro API $end$ We need an Avro API to stream Avro writes to ColumnStore $acceptance criteria:$,0,0,0,0,0,0,1,2695.62,22,2,0.0909091,2,0.0909091,1,0.0454545,1,0.0454545,1,0.0454545
480,MCOL-742,New Feature,MCOL,2017-06-05 14:10:49,,0,Create Kafka consumer application,,,Create Kafka consumer application $end$ $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,10,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-09-25 21:46:38,Create Kafka consumer application,,,0,0,0,0,0.0,Create Kafka consumer application $end$ $acceptance criteria:$,0,0,0,0,0,0,1,2695.58,23,2,0.0869565,2,0.0869565,1,0.0434783,1,0.0434783,1,0.0434783
481,MCOL-743,Sub-Task,MCOL,2017-06-05 14:11:55,,0,Write native API documentation,A python-sphinx based rst documentation build system is already in the codebase to generate HTML and PDF documentation. The documentation just needs to be written and we need to add entries into the KB.,,Write native API documentation $end$ A python-sphinx based rst documentation build system is already in the codebase to generate HTML and PDF documentation. The documentation just needs to be written and we need to add entries into the KB. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,3,,0,1,0,11,0,0,0,,0,850,1,0,0,2017-06-05 14:11:55,Write native API documentation,A python-sphinx based rst documentation build system is already in the codebase to generate HTML and PDF documentation. The documentation just needs to be written and we need to add entries into the KB.,,0,0,0,0,0.0,Write native API documentation $end$ A python-sphinx based rst documentation build system is already in the codebase to generate HTML and PDF documentation. The documentation just needs to be written and we need to add entries into the KB. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,24,2,0.0833333,2,0.0833333,1,0.0416667,1,0.0416667,1,0.0416667
482,MCOL-745,Sub-Task,MCOL,2017-06-05 21:51:31,,0,Modify OAM glusterctl functionality,move functionality that used to exist in external shell scripts to liboamcpp,,Modify OAM glusterctl functionality $end$ move functionality that used to exist in external shell scripts to liboamcpp $acceptance criteria:$,,Ben Thompson,Ben Thompson,Major,2,,0,0,0,13,0,0,0,,0,850,0,0,0,2017-06-05 21:51:31,Modify OAM glusterctl functionality,move functionality that used to exist in external shell scripts to liboamcpp,,0,0,0,0,0.0,Modify OAM glusterctl functionality $end$ move functionality that used to exist in external shell scripts to liboamcpp $acceptance criteria:$,0,0,0,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
483,MCOL-746,Sub-Task,MCOL,2017-06-05 21:54:44,,0,Create glusterconf functionality in OAM,"create functionality that used to exist in external shell scripts to liboamcpp
",,"Create glusterconf functionality in OAM $end$ create functionality that used to exist in external shell scripts to liboamcpp
 $acceptance criteria:$",,Ben Thompson,Ben Thompson,Major,2,,0,0,0,13,0,0,0,,0,850,0,0,0,2017-06-05 21:54:44,Create glusterconf functionality in OAM,"create functionality that used to exist in external shell scripts to liboamcpp
",,0,0,0,0,0.0,"Create glusterconf functionality in OAM $end$ create functionality that used to exist in external shell scripts to liboamcpp
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
484,MCOL-747,Sub-Task,MCOL,2017-06-05 21:58:15,,0,postConfigure Data Redundancy section addition,Add the Data Redundancy section to postConfigure to configure and start gluster data redundancy during ColumnStore install,,postConfigure Data Redundancy section addition $end$ Add the Data Redundancy section to postConfigure to configure and start gluster data redundancy during ColumnStore install $acceptance criteria:$,,Ben Thompson,Ben Thompson,Major,2,,0,0,0,13,0,0,0,,0,850,0,0,0,2017-06-05 21:58:15,postConfigure Data Redundancy section addition,Add the Data Redundancy section to postConfigure to configure and start gluster data redundancy during ColumnStore install,,0,0,0,0,0.0,postConfigure Data Redundancy section addition $end$ Add the Data Redundancy section to postConfigure to configure and start gluster data redundancy during ColumnStore install $acceptance criteria:$,0,0,0,0,0,0,1,0.0,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
485,MCOL-748,Sub-Task,MCOL,2017-06-05 22:01:56,,0,Documentation,,,Documentation $end$ $acceptance criteria:$,,Ben Thompson,Ben Thompson,Major,2,,0,0,0,13,0,0,0,,0,850,0,0,0,2017-06-05 22:01:56,Documentation,,,0,0,0,0,0.0,Documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
486,MCOL-750,New Feature,MCOL,2017-06-06 14:15:38,,0,make the remote server install scripts run faster by checking for ssh/scp return codes,"A change was made to the remote_command.sh script for MCOL-723 that made it return faster to the caller by adding a check for return codes from ssh and adding 'ssh -v' option.

The same changes need to be applied to the other installer scripts to make the install and addModule functions faster.

These scripts will be changed:

user_installer.sh
performance_installer.sh
binary_installer.sh",,"make the remote server install scripts run faster by checking for ssh/scp return codes $end$ A change was made to the remote_command.sh script for MCOL-723 that made it return faster to the caller by adding a check for return codes from ssh and adding 'ssh -v' option.

The same changes need to be applied to the other installer scripts to make the install and addModule functions faster.

These scripts will be changed:

user_installer.sh
performance_installer.sh
binary_installer.sh $acceptance criteria:$",,David Hill,David Hill,Trivial,24,,0,8,0,6,0,1,0,,0,850,8,0,0,2017-06-06 14:15:38,make the remove server install scripts run faster by checking for ssh/scp return codes,"A change was made to the remote_command.sh script for MCOL-723 that made it return faster to the caller by adding a check for return codes from ssh and adding 'ssh -v' option.

The same changes need to be applied to the other installer scripts to make the install and addModule functions faster.

These scripts will be changed:

user_installer.sh
performance_installer.sh
binary_installer.sh",,1,0,0,2,0.012987,"make the remove server install scripts run faster by checking for ssh/scp return codes $end$ A change was made to the remote_command.sh script for MCOL-723 that made it return faster to the caller by adding a check for return codes from ssh and adding 'ssh -v' option.

The same changes need to be applied to the other installer scripts to make the install and addModule functions faster.

These scripts will be changed:

user_installer.sh
performance_installer.sh
binary_installer.sh $acceptance criteria:$",1,1,0,0,0,0,1,0.0,18,2,0.111111,0,0.0,0,0.0,0,0.0,0,0.0
487,MCOL-751,Sub-Task,MCOL,2017-06-06 17:22:16,,0,define the api,,,define the api $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,1,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:22:16,define the api,,,0,0,0,0,0.0,define the api $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,32,4,0.125,4,0.125,3,0.09375,2,0.0625,2,0.0625
488,MCOL-752,Sub-Task,MCOL,2017-06-06 17:22:37,,0,modify the connector,,,modify the connector $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,1,0,11,0,0,0,,0,850,1,0,0,2017-06-06 17:22:37,modify the connector,,,0,0,0,0,0.0,modify the connector $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,33,4,0.121212,4,0.121212,3,0.0909091,2,0.0606061,2,0.0606061
489,MCOL-753,Sub-Task,MCOL,2017-06-06 17:22:58,,0,modify exemgr,,,modify exemgr $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,1,0,11,0,0,0,,0,850,1,0,0,2017-06-06 17:22:58,modify exemgr,,,0,0,0,0,0.0,modify exemgr $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,34,4,0.117647,4,0.117647,3,0.0882353,2,0.0588235,2,0.0588235
490,MCOL-754,Sub-Task,MCOL,2017-06-06 17:23:24,,0,modify primproc,,,modify primproc $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:23:24,modify primproc,,,0,0,0,0,0.0,modify primproc $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,35,4,0.114286,4,0.114286,3,0.0857143,2,0.0571429,2,0.0571429
491,MCOL-755,Sub-Task,MCOL,2017-06-06 17:23:49,,0,modify joblist,,,modify joblist $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:23:49,modify joblist,,,0,0,0,0,0.0,modify joblist $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,36,4,0.111111,4,0.111111,3,0.0833333,2,0.0555556,2,0.0555556
492,MCOL-756,Sub-Task,MCOL,2017-06-06 17:24:11,,0,design memory allocation,,,design memory allocation $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:24:11,design memory allocation,,,0,0,0,0,0.0,design memory allocation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,37,4,0.108108,4,0.108108,3,0.0810811,2,0.0540541,2,0.0540541
493,MCOL-757,Sub-Task,MCOL,2017-06-06 17:24:40,,0,transfer the context to pms,,,transfer the context to pms $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:24:40,transfer the context to pms,,,0,0,0,0,0.0,transfer the context to pms $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,38,4,0.105263,4,0.105263,3,0.0789474,2,0.0526316,2,0.0526316
494,MCOL-758,Sub-Task,MCOL,2017-06-06 17:25:04,,0,build UDAF framework,,,build UDAF framework $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:25:04,build UDAF framework,,,0,0,0,0,0.0,build UDAF framework $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,39,4,0.102564,4,0.102564,3,0.0769231,2,0.0512821,2,0.0512821
495,MCOL-759,Sub-Task,MCOL,2017-06-06 17:25:29,,0,build UDAnF framework,,,build UDAnF framework $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,2,,0,0,0,11,0,0,0,,0,850,0,0,0,2017-06-06 17:25:29,build UDAnF framework,,,0,0,0,0,0.0,build UDAnF framework $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,40,4,0.1,4,0.1,3,0.075,2,0.05,2,0.05
496,MCOL-769,Sub-Task,MCOL,2017-06-12 15:38:24,,0,Create new writeengine batch write command,"I had originally planned to use WE_SVR_BATCH_INSERT but it is far too verbose for what I need and the performance wouldn't be great.

I need to create a similar command which uses binary data and removes the verbose command messaging.",,"Create new writeengine batch write command $end$ I had originally planned to use WE_SVR_BATCH_INSERT but it is far too verbose for what I need and the performance wouldn't be great.

I need to create a similar command which uses binary data and removes the verbose command messaging. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,16,,0,6,1,11,0,0,0,,0,850,6,0,0,2017-06-12 15:38:24,Create new writeengine batch write command,"I had originally planned to use WE_SVR_BATCH_INSERT but it is far too verbose for what I need and the performance wouldn't be great.

I need to create a similar command which uses binary data and removes the verbose command messaging.",,0,0,0,0,0.0,"Create new writeengine batch write command $end$ I had originally planned to use WE_SVR_BATCH_INSERT but it is far too verbose for what I need and the performance wouldn't be great.

I need to create a similar command which uses binary data and removes the verbose command messaging. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,25,2,0.08,2,0.08,1,0.04,1,0.04,1,0.04
497,MCOL-770,New Feature,MCOL,2017-06-13 14:23:46,,0,support pre-installed software in postConfigure and addModule - phase 2,"This feature is being in 2 parts - phase 1 required the user to preinstall the software and postConfigure and procmgr (addModule) would do the software install part, but would still send messages to the new node to set it up.

Phase 2 - remove the constraint of postConfigure and addModule to communicate with the new module to set it up. Looks at these options for phase 2

1. have the user manually do some scripts that would do the setup after he has installed the software
2. put the logic in ProcMon for it to do the setup on a newly installed server when it starts up. Procmon already will request a copy of the Columnstore.xml file at startup.",,"support pre-installed software in postConfigure and addModule - phase 2 $end$ This feature is being in 2 parts - phase 1 required the user to preinstall the software and postConfigure and procmgr (addModule) would do the software install part, but would still send messages to the new node to set it up.

Phase 2 - remove the constraint of postConfigure and addModule to communicate with the new module to set it up. Looks at these options for phase 2

1. have the user manually do some scripts that would do the setup after he has installed the software
2. put the logic in ProcMon for it to do the setup on a newly installed server when it starts up. Procmon already will request a copy of the Columnstore.xml file at startup. $acceptance criteria:$",,David Hill,David Hill,Major,12,,0,10,0,7,0,0,0,,0,850,10,0,0,2017-06-13 14:26:04,support pre-installed software in postConfigure and addModule - phase 2,"This feature is being in 2 parts - phase 1 required the user to preinstall the software and postConfigure and procmgr (addModule) would do the software install part, but would still send messages to the new node to set it up.

Phase 2 - remove the constraint of postConfigure and addModule to communicate with the new module to set it up. Looks at these options for phase 2

1. have the user manually do some scripts that would do the setup after he has installed the software
2. put the logic in ProcMon for it to do the setup on a newly installed server when it starts up. Procmon already will request a copy of the Columnstore.xml file at startup.",,0,0,0,0,0.0,"support pre-installed software in postConfigure and addModule - phase 2 $end$ This feature is being in 2 parts - phase 1 required the user to preinstall the software and postConfigure and procmgr (addModule) would do the software install part, but would still send messages to the new node to set it up.

Phase 2 - remove the constraint of postConfigure and addModule to communicate with the new module to set it up. Looks at these options for phase 2

1. have the user manually do some scripts that would do the setup after he has installed the software
2. put the logic in ProcMon for it to do the setup on a newly installed server when it starts up. Procmon already will request a copy of the Columnstore.xml file at startup. $acceptance criteria:$",0,0,0,0,0,0,1,0.0333333,19,3,0.157895,0,0.0,0,0.0,0,0.0,0,0.0
498,MCOL-777,Task,MCOL,2017-06-19 07:40:28,,0,Autotools files need to go,"Our codebase is littered with autotools files, they were left there in case the CMake conversion had issues. I believe it is safe to remove them now, we can use git to retrieve them if we really need them.",,"Autotools files need to go $end$ Our codebase is littered with autotools files, they were left there in case the CMake conversion had issues. I believe it is safe to remove them now, we can use git to retrieve them if we really need them. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Minor,8,,0,4,0,4,0,0,0,,0,850,4,0,0,2017-07-27 10:47:33,Autotools files need to go,"Our codebase is littered with autotools files, they were left there in case the CMake conversion had issues. I believe it is safe to remove them now, we can use git to retrieve them if we really need them.",,0,0,0,0,0.0,"Autotools files need to go $end$ Our codebase is littered with autotools files, they were left there in case the CMake conversion had issues. I believe it is safe to remove them now, we can use git to retrieve them if we really need them. $acceptance criteria:$",0,0,0,0,0,0,1,915.117,26,2,0.0769231,2,0.0769231,1,0.0384615,1,0.0384615,1,0.0384615
499,MCOL-787,New Feature,MCOL,2017-06-26 14:44:59,,0,run command to create system tables after startsystem,"A few times a system has failed to startup correctly because postConfigure didn't run to successful completion due to some install issue. When this happens, user might correct the problem then run ""mcsadmin startsystem"" to bring the system up. But when it done this way, the System Catalog isn't created and all create table will fail. 
This enhancement will allow the system to come up in a functional state if this happens, postConfigure doesn't finished and user then does a startSystem.

If the system catalog already exist when this command is done, nothing is effected. Both ProcMgr and the script checks if the system catalog already exist and it will bypass running the script.
",,"run command to create system tables after startsystem $end$ A few times a system has failed to startup correctly because postConfigure didn't run to successful completion due to some install issue. When this happens, user might correct the problem then run ""mcsadmin startsystem"" to bring the system up. But when it done this way, the System Catalog isn't created and all create table will fail. 
This enhancement will allow the system to come up in a functional state if this happens, postConfigure doesn't finished and user then does a startSystem.

If the system catalog already exist when this command is done, nothing is effected. Both ProcMgr and the script checks if the system catalog already exist and it will bypass running the script.
 $acceptance criteria:$",,David Hill,David Hill,Minor,12,,0,10,0,1,0,0,0,,0,850,10,0,0,2017-06-26 14:46:51,run command to create system tables after startsystem,"A few times a system has failed to startup correctly because postConfigure didn't run to successful completion due to some install issue. When this happens, user might correct the problem then run ""mcsadmin startsystem"" to bring the system up. But when it done this way, the System Catalog isn't created and all create table will fail. 
This enhancement will allow the system to come up in a functional state if this happens, postConfigure doesn't finished and user then does a startSystem.

If the system catalog already exist when this command is done, nothing is effected. Both ProcMgr and the script checks if the system catalog already exist and it will bypass running the script.
",,0,0,0,0,0.0,"run command to create system tables after startsystem $end$ A few times a system has failed to startup correctly because postConfigure didn't run to successful completion due to some install issue. When this happens, user might correct the problem then run ""mcsadmin startsystem"" to bring the system up. But when it done this way, the System Catalog isn't created and all create table will fail. 
This enhancement will allow the system to come up in a functional state if this happens, postConfigure doesn't finished and user then does a startSystem.

If the system catalog already exist when this command is done, nothing is effected. Both ProcMgr and the script checks if the system catalog already exist and it will bypass running the script.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0166667,20,3,0.15,0,0.0,0,0.0,0,0.0,0,0.0
500,MCOL-792,New Feature,MCOL,2017-06-28 17:45:59,,0,support debian 9,"work on getting a debian 9 build and build machines and make the 1.1.0 ColumnStore build on debian 9 OS.
Early work got the server compiling, make issues in the engine code to work out.

",,"support debian 9 $end$ work on getting a debian 9 build and build machines and make the 1.1.0 ColumnStore build on debian 9 OS.
Early work got the server compiling, make issues in the engine code to work out.

 $acceptance criteria:$",,David Hill,David Hill,Minor,11,,0,8,0,4,0,0,0,,0,850,8,0,0,2017-06-28 17:45:59,support debian 9,"work on getting a debian 9 build and build machines and make the 1.1.0 ColumnStore build on debian 9 OS.
Early work got the server compiling, make issues in the engine code to work out.

",,0,0,0,0,0.0,"support debian 9 $end$ work on getting a debian 9 build and build machines and make the 1.1.0 ColumnStore build on debian 9 OS.
Early work got the server compiling, make issues in the engine code to work out.

 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,21,3,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
501,MCOL-802,Task,MCOL,2017-07-05 06:37:07,,0,Merge MariaDB 10.1.25 into 1.0,MariaDB 10.1.25 has just been released. It needs to be merged into 1.0.,,Merge MariaDB 10.1.25 into 1.0 $end$ MariaDB 10.1.25 has just been released. It needs to be merged into 1.0. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,13,,0,3,0,1,0,0,0,,0,850,3,0,0,2017-07-05 06:37:07,Merge MariaDB 10.1.25 into 1.0,MariaDB 10.1.25 has just been released. It needs to be merged into 1.0.,,0,0,0,0,0.0,Merge MariaDB 10.1.25 into 1.0 $end$ MariaDB 10.1.25 has just been released. It needs to be merged into 1.0. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,27,2,0.0740741,2,0.0740741,1,0.037037,1,0.037037,1,0.037037
502,MCOL-809,New Feature,MCOL,2017-07-10 14:00:01,,0,Improve mcsapi exception naming,We currently shove all of mcsapi's exceptions in a couple of buckets. We need to add more descriptive exception types that are children of ColumnStoreException.,,Improve mcsapi exception naming $end$ We currently shove all of mcsapi's exceptions in a couple of buckets. We need to add more descriptive exception types that are children of ColumnStoreException. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,10,,0,2,0,3,0,0,0,,0,850,2,0,0,2017-08-09 18:43:04,Improve mcsapi exception naming,We currently shove all of mcsapi's exceptions in a couple of buckets. We need to add more descriptive exception types that are children of ColumnStoreException.,,0,0,0,0,0.0,Improve mcsapi exception naming $end$ We currently shove all of mcsapi's exceptions in a couple of buckets. We need to add more descriptive exception types that are children of ColumnStoreException. $acceptance criteria:$,0,0,0,0,0,0,1,724.717,28,2,0.0714286,2,0.0714286,1,0.0357143,1,0.0357143,1,0.0357143
503,MCOL-813,Task,MCOL,2017-07-14 14:48:52,,0,Merge MariaDB 10.2.7 into 1.1,MariaDB 10.2.7 has been release so we need to sync with it.,,Merge MariaDB 10.2.7 into 1.1 $end$ MariaDB 10.2.7 has been release so we need to sync with it. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-07-14 14:48:52,Merge MariaDB 10.2.7 into 1.1,MariaDB 10.2.7 has been release so we need to sync with it.,,0,0,0,0,0.0,Merge MariaDB 10.2.7 into 1.1 $end$ MariaDB 10.2.7 has been release so we need to sync with it. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,29,2,0.0689655,2,0.0689655,1,0.0344828,1,0.0344828,1,0.0344828
504,MCOL-821,Task,MCOL,2017-07-20 21:51:33,,0,update name of reference udf functions,"The UDF reference implementation classes are named IDB_, these should  be update to MCS_ to match columnstore naming for clarity.",,"update name of reference udf functions $end$ The UDF reference implementation classes are named IDB_, these should  be update to MCS_ to match columnstore naming for clarity. $acceptance criteria:$",,David Thompson,David Thompson,Major,9,,0,3,1,2,0,0,0,,0,850,3,0,0,2017-07-20 21:51:37,update name of reference udf functions,"The UDF reference implementation classes are named IDB_, these should  be update to MCS_ to match columnstore naming for clarity.",,0,0,0,0,0.0,"update name of reference udf functions $end$ The UDF reference implementation classes are named IDB_, these should  be update to MCS_ to match columnstore naming for clarity. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,41,4,0.097561,4,0.097561,3,0.0731707,2,0.0487805,2,0.0487805
505,MCOL-825,Task,MCOL,2017-07-21 15:27:27,,0,Port UDF changes to 1.1,The UDF changes in MCOL-821 need porting to 1.1,,Port UDF changes to 1.1 $end$ The UDF changes in MCOL-821 need porting to 1.1 $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,9,,0,1,1,4,0,0,0,,0,850,1,0,0,2017-07-25 00:56:58,Port UDF changes to 1.1,The UDF changes in MCOL-821 need porting to 1.1,,0,0,0,0,0.0,Port UDF changes to 1.1 $end$ The UDF changes in MCOL-821 need porting to 1.1 $acceptance criteria:$,0,0,0,0,0,0,1,81.4833,30,2,0.0666667,2,0.0666667,1,0.0333333,1,0.0333333,1,0.0333333
506,MCOL-827,Task,MCOL,2017-07-21 18:56:39,,0,Document where to run troubleshooting utilities,"The MariaDB ColumnStore documentation lists several troubleshooting utilities here:

https://mariadb.com/kb/en/mariadb/system-troubleshooting-mariadb-columnstore/#mariadb-columnstore-troubleshooting-utilities

But the documentation doesn't say on which node to run these utilities. Are they run on UM1 or PM1? Or do they need to be run on each node individually?",,"Document where to run troubleshooting utilities $end$ The MariaDB ColumnStore documentation lists several troubleshooting utilities here:

https://mariadb.com/kb/en/mariadb/system-troubleshooting-mariadb-columnstore/#mariadb-columnstore-troubleshooting-utilities

But the documentation doesn't say on which node to run these utilities. Are they run on UM1 or PM1? Or do they need to be run on each node individually? $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,5,,0,6,0,1,0,0,0,,0,850,4,0,0,2017-09-20 23:10:22,Document where to run troubleshooting utilities,"The MariaDB ColumnStore documentation lists several troubleshooting utilities here:

https://mariadb.com/kb/en/mariadb/system-troubleshooting-mariadb-columnstore/#mariadb-columnstore-troubleshooting-utilities

But the documentation doesn't say on which node to run these utilities. Are they run on UM1 or PM1? Or do they need to be run on each node individually?",,0,0,0,0,0.0,"Document where to run troubleshooting utilities $end$ The MariaDB ColumnStore documentation lists several troubleshooting utilities here:

https://mariadb.com/kb/en/mariadb/system-troubleshooting-mariadb-columnstore/#mariadb-columnstore-troubleshooting-utilities

But the documentation doesn't say on which node to run these utilities. Are they run on UM1 or PM1? Or do they need to be run on each node individually? $acceptance criteria:$",0,0,0,0,0,0,0,1468.22,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
507,MCOL-829,New Feature,MCOL,2017-07-24 17:13:16,,0,Implement stored procedure INSERT...SELECT,vtable mode only supports SELECT. I believe we can implement INSERT...SELECT by setting the 	thd->infinidb_vtable.isInsertSelect flag when one is found.,,Implement stored procedure INSERT...SELECT $end$ vtable mode only supports SELECT. I believe we can implement INSERT...SELECT by setting the 	thd->infinidb_vtable.isInsertSelect flag when one is found. $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,7,,0,3,0,2,0,0,0,,0,850,2,0,0,2017-07-24 18:35:01,Implement stored procedure INSERT...SELECT,vtable mode only supports SELECT. I believe we can implement INSERT...SELECT by setting the 	thd->infinidb_vtable.isInsertSelect flag when one is found.,,0,0,0,0,0.0,Implement stored procedure INSERT...SELECT $end$ vtable mode only supports SELECT. I believe we can implement INSERT...SELECT by setting the 	thd->infinidb_vtable.isInsertSelect flag when one is found. $acceptance criteria:$,0,0,0,0,0,0,1,1.35,31,2,0.0645161,2,0.0645161,1,0.0322581,1,0.0322581,1,0.0322581
508,MCOL-84,Task,MCOL,2016-05-26 16:05:59,,0,"amazonInstaller script, update or not support","On the InfiniDB product,we have a script that will automatically will setup an Amazon System based Parameters provide by user in a configure file. This script, called amazonInstaller is pre-installed on the AMI. It will auto create all the instances and EBS volumes foe the user.

This script needs to be updated to work on the ColumnStore product or we can decide just to use the postConfigure installer, which will also pre-create the instances and EBS volumes for the user. The main difference is postConfigure is a prompting configuration tool and the amazonInstaller reads the dat from the configuration file, making it a quicker installer to bring up a multiple-node cluster.",,"amazonInstaller script, update or not support $end$ On the InfiniDB product,we have a script that will automatically will setup an Amazon System based Parameters provide by user in a configure file. This script, called amazonInstaller is pre-installed on the AMI. It will auto create all the instances and EBS volumes foe the user.

This script needs to be updated to work on the ColumnStore product or we can decide just to use the postConfigure installer, which will also pre-create the instances and EBS volumes for the user. The main difference is postConfigure is a prompting configuration tool and the amazonInstaller reads the dat from the configuration file, making it a quicker installer to bring up a multiple-node cluster. $acceptance criteria:$",,David Hill,David Hill,Minor,3,,0,2,0,1,0,0,0,,0,850,1,0,0,2016-06-04 02:35:35,"amazonInstaller script, update or not support","On the InfiniDB product,we have a script that will automatically will setup an Amazon System based Parameters provide by user in a configure file. This script, called amazonInstaller is pre-installed on the AMI. It will auto create all the instances and EBS volumes foe the user.

This script needs to be updated to work on the ColumnStore product or we can decide just to use the postConfigure installer, which will also pre-create the instances and EBS volumes for the user. The main difference is postConfigure is a prompting configuration tool and the amazonInstaller reads the dat from the configuration file, making it a quicker installer to bring up a multiple-node cluster.",,0,0,0,0,0.0,"amazonInstaller script, update or not support $end$ On the InfiniDB product,we have a script that will automatically will setup an Amazon System based Parameters provide by user in a configure file. This script, called amazonInstaller is pre-installed on the AMI. It will auto create all the instances and EBS volumes foe the user.

This script needs to be updated to work on the ColumnStore product or we can decide just to use the postConfigure installer, which will also pre-create the instances and EBS volumes for the user. The main difference is postConfigure is a prompting configuration tool and the amazonInstaller reads the dat from the configuration file, making it a quicker installer to bring up a multiple-node cluster. $acceptance criteria:$",0,0,0,0,0,0,0,202.483,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
509,MCOL-868,Task,MCOL,2017-08-10 22:19:49,,0,merge mariadb server 10.1.26,,,merge mariadb server 10.1.26 $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,7,,0,3,0,1,0,0,0,,0,850,3,0,0,2017-08-11 12:53:52,merge mariadb server 10.1.26,,,0,0,0,0,0.0,merge mariadb server 10.1.26 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,14.5667,42,4,0.0952381,4,0.0952381,3,0.0714286,2,0.047619,2,0.047619
510,MCOL-887,Task,MCOL,2017-08-19 20:39:55,,0,Merge MariaDB 10.2.8,"MariaDB 10.2.8 has been released, we need to merge this into the 1.1 server tree.",,"Merge MariaDB 10.2.8 $end$ MariaDB 10.2.8 has been released, we need to merge this into the 1.1 server tree. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,9,,0,1,0,3,0,0,0,,0,850,1,0,0,2017-08-19 20:39:55,Merge MariaDB 10.2.8,"MariaDB 10.2.8 has been released, we need to merge this into the 1.1 server tree.",,0,0,0,0,0.0,"Merge MariaDB 10.2.8 $end$ MariaDB 10.2.8 has been released, we need to merge this into the 1.1 server tree. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,32,2,0.0625,2,0.0625,1,0.03125,1,0.03125,1,0.03125
511,MCOL-894,New Feature,MCOL,2017-08-29 09:10:04,MCOL-3525,0,make order by more scalable / faster,"Hello,

Order by processing from MariaDB is still single threaded.

The impression of columnstore suffer, if the query runs fast in the columnstore part,
but slow in total, because order by can be slowdown.

",,"make order by more scalable / faster $end$ Hello,

Order by processing from MariaDB is still single threaded.

The impression of columnstore suffer, if the query runs fast in the columnstore part,
but slow in total, because order by can be slowdown.

 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Critical,16,,0,4,0,1,0,0,1,,0,850,3,0,0,2019-10-31 16:28:16,make order by more scalable / faster,"Hello,

Order by processing from MariaDB is still single threaded.

The impression of columnstore suffer, if the query runs fast in the columnstore part,
but slow in total, because order by can be slowdown.

",,0,0,0,0,0.0,"make order by more scalable / faster $end$ Hello,

Order by processing from MariaDB is still single threaded.

The impression of columnstore suffer, if the query runs fast in the columnstore part,
but slow in total, because order by can be slowdown.

 $acceptance criteria:$",0,0,0,0,0,0,0,19039.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
512,MCOL-9,Task,MCOL,2016-05-02 17:26:11,MCOL-10,0,MariaDB ColumnStore Syntax Guide ,"Create Syntax Guide based on current MariaDB ColumnStore SQL syntax.

Ian is making the fist pass based on 2010 GPL document from InfiniDB and putting it in KB, Dipti is to review it. Then additional content to be added for what is new between 2010 and 2016.",,"MariaDB ColumnStore Syntax Guide  $end$ Create Syntax Guide based on current MariaDB ColumnStore SQL syntax.

Ian is making the fist pass based on 2010 GPL document from InfiniDB and putting it in KB, Dipti is to review it. Then additional content to be added for what is new between 2010 and 2016. $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,14,,0,2,0,1,0,3,0,,0,850,1,3,0,2016-05-17 19:57:04,MariaDB ColumnStore Syntax Guide ,"Create Syntax Guide based on current MariaDB ColumnStore SQL syntax.

Ian is making the fist pass based on 2010 GPL document from InfiniDB and putting it in KB, Dipti is to review it. Then additional content to be added for what is new between 2010 and 2016.",,0,0,0,0,0.0,"MariaDB ColumnStore Syntax Guide  $end$ Create Syntax Guide based on current MariaDB ColumnStore SQL syntax.

Ian is making the fist pass based on 2010 GPL document from InfiniDB and putting it in KB, Dipti is to review it. Then additional content to be added for what is new between 2010 and 2016. $acceptance criteria:$",0,0,0,0,0,0,0,362.5,5,2,0.4,0,0.0,0,0.0,0,0.0,0,0.0
513,MCOL-904,Sub-Task,MCOL,2017-09-05 18:32:53,,0,API call to get system catalog,Add method to retrieve the system catalog. We already have it cached in memory,,API call to get system catalog $end$ Add method to retrieve the system catalog. We already have it cached in memory $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,4,,0,1,0,11,0,0,0,,0,850,1,0,0,2017-09-05 18:32:53,API call to get system catalog,Add method to retrieve the system catalog. We already have it cached in memory,,0,0,0,0,0.0,API call to get system catalog $end$ Add method to retrieve the system catalog. We already have it cached in memory $acceptance criteria:$,0,0,0,0,0,0,1,0.0,33,2,0.0606061,2,0.0606061,1,0.030303,1,0.030303,1,0.030303
514,MCOL-905,Sub-Task,MCOL,2017-09-05 20:36:47,,0,Fix package naming convension,Packages don't follow the same naming convention as ColumnStore,,Fix package naming convension $end$ Packages don't follow the same naming convention as ColumnStore $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,1,,0,2,0,11,0,0,0,,0,850,2,0,0,2017-09-05 20:36:47,Fix package naming convension,Packages don't follow the same naming convention as ColumnStore,,0,0,0,0,0.0,Fix package naming convension $end$ Packages don't follow the same naming convention as ColumnStore $acceptance criteria:$,0,0,0,0,0,0,1,0.0,34,2,0.0588235,2,0.0588235,1,0.0294118,1,0.0294118,1,0.0294118
515,MCOL-940,Task,MCOL,2017-09-26 15:28:44,,0,merge server 10.1.28,,,merge server 10.1.28 $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,11,,0,1,0,5,0,1,0,,0,850,1,1,0,2017-10-02 17:02:47,merge server 10.1.28,,,0,0,0,0,0.0,merge server 10.1.28 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,145.567,43,4,0.0930233,4,0.0930233,3,0.0697674,2,0.0465116,2,0.0465116
516,MCOL-942,New Feature,MCOL,2017-09-27 16:28:22,,0,Setting up of the Master/Slave Replication should not be done on start system,"Currently in 1.0, the master/slave is setup at the end of postConfigure AND will also be resetup when a restartsystem or system servers are rebooted.
In 1.1, this logic is only in ProcMgr for startsystem.

It is recommended that this not be setup everything the system is restarted. I did a reboot test and the master/slave setup remain when I took out the step in ProcMgr to set it up on startsystem.

So for 1.0 and 1.1, it should only be initially setup in postConfigure. and removed from the startsystem in ProcMgr. It will still be called by procmgr when the master node needs to be changed.",,"Setting up of the Master/Slave Replication should not be done on start system $end$ Currently in 1.0, the master/slave is setup at the end of postConfigure AND will also be resetup when a restartsystem or system servers are rebooted.
In 1.1, this logic is only in ProcMgr for startsystem.

It is recommended that this not be setup everything the system is restarted. I did a reboot test and the master/slave setup remain when I took out the step in ProcMgr to set it up on startsystem.

So for 1.0 and 1.1, it should only be initially setup in postConfigure. and removed from the startsystem in ProcMgr. It will still be called by procmgr when the master node needs to be changed. $acceptance criteria:$",,David Hill,David Hill,Minor,10,,0,5,0,1,0,0,0,,0,850,4,0,0,2017-11-27 04:37:36,Setting up of the Master/Slave Replication should not be done on start system,"Currently in 1.0, the master/slave is setup at the end of postConfigure AND will also be resetup when a restartsystem or system servers are rebooted.
In 1.1, this logic is only in ProcMgr for startsystem.

It is recommended that this not be setup everything the system is restarted. I did a reboot test and the master/slave setup remain when I took out the step in ProcMgr to set it up on startsystem.

So for 1.0 and 1.1, it should only be initially setup in postConfigure. and removed from the startsystem in ProcMgr. It will still be called by procmgr when the master node needs to be changed.",,0,0,0,0,0.0,"Setting up of the Master/Slave Replication should not be done on start system $end$ Currently in 1.0, the master/slave is setup at the end of postConfigure AND will also be resetup when a restartsystem or system servers are rebooted.
In 1.1, this logic is only in ProcMgr for startsystem.

It is recommended that this not be setup everything the system is restarted. I did a reboot test and the master/slave setup remain when I took out the step in ProcMgr to set it up on startsystem.

So for 1.0 and 1.1, it should only be initially setup in postConfigure. and removed from the startsystem in ProcMgr. It will still be called by procmgr when the master node needs to be changed. $acceptance criteria:$",0,0,0,0,0,0,0,1452.15,22,3,0.136364,0,0.0,0,0.0,0,0.0,0,0.0
517,MCOL-946,Task,MCOL,2017-09-28 17:30:10,,0,migrate to swig for mcsapi python binding,the initial choice for python binding framework pybind11 seems to have an excessive runtime overhead plus requires maintenance of a macro per api call. Evaluating swig shows this has a simpler interface file plus is about 2-2.5 times faster for a million row insert test case.,,migrate to swig for mcsapi python binding $end$ the initial choice for python binding framework pybind11 seems to have an excessive runtime overhead plus requires maintenance of a macro per api call. Evaluating swig shows this has a simpler interface file plus is about 2-2.5 times faster for a million row insert test case. $acceptance criteria:$,,David Thompson,David Thompson,Major,5,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-09-28 17:30:10,migrate to swig for mcsapi python binding,the initial choice for python binding framework pybind11 seems to have an excessive runtime overhead plus requires maintenance of a macro per api call. Evaluating swig shows this has a simpler interface file plus is about 2-2.5 times faster for a million row insert test case.,,0,0,0,0,0.0,migrate to swig for mcsapi python binding $end$ the initial choice for python binding framework pybind11 seems to have an excessive runtime overhead plus requires maintenance of a macro per api call. Evaluating swig shows this has a simpler interface file plus is about 2-2.5 times faster for a million row insert test case. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,44,4,0.0909091,4,0.0909091,3,0.0681818,2,0.0454545,2,0.0454545
518,MCOL-962,New Feature,MCOL,2017-10-09 16:14:11,,0,Function/table to find out if ColumnStore instance (UM) is ready to process SQL queries against ColumnStore tables,"Currently there is no predefined way to check with SQL statement if ColumnStore UM is able to process SQL statements against ColumnStore tables. 

Surely, one can try to create customer UDF around mcsadmin command, or just try to create and access ColumnStore table and process errors etc, but official and supported way is still needed. It may be used by MaxScale or any monitoring software to decide how to route queries or when to alert DBA about the problem.",,"Function/table to find out if ColumnStore instance (UM) is ready to process SQL queries against ColumnStore tables $end$ Currently there is no predefined way to check with SQL statement if ColumnStore UM is able to process SQL statements against ColumnStore tables. 

Surely, one can try to create customer UDF around mcsadmin command, or just try to create and access ColumnStore table and process errors etc, but official and supported way is still needed. It may be used by MaxScale or any monitoring software to decide how to route queries or when to alert DBA about the problem. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,27,,0,13,1,5,0,1,0,,0,850,12,0,0,2017-11-27 04:37:36,Funtcion/table to find out if ColumnStore instance (UM) is ready to process SQL queries against ColumnStore tables,"Currently there is no predefined way to check with SQL statement if ColumnStore UM is able to process SQL statements against ColumnStore tables. 

Surely, one can try to create customer UDF around mcsadmin command, or just try to create and access ColumnStore table and process errors etc, but official and supported way is still needed. It may be used by MaxScale or any monitoring software to decide how to route queries or when to alert DBA about the problem.",,1,0,0,2,0.010101,"Funtcion/table to find out if ColumnStore instance (UM) is ready to process SQL queries against ColumnStore tables $end$ Currently there is no predefined way to check with SQL statement if ColumnStore UM is able to process SQL statements against ColumnStore tables. 

Surely, one can try to create customer UDF around mcsadmin command, or just try to create and access ColumnStore table and process errors etc, but official and supported way is still needed. It may be used by MaxScale or any monitoring software to decide how to route queries or when to alert DBA about the problem. $acceptance criteria:$",1,1,0,0,0,0,1,1164.38,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
519,MCOL-978,New Feature,MCOL,2017-10-17 13:46:35,MCOL-1096,0,Disable Query Cache for ColumnStore,"This is a minor change to our storage engine plugin to state that query cache for ColumnStore is not possible (since writes can happen from other MariaDB servers):

{code:cpp}
handler::table_cache_type()
return HA_CACHE_TBL_NOCACHE;
{code}

Also undo any modifications to server that do the same thing.",,"Disable Query Cache for ColumnStore $end$ This is a minor change to our storage engine plugin to state that query cache for ColumnStore is not possible (since writes can happen from other MariaDB servers):

{code:cpp}
handler::table_cache_type()
return HA_CACHE_TBL_NOCACHE;
{code}

Also undo any modifications to server that do the same thing. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,20,,0,2,0,11,0,0,0,,0,850,2,0,0,2017-12-12 21:15:04,Disable Query Cache for ColumnStore,"This is a minor change to our storage engine plugin to state that query cache for ColumnStore is not possible (since writes can happen from other MariaDB servers):

{code:cpp}
handler::table_cache_type()
return HA_CACHE_TBL_NOCACHE;
{code}

Also undo any modifications to server that do the same thing.",,0,0,0,0,0.0,"Disable Query Cache for ColumnStore $end$ This is a minor change to our storage engine plugin to state that query cache for ColumnStore is not possible (since writes can happen from other MariaDB servers):

{code:cpp}
handler::table_cache_type()
return HA_CACHE_TBL_NOCACHE;
{code}

Also undo any modifications to server that do the same thing. $acceptance criteria:$",0,0,0,0,0,0,1,1351.47,35,2,0.0571429,2,0.0571429,1,0.0285714,1,0.0285714,1,0.0285714
520,MCOL-982,Task,MCOL,2017-10-23 15:37:45,,0,Merge MariaDB 10.2.9 into ColumnStore,Merge MariaDB 10.2.9 into 1.1/1.2 trees,,Merge MariaDB 10.2.9 into ColumnStore $end$ Merge MariaDB 10.2.9 into 1.1/1.2 trees $acceptance criteria:$,,Andrew Hutchings,Andrew Hutchings,Major,5,,0,2,0,1,0,0,0,,0,850,2,0,0,2017-10-23 15:37:45,Merge MariaDB 10.2.9 into ColumnStore,Merge MariaDB 10.2.9 into 1.1/1.2 trees,,0,0,0,0,0.0,Merge MariaDB 10.2.9 into ColumnStore $end$ Merge MariaDB 10.2.9 into 1.1/1.2 trees $acceptance criteria:$,0,0,0,0,0,0,0,0.0,36,2,0.0555556,2,0.0555556,1,0.0277778,1,0.0277778,1,0.0277778
521,MCOL-987,New Feature,MCOL,2017-10-25 19:08:12,MCOL-3351,0,LZ4 compression for on-disk columnar data,"According to different comparisions, e.g. [here|https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/] LZ4 might have:
* better compression rate 
* better decompression speed
* almost the same compression speed
compared with Snappy. MCS uses Snappy by default for both columnar files and CompressedInetStreamSocket(TCP socket) implementation.

The chunk size is an important parameter used to define how much worth of data is compressed in one go to store in the compressed columnar file. As of now it is set to 4MB that might be less apropriate for LZ4 so one should compare different compressed chunk size values.
In the end MCS must have another compression method that is controlled via the session variable columnstore_compression_type. There will be no separate knob to control compression used by CompressedInetStreamSocket. 

If LZ4 performs as well as expected(faster decompression, better compression, compression speed parity) it will become our default.",,"LZ4 compression for on-disk columnar data $end$ According to different comparisions, e.g. [here|https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/] LZ4 might have:
* better compression rate 
* better decompression speed
* almost the same compression speed
compared with Snappy. MCS uses Snappy by default for both columnar files and CompressedInetStreamSocket(TCP socket) implementation.

The chunk size is an important parameter used to define how much worth of data is compressed in one go to store in the compressed columnar file. As of now it is set to 4MB that might be less apropriate for LZ4 so one should compare different compressed chunk size values.
In the end MCS must have another compression method that is controlled via the session variable columnstore_compression_type. There will be no separate knob to control compression used by CompressedInetStreamSocket. 

If LZ4 performs as well as expected(faster decompression, better compression, compression speed parity) it will become our default. $acceptance criteria:$",,Andrew Hutchings,Andrew Hutchings,Major,42,,0,5,0,5,0,7,1,,0,850,4,7,0,2021-03-23 14:50:46,LZ4 compression for on-disk columnar data,"According to different comparisions, e.g. [here|https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/] LZ4 might have:
* better compression rate 
* better decompression speed
* almost the same compression speed
compared with Snappy. MCS uses Snappy by default for both columnar files and CompressedInetStreamSocket(TCP socket) implementation.

The chunk size is an important parameter used to define how much worth of data is compressed in one go to store in the compressed columnar file. As of now it is set to 4MB that might be less apropriate for LZ4 so one should compare different compressed chunk size values.
In the end MCS must have another compression method that is controlled via the session variable columnstore_compression_type. There will be no separate knob to control compression used by CompressedInetStreamSocket. 

If LZ4 performs as well as expected(faster decompression, better compression, compression speed parity) it will become our default.",,0,0,0,0,0.0,"LZ4 compression for on-disk columnar data $end$ According to different comparisions, e.g. [here|https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/] LZ4 might have:
* better compression rate 
* better decompression speed
* almost the same compression speed
compared with Snappy. MCS uses Snappy by default for both columnar files and CompressedInetStreamSocket(TCP socket) implementation.

The chunk size is an important parameter used to define how much worth of data is compressed in one go to store in the compressed columnar file. As of now it is set to 4MB that might be less apropriate for LZ4 so one should compare different compressed chunk size values.
In the end MCS must have another compression method that is controlled via the session variable columnstore_compression_type. There will be no separate knob to control compression used by CompressedInetStreamSocket. 

If LZ4 performs as well as expected(faster decompression, better compression, compression speed parity) it will become our default. $acceptance criteria:$",0,0,0,0,0,0,1,29875.7,37,2,0.0540541,2,0.0540541,1,0.027027,1,0.027027,1,0.027027
522,MCOL-990,Task,MCOL,2017-10-27 18:02:18,,0,provide a resetRow method,"We should provide a 'reset row' method that allows you to effectively reset / cancel the current row write. This should always succeed and should have the effect that the temporary holding area for the current working row is emptied allowing the prior records in the batch to be committed.

 What this would allow you to do is to commit the prior rows in your transaction should you get some sort of error in the middle of processing the current row.  Right now if you have an error the only solution is to rollback the entire transaction. This could result in you losing a significant number of prior records in a streaming / micro batched use case (or requires you to stage that locally which makes it more complex.  

The following python code should work after this is implemented (in a real case it would probably be in an exception handler of course):
{code}
import pymcsapi
driver = pymcsapi.ColumnStoreDriver()
bulk = driver.createBulkInsert('test', 't1', 0,0)
bulk.setColumn(0, 1)
bulk.setColumn(1, 'A')
bulk.writeRow()
bulk.setColumn(0, 2)
bulk.resetRow()
bulk.commit()
{code}",,"provide a resetRow method $end$ We should provide a 'reset row' method that allows you to effectively reset / cancel the current row write. This should always succeed and should have the effect that the temporary holding area for the current working row is emptied allowing the prior records in the batch to be committed.

 What this would allow you to do is to commit the prior rows in your transaction should you get some sort of error in the middle of processing the current row.  Right now if you have an error the only solution is to rollback the entire transaction. This could result in you losing a significant number of prior records in a streaming / micro batched use case (or requires you to stage that locally which makes it more complex.  

The following python code should work after this is implemented (in a real case it would probably be in an exception handler of course):
{code}
import pymcsapi
driver = pymcsapi.ColumnStoreDriver()
bulk = driver.createBulkInsert('test', 't1', 0,0)
bulk.setColumn(0, 1)
bulk.setColumn(1, 'A')
bulk.writeRow()
bulk.setColumn(0, 2)
bulk.resetRow()
bulk.commit()
{code} $acceptance criteria:$",,David Thompson,David Thompson,Critical,9,,0,2,0,1,0,0,0,,0,850,2,0,0,2017-10-27 19:49:44,provide a resetRow method,"We should provide a 'reset row' method that allows you to effectively reset / cancel the current row write. This should always succeed and should have the effect that the temporary holding area for the current working row is emptied allowing the prior records in the batch to be committed.

 What this would allow you to do is to commit the prior rows in your transaction should you get some sort of error in the middle of processing the current row.  Right now if you have an error the only solution is to rollback the entire transaction. This could result in you losing a significant number of prior records in a streaming / micro batched use case (or requires you to stage that locally which makes it more complex.  

The following python code should work after this is implemented (in a real case it would probably be in an exception handler of course):
{code}
import pymcsapi
driver = pymcsapi.ColumnStoreDriver()
bulk = driver.createBulkInsert('test', 't1', 0,0)
bulk.setColumn(0, 1)
bulk.setColumn(1, 'A')
bulk.writeRow()
bulk.setColumn(0, 2)
bulk.resetRow()
bulk.commit()
{code}",,0,0,0,0,0.0,"provide a resetRow method $end$ We should provide a 'reset row' method that allows you to effectively reset / cancel the current row write. This should always succeed and should have the effect that the temporary holding area for the current working row is emptied allowing the prior records in the batch to be committed.

 What this would allow you to do is to commit the prior rows in your transaction should you get some sort of error in the middle of processing the current row.  Right now if you have an error the only solution is to rollback the entire transaction. This could result in you losing a significant number of prior records in a streaming / micro batched use case (or requires you to stage that locally which makes it more complex.  

The following python code should work after this is implemented (in a real case it would probably be in an exception handler of course):
{code}
import pymcsapi
driver = pymcsapi.ColumnStoreDriver()
bulk = driver.createBulkInsert('test', 't1', 0,0)
bulk.setColumn(0, 1)
bulk.setColumn(1, 'A')
bulk.writeRow()
bulk.setColumn(0, 2)
bulk.resetRow()
bulk.commit()
{code} $acceptance criteria:$",0,0,0,0,0,0,0,1.78333,45,4,0.0888889,4,0.0888889,3,0.0666667,2,0.0444444,2,0.0444444
523,MCOL-992,Task,MCOL,2017-10-28 04:25:17,,0,java binding for mcsapi,,,java binding for mcsapi $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,5,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-10-28 04:25:17,java binding for mcsapi,,,0,0,0,0,0.0,java binding for mcsapi $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,46,4,0.0869565,4,0.0869565,3,0.0652174,2,0.0434783,2,0.0434783
524,MDEV-10030,Task,MDEV,2016-05-05 08:09:47,,0,"sql_yacc.yy: Split table_expression and remove PROCEDURE from create_select, select_paren_derived, select_derived2, query_specification","The ""table_expression"" rule currently looks like this:
{code}
 table_expression:
          opt_from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_select_lock_type
         ;
{code}

We'll do the following refactoring:

1. Remove  the extra parts (opt_order_clause, opt_limit_clause, opt_procedure_clause,  opt_select_lock_type parts), and make the ""from_clause"" non-optional:

{code}
 table_expression:
          from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
         ;
{code}
After this change table_expression will match the SQL Standard <table expression> production.

2. Add the ""opt_table_expression"" rule

3. Move the extra parts of the ex-table_expression to the  ""caller"" rules, and remove the opt_procedure_clause from non-relevant places.

It will change these queries using subselects and derived table:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
SELECT ((SELECT 1 FROM t1 PROCEDURE ANALYSE())) FROM t2;
{code}
as well as:
{code:sql}
CREATE TABLE t2 SELECT  * FROMt1 PROCEDURE ANALYSE();
{code}
to return ""syntax error"" instead of ""Incorrect usage"".
",,"sql_yacc.yy: Split table_expression and remove PROCEDURE from create_select, select_paren_derived, select_derived2, query_specification $end$ The ""table_expression"" rule currently looks like this:
{code}
 table_expression:
          opt_from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_select_lock_type
         ;
{code}

We'll do the following refactoring:

1. Remove  the extra parts (opt_order_clause, opt_limit_clause, opt_procedure_clause,  opt_select_lock_type parts), and make the ""from_clause"" non-optional:

{code}
 table_expression:
          from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
         ;
{code}
After this change table_expression will match the SQL Standard <table expression> production.

2. Add the ""opt_table_expression"" rule

3. Move the extra parts of the ex-table_expression to the  ""caller"" rules, and remove the opt_procedure_clause from non-relevant places.

It will change these queries using subselects and derived table:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
SELECT ((SELECT 1 FROM t1 PROCEDURE ANALYSE())) FROM t2;
{code}
as well as:
{code:sql}
CREATE TABLE t2 SELECT  * FROMt1 PROCEDURE ANALYSE();
{code}
to return ""syntax error"" instead of ""Incorrect usage"".
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,11,,0,0,1,1,0,0,0,,0,850,0,0,0,2016-05-05 08:33:34,"sql_yacc.yy: Split table_expression and remove PROCEDURE from create_select, select_paren_derived, select_derived2, query_specification","The ""table_expression"" rule currently looks like this:
{code}
 table_expression:
          opt_from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_select_lock_type
         ;
{code}

We'll do the following refactoring:

1. Remove  the extra parts (opt_order_clause, opt_limit_clause, opt_procedure_clause,  opt_select_lock_type parts), and make the ""from_clause"" non-optional:

{code}
 table_expression:
          from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
         ;
{code}
After this change table_expression will match the SQL Standard <table expression> production.

2. Add the ""opt_table_expression"" rule

3. Move the extra parts of the ex-table_expression to the  ""caller"" rules, and remove the opt_procedure_clause from non-relevant places.

It will change these queries using subselects and derived table:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
SELECT ((SELECT 1 FROM t1 PROCEDURE ANALYSE())) FROM t2;
{code}
as well as:
{code:sql}
CREATE TABLE t2 SELECT  * FROMt1 PROCEDURE ANALYSE();
{code}
to return ""syntax error"" instead of ""Incorrect usage"".
",,0,0,0,0,0.0,"sql_yacc.yy: Split table_expression and remove PROCEDURE from create_select, select_paren_derived, select_derived2, query_specification $end$ The ""table_expression"" rule currently looks like this:
{code}
 table_expression:
          opt_from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_select_lock_type
         ;
{code}

We'll do the following refactoring:

1. Remove  the extra parts (opt_order_clause, opt_limit_clause, opt_procedure_clause,  opt_select_lock_type parts), and make the ""from_clause"" non-optional:

{code}
 table_expression:
          from_clause
           opt_where_clause
           opt_group_clause
           opt_having_clause
           opt_window_clause
         ;
{code}
After this change table_expression will match the SQL Standard <table expression> production.

2. Add the ""opt_table_expression"" rule

3. Move the extra parts of the ex-table_expression to the  ""caller"" rules, and remove the opt_procedure_clause from non-relevant places.

It will change these queries using subselects and derived table:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
SELECT ((SELECT 1 FROM t1 PROCEDURE ANALYSE())) FROM t2;
{code}
as well as:
{code:sql}
CREATE TABLE t2 SELECT  * FROMt1 PROCEDURE ANALYSE();
{code}
to return ""syntax error"" instead of ""Incorrect usage"".
 $acceptance criteria:$",0,0,0,0,0,0,0,0.383333,8,3,0.375,3,0.375,3,0.375,2,0.25,2,0.25
525,MDEV-10036,Task,MDEV,2016-05-06 10:13:07,,0,"sql_yacc.yy: Split select_part2 to disallow syntactically bad constructs with INTO, PROCEDURE, UNION","sql_yacc.yy has the following code:
{code:c}
select_part2:
          select_options_and_item_list
          opt_order_clause
          opt_limit_clause
          opt_select_lock_type
        | select_options_and_item_list into opt_select_lock_type
        | select_options_and_item_list
          opt_into
          table_expression
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_into
          opt_select_lock_type
          {
            if ($2 && $7)
            {
              /* double ""INTO"" clause */
              my_error(ER_WRONG_USAGE, MYF(0), ""INTO"", ""INTO"");
              MYSQL_YYABORT;
            }
            if ($6 && ($2 || $7))
            {
              /* ""INTO"" with ""PROCEDURE ANALYSE"" */
              my_error(ER_WRONG_USAGE, MYF(0), ""PROCEDURE"", ""INTO"");
              MYSQL_YYABORT;
            }
          }
        ;
{code}
This task will refactor select_part2 to disallow wrong constructs so instead of
{noformat}
Incorrect usage of INTO and INTO
Incorrect usage of INTO and PROCEDURE
Incorrect usage of INTO and UNION
Incorrect usage of UNON and PROCEDURE
{noformat}

the parser will generate ""syntax error"", like this:

{code:sql}
SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1;
{code}
{noformat}
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'UNION ALL SELECT 1' at line 1
{noformat}

Note, this change will not affect subselects,  they will still return ""Incorrect usage"":
{code:sql}
SELECT (SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1);
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}
Subselect will be fixed in a separate patch. We'll do one small step at a time!
",,"sql_yacc.yy: Split select_part2 to disallow syntactically bad constructs with INTO, PROCEDURE, UNION $end$ sql_yacc.yy has the following code:
{code:c}
select_part2:
          select_options_and_item_list
          opt_order_clause
          opt_limit_clause
          opt_select_lock_type
        | select_options_and_item_list into opt_select_lock_type
        | select_options_and_item_list
          opt_into
          table_expression
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_into
          opt_select_lock_type
          {
            if ($2 && $7)
            {
              /* double ""INTO"" clause */
              my_error(ER_WRONG_USAGE, MYF(0), ""INTO"", ""INTO"");
              MYSQL_YYABORT;
            }
            if ($6 && ($2 || $7))
            {
              /* ""INTO"" with ""PROCEDURE ANALYSE"" */
              my_error(ER_WRONG_USAGE, MYF(0), ""PROCEDURE"", ""INTO"");
              MYSQL_YYABORT;
            }
          }
        ;
{code}
This task will refactor select_part2 to disallow wrong constructs so instead of
{noformat}
Incorrect usage of INTO and INTO
Incorrect usage of INTO and PROCEDURE
Incorrect usage of INTO and UNION
Incorrect usage of UNON and PROCEDURE
{noformat}

the parser will generate ""syntax error"", like this:

{code:sql}
SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1;
{code}
{noformat}
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'UNION ALL SELECT 1' at line 1
{noformat}

Note, this change will not affect subselects,  they will still return ""Incorrect usage"":
{code:sql}
SELECT (SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1);
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}
Subselect will be fixed in a separate patch. We'll do one small step at a time!
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,12,,0,0,1,1,0,0,0,,0,850,0,0,0,2016-05-06 10:13:07,"sql_yacc.yy: Split select_part2 to disallow syntactically bad constructs with INTO, PROCEDURE, UNION","sql_yacc.yy has the following code:
{code:c}
select_part2:
          select_options_and_item_list
          opt_order_clause
          opt_limit_clause
          opt_select_lock_type
        | select_options_and_item_list into opt_select_lock_type
        | select_options_and_item_list
          opt_into
          table_expression
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_into
          opt_select_lock_type
          {
            if ($2 && $7)
            {
              /* double ""INTO"" clause */
              my_error(ER_WRONG_USAGE, MYF(0), ""INTO"", ""INTO"");
              MYSQL_YYABORT;
            }
            if ($6 && ($2 || $7))
            {
              /* ""INTO"" with ""PROCEDURE ANALYSE"" */
              my_error(ER_WRONG_USAGE, MYF(0), ""PROCEDURE"", ""INTO"");
              MYSQL_YYABORT;
            }
          }
        ;
{code}
This task will refactor select_part2 to disallow wrong constructs so instead of
{noformat}
Incorrect usage of INTO and INTO
Incorrect usage of INTO and PROCEDURE
Incorrect usage of INTO and UNION
Incorrect usage of UNON and PROCEDURE
{noformat}

the parser will generate ""syntax error"", like this:

{code:sql}
SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1;
{code}
{noformat}
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'UNION ALL SELECT 1' at line 1
{noformat}

Note, this change will not affect subselects,  they will still return ""Incorrect usage"":
{code:sql}
SELECT (SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1);
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}
Subselect will be fixed in a separate patch. We'll do one small step at a time!
",,0,0,0,0,0.0,"sql_yacc.yy: Split select_part2 to disallow syntactically bad constructs with INTO, PROCEDURE, UNION $end$ sql_yacc.yy has the following code:
{code:c}
select_part2:
          select_options_and_item_list
          opt_order_clause
          opt_limit_clause
          opt_select_lock_type
        | select_options_and_item_list into opt_select_lock_type
        | select_options_and_item_list
          opt_into
          table_expression
          opt_order_clause
          opt_limit_clause
          opt_procedure_clause
          opt_into
          opt_select_lock_type
          {
            if ($2 && $7)
            {
              /* double ""INTO"" clause */
              my_error(ER_WRONG_USAGE, MYF(0), ""INTO"", ""INTO"");
              MYSQL_YYABORT;
            }
            if ($6 && ($2 || $7))
            {
              /* ""INTO"" with ""PROCEDURE ANALYSE"" */
              my_error(ER_WRONG_USAGE, MYF(0), ""PROCEDURE"", ""INTO"");
              MYSQL_YYABORT;
            }
          }
        ;
{code}
This task will refactor select_part2 to disallow wrong constructs so instead of
{noformat}
Incorrect usage of INTO and INTO
Incorrect usage of INTO and PROCEDURE
Incorrect usage of INTO and UNION
Incorrect usage of UNON and PROCEDURE
{noformat}

the parser will generate ""syntax error"", like this:

{code:sql}
SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1;
{code}
{noformat}
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'UNION ALL SELECT 1' at line 1
{noformat}

Note, this change will not affect subselects,  they will still return ""Incorrect usage"":
{code:sql}
SELECT (SELECT * FROM t1 LIMIT 1  UNION ALL SELECT 1);
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}
Subselect will be fixed in a separate patch. We'll do one small step at a time!
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,9,3,0.333333,3,0.333333,3,0.333333,2,0.222222,2,0.222222
526,MDEV-10059,Task,MDEV,2016-05-12 13:17:03,,0,Compute window functions with same sorting criteria simultaneously,"While computing window functions, we can take advantage of the general use case where they specify the same PARTITION and ORDER BY criteria.
We already take advantage of this fact by only sorting the internal tmp table as few times as possible. Filling the window function values within the internal tmp table can be performed during one pass / sort as well. Currently we do a pass for each window function.

In order to do this, we must refactor the computation code to be aware of multiple functions during a scan.",,"Compute window functions with same sorting criteria simultaneously $end$ While computing window functions, we can take advantage of the general use case where they specify the same PARTITION and ORDER BY criteria.
We already take advantage of this fact by only sorting the internal tmp table as few times as possible. Filling the window function values within the internal tmp table can be performed during one pass / sort as well. Currently we do a pass for each window function.

In order to do this, we must refactor the computation code to be aware of multiple functions during a scan. $acceptance criteria:$",,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,18,,0,2,0,9,0,0,0,,0,850,0,0,0,,Compute window functions with same sorting criteria simultaneously,"While computing window functions, we can take advantage of the general use case where they specify the same PARTITION and ORDER BY criteria.
We already take advantage of this fact by only sorting the internal tmp table as few times as possible. Filling the window function values within the internal tmp table can be performed during one pass / sort as well. Currently we do a pass for each window function.

In order to do this, we must refactor the computation code to be aware of multiple functions during a scan.",,0,0,0,0,0.0,"Compute window functions with same sorting criteria simultaneously $end$ While computing window functions, we can take advantage of the general use case where they specify the same PARTITION and ORDER BY criteria.
We already take advantage of this fact by only sorting the internal tmp table as few times as possible. Filling the window function values within the internal tmp table can be performed during one pass / sort as well. Currently we do a pass for each window function.

In order to do this, we must refactor the computation code to be aware of multiple functions during a scan. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
527,MDEV-10084,Task,MDEV,2016-05-19 07:32:08,,0,SQL batch united response,Put all OK packets in one physical packet for usual SQL batch separated with ';'.,,SQL batch united response $end$ Put all OK packets in one physical packet for usual SQL batch separated with ';'. $acceptance criteria:$,,Oleksandr Byelkin,Oleksandr Byelkin,Major,11,,0,3,1,1,0,0,0,,0,850,3,0,0,2016-06-29 08:11:51,SQL batch united response,Put all OK packets in one physical packet for usual SQL batch separated with ';'.,,0,0,0,0,0.0,SQL batch united response $end$ Put all OK packets in one physical packet for usual SQL batch separated with ';'. $acceptance criteria:$,0,0,0,0,0,0,0,984.65,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
528,MDEV-10134,Task,MDEV,2016-05-27 06:07:08,MDEV-10872,0,Add full support for DEFAULT,"Add full support for DEFAULT
- Support for functions:  DEFAULT (1+1)
- Referring to previous fields in the table
- Support for DEFAULT for blobs

In addition this task will lift the current restrictions for virtual columns:
- Non stored virtual columns can have almost any expression (including constant expression)
- No 300 character limit. (Limit is 64K for all stored expressions + column definitions)

Example:

create table t1 (a bigint default uuid_short(), b blob DEFAULT ""look at me!"" NOT NULL);

",,"Add full support for DEFAULT $end$ Add full support for DEFAULT
- Support for functions:  DEFAULT (1+1)
- Referring to previous fields in the table
- Support for DEFAULT for blobs

In addition this task will lift the current restrictions for virtual columns:
- Non stored virtual columns can have almost any expression (including constant expression)
- No 300 character limit. (Limit is 64K for all stored expressions + column definitions)

Example:

create table t1 (a bigint default uuid_short(), b blob DEFAULT ""look at me!"" NOT NULL);

 $acceptance criteria:$",,Michael Widenius,Michael Widenius,Major,13,,10,1,13,1,0,1,0,,0,850,0,1,0,2016-09-07 16:08:16,Add full support for DEFAULT,"Add full support for DEFAULT
- Support for functions:  DEFAULT (1+1)
- Referring to previous fields in the table
- Support for DEFAULT for blobs

In addition this task will lift the current restrictions for virtual columns:
- Non stored virtual columns can have almost any expression (including constant expression)
- No 300 character limit. (Limit is 64K for all stored expressions + column definitions)

Example:

create table t1 (a bigint default uuid_short(), b blob DEFAULT ""look at me!"" NOT NULL);

",,0,0,0,0,0.0,"Add full support for DEFAULT $end$ Add full support for DEFAULT
- Support for functions:  DEFAULT (1+1)
- Referring to previous fields in the table
- Support for DEFAULT for blobs

In addition this task will lift the current restrictions for virtual columns:
- Non stored virtual columns can have almost any expression (including constant expression)
- No 300 character limit. (Limit is 64K for all stored expressions + column definitions)

Example:

create table t1 (a bigint default uuid_short(), b blob DEFAULT ""look at me!"" NOT NULL);

 $acceptance criteria:$",0,0,0,0,0,0,0,2482.02,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,0,0.0
529,MDEV-10138,Task,MDEV,2016-05-27 17:06:48,MDEV-10872,0,Support for decimals up to 38 digits,"Currently DECIMAL only supports 30 decimals.
We should extend support to at least 38 decimals, to be more compatible with financial applications that depends on this limit

FLOAT and DOUBLE should still have the old limit of 30  decimals.

The change should be done so there is a minimum of notable change for old clients, except if they are using decimal fields with more than 30 decimals.

One change that is unavoidable is that decimal expressions will return results with more than 30 decimal and that if one converts a string to decimal it will have up to 38 decimals.

CREATE ... SELECT with a decimal with > 30 decimals will create a column
with a smaller range than before as we are trying to preserve the number of
decimals.

CREATE TABLE t1 SELECT 123456789012345678901234567890123456789012345.123456789012345678901234567890123456789012345;
Will change the value from:
99999999999999999999999999999999999.999999999999999999999999999999
to
99999999999999999999999999999.999999999999999999999999999999999999
",,"Support for decimals up to 38 digits $end$ Currently DECIMAL only supports 30 decimals.
We should extend support to at least 38 decimals, to be more compatible with financial applications that depends on this limit

FLOAT and DOUBLE should still have the old limit of 30  decimals.

The change should be done so there is a minimum of notable change for old clients, except if they are using decimal fields with more than 30 decimals.

One change that is unavoidable is that decimal expressions will return results with more than 30 decimal and that if one converts a string to decimal it will have up to 38 decimals.

CREATE ... SELECT with a decimal with > 30 decimals will create a column
with a smaller range than before as we are trying to preserve the number of
decimals.

CREATE TABLE t1 SELECT 123456789012345678901234567890123456789012345.123456789012345678901234567890123456789012345;
Will change the value from:
99999999999999999999999999999999999.999999999999999999999999999999
to
99999999999999999999999999999.999999999999999999999999999999999999
 $acceptance criteria:$",,Michael Widenius,Michael Widenius,Major,14,,1,3,1,1,0,3,0,,0,850,0,3,0,2016-09-07 16:04:31,Support for decimals up to 38 digits,"Currently DECIMAL only supports 30 decimals.
We should extend support to at least 38 decimals, to be more compatible with financial applications that depends on this limit

FLOAT and DOUBLE should still have the old limit of 30  decimals.

The change should be done so there is a minimum of notable change for old clients, except if they are using decimal fields with more than 30 decimals.

One change that is unavoidable is that decimal expressions will return results with more than 30 decimal and that if one converts a string to decimal it will have up to 38 decimals.

CREATE ... SELECT with a decimal with > 30 decimals will create a column
with a smaller range than before as we are trying to preserve the number of
decimals.

CREATE TABLE t1 SELECT 123456789012345678901234567890123456789012345.123456789012345678901234567890123456789012345;
Will change the value from:
99999999999999999999999999999999999.999999999999999999999999999999
to
99999999999999999999999999999.999999999999999999999999999999999999
",,0,0,0,0,0.0,"Support for decimals up to 38 digits $end$ Currently DECIMAL only supports 30 decimals.
We should extend support to at least 38 decimals, to be more compatible with financial applications that depends on this limit

FLOAT and DOUBLE should still have the old limit of 30  decimals.

The change should be done so there is a minimum of notable change for old clients, except if they are using decimal fields with more than 30 decimals.

One change that is unavoidable is that decimal expressions will return results with more than 30 decimal and that if one converts a string to decimal it will have up to 38 decimals.

CREATE ... SELECT with a decimal with > 30 decimals will create a column
with a smaller range than before as we are trying to preserve the number of
decimals.

CREATE TABLE t1 SELECT 123456789012345678901234567890123456789012345.123456789012345678901234567890123456789012345;
Will change the value from:
99999999999999999999999999999999999.999999999999999999999999999999
to
99999999999999999999999999999.999999999999999999999999999999999999
 $acceptance criteria:$",0,0,0,0,0,0,0,2470.95,4,1,0.25,1,0.25,1,0.25,1,0.25,0,0.0
530,MDEV-10141,Task,MDEV,2016-05-27 17:30:28,MDEV-10872,0,Add support for INTERSECT (and common parts for EXCEPT),"The INTERSECT/EXCEPT clause has this general form:

select_statement INTERSECT select_statement

select_statement is any SELECT statement without an ORDER BY, LIMIT, or FOR UPDATE clause.

The INTERSECT operator computes the set intersection of the rows returned by the involved SELECT statements. A row is in the intersection of two result sets if it appears in both result sets.

The result of INTERSECT does not contain any duplicate rows unless the ALL option is specified. With ALL, a row that has m duplicates in the left table and n duplicates in the right table will appear min(m,n) times in the result set.


*Steps to implement INTERSECT/EXCEPT*

1. Functionality
  1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  1.2. ALL clause
  1.3. Recursive CTE support
2. Optimizations
  2.1 Conversion to nested JOIN
  2.2 Optimize unique index


1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  
  Main idea:

  First query works like with UNION (distinct) select_union (inherited from select_result) results interceptor and collect data in a temporary table with an unique index.

  Second (and so on) has its own result interceptor
    - EXCEPT
      a)  just delete records from the table as soon as finds duplicate.
      b) (by Sergei Petrunia) make table of second select, then sent record of first not found in the table of second.
    - INTERSECT
      a) has its own table (Igor's idea to have 2 tables and switch them if there is more then 2 SELECTs) and pass through only ""duplicate"" records (in case of 2 SELECTs it can just return that rows).
      b) (Final) Add 1 hidden field and write there number of stage (0,1,2 and so on). At the beginning we write 0 when first select collects data. Next stage number N will be written only if we already have there N-1 in the found row (if no then record can be deleted). Finally records marked with last N is the result. If no record was marked during iteration we should stop and return empty set.

  UNION/EXCEPT/INTERSECT has different priority and they can be put on one level of UNIT/SELECT tree (SELECT_LEX/SELECT_LEX_UNIT) only in case if execution corresponds order. For beginner I think better to divide levels. Later (as optimization) possible to ""join"" levels in case sequential execution order and eliminate so called ""fake_select"" if results can be passed without processing.

1.2. ALL clause

  In basic case can we build non unique index and work with all records which fit the same way as in 1.1.

1.3. Recursive CTE support


2.1 Conversion to nested JOIN

  In many cases (when there is no grouping operations) the operation can be put in one SELECT with joining all records (better when there is unique indices from both sides)

2.2 Optimize unique index

  There is cases when we know that there is some unique set of field (also can work for UNION but need the same unique set from the other end)
  a) PRIMARY/UNIQUE index in source table, or tables if TABLE1.UNIQUE_FIELD = TABLE2.UNIQUE_FIELD used
  b) Result of GROUP BY is unique
  It is also can be used for JOIN of 2.1 step.



In conversation with Sergei Petrunia we got idea of ""oppening"" brackets with not standart operations:

S1  intersect (S2 union S3) can be unrolled to:
1) add S1 records with marker 0
2) find S2 records with marker 0 and put marker 2
3) find S3 records with marker 0 and put marker 2
3.1) filter tmp table with marker 2

S1 except (S2 except S2) :
1) add S1 records with marker 0
2) find S2 records with marker 0 and put marker 2
3) find S3 records with marker 2 and put marker 0
3.1) filter tmp table with marker not 2

...

The question is: is it possible all brackets ""open"" in such way?
",,"Add support for INTERSECT (and common parts for EXCEPT) $end$ The INTERSECT/EXCEPT clause has this general form:

select_statement INTERSECT select_statement

select_statement is any SELECT statement without an ORDER BY, LIMIT, or FOR UPDATE clause.

The INTERSECT operator computes the set intersection of the rows returned by the involved SELECT statements. A row is in the intersection of two result sets if it appears in both result sets.

The result of INTERSECT does not contain any duplicate rows unless the ALL option is specified. With ALL, a row that has m duplicates in the left table and n duplicates in the right table will appear min(m,n) times in the result set.


*Steps to implement INTERSECT/EXCEPT*

1. Functionality
  1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  1.2. ALL clause
  1.3. Recursive CTE support
2. Optimizations
  2.1 Conversion to nested JOIN
  2.2 Optimize unique index


1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  
  Main idea:

  First query works like with UNION (distinct) select_union (inherited from select_result) results interceptor and collect data in a temporary table with an unique index.

  Second (and so on) has its own result interceptor
    - EXCEPT
      a)  just delete records from the table as soon as finds duplicate.
      b) (by Sergei Petrunia) make table of second select, then sent record of first not found in the table of second.
    - INTERSECT
      a) has its own table (Igor's idea to have 2 tables and switch them if there is more then 2 SELECTs) and pass through only ""duplicate"" records (in case of 2 SELECTs it can just return that rows).
      b) (Final) Add 1 hidden field and write there number of stage (0,1,2 and so on). At the beginning we write 0 when first select collects data. Next stage number N will be written only if we already have there N-1 in the found row (if no then record can be deleted). Finally records marked with last N is the result. If no record was marked during iteration we should stop and return empty set.

  UNION/EXCEPT/INTERSECT has different priority and they can be put on one level of UNIT/SELECT tree (SELECT_LEX/SELECT_LEX_UNIT) only in case if execution corresponds order. For beginner I think better to divide levels. Later (as optimization) possible to ""join"" levels in case sequential execution order and eliminate so called ""fake_select"" if results can be passed without processing.

1.2. ALL clause

  In basic case can we build non unique index and work with all records which fit the same way as in 1.1.

1.3. Recursive CTE support


2.1 Conversion to nested JOIN

  In many cases (when there is no grouping operations) the operation can be put in one SELECT with joining all records (better when there is unique indices from both sides)

2.2 Optimize unique index

  There is cases when we know that there is some unique set of field (also can work for UNION but need the same unique set from the other end)
  a) PRIMARY/UNIQUE index in source table, or tables if TABLE1.UNIQUE_FIELD = TABLE2.UNIQUE_FIELD used
  b) Result of GROUP BY is unique
  It is also can be used for JOIN of 2.1 step.



In conversation with Sergei Petrunia we got idea of ""oppening"" brackets with not standart operations:

S1  intersect (S2 union S3) can be unrolled to:
1) add S1 records with marker 0
2) find S2 records with marker 0 and put marker 2
3) find S3 records with marker 0 and put marker 2
3.1) filter tmp table with marker 2

S1 except (S2 except S2) :
1) add S1 records with marker 0
2) find S2 records with marker 0 and put marker 2
3) find S3 records with marker 2 and put marker 0
3.1) filter tmp table with marker not 2

...

The question is: is it possible all brackets ""open"" in such way?
 $acceptance criteria:$",,Michael Widenius,Michael Widenius,Major,38,,2,2,4,3,0,12,0,,0,850,2,5,0,2016-11-09 13:52:37,Add support for INTERSECT and EXCEPT,"The INTERSECT/EXCEPT clause has this general form:

select_statement INTERSECT [ ALL ] select_statement

select_statement is any SELECT statement without an ORDER BY, LIMIT, or FOR UPDATE clause.

The INTERSECT operator computes the set intersection of the rows returned by the involved SELECT statements. A row is in the intersection of two result sets if it appears in both result sets.

The result of INTERSECT does not contain any duplicate rows unless the ALL option is specified. With ALL, a row that has m duplicates in the left table and n duplicates in the right table will appear min(m,n) times in the result set.


*Steps to implement INTERSECT/EXCEPT*

1. Functionality
  1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  1.2. ALL clause
  1.3. Recursive CTE support
2. Optimizations
  2.1 Conversion to nested JOIN
  2.2 Optimize unique index


1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  
  Main idea:

  First query works like with UNION (distinct) select_union (inherited from select_result) results interceptor and collect data in a temporary table with an unique index.

  Second (and so on) has its own result interceptor
    - EXCEPT just delete records from the table as soon as finds duplicate.
    - INTERSECT has its own table (Igor's idea to have 2 tables and switch them if there is more then 2 SELECTs) and pass through only ""duplicate"" records (in case of 2 SELECTs it can just return that rows).


  UNION/EXCEPT/INTERSECT has different priority and they can be put on one level of UNIT/SELECT tree (SELECT_LEX/SELECT_LEX_UNIT) only in case if execution corresponds order. For beginner I think better to divide levels. Later (as optimization) possible to ""join"" levels in case sequential execution order and eliminate so called ""fake_select"" if results can be passed without processing.

1.2. ALL clause

  In basic case can we build non unique index and work with all records which fit the same way as in 1.1.

1.3. Recursive CTE support


2.1 Conversion to nested JOIN

  In many cases (when there is no grouping operations) the operation can be put in one SELECT with joining all records (better when there is unique indices from both sides)

2.2 Optimize unique index

  There is cases when we know that there is some unique set of field (also can work for UNION but need the same unique set from the other end)
  a) PRIMARY/UNIQUE index in source table, or tables if TABLE1.UNIQUE_FIELD = TABLE2.UNIQUE_FIELD used
  b) Result of GROUP BY is unique
  It is also can be used for JOIN of 2.1 step.

",,2,5,0,224,0.504545,"Add support for INTERSECT and EXCEPT $end$ The INTERSECT/EXCEPT clause has this general form:

select_statement INTERSECT [ ALL ] select_statement

select_statement is any SELECT statement without an ORDER BY, LIMIT, or FOR UPDATE clause.

The INTERSECT operator computes the set intersection of the rows returned by the involved SELECT statements. A row is in the intersection of two result sets if it appears in both result sets.

The result of INTERSECT does not contain any duplicate rows unless the ALL option is specified. With ALL, a row that has m duplicates in the left table and n duplicates in the right table will appear min(m,n) times in the result set.


*Steps to implement INTERSECT/EXCEPT*

1. Functionality
  1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  1.2. ALL clause
  1.3. Recursive CTE support
2. Optimizations
  2.1 Conversion to nested JOIN
  2.2 Optimize unique index


1.1. Basic (UNION LIKE) functionality with ORDER BY, GROUP BY, LIMIT etc. (no ALL, no recursive CTE)
  
  Main idea:

  First query works like with UNION (distinct) select_union (inherited from select_result) results interceptor and collect data in a temporary table with an unique index.

  Second (and so on) has its own result interceptor
    - EXCEPT just delete records from the table as soon as finds duplicate.
    - INTERSECT has its own table (Igor's idea to have 2 tables and switch them if there is more then 2 SELECTs) and pass through only ""duplicate"" records (in case of 2 SELECTs it can just return that rows).


  UNION/EXCEPT/INTERSECT has different priority and they can be put on one level of UNIT/SELECT tree (SELECT_LEX/SELECT_LEX_UNIT) only in case if execution corresponds order. For beginner I think better to divide levels. Later (as optimization) possible to ""join"" levels in case sequential execution order and eliminate so called ""fake_select"" if results can be passed without processing.

1.2. ALL clause

  In basic case can we build non unique index and work with all records which fit the same way as in 1.1.

1.3. Recursive CTE support


2.1 Conversion to nested JOIN

  In many cases (when there is no grouping operations) the operation can be put in one SELECT with joining all records (better when there is unique indices from both sides)

2.2 Optimize unique index

  There is cases when we know that there is some unique set of field (also can work for UNION but need the same unique set from the other end)
  a) PRIMARY/UNIQUE index in source table, or tables if TABLE1.UNIQUE_FIELD = TABLE2.UNIQUE_FIELD used
  b) Result of GROUP BY is unique
  It is also can be used for JOIN of 2.1 step.

 $acceptance criteria:$",7,1,1,1,1,1,1,3980.37,5,1,0.2,1,0.2,1,0.2,1,0.2,0,0.0
531,MDEV-10142,Task,MDEV,2016-05-27 17:34:55,MDEV-11070,0,PL/SQL parser,"Add a new parser to understand Oracle PL/SQL dialect.

This task holds all work done on the infrastructure that is not part of other sub tasks",,"PL/SQL parser $end$ Add a new parser to understand Oracle PL/SQL dialect.

This task holds all work done on the infrastructure that is not part of other sub tasks $acceptance criteria:$",,Michael Widenius,Michael Widenius,Critical,38,,3,1,8,5,0,4,71,,0,850,1,0,0,2016-08-24 10:24:49,Pluggable parser,"Add a pluggable parser, to be able to more easily add support for SQLsyntax  from other databases.",,2,2,0,37,1.04545,"Pluggable parser $end$ Add a pluggable parser, to be able to more easily add support for SQLsyntax  from other databases. $acceptance criteria:$",4,1,1,1,1,1,1,2128.82,6,2,0.333333,2,0.333333,2,0.333333,2,0.333333,1,0.166667
532,MDEV-10224,Task,MDEV,2016-06-13 17:55:41,,0,10.0.26 merge,"* 5.5 (/) 5.5.50
* InnoDB (/) 5.6.31
* XtraDB (/) 5.6.30-76.3
* P_S (/) 5.6.31
* Connect (/) 1.04.0006
* Spider (/) _nothing to do_
* PCRE (/) 8.39
* Mroonga (/) _nothing to do_
* TokuDB (/) 5.6.30-76.3
",,"10.0.26 merge $end$ * 5.5 (/) 5.5.50
* InnoDB (/) 5.6.31
* XtraDB (/) 5.6.30-76.3
* P_S (/) 5.6.31
* Connect (/) 1.04.0006
* Spider (/) _nothing to do_
* PCRE (/) 8.39
* Mroonga (/) _nothing to do_
* TokuDB (/) 5.6.30-76.3
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,10,,5,0,5,1,0,4,0,,0,850,0,0,0,2016-06-14 12:09:44,10.0.26 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
",,0,4,0,22,0.956522,"10.0.26 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
 $acceptance criteria:$",4,1,1,1,1,1,1,18.2333,37,17,0.459459,11,0.297297,9,0.243243,9,0.243243,8,0.216216
533,MDEV-10296,Task,MDEV,2016-06-28 11:33:14,,0,Multi-instance table cache,Improve scalability by implementing multi-instance table cache.,,Multi-instance table cache $end$ Improve scalability by implementing multi-instance table cache. $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,26,,0,8,1,5,0,0,0,,0,850,0,0,0,,Multi-instance table cache,Improve scalability by implementing multi-instance table cache.,,0,0,0,0,0.0,Multi-instance table cache $end$ Improve scalability by implementing multi-instance table cache. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,14,1,0.0714286,0,0.0,0,0.0,0,0.0,0,0.0
534,MDEV-10297,Task,MDEV,2016-06-28 13:20:24,,0,Threadpool : add priorization features,"There is  feature that is there in Percona tree 5.5+  and also in MySQL/Enterprise
but not in MariaDB is priorization

- By default, connection that is in transaction gets a priority boost (which translated to 
in transaction= low prio, outside of transaction => high prio). The first statement after ""begin"" does not need to be high prio (this transaction does not actually use any resources)

-  We need to have a variable that will allow to specify per-connection or global scheduling behavior (auto = based on transaction status, high - always high prio, low- always low prio)
This way, it would be easy to return back to FIFO scheduling if someone prefers that (i.e set global threadpool_priority=high). Or for a connection to be ""nice"" (setting priority = low regardless of transaction status), if it does not unimportant work . This variable is akin to Percona's threadpool_high_prio_mode=[transactions|statements|none] , but with an understandable name. 

- On Unix ,we'll use separate queues for high priority and low priority items. high priority queue is checked before low prio when an event is dequeued.

- On Windows threadpool,  it is possible to 'yield' low prio work items.

- On Unix, to avoid starvation of the low prio items, periodically the old items in low prio queue need to be merged at the end of high prio queue. We can use 'thread_pool_kickup_timer' logic, like MySQL Enterprise is doing it.

- We won't be using 'throttling of low prio queue' like Percona, as restricting the number of active+waiting threads can easily lead to a deadlock (single global lock or table lock would suffice to check that)",,"Threadpool : add priorization features $end$ There is  feature that is there in Percona tree 5.5+  and also in MySQL/Enterprise
but not in MariaDB is priorization

- By default, connection that is in transaction gets a priority boost (which translated to 
in transaction= low prio, outside of transaction => high prio). The first statement after ""begin"" does not need to be high prio (this transaction does not actually use any resources)

-  We need to have a variable that will allow to specify per-connection or global scheduling behavior (auto = based on transaction status, high - always high prio, low- always low prio)
This way, it would be easy to return back to FIFO scheduling if someone prefers that (i.e set global threadpool_priority=high). Or for a connection to be ""nice"" (setting priority = low regardless of transaction status), if it does not unimportant work . This variable is akin to Percona's threadpool_high_prio_mode=[transactions|statements|none] , but with an understandable name. 

- On Unix ,we'll use separate queues for high priority and low priority items. high priority queue is checked before low prio when an event is dequeued.

- On Windows threadpool,  it is possible to 'yield' low prio work items.

- On Unix, to avoid starvation of the low prio items, periodically the old items in low prio queue need to be merged at the end of high prio queue. We can use 'thread_pool_kickup_timer' logic, like MySQL Enterprise is doing it.

- We won't be using 'throttling of low prio queue' like Percona, as restricting the number of active+waiting threads can easily lead to a deadlock (single global lock or table lock would suffice to check that) $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,15,,0,1,0,3,0,3,0,,0,850,1,0,0,2016-07-12 13:25:59,Port percona threadpool improvements to MariaDB tree,"Main feature that is there in Percona tree 5.5+ and later an not in hours is
1. Priorization mode (in-transaction queries can be prioritized higher, nontransaction can be prioritized higher).
https://www.percona.com/blog/2014/01/29/percona-server-thread-pool-improvements
has benchmark details.
2. Some smaller stuff to play well with performance schema (idle waits for network), instrumentation for threads ",,1,2,0,304,4.22951,"Port percona threadpool improvements to MariaDB tree $end$ Main feature that is there in Percona tree 5.5+ and later an not in hours is
1. Priorization mode (in-transaction queries can be prioritized higher, nontransaction can be prioritized higher).
https://www.percona.com/blog/2014/01/29/percona-server-thread-pool-improvements
has benchmark details.
2. Some smaller stuff to play well with performance schema (idle waits for network), instrumentation for threads  $acceptance criteria:$",3,1,1,1,1,1,1,336.083,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
535,MDEV-10340,Task,MDEV,2016-07-06 21:33:55,,0,support COM_RESET_CONNECTION,"MySQL added the COM_RESET_CONNECTION command as a lightweight way to reset the session state. This is an extremely important feature to ensure pooled connections have a predictable state. It's also important because MySQL clients are beginning to support this command.


[https://dev.mysql.com/doc/internals/en/com-reset-connection.html]
[https://dev.mysql.com/doc/refman/5.7/en/mysql-reset-connection.html]",,"support COM_RESET_CONNECTION $end$ MySQL added the COM_RESET_CONNECTION command as a lightweight way to reset the session state. This is an extremely important feature to ensure pooled connections have a predictable state. It's also important because MySQL clients are beginning to support this command.


[https://dev.mysql.com/doc/internals/en/com-reset-connection.html]
[https://dev.mysql.com/doc/refman/5.7/en/mysql-reset-connection.html] $acceptance criteria:$",,Ben Page,Ben Page,Major,23,,0,5,1,3,0,0,0,,0,850,5,0,0,2016-10-13 06:24:27,support COM_RESET_CONNECTION,"MySQL added the COM_RESET_CONNECTION command as a lightweight way to reset the session state. This is an extremely important feature to ensure pooled connections have a predictable state. It's also important because MySQL clients are beginning to support this command.


[https://dev.mysql.com/doc/internals/en/com-reset-connection.html]
[https://dev.mysql.com/doc/refman/5.7/en/mysql-reset-connection.html]",,0,0,0,0,0.0,"support COM_RESET_CONNECTION $end$ MySQL added the COM_RESET_CONNECTION command as a lightweight way to reset the session state. This is an extremely important feature to ensure pooled connections have a predictable state. It's also important because MySQL clients are beginning to support this command.


[https://dev.mysql.com/doc/internals/en/com-reset-connection.html]
[https://dev.mysql.com/doc/refman/5.7/en/mysql-reset-connection.html] $acceptance criteria:$",0,0,0,0,0,0,1,2360.83,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
536,MDEV-10343,Technical task,MDEV,2016-07-07 08:55:07,,0,sql_mode=ORACLE: Providing compatibility for basic SQL data types,"The following should be supported in MariaDB when running with {{sql_mode=ORACLE}}:

* {{VARCHAR2}} - a synonym to {{VARCHAR}}
* {{NUMBER}} - a synonym to {{DECIMAL}}
* {{DATE}} (with time portion) - a synonym to MariaDB {{DATETIME}}
* {{RAW}} - a synonym to {{VARBINARY}}
* {{CLOB}} - a synonym to {{LONGTEXT}}
* {{BLOB}} - a synonym to {{LONGBLOB}}",,"sql_mode=ORACLE: Providing compatibility for basic SQL data types $end$ The following should be supported in MariaDB when running with {{sql_mode=ORACLE}}:

* {{VARCHAR2}} - a synonym to {{VARCHAR}}
* {{NUMBER}} - a synonym to {{DECIMAL}}
* {{DATE}} (with time portion) - a synonym to MariaDB {{DATETIME}}
* {{RAW}} - a synonym to {{VARBINARY}}
* {{CLOB}} - a synonym to {{LONGTEXT}}
* {{BLOB}} - a synonym to {{LONGBLOB}} $acceptance criteria:$",,Dmitry Tolpeko,Dmitry Tolpeko,Major,19,,0,1,0,5,0,5,0,,0,850,1,0,0,2016-07-07 08:55:07,Providing compatibility for basic SQL data types,"The following should be supported in MariaDB:

* VARCHAR2
* NUMBER
* DATE (with time portion)
* RAW
* CLOB (as a large object)
* BLOB (as a large object)",,1,4,0,54,1.025,"Providing compatibility for basic SQL data types $end$ The following should be supported in MariaDB:

* VARCHAR2
* NUMBER
* DATE (with time portion)
* RAW
* CLOB (as a large object)
* BLOB (as a large object) $acceptance criteria:$",5,1,1,1,1,1,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
537,MDEV-10384,Task,MDEV,2016-07-16 16:02:04,,0,Refactor threading in server startup (Windows),"Specifically on Windows, server startup creates too many unnecessary threads. There is a thread that does nothing but waits for shutdown event, and possibly thread that handles socket connections, and possibly a thread that  handles named pipe connections, and perhaps a thread that does shared memory connections. And there is a main thread that does nothing but waits for all other threads

None of that is necessary, does not look good and is prone to races (shutdown thread vs main thread for example).

 It could be relatively easily rewritten to run in the main thread.
IOCP/ GetQueuedCompletionStatus loop in the main thread could handle l sockets and named pipe connections directly .Also waiting for events can be rewritten that they do not block current thread, RegisterWaitForSingleObject does that, and can be passed handler routine   which forwards event to the main thread's  completion port via PostQueuedCompletionStatus",,"Refactor threading in server startup (Windows) $end$ Specifically on Windows, server startup creates too many unnecessary threads. There is a thread that does nothing but waits for shutdown event, and possibly thread that handles socket connections, and possibly a thread that  handles named pipe connections, and perhaps a thread that does shared memory connections. And there is a main thread that does nothing but waits for all other threads

None of that is necessary, does not look good and is prone to races (shutdown thread vs main thread for example).

 It could be relatively easily rewritten to run in the main thread.
IOCP/ GetQueuedCompletionStatus loop in the main thread could handle l sockets and named pipe connections directly .Also waiting for events can be rewritten that they do not block current thread, RegisterWaitForSingleObject does that, and can be passed handler routine   which forwards event to the main thread's  completion port via PostQueuedCompletionStatus $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,11,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-01-24 12:04:38,Refactor threading in server startup (Windows),"Specifically on Windows, server startup creates too many unnecessary threads. There is a thread that does nothing but waits for shutdown event, and possibly thread that handles socket connections, and possibly a thread that  handles named pipe connections, and perhaps a thread that does shared memory connections. And there is a main thread that does nothing but waits for all other threads

None of that is necessary, does not look good and is prone to races (shutdown thread vs main thread for example).

 It could be relatively easily rewritten to run in the main thread.
IOCP/ GetQueuedCompletionStatus loop in the main thread could handle l sockets and named pipe connections directly .Also waiting for events can be rewritten that they do not block current thread, RegisterWaitForSingleObject does that, and can be passed handler routine   which forwards event to the main thread's  completion port via PostQueuedCompletionStatus",,0,0,0,0,0.0,"Refactor threading in server startup (Windows) $end$ Specifically on Windows, server startup creates too many unnecessary threads. There is a thread that does nothing but waits for shutdown event, and possibly thread that handles socket connections, and possibly a thread that  handles named pipe connections, and perhaps a thread that does shared memory connections. And there is a main thread that does nothing but waits for all other threads

None of that is necessary, does not look good and is prone to races (shutdown thread vs main thread for example).

 It could be relatively easily rewritten to run in the main thread.
IOCP/ GetQueuedCompletionStatus loop in the main thread could handle l sockets and named pipe connections directly .Also waiting for events can be rewritten that they do not block current thread, RegisterWaitForSingleObject does that, and can be passed handler routine   which forwards event to the main thread's  completion port via PostQueuedCompletionStatus $acceptance criteria:$",0,0,0,0,0,0,0,13364.0,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
538,MDEV-10411,Technical task,MDEV,2016-07-21 11:22:31,,0,sql_mode=ORACLE: Providing compatibility for basic PL/SQL constructs,Add support for PL/SQL stored procedures in MariaDB. ,,sql_mode=ORACLE: Providing compatibility for basic PL/SQL constructs $end$ Add support for PL/SQL stored procedures in MariaDB.  $acceptance criteria:$,,Dmitry Tolpeko,Dmitry Tolpeko,Major,17,,0,3,0,5,0,1,0,,0,850,3,0,0,2016-07-21 11:22:31,Providing compatibility for basic PL/SQL constructs,Add support for PL/SQL stored procedures in MariaDB. ,,1,0,0,1,0.0588235,Providing compatibility for basic PL/SQL constructs $end$ Add support for PL/SQL stored procedures in MariaDB.  $acceptance criteria:$,1,1,0,0,0,0,1,0.0,1,1,1.0,1,1.0,1,1.0,1,1.0,1,1.0
539,MDEV-10483,Task,MDEV,2016-08-02 13:52:05,,0,5.5.51 merge,"* mysql-5.5.51
* xtradb",,"5.5.51 merge $end$ * mysql-5.5.51
* xtradb $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,5,,1,0,1,1,0,0,0,,0,850,0,0,0,2016-08-03 10:23:34,5.5.51 merge,"* mysql-5.5.51
* xtradb",,0,0,0,0,0.0,"5.5.51 merge $end$ * mysql-5.5.51
* xtradb $acceptance criteria:$",0,0,0,0,0,0,0,20.5167,38,18,0.473684,12,0.315789,10,0.263158,10,0.263158,9,0.236842
540,MDEV-10570,Task,MDEV,2016-08-17 11:09:51,MDEV-10459,0,Integrate existing DML only “Flashback” for MariaDB Server 10.2,"An implementation exists which needs to be
- ported to MariaDB Server 10.2. 
- tested with 10.2
- mysqlbinlog changes need to be tested against 10.2

This Sub-Tasks does not cover new implementations to be able to ""undo"" DDL based changes.

==== Description ====

Flashback can rollback the instances/databases/tables to an old snapshot.
It's implement on Server-Level by full image format binary logs (--binlog-row-image=FULL), so it supports all engines.
Currently, it’s a feature inside mysqlbinlog tool (with --flashback arguments).

Because the flashback binlog events will store in the memory, you should check if there is enough memory in your machine.

==== New Arguments ====

--flashback (-B)
It will let mysqlbinlog to work on FLASHBACK mode.

==== Example ====

I have a table ""t"" in database ""test"", we can compare the output with ""--flashback"" and without.

# client/mysqlbinlog /data/mysqldata_10.0/binlog/mysql-bin.000001 -vv -d test -T t --start-datetime=""2013-03-27 14:54:00"" > /tmp/1.sql
# client/mysqlbinlog /data/mysqldata_10.0/binlog/mysql-bin.000001 -vv -d test -T t --start-datetime=""2013-03-27 14:54:00"" -B > /tmp/2.sql

Then, importing the output flashback file (/tmp/2.log), it can flashback your database/table to the special time (--start-datetime).
And if you know the exact postion, "" -- start-postion "" is also works, mysqlbinlog will output the flashback logs that can flashback to "" -- start-postion "" position.

==== Implement ====

1. As we know, if binlog_format is ROW (binlog-row-image=FULL in 10.1 and later), all columns value are store in the row event, so we can get the data before mis-operation.

2. Just do following things:

  2.1 Change Event Type, INSERT->DELETE, DELETE->INSERT.
  For example:
INSERT INTO t VALUES ( ... )  ---> DELETE FROM t WHERE ...
DELETE FROM t ...  ---> INSERT INTO t VALUES ( ... )

  2.2 For Update_Event, swapping the SET part and WHERE part.
  For example:
UPDATE t SET cols1 = vals1 WHERE cols2 = vals2
--->
UPDATE t SET cols2 = vals2 WHERE cols1 = vals1

  2.3 For Multi-Rows Event, reverse the rows sequence, from the last row to the first row.
  For example:
DELETE FROM t WHERE id=1; DELETE FROM t WHERE id=2; ... ; DELETE FROM t WHERE id=n;
--->
DELETE FROM t WHERE id=n; ... ; DELETE FROM t WHERE id=2; DELETE FROM t WHERE id=1;

  2.4 Output those events from the last one to the first one which mis-operation happened.
  For example:
Evnet(1); Event(2); ... ; Event( n );
--->
Event( n ); ... ; Event(2); EVent(1);

  2.5 Import the output file, then all the data will be recovered by inverse operations of mis-oprerations.
",,"Integrate existing DML only “Flashback” for MariaDB Server 10.2 $end$ An implementation exists which needs to be
- ported to MariaDB Server 10.2. 
- tested with 10.2
- mysqlbinlog changes need to be tested against 10.2

This Sub-Tasks does not cover new implementations to be able to ""undo"" DDL based changes.

==== Description ====

Flashback can rollback the instances/databases/tables to an old snapshot.
It's implement on Server-Level by full image format binary logs (--binlog-row-image=FULL), so it supports all engines.
Currently, it’s a feature inside mysqlbinlog tool (with --flashback arguments).

Because the flashback binlog events will store in the memory, you should check if there is enough memory in your machine.

==== New Arguments ====

--flashback (-B)
It will let mysqlbinlog to work on FLASHBACK mode.

==== Example ====

I have a table ""t"" in database ""test"", we can compare the output with ""--flashback"" and without.

# client/mysqlbinlog /data/mysqldata_10.0/binlog/mysql-bin.000001 -vv -d test -T t --start-datetime=""2013-03-27 14:54:00"" > /tmp/1.sql
# client/mysqlbinlog /data/mysqldata_10.0/binlog/mysql-bin.000001 -vv -d test -T t --start-datetime=""2013-03-27 14:54:00"" -B > /tmp/2.sql

Then, importing the output flashback file (/tmp/2.log), it can flashback your database/table to the special time (--start-datetime).
And if you know the exact postion, "" -- start-postion "" is also works, mysqlbinlog will output the flashback logs that can flashback to "" -- start-postion "" position.

==== Implement ====

1. As we know, if binlog_format is ROW (binlog-row-image=FULL in 10.1 and later), all columns value are store in the row event, so we can get the data before mis-operation.

2. Just do following things:

  2.1 Change Event Type, INSERT->DELETE, DELETE->INSERT.
  For example:
INSERT INTO t VALUES ( ... )  ---> DELETE FROM t WHERE ...
DELETE FROM t ...  ---> INSERT INTO t VALUES ( ... )

  2.2 For Update_Event, swapping the SET part and WHERE part.
  For example:
UPDATE t SET cols1 = vals1 WHERE cols2 = vals2
--->
UPDATE t SET cols2 = vals2 WHERE cols1 = vals1

  2.3 For Multi-Rows Event, reverse the rows sequence, from the last row to the first row.
  For example:
DELETE FROM t WHERE id=1; DELETE FROM t WHERE id=2; ... ; DELETE FROM t WHERE id=n;
--->
DELETE FROM t WHERE id=n; ... ; DELETE FROM t WHERE id=2; DELETE FROM t WHERE id=1;

  2.4 Output those events from the last one to the first one which mis-operation happened.
  For example:
Evnet(1); Event(2); ... ; Event( n );
--->
Event( n ); ... ; Event(2); EVent(1);

  2.5 Import the output file, then all the data will be recovered by inverse operations of mis-oprerations.
 $acceptance criteria:$",,Ralf Gebhardt,Ralf Gebhardt,Major,17,,1,1,1,1,0,5,0,,0,850,1,0,0,2016-09-14 13:28:32,Integrate existing DML only “Flashback” for MariaDB Server 10.2,"An implementation exists which needs to be
- ported to MariaDB Server 10.2. 
- tested with 10.2
- mysqlbinlog changes need to be tested against 10.2

This Sub-Tasks does not cover new implementations to be able to ""undo"" DDL based changes.",,0,5,0,371,7.0,"Integrate existing DML only “Flashback” for MariaDB Server 10.2 $end$ An implementation exists which needs to be
- ported to MariaDB Server 10.2. 
- tested with 10.2
- mysqlbinlog changes need to be tested against 10.2

This Sub-Tasks does not cover new implementations to be able to ""undo"" DDL based changes. $acceptance criteria:$",5,1,1,1,1,1,1,674.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
541,MDEV-10577,Technical task,MDEV,2016-08-17 14:52:12,,0,sql_mode=ORACLE:  %TYPE in variable declarations,"When running in {{sql_mode=ORACLE}}, MariaDB should support {{%TYPE}} in variable declarations, in function and procedure parameter delcarations, and function {{RETURN}} clauses:

{code:sql}
  tmp t1.a%TYPE;
{code}
The above statement declares a variable that has the same data type with the column {{a}} in the table {{t1}}.

Example:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);

DROP FUNCTION f1;
CREATE FUNCTION f1(prm t1.a%TYPE) RETURN t1.a%TYPE
AS
  tmp t1.a%TYPE;
BEGIN
  SELECT MAX(a) INTO tmp FROM t1;
  RETURN tmp + prm;
END;
/
SELECT f1(5) FROM DUAL;
{code}
{noformat}
SQL> 
     F1(5)
----------
	25
{noformat}

h1. Scope of this task
This task will implement %TYPE only for local variables and parameters. Using %TYPE in stored function RETURN clause will be done under terms of MDEV-11210.

h1. Oracle behavior

h2. 1. %TYPE and missing tables during routine {{CREATE}} time
Oracle checks if the referenced table exists during the routine {{CREATE}} time. If the referenced table does not exists, a warning is issued, but the routine is created.
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
 a t1.a%TYPE := 123;
BEGIN
  EXECUTE IMMEDIATE 'INSERT INTO t1 (a) VALUES(:a)' USING a;
END;
/
SHOW ERRORS;
{code}
{noformat}
LINE/COL ERROR
-------- -----------------------------------------------------------------
3/4	 PL/SQL: Item ignored
3/4	 PLS-00201: identifier 'T1.A' must be declared
5/3	 PL/SQL: Statement ignored
5/59	 PLS-00320: the declaration of the type of this expression is
	 incomplete or malformed
{noformat}

The routine created with a missing identifier warning can be used as soon as the referenced table is created:
{code}
CREATE TABLE t1 (a INT);
CALL p1();
SELECT * FROM t1;
{code}
{noformat}
	 A
----------
       123
{noformat}
Notice, the {{CALL}} statement inserted {{123}} into {{t1}}.


h2. 2. Dropping the referenced table before the routine execution leads to an error on {{CALL}}
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
  a t1.a%TYPE:='xxx';
BEGIN
  NULL;
END;
/
CALL p1();
DROP TABLE t1;
CALL p1();
SELECT * FROM user_errors;
{code}
{noformat}
NAME			       TYPE	      SEQUENCE	     LINE   POSITION
------------------------------ ------------ ---------- ---------- ---------- 
TEXT
--------------------------------------------------------------------------------
ATTRIBUTE MESSAGE_NUMBER
--------- --------------
P1			       PROCEDURE	     2		3	   5
PL/SQL: Item ignored
ERROR		       0

P1			       PROCEDURE	     1		3	   5
PLS-00201: identifier 'T1.A' must be declared
ERROR		     201

NAME			       TYPE	      SEQUENCE	     LINE   POSITION
------------------------------ ------------ ---------- ---------- ----------
TEXT
--------------------------------------------------------------------------------
ATTRIBUTE MESSAGE_NUMBER
--------- --------------
{noformat}



h2. 3. Dropping the referenced table during the routine execution does not affect %TYPE variables
{code:sql}
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a VARCHAR(10));
CREATE TABLE t2 (a VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  EXECUTE IMMEDIATE 'DROP TABLE t1';
  DECLARE
    a t1.a%TYPE:='xxx';
  BEGIN
    EXECUTE IMMEDIATE 'INSERT INTO t2 VALUES (:1)' USING a;
  END;
END;
/
CALL p1();
SELECT * FROM t2;
{code}
{noformat}
A
----------
xxx
{noformat}

Notice, the variable was declared after {{DROP TABLE}}.

h2. MariaDB behavior
To make MariaDB reproduce Oracle's behavior as close as possible, we'll assing data type to %TYPE variables at sp_rcontext::create() time.

Like Oracle, we'll not require the referenced tables to exists at the routine {{CREATE}} time. Unlike Oracle, we won't check the referenced tables and won't generate warnings.
",,"sql_mode=ORACLE:  %TYPE in variable declarations $end$ When running in {{sql_mode=ORACLE}}, MariaDB should support {{%TYPE}} in variable declarations, in function and procedure parameter delcarations, and function {{RETURN}} clauses:

{code:sql}
  tmp t1.a%TYPE;
{code}
The above statement declares a variable that has the same data type with the column {{a}} in the table {{t1}}.

Example:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);

DROP FUNCTION f1;
CREATE FUNCTION f1(prm t1.a%TYPE) RETURN t1.a%TYPE
AS
  tmp t1.a%TYPE;
BEGIN
  SELECT MAX(a) INTO tmp FROM t1;
  RETURN tmp + prm;
END;
/
SELECT f1(5) FROM DUAL;
{code}
{noformat}
SQL> 
     F1(5)
----------
	25
{noformat}

h1. Scope of this task
This task will implement %TYPE only for local variables and parameters. Using %TYPE in stored function RETURN clause will be done under terms of MDEV-11210.

h1. Oracle behavior

h2. 1. %TYPE and missing tables during routine {{CREATE}} time
Oracle checks if the referenced table exists during the routine {{CREATE}} time. If the referenced table does not exists, a warning is issued, but the routine is created.
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
 a t1.a%TYPE := 123;
BEGIN
  EXECUTE IMMEDIATE 'INSERT INTO t1 (a) VALUES(:a)' USING a;
END;
/
SHOW ERRORS;
{code}
{noformat}
LINE/COL ERROR
-------- -----------------------------------------------------------------
3/4	 PL/SQL: Item ignored
3/4	 PLS-00201: identifier 'T1.A' must be declared
5/3	 PL/SQL: Statement ignored
5/59	 PLS-00320: the declaration of the type of this expression is
	 incomplete or malformed
{noformat}

The routine created with a missing identifier warning can be used as soon as the referenced table is created:
{code}
CREATE TABLE t1 (a INT);
CALL p1();
SELECT * FROM t1;
{code}
{noformat}
	 A
----------
       123
{noformat}
Notice, the {{CALL}} statement inserted {{123}} into {{t1}}.


h2. 2. Dropping the referenced table before the routine execution leads to an error on {{CALL}}
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
  a t1.a%TYPE:='xxx';
BEGIN
  NULL;
END;
/
CALL p1();
DROP TABLE t1;
CALL p1();
SELECT * FROM user_errors;
{code}
{noformat}
NAME			       TYPE	      SEQUENCE	     LINE   POSITION
------------------------------ ------------ ---------- ---------- ---------- 
TEXT
--------------------------------------------------------------------------------
ATTRIBUTE MESSAGE_NUMBER
--------- --------------
P1			       PROCEDURE	     2		3	   5
PL/SQL: Item ignored
ERROR		       0

P1			       PROCEDURE	     1		3	   5
PLS-00201: identifier 'T1.A' must be declared
ERROR		     201

NAME			       TYPE	      SEQUENCE	     LINE   POSITION
------------------------------ ------------ ---------- ---------- ----------
TEXT
--------------------------------------------------------------------------------
ATTRIBUTE MESSAGE_NUMBER
--------- --------------
{noformat}



h2. 3. Dropping the referenced table during the routine execution does not affect %TYPE variables
{code:sql}
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a VARCHAR(10));
CREATE TABLE t2 (a VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  EXECUTE IMMEDIATE 'DROP TABLE t1';
  DECLARE
    a t1.a%TYPE:='xxx';
  BEGIN
    EXECUTE IMMEDIATE 'INSERT INTO t2 VALUES (:1)' USING a;
  END;
END;
/
CALL p1();
SELECT * FROM t2;
{code}
{noformat}
A
----------
xxx
{noformat}

Notice, the variable was declared after {{DROP TABLE}}.

h2. MariaDB behavior
To make MariaDB reproduce Oracle's behavior as close as possible, we'll assing data type to %TYPE variables at sp_rcontext::create() time.

Like Oracle, we'll not require the referenced tables to exists at the routine {{CREATE}} time. Unlike Oracle, we won't check the referenced tables and won't generate warnings.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,32,,0,1,4,5,0,11,0,,0,850,1,0,0,2016-08-17 14:52:12,sql_mode=ORACLE:  %TYPE in variable declarations,"When running in {{sql_mode=ORACLE}}, MariaDB should support {{%TYPE}} in variable declarations and function {{RETURN}} clauses:

{code:sql}
  tmp t1.a%TYPE;
{code}
The above statement declares a variable that has the same data type with the column {{a}} in the table {{t1}}.

Example:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);

DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN t1.a%TYPE
AS
  tmp t1.a%TYPE;
BEGIN
  SELECT MAX(a) INTO tmp FROM t1;
  RETURN tmp;
END;
/
SELECT f1() FROM DUAL;
{code}
{noformat}
SQL> 
      F1()
----------
	20
{noformat}
",,0,11,0,442,4.34,"sql_mode=ORACLE:  %TYPE in variable declarations $end$ When running in {{sql_mode=ORACLE}}, MariaDB should support {{%TYPE}} in variable declarations and function {{RETURN}} clauses:

{code:sql}
  tmp t1.a%TYPE;
{code}
The above statement declares a variable that has the same data type with the column {{a}} in the table {{t1}}.

Example:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);

DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN t1.a%TYPE
AS
  tmp t1.a%TYPE;
BEGIN
  SELECT MAX(a) INTO tmp FROM t1;
  RETURN tmp;
END;
/
SELECT f1() FROM DUAL;
{code}
{noformat}
SQL> 
      F1()
----------
	20
{noformat}
 $acceptance criteria:$",11,1,1,1,1,1,1,0.0,10,3,0.3,3,0.3,3,0.3,2,0.2,2,0.2
542,MDEV-10578,Technical task,MDEV,2016-08-17 15:11:41,,0,"sql_mode=ORACLE: SP control functions SQLCODE, SQLERRM","h2. SQLCODE
The function {{SQLCODE}} returns the number code of the most recent exception.

Outside of SP the function {{SQLCODE}} is not available and is treated as a normal identifier.

Inside an SP the function {{SQLCODE}} can be hidden by a user-defined identifier visible in the current context:

This example returns the error code normally:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS BEGIN RETURN SQLCODE; END;
/
SELECT f1() FROM DUAL;
{code}

This example returns the value of the column {{t1.sqlcode}}:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (SQLCODE INT);
INSERT INTO t1 VALUES (10);   
COMMIT;
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN NUMBER AS
  v INT;
BEGIN   
  SELECT MIN(SQLCODE) INTO v FROM t1;
  RETURN v;
END;
/   
SELECT f1 FROM dual;
{code}

This example returns the value of the variable {{SQLCODE}}:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS
  SQLCODE INT:=123;
BEGIN
  RETURN SQLCODE;
END;
/   
SELECT f1 FROM dual;
{code}

h2. SQLERRM
The function SQLERRM returns the error message associated with its error-number argument. If the argument is omitted, it returns the error message associated with the current value of SQLCODE

The function {{SQLERRM}} is not available outside of an SP and {{SQLERRM}} is treated as a normal identifier.

h2. Warning-alike errors
{{NO_DATA_FOUND}} is more like MariaDB warning. It should be correctly handled by {{SQLCODE}} and {{SQLERRM}}:
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
CREATE FUNCTION f1 RETURN VARCHAR
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
  RETURN 'No exception ' || TO_CHAR(SQLCODE) || ' ' || SQLERRM;
EXCEPTION
  WHEN OTHERS THEN
  RETURN 'Exception ' || TO_CHAR(SQLCODE) || ' ' || SQLERRM;
END;
/
SELECT f1 FROM DUAL;
{code}
{noformat}
F1
--------------------------------------------------------------------------------
Exception 100 ORA-01403: no data found
{noformat}

h2. SQLCODE and SQLERRM should be cleared on functions return
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
DROP FUNCTION f2;
CREATE TABLE t1 (a INT);
CREATE FUNCTION f1 RETURN VARCHAR
AS
  a INT:=10;
BEGIN
  SELECT a INTO a FROM t1;
  RETURN 'Value='|| TO_CHAR(a);
EXCEPTION
  WHEN OTHERS THEN RETURN 'Exception|' || SQLCODE || ' ' || SQLERRM;
END;
/
CREATE FUNCTION f2 RETURN VARCHAR
AS
  a VARCHAR(128);
BEGIN
  RETURN f1 || '|' || SQLCODE || ' ' || SQLERRM;
END;
/
SELECT f2 FROM DUAL;
{code}
{noformat}
F2
--------------------------------------------------------------------------------
Exception|100 ORA-01403: no data found|0 ORA-0000: normal, successful completion
{noformat}
Notice, the calls for {{SQLCODE}} and {{SQLERRM}} in {{f2}} generated {{0 ORA-0000: normal, successful completion}}

The behavior is the same on a procedure return:
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
DROP FUNCTION f2;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1(res OUT VARCHAR)
AS
  a INT:=10;
BEGIN
  SELECT a INTO a FROM t1;
  res:='Value='|| TO_CHAR(a);
EXCEPTION
  WHEN OTHERS THEN res:='Exception|' || SQLCODE || ' ' || SQLERRM;
END;
/
CREATE FUNCTION f2 RETURN VARCHAR
AS
  res VARCHAR(128);
BEGIN
  p1(res);
  RETURN res || '|' || SQLCODE || ' ' || SQLERRM;
END;
/
SELECT f2() FROM DUAL;
{code}
{noformat}
F2()
--------------------------------------------------------------------------------
Exception|100 ORA-01403: no data found|0 ORA-0000: normal, successful completion
{noformat}


h2. Limitations
- This task will add {{SQLCODE}} and {{SQLERRM}} with no parentheses
- This task will allow {{SQLCODE}} and {{SQLERRM}} to be shadowed by SP variables only, but not table columns.

h2. Further related tasks
- Calling {{SQLCODE}} and {{SQLERRM}} with empty parentheses will be done in MDEV-10576
- Calling {{SQLERRM}} with an error number argument will be done separately in MDEV-11022
- Shadowing  {{SQLCODE}} and {{SQLERRM}} functions by table column names will be done in MDEV-10576
",,"sql_mode=ORACLE: SP control functions SQLCODE, SQLERRM $end$ h2. SQLCODE
The function {{SQLCODE}} returns the number code of the most recent exception.

Outside of SP the function {{SQLCODE}} is not available and is treated as a normal identifier.

Inside an SP the function {{SQLCODE}} can be hidden by a user-defined identifier visible in the current context:

This example returns the error code normally:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS BEGIN RETURN SQLCODE; END;
/
SELECT f1() FROM DUAL;
{code}

This example returns the value of the column {{t1.sqlcode}}:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (SQLCODE INT);
INSERT INTO t1 VALUES (10);   
COMMIT;
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN NUMBER AS
  v INT;
BEGIN   
  SELECT MIN(SQLCODE) INTO v FROM t1;
  RETURN v;
END;
/   
SELECT f1 FROM dual;
{code}

This example returns the value of the variable {{SQLCODE}}:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS
  SQLCODE INT:=123;
BEGIN
  RETURN SQLCODE;
END;
/   
SELECT f1 FROM dual;
{code}

h2. SQLERRM
The function SQLERRM returns the error message associated with its error-number argument. If the argument is omitted, it returns the error message associated with the current value of SQLCODE

The function {{SQLERRM}} is not available outside of an SP and {{SQLERRM}} is treated as a normal identifier.

h2. Warning-alike errors
{{NO_DATA_FOUND}} is more like MariaDB warning. It should be correctly handled by {{SQLCODE}} and {{SQLERRM}}:
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
CREATE FUNCTION f1 RETURN VARCHAR
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
  RETURN 'No exception ' || TO_CHAR(SQLCODE) || ' ' || SQLERRM;
EXCEPTION
  WHEN OTHERS THEN
  RETURN 'Exception ' || TO_CHAR(SQLCODE) || ' ' || SQLERRM;
END;
/
SELECT f1 FROM DUAL;
{code}
{noformat}
F1
--------------------------------------------------------------------------------
Exception 100 ORA-01403: no data found
{noformat}

h2. SQLCODE and SQLERRM should be cleared on functions return
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
DROP FUNCTION f2;
CREATE TABLE t1 (a INT);
CREATE FUNCTION f1 RETURN VARCHAR
AS
  a INT:=10;
BEGIN
  SELECT a INTO a FROM t1;
  RETURN 'Value='|| TO_CHAR(a);
EXCEPTION
  WHEN OTHERS THEN RETURN 'Exception|' || SQLCODE || ' ' || SQLERRM;
END;
/
CREATE FUNCTION f2 RETURN VARCHAR
AS
  a VARCHAR(128);
BEGIN
  RETURN f1 || '|' || SQLCODE || ' ' || SQLERRM;
END;
/
SELECT f2 FROM DUAL;
{code}
{noformat}
F2
--------------------------------------------------------------------------------
Exception|100 ORA-01403: no data found|0 ORA-0000: normal, successful completion
{noformat}
Notice, the calls for {{SQLCODE}} and {{SQLERRM}} in {{f2}} generated {{0 ORA-0000: normal, successful completion}}

The behavior is the same on a procedure return:
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
DROP FUNCTION f2;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1(res OUT VARCHAR)
AS
  a INT:=10;
BEGIN
  SELECT a INTO a FROM t1;
  res:='Value='|| TO_CHAR(a);
EXCEPTION
  WHEN OTHERS THEN res:='Exception|' || SQLCODE || ' ' || SQLERRM;
END;
/
CREATE FUNCTION f2 RETURN VARCHAR
AS
  res VARCHAR(128);
BEGIN
  p1(res);
  RETURN res || '|' || SQLCODE || ' ' || SQLERRM;
END;
/
SELECT f2() FROM DUAL;
{code}
{noformat}
F2()
--------------------------------------------------------------------------------
Exception|100 ORA-01403: no data found|0 ORA-0000: normal, successful completion
{noformat}


h2. Limitations
- This task will add {{SQLCODE}} and {{SQLERRM}} with no parentheses
- This task will allow {{SQLCODE}} and {{SQLERRM}} to be shadowed by SP variables only, but not table columns.

h2. Further related tasks
- Calling {{SQLCODE}} and {{SQLERRM}} with empty parentheses will be done in MDEV-10576
- Calling {{SQLERRM}} with an error number argument will be done separately in MDEV-11022
- Shadowing  {{SQLCODE}} and {{SQLERRM}} functions by table column names will be done in MDEV-10576
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,29,,0,0,2,5,0,12,0,,0,850,0,0,0,2016-08-17 15:11:41,"sql_mode=ORACLE: SP control functions SQLCODE, SQLERRM","h2. SQLCODE
The function {{SQLCODE}} returns the number code of the most recent exception.

Outside of SP the function {{SQLCODE}} is not available and is treated as a normal identifier.

Inside an SP the function {{SQLCODE}} can be hidden by a user-define identifier visible in the current context:

This example returns the error code normally:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS BEGIN RETURN SQLCODE; END;
/
SELECT f1() FROM DUAL;
{code}

This example returns the value of the column {{t1.sqlcode}}:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (SQLCODE INT);
INSERT INTO t1 VALUES (10);   
COMMIT;
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN NUMBER AS
  v INT;
BEGIN   
  SELECT MIN(SQLCODE) INTO v FROM t1;
  RETURN v;
END;
/   
SELECT f1 FROM dual;
{code}

This example returns the value of the variable {{SQLCODE}}:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS
  SQLCODE INT:=123;
BEGIN
  RETURN SQLCODE;
END;
/   
SELECT f1 FROM dual;
{code}

h2. SQLERRM
The function SQLERRM returns the error message associated with its error-number argument. If the argument is omitted, it returns the error message associated with the current value of SQLCODE

The function {{SQLERRM}} is not available outside of an SP and {{SQLERRM}} is treated as a normal identifier.
",,0,12,0,376,1.74419,"sql_mode=ORACLE: SP control functions SQLCODE, SQLERRM $end$ h2. SQLCODE
The function {{SQLCODE}} returns the number code of the most recent exception.

Outside of SP the function {{SQLCODE}} is not available and is treated as a normal identifier.

Inside an SP the function {{SQLCODE}} can be hidden by a user-define identifier visible in the current context:

This example returns the error code normally:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS BEGIN RETURN SQLCODE; END;
/
SELECT f1() FROM DUAL;
{code}

This example returns the value of the column {{t1.sqlcode}}:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (SQLCODE INT);
INSERT INTO t1 VALUES (10);   
COMMIT;
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN NUMBER AS
  v INT;
BEGIN   
  SELECT MIN(SQLCODE) INTO v FROM t1;
  RETURN v;
END;
/   
SELECT f1 FROM dual;
{code}

This example returns the value of the variable {{SQLCODE}}:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR AS
  SQLCODE INT:=123;
BEGIN
  RETURN SQLCODE;
END;
/   
SELECT f1 FROM dual;
{code}

h2. SQLERRM
The function SQLERRM returns the error message associated with its error-number argument. If the argument is omitted, it returns the error message associated with the current value of SQLCODE

The function {{SQLERRM}} is not available outside of an SP and {{SQLERRM}} is treated as a normal identifier.
 $acceptance criteria:$",12,1,1,1,1,1,1,0.0,11,4,0.363636,4,0.363636,4,0.363636,3,0.272727,3,0.272727
543,MDEV-10579,Technical task,MDEV,2016-08-17 16:45:34,,0,sql_mode=ORACLE: Triggers: Understand  :NEW.c1 and :OLD.c1 instead of NEW.c1 and OLD.c1,"When running in {{sql_mode=ORACLE}}, MariaDB should understand {{:NEW}} and {{:OLD}} as references to new and old row values in addition or instead of {{NEW}} and {{OLD}} without colon.
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
DROP TRIGGER tr1;
CREATE TRIGGER tr1 BEFORE INSERT ON t1
FOR EACH ROW
DECLARE
  cnt INT := 0;
BEGIN
  IF :NEW.a IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.b IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.c IS NULL THEN :NEW.c:=cnt; END IF;
END;
/
{code}
",,"sql_mode=ORACLE: Triggers: Understand  :NEW.c1 and :OLD.c1 instead of NEW.c1 and OLD.c1 $end$ When running in {{sql_mode=ORACLE}}, MariaDB should understand {{:NEW}} and {{:OLD}} as references to new and old row values in addition or instead of {{NEW}} and {{OLD}} without colon.
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
DROP TRIGGER tr1;
CREATE TRIGGER tr1 BEFORE INSERT ON t1
FOR EACH ROW
DECLARE
  cnt INT := 0;
BEGIN
  IF :NEW.a IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.b IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.c IS NULL THEN :NEW.c:=cnt; END IF;
END;
/
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,15,,0,0,0,5,0,1,0,,0,850,0,0,0,2016-08-17 16:45:34,sql_mode=ORACLE: Triggers: Understand  :NEW.c1 and :OLD.c1 instead of NEW.c1 and OLD.c1,"When running in {{sql_mode=ORACLE}}, MariaDB should understand {{:NEW}} and {{:OLD} as references to new and old row values in addition or instead of {{NEW}} and {{OLD}} without colon.
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
DROP TRIGGER tr1;
CREATE TRIGGER tr1 BEFORE INSERT ON t1
FOR EACH ROW
DECLARE
  cnt INT := 0;
BEGIN
  IF :NEW.a IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.b IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.c IS NULL THEN :NEW.c:=cnt; END IF;
END;
/
{code}
",,0,1,0,2,0.00990099,"sql_mode=ORACLE: Triggers: Understand  :NEW.c1 and :OLD.c1 instead of NEW.c1 and OLD.c1 $end$ When running in {{sql_mode=ORACLE}}, MariaDB should understand {{:NEW}} and {{:OLD} as references to new and old row values in addition or instead of {{NEW}} and {{OLD}} without colon.
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
DROP TRIGGER tr1;
CREATE TRIGGER tr1 BEFORE INSERT ON t1
FOR EACH ROW
DECLARE
  cnt INT := 0;
BEGIN
  IF :NEW.a IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.b IS NULL THEN cnt:=cnt+1; END IF;
  IF :NEW.c IS NULL THEN :NEW.c:=cnt; END IF;
END;
/
{code}
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,12,5,0.416667,5,0.416667,5,0.416667,4,0.333333,4,0.333333
544,MDEV-10580,Technical task,MDEV,2016-08-17 17:40:40,,0,sql_mode=ORACLE: FOR loop statement,"Add support for the Oracle-style {{FOR}} loop when running in {{sql_mode=ORACLE}}
{code:sql}
FOR index IN [ REVERSE ] lower_bound .. upper_bound
LOOP statements
END LOOP [ label ] ;
{code}

There must be at least one statement.

The two dots must have no spaces between, otherwise an error is returned:
{code:sql}
BEGIN
  FOR i IN 1 .  . 10 LOOP
    NULL;
  END LOOP;
END;
/
{code}
{noformat}
ORA-06550: line 2, column 14:
PLS-00103: Encountered the symbol ""."" when expecting one of the following:
* & - + / at mod remainder rem .. <an exponent (**)> ||
multiset
{noformat}
",,"sql_mode=ORACLE: FOR loop statement $end$ Add support for the Oracle-style {{FOR}} loop when running in {{sql_mode=ORACLE}}
{code:sql}
FOR index IN [ REVERSE ] lower_bound .. upper_bound
LOOP statements
END LOOP [ label ] ;
{code}

There must be at least one statement.

The two dots must have no spaces between, otherwise an error is returned:
{code:sql}
BEGIN
  FOR i IN 1 .  . 10 LOOP
    NULL;
  END LOOP;
END;
/
{code}
{noformat}
ORA-06550: line 2, column 14:
PLS-00103: Encountered the symbol ""."" when expecting one of the following:
* & - + / at mod remainder rem .. <an exponent (**)> ||
multiset
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,0,0,1,5,0,1,0,,0,850,0,0,0,2016-08-17 17:40:40,sql_mode=ORACLE: FOR loop statement,"Add support for the Oracle-style {{FOR}} loop when running in {{sql_mode=ORACLE}}
{code:sql}
FOR index IN [ REVERSE ] lower_bound .. upper_bound
LOOP statements
END LOOP [ label ] ;
{code}
",,0,1,0,69,1.86486,"sql_mode=ORACLE: FOR loop statement $end$ Add support for the Oracle-style {{FOR}} loop when running in {{sql_mode=ORACLE}}
{code:sql}
FOR index IN [ REVERSE ] lower_bound .. upper_bound
LOOP statements
END LOOP [ label ] ;
{code}
 $acceptance criteria:$",1,1,1,1,1,1,1,0.0,13,6,0.461538,5,0.384615,5,0.384615,4,0.307692,4,0.307692
545,MDEV-10581,Technical task,MDEV,2016-08-17 17:56:25,,0,sql_mode=ORACLE: Explicit cursor FOR LOOP,"Add support for explicit cursor FOR loops when running in {{sql_mode=ORACLE}}:
{code:sql}
FOR rec IN cur
LOOP
  -- statements
END LOOP;
{code}

The benefits of this syntax is:
- No declaration of the {{FOR}} variable {{rec}} is needed.  The cursor {{FOR}} statement implicitly declares a record variable (i.e. a  {{ROW}} type variable}}  that has the same structure with the data returned by the cursor.
- No needs to do {{OPEN}} or {{CLOSE}} for the cursor. It's automatically opened before the loop, and is closed after the loop.
- No needs to do {{CLOSE}} from inside the {{FOR}} loop if one uses a {{GOTO}} statement from inside the loop. The cursor is closed automatically.
- No needs to do an explicit {{FETCH}}. The next row is automatically fetched in the beginning of every iteration.
- No needs to do an explicit {{EXIT}} to leave the loop when {{FETCH}} returned no data.

Example:
{code:sql}
DECLARE
  CURSOR c1 IS
    SELECT last_name, job_id FROM employees
    WHERE job_id LIKE '%CLERK%' AND manager_id > 120
    ORDER BY last_name;
BEGIN
  FOR item IN c1
  LOOP
    DBMS_OUTPUT.PUT_LINE
      ('Name = ' || item.last_name || ', Job = ' || item.job_id);
  END LOOP;
END;
/
{code}


The fields of the implicitly declared record variable must be updatable:
{code:sql}
DECLARE
  CURSOR cur IS
    SELECT 'Black' AS last_name, 10 AS job_id FROM DUAL UNION
    SELECT 'White' AS last_name, 20 AS job_id FROM DUAL;
BEGIN
  FOR rec IN cur
  LOOP
    rec.job_id:= rec.job_id + 100;
    DBMS_OUTPUT.PUT_LINE ('Name=' || rec.last_name || ', Job=' || rec.job_id);
  END LOOP;
END;
/
{code}


Internally, the explicit cursor {{FOR LOOP}} will be transparently translated to:
- {{OPEN}}
- Declaration of a {{%ROWTYPE}} variable
- Simple {{LOOP}} with {{EXIT}}
- {{CLOSE}}

So the block in the previous example will be effectively equal to:
{code:sql}
DECLARE
  CURSOR cur IS
    SELECT 'Black' AS last_name, 10 AS job_id FROM DUAL UNION
    SELECT 'White' AS last_name, 20 AS job_id FROM DUAL;
BEGIN
  OPEN cur;
  DECLARE
    rec cur%ROWTYPE;
  BEGIN
    LOOP
      FETCH cur INTO rec;
      EXIT WHEN cur%NOTFOUND;
      rec.job_id:= rec.job_id + 100;
      DBMS_OUTPUT.PUT_LINE ('Name=' || rec.last_name || ', Job=' || rec.job_id);
    END LOOP;
    CLOSE cur;
  END;
END;
/
{code}
",,"sql_mode=ORACLE: Explicit cursor FOR LOOP $end$ Add support for explicit cursor FOR loops when running in {{sql_mode=ORACLE}}:
{code:sql}
FOR rec IN cur
LOOP
  -- statements
END LOOP;
{code}

The benefits of this syntax is:
- No declaration of the {{FOR}} variable {{rec}} is needed.  The cursor {{FOR}} statement implicitly declares a record variable (i.e. a  {{ROW}} type variable}}  that has the same structure with the data returned by the cursor.
- No needs to do {{OPEN}} or {{CLOSE}} for the cursor. It's automatically opened before the loop, and is closed after the loop.
- No needs to do {{CLOSE}} from inside the {{FOR}} loop if one uses a {{GOTO}} statement from inside the loop. The cursor is closed automatically.
- No needs to do an explicit {{FETCH}}. The next row is automatically fetched in the beginning of every iteration.
- No needs to do an explicit {{EXIT}} to leave the loop when {{FETCH}} returned no data.

Example:
{code:sql}
DECLARE
  CURSOR c1 IS
    SELECT last_name, job_id FROM employees
    WHERE job_id LIKE '%CLERK%' AND manager_id > 120
    ORDER BY last_name;
BEGIN
  FOR item IN c1
  LOOP
    DBMS_OUTPUT.PUT_LINE
      ('Name = ' || item.last_name || ', Job = ' || item.job_id);
  END LOOP;
END;
/
{code}


The fields of the implicitly declared record variable must be updatable:
{code:sql}
DECLARE
  CURSOR cur IS
    SELECT 'Black' AS last_name, 10 AS job_id FROM DUAL UNION
    SELECT 'White' AS last_name, 20 AS job_id FROM DUAL;
BEGIN
  FOR rec IN cur
  LOOP
    rec.job_id:= rec.job_id + 100;
    DBMS_OUTPUT.PUT_LINE ('Name=' || rec.last_name || ', Job=' || rec.job_id);
  END LOOP;
END;
/
{code}


Internally, the explicit cursor {{FOR LOOP}} will be transparently translated to:
- {{OPEN}}
- Declaration of a {{%ROWTYPE}} variable
- Simple {{LOOP}} with {{EXIT}}
- {{CLOSE}}

So the block in the previous example will be effectively equal to:
{code:sql}
DECLARE
  CURSOR cur IS
    SELECT 'Black' AS last_name, 10 AS job_id FROM DUAL UNION
    SELECT 'White' AS last_name, 20 AS job_id FROM DUAL;
BEGIN
  OPEN cur;
  DECLARE
    rec cur%ROWTYPE;
  BEGIN
    LOOP
      FETCH cur INTO rec;
      EXIT WHEN cur%NOTFOUND;
      rec.job_id:= rec.job_id + 100;
      DBMS_OUTPUT.PUT_LINE ('Name=' || rec.last_name || ', Job=' || rec.job_id);
    END LOOP;
    CLOSE cur;
  END;
END;
/
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,38,,0,4,6,5,0,19,0,,0,850,4,0,0,2016-08-17 17:56:25,sql_mode=ORACLE: Explicit cursor FOR LOOP,"Add support for explicit cursor FOR loops when running in {{sql_mode=ORACLE}}.

Example:
{code:sql}
DECLARE
  CURSOR c1 IS
    SELECT last_name, job_id FROM employees
    WHERE job_id LIKE '%CLERK%' AND manager_id > 120
    ORDER BY last_name;
BEGIN
  FOR item IN c1
  LOOP
    DBMS_OUTPUT.PUT_LINE
      ('Name = ' || item.last_name || ', Job = ' || item.job_id);
  END LOOP;
END;
/
{code}
",,0,19,0,298,4.56923,"sql_mode=ORACLE: Explicit cursor FOR LOOP $end$ Add support for explicit cursor FOR loops when running in {{sql_mode=ORACLE}}.

Example:
{code:sql}
DECLARE
  CURSOR c1 IS
    SELECT last_name, job_id FROM employees
    WHERE job_id LIKE '%CLERK%' AND manager_id > 120
    ORDER BY last_name;
BEGIN
  FOR item IN c1
  LOOP
    DBMS_OUTPUT.PUT_LINE
      ('Name = ' || item.last_name || ', Job = ' || item.job_id);
  END LOOP;
END;
/
{code}
 $acceptance criteria:$",19,1,1,1,1,1,1,0.0,14,7,0.5,6,0.428571,6,0.428571,5,0.357143,5,0.357143
546,MDEV-10582,Technical task,MDEV,2016-08-17 17:59:04,,0,"sql_mode=ORACLE: Explicit cursor attributes %ISOPEN, %ROWCOUNT, %FOUND, %NOTFOUND","Understand explicit cursor attributes when running in {{sql_mode=ORACLE}}:
- {{cursor_name%ISOPEN}}
  Return {{TRUE}} if {{cursor_name}} was already open with {{OPEN}}, or {{FALSE}} otherwise

- {{cursor_name%ROWCOUNT}}
  Return {{0}} before the first {{FETCH}}, afterwards return the number of rows that were already fetched from {{cursor_name}} using {{FETCH}}.

- {{cursor_name%FOUND}}
  Return {{NULL}} before the first {{FETCH}}. Return {{TRUE}} if the last {{FETCH}} returned a row, or {{FALSE}} otherwise.

- {{cursor_name%NOTFOUND}}
  Return {{NULL}} before the first {{FETCH}}. Return {{TRUE}} if the last {{FETCH}} returned no rows, or {{FALSE}} if the last {{FETCH}} returned some rows.

Due to a grammar conflict, this task will remove the modulo operator '%'. One should use {{MOD}} in Oracle.

This task will also implement the predefined exception {{INVALID_CURSOR}}. If a cursor is not open, referencing it with %FOUND, %NOTFOUND, or %ROWCOUNT raises {{INVALID_CURSOR}}.

If a cursor is closed and then opened again, {{%FOUND}} and {{%NOTFOUND}} are reset to {{NULL}} and {{%ROWCOUNT}} is reset to {{0}}.

Note, we'll also change behavior of the {{FETCH}} statement to raise no conditions (exceptions) if no rows were returned. Instead of catching conditions, in Oracle one should test %FOUND or %NOTFOUND attributes to know if {{FETCH}} returned a row.

Example:
{code:sql}
  BEGIN
    FETCH cur INTO v1,v2;
    EXIT WHEN cur%NOTFOUND;
    ...
  END; 
{code}
",,"sql_mode=ORACLE: Explicit cursor attributes %ISOPEN, %ROWCOUNT, %FOUND, %NOTFOUND $end$ Understand explicit cursor attributes when running in {{sql_mode=ORACLE}}:
- {{cursor_name%ISOPEN}}
  Return {{TRUE}} if {{cursor_name}} was already open with {{OPEN}}, or {{FALSE}} otherwise

- {{cursor_name%ROWCOUNT}}
  Return {{0}} before the first {{FETCH}}, afterwards return the number of rows that were already fetched from {{cursor_name}} using {{FETCH}}.

- {{cursor_name%FOUND}}
  Return {{NULL}} before the first {{FETCH}}. Return {{TRUE}} if the last {{FETCH}} returned a row, or {{FALSE}} otherwise.

- {{cursor_name%NOTFOUND}}
  Return {{NULL}} before the first {{FETCH}}. Return {{TRUE}} if the last {{FETCH}} returned no rows, or {{FALSE}} if the last {{FETCH}} returned some rows.

Due to a grammar conflict, this task will remove the modulo operator '%'. One should use {{MOD}} in Oracle.

This task will also implement the predefined exception {{INVALID_CURSOR}}. If a cursor is not open, referencing it with %FOUND, %NOTFOUND, or %ROWCOUNT raises {{INVALID_CURSOR}}.

If a cursor is closed and then opened again, {{%FOUND}} and {{%NOTFOUND}} are reset to {{NULL}} and {{%ROWCOUNT}} is reset to {{0}}.

Note, we'll also change behavior of the {{FETCH}} statement to raise no conditions (exceptions) if no rows were returned. Instead of catching conditions, in Oracle one should test %FOUND or %NOTFOUND attributes to know if {{FETCH}} returned a row.

Example:
{code:sql}
  BEGIN
    FETCH cur INTO v1,v2;
    EXIT WHEN cur%NOTFOUND;
    ...
  END; 
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,35,,1,0,1,5,0,16,0,,0,850,0,0,0,2016-08-17 17:59:04,sql_mode=ORACLE: cursor_name%NOTFOUND,"Understand cursor_name%NOTFOUND when running in {{sql_mode=ORACLE}}

Example:
{code:sql}
  BEGIN
    FETCH cur INTO v1,v2;
    EXIT WHEN cur%NOTFOUND;
    ...
  END; 
{code}
",,3,13,0,200,8.20833,"sql_mode=ORACLE: cursor_name%NOTFOUND $end$ Understand cursor_name%NOTFOUND when running in {{sql_mode=ORACLE}}

Example:
{code:sql}
  BEGIN
    FETCH cur INTO v1,v2;
    EXIT WHEN cur%NOTFOUND;
    ...
  END; 
{code}
 $acceptance criteria:$",16,1,1,1,1,1,1,0.0,15,8,0.533333,7,0.466667,7,0.466667,6,0.4,6,0.4
547,MDEV-10583,Technical task,MDEV,2016-08-17 18:01:42,,0,sql_mode=ORACLE: SQL%ROWCOUNT,"Understand {{SQL%ROWCOUNT}} when running in {{sql_mode=ORACLE}}

Example:
{code:sql}
UPDATE t1 SET a=10;
cnt:= cnt + SQL%ROWCOUNT;
{code}

Oracle's {{SQL%ROWCOUNT}} looks very similar to MariaDB function {{ROW_COUNT()}}, with the following differences:
- When a query like this:
{code:sql}
SELECT a INTO spvar FROM t1;
{code}
finds more than one rows, {{SQL%ROWCOUNT}} returns {{1}} rather than {{-1}}. We'll implement {{SQL%ROWCOUNT}} in MariaDB in the same way.
- When no {{DELETE}}, {{INSERT}}, {{UPDATE}} or {{SELECT .. INTO..}} queries happened during this session, {{SQL%ROWCOUNT}} returns {{NULL}}. Note, as this is a very minor issue, for simplicity of the implementation we'll return {{1}} instead.

h2. Example - no rows involved
{code:sql}
SET SERVEROUT ON;
BEGIN
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS
{noformat}

h2. Example - {{UPDATE}}
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
BEGIN
  UPDATE t1 SET a=30;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 0
{noformat}

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
BEGIN
  UPDATE t1 SET a=30;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 2
{noformat}

h2. Example - {{DELETE}}

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
BEGIN
  DELETE FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
CALL p1();
{code}
{noformat}
SQL%ROWCOUNT IS 0
{noformat}

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
BEGIN
  DELETE FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 2
{noformat}

h2. Example - {{SELECT..INTO var FROM..}} - one row found
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
DECLARE
  va INT;
BEGIN
  SELECT a INTO va FROM t1 WHERE ROWNUM<2;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 1
{noformat}

h2. Example - {{SELECT..INTO var FROM..}} - no rows found
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  va INT;
BEGIN
  SELECT a INTO va FROM t1;
  DBMS_OUTPUT.put_line('Still here');
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '|| COALESCE(SQL%ROWCOUNT,''));
END;
/
CALL p1();
{code}
Note, no output!

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  va INT;
BEGIN
  SELECT a INTO va FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
EXCEPTION
  WHEN NO_DATA_FOUND THEN DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT||' (EXCEPTION)');
END;
/
CALL p1();
{code}
{noformat}
SQL%ROWCOUNT IS 0 (EXCEPTION)
{noformat}


h2. Example - {{SELECT..INTO var FROM..}} - multiple rows found
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
CREATE PROCEDURE p1
AS
  va INT:=1;
BEGIN
  SELECT a INTO va FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
EXCEPTION
  WHEN TOO_MANY_ROWS THEN DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT || ' (EXCEPTION) va='||va);
END;
/
CALL p1();
{code}
{noformat}
SQL%ROWCOUNT IS 1 (EXCEPTION) va=1
{noformat}



h2. Example - {{INSERT INTO t2 SELECT .. FROM t1}}
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a INT);
CREATE TABLE t2 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
BEGIN
  INSERT INTO t2 SELECT * FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 2
{noformat}
",,"sql_mode=ORACLE: SQL%ROWCOUNT $end$ Understand {{SQL%ROWCOUNT}} when running in {{sql_mode=ORACLE}}

Example:
{code:sql}
UPDATE t1 SET a=10;
cnt:= cnt + SQL%ROWCOUNT;
{code}

Oracle's {{SQL%ROWCOUNT}} looks very similar to MariaDB function {{ROW_COUNT()}}, with the following differences:
- When a query like this:
{code:sql}
SELECT a INTO spvar FROM t1;
{code}
finds more than one rows, {{SQL%ROWCOUNT}} returns {{1}} rather than {{-1}}. We'll implement {{SQL%ROWCOUNT}} in MariaDB in the same way.
- When no {{DELETE}}, {{INSERT}}, {{UPDATE}} or {{SELECT .. INTO..}} queries happened during this session, {{SQL%ROWCOUNT}} returns {{NULL}}. Note, as this is a very minor issue, for simplicity of the implementation we'll return {{1}} instead.

h2. Example - no rows involved
{code:sql}
SET SERVEROUT ON;
BEGIN
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS
{noformat}

h2. Example - {{UPDATE}}
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
BEGIN
  UPDATE t1 SET a=30;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 0
{noformat}

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
BEGIN
  UPDATE t1 SET a=30;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 2
{noformat}

h2. Example - {{DELETE}}

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
BEGIN
  DELETE FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
CALL p1();
{code}
{noformat}
SQL%ROWCOUNT IS 0
{noformat}

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
BEGIN
  DELETE FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 2
{noformat}

h2. Example - {{SELECT..INTO var FROM..}} - one row found
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
DECLARE
  va INT;
BEGIN
  SELECT a INTO va FROM t1 WHERE ROWNUM<2;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 1
{noformat}

h2. Example - {{SELECT..INTO var FROM..}} - no rows found
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  va INT;
BEGIN
  SELECT a INTO va FROM t1;
  DBMS_OUTPUT.put_line('Still here');
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '|| COALESCE(SQL%ROWCOUNT,''));
END;
/
CALL p1();
{code}
Note, no output!

{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  va INT;
BEGIN
  SELECT a INTO va FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
EXCEPTION
  WHEN NO_DATA_FOUND THEN DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT||' (EXCEPTION)');
END;
/
CALL p1();
{code}
{noformat}
SQL%ROWCOUNT IS 0 (EXCEPTION)
{noformat}


h2. Example - {{SELECT..INTO var FROM..}} - multiple rows found
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
CREATE PROCEDURE p1
AS
  va INT:=1;
BEGIN
  SELECT a INTO va FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
EXCEPTION
  WHEN TOO_MANY_ROWS THEN DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT || ' (EXCEPTION) va='||va);
END;
/
CALL p1();
{code}
{noformat}
SQL%ROWCOUNT IS 1 (EXCEPTION) va=1
{noformat}



h2. Example - {{INSERT INTO t2 SELECT .. FROM t1}}
{code:sql}
SET SERVEROUT ON;
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a INT);
CREATE TABLE t2 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
BEGIN
  INSERT INTO t2 SELECT * FROM t1;
  DBMS_OUTPUT.put_line('SQL%ROWCOUNT IS '||SQL%ROWCOUNT);
END;
/
{code}
{noformat}
SQL%ROWCOUNT IS 2
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,31,,0,1,0,5,0,15,0,,0,850,1,0,0,2016-08-17 18:01:42,sql_mode=ORACLE: SQL%ROWCOUNT,"Understand {{SQL%ROWCOUNT}} when running in {{sql_mode=ORACLE}}

Example:
{code:sql}
UPDATE t1 SET a=10;
cnt:= cnt + SQL%ROWCOUNT;
{code}

Looks similar to MariaDB's {{ROW_COUNT()}}.

",,0,15,0,541,19.9259,"sql_mode=ORACLE: SQL%ROWCOUNT $end$ Understand {{SQL%ROWCOUNT}} when running in {{sql_mode=ORACLE}}

Example:
{code:sql}
UPDATE t1 SET a=10;
cnt:= cnt + SQL%ROWCOUNT;
{code}

Looks similar to MariaDB's {{ROW_COUNT()}}.

 $acceptance criteria:$",15,1,1,1,1,1,1,0.0,16,9,0.5625,8,0.5,8,0.5,7,0.4375,7,0.4375
548,MDEV-10585,Technical task,MDEV,2016-08-17 18:11:37,,0,EXECUTE IMMEDIATE statement,"Add support for Oracle-style {{EXECUTE IMMEDIATE}} statement.

This is used to execute a sql-statement stored in a string or variable, with possible arguments.

Examples:
{code:sql}
EXECUTE IMMEDIATE 'SELECT 1' 
{code}

This is a shorthand for:
{code:sql}
prepare stmt from ""select 1"";
execute stmt;
deallocate prepare stmt;
{code}

",,"EXECUTE IMMEDIATE statement $end$ Add support for Oracle-style {{EXECUTE IMMEDIATE}} statement.

This is used to execute a sql-statement stored in a string or variable, with possible arguments.

Examples:
{code:sql}
EXECUTE IMMEDIATE 'SELECT 1' 
{code}

This is a shorthand for:
{code:sql}
prepare stmt from ""select 1"";
execute stmt;
deallocate prepare stmt;
{code}

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,28,,0,3,4,5,0,6,0,,0,850,3,0,0,2016-08-17 18:11:37,EXECUTE IMMEDIATE statement,"Add support for Oracle-style {{EXECUTE IMMEDIATE}} statement.

Example:
{code:sql}
DECLARE
  plsql_block VARCHAR2(500);
  new_deptid  NUMBER(4);
  new_dname   VARCHAR2(30) := 'Advertising';
  new_mgrid   NUMBER(6)    := 200;
  new_locid   NUMBER(4)    := 1700;
BEGIN
 -- Dynamic PL/SQL block invokes subprogram:
  plsql_block := 'BEGIN create_dept(:a, :b, :c, :d); END;';

 /* Specify bind arguments in USING clause.
    Specify mode for first parameter.
    Modes of other parameters are correct by default. */
  EXECUTE IMMEDIATE plsql_block
    USING IN OUT new_deptid, new_dname, new_mgrid, new_locid;
END;
/
{code}
",,0,6,0,100,0.82716,"EXECUTE IMMEDIATE statement $end$ Add support for Oracle-style {{EXECUTE IMMEDIATE}} statement.

Example:
{code:sql}
DECLARE
  plsql_block VARCHAR2(500);
  new_deptid  NUMBER(4);
  new_dname   VARCHAR2(30) := 'Advertising';
  new_mgrid   NUMBER(6)    := 200;
  new_locid   NUMBER(4)    := 1700;
BEGIN
 -- Dynamic PL/SQL block invokes subprogram:
  plsql_block := 'BEGIN create_dept(:a, :b, :c, :d); END;';

 /* Specify bind arguments in USING clause.
    Specify mode for first parameter.
    Modes of other parameters are correct by default. */
  EXECUTE IMMEDIATE plsql_block
    USING IN OUT new_deptid, new_dname, new_mgrid, new_locid;
END;
/
{code}
 $acceptance criteria:$",6,1,1,1,1,1,1,0.0,17,10,0.588235,9,0.529412,9,0.529412,8,0.470588,8,0.470588
549,MDEV-10587,Technical task,MDEV,2016-08-18 04:08:02,,0,sql_mode=ORACLE: User defined exceptions,"When running {{sql_mode=ORACLE}}, MariaDB should support:
- user defined {{EXCEPTION}} declaration
- using user defined exceptions in {{RAISE name}} (signal)
- using user defined exceptions in {{RAISE}} (resignal)
- using user defined exceptions in {{EXCEPTION WHEN}} 

{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 (a INT) RETURN INT
AS
  e1 EXCEPTION;
BEGIN
  IF a < 0 THEN
    RAISE e1;  
  END IF;
  RETURN 0;
EXCEPTION  
  WHEN e1 THEN RETURN 1;
END;
/
{code}

User defined exception names should be case insensitive.

h2. Details
MariaDB does not support multiple handlers for the same SQLSTATE. This script returns an error:
{code:sql}
SET sql_mode=DEFAULT;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE c0 CONDITION FOR SQLSTATE '45000';
  DECLARE c1 CONDITION FOR SQLSTATE '45000';
  DECLARE CONTINUE HANDLER FOR c0 SET @c0= 0;
  DECLARE CONTINUE HANDLER FOR c1 SET @c0= 1;
END;
$$
{code}
{noformat}
ERROR 1413 (42000): Duplicate handler declared in the same block
{noformat}

In sql_mode=ORACLE it will be possible to have multiple user-defined exceptions in the same block. All user defined exceptions will be associated with {{SQLSTATE '45000'}} and MySQL errno {{ER_SIGNAL_EXCEPTION}}
{code:sql}
SET sql_mode=ORACLE;
DELIMITER $$;
CREATE FUNCTION f1(c VARCHAR) RETURN VARCHAR
AS
  e EXCEPTION;
  f EXCEPTION;
  a VARCHAR(64):='';
BEGIN
  BEGIN
    IF c = 'e' THEN RAISE e; END IF;
    IF c = 'f' THEN RAISE f; END IF;
  EXCEPTION
    WHEN e THEN BEGIN a:='Got EXCEPTION1/e; '; RAISE e; END;
    WHEN f THEN BEGIN a:='Got EXCEPTION1/f; '; RAISE f; END;
  END;
  RETURN 'Got no exceptions';
EXCEPTION
  WHEN OTHERS THEN RETURN a || 'Got EXCEPTION2/OTHERS;';
END;
$$
{code}
Notice, both {{e}} and {{f}} are associated with the same SQLSTATE and errno, but having {{WHEN e}} followed by {{WHEN f}} is valid, because {{e}} and {{f}}  are different exceptions.

However, specifying the same exception multiple times in {{WHEN}} clause will not be possible:
{code:sql}
SET sql_mode=ORACLE;
DELIMITER $$;
CREATE FUNCTION f1() RETURN VARCHAR
AS
  e EXCEPTION;
BEGIN
  RETURN 'Got no exceptions';
EXCEPTION
  WHEN e THEN RETURN 'Got exception e';
  WHEN e THEN RETURN 'Got exception e';
END;
$$
{code}
The above script will return the {{ER_SP_DUP_HANDLER}} error:
{noformat}
ERROR 42000: Duplicate handler declared in the same block
{noformat}


h2. Name space
In MariaDB variables and exceptions (conditions) are in separate name spaces. Oracle-alike user defined exceptions will be in the same name space with MariaDB conditions.
In Oracle, variables and exceptions (conditions) are in the same name space. Placing exceptions and variables into the same name space will be done separately. See MDEV-11058.
",,"sql_mode=ORACLE: User defined exceptions $end$ When running {{sql_mode=ORACLE}}, MariaDB should support:
- user defined {{EXCEPTION}} declaration
- using user defined exceptions in {{RAISE name}} (signal)
- using user defined exceptions in {{RAISE}} (resignal)
- using user defined exceptions in {{EXCEPTION WHEN}} 

{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 (a INT) RETURN INT
AS
  e1 EXCEPTION;
BEGIN
  IF a < 0 THEN
    RAISE e1;  
  END IF;
  RETURN 0;
EXCEPTION  
  WHEN e1 THEN RETURN 1;
END;
/
{code}

User defined exception names should be case insensitive.

h2. Details
MariaDB does not support multiple handlers for the same SQLSTATE. This script returns an error:
{code:sql}
SET sql_mode=DEFAULT;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE c0 CONDITION FOR SQLSTATE '45000';
  DECLARE c1 CONDITION FOR SQLSTATE '45000';
  DECLARE CONTINUE HANDLER FOR c0 SET @c0= 0;
  DECLARE CONTINUE HANDLER FOR c1 SET @c0= 1;
END;
$$
{code}
{noformat}
ERROR 1413 (42000): Duplicate handler declared in the same block
{noformat}

In sql_mode=ORACLE it will be possible to have multiple user-defined exceptions in the same block. All user defined exceptions will be associated with {{SQLSTATE '45000'}} and MySQL errno {{ER_SIGNAL_EXCEPTION}}
{code:sql}
SET sql_mode=ORACLE;
DELIMITER $$;
CREATE FUNCTION f1(c VARCHAR) RETURN VARCHAR
AS
  e EXCEPTION;
  f EXCEPTION;
  a VARCHAR(64):='';
BEGIN
  BEGIN
    IF c = 'e' THEN RAISE e; END IF;
    IF c = 'f' THEN RAISE f; END IF;
  EXCEPTION
    WHEN e THEN BEGIN a:='Got EXCEPTION1/e; '; RAISE e; END;
    WHEN f THEN BEGIN a:='Got EXCEPTION1/f; '; RAISE f; END;
  END;
  RETURN 'Got no exceptions';
EXCEPTION
  WHEN OTHERS THEN RETURN a || 'Got EXCEPTION2/OTHERS;';
END;
$$
{code}
Notice, both {{e}} and {{f}} are associated with the same SQLSTATE and errno, but having {{WHEN e}} followed by {{WHEN f}} is valid, because {{e}} and {{f}}  are different exceptions.

However, specifying the same exception multiple times in {{WHEN}} clause will not be possible:
{code:sql}
SET sql_mode=ORACLE;
DELIMITER $$;
CREATE FUNCTION f1() RETURN VARCHAR
AS
  e EXCEPTION;
BEGIN
  RETURN 'Got no exceptions';
EXCEPTION
  WHEN e THEN RETURN 'Got exception e';
  WHEN e THEN RETURN 'Got exception e';
END;
$$
{code}
The above script will return the {{ER_SP_DUP_HANDLER}} error:
{noformat}
ERROR 42000: Duplicate handler declared in the same block
{noformat}


h2. Name space
In MariaDB variables and exceptions (conditions) are in separate name spaces. Oracle-alike user defined exceptions will be in the same name space with MariaDB conditions.
In Oracle, variables and exceptions (conditions) are in the same name space. Placing exceptions and variables into the same name space will be done separately. See MDEV-11058.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,38,,0,0,2,5,0,14,0,,0,850,0,6,0,2016-10-13 06:36:46,sql_mode=ORACLE: User defined exceptions,"When running {{sql_mode=ORACLE}}, MariaDB should support:
- user defined {{EXCEPTION}} declaration
- using user defined exceptions in {{RAISE name}} (signal)
- using user defined exceptions in {{RAISE}} (resignal)
- using user defined exceptions in {{EXCEPTION WHEN}} 

{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 (a INT) RETURN INT
AS
  e1 EXCEPTION;
BEGIN
  IF a < 0 THEN
    RAISE e1;  
  END IF;
  RETURN 0;
EXCEPTION  
  WHEN e1 THEN RETURN 1;
END;
/
{code}

User defined exception names should be case insensitive.

Variables and exceptions are in the same name space. Having a variable and an exception with the same name is not possible in the same scope:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR
AS
  e INT:=10;
  e EXCEPTION;
BEGIN
  RAISE e;
END;
/
SHOW ERRORS;
{code}
{noformat}
LINE/COL ERROR
-------- -----------------------------------------------------------------
6/3	 PL/SQL: Statement ignored
6/9	 PLS-00371: at most one declaration for 'E' is permitted
{noformat}
",,0,8,0,365,2.01299,"sql_mode=ORACLE: User defined exceptions $end$ When running {{sql_mode=ORACLE}}, MariaDB should support:
- user defined {{EXCEPTION}} declaration
- using user defined exceptions in {{RAISE name}} (signal)
- using user defined exceptions in {{RAISE}} (resignal)
- using user defined exceptions in {{EXCEPTION WHEN}} 

{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 (a INT) RETURN INT
AS
  e1 EXCEPTION;
BEGIN
  IF a < 0 THEN
    RAISE e1;  
  END IF;
  RETURN 0;
EXCEPTION  
  WHEN e1 THEN RETURN 1;
END;
/
{code}

User defined exception names should be case insensitive.

Variables and exceptions are in the same name space. Having a variable and an exception with the same name is not possible in the same scope:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN VARCHAR
AS
  e INT:=10;
  e EXCEPTION;
BEGIN
  RAISE e;
END;
/
SHOW ERRORS;
{code}
{noformat}
LINE/COL ERROR
-------- -----------------------------------------------------------------
6/3	 PL/SQL: Statement ignored
6/9	 PLS-00371: at most one declaration for 'E' is permitted
{noformat}
 $acceptance criteria:$",8,1,1,1,1,1,1,1346.47,18,11,0.611111,10,0.555556,10,0.555556,9,0.5,9,0.5
550,MDEV-10588,Technical task,MDEV,2016-08-18 04:17:45,,0,sql_mode=ORACLE: TRUNCATE TABLE t1 [ {DROP|REUSE} STORAGE ],"When running with {{sql_mode=ORACLE}}, MariaDB will support an optional {{STORAGE}} clause.

Example:
{code:sql}
TRUNCATE TABLE t1 DROP STORAGE;
{code}

An open question is if the parser should just consume and ignore the {{STORAGE}} clause, or the underlying engine should perform some actions, depending on the choice between {{DROP}} or {{REUSE}}.

The default behavior is {{DROP STORAGE}}.

Note, although {{REUSE}} is a reserved keyword in Oracle, we won't reserve it even in {{sql_mode=ORACLE}} under terms of this task, as {{REUSE}} does not introduce grammar conflicts.
",,"sql_mode=ORACLE: TRUNCATE TABLE t1 [ {DROP|REUSE} STORAGE ] $end$ When running with {{sql_mode=ORACLE}}, MariaDB will support an optional {{STORAGE}} clause.

Example:
{code:sql}
TRUNCATE TABLE t1 DROP STORAGE;
{code}

An open question is if the parser should just consume and ignore the {{STORAGE}} clause, or the underlying engine should perform some actions, depending on the choice between {{DROP}} or {{REUSE}}.

The default behavior is {{DROP STORAGE}}.

Note, although {{REUSE}} is a reserved keyword in Oracle, we won't reserve it even in {{sql_mode=ORACLE}} under terms of this task, as {{REUSE}} does not introduce grammar conflicts.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,21,,0,1,0,5,0,1,0,,0,850,1,0,0,2016-08-18 04:17:45,sql_mode=ORACLE: TRUNCATE TABLE t1 [ {DROP|REUSE} STORAGE ],"When running with {{sql_mode=ORACLE}}, MariaDB will support an optional {{STORAGE}} clause.

Example:
{code:sql}
TRUNCATE TABLE t1 DROP STORAGE;
{code}

An open question is if the parser should just consume and ignore the {{STORAGE}} clause, or the underlying engine should perform some actions, depending on the choice between {{DROP}} or {{REUSE}}.

The default behavior is {{DROP STORAGE}}.
",,0,1,0,28,0.41791,"sql_mode=ORACLE: TRUNCATE TABLE t1 [ {DROP|REUSE} STORAGE ] $end$ When running with {{sql_mode=ORACLE}}, MariaDB will support an optional {{STORAGE}} clause.

Example:
{code:sql}
TRUNCATE TABLE t1 DROP STORAGE;
{code}

An open question is if the parser should just consume and ignore the {{STORAGE}} clause, or the underlying engine should perform some actions, depending on the choice between {{DROP}} or {{REUSE}}.

The default behavior is {{DROP STORAGE}}.
 $acceptance criteria:$",1,1,1,1,1,1,1,0.0,19,12,0.631579,11,0.578947,11,0.578947,10,0.526316,10,0.526316
551,MDEV-10591,Technical task,MDEV,2016-08-18 06:07:13,,0,Oracle-style packages,"h2. General information

A package is a schema object that groups logically related PL/SQL data types, items (e.g. variables, cursors, exceptions) and subprograms.

Packages usually have two parts:
- a specification
- a body

Sometimes the body is unnecessary.

The specification describes the interface of the package; it declares the types, variables, constants, exceptions, cursors, and subprograms available for use. The body fully defines cursors and subprograms, and so implements the specification.
A package can have an initialization block which is executed once, when the package is referenced for the first time.

If we think of packages in terms of {{C++}}, the specification is a class definition in a {{.h}} file, and the body is method implementations in {{.cc}} files.

All routines that have prototypes in {{CREATE PACKAGE}} must be further implemented in {{CREATE PACKAGE BODY}}. An attempt to create an incomplete package body returns an error.

The body can have its own declarations (e.g. types, items, subprograms) not specified in the interface. In terms of {{C++}}, these declarations are similar to {{private}} declarations inside a class.

h2. Name resolution
This example calls the function {{f1}} from the package {{test1}}:
{code:sql}
SELECT test1.f1() FROM DUAL;
{code}

But this can also mean the function {{f1}} from the schema {{test1}}.

Rules:
- Packages and schemas with the same name can co-exist.
- If a package with the name {{test1}} exists in the current schema, then Oracle searches for function {{f1}} in this package {{test1}} only. It doesn't search in the schema {{test1}} any more. So packages hide shemas with the same name.

h2. Name space
Package names use the same name space with functions and procedures. For example, a {{CREATE FUNCTION test1}} followed by {{CREATE PACKAGE test1}} returns an error:
{code:sql}
DROP FUNCTION test1;
DROP PACKAGE test1;
DROP PROCEDURE test1;

CREATE FUNCTION test1 RETURN INT
AS
  a INT :=10;
BEGIN
  RETURN a;
END;
/

CREATE PACKAGE test1
AS
  a INT := 10;
FUNCTION f1 RETURN INT;
END test1;
/
{code}
{noformat}
ORA-00955: name is already used by an existing object
{noformat}

h2. Name references
Functions defined in a package must be referenced with the package name qualifier:
{code:sql}
SELECT test1.f1() FROM DUAL;
{code}

There is no a way to call the function {{f1}} from outside of the package without using the {{test1}} qualifier.

Function implementations defined in the same {{CREATE PACKAGE BODY test1}} do not have to use the qualifier {{test1}} to refer to each other.

h2. DDL statements

Package related statements include:

- {{CREATE PACKAGE}}
- {{CREATE PACKAGE BODY}}
- {{ALTER PACKAGE}}
- {{DROP PACKAGE BODY}}
- {{DROP PACKAGE}}

{{DROP PACKAGE}} drops both body and specification.

h2. SHOW statements
We'll implement the following {{SHOW}} statements:
- {{SHOW PACKAGE STATUS [LIKE 'pkg_name'];}}
- {{SHOW PACKAGE BODY STATUS [LIKE 'pkg_name']}}
- {{SHOW CREATE PACKAGE pkg_name;}}
- {{SHOW create PACKAGE BODY pkg_name;}}

Qualified package names will also be supported:
{code:sql}
SHOW CREATE PACKAGE db_name.pkg_name;
SHOW CREATE PACKAGE BODY db_name.pkg_name;
{code}

h2. {{DBA_PROCEDURES}}, {{ALL_PROCEDURES}}, {{USER_PROCEDURES}} metadata views.

One can query the metadata views {{DBA_PROCEDURES}}, {{ALL_PROCEDURES}}, {{USER_PROCEDURES}} to list routines defined inside packages.

{code:sql}
DROP PACKAGE test1;
DROP FUNCTION test1;
DROP PROCEDURE test1;
CREATE PACKAGE test1 AS
  FUNCTION f1 RETURN INT;
  FUNCTION f2 RETURN INT;
END test1;
/
{code}
Now this query:
{code:sql}
SELECT OBJECT_NAME || ' ' ||OBJECT_TYPE ||' '|| PROCEDURE_NAME AS c FROM USER_PROCEDURES WHERE OBJECT_NAME='TEST1';
{code}
returns
{noformat}
C
--------------------------------------------------------------------------------
TEST1 PACKAGE F1
TEST1 PACKAGE F2
TEST1 PACKAGE
{noformat}
Note, the routines defined in a {{CREATE PACKAGE}} statement are immediately seen in the {{XXX_PROCEDURES}} metadata views, even before the corresponding {{CREATE PACKAGE BODY}} statement.


Note, stand-alone routines are not visible in the {{XXX_PROCEDURES}} metadata views. This script:
{code:sql}
DROP PACKAGE test1;
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN INT AS BEGIN RETURN 10; END;
/
SELECT OBJECT_NAME || ' ' ||OBJECT_TYPE ||' '|| PROCEDURE_NAME AS c FROM USER_PROCEDURES WHERE PROCEDURE_NAME='F1';
{code}
returns no records.

h2. {{ALL_SOURCE}} metadata view

The {{ALL_SOURCE}} metadata view can be used to see the source code of packages and their bodies, as well as stand-alone routines.

{code:sql}
DROP FUNCTION f1;
DROP PACKAGE test1;
CREATE FUNCTION f1 RETURN INT AS BEGIN RETURN 10; END;
/
CREATE PACKAGE test1 AS
  // This is a comment
  FUNCTION f2 RETURN INT;
END;
/
CREATE PACKAGE BODY test1 AS
  // This is one more comment
  FUNCTION f2 RETURN INT AS BEGIN RETURN 11; END;
END;
/
SELECT OWNER || ' ' || TYPE || ' ' || NAME || ' '|| LINE || ' '|| TEXT AS l
FROM all_source
WHERE name IN ('TEST1','F1') ORDER BY NAME, TYPE, LINE;
{code}
{noformat}
L
--------------------------------------------------------------------------------
SYSTEM FUNCTION F1 1 FUNCTION f1 RETURN INT AS BEGIN RETURN 10; END;
SYSTEM PACKAGE TEST1 1 PACKAGE test1 AS
SYSTEM PACKAGE TEST1 2	 // This is a comment
SYSTEM PACKAGE TEST1 3	 FUNCTION f2 RETURN INT;
SYSTEM PACKAGE TEST1 4 END;
SYSTEM PACKAGE BODY TEST1 1 PACKAGE BODY test1 AS
SYSTEM PACKAGE BODY TEST1 2   // This is one more comment
SYSTEM PACKAGE BODY TEST1 3   FUNCTION f2 RETURN INT AS BEGIN RETURN 11; END;
SYSTEM PACKAGE BODY TEST1 4 END;
{noformat}

Notice, comment lines inside {{CREATE PACKAGE}} and {{CREATE PACKAGE BODY}} are preserved.

h2. Grammar for the {{CREATE PACKAGE}} statement

The exact grammar can slightly vary between Oracle version.

{code:sql}
create_package_statement ::=
CREATE [ OR REPLACE ] PACKAGE [ schema. ] package
   [ invoker_rights_clause ]
   { IS | AS } [ item_list_1 ] END [ package_name ] ;

invoker_rights_clause ::= AUTHID { CURRENT_USER | DEFINER }

item_list_1 ::= item_list_1_initial {item_list_1_initial | pragma}...

item_list_1_initial ::= 
    type_definition
  | item_declaration
  | function_declaration
  | procedure_declaration

type_definition ::=
    record_type_definition
  | ref_cursor_type_definition
  | subtype_definition
  | collection_type_definition

record_type_definition ::=
  TYPE type_name IS RECORD ( field_declaration [, field_declaration]... ) ;

ref_cursor_type_definition ::= 
  TYPE type_name IS REF CURSOR
  [ RETURN
    { {db_table_name | cursor_name | cursor_variable_name}%ROWTYPE
    | record_name%TYPE
    | record_type_name
    | ref_cursor_type_name
    }
  ] ;

subtype_definition ::= SUBTYPE subtype_name IS base_type [ ( constraint ) ] [ NOT NULL ]

collection_type_definition ::= TYPE type_name IS collection_type_def

collection_type_def ::= 
    assoc_array_type_def
  | nested_table_type_def
  | varray_type_def


assoc_array_type_def ::= TABLE OF element_type [ NOT NULL ]
   [ INDEX BY { PLS_INTEGER | BINARY_INTEGER | VARCHAR2 ( v_size ) } ]

nested_table_type_def ::= TABLE OF element_type [ NOT NULL ]

element_type ::=
    cursor_name%ROWTYPE
  | db_table_name{%ROWTYPE | .column_name%TYPE}
  | object_name%TYPE
  | [REF] object_type_name
  | record_name[.field_name]%TYPE
  | record_type_name
  | scalar_datatype_name
  | variable_name%TYPE

item_declaration ::=
  collection_variable_declaration
| constant_declaration
| cursor_declaration
| cursor_variable_declaration
| exception_declaration
| object_declaration      -- TODO: add grammar
| object_ref_declaration  -- TODO: add grammar
| record_declaration      -- TODO: add grammar
| variable_declaration

collection_variable_declaration ::= collection_name type_name

constant_declaration ::= constant_name CONSTANT datatype [NOT NULL] { := | DEFAULT } expression ;

cursor_declaration ::= CURSOR cursor_name
 [ ( cursor_parameter_declaration [, cursor_parameter_declaration ]... )]
   [ RETURN rowtype] IS select_statement

cursor_variable_declaration ::= TYPE type IS REF CURSOR
  [ RETURN
    { {db_table_or_view | cursor | cursor_variable}%ROWTYPE
    | record%TYPE
    | record_type
    | ref_cursor_type
    }
  ]

exception_declaration ::= exception_name EXCEPTION

variable_declaration ::=
  variable_name datatype [ [ NOT NULL] {:= | DEFAULT} expression ]
{code}

h2. Grammar for the {{CREATE PACKAGE BODY}} statement
{code:sql}
create_package_body_statement ::=
CREATE [ OR REPLACE ] PACKAGE BODY [ schema. ] package
   { IS | AS } [ declare_section ] { body | END package_name } ;

declare_section ::=
   item_list_1
 | item_list_2
 | item_list_1 item_list_2


item_list_2 ::=
{ function_declaration
| function_definition
| procedure_declaration
| procedure_definition
}
  [ { function_declaration
    | function_definition
    | procedure_declaration
    | procedure_definition
    | pragma
    }
  ]...


function_declaration ::=
  function_heading [ DETERMINISTIC | PIPELINED | RESULT_CACHE ]... ;

function_heading ::=
  FUNCTION function_name [ ( parameter_declaration ) ] RETURN datatype

function_definition ::=
  function_heading [ DETERMINISTIC
                   | PIPELINED
                   | result_cache_clause
                   ]... { IS | AS } [ declare_section ] body

procedure_declaration ::= procedure_heading

procedure_heading ::=
  PROCEDURE procedure_name
  [ ( parameter_declaration [, parameter_declaration ]... ) ] ;

procedure_definition ::=
  procedure_heading { IS | AS } [ declare_section ] body

body ::=
  BEGIN statement [ statement | pragma ]...
  [ EXCEPTION exception_handler [ exception_handler ]... ]
  END [ name ] ;
{code}

h2. Possible implementation solutions.

h3. 1. Store the entire package definition in a new system table {{packages}}.

Note, it's not desirable to use {{package}} as the new table name, because it's a reserved keyword in Oracle.

h4. a. Store the entire package as a record in the new system table, in binary format.

The apprimate structure of the new table:
{code:sql}
CREATE TABLE packages (
  db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  name char(64) NOT NULL,
  security_type enum('INVOKER','DEFINER') NOT NULL DEFAULT 'DEFINER',
  interface longblob NOT NULL,
  body longblob,
  definer char(141) NOT NULL,
  sql_mode set (<the same definition with sql_mode in mysql.proc>) NOT NULL DEFAULT '',
  character_set_client char(32) NOT NULL,
  collation_connection char(32) NOT NULL,
  db_collation char(32) NOT NILL,
  body_utf8 longblob,
  PRIMARY KEY (db,name)
) DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;
{code}

The meaning of the columns {{sql_mode}}, {{character_set_client}}, {{character_set_connection}}, {{db_collation}}, {{body_utf8}} is the same with the meaning of similar columns in the table {{mysql.proc}}.

Data flow:
- {{CREATE PACKAGE}} will create a new record in the table {{mysql.packages}} and populate the column {{packages.interface}}, while the column {{packages.body}} will remain {{NULL}}.
- {{CREATE PACKAGE BODY}} will populate the column {{packages.body}}.
- {{DROP PACKAGE}} will delete the corresponding record in {{packages}}.
- {{DROP PACKAGE BODY}} will set {{packages.body}} to {{NULL}}
- {{ALTER PACKAGE BODY}} will replace {{packages.body}}


Open questions:

- Q1. What should we do if the character set variables during the {{CREATE PACKAGE}} and {{CREATE PACKAGE BODY}} are different. We could return an error, or apply some conversion. But both seems to be potentially buggy.


h4. b. Similar to #2, but store {{packages.interface}} and {{packages.body}} in the {{utf8}} character set:
{code:sql}
CREATE TABLE packages (
  db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  name char(64) NOT NULL,
  security_type enum('INVOKER','DEFINER') NOT NULL DEFAULT 'DEFINER',
  interface longtext NOT NULL,
  body longtext,
  definer char(141) NOT NULL,
  sql_mode set (<the same definition with sql_mode in mysql.proc>) NOT NULL DEFAULT '',
  PRIMARY KEY (db,name)
) DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;
{code}

The {{CREATE PACKAGE}} and {{CREATE PACKAGE BODY}} statements will convert the information from the client character set to {{utf8}} and store in the columns {{interface}} and {{body}}.

This simplifies the things and solves the question {{Q1}} in the section {{a}}, however will impose some minor limitations in the first version: some tricky string literals won't be able to be stored. Later we'll implement {{Unicode Escape Sequences}} as described in MySQL WL#3529 to overcome this minor limitation. This should be a more future proof solution than having the four extra character set conversion related columns in the new table.

h2. 2. Store the entire package definiton in files in the database directory (like views and triggers do).

This is very similar to {{1.a}} and {{1.b}}, however the interface and the implementation of a package are stored in files in the database directory. The interface is stored in a file with the extension {{.pin}}, and the implementation is stored in the file with the extension {{.pif}}.

As in {{1}}, two formats are possible:
- a. Binary format with character set information
- b. utf8 format

Data flow:
- {{CREATE PACKAGE pkg1}} stores information in the file {{pkg1.pin}} in the database directory.
- {{CREATE PACKAGE BODY pkg1}} stored information in the fike {{pkg1.pif}} in the database directory.
- {{DROP PACKAGE pgk1}} deletes both files {{pkg1.pin}} and {{pkg1.pif}}
- {{DROP PACKAGE BODY pgk1}} deletes the file {{pkg1.pif}}
- {{ALTER PACKAGE BODY pkg1}} replaces information in the file {{pkg1.pif}}


h2. 3. Do not store the entire package implementation as a single object. Store definition of every package object separately. Every function and procedure will generate a record in the table {{mysql.proc}}.

This will need some changes in the table {{mysql.proc}}.
{code:sql}
ALTER TABLE proc DROP PRIMARY KEY;
ALTER TABLE proc ADD ordinal_position INT NOT NULL DEFAULT 0;
ALTER TABLE proc ADD package_name VARCHAR(64) NOT NULL DEFAULT '';
ALTER TABLE proc ADD PRIMARY KEY (db,package_name,name,type);
{code}

Open questions:
- Where to store the package interface? It's useful for fast object lookup.
- What should we do on errors if during a {{CREATE PACKATE BODY}} query {{n}} routines have been already created and the {{n+1}}-th routine fails to create for some reasons (e.g. disk full). All created routine definitions should be deleted.
- It's not clear what to with with comments inside {{CREATE PACKAGE [BODY]}}. It would be nice to preserve them somehow.
- This will bring some data duplication. All routines inside a package use the same {{SQL SECURITY}} (i.e. {{INVOKER}} or {{DEFINER}}, as well as the same {{sql_mode}}.  It's not clear what to do if somebody manually modifies the {{security_type}} or the {{sql_mode}} for some routine in a package without modifying the other routines of the same package.

h2. 4. Store packages in preudo-databases
Every {{CREATE PACKAGE}} could create a preudo-database with a special name which will be a combination of the owning database name and the package name. For example, a {{CREATE PACKAGE pkg1}} in a database {{db1}} could create a pseudo database with name `db1#pkg1`, while a later {{CREATE PACKAGE BODY pkg1}} could create routines in this new pseudo-database. Preudo databases won't be visible for normal {{SHOW DATABASES}}. 

Data flow:
- {{CREATE PACKAGE}} creates a pseudo-database.
- {{CREATE PACKAGE BODY}} creates routines in the preuso-database.
- {{DROP PACKAGE}} drops the pseudo-database.
- {{DROP PACKAGE BODY}} drops all routines in the pseudo-database.
- {{ALTER PACKAGE BODY}} drops all routines in the preuso-database and creates them again.

Open questions:
- The column {{proc.db}} has a limit of {{64}} characters. Using it to store the combination of the database name and the package name will most likely require to extent the column {{proc.db}} to {{129}} characters, to fit both.


h2. 5. Store the entire package interfaces and bodies in {{mysql.proc}}

In Oracle routines and packages reuse the same name space. We cannot exactly reproduce this, because {{mysql.proc}} has a primary key on {{(db,name,type)}} and thus {{PROCEDURE p1}} and {{FUNCTION p1}} can co-exists.

But we could store package interfaces and package bodies in the same table {{mysql.proc}}. It is very similar to {{1a}}, however every package will generate two records instead of one record (i.e. one record for the interface and one record for the body). No records for the individual package routines will be inserted into {{mysql.proc}}.

This will need a change in the table:

{code:sql}
ALTER TABLE mysql.proc MODIFY type ENUM('FUNCTION','PROCEDURE','PACKAGE','PACKAGE BODY');
{code}

Data flow:
- {{CREATE PACKAGE pkg1}} inserts a record with type 'PACKAGE' and name 'pkg1'
- {{CREATE PACKAGE BODY pkg1}} inserts a record with type 'PACKAGE BODY' and name 'pkg1'
- {{ALTER PACKAGE BODY pkg1}} updates the record with type 'PACKAGE BODY' and name 'pkg1'
- {{DROP PACKAGE BODY pkg1}} deletes the record with type 'PACKAGE BODY' and name 'pkg1'
- {{DROP PACKAGE pkg1}} deletes the records with name 'pkg1' and types 'PACKAGE' and 'PACKAGE BODY'

Open questions:
- Some columns, e.g. {{is_deterministic}}, {{param_list}} are not applicable to packages. These columns can be set to {{NULL}} for the package related records. However, this brings some data redundancy.
- We'll have to keep maintaining the {{body_utf8}}, {{character_set_client}}, {{collation_connection}} columns for packages. This makes it harder to get rid of theme eventually (to use the column {{body}} for both {{SHOW}} and {{I_S}} purposes).

h2. Conclusions
The solutions {{1.b}} and {{2.b}} look the most promising.

{{2017-01-20 UPDATE}}: during a discussion with Monty and Bar, it was decided that we'd go {{1.b}}. This will help to avoid storing two definitions:
- binary definition for parsing purposes
- utf8 definition for {{INFORMATION_SCHEMA}} purposes

TODO:
- think about MDL to avoid concurrent {{CREATE PACKAGE pkg1}}, {{DROP PACKAGE pkg1}}, {{CALL pkg1.proc1()}}.
",,"Oracle-style packages $end$ h2. General information

A package is a schema object that groups logically related PL/SQL data types, items (e.g. variables, cursors, exceptions) and subprograms.

Packages usually have two parts:
- a specification
- a body

Sometimes the body is unnecessary.

The specification describes the interface of the package; it declares the types, variables, constants, exceptions, cursors, and subprograms available for use. The body fully defines cursors and subprograms, and so implements the specification.
A package can have an initialization block which is executed once, when the package is referenced for the first time.

If we think of packages in terms of {{C++}}, the specification is a class definition in a {{.h}} file, and the body is method implementations in {{.cc}} files.

All routines that have prototypes in {{CREATE PACKAGE}} must be further implemented in {{CREATE PACKAGE BODY}}. An attempt to create an incomplete package body returns an error.

The body can have its own declarations (e.g. types, items, subprograms) not specified in the interface. In terms of {{C++}}, these declarations are similar to {{private}} declarations inside a class.

h2. Name resolution
This example calls the function {{f1}} from the package {{test1}}:
{code:sql}
SELECT test1.f1() FROM DUAL;
{code}

But this can also mean the function {{f1}} from the schema {{test1}}.

Rules:
- Packages and schemas with the same name can co-exist.
- If a package with the name {{test1}} exists in the current schema, then Oracle searches for function {{f1}} in this package {{test1}} only. It doesn't search in the schema {{test1}} any more. So packages hide shemas with the same name.

h2. Name space
Package names use the same name space with functions and procedures. For example, a {{CREATE FUNCTION test1}} followed by {{CREATE PACKAGE test1}} returns an error:
{code:sql}
DROP FUNCTION test1;
DROP PACKAGE test1;
DROP PROCEDURE test1;

CREATE FUNCTION test1 RETURN INT
AS
  a INT :=10;
BEGIN
  RETURN a;
END;
/

CREATE PACKAGE test1
AS
  a INT := 10;
FUNCTION f1 RETURN INT;
END test1;
/
{code}
{noformat}
ORA-00955: name is already used by an existing object
{noformat}

h2. Name references
Functions defined in a package must be referenced with the package name qualifier:
{code:sql}
SELECT test1.f1() FROM DUAL;
{code}

There is no a way to call the function {{f1}} from outside of the package without using the {{test1}} qualifier.

Function implementations defined in the same {{CREATE PACKAGE BODY test1}} do not have to use the qualifier {{test1}} to refer to each other.

h2. DDL statements

Package related statements include:

- {{CREATE PACKAGE}}
- {{CREATE PACKAGE BODY}}
- {{ALTER PACKAGE}}
- {{DROP PACKAGE BODY}}
- {{DROP PACKAGE}}

{{DROP PACKAGE}} drops both body and specification.

h2. SHOW statements
We'll implement the following {{SHOW}} statements:
- {{SHOW PACKAGE STATUS [LIKE 'pkg_name'];}}
- {{SHOW PACKAGE BODY STATUS [LIKE 'pkg_name']}}
- {{SHOW CREATE PACKAGE pkg_name;}}
- {{SHOW create PACKAGE BODY pkg_name;}}

Qualified package names will also be supported:
{code:sql}
SHOW CREATE PACKAGE db_name.pkg_name;
SHOW CREATE PACKAGE BODY db_name.pkg_name;
{code}

h2. {{DBA_PROCEDURES}}, {{ALL_PROCEDURES}}, {{USER_PROCEDURES}} metadata views.

One can query the metadata views {{DBA_PROCEDURES}}, {{ALL_PROCEDURES}}, {{USER_PROCEDURES}} to list routines defined inside packages.

{code:sql}
DROP PACKAGE test1;
DROP FUNCTION test1;
DROP PROCEDURE test1;
CREATE PACKAGE test1 AS
  FUNCTION f1 RETURN INT;
  FUNCTION f2 RETURN INT;
END test1;
/
{code}
Now this query:
{code:sql}
SELECT OBJECT_NAME || ' ' ||OBJECT_TYPE ||' '|| PROCEDURE_NAME AS c FROM USER_PROCEDURES WHERE OBJECT_NAME='TEST1';
{code}
returns
{noformat}
C
--------------------------------------------------------------------------------
TEST1 PACKAGE F1
TEST1 PACKAGE F2
TEST1 PACKAGE
{noformat}
Note, the routines defined in a {{CREATE PACKAGE}} statement are immediately seen in the {{XXX_PROCEDURES}} metadata views, even before the corresponding {{CREATE PACKAGE BODY}} statement.


Note, stand-alone routines are not visible in the {{XXX_PROCEDURES}} metadata views. This script:
{code:sql}
DROP PACKAGE test1;
DROP FUNCTION f1;
CREATE FUNCTION f1 RETURN INT AS BEGIN RETURN 10; END;
/
SELECT OBJECT_NAME || ' ' ||OBJECT_TYPE ||' '|| PROCEDURE_NAME AS c FROM USER_PROCEDURES WHERE PROCEDURE_NAME='F1';
{code}
returns no records.

h2. {{ALL_SOURCE}} metadata view

The {{ALL_SOURCE}} metadata view can be used to see the source code of packages and their bodies, as well as stand-alone routines.

{code:sql}
DROP FUNCTION f1;
DROP PACKAGE test1;
CREATE FUNCTION f1 RETURN INT AS BEGIN RETURN 10; END;
/
CREATE PACKAGE test1 AS
  // This is a comment
  FUNCTION f2 RETURN INT;
END;
/
CREATE PACKAGE BODY test1 AS
  // This is one more comment
  FUNCTION f2 RETURN INT AS BEGIN RETURN 11; END;
END;
/
SELECT OWNER || ' ' || TYPE || ' ' || NAME || ' '|| LINE || ' '|| TEXT AS l
FROM all_source
WHERE name IN ('TEST1','F1') ORDER BY NAME, TYPE, LINE;
{code}
{noformat}
L
--------------------------------------------------------------------------------
SYSTEM FUNCTION F1 1 FUNCTION f1 RETURN INT AS BEGIN RETURN 10; END;
SYSTEM PACKAGE TEST1 1 PACKAGE test1 AS
SYSTEM PACKAGE TEST1 2	 // This is a comment
SYSTEM PACKAGE TEST1 3	 FUNCTION f2 RETURN INT;
SYSTEM PACKAGE TEST1 4 END;
SYSTEM PACKAGE BODY TEST1 1 PACKAGE BODY test1 AS
SYSTEM PACKAGE BODY TEST1 2   // This is one more comment
SYSTEM PACKAGE BODY TEST1 3   FUNCTION f2 RETURN INT AS BEGIN RETURN 11; END;
SYSTEM PACKAGE BODY TEST1 4 END;
{noformat}

Notice, comment lines inside {{CREATE PACKAGE}} and {{CREATE PACKAGE BODY}} are preserved.

h2. Grammar for the {{CREATE PACKAGE}} statement

The exact grammar can slightly vary between Oracle version.

{code:sql}
create_package_statement ::=
CREATE [ OR REPLACE ] PACKAGE [ schema. ] package
   [ invoker_rights_clause ]
   { IS | AS } [ item_list_1 ] END [ package_name ] ;

invoker_rights_clause ::= AUTHID { CURRENT_USER | DEFINER }

item_list_1 ::= item_list_1_initial {item_list_1_initial | pragma}...

item_list_1_initial ::= 
    type_definition
  | item_declaration
  | function_declaration
  | procedure_declaration

type_definition ::=
    record_type_definition
  | ref_cursor_type_definition
  | subtype_definition
  | collection_type_definition

record_type_definition ::=
  TYPE type_name IS RECORD ( field_declaration [, field_declaration]... ) ;

ref_cursor_type_definition ::= 
  TYPE type_name IS REF CURSOR
  [ RETURN
    { {db_table_name | cursor_name | cursor_variable_name}%ROWTYPE
    | record_name%TYPE
    | record_type_name
    | ref_cursor_type_name
    }
  ] ;

subtype_definition ::= SUBTYPE subtype_name IS base_type [ ( constraint ) ] [ NOT NULL ]

collection_type_definition ::= TYPE type_name IS collection_type_def

collection_type_def ::= 
    assoc_array_type_def
  | nested_table_type_def
  | varray_type_def


assoc_array_type_def ::= TABLE OF element_type [ NOT NULL ]
   [ INDEX BY { PLS_INTEGER | BINARY_INTEGER | VARCHAR2 ( v_size ) } ]

nested_table_type_def ::= TABLE OF element_type [ NOT NULL ]

element_type ::=
    cursor_name%ROWTYPE
  | db_table_name{%ROWTYPE | .column_name%TYPE}
  | object_name%TYPE
  | [REF] object_type_name
  | record_name[.field_name]%TYPE
  | record_type_name
  | scalar_datatype_name
  | variable_name%TYPE

item_declaration ::=
  collection_variable_declaration
| constant_declaration
| cursor_declaration
| cursor_variable_declaration
| exception_declaration
| object_declaration      -- TODO: add grammar
| object_ref_declaration  -- TODO: add grammar
| record_declaration      -- TODO: add grammar
| variable_declaration

collection_variable_declaration ::= collection_name type_name

constant_declaration ::= constant_name CONSTANT datatype [NOT NULL] { := | DEFAULT } expression ;

cursor_declaration ::= CURSOR cursor_name
 [ ( cursor_parameter_declaration [, cursor_parameter_declaration ]... )]
   [ RETURN rowtype] IS select_statement

cursor_variable_declaration ::= TYPE type IS REF CURSOR
  [ RETURN
    { {db_table_or_view | cursor | cursor_variable}%ROWTYPE
    | record%TYPE
    | record_type
    | ref_cursor_type
    }
  ]

exception_declaration ::= exception_name EXCEPTION

variable_declaration ::=
  variable_name datatype [ [ NOT NULL] {:= | DEFAULT} expression ]
{code}

h2. Grammar for the {{CREATE PACKAGE BODY}} statement
{code:sql}
create_package_body_statement ::=
CREATE [ OR REPLACE ] PACKAGE BODY [ schema. ] package
   { IS | AS } [ declare_section ] { body | END package_name } ;

declare_section ::=
   item_list_1
 | item_list_2
 | item_list_1 item_list_2


item_list_2 ::=
{ function_declaration
| function_definition
| procedure_declaration
| procedure_definition
}
  [ { function_declaration
    | function_definition
    | procedure_declaration
    | procedure_definition
    | pragma
    }
  ]...


function_declaration ::=
  function_heading [ DETERMINISTIC | PIPELINED | RESULT_CACHE ]... ;

function_heading ::=
  FUNCTION function_name [ ( parameter_declaration ) ] RETURN datatype

function_definition ::=
  function_heading [ DETERMINISTIC
                   | PIPELINED
                   | result_cache_clause
                   ]... { IS | AS } [ declare_section ] body

procedure_declaration ::= procedure_heading

procedure_heading ::=
  PROCEDURE procedure_name
  [ ( parameter_declaration [, parameter_declaration ]... ) ] ;

procedure_definition ::=
  procedure_heading { IS | AS } [ declare_section ] body

body ::=
  BEGIN statement [ statement | pragma ]...
  [ EXCEPTION exception_handler [ exception_handler ]... ]
  END [ name ] ;
{code}

h2. Possible implementation solutions.

h3. 1. Store the entire package definition in a new system table {{packages}}.

Note, it's not desirable to use {{package}} as the new table name, because it's a reserved keyword in Oracle.

h4. a. Store the entire package as a record in the new system table, in binary format.

The apprimate structure of the new table:
{code:sql}
CREATE TABLE packages (
  db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  name char(64) NOT NULL,
  security_type enum('INVOKER','DEFINER') NOT NULL DEFAULT 'DEFINER',
  interface longblob NOT NULL,
  body longblob,
  definer char(141) NOT NULL,
  sql_mode set (<the same definition with sql_mode in mysql.proc>) NOT NULL DEFAULT '',
  character_set_client char(32) NOT NULL,
  collation_connection char(32) NOT NULL,
  db_collation char(32) NOT NILL,
  body_utf8 longblob,
  PRIMARY KEY (db,name)
) DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;
{code}

The meaning of the columns {{sql_mode}}, {{character_set_client}}, {{character_set_connection}}, {{db_collation}}, {{body_utf8}} is the same with the meaning of similar columns in the table {{mysql.proc}}.

Data flow:
- {{CREATE PACKAGE}} will create a new record in the table {{mysql.packages}} and populate the column {{packages.interface}}, while the column {{packages.body}} will remain {{NULL}}.
- {{CREATE PACKAGE BODY}} will populate the column {{packages.body}}.
- {{DROP PACKAGE}} will delete the corresponding record in {{packages}}.
- {{DROP PACKAGE BODY}} will set {{packages.body}} to {{NULL}}
- {{ALTER PACKAGE BODY}} will replace {{packages.body}}


Open questions:

- Q1. What should we do if the character set variables during the {{CREATE PACKAGE}} and {{CREATE PACKAGE BODY}} are different. We could return an error, or apply some conversion. But both seems to be potentially buggy.


h4. b. Similar to #2, but store {{packages.interface}} and {{packages.body}} in the {{utf8}} character set:
{code:sql}
CREATE TABLE packages (
  db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  name char(64) NOT NULL,
  security_type enum('INVOKER','DEFINER') NOT NULL DEFAULT 'DEFINER',
  interface longtext NOT NULL,
  body longtext,
  definer char(141) NOT NULL,
  sql_mode set (<the same definition with sql_mode in mysql.proc>) NOT NULL DEFAULT '',
  PRIMARY KEY (db,name)
) DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;
{code}

The {{CREATE PACKAGE}} and {{CREATE PACKAGE BODY}} statements will convert the information from the client character set to {{utf8}} and store in the columns {{interface}} and {{body}}.

This simplifies the things and solves the question {{Q1}} in the section {{a}}, however will impose some minor limitations in the first version: some tricky string literals won't be able to be stored. Later we'll implement {{Unicode Escape Sequences}} as described in MySQL WL#3529 to overcome this minor limitation. This should be a more future proof solution than having the four extra character set conversion related columns in the new table.

h2. 2. Store the entire package definiton in files in the database directory (like views and triggers do).

This is very similar to {{1.a}} and {{1.b}}, however the interface and the implementation of a package are stored in files in the database directory. The interface is stored in a file with the extension {{.pin}}, and the implementation is stored in the file with the extension {{.pif}}.

As in {{1}}, two formats are possible:
- a. Binary format with character set information
- b. utf8 format

Data flow:
- {{CREATE PACKAGE pkg1}} stores information in the file {{pkg1.pin}} in the database directory.
- {{CREATE PACKAGE BODY pkg1}} stored information in the fike {{pkg1.pif}} in the database directory.
- {{DROP PACKAGE pgk1}} deletes both files {{pkg1.pin}} and {{pkg1.pif}}
- {{DROP PACKAGE BODY pgk1}} deletes the file {{pkg1.pif}}
- {{ALTER PACKAGE BODY pkg1}} replaces information in the file {{pkg1.pif}}


h2. 3. Do not store the entire package implementation as a single object. Store definition of every package object separately. Every function and procedure will generate a record in the table {{mysql.proc}}.

This will need some changes in the table {{mysql.proc}}.
{code:sql}
ALTER TABLE proc DROP PRIMARY KEY;
ALTER TABLE proc ADD ordinal_position INT NOT NULL DEFAULT 0;
ALTER TABLE proc ADD package_name VARCHAR(64) NOT NULL DEFAULT '';
ALTER TABLE proc ADD PRIMARY KEY (db,package_name,name,type);
{code}

Open questions:
- Where to store the package interface? It's useful for fast object lookup.
- What should we do on errors if during a {{CREATE PACKATE BODY}} query {{n}} routines have been already created and the {{n+1}}-th routine fails to create for some reasons (e.g. disk full). All created routine definitions should be deleted.
- It's not clear what to with with comments inside {{CREATE PACKAGE [BODY]}}. It would be nice to preserve them somehow.
- This will bring some data duplication. All routines inside a package use the same {{SQL SECURITY}} (i.e. {{INVOKER}} or {{DEFINER}}, as well as the same {{sql_mode}}.  It's not clear what to do if somebody manually modifies the {{security_type}} or the {{sql_mode}} for some routine in a package without modifying the other routines of the same package.

h2. 4. Store packages in preudo-databases
Every {{CREATE PACKAGE}} could create a preudo-database with a special name which will be a combination of the owning database name and the package name. For example, a {{CREATE PACKAGE pkg1}} in a database {{db1}} could create a pseudo database with name `db1#pkg1`, while a later {{CREATE PACKAGE BODY pkg1}} could create routines in this new pseudo-database. Preudo databases won't be visible for normal {{SHOW DATABASES}}. 

Data flow:
- {{CREATE PACKAGE}} creates a pseudo-database.
- {{CREATE PACKAGE BODY}} creates routines in the preuso-database.
- {{DROP PACKAGE}} drops the pseudo-database.
- {{DROP PACKAGE BODY}} drops all routines in the pseudo-database.
- {{ALTER PACKAGE BODY}} drops all routines in the preuso-database and creates them again.

Open questions:
- The column {{proc.db}} has a limit of {{64}} characters. Using it to store the combination of the database name and the package name will most likely require to extent the column {{proc.db}} to {{129}} characters, to fit both.


h2. 5. Store the entire package interfaces and bodies in {{mysql.proc}}

In Oracle routines and packages reuse the same name space. We cannot exactly reproduce this, because {{mysql.proc}} has a primary key on {{(db,name,type)}} and thus {{PROCEDURE p1}} and {{FUNCTION p1}} can co-exists.

But we could store package interfaces and package bodies in the same table {{mysql.proc}}. It is very similar to {{1a}}, however every package will generate two records instead of one record (i.e. one record for the interface and one record for the body). No records for the individual package routines will be inserted into {{mysql.proc}}.

This will need a change in the table:

{code:sql}
ALTER TABLE mysql.proc MODIFY type ENUM('FUNCTION','PROCEDURE','PACKAGE','PACKAGE BODY');
{code}

Data flow:
- {{CREATE PACKAGE pkg1}} inserts a record with type 'PACKAGE' and name 'pkg1'
- {{CREATE PACKAGE BODY pkg1}} inserts a record with type 'PACKAGE BODY' and name 'pkg1'
- {{ALTER PACKAGE BODY pkg1}} updates the record with type 'PACKAGE BODY' and name 'pkg1'
- {{DROP PACKAGE BODY pkg1}} deletes the record with type 'PACKAGE BODY' and name 'pkg1'
- {{DROP PACKAGE pkg1}} deletes the records with name 'pkg1' and types 'PACKAGE' and 'PACKAGE BODY'

Open questions:
- Some columns, e.g. {{is_deterministic}}, {{param_list}} are not applicable to packages. These columns can be set to {{NULL}} for the package related records. However, this brings some data redundancy.
- We'll have to keep maintaining the {{body_utf8}}, {{character_set_client}}, {{collation_connection}} columns for packages. This makes it harder to get rid of theme eventually (to use the column {{body}} for both {{SHOW}} and {{I_S}} purposes).

h2. Conclusions
The solutions {{1.b}} and {{2.b}} look the most promising.

{{2017-01-20 UPDATE}}: during a discussion with Monty and Bar, it was decided that we'd go {{1.b}}. This will help to avoid storing two definitions:
- binary definition for parsing purposes
- utf8 definition for {{INFORMATION_SCHEMA}} purposes

TODO:
- think about MDL to avoid concurrent {{CREATE PACKAGE pkg1}}, {{DROP PACKAGE pkg1}}, {{CALL pkg1.proc1()}}.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,61,,3,1,23,5,0,42,0,,0,850,1,0,0,2016-08-18 06:07:13,Oracle-style packages,"Add support for Oracle-style stored packages, which are collections of related SP routines and other program objects stored together in the database.

New statements:
- {{CREATE PACKAGE}}
- {{CREATE PACKAGE BODY}}
- {{ALTER PACKAGE}}
- {{DROP PACKAGE}}
",,0,42,0,2574,60.7381,"Oracle-style packages $end$ Add support for Oracle-style stored packages, which are collections of related SP routines and other program objects stored together in the database.

New statements:
- {{CREATE PACKAGE}}
- {{CREATE PACKAGE BODY}}
- {{ALTER PACKAGE}}
- {{DROP PACKAGE}}
 $acceptance criteria:$",42,1,1,1,1,1,1,0.0,20,13,0.65,12,0.6,12,0.6,11,0.55,11,0.55
552,MDEV-10596,Technical task,MDEV,2016-08-19 07:47:49,,0,sql_mode=ORACLE: Allow VARCHAR and VARCHAR2 without length as a data type of routine parameters and in RETURN clause,"In Oracle, {{VARCHAR}}, {{VARCHAR2}}, {{RAW}} can be specified without length in parameters and {{RETURN}} clause.

Note, variable declarations still require length.

Example:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a VARCHAR2) RETURN VARCHAR2
AS
  b VARCHAR2(20):= a;
BEGIN
  RETURN b;
END;
/
SELECT f1('test') FROM DUAL;
{code}

Example:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a RAW) RETURN RAW
AS
  b RAW(20):= a;
BEGIN
  RETURN b;
END;
/
SELECT f1('616263') FROM DUAL;
{code}

In case of {{CHAR}} data type, when used in a parameter or a return value, a data type without parentheses means maximum size, while in a variable definition, {{CHAR}} with no parentheses still means {{CHAR(1)}}.
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a CHAR) RETURN CHAR
AS
  b CHAR(10):= a;
BEGIN
  RETURN b;
END;
/
{code}

In Oracle, {{CHAR}} and {{RAW}} data types have a limit of {{2000}} bytes. In MariaDB, fixed length types are limited to 255 characters. To guarantee that all possible values can fit, we'll translate CHAR without length in SP parameters or return values to {{VARCHAR(2000)}}.

In Oracle, {{VARCHAR}} is limited to {{4000}} bytes. We'll translate a {{VARCHAR}} with no length (in SP parameters or return values) to {{VARCHAR(4000)}} as well.

The full translation list:
- CHAR -> VARCHAR(2000)
- NCHAR -> NVARCHAR(2000)
- RAW -> VARBINARY(2000)
- VARCHAR -> VARCHAR(4000)
- NVARCHAR -> NVARCHAR(4000)
",,"sql_mode=ORACLE: Allow VARCHAR and VARCHAR2 without length as a data type of routine parameters and in RETURN clause $end$ In Oracle, {{VARCHAR}}, {{VARCHAR2}}, {{RAW}} can be specified without length in parameters and {{RETURN}} clause.

Note, variable declarations still require length.

Example:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a VARCHAR2) RETURN VARCHAR2
AS
  b VARCHAR2(20):= a;
BEGIN
  RETURN b;
END;
/
SELECT f1('test') FROM DUAL;
{code}

Example:
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a RAW) RETURN RAW
AS
  b RAW(20):= a;
BEGIN
  RETURN b;
END;
/
SELECT f1('616263') FROM DUAL;
{code}

In case of {{CHAR}} data type, when used in a parameter or a return value, a data type without parentheses means maximum size, while in a variable definition, {{CHAR}} with no parentheses still means {{CHAR(1)}}.
{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a CHAR) RETURN CHAR
AS
  b CHAR(10):= a;
BEGIN
  RETURN b;
END;
/
{code}

In Oracle, {{CHAR}} and {{RAW}} data types have a limit of {{2000}} bytes. In MariaDB, fixed length types are limited to 255 characters. To guarantee that all possible values can fit, we'll translate CHAR without length in SP parameters or return values to {{VARCHAR(2000)}}.

In Oracle, {{VARCHAR}} is limited to {{4000}} bytes. We'll translate a {{VARCHAR}} with no length (in SP parameters or return values) to {{VARCHAR(4000)}} as well.

The full translation list:
- CHAR -> VARCHAR(2000)
- NCHAR -> NVARCHAR(2000)
- RAW -> VARBINARY(2000)
- VARCHAR -> VARCHAR(4000)
- NVARCHAR -> NVARCHAR(4000)
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,22,,0,0,0,5,0,8,0,,0,850,0,0,0,2016-08-19 07:47:49,Allow VARCHAR and VARCHAR2 without length as a data type of routine parameters and in RETURN clause,"In Oracle, {{VARCHAR}} and {{VARCHAR2}} can be specified without length in parameters and {{RETURN}} clause.

Note, variable declarations still require length.

{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a VARCHAR2) RETURN VARCHAR2
AS
  b VARCHAR2(20):= a;
BEGIN
  RETURN b;
END;
/
SELECT f1('test') FROM DUAL;
{code}
",,1,7,0,181,2.73846,"Allow VARCHAR and VARCHAR2 without length as a data type of routine parameters and in RETURN clause $end$ In Oracle, {{VARCHAR}} and {{VARCHAR2}} can be specified without length in parameters and {{RETURN}} clause.

Note, variable declarations still require length.

{code:sql}
DROP FUNCTION f1;
CREATE FUNCTION f1(a VARCHAR2) RETURN VARCHAR2
AS
  b VARCHAR2(20):= a;
BEGIN
  RETURN b;
END;
/
SELECT f1('test') FROM DUAL;
{code}
 $acceptance criteria:$",8,1,1,1,1,1,1,0.0,21,14,0.666667,13,0.619048,13,0.619048,12,0.571429,12,0.571429
553,MDEV-10597,Technical task,MDEV,2016-08-19 08:37:19,,0,sql_mode=ORACLE: Cursors with parameters,"Add optional parameter list into {{CURSOR}} declaration and {{OPEN}}.

Example:
{code:sq}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
INSERT INTO t1 VALUES (1,2,3);
DECLARE
  CURSOR c (prm_a VARCHAR2, prm_b VARCHAR2) IS
    SELECT a, b, c FROM t1 WHERE a=prm_a AND b=prm_b;
  v_a INT;
  v_b INT;
  v_c INT;
BEGIN
  OPEN c(1, 2);
  FETCH c INTO v_a, v_b, v_c;
  CLOSE c;
  INSERT INTO t1 VALUES (v_a + 10, v_b + 10, v_c + 10);
EXCEPTION
  WHEN NO_DATA_FOUND THEN RETURN;
END;
/
SELECT * FROM t1;
DROP TABLE t1;
{code}
{noformat}
	 A	    B	       C
---------- ---------- ----------
	 1	    2	       3
	11	   12	      13
{noformat}

h2. Cursor parameter name space

Cursor parameters exists in a separate name space and shadow the variables declared on the same level with the cursor, or on upper levels. This script:
{code:sql}
SET SERVEROUTPUT ON ;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (1);
CREATE PROCEDURE p1(a INT)
AS
  v_a INT:=NULL;
  p_a INT:=NULL;
  CURSOR c (p_a VARCHAR2) IS SELECT a FROM t1 WHERE p_a IS NOT NULL;
BEGIN
  OPEN c(a);
  FETCH c INTO v_a;
  IF c%NOTFOUND THEN
  BEGIN
    DBMS_OUTPUT.PUT_LINE('No records found');
    RETURN;
  END;
  END IF;
  CLOSE c;
  DBMS_OUTPUT.PUT_LINE('Fetched a record a='||TO_CHAR(v_a));
  INSERT INTO t1 VALUES (v_a);
  COMMIT;
END;
/
CALL p1(1);
SELECT * FROM t1;
{code}
returns
{noformat}
SQL> Fetched a record a=1
{noformat}
{noformat}
SQL> 
	 A
----------
	 1
	 1
{noformat}
Notice, the procedure call inserted the second record into the table {{t1}}, because the cursor parameter {{p_a}} shadowed the local variable {{p_a}} and the condition {{WHERE p_a IS NOT NULL}} evaluated to {{TRUE}}, because the cursor parameter {{p_a}} was initialized to {{1}} at {{OPEN c(a)}} time. The value of the local variable {{p_a}} is not important, as it's not visible in the cursor {{SELECT}} expression.

",,"sql_mode=ORACLE: Cursors with parameters $end$ Add optional parameter list into {{CURSOR}} declaration and {{OPEN}}.

Example:
{code:sq}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
INSERT INTO t1 VALUES (1,2,3);
DECLARE
  CURSOR c (prm_a VARCHAR2, prm_b VARCHAR2) IS
    SELECT a, b, c FROM t1 WHERE a=prm_a AND b=prm_b;
  v_a INT;
  v_b INT;
  v_c INT;
BEGIN
  OPEN c(1, 2);
  FETCH c INTO v_a, v_b, v_c;
  CLOSE c;
  INSERT INTO t1 VALUES (v_a + 10, v_b + 10, v_c + 10);
EXCEPTION
  WHEN NO_DATA_FOUND THEN RETURN;
END;
/
SELECT * FROM t1;
DROP TABLE t1;
{code}
{noformat}
	 A	    B	       C
---------- ---------- ----------
	 1	    2	       3
	11	   12	      13
{noformat}

h2. Cursor parameter name space

Cursor parameters exists in a separate name space and shadow the variables declared on the same level with the cursor, or on upper levels. This script:
{code:sql}
SET SERVEROUTPUT ON ;
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (1);
CREATE PROCEDURE p1(a INT)
AS
  v_a INT:=NULL;
  p_a INT:=NULL;
  CURSOR c (p_a VARCHAR2) IS SELECT a FROM t1 WHERE p_a IS NOT NULL;
BEGIN
  OPEN c(a);
  FETCH c INTO v_a;
  IF c%NOTFOUND THEN
  BEGIN
    DBMS_OUTPUT.PUT_LINE('No records found');
    RETURN;
  END;
  END IF;
  CLOSE c;
  DBMS_OUTPUT.PUT_LINE('Fetched a record a='||TO_CHAR(v_a));
  INSERT INTO t1 VALUES (v_a);
  COMMIT;
END;
/
CALL p1(1);
SELECT * FROM t1;
{code}
returns
{noformat}
SQL> Fetched a record a=1
{noformat}
{noformat}
SQL> 
	 A
----------
	 1
	 1
{noformat}
Notice, the procedure call inserted the second record into the table {{t1}}, because the cursor parameter {{p_a}} shadowed the local variable {{p_a}} and the condition {{WHERE p_a IS NOT NULL}} evaluated to {{TRUE}}, because the cursor parameter {{p_a}} was initialized to {{1}} at {{OPEN c(a)}} time. The value of the local variable {{p_a}} is not important, as it's not visible in the cursor {{SELECT}} expression.

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,20,,1,0,3,5,0,4,0,,0,850,0,0,0,2016-08-19 08:37:19,Cursors with parameters,"Add optional parameter list into {{CURSOR}} declaration and {{OPEN}}.

Example:
{code:sq}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
INSERT INTO t1 VALUES (1,2,3);
DECLARE
  CURSOR c (prm_a VARCHAR2, prm_b VARCHAR2) IS
    SELECT a, b, c FROM t1 WHERE a=prm_a AND b=prm_b;
  v_a INT;
  v_b INT;
  v_c INT;
BEGIN
  OPEN c(1, 2);
  FETCH c INTO v_a, v_b, v_c;
  CLOSE c;
  INSERT INTO t1 VALUES (v_a + 10, v_b + 10, v_c + 10);
EXCEPTION
  WHEN NO_DATA_FOUND THEN RETURN;
END;
/
SELECT * FROM t1;
DROP TABLE t1;
{code}
{noformat}
	 A	    B	       C
---------- ---------- ----------
	 1	    2	       3
	11	   12	      13
{noformat}
",,1,3,0,195,1.74107,"Cursors with parameters $end$ Add optional parameter list into {{CURSOR}} declaration and {{OPEN}}.

Example:
{code:sq}
DROP TABLE t1;
CREATE TABLE t1 (a INT, b INT, c INT);
INSERT INTO t1 VALUES (1,2,3);
DECLARE
  CURSOR c (prm_a VARCHAR2, prm_b VARCHAR2) IS
    SELECT a, b, c FROM t1 WHERE a=prm_a AND b=prm_b;
  v_a INT;
  v_b INT;
  v_c INT;
BEGIN
  OPEN c(1, 2);
  FETCH c INTO v_a, v_b, v_c;
  CLOSE c;
  INSERT INTO t1 VALUES (v_a + 10, v_b + 10, v_c + 10);
EXCEPTION
  WHEN NO_DATA_FOUND THEN RETURN;
END;
/
SELECT * FROM t1;
DROP TABLE t1;
{code}
{noformat}
	 A	    B	       C
---------- ---------- ----------
	 1	    2	       3
	11	   12	      13
{noformat}
 $acceptance criteria:$",4,1,1,1,1,1,1,0.0,22,15,0.681818,14,0.636364,14,0.636364,13,0.590909,13,0.590909
554,MDEV-10598,Technical task,MDEV,2016-08-19 09:08:32,,0,sql_mode=ORACLE: Variable declarations can go after cursor declarations,"In MariaDB variable declarations cannot go after cursor declarations.

In Oracle there is no such restriction:
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (1);
CREATE FUNCTION f1 RETURN INT
AS
  CURSOR c IS SELECT a FROM t1;
  v_a INT;
BEGIN
  OPEN c;
  FETCH c INTO v_a;
  CLOSE c;
  RETURN v_a;
EXCEPTION
  WHEN OTHERS THEN RETURN -1;
END;
/
SELECT f1() FROM DUAL;
{code}
{noformat}
      F1()
----------
	 1
{noformat}
",,"sql_mode=ORACLE: Variable declarations can go after cursor declarations $end$ In MariaDB variable declarations cannot go after cursor declarations.

In Oracle there is no such restriction:
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (1);
CREATE FUNCTION f1 RETURN INT
AS
  CURSOR c IS SELECT a FROM t1;
  v_a INT;
BEGIN
  OPEN c;
  FETCH c INTO v_a;
  CLOSE c;
  RETURN v_a;
EXCEPTION
  WHEN OTHERS THEN RETURN -1;
END;
/
SELECT f1() FROM DUAL;
{code}
{noformat}
      F1()
----------
	 1
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,1,1,1,5,0,1,0,,0,850,1,0,0,2016-08-19 09:08:32,Variable declarations can go after cursor declarations,"In MariaDB variable declarations cannot go after cursor declarations.

In Oracle there is no such restriction:
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (1);
CREATE FUNCTION f1 RETURN INT
AS
  CURSOR c IS SELECT a FROM t1;
  v_a INT;
BEGIN
  OPEN c;
  FETCH c INTO v_a;
  CLOSE c;
  RETURN v_a;
EXCEPTION
  WHEN OTHERS THEN RETURN -1;
END;
/
SELECT f1() FROM DUAL;
{code}
{noformat}
      F1()
----------
	 1
{noformat}
",,1,0,0,1,0.0114943,"Variable declarations can go after cursor declarations $end$ In MariaDB variable declarations cannot go after cursor declarations.

In Oracle there is no such restriction:
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (1);
CREATE FUNCTION f1 RETURN INT
AS
  CURSOR c IS SELECT a FROM t1;
  v_a INT;
BEGIN
  OPEN c;
  FETCH c INTO v_a;
  CLOSE c;
  RETURN v_a;
EXCEPTION
  WHEN OTHERS THEN RETURN -1;
END;
/
SELECT f1() FROM DUAL;
{code}
{noformat}
      F1()
----------
	 1
{noformat}
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,23,16,0.695652,15,0.652174,15,0.652174,14,0.608696,14,0.608696
555,MDEV-10655,Technical task,MDEV,2016-08-24 11:35:05,,0,sql_mode=ORACLE: Anonymous blocks,"MariaDB uses {{BEGIN NOT ATOMIC}} to start anonymous blocks that can be used directly, outside of an SP context:
MariaDB:
{code:sql}
delimiter ;;
BEGIN NOT ATOMIC
 ...
END;;
{code}

When running with {{sql_mode=ORACLE}}, MariaDB will understand this syntax for anonymous blocks.
{code:sql}
DECLARE
  ...
BEGIN
  ...
END;;
{code}

In ORACLE mode, {{BEGIN}} is same as {{BEGIN NOT ATOMIC}}

{code:sql}
BEGIN
 ...
END;;
{code}

This change will disallow {{BEGIN}} and {{BEGIN WORK}} as a transaction start for {{sql_mode=ORACLE}}. But one can still use {{START TRANSACTION}} if needed.
",,"sql_mode=ORACLE: Anonymous blocks $end$ MariaDB uses {{BEGIN NOT ATOMIC}} to start anonymous blocks that can be used directly, outside of an SP context:
MariaDB:
{code:sql}
delimiter ;;
BEGIN NOT ATOMIC
 ...
END;;
{code}

When running with {{sql_mode=ORACLE}}, MariaDB will understand this syntax for anonymous blocks.
{code:sql}
DECLARE
  ...
BEGIN
  ...
END;;
{code}

In ORACLE mode, {{BEGIN}} is same as {{BEGIN NOT ATOMIC}}

{code:sql}
BEGIN
 ...
END;;
{code}

This change will disallow {{BEGIN}} and {{BEGIN WORK}} as a transaction start for {{sql_mode=ORACLE}}. But one can still use {{START TRANSACTION}} if needed.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,19,,0,3,0,5,0,5,0,,0,850,3,0,0,2016-08-24 11:35:05,Anonymous blocks,"MariaDB uses {{BEGIN NOT ATOMIC}} to start anonymous blocks that can be used directly, outside of an SP context:
MariaDB:
{code:sql}
BEGIN NOT ATOMIC;
 ...
END;
{code}

When running with {{sql_mode=ORACLE}}, MariaDB will understand this syntax for anonymous blocks.
{code:sql}
DECLARE
  ...
BEGIN
  ...
END;
{code}
{code:sql}
BEGIN
 ...
END;
{code}
",,1,4,0,44,0.714286,"Anonymous blocks $end$ MariaDB uses {{BEGIN NOT ATOMIC}} to start anonymous blocks that can be used directly, outside of an SP context:
MariaDB:
{code:sql}
BEGIN NOT ATOMIC;
 ...
END;
{code}

When running with {{sql_mode=ORACLE}}, MariaDB will understand this syntax for anonymous blocks.
{code:sql}
DECLARE
  ...
BEGIN
  ...
END;
{code}
{code:sql}
BEGIN
 ...
END;
{code}
 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,24,17,0.708333,15,0.625,15,0.625,14,0.583333,14,0.583333
556,MDEV-10664,Task,MDEV,2016-08-25 14:43:09,,0,Add statuses about optimistic parallel replication stalls.,"Hi,

from the documentation [1], I can read:

{quote}
Non-transactional DML and DDL is not safe to optimistically apply in parallel, as it cannot be rolled back in case of conflicts. Thus, in optimistic mode, non-transactional (such as MyISAM) updates are not applied in parallel with earlier events (it is however possible to apply a MyISAM update in parallel with a later InnoDB update). DDL statements are not applied in parallel with any other transactions, earlier or later.
{quote}

[1]: https://mariadb.com/kb/en/mariadb/parallel-replication/

It would be very helpful to have slave-side counters (statuses) for both of those non-transnational updates and DDL statements.  This would allow to see if optimistic parallel replication is slowed-down by non-transnational updates and DDL statements.

Many thanks,

JFG",,"Add statuses about optimistic parallel replication stalls. $end$ Hi,

from the documentation [1], I can read:

{quote}
Non-transactional DML and DDL is not safe to optimistically apply in parallel, as it cannot be rolled back in case of conflicts. Thus, in optimistic mode, non-transactional (such as MyISAM) updates are not applied in parallel with earlier events (it is however possible to apply a MyISAM update in parallel with a later InnoDB update). DDL statements are not applied in parallel with any other transactions, earlier or later.
{quote}

[1]: https://mariadb.com/kb/en/mariadb/parallel-replication/

It would be very helpful to have slave-side counters (statuses) for both of those non-transnational updates and DDL statements.  This would allow to see if optimistic parallel replication is slowed-down by non-transnational updates and DDL statements.

Many thanks,

JFG $acceptance criteria:$",,Jean-François Gagné,Jean-François Gagné,Blocker,28,,0,6,0,4,0,0,0,,0,850,5,0,0,2017-01-11 08:27:10,Add statuses about optimistic parallel replication stalls.,"Hi,

from the documentation [1], I can read:

{quote}
Non-transactional DML and DDL is not safe to optimistically apply in parallel, as it cannot be rolled back in case of conflicts. Thus, in optimistic mode, non-transactional (such as MyISAM) updates are not applied in parallel with earlier events (it is however possible to apply a MyISAM update in parallel with a later InnoDB update). DDL statements are not applied in parallel with any other transactions, earlier or later.
{quote}

[1]: https://mariadb.com/kb/en/mariadb/parallel-replication/

It would be very helpful to have slave-side counters (statuses) for both of those non-transnational updates and DDL statements.  This would allow to see if optimistic parallel replication is slowed-down by non-transnational updates and DDL statements.

Many thanks,

JFG",,0,0,0,0,0.0,"Add statuses about optimistic parallel replication stalls. $end$ Hi,

from the documentation [1], I can read:

{quote}
Non-transactional DML and DDL is not safe to optimistically apply in parallel, as it cannot be rolled back in case of conflicts. Thus, in optimistic mode, non-transactional (such as MyISAM) updates are not applied in parallel with earlier events (it is however possible to apply a MyISAM update in parallel with a later InnoDB update). DDL statements are not applied in parallel with any other transactions, earlier or later.
{quote}

[1]: https://mariadb.com/kb/en/mariadb/parallel-replication/

It would be very helpful to have slave-side counters (statuses) for both of those non-transnational updates and DDL statements.  This would allow to see if optimistic parallel replication is slowed-down by non-transnational updates and DDL statements.

Many thanks,

JFG $acceptance criteria:$",0,0,0,0,0,0,1,3329.73,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
557,MDEV-10697,Technical task,MDEV,2016-08-29 05:21:38,,0,sql_mode=ORACLE: GOTO statement,"Add support for the {{GOTO}} statement in stored procedures, for Oracle compatibility.

Syntax:
{code:sql}
GOTO label_name;
{code}

Note, Oracle's implementation has a number of restrictions.
A {{GOTO}} statement cannot transfer control:

- into an {{IF}} statement, {{CASE}} statement, {{LOOP}} statement, or sub-block.
- from one {{IF}} statement clause to another, or from one {{CASE}} statement {{WHEN}} clause to another.
- out of a subprogram.
- into an exception handler.
- from an exception handler back into the current block (but it can transfer control from an exception handler into an enclosing block).


Under terms of this task will also labels to be used with non-block and non-loop statements:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
BEGIN
<<label>>
  SELECT 1;
END;
$$
DELIMITER ;
{code}
Currently the above definition returns a syntax error:
{noformat}
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'SELECT 1;
{noformat}
Labels with non-block/loop statements are needed, for example, to leave a loop which is inside a loop, which is inside a loop.
",,"sql_mode=ORACLE: GOTO statement $end$ Add support for the {{GOTO}} statement in stored procedures, for Oracle compatibility.

Syntax:
{code:sql}
GOTO label_name;
{code}

Note, Oracle's implementation has a number of restrictions.
A {{GOTO}} statement cannot transfer control:

- into an {{IF}} statement, {{CASE}} statement, {{LOOP}} statement, or sub-block.
- from one {{IF}} statement clause to another, or from one {{CASE}} statement {{WHEN}} clause to another.
- out of a subprogram.
- into an exception handler.
- from an exception handler back into the current block (but it can transfer control from an exception handler into an enclosing block).


Under terms of this task will also labels to be used with non-block and non-loop statements:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
BEGIN
<<label>>
  SELECT 1;
END;
$$
DELIMITER ;
{code}
Currently the above definition returns a syntax error:
{noformat}
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'SELECT 1;
{noformat}
Labels with non-block/loop statements are needed, for example, to leave a loop which is inside a loop, which is inside a loop.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,16,,0,0,0,5,0,5,0,,0,850,0,0,0,2016-08-29 05:21:38,GOTO statement,"Add support for the {{GOTO}} statement in stored procedures, for Oracle compatibility.

Syntax:
{code:sql}
GOTO label_name;
{code}

Note, Oracle's implementation has a number of restrictions.
A {{GOTO}} statement cannot transfer control:

- into an {{IF}} statement, {{CASE}} statement, {{LOOP}} statement, or sub-block.
- from one {{IF}} statement clause to another, or from one {{CASE}} statement {{WHEN}} clause to another.
- out of a subprogram.
- into an exception handler.
- from an exception handler back into the current block (but it can transfer control from an exception handler into an enclosing block).
",,1,4,0,100,1.03093,"GOTO statement $end$ Add support for the {{GOTO}} statement in stored procedures, for Oracle compatibility.

Syntax:
{code:sql}
GOTO label_name;
{code}

Note, Oracle's implementation has a number of restrictions.
A {{GOTO}} statement cannot transfer control:

- into an {{IF}} statement, {{CASE}} statement, {{LOOP}} statement, or sub-block.
- from one {{IF}} statement clause to another, or from one {{CASE}} statement {{WHEN}} clause to another.
- out of a subprogram.
- into an exception handler.
- from an exception handler back into the current block (but it can transfer control from an exception handler into an enclosing block).
 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,25,18,0.72,16,0.64,16,0.64,15,0.6,15,0.6
558,MDEV-10709,Technical task,MDEV,2016-08-31 10:02:05,,0,Expressions as parameters to Dynamic SQL,"MariaDB allows to use only user variables in {{EXECUTE..USING}}:
{code:sql}
EXECUTE stmt USING @a;
{code}

Under terms of this task, we'll allow passing expressions as parameters to Dynamic SQL:
{code:sql}
PREPARE stmt FROM 'SELECT ? FROM t1';
EXECUTE stmt USING 1+2;
{code}

Note, these expression types should work as output parameters (in addition to user variables):
- SP variables
- Trigger NEW and OLD fields

Note, stored functions and subselects as parameters will not be supported under terms of this task. Using stored functions and subselects would require some additional changes in table locking, SP cache and transaction handling (for the same reason, {{SET STATEMENT}} disallows stored functions and subselects as variable values). So the following scripts will return errors:
{code:sql}
PREPARE stmt FROM 'SELECT ? FROM DUAL';
EXECUTE stmt USING (SELECT 1);
{code}
{code:sql}
CREATE FUNCTION f1() RETURNS VARCHAR(10) RETURN 'test';
PREPARE stmt FROM 'SELECT ? FROM DUAL';
EXECUTE stmt USING f1();
{code}
Support for stored functions and subselects as parameters (as well as in {{SET STATEMENT}} variable values) will be added under terms of a separate task.
",,"Expressions as parameters to Dynamic SQL $end$ MariaDB allows to use only user variables in {{EXECUTE..USING}}:
{code:sql}
EXECUTE stmt USING @a;
{code}

Under terms of this task, we'll allow passing expressions as parameters to Dynamic SQL:
{code:sql}
PREPARE stmt FROM 'SELECT ? FROM t1';
EXECUTE stmt USING 1+2;
{code}

Note, these expression types should work as output parameters (in addition to user variables):
- SP variables
- Trigger NEW and OLD fields

Note, stored functions and subselects as parameters will not be supported under terms of this task. Using stored functions and subselects would require some additional changes in table locking, SP cache and transaction handling (for the same reason, {{SET STATEMENT}} disallows stored functions and subselects as variable values). So the following scripts will return errors:
{code:sql}
PREPARE stmt FROM 'SELECT ? FROM DUAL';
EXECUTE stmt USING (SELECT 1);
{code}
{code:sql}
CREATE FUNCTION f1() RETURNS VARCHAR(10) RETURN 'test';
PREPARE stmt FROM 'SELECT ? FROM DUAL';
EXECUTE stmt USING f1();
{code}
Support for stored functions and subselects as parameters (as well as in {{SET STATEMENT}} variable values) will be added under terms of a separate task.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,22,,3,0,8,5,0,5,0,,0,850,0,0,0,2016-08-31 10:02:05,Expressions as parameters to Dynamic SQL,"MariaDB allows to use only user variables in {{EXECUTE..USING}}:
{code:sql}
EXECUTE stmt USING @a;
{code}

Under terms of this task, we'll allow passing expressions as parameters to Dynamic SQL:
{code:sql}
PREPARE stmt FROM 'SELECT ? FROM t1';
EXECUTE stmt USING 1+2;
{code}

Note, these expression types should work as output parameters (in addition to user variables):
- SP variables
- Trigger NEW and OLD fields
",,0,5,0,114,1.54054,"Expressions as parameters to Dynamic SQL $end$ MariaDB allows to use only user variables in {{EXECUTE..USING}}:
{code:sql}
EXECUTE stmt USING @a;
{code}

Under terms of this task, we'll allow passing expressions as parameters to Dynamic SQL:
{code:sql}
PREPARE stmt FROM 'SELECT ? FROM t1';
EXECUTE stmt USING 1+2;
{code}

Note, these expression types should work as output parameters (in addition to user variables):
- SP variables
- Trigger NEW and OLD fields
 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,26,19,0.730769,17,0.653846,17,0.653846,16,0.615385,16,0.615385
559,MDEV-10742,Technical task,MDEV,2016-09-05 06:02:30,,0,LDML: make conf_to_src reuse common data between collations,"{{conf_to_src}} duplicates the {{ctype}}, {{to_lower}}, {{to_upper}} and {{tab_to_uni}} arrays when dumping data from the XML files in {{/sql/share/charsets/}} to {{ctype-extra.c}}

For example, the {{ctype}} map from {{cp1251.xml}} is dumped five times:

{code:c}
static const uchar ctype_cp1251_bulgarian_ci[] = ..
static const uchar ctype_cp1251_ukrainian_ci[] = ..
static const uchar ctype_cp1251_bin[] = ..
static const uchar ctype_cp1251_general_ci[] = ..
static const uchar ctype_cp1251_general_cs[] = ..
{code}

After adding MDEV-9711 the number of duplicate data will grow even further two times.

We'll change {{conf_to_src.c}} to detect duplicate arrays. Non-primary collations will reuse arrays from the primary collation if the data is the same.
",,"LDML: make conf_to_src reuse common data between collations $end$ {{conf_to_src}} duplicates the {{ctype}}, {{to_lower}}, {{to_upper}} and {{tab_to_uni}} arrays when dumping data from the XML files in {{/sql/share/charsets/}} to {{ctype-extra.c}}

For example, the {{ctype}} map from {{cp1251.xml}} is dumped five times:

{code:c}
static const uchar ctype_cp1251_bulgarian_ci[] = ..
static const uchar ctype_cp1251_ukrainian_ci[] = ..
static const uchar ctype_cp1251_bin[] = ..
static const uchar ctype_cp1251_general_ci[] = ..
static const uchar ctype_cp1251_general_cs[] = ..
{code}

After adding MDEV-9711 the number of duplicate data will grow even further two times.

We'll change {{conf_to_src.c}} to detect duplicate arrays. Non-primary collations will reuse arrays from the primary collation if the data is the same.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,5,,0,0,1,2,0,0,0,,0,850,0,0,0,2016-09-05 06:02:30,LDML: make conf_to_src reuse common data between collations,"{{conf_to_src}} duplicates the {{ctype}}, {{to_lower}}, {{to_upper}} and {{tab_to_uni}} arrays when dumping data from the XML files in {{/sql/share/charsets/}} to {{ctype-extra.c}}

For example, the {{ctype}} map from {{cp1251.xml}} is dumped five times:

{code:c}
static const uchar ctype_cp1251_bulgarian_ci[] = ..
static const uchar ctype_cp1251_ukrainian_ci[] = ..
static const uchar ctype_cp1251_bin[] = ..
static const uchar ctype_cp1251_general_ci[] = ..
static const uchar ctype_cp1251_general_cs[] = ..
{code}

After adding MDEV-9711 the number of duplicate data will grow even further two times.

We'll change {{conf_to_src.c}} to detect duplicate arrays. Non-primary collations will reuse arrays from the primary collation if the data is the same.
",,0,0,0,0,0.0,"LDML: make conf_to_src reuse common data between collations $end$ {{conf_to_src}} duplicates the {{ctype}}, {{to_lower}}, {{to_upper}} and {{tab_to_uni}} arrays when dumping data from the XML files in {{/sql/share/charsets/}} to {{ctype-extra.c}}

For example, the {{ctype}} map from {{cp1251.xml}} is dumped five times:

{code:c}
static const uchar ctype_cp1251_bulgarian_ci[] = ..
static const uchar ctype_cp1251_ukrainian_ci[] = ..
static const uchar ctype_cp1251_bin[] = ..
static const uchar ctype_cp1251_general_ci[] = ..
static const uchar ctype_cp1251_general_cs[] = ..
{code}

After adding MDEV-9711 the number of duplicate data will grow even further two times.

We'll change {{conf_to_src.c}} to detect duplicate arrays. Non-primary collations will reuse arrays from the primary collation if the data is the same.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,27,20,0.740741,18,0.666667,18,0.666667,17,0.62963,17,0.62963
560,MDEV-10743,Technical task,MDEV,2016-09-05 09:58:42,,0,LDML: a new syntax to reuse sort order from another 8bit simple collation,"We'll add a new XML syntax to reuse sort order from another collation:

{code:xml}
<collation name=""latin1_test_ci"" id=""300"">
  <rules>
    <import source=""latin1_swedish_ci""/>
  </rules>
</collation>
{code}

This task is about 8bit simple collations only.

The same syntax will however later be reused by other collation types (e.g. non-8bit or non-simple)

References:
http://cldr.unicode.org/development/development-process/design-proposals/collation-additions#TOC-Collation-Import
",,"LDML: a new syntax to reuse sort order from another 8bit simple collation $end$ We'll add a new XML syntax to reuse sort order from another collation:

{code:xml}
<collation name=""latin1_test_ci"" id=""300"">
  <rules>
    <import source=""latin1_swedish_ci""/>
  </rules>
</collation>
{code}

This task is about 8bit simple collations only.

The same syntax will however later be reused by other collation types (e.g. non-8bit or non-simple)

References:
http://cldr.unicode.org/development/development-process/design-proposals/collation-additions#TOC-Collation-Import
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,0,0,1,2,0,0,0,,0,850,0,0,0,2016-09-05 09:58:42,LDML: a new syntax to reuse sort order from another 8bit simple collation,"We'll add a new XML syntax to reuse sort order from another collation:

{code:xml}
<collation name=""latin1_test_ci"" id=""300"">
  <rules>
    <import source=""latin1_swedish_ci""/>
  </rules>
</collation>
{code}

This task is about 8bit simple collations only.

The same syntax will however later be reused by other collation types (e.g. non-8bit or non-simple)

References:
http://cldr.unicode.org/development/development-process/design-proposals/collation-additions#TOC-Collation-Import
",,0,0,0,0,0.0,"LDML: a new syntax to reuse sort order from another 8bit simple collation $end$ We'll add a new XML syntax to reuse sort order from another collation:

{code:xml}
<collation name=""latin1_test_ci"" id=""300"">
  <rules>
    <import source=""latin1_swedish_ci""/>
  </rules>
</collation>
{code}

This task is about 8bit simple collations only.

The same syntax will however later be reused by other collation types (e.g. non-8bit or non-simple)

References:
http://cldr.unicode.org/development/development-process/design-proposals/collation-additions#TOC-Collation-Import
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,28,20,0.714286,18,0.642857,18,0.642857,17,0.607143,17,0.607143
561,MDEV-10772,Technical task,MDEV,2016-09-08 11:42:05,,0,Introduce Item_param::CONVERSION_INFO,"We'll join character set related members in Item_param into a structure like this:

{code:cpp}
struct CONVERSION_INFO
{
  CHARSET_INFO *character_set_client;
  CHARSET_INFO *character_set_of_placeholder;
  CHARSET_INFO *final_character_set_of_str_value;
} cs_info;
{code}

This is needed for easier further work, e,g, MDEV-10709
",,"Introduce Item_param::CONVERSION_INFO $end$ We'll join character set related members in Item_param into a structure like this:

{code:cpp}
struct CONVERSION_INFO
{
  CHARSET_INFO *character_set_client;
  CHARSET_INFO *character_set_of_placeholder;
  CHARSET_INFO *final_character_set_of_str_value;
} cs_info;
{code}

This is needed for easier further work, e,g, MDEV-10709
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,15,,0,0,1,5,0,1,0,,0,850,0,0,0,2016-09-08 11:42:05,Introduce Item_param::CONVERSION_INFO,"We'll join character set related members if Item_param into a structure like this:

{code:cpp}
struct CONVERSION_INFO
{
  CHARSET_INFO *character_set_client;
  CHARSET_INFO *character_set_of_placeholder;
  CHARSET_INFO *final_character_set_of_str_value;
} cs_info;
{code}

This is needed for easier further work, e,g, MDEV-10709
",,0,1,0,2,0.025,"Introduce Item_param::CONVERSION_INFO $end$ We'll join character set related members if Item_param into a structure like this:

{code:cpp}
struct CONVERSION_INFO
{
  CHARSET_INFO *character_set_client;
  CHARSET_INFO *character_set_of_placeholder;
  CHARSET_INFO *final_character_set_of_str_value;
} cs_info;
{code}

This is needed for easier further work, e,g, MDEV-10709
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,29,20,0.689655,18,0.62069,18,0.62069,17,0.586207,17,0.586207
562,MDEV-10801,Technical task,MDEV,2016-09-13 06:26:06,,0,sql_mode=ORACLE: Dynamic SQL placeholders,"When running with {{sql_mode=ORACLE}}, the parser should understand Oracle style placeholders in {{EXECUTE IMMEDIATE}}  and in {{PREPARE}}.

Placeholders are designated as a colon followed by a regular identifier, a delimited identifier, or an integer unsigned number in the range 0..65535: 
{code:sq}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:y) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:""x"",:""y"") FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:2) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:1) FROM DUAL' USING 10,20;
{code}

Placeholders in individual statements are associated by position, not by name. All the above queries insert values 10 and 20 into exactly the same columns of the table t1.

Placeholders can have duplicate names. In this case, every placeholder must have a bind value in the {{USING}} clause.
{code:sql}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:x) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:2) FROM DUAL' USING 10,20;
{code}

Unlike individual statements, rules in blocks are different. Only each *unique* placeholder must have an unique value. Blocks are out of scope of this task.
",,"sql_mode=ORACLE: Dynamic SQL placeholders $end$ When running with {{sql_mode=ORACLE}}, the parser should understand Oracle style placeholders in {{EXECUTE IMMEDIATE}}  and in {{PREPARE}}.

Placeholders are designated as a colon followed by a regular identifier, a delimited identifier, or an integer unsigned number in the range 0..65535: 
{code:sq}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:y) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:""x"",:""y"") FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:2) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:1) FROM DUAL' USING 10,20;
{code}

Placeholders in individual statements are associated by position, not by name. All the above queries insert values 10 and 20 into exactly the same columns of the table t1.

Placeholders can have duplicate names. In this case, every placeholder must have a bind value in the {{USING}} clause.
{code:sql}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:x) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:2) FROM DUAL' USING 10,20;
{code}

Unlike individual statements, rules in blocks are different. Only each *unique* placeholder must have an unique value. Blocks are out of scope of this task.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,27,,0,0,2,5,0,2,0,,0,850,0,0,0,2016-09-13 06:26:06,sql_mode: dynamic SQL placeholders,"When running with {{sql_mode=ORACLE}}, the parser should understand Oracle style placeholders in {{EXECUTE IMMEDIATE}}  and in {{PREPARE}}.

Placeholders are designated as a colon followed by a regular identifier, a delimited identifier, or an integer unsigned number in the range 0..65535: 
{code:sq}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:y) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:""x"",:""y"") FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:2) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:1) FROM DUAL' USING 10,20;
{code}

Placeholders in individual statements are associated by position, not by name. All the above queries insert values 10 and 20 into exactly the same columns of the table t1.

Placeholders can have duplicate names. In this case, every placeholder must have a bind value in the {{USING}} clause.
{code:sql}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:x) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:2) FROM DUAL' USING 10,20;
{code}

Unlike individual statements, rules in blocks are different. Only each *unique* placeholder must have an unique value. Blocks are out of scope of this task.
",,2,0,0,4,0.00943396,"sql_mode: dynamic SQL placeholders $end$ When running with {{sql_mode=ORACLE}}, the parser should understand Oracle style placeholders in {{EXECUTE IMMEDIATE}}  and in {{PREPARE}}.

Placeholders are designated as a colon followed by a regular identifier, a delimited identifier, or an integer unsigned number in the range 0..65535: 
{code:sq}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:y) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:""x"",:""y"") FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:2) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:1) FROM DUAL' USING 10,20;
{code}

Placeholders in individual statements are associated by position, not by name. All the above queries insert values 10 and 20 into exactly the same columns of the table t1.

Placeholders can have duplicate names. In this case, every placeholder must have a bind value in the {{USING}} clause.
{code:sql}
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:x,:x) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:1,:1) FROM DUAL' USING 10,20;
EXECUTE IMMEDIATE 'INSERT INTO t1 SELECT (:2,:2) FROM DUAL' USING 10,20;
{code}

Unlike individual statements, rules in blocks are different. Only each *unique* placeholder must have an unique value. Blocks are out of scope of this task.
 $acceptance criteria:$",2,1,0,0,0,0,1,0.0,30,21,0.7,18,0.6,18,0.6,17,0.566667,17,0.566667
563,MDEV-10813,Task,MDEV,2016-09-15 09:18:04,,0,"Clean-up InnoDB atomics, memory barriers and mutexes","InnoDB atomics implementation is getting worse: 5.7 implementation is not maintainable. We should remove it and use atomics provided by server instead.

InnoDB is now full of meaningless memory barriers, these should be removed along with memory barriers implementation.

We should restore fixes for InnoDB rwlocks.

Mutexes should be cleaned up so that their code is readable. Also we should ensure that mutexes issue proper memory barriers.

NOTE: there're 4 mutex implementations in InnoDB: futex base, spin lock (never used), pthread/critical_section based, event based (default). We should consider removing 3 implementations and leaving only pthread/critical_section based solution.",,"Clean-up InnoDB atomics, memory barriers and mutexes $end$ InnoDB atomics implementation is getting worse: 5.7 implementation is not maintainable. We should remove it and use atomics provided by server instead.

InnoDB is now full of meaningless memory barriers, these should be removed along with memory barriers implementation.

We should restore fixes for InnoDB rwlocks.

Mutexes should be cleaned up so that their code is readable. Also we should ensure that mutexes issue proper memory barriers.

NOTE: there're 4 mutex implementations in InnoDB: futex base, spin lock (never used), pthread/critical_section based, event based (default). We should consider removing 3 implementations and leaving only pthread/critical_section based solution. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Critical,11,,3,15,5,2,0,1,0,,0,850,5,1,0,2016-09-30 12:33:05,"Clean-up InnoDB atomics, memory barriers and mutexes","InnoDB atomics implementation is getting worse: 5.7 implementation is not maintainable. We should remove it and use atomics provided by server instead.

InnoDB is now full of meaningless memory barriers, these should be removed along with memory barriers implementation.

We should restore fixes for InnoDB rwlocks.

Mutexes should be cleaned up so that their code is readable. Also we should ensure that mutexes issue proper memory barriers.

NOTE: there're 4 mutex implementations in InnoDB: futex base, spin lock (never used), pthread/critical_section based, event based (default). We should consider removing 3 implementations and leaving only pthread/critical_section based solution.",,0,0,0,0,0.0,"Clean-up InnoDB atomics, memory barriers and mutexes $end$ InnoDB atomics implementation is getting worse: 5.7 implementation is not maintainable. We should remove it and use atomics provided by server instead.

InnoDB is now full of meaningless memory barriers, these should be removed along with memory barriers implementation.

We should restore fixes for InnoDB rwlocks.

Mutexes should be cleaned up so that their code is readable. Also we should ensure that mutexes issue proper memory barriers.

NOTE: there're 4 mutex implementations in InnoDB: futex base, spin lock (never used), pthread/critical_section based, event based (default). We should consider removing 3 implementations and leaving only pthread/critical_section based solution. $acceptance criteria:$",0,0,0,0,0,0,1,363.25,15,1,0.0666667,0,0.0,0,0.0,0,0.0,0,0.0
564,MDEV-10839,Technical task,MDEV,2016-09-20 09:40:58,,0,"sql_mode=ORACLE: Predefined exceptions: TOO_MANY_ROWS, NO_DATA_FOUND, DUP_VAL_ON_INDEX","When running in {{sql_mode=ORACLE}}, MariaDB should understand predefined exception names.
{code:sql}
BEGIN
  ...
EXCEPTION
  WHEN TOO_MANY_ROWS THEN ...
  WHEN NO_DATA_FOUND THEN ...
  WHEN OTHERS THEN NULL;
END;
{code}

Under terms of this task we'll implement the most important predefined exceptions:
- TOO_MANY_ROWS
- NO_DATA_FOUND
- DUP_VAL_ON_INDEX

Other predefined exception names will be implemented by MDEV-10586.
",,"sql_mode=ORACLE: Predefined exceptions: TOO_MANY_ROWS, NO_DATA_FOUND, DUP_VAL_ON_INDEX $end$ When running in {{sql_mode=ORACLE}}, MariaDB should understand predefined exception names.
{code:sql}
BEGIN
  ...
EXCEPTION
  WHEN TOO_MANY_ROWS THEN ...
  WHEN NO_DATA_FOUND THEN ...
  WHEN OTHERS THEN NULL;
END;
{code}

Under terms of this task we'll implement the most important predefined exceptions:
- TOO_MANY_ROWS
- NO_DATA_FOUND
- DUP_VAL_ON_INDEX

Other predefined exception names will be implemented by MDEV-10586.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,12,,0,0,1,5,0,0,0,,0,850,0,0,0,2016-09-20 09:40:58,"sql_mode=ORACLE: Predefined exceptions: TOO_MANY_ROWS, NO_DATA_FOUND, DUP_VAL_ON_INDEX","When running in {{sql_mode=ORACLE}}, MariaDB should understand predefined exception names.
{code:sql}
BEGIN
  ...
EXCEPTION
  WHEN TOO_MANY_ROWS THEN ...
  WHEN NO_DATA_FOUND THEN ...
  WHEN OTHERS THEN NULL;
END;
{code}

Under terms of this task we'll implement the most important predefined exceptions:
- TOO_MANY_ROWS
- NO_DATA_FOUND
- DUP_VAL_ON_INDEX

Other predefined exception names will be implemented by MDEV-10586.
",,0,0,0,0,0.0,"sql_mode=ORACLE: Predefined exceptions: TOO_MANY_ROWS, NO_DATA_FOUND, DUP_VAL_ON_INDEX $end$ When running in {{sql_mode=ORACLE}}, MariaDB should understand predefined exception names.
{code:sql}
BEGIN
  ...
EXCEPTION
  WHEN TOO_MANY_ROWS THEN ...
  WHEN NO_DATA_FOUND THEN ...
  WHEN OTHERS THEN NULL;
END;
{code}

Under terms of this task we'll implement the most important predefined exceptions:
- TOO_MANY_ROWS
- NO_DATA_FOUND
- DUP_VAL_ON_INDEX

Other predefined exception names will be implemented by MDEV-10586.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,31,22,0.709677,18,0.580645,18,0.580645,17,0.548387,17,0.548387
565,MDEV-10840,Technical task,MDEV,2016-09-20 12:20:06,,0,sql_mode=ORACLE: RAISE statement for predefined exceptions,"Implement the {{RAISE}} statement:
{code:sql}
RAISE [exception_name];
{code}
where {{exceptions_name}} is one of those implemented in MDEV-10839 and MDEV-10582:
- NO_DATA_FOUND
- TOO_MANY_ROWS
- DUP_VAL_ON_INDEX
- INVALID_CURSOR

h2. {{NO_DATA_FOUND}}

Oracle's {{NO_DATA_FOUND}} will be translated to MariaDB warning {{ER_SP_FETCH_NO_DATA}}.

The following three scripts do not return any errors, which proves that {{NO_DATA_FOUND}} is cought and raised silently and therefore is more like a warning than an error:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  RAISE NO_DATA_FOUND;
END;
/
CALL p1();
{code}
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
END;
/
CALL p1();
{code}
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
EXCEPTION
  WHEN NO_DATA_FOUND THEN RAISE;
END;
/
CALL p1();
{code}

This script demonstrates that {{NO_DATA_FOUND}} is actually cought and  can be translated to a fatal error using an {{EXCEPTION..RAISE}} statement.
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  a INT;
  e EXCEPTION;
BEGIN
  SELECT a INTO a FROM t1;
EXCEPTION
  WHEN NO_DATA_FOUND THEN RAISE e;
END;
/
CALL p1();
{code}



h2. {{TOO_MANY_ROWS}}

Oracle's {{TOO_MANY_ROWS}} will be translated to MariaDB error {{TOO_MANY_ROWS}}.

The following three scripts return an error:
{noformat}
ORA-01422: exact fetch returns more than requested number of rows
{noformat}

{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  RAISE TOO_MANY_ROWS;
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
EXCEPTION
  WHEN TOO_MANY_ROWS THEN RAISE;
END;
/
CALL p1();
{code}


h2. {{DUP_VAL_ON_INDEX}}

Oracle's {{DUP_VAL_ON_INDEX}} will be translated to MariaDB error {{ER_DUP_ENTRY}}.

The following three scripts return an error:
{noformat}
ORA-00001: unique constraint ... violated
{noformat}

{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  RAISE DUP_VAL_ON_INDEX;
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT PRIMARY KEY);
CREATE PROCEDURE p1
AS
BEGIN
  INSERT INTO t1 VALUES (10);
  INSERT INTO t1 VALUES (10);
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT PRIMARY KEY);
CREATE PROCEDURE p1
AS
BEGIN
  INSERT INTO t1 VALUES (10);
  INSERT INTO t1 VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN RAISE;
END;
/
CALL p1();
{code}


h2. {{INVALID_CURSOR}}

Oracle's {{INVALID_CURSOR}} will be translated to MariaDB error {{ER_SP_CURSOR_NOT_OPEN}}.
",,"sql_mode=ORACLE: RAISE statement for predefined exceptions $end$ Implement the {{RAISE}} statement:
{code:sql}
RAISE [exception_name];
{code}
where {{exceptions_name}} is one of those implemented in MDEV-10839 and MDEV-10582:
- NO_DATA_FOUND
- TOO_MANY_ROWS
- DUP_VAL_ON_INDEX
- INVALID_CURSOR

h2. {{NO_DATA_FOUND}}

Oracle's {{NO_DATA_FOUND}} will be translated to MariaDB warning {{ER_SP_FETCH_NO_DATA}}.

The following three scripts do not return any errors, which proves that {{NO_DATA_FOUND}} is cought and raised silently and therefore is more like a warning than an error:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  RAISE NO_DATA_FOUND;
END;
/
CALL p1();
{code}
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
END;
/
CALL p1();
{code}
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
EXCEPTION
  WHEN NO_DATA_FOUND THEN RAISE;
END;
/
CALL p1();
{code}

This script demonstrates that {{NO_DATA_FOUND}} is actually cought and  can be translated to a fatal error using an {{EXCEPTION..RAISE}} statement.
{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
CREATE PROCEDURE p1
AS
  a INT;
  e EXCEPTION;
BEGIN
  SELECT a INTO a FROM t1;
EXCEPTION
  WHEN NO_DATA_FOUND THEN RAISE e;
END;
/
CALL p1();
{code}



h2. {{TOO_MANY_ROWS}}

Oracle's {{TOO_MANY_ROWS}} will be translated to MariaDB error {{TOO_MANY_ROWS}}.

The following three scripts return an error:
{noformat}
ORA-01422: exact fetch returns more than requested number of rows
{noformat}

{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  RAISE TOO_MANY_ROWS;
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT);
INSERT INTO t1 VALUES (10);
INSERT INTO t1 VALUES (20);
CREATE PROCEDURE p1
AS
  a INT;
BEGIN
  SELECT a INTO a FROM t1;
EXCEPTION
  WHEN TOO_MANY_ROWS THEN RAISE;
END;
/
CALL p1();
{code}


h2. {{DUP_VAL_ON_INDEX}}

Oracle's {{DUP_VAL_ON_INDEX}} will be translated to MariaDB error {{ER_DUP_ENTRY}}.

The following three scripts return an error:
{noformat}
ORA-00001: unique constraint ... violated
{noformat}

{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
BEGIN
  RAISE DUP_VAL_ON_INDEX;
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT PRIMARY KEY);
CREATE PROCEDURE p1
AS
BEGIN
  INSERT INTO t1 VALUES (10);
  INSERT INTO t1 VALUES (10);
END;
/
CALL p1();
{code}

{code:sql}
DROP TABLE t1;
DROP PROCEDURE p1;
CREATE TABLE t1 (a INT PRIMARY KEY);
CREATE PROCEDURE p1
AS
BEGIN
  INSERT INTO t1 VALUES (10);
  INSERT INTO t1 VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN RAISE;
END;
/
CALL p1();
{code}


h2. {{INVALID_CURSOR}}

Oracle's {{INVALID_CURSOR}} will be translated to MariaDB error {{ER_SP_CURSOR_NOT_OPEN}}.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,25,,0,0,0,5,0,14,0,,0,850,0,0,0,2016-09-20 12:20:06,sql_mode=ORACLE: RAISE statement,"Implement the {{RAISE}} statement:
{code:sql}
RAISE [exception_name];
{code}
",,2,12,0,456,32.5714,"sql_mode=ORACLE: RAISE statement $end$ Implement the {{RAISE}} statement:
{code:sql}
RAISE [exception_name];
{code}
 $acceptance criteria:$",14,1,1,1,1,1,1,0.0,32,22,0.6875,18,0.5625,18,0.5625,17,0.53125,17,0.53125
566,MDEV-10847,Task,MDEV,2016-09-20 20:10:11,,0,Bring AWS KMS encryption plugin up-to-date with released SDK,"AWS C++ SDK announced its release recently.
https://aws.amazon.com/blogs/aws/aws-sdk-for-c-now-ready-for-production-use/

We're still building for an earlier beta version. We need to build against released SDK. some APIs around startup/shutdown AWS SDK have been introduced and are now mandatory, so at least this needs checking.
",,"Bring AWS KMS encryption plugin up-to-date with released SDK $end$ AWS C++ SDK announced its release recently.
https://aws.amazon.com/blogs/aws/aws-sdk-for-c-now-ready-for-production-use/

We're still building for an earlier beta version. We need to build against released SDK. some APIs around startup/shutdown AWS SDK have been introduced and are now mandatory, so at least this needs checking.
 $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-09-20 20:10:34,Bring AWS KMS encryption plugin up-to-date with released SDK,"AWS C++ SDK announced its release recently.
https://aws.amazon.com/blogs/aws/aws-sdk-for-c-now-ready-for-production-use/

We're still building for an earlier beta version. We need to build against released SDK. some APIs around startup/shutdown AWS SDK have been introduced and are now mandatory, so at least this needs checking.
",,0,0,0,0,0.0,"Bring AWS KMS encryption plugin up-to-date with released SDK $end$ AWS C++ SDK announced its release recently.
https://aws.amazon.com/blogs/aws/aws-sdk-for-c-now-ready-for-production-use/

We're still building for an earlier beta version. We need to build against released SDK. some APIs around startup/shutdown AWS SDK have been introduced and are now mandatory, so at least this needs checking.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
567,MDEV-10856,Task,MDEV,2016-09-21 12:02:43,,0,10.1.18 merge,"* 5.5 (/)
* xtradb (/) 5.6.32-78.1
* innodb (/) 5.6.33
* tokudb (/) 5.6.32-78.1
* perfschema (/) 5.6.33
* 10.0 (/)
* 10.0-galera (/)",,"10.1.18 merge $end$ * 5.5 (/)
* xtradb (/) 5.6.32-78.1
* innodb (/) 5.6.33
* tokudb (/) 5.6.32-78.1
* perfschema (/) 5.6.33
* 10.0 (/)
* 10.0-galera (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,9,,0,1,0,1,0,4,0,,0,850,1,0,0,2016-09-21 12:02:54,10.1.18 merge,"* 10.0
* 10.0-galera",,0,4,0,21,2.33333,"10.1.18 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",4,1,1,1,1,1,1,0.0,39,18,0.461538,12,0.307692,10,0.25641,10,0.25641,9,0.230769
568,MDEV-10866,Technical task,MDEV,2016-09-22 10:32:48,,0,Extend PREPARE and EXECUTE IMMEDIATE to understand expressions,"Currently, {{PREPARE}} understands string literals or user variables:

{code:sql}
PREPARE stmt1 FROM 'SELECT 1';
SET @s='SELECT 1';
PREPARE stmt2 FROM @s;
{code}

We'll extend {{PREPARE}} and {{EXECUTE IMMEDIATE}} to understand (almost) any kind of expressions:
{code:sql}
PREPARE stmt FROM CONCAT('SELECT * FROM ', table_name);
{code}

Using stored functions and subselects as a prepare source is out of scope of this task.
",,"Extend PREPARE and EXECUTE IMMEDIATE to understand expressions $end$ Currently, {{PREPARE}} understands string literals or user variables:

{code:sql}
PREPARE stmt1 FROM 'SELECT 1';
SET @s='SELECT 1';
PREPARE stmt2 FROM @s;
{code}

We'll extend {{PREPARE}} and {{EXECUTE IMMEDIATE}} to understand (almost) any kind of expressions:
{code:sql}
PREPARE stmt FROM CONCAT('SELECT * FROM ', table_name);
{code}

Using stored functions and subselects as a prepare source is out of scope of this task.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,20,,2,1,3,5,0,5,0,,0,850,1,0,0,2016-09-22 10:32:48,Extend PREPARE and EXECUTE IMMEDIATE to userstand expressions,"Currently, {{PREPARE}} understands string literals or user variables:

{code:sql}
PREPARE stmt1 FROM 'SELECT 1';
SET @s='SELECT 1';
PREPARE stmt2 FROM 'SELECT 1';
{code}

We'll extend {{PREPARE}} to understand any kind of expressions:
{code:sql}
PREPARE stmt FROM CONCTA('SELECT * FROM ', table_name);
{code}

",,1,4,0,27,0.45283,"Extend PREPARE and EXECUTE IMMEDIATE to userstand expressions $end$ Currently, {{PREPARE}} understands string literals or user variables:

{code:sql}
PREPARE stmt1 FROM 'SELECT 1';
SET @s='SELECT 1';
PREPARE stmt2 FROM 'SELECT 1';
{code}

We'll extend {{PREPARE}} to understand any kind of expressions:
{code:sql}
PREPARE stmt FROM CONCTA('SELECT * FROM ', table_name);
{code}

 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,33,23,0.69697,19,0.575758,19,0.575758,18,0.545455,18,0.545455
569,MDEV-10871,Task,MDEV,2016-09-22 16:04:58,,0,Add logging capability to pam_user_map.c,"The PAM user mapping plugin doesn't currently seem to log any information to /var/log/secure. It would probably be helpful if the plugin had some way to enable verbose logging during testing, so that it would be easier to debug configuration problems.

I expect the best way to implement this would be to create one or more module arguments for the plugin that controls logging. PAM module arguments are explained here:

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/PAM_Configuration_Files.html#pam-mod-args

For example, maybe a configuration like this could enable very verbose debugging logging to /var/log/secure:

{noformat}
auth required pam_user_map.so debug
{noformat}

Or if we wanted the ability to specify a specific log, maybe we could do something like this:

{noformat}
auth required pam_user_map.so debug_log=/tmp/pam_user_map.log
{noformat}

But these are just suggestions.",,"Add logging capability to pam_user_map.c $end$ The PAM user mapping plugin doesn't currently seem to log any information to /var/log/secure. It would probably be helpful if the plugin had some way to enable verbose logging during testing, so that it would be easier to debug configuration problems.

I expect the best way to implement this would be to create one or more module arguments for the plugin that controls logging. PAM module arguments are explained here:

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/PAM_Configuration_Files.html#pam-mod-args

For example, maybe a configuration like this could enable very verbose debugging logging to /var/log/secure:

{noformat}
auth required pam_user_map.so debug
{noformat}

Or if we wanted the ability to specify a specific log, maybe we could do something like this:

{noformat}
auth required pam_user_map.so debug_log=/tmp/pam_user_map.log
{noformat}

But these are just suggestions. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,12,,1,4,1,2,0,0,0,,0,850,3,0,0,2017-12-12 15:44:13,Add logging capability to pam_user_map.c,"The PAM user mapping plugin doesn't currently seem to log any information to /var/log/secure. It would probably be helpful if the plugin had some way to enable verbose logging during testing, so that it would be easier to debug configuration problems.

I expect the best way to implement this would be to create one or more module arguments for the plugin that controls logging. PAM module arguments are explained here:

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/PAM_Configuration_Files.html#pam-mod-args

For example, maybe a configuration like this could enable very verbose debugging logging to /var/log/secure:

{noformat}
auth required pam_user_map.so debug
{noformat}

Or if we wanted the ability to specify a specific log, maybe we could do something like this:

{noformat}
auth required pam_user_map.so debug_log=/tmp/pam_user_map.log
{noformat}

But these are just suggestions.",,0,0,0,0,0.0,"Add logging capability to pam_user_map.c $end$ The PAM user mapping plugin doesn't currently seem to log any information to /var/log/secure. It would probably be helpful if the plugin had some way to enable verbose logging during testing, so that it would be easier to debug configuration problems.

I expect the best way to implement this would be to create one or more module arguments for the plugin that controls logging. PAM module arguments are explained here:

https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Managing_Smart_Cards/PAM_Configuration_Files.html#pam-mod-args

For example, maybe a configuration like this could enable very verbose debugging logging to /var/log/secure:

{noformat}
auth required pam_user_map.so debug
{noformat}

Or if we wanted the ability to specify a specific log, maybe we could do something like this:

{noformat}
auth required pam_user_map.so debug_log=/tmp/pam_user_map.log
{noformat}

But these are just suggestions. $acceptance criteria:$",0,0,0,0,0,0,1,10703.7,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
570,MDEV-10877,Technical task,MDEV,2016-09-23 08:33:05,,0,xxx_unicode_nopad_ci collations,"MDEV-9711 added nopad style collations for all default and _bin collations.
This task is to add _unicode_nopad_ci and _unicode_520_nopad_ci collations.
",,"xxx_unicode_nopad_ci collations $end$ MDEV-9711 added nopad style collations for all default and _bin collations.
This task is to add _unicode_nopad_ci and _unicode_520_nopad_ci collations.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,0,0,1,2,0,0,0,,0,850,0,0,0,2016-09-23 08:33:05,xxx_unicode_nopad_ci collations,"MDEV-9711 added nopad style collations for all default and _bin collations.
This task is to add _unicode_nopad_ci and _unicode_520_nopad_ci collations.
",,0,0,0,0,0.0,"xxx_unicode_nopad_ci collations $end$ MDEV-9711 added nopad style collations for all default and _bin collations.
This task is to add _unicode_nopad_ci and _unicode_520_nopad_ci collations.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,34,24,0.705882,20,0.588235,20,0.588235,19,0.558824,19,0.558824
571,MDEV-10914,Technical task,MDEV,2016-09-28 10:41:18,,0,ROW data type for stored routine variables,"Add support for the {{ROW}} data type variables in stored routines according to this SQL Standard syntax:
{noformat}
<row type> ::= ROW <row type body>

<row type body> ::= <left paren> <field definition> [ { <comma> <field definition> }... ] <right paren>

<field definition> ::= <field name> <data type>

<data type> ::= <predefined type>
{noformat}

and add support for a new expression type, a ROW field reference, as follows:
{noformat}
<field reference> ::= <row variable> <period> <field name>
{noformat}
where {{<row variable>}} is a stored routine variable declared using {{<row type>}}.

Example:
{code:sql}
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE a ROW (c1 INT, c2 VARCHAR(10));
  SET a.c1= 10;
  SET a.c2= 'test';
  INSERT INTO t1 VALUES (a.c1, a.c2);
END;
$$
DELIMITER ;
CALL p1();
{code}

This task is needed to create infrastructure for MDEV-10593

This task is also needed as a prerequisite for MDEV-10581, where this statement:
{code:sql}
FOR rec IN cursor
{code}
will automatically declare an index variable {{rec}} of the {{ROW}} data type, according to the cursor structure.

h2. Row data type features:
- Declaration of a ROW type stored routine variable (both local variables and parameters)
- Declaration of a ROW type stored procedure OUT parameter
- Default values in a ROW type variable declaration (e.g. {{DEFAULT ROW(1,2)}})
- Assignment of a ROW type variable from another ROW type variable (using the {{SET}} command and the {{:=}} operator in {{sql_mode=ORACLE}})
- Assignment of a ROW type variable from a ROW() function result (using the {{SET}} command and the {{:=}} operator in {{sql_mode=ORACLE}})
- Passing a ROW type variable and a {{ROW()}} function result to stored routines
- Comparison of a ROW type variable to another ROW type variable
- Comparison of a ROW type variable to ROW() function

h2. Row field features:
ROW fields (members) will act as normal variables, and will be able to appear in all query parts where an SP variable is allowed:
- assignment (using the {{SET}} command)
{code:sql}
SET a.x= 10, a.y=20, a.z= b.z;
{code}
- assignment ({{sql_mode=ORACLE}} specific syntax)
{code:sql}
a.x:= 10;
a.x:= b.x;
{code}
- passing to functions and operators
{code:sql}
SELECT f1(rec.a), rec.a<10;
{code}
- clauses: select list, {{WHERE}}, {{HAVING}}, {{LIMIT}}, etc
{code:sql}
SELECT var.a, t1.b FROM t1 WHERE t1.b=var.b LIMIT var.c;
{code}
- {{INSERT}} values
{code:sql}
INSERT INTO t1 VALUES (rec.a, rec.b, rec.c);
{code}
- {{SELECT .. INTO}} targets
{code:sql}
SELECT a,b INTO rec.a, rec.b FROM t1 WHERE t1.id=10;
{code}
- Dynamic SQL out parameters ({{EXECUTE}} and {{EXECUTE IMMEDIATE}})
{code:sql}
EXECUTE IMMEDIATE 'CALL proc_with_out_param(?)' USING rec.a;
{code}

h2. Features not supported:
The following features are out of scope of this task and will be implemented separately:
- Returning a ROW type expression from a stored function (see MDEV-12252). This will need some grammar change to support field names after parentheses:
{code:sql}
SELECT f1().x FROM DUAL;
{code}
- Returning a ROW type expression from a built-in hybrid type function, such as {{CASE}}, {{IF}}, etc. 
- ROW of ROWs
",,"ROW data type for stored routine variables $end$ Add support for the {{ROW}} data type variables in stored routines according to this SQL Standard syntax:
{noformat}
<row type> ::= ROW <row type body>

<row type body> ::= <left paren> <field definition> [ { <comma> <field definition> }... ] <right paren>

<field definition> ::= <field name> <data type>

<data type> ::= <predefined type>
{noformat}

and add support for a new expression type, a ROW field reference, as follows:
{noformat}
<field reference> ::= <row variable> <period> <field name>
{noformat}
where {{<row variable>}} is a stored routine variable declared using {{<row type>}}.

Example:
{code:sql}
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE a ROW (c1 INT, c2 VARCHAR(10));
  SET a.c1= 10;
  SET a.c2= 'test';
  INSERT INTO t1 VALUES (a.c1, a.c2);
END;
$$
DELIMITER ;
CALL p1();
{code}

This task is needed to create infrastructure for MDEV-10593

This task is also needed as a prerequisite for MDEV-10581, where this statement:
{code:sql}
FOR rec IN cursor
{code}
will automatically declare an index variable {{rec}} of the {{ROW}} data type, according to the cursor structure.

h2. Row data type features:
- Declaration of a ROW type stored routine variable (both local variables and parameters)
- Declaration of a ROW type stored procedure OUT parameter
- Default values in a ROW type variable declaration (e.g. {{DEFAULT ROW(1,2)}})
- Assignment of a ROW type variable from another ROW type variable (using the {{SET}} command and the {{:=}} operator in {{sql_mode=ORACLE}})
- Assignment of a ROW type variable from a ROW() function result (using the {{SET}} command and the {{:=}} operator in {{sql_mode=ORACLE}})
- Passing a ROW type variable and a {{ROW()}} function result to stored routines
- Comparison of a ROW type variable to another ROW type variable
- Comparison of a ROW type variable to ROW() function

h2. Row field features:
ROW fields (members) will act as normal variables, and will be able to appear in all query parts where an SP variable is allowed:
- assignment (using the {{SET}} command)
{code:sql}
SET a.x= 10, a.y=20, a.z= b.z;
{code}
- assignment ({{sql_mode=ORACLE}} specific syntax)
{code:sql}
a.x:= 10;
a.x:= b.x;
{code}
- passing to functions and operators
{code:sql}
SELECT f1(rec.a), rec.a<10;
{code}
- clauses: select list, {{WHERE}}, {{HAVING}}, {{LIMIT}}, etc
{code:sql}
SELECT var.a, t1.b FROM t1 WHERE t1.b=var.b LIMIT var.c;
{code}
- {{INSERT}} values
{code:sql}
INSERT INTO t1 VALUES (rec.a, rec.b, rec.c);
{code}
- {{SELECT .. INTO}} targets
{code:sql}
SELECT a,b INTO rec.a, rec.b FROM t1 WHERE t1.id=10;
{code}
- Dynamic SQL out parameters ({{EXECUTE}} and {{EXECUTE IMMEDIATE}})
{code:sql}
EXECUTE IMMEDIATE 'CALL proc_with_out_param(?)' USING rec.a;
{code}

h2. Features not supported:
The following features are out of scope of this task and will be implemented separately:
- Returning a ROW type expression from a stored function (see MDEV-12252). This will need some grammar change to support field names after parentheses:
{code:sql}
SELECT f1().x FROM DUAL;
{code}
- Returning a ROW type expression from a built-in hybrid type function, such as {{CASE}}, {{IF}}, etc. 
- ROW of ROWs
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,28,,0,1,8,5,0,13,0,,0,850,1,0,0,2016-09-28 10:41:18,ROW data type for stored routine variables,"Add support for the {{ROW}} data type variables in stored routines according to this SQL Standard syntax:
{noformat}
<row type> ::= ROW <row type body>

<row type body> ::= <left paren> <field definition> [ { <comma> <field definition> }... ] <right paren>

<field definition> ::= <field name> <data type>

<data type> ::= <predefined type>
{noformat}

and add support for a new expression type, a ROW field reference, as follows:
{noformat}
<field reference> ::= <row variable> <period> <field name>
{noformat}
where {{<row variable>}} is a stored routine variable declared using {{<row type>}}.

Example:
{code:sql}
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE a ROW (c1 INT, c2 VARCHAR(10));
  SET a.c1= 10;
  SET a.c2= 'test';
  INSERT INTO t1 VALUES (a.c1, a.c2);
END;
$$
DELIMITER ;
CALL p1();
{code}


This task is needed as a prerequisite for MDEV-10581, where this statement:
{code:sql}
FOR rec IN cursor
{code}
will automatically declare an index variable {{rec}} of the {{ROW}} data type, according to the cursor structure. Therefore, this task includes only basic {{ROW}} functionality.

Extended SQL Standard {{ROW}} functionality will be implemented in separate tasks, e.g.:
- This task is limited to only scalar ROW elements. ROW inside ROW will be done in a separate task.
- DEFAULT clause in DECLARE will not be supported for ROW variables. It will be done in a separate task.
- ROW variables won't be supported as parameters to native functions and stored routines. It will be done in a separate task.
",,0,13,0,389,1.2381,"ROW data type for stored routine variables $end$ Add support for the {{ROW}} data type variables in stored routines according to this SQL Standard syntax:
{noformat}
<row type> ::= ROW <row type body>

<row type body> ::= <left paren> <field definition> [ { <comma> <field definition> }... ] <right paren>

<field definition> ::= <field name> <data type>

<data type> ::= <predefined type>
{noformat}

and add support for a new expression type, a ROW field reference, as follows:
{noformat}
<field reference> ::= <row variable> <period> <field name>
{noformat}
where {{<row variable>}} is a stored routine variable declared using {{<row type>}}.

Example:
{code:sql}
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE a ROW (c1 INT, c2 VARCHAR(10));
  SET a.c1= 10;
  SET a.c2= 'test';
  INSERT INTO t1 VALUES (a.c1, a.c2);
END;
$$
DELIMITER ;
CALL p1();
{code}


This task is needed as a prerequisite for MDEV-10581, where this statement:
{code:sql}
FOR rec IN cursor
{code}
will automatically declare an index variable {{rec}} of the {{ROW}} data type, according to the cursor structure. Therefore, this task includes only basic {{ROW}} functionality.

Extended SQL Standard {{ROW}} functionality will be implemented in separate tasks, e.g.:
- This task is limited to only scalar ROW elements. ROW inside ROW will be done in a separate task.
- DEFAULT clause in DECLARE will not be supported for ROW variables. It will be done in a separate task.
- ROW variables won't be supported as parameters to native functions and stored routines. It will be done in a separate task.
 $acceptance criteria:$",13,1,1,1,1,1,1,0.0,35,24,0.685714,20,0.571429,20,0.571429,19,0.542857,19,0.542857
572,MDEV-10966,Task,MDEV,2016-10-06 09:54:13,,0,Packaging for MariaRocks,"This is about packaging MyRocks storage engine.

Build steps for the Facebook tree:  https://github.com/facebook/mysql-5.6/wiki/Build-Steps

h2. Compiling
* RocksDB requires a recent C++ compiler, more recent than some MariaDB platforms have.
* It's the same with TokuDB, so we can borrow its CMake code for checking compiler version.

h2. Dependencies
RocksDB depends on compression libraries:
* zlip
* snappy
* ....
* zstandard (new addition) 

Ubuntu 16.0.4 Xenial LTS has packages for all libraries.
Other versions support a subset.

h2. What to build

MariaRocks must be a loadable module (don't link statically).

h2. Other
Facebook builds RocksDB with Jemalloc (we build Toku with Jemalloc, too)",,"Packaging for MariaRocks $end$ This is about packaging MyRocks storage engine.

Build steps for the Facebook tree:  https://github.com/facebook/mysql-5.6/wiki/Build-Steps

h2. Compiling
* RocksDB requires a recent C++ compiler, more recent than some MariaDB platforms have.
* It's the same with TokuDB, so we can borrow its CMake code for checking compiler version.

h2. Dependencies
RocksDB depends on compression libraries:
* zlip
* snappy
* ....
* zstandard (new addition) 

Ubuntu 16.0.4 Xenial LTS has packages for all libraries.
Other versions support a subset.

h2. What to build

MariaRocks must be a loadable module (don't link statically).

h2. Other
Facebook builds RocksDB with Jemalloc (we build Toku with Jemalloc, too) $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,14,,0,5,3,3,0,2,1,,0,850,5,2,0,2016-12-08 11:50:56,Packaging for MariaRocks,"This is about packaging MyRocks storage engine.

Build steps for the Facebook tree:  https://github.com/facebook/mysql-5.6/wiki/Build-Steps

h2. Compiling
* RocksDB requires a recent C++ compiler, more recent than some MariaDB platforms have.
* It's the same with TokuDB, so we can borrow its CMake code for checking compiler version.

h2. Dependencies
RocksDB depends on compression libraries:
* zlip
* snappy
* ....
* zstandard (new addition) 

Ubuntu 16.0.4 Xenial LTS has packages for all libraries.
Other versions support a subset.

h2. What to build

MariaRocks must be a loadable module (don't link statically).

h2. Other
Facebook builds RocksDB with Jemalloc (we build Toku with Jemalloc, too)",,0,0,0,0,0.0,"Packaging for MariaRocks $end$ This is about packaging MyRocks storage engine.

Build steps for the Facebook tree:  https://github.com/facebook/mysql-5.6/wiki/Build-Steps

h2. Compiling
* RocksDB requires a recent C++ compiler, more recent than some MariaDB platforms have.
* It's the same with TokuDB, so we can borrow its CMake code for checking compiler version.

h2. Dependencies
RocksDB depends on compression libraries:
* zlip
* snappy
* ....
* zstandard (new addition) 

Ubuntu 16.0.4 Xenial LTS has packages for all libraries.
Other versions support a subset.

h2. What to build

MariaRocks must be a loadable module (don't link statically).

h2. Other
Facebook builds RocksDB with Jemalloc (we build Toku with Jemalloc, too) $acceptance criteria:$",0,0,0,0,0,0,1,1513.93,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
573,MDEV-11037,Technical task,MDEV,2016-10-12 13:25:41,,0,Diagnostics_area refactoring for user defined exceptions,"Currently members to store sqlstate, sql errno and sql condition level present in the following classes:
- Sql_condition (m_returned_sqlstate, m_sql_errno, m_level)
- Diagnostics_area (m_sql_errno, m_sqlstate)
- Sql_condition_info (sql_errno, sql_state, level)
- sp_condition_value (mysqlerr, sql_state)

This produces a lot of duplicate code.
For the purposes of MDEV-10587 we'll also have to add a pointer to user defined exceptions at least in the following classes:
- Sql_condition
- Sql_condition_info

That will generate more duplicate code.
To avoid code duplication, we'll introduce the following class hierarchy:
{code}
Sql_state
  Sql_state_errno
    Diagnostics_area
    sp_condition_value
    Sql_state_errno_level
      Sql_condition_info
      Sql_condition   
{code}
",,"Diagnostics_area refactoring for user defined exceptions $end$ Currently members to store sqlstate, sql errno and sql condition level present in the following classes:
- Sql_condition (m_returned_sqlstate, m_sql_errno, m_level)
- Diagnostics_area (m_sql_errno, m_sqlstate)
- Sql_condition_info (sql_errno, sql_state, level)
- sp_condition_value (mysqlerr, sql_state)

This produces a lot of duplicate code.
For the purposes of MDEV-10587 we'll also have to add a pointer to user defined exceptions at least in the following classes:
- Sql_condition
- Sql_condition_info

That will generate more duplicate code.
To avoid code duplication, we'll introduce the following class hierarchy:
{code}
Sql_state
  Sql_state_errno
    Diagnostics_area
    sp_condition_value
    Sql_state_errno_level
      Sql_condition_info
      Sql_condition   
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,0,0,1,5,0,0,0,,0,850,0,0,0,2016-10-12 13:25:41,Diagnostics_area refactoring for user defined exceptions,"Currently members to store sqlstate, sql errno and sql condition level present in the following classes:
- Sql_condition (m_returned_sqlstate, m_sql_errno, m_level)
- Diagnostics_area (m_sql_errno, m_sqlstate)
- Sql_condition_info (sql_errno, sql_state, level)
- sp_condition_value (mysqlerr, sql_state)

This produces a lot of duplicate code.
For the purposes of MDEV-10587 we'll also have to add a pointer to user defined exceptions at least in the following classes:
- Sql_condition
- Sql_condition_info

That will generate more duplicate code.
To avoid code duplication, we'll introduce the following class hierarchy:
{code}
Sql_state
  Sql_state_errno
    Diagnostics_area
    sp_condition_value
    Sql_state_errno_level
      Sql_condition_info
      Sql_condition   
{code}
",,0,0,0,0,0.0,"Diagnostics_area refactoring for user defined exceptions $end$ Currently members to store sqlstate, sql errno and sql condition level present in the following classes:
- Sql_condition (m_returned_sqlstate, m_sql_errno, m_level)
- Diagnostics_area (m_sql_errno, m_sqlstate)
- Sql_condition_info (sql_errno, sql_state, level)
- sp_condition_value (mysqlerr, sql_state)

This produces a lot of duplicate code.
For the purposes of MDEV-10587 we'll also have to add a pointer to user defined exceptions at least in the following classes:
- Sql_condition
- Sql_condition_info

That will generate more duplicate code.
To avoid code duplication, we'll introduce the following class hierarchy:
{code}
Sql_state
  Sql_state_errno
    Diagnostics_area
    sp_condition_value
    Sql_state_errno_level
      Sql_condition_info
      Sql_condition   
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,36,25,0.694444,21,0.583333,21,0.583333,20,0.555556,20,0.555556
574,MDEV-11042,Task,MDEV,2016-10-13 06:22:57,,0,Implement GeoJSON functions.,"Implement  ST_AsGeoJSON and  ST_GeomFromGeoJSON functions
so the spatial features can be imported/exported using GeoJSON format.
GeoJSON explanations: http://geojson.org/",,"Implement GeoJSON functions. $end$ Implement  ST_AsGeoJSON and  ST_GeomFromGeoJSON functions
so the spatial features can be imported/exported using GeoJSON format.
GeoJSON explanations: http://geojson.org/ $acceptance criteria:$",,Alexey Botchkov,Alexey Botchkov,Major,20,,0,1,0,4,0,0,0,,0,850,1,0,0,2016-10-13 06:24:43,Implement GeoJSON functions.,"Implement  ST_AsGeoJSON and  ST_GeomFromGeoJSON functions
so the spatial features can be imported/exported using GeoJSON format.
GeoJSON explanations: http://geojson.org/",,0,0,0,0,0.0,"Implement GeoJSON functions. $end$ Implement  ST_AsGeoJSON and  ST_GeomFromGeoJSON functions
so the spatial features can be imported/exported using GeoJSON format.
GeoJSON explanations: http://geojson.org/ $acceptance criteria:$",0,0,0,0,0,0,1,0.0166667,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
575,MDEV-11097,Task,MDEV,2016-10-21 10:09:12,,0,Update the list of unstable tests,unstable-tests list which we maintain for distributions needs to be brought up to date. ,,Update the list of unstable tests $end$ unstable-tests list which we maintain for distributions needs to be brought up to date.  $acceptance criteria:$,,Elena Stepanova,Elena Stepanova,Blocker,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-10-21 10:56:38,Update the list of unstable tests,unstable-tests list which we maintain for distributions needs to be brought up to date. ,,0,0,0,0,0.0,Update the list of unstable tests $end$ unstable-tests list which we maintain for distributions needs to be brought up to date.  $acceptance criteria:$,0,0,0,0,0,0,0,0.783333,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
576,MDEV-11130,Task,MDEV,2016-10-25 13:24:00,,0,Update the list of unstable tests for 10.1,"- remove from the list tests which have been fixed since the last 10.1 release;
- remove from the list tests which were added there because they had been newly created or updated, but it was over 4 months ago (before Jul 1, 2016);
- add to the list tests which have failed in 10.1 since the last 10.1 release;
- add to the list tests which have been newly created or directly/indirectly updated since the last 10.1 release",,"Update the list of unstable tests for 10.1 $end$ - remove from the list tests which have been fixed since the last 10.1 release;
- remove from the list tests which were added there because they had been newly created or updated, but it was over 4 months ago (before Jul 1, 2016);
- add to the list tests which have failed in 10.1 since the last 10.1 release;
- add to the list tests which have been newly created or directly/indirectly updated since the last 10.1 release $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Blocker,10,,0,1,0,1,0,7,0,,0,850,1,0,0,2016-10-26 20:03:43,Update the lists for unstable tests for 10.1,"- remove from the list tests which have been fixed since the last release;
- remove from the list tests which were added there because they had been newly created or updated, and haven't failed over the past 3 months;
- add to the list tests which have failed since the last release;
- add to the list tests which have been newly created or directly/indirectly updated since the last release",,1,6,0,26,0.207317,"Update the lists for unstable tests for 10.1 $end$ - remove from the list tests which have been fixed since the last release;
- remove from the list tests which were added there because they had been newly created or updated, and haven't failed over the past 3 months;
- add to the list tests which have failed since the last release;
- add to the list tests which have been newly created or directly/indirectly updated since the last release $acceptance criteria:$",7,1,1,1,1,1,1,30.65,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
577,MDEV-11153,Task,MDEV,2016-10-26 17:34:50,,0,Introduce status variables for table cache monitoring and tuning,"1. Since MariaDB's definition and use of {{table_open_cache_instances}} is different from MySQL's, it naturally calls for a status variable which would indicate how many instances are in use at the moment. MySQL does not have it, but it's not needed there, because there {{table_open_cache_instances}} shows exactly that. 

2. MySQL 5.6/5.7 has status variables {{Table_open_cache_hits}}, {{Table_open_cache_misses}}, {{Table_open_cache_overflows}}. Even though MariaDB's table cache tuning is more automated than MySQL's, the variables can still be useful in MariaDB too. ",,"Introduce status variables for table cache monitoring and tuning $end$ 1. Since MariaDB's definition and use of {{table_open_cache_instances}} is different from MySQL's, it naturally calls for a status variable which would indicate how many instances are in use at the moment. MySQL does not have it, but it's not needed there, because there {{table_open_cache_instances}} shows exactly that. 

2. MySQL 5.6/5.7 has status variables {{Table_open_cache_hits}}, {{Table_open_cache_misses}}, {{Table_open_cache_overflows}}. Even though MariaDB's table cache tuning is more automated than MySQL's, the variables can still be useful in MariaDB too.  $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Minor,10,,0,1,1,1,0,0,0,,0,850,0,0,0,2017-10-24 16:44:09,Introduce status variables for table cache monitoring and tuning,"1. Since MariaDB's definition and use of {{table_open_cache_instances}} is different from MySQL's, it naturally calls for a status variable which would indicate how many instances are in use at the moment. MySQL does not have it, but it's not needed there, because there {{table_open_cache_instances}} shows exactly that. 

2. MySQL 5.6/5.7 has status variables {{Table_open_cache_hits}}, {{Table_open_cache_misses}}, {{Table_open_cache_overflows}}. Even though MariaDB's table cache tuning is more automated than MySQL's, the variables can still be useful in MariaDB too. ",,0,0,0,0,0.0,"Introduce status variables for table cache monitoring and tuning $end$ 1. Since MariaDB's definition and use of {{table_open_cache_instances}} is different from MySQL's, it naturally calls for a status variable which would indicate how many instances are in use at the moment. MySQL does not have it, but it's not needed there, because there {{table_open_cache_instances}} shows exactly that. 

2. MySQL 5.6/5.7 has status variables {{Table_open_cache_hits}}, {{Table_open_cache_misses}}, {{Table_open_cache_overflows}}. Even though MariaDB's table cache tuning is more automated than MySQL's, the variables can still be useful in MariaDB too.  $acceptance criteria:$",0,0,0,0,0,0,0,8711.15,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
578,MDEV-11159,Task,MDEV,2016-10-27 11:26:09,,0,Server Proxy Protocol Support,"https://blueprints.launchpad.net/percona-server/+spec/proxy-protocol

we want to support both versions 1 and 2",,"Server Proxy Protocol Support $end$ https://blueprints.launchpad.net/percona-server/+spec/proxy-protocol

we want to support both versions 1 and 2 $acceptance criteria:$",,VAROQUI Stephane,VAROQUI Stephane,Critical,15,,0,5,1,2,0,1,0,,0,850,1,0,0,2017-05-24 12:17:28,Server Proxy Protocol Support,https://blueprints.launchpad.net/percona-server/+spec/proxy-protocol,,0,1,0,9,1.125,Server Proxy Protocol Support $end$ https://blueprints.launchpad.net/percona-server/+spec/proxy-protocol $acceptance criteria:$,1,1,1,0,0,0,1,5016.85,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
579,MDEV-11200,Task,MDEV,2016-11-01 14:14:09,,0,10.1.19 merge,"* 10.0
* 10.0-galera",,"10.1.19 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,4,,0,1,0,1,0,0,0,,0,850,0,0,0,2016-11-02 08:08:47,10.1.19 merge,"* 10.0
* 10.0-galera",,0,0,0,0,0.0,"10.1.19 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",0,0,0,0,0,0,0,17.9,40,19,0.475,13,0.325,11,0.275,11,0.275,10,0.25
580,MDEV-11212,Task,MDEV,2016-11-02 11:11:27,,0,Clean-up MariaDB atomic operations,,,Clean-up MariaDB atomic operations $end$ $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,13,,1,7,1,1,0,0,0,,0,850,4,0,0,2016-11-09 13:53:46,Clean-up MariaDB atomic operations,,,0,0,0,0,0.0,Clean-up MariaDB atomic operations $end$ $acceptance criteria:$,0,0,0,0,0,0,0,170.7,16,1,0.0625,0,0.0,0,0.0,0,0.0,0,0.0
581,MDEV-11239,Task,MDEV,2016-11-04 19:26:18,,0,Allow wsrep_dirty_reads to be set globally,I would like to request that wsrep_dirty_reads be elevated to a global variable so it can be set in my.cnf. A lot of people have complex application configurations that would make appending this before each query a difficult task. MySQL has made this variable a global one as of 5.6.26-25.12. I think it would be a good idea for MariaDB to do it as well. https://www.percona.com/doc/percona-xtradb-cluster/5.6/wsrep-system-index.html#wsrep_dirty_reads,,Allow wsrep_dirty_reads to be set globally $end$ I would like to request that wsrep_dirty_reads be elevated to a global variable so it can be set in my.cnf. A lot of people have complex application configurations that would make appending this before each query a difficult task. MySQL has made this variable a global one as of 5.6.26-25.12. I think it would be a good idea for MariaDB to do it as well. https://www.percona.com/doc/percona-xtradb-cluster/5.6/wsrep-system-index.html#wsrep_dirty_reads $acceptance criteria:$,,Eric Howey,Eric Howey,Minor,11,,0,1,1,1,0,0,0,,0,850,0,0,0,2016-11-17 14:06:31,Allow wsrep_dirty_reads to be set globally,I would like to request that wsrep_dirty_reads be elevated to a global variable so it can be set in my.cnf. A lot of people have complex application configurations that would make appending this before each query a difficult task. MySQL has made this variable a global one as of 5.6.26-25.12. I think it would be a good idea for MariaDB to do it as well. https://www.percona.com/doc/percona-xtradb-cluster/5.6/wsrep-system-index.html#wsrep_dirty_reads,,0,0,0,0,0.0,Allow wsrep_dirty_reads to be set globally $end$ I would like to request that wsrep_dirty_reads be elevated to a global variable so it can be set in my.cnf. A lot of people have complex application configurations that would make appending this before each query a difficult task. MySQL has made this variable a global one as of 5.6.26-25.12. I think it would be a good idea for MariaDB to do it as well. https://www.percona.com/doc/percona-xtradb-cluster/5.6/wsrep-system-index.html#wsrep_dirty_reads $acceptance criteria:$,0,0,0,0,0,0,0,306.667,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
582,MDEV-11245,Task,MDEV,2016-11-07 09:54:40,,0,Move prepare_create_field and sp_prepare_create_field() as methods to Column_definition,"Currently prepare_create_field() and sp_prepare_create_field() are global functions defined in sql_table.cc.
MDEV-10577 and MDEV-10914 will use more prepare_create_field() and sp_prepare_create_field().
It's better to move these functions as method to Column_definition.
",,"Move prepare_create_field and sp_prepare_create_field() as methods to Column_definition $end$ Currently prepare_create_field() and sp_prepare_create_field() are global functions defined in sql_table.cc.
MDEV-10577 and MDEV-10914 will use more prepare_create_field() and sp_prepare_create_field().
It's better to move these functions as method to Column_definition.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,12,,0,1,2,1,0,1,0,,0,850,1,0,0,2016-11-09 13:43:25,Move prepare_create_field and sp_prepare_create_field() as methods to Column_definition,"Currently prepare_create_field() and sp_prepare_create_field() are global functions defined in sql_table.cc.
MDEV-10577 will use more prepare_create_field() and sp_prepare_create_field().
It's better to move these functions as method to Column_definition.
",,0,1,0,2,0.0526316,"Move prepare_create_field and sp_prepare_create_field() as methods to Column_definition $end$ Currently prepare_create_field() and sp_prepare_create_field() are global functions defined in sql_table.cc.
MDEV-10577 will use more prepare_create_field() and sp_prepare_create_field().
It's better to move these functions as method to Column_definition.
 $acceptance criteria:$",1,1,0,0,0,0,0,51.8,37,25,0.675676,21,0.567568,21,0.567568,20,0.540541,20,0.540541
583,MDEV-11271,Task,MDEV,2016-11-11 11:33:15,,0,"Add ""leaves"" algorithm to OQGRAPH.","This algorithm returns all reachable leaf nodes from a given origin, or all root nodes that can reach a given destination.

Currently, I had to perform a self-join on the graph table to get a list of all leaf nodes (all destids that themselves are not an origid). This becomes rather expensive the larger the graph is.
I created a simple, large graph of 2 million nodes in (essentially just a chain of nodes, so A->B->C->...) and finding the leaf node from the root node took about 45 seconds. With the new
""leaves"" latch it completes in about 7 seconds.

Briefly discussed on oqgraph-dev (thread starting at https://lists.launchpad.net/oqgraph-dev/msg00314.html), but no review was done yet (other than mtr and works for me).",,"Add ""leaves"" algorithm to OQGRAPH. $end$ This algorithm returns all reachable leaf nodes from a given origin, or all root nodes that can reach a given destination.

Currently, I had to perform a self-join on the graph table to get a list of all leaf nodes (all destids that themselves are not an origid). This becomes rather expensive the larger the graph is.
I created a simple, large graph of 2 million nodes in (essentially just a chain of nodes, so A->B->C->...) and finding the leaf node from the root node took about 45 seconds. With the new
""leaves"" latch it completes in about 7 seconds.

Briefly discussed on oqgraph-dev (thread starting at https://lists.launchpad.net/oqgraph-dev/msg00314.html), but no review was done yet (other than mtr and works for me). $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Critical,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-06-22 05:06:16,"Add ""leaves"" algorithm to OQGRAPH.","This algorithm returns all reachable leaf nodes from a given origin, or all root nodes that can reach a given destination.

Currently, I had to perform a self-join on the graph table to get a list of all leaf nodes (all destids that themselves are not an origid). This becomes rather expensive the larger the graph is.
I created a simple, large graph of 2 million nodes in (essentially just a chain of nodes, so A->B->C->...) and finding the leaf node from the root node took about 45 seconds. With the new
""leaves"" latch it completes in about 7 seconds.

Briefly discussed on oqgraph-dev (thread starting at https://lists.launchpad.net/oqgraph-dev/msg00314.html), but no review was done yet (other than mtr and works for me).",,0,0,0,0,0.0,"Add ""leaves"" algorithm to OQGRAPH. $end$ This algorithm returns all reachable leaf nodes from a given origin, or all root nodes that can reach a given destination.

Currently, I had to perform a self-join on the graph table to get a list of all leaf nodes (all destids that themselves are not an origid). This becomes rather expensive the larger the graph is.
I created a simple, large graph of 2 million nodes in (essentially just a chain of nodes, so A->B->C->...) and finding the leaf node from the root node took about 45 seconds. With the new
""leaves"" latch it completes in about 7 seconds.

Briefly discussed on oqgraph-dev (thread starting at https://lists.launchpad.net/oqgraph-dev/msg00314.html), but no review was done yet (other than mtr and works for me). $acceptance criteria:$",0,0,0,0,0,0,1,5345.55,17,1,0.0588235,0,0.0,0,0.0,0,0.0,0,0.0
584,MDEV-11275,Technical task,MDEV,2016-11-12 04:00:41,,0,sql_mode=ORACLE: CAST(..AS VARCHAR(N)),"When running in {{sql_mode=ORACLE}}, MariaDB should support {{CAST(expr AS VARCHAR(N))}} and {{CAST(expr AS VARCHAR2(N))}}

{{CAST(expr AS VARCHAR(N))}} should eventually be supported not only in {{sql_mode=ORACLE}}, but in other modes as well. See MDEV-11283.

This task is limited to {{sql_mode=ORACLE}} only.
",,"sql_mode=ORACLE: CAST(..AS VARCHAR(N)) $end$ When running in {{sql_mode=ORACLE}}, MariaDB should support {{CAST(expr AS VARCHAR(N))}} and {{CAST(expr AS VARCHAR2(N))}}

{{CAST(expr AS VARCHAR(N))}} should eventually be supported not only in {{sql_mode=ORACLE}}, but in other modes as well. See MDEV-11283.

This task is limited to {{sql_mode=ORACLE}} only.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,22,,0,0,1,5,0,4,0,,0,850,0,0,0,2016-11-12 04:00:41,sql_mode=ORACLE: CAST(..AS VARCHAR(N)),"When running in {{sql_mode=ORACLE}}, MariaDB should support {{CAST(expr AS VARCHAR(N))}}.

Perhaps not only in {{sql_mode=ORACLE}}, but in other modes as well.

{{CAST(expr AS VARCHAR(N))}} is supported by:
- Oracle
- SQL Server
- PostgreSQL
- SQLite
- IBM DB2
- Sybase/SAP ASE

{{CAST(expr AS VARCHAR)}} (without length) is supported by:
- SQL Server
- PostgreSQL
- SQLite
- Sybase/SAP ASE
",,0,4,0,60,0.712121,"sql_mode=ORACLE: CAST(..AS VARCHAR(N)) $end$ When running in {{sql_mode=ORACLE}}, MariaDB should support {{CAST(expr AS VARCHAR(N))}}.

Perhaps not only in {{sql_mode=ORACLE}}, but in other modes as well.

{{CAST(expr AS VARCHAR(N))}} is supported by:
- Oracle
- SQL Server
- PostgreSQL
- SQLite
- IBM DB2
- Sybase/SAP ASE

{{CAST(expr AS VARCHAR)}} (without length) is supported by:
- SQL Server
- PostgreSQL
- SQLite
- Sybase/SAP ASE
 $acceptance criteria:$",4,1,1,1,1,1,1,0.0,38,26,0.684211,21,0.552632,21,0.552632,20,0.526316,20,0.526316
585,MDEV-11297,Task,MDEV,2016-11-16 12:42:34,,0,Add support for LIMIT clause in GROUP_CONCAT(),"GROUP_CONCAT supports ORDER BY and DISTINCT but not LIMIT clause.
Supporting LIMIT N[,M] could simplify queries.

something like

SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC),"","",1)

could be written as

GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC LIMIT 1)

this can be used in timeseries-like calculations. Suppose we have a table (date,cnt) and we want to produce a list of dates, for each year, where cnt was maximum for that year, showing such date,cnt for each year

currently this is written in an awkward manner and is also quite inefficient as we SELECT much more data that is necessary for the task:

SELECT LEFT(col1,4) AS _yyyy
, SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",DATE_FORMAT(col1,""%a %Y-%m-%d""),LPAD(col2,8,"" "")) ORDER BY col2 DESC),"","",1) AS col3
FROM t1
GROUP BY 1
;",,"Add support for LIMIT clause in GROUP_CONCAT() $end$ GROUP_CONCAT supports ORDER BY and DISTINCT but not LIMIT clause.
Supporting LIMIT N[,M] could simplify queries.

something like

SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC),"","",1)

could be written as

GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC LIMIT 1)

this can be used in timeseries-like calculations. Suppose we have a table (date,cnt) and we want to produce a list of dates, for each year, where cnt was maximum for that year, showing such date,cnt for each year

currently this is written in an awkward manner and is also quite inefficient as we SELECT much more data that is necessary for the task:

SELECT LEFT(col1,4) AS _yyyy
, SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",DATE_FORMAT(col1,""%a %Y-%m-%d""),LPAD(col2,8,"" "")) ORDER BY col2 DESC),"","",1) AS col3
FROM t1
GROUP BY 1
; $acceptance criteria:$",,Varun Gupta,Varun Gupta,Major,37,,1,10,3,6,0,2,0,,0,850,9,1,0,2016-11-18 14:20:16,add support for LIMIT clause in GROUP_CONCAT(),"GROUP_CONCAT supports ORDER BY and DISTINCT but not LIMIT clause.
Supporting LIMIT N[,M] could simplify queries.

something like

SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC),"","",1)

could be written as

GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC LIMIT 1)

this can be used in timeseries-like calculations. Suppose we have a table (date,cnt) and we want to produce a list of dates, for each year, where cnt was maximum for that year, showing such date,cnt for each year

currently this is written in an awkward manner and is also quite inefficient as we SELECT much more data that is necessary for the task:

SELECT LEFT(col1,4) AS _yyyy
, SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",DATE_FORMAT(col1,""%a %Y-%m-%d""),LPAD(col2,8,"" "")) ORDER BY col2 DESC),"","",1) AS col3
FROM t1
GROUP BY 1
;",,1,0,0,2,0.00787402,"add support for LIMIT clause in GROUP_CONCAT() $end$ GROUP_CONCAT supports ORDER BY and DISTINCT but not LIMIT clause.
Supporting LIMIT N[,M] could simplify queries.

something like

SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC),"","",1)

could be written as

GROUP_CONCAT(CONCAT_WS("":"",date,cnt) ORDER BY cnt DESC LIMIT 1)

this can be used in timeseries-like calculations. Suppose we have a table (date,cnt) and we want to produce a list of dates, for each year, where cnt was maximum for that year, showing such date,cnt for each year

currently this is written in an awkward manner and is also quite inefficient as we SELECT much more data that is necessary for the task:

SELECT LEFT(col1,4) AS _yyyy
, SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",DATE_FORMAT(col1,""%a %Y-%m-%d""),LPAD(col2,8,"" "")) ORDER BY col2 DESC),"","",1) AS col3
FROM t1
GROUP BY 1
; $acceptance criteria:$",1,1,0,0,0,0,1,49.6167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
586,MDEV-11340,Task,MDEV,2016-11-23 08:51:32,,0,Allow multiple alternative authentication methods for the same user,"In some cases there is a need to authenticate the same user in different means, more specifically multiple different authentications methods, or IDENTIFIED BY clauses for the same user. The adjusted CREATE USER syntax would allow multiple IDENTIFIED BY sections for one single user.

Questions:
* What logic should be applied? All of authentication methods should succeed? One of them should succeed? Should we allow complex rules like *(first_auth OR second_auth) AND third_auth* ? Or, instead of AND/OR may be we should go with PAM model or sufficient/required/etc?
* What syntax should be used?
* stuff like SET PASSWORD — will they be not allowed? allowed? if allowed, how will they work?
* where the new authentication rules will be stored, in what table, what columns?",,"Allow multiple alternative authentication methods for the same user $end$ In some cases there is a need to authenticate the same user in different means, more specifically multiple different authentications methods, or IDENTIFIED BY clauses for the same user. The adjusted CREATE USER syntax would allow multiple IDENTIFIED BY sections for one single user.

Questions:
* What logic should be applied? All of authentication methods should succeed? One of them should succeed? Should we allow complex rules like *(first_auth OR second_auth) AND third_auth* ? Or, instead of AND/OR may be we should go with PAM model or sufficient/required/etc?
* What syntax should be used?
* stuff like SET PASSWORD — will they be not allowed? allowed? if allowed, how will they work?
* where the new authentication rules will be stored, in what table, what columns? $acceptance criteria:$",,Anders Karlsson,Anders Karlsson,Critical,23,,2,15,10,1,0,1,0,,0,850,2,1,0,2018-05-29 07:54:07,Allow multiple alternative authentication methods for the same user,"In some cases there is a need to authenticate the same user in different means, more specifically multiple different authentications methods, or IDENTIFIED BY clauses for the same user. The adjusted CREATE USER syntax would allow multiple IDENTIFIED BY sections for one single user.

Questions:
* What logic should be applied? All of authentication methods should succeed? One of them should succeed? Should we allow complex rules like *(first_auth OR second_auth) AND third_auth* ? Or, instead of AND/OR may be we should go with PAM model or sufficient/required/etc?
* What syntax should be used?
* stuff like SET PASSWORD — will they be not allowed? allowed? if allowed, how will they work?
* where the new authentication rules will be stored, in what table, what columns?",,0,0,0,0,0.0,"Allow multiple alternative authentication methods for the same user $end$ In some cases there is a need to authenticate the same user in different means, more specifically multiple different authentications methods, or IDENTIFIED BY clauses for the same user. The adjusted CREATE USER syntax would allow multiple IDENTIFIED BY sections for one single user.

Questions:
* What logic should be applied? All of authentication methods should succeed? One of them should succeed? Should we allow complex rules like *(first_auth OR second_auth) AND third_auth* ? Or, instead of AND/OR may be we should go with PAM model or sufficient/required/etc?
* What syntax should be used?
* stuff like SET PASSWORD — will they be not allowed? allowed? if allowed, how will they work?
* where the new authentication rules will be stored, in what table, what columns? $acceptance criteria:$",0,0,0,0,0,0,0,13247.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
587,MDEV-11346,Technical task,MDEV,2016-11-24 17:55:42,,0,Move functions case_stmt_xxx and add_select_to_union_list as methods to LEX,"The task MDEV-10142 will introduce the second *.yy file, with Oracle PL/SQL grammar.
Before we add the second *.yy file, we should move the functions defined in sql_yacc.yy as methods to lex, to avoid code duplication.
Under terms of this tasks we'll move:
{code:cpp}
int case_stmt_action_expr(LEX *, Item* expr);
int case_stmt_action_when(LEX *, Item *when, bool simple);
int case_stmt_action_then(LEX *);
bool add_select_to_union_list(LEX *,bool is_union_distinct,  bool is_top_level);
{code}
",,"Move functions case_stmt_xxx and add_select_to_union_list as methods to LEX $end$ The task MDEV-10142 will introduce the second *.yy file, with Oracle PL/SQL grammar.
Before we add the second *.yy file, we should move the functions defined in sql_yacc.yy as methods to lex, to avoid code duplication.
Under terms of this tasks we'll move:
{code:cpp}
int case_stmt_action_expr(LEX *, Item* expr);
int case_stmt_action_when(LEX *, Item *when, bool simple);
int case_stmt_action_then(LEX *);
bool add_select_to_union_list(LEX *,bool is_union_distinct,  bool is_top_level);
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,12,,0,1,0,5,0,0,0,,0,850,1,0,0,2016-11-24 17:55:42,Move functions case_stmt_xxx and add_select_to_union_list as methods to LEX,"The task MDEV-10142 will introduce the second *.yy file, with Oracle PL/SQL grammar.
Before we add the second *.yy file, we should move the functions defined in sql_yacc.yy as methods to lex, to avoid code duplication.
Under terms of this tasks we'll move:
{code:cpp}
int case_stmt_action_expr(LEX *, Item* expr);
int case_stmt_action_when(LEX *, Item *when, bool simple);
int case_stmt_action_then(LEX *);
bool add_select_to_union_list(LEX *,bool is_union_distinct,  bool is_top_level);
{code}
",,0,0,0,0,0.0,"Move functions case_stmt_xxx and add_select_to_union_list as methods to LEX $end$ The task MDEV-10142 will introduce the second *.yy file, with Oracle PL/SQL grammar.
Before we add the second *.yy file, we should move the functions defined in sql_yacc.yy as methods to lex, to avoid code duplication.
Under terms of this tasks we'll move:
{code:cpp}
int case_stmt_action_expr(LEX *, Item* expr);
int case_stmt_action_when(LEX *, Item *when, bool simple);
int case_stmt_action_then(LEX *);
bool add_select_to_union_list(LEX *,bool is_union_distinct,  bool is_top_level);
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,39,27,0.692308,22,0.564103,22,0.564103,21,0.538462,21,0.538462
588,MDEV-11347,Technical task,MDEV,2016-11-24 19:40:34,,0,"Move add_create_index_prepare(), add_key_to_list(), set_trigger_new_row(), set_local_variable(), set_system_variable(), create_item_for_sp_var() as methods to LEX","This task is similar for MDEV-11346.
We'll move another banch of functions implemented in sql_yacc.yy as methods to LEX, to be able to reuse them between sql_yacc.yy and sql_yacc_ora.yy easier:
{code:cpp}
add_create_index_prepare()
add_key_to_list()
set_trigger_new_row()
set_local_variable()
set_system_variable()
create_item_for_sp_var()
{code}
",,"Move add_create_index_prepare(), add_key_to_list(), set_trigger_new_row(), set_local_variable(), set_system_variable(), create_item_for_sp_var() as methods to LEX $end$ This task is similar for MDEV-11346.
We'll move another banch of functions implemented in sql_yacc.yy as methods to LEX, to be able to reuse them between sql_yacc.yy and sql_yacc_ora.yy easier:
{code:cpp}
add_create_index_prepare()
add_key_to_list()
set_trigger_new_row()
set_local_variable()
set_system_variable()
create_item_for_sp_var()
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,12,,0,2,0,5,0,0,0,,0,850,2,0,0,2016-11-24 19:40:34,"Move add_create_index_prepare(), add_key_to_list(), set_trigger_new_row(), set_local_variable(), set_system_variable(), create_item_for_sp_var() as methods to LEX","This task is similar for MDEV-11346.
We'll move another banch of functions implemented in sql_yacc.yy as methods to LEX, to be able to reuse them between sql_yacc.yy and sql_yacc_ora.yy easier:
{code:cpp}
add_create_index_prepare()
add_key_to_list()
set_trigger_new_row()
set_local_variable()
set_system_variable()
create_item_for_sp_var()
{code}
",,0,0,0,0,0.0,"Move add_create_index_prepare(), add_key_to_list(), set_trigger_new_row(), set_local_variable(), set_system_variable(), create_item_for_sp_var() as methods to LEX $end$ This task is similar for MDEV-11346.
We'll move another banch of functions implemented in sql_yacc.yy as methods to LEX, to be able to reuse them between sql_yacc.yy and sql_yacc_ora.yy easier:
{code:cpp}
add_create_index_prepare()
add_key_to_list()
set_trigger_new_row()
set_local_variable()
set_system_variable()
create_item_for_sp_var()
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,40,27,0.675,22,0.55,22,0.55,21,0.525,21,0.525
589,MDEV-11369,Task,MDEV,2016-11-28 10:36:14,,0,Instant add column for InnoDB,"Add support for instant add column for InnoDB when adding a new column with a default value last.

We have got patches from Alibaba and TenCent for doing this.  We should choose either one or merge them.

The idea in both patches is to add a new row type to InnoDB where the number of columns
are stored in each row in 1-2 bytes.  When creating a row we store the current number of columns.  This allows us to fill any extra columns with default values on read.",,"Instant add column for InnoDB $end$ Add support for instant add column for InnoDB when adding a new column with a default value last.

We have got patches from Alibaba and TenCent for doing this.  We should choose either one or merge them.

The idea in both patches is to add a new row type to InnoDB where the number of columns
are stored in each row in 1-2 bytes.  When creating a row we store the current number of columns.  This allows us to fill any extra columns with default values on read. $acceptance criteria:$",,Michael Widenius,Michael Widenius,Major,16,,60,18,67,2,0,1,0,,0,850,12,1,0,2016-12-22 10:10:07,Instant add column for InnoDB,"Add support for instant add column for InnoDB when adding a new column with a default value last.

We have got patches from Alibaba and TenCent for doing this.  We should choose either one or merge them.

The idea in both patches is to add a new row type to InnoDB where the number of columns
are stored in each row in 1-2 bytes.  When creating a row we store the current number of columns.  This allows us to fill any extra columns with default values on read.",,0,0,0,0,0.0,"Instant add column for InnoDB $end$ Add support for instant add column for InnoDB when adding a new column with a default value last.

We have got patches from Alibaba and TenCent for doing this.  We should choose either one or merge them.

The idea in both patches is to add a new row type to InnoDB where the number of columns
are stored in each row in 1-2 bytes.  When creating a row we store the current number of columns.  This allows us to fill any extra columns with default values on read. $acceptance criteria:$",0,0,0,0,0,0,1,575.55,7,3,0.428571,3,0.428571,3,0.428571,3,0.428571,2,0.285714
590,MDEV-11371,Task,MDEV,2016-11-29 07:47:07,,0,Big column compressed,"Storage engine independent support for column compression.

TINYBLOB, BLOB, MEDIUMBLOB, LONGBLOB, TINYTEXT, TEXT, MEDIUMTEXT, LONGTEXT,
VARCHAR and VARBINARY columns can be compressed.

New COMPRESSED column attribute added:
COMPRESSED[=<compression_method>]

The only supported method currently is zlib. It is not possible to create index over compressed column.
CSV storage engine stores compressed field data uncompressed on disk.
Binary log stores compressed field data compressed on disk.

System variables added:
column_compression_threshold - Minimum column data length eligible for compression.
column_compression_zlib_level - zlib compression level (1 gives best speed, 9 gives best compression).
column_compression_zlib_strategy - The strategy parameter is used to tune the compression algorithm. Use the value DEFAULT_STRATEGY for normal data, FILTERED for data produced by a filter (or predictor), HUFFMAN_ONLY to force Huffman encoding only (no string match), or RLE to limit match distances to one (run-length encoding). Filtered data consists mostly of small values with a somewhat random distribution. In this case, the compression algorithm is tuned to compress them better. The effect of FILTERED is to force more Huffman coding and less string matching; it is somewhat intermediate between DEFAULT_STRATEGY and HUFFMAN_ONLY.  RLE is designed to be almost as fast as HUFFMAN_ONLY, but give better compression for PNG image data. The strategy parameter only affects the compression ratio but not the correctness of the compressed output even if it is not set appropriately. FIXED prevents the use of dynamic Huffman codes, allowing for a simpler decoder for special applications.
column_compression_zlib_wrap - Generate zlib header and trailer and compute adler32 check value. It can be used with storage engines that don't provide data integrity verification to detect data corruption.

Status variables added:
Column_compressions - incremented every time field data is compressed.
Column_decompressions - incremented every time field data is decompressed.",,"Big column compressed $end$ Storage engine independent support for column compression.

TINYBLOB, BLOB, MEDIUMBLOB, LONGBLOB, TINYTEXT, TEXT, MEDIUMTEXT, LONGTEXT,
VARCHAR and VARBINARY columns can be compressed.

New COMPRESSED column attribute added:
COMPRESSED[=<compression_method>]

The only supported method currently is zlib. It is not possible to create index over compressed column.
CSV storage engine stores compressed field data uncompressed on disk.
Binary log stores compressed field data compressed on disk.

System variables added:
column_compression_threshold - Minimum column data length eligible for compression.
column_compression_zlib_level - zlib compression level (1 gives best speed, 9 gives best compression).
column_compression_zlib_strategy - The strategy parameter is used to tune the compression algorithm. Use the value DEFAULT_STRATEGY for normal data, FILTERED for data produced by a filter (or predictor), HUFFMAN_ONLY to force Huffman encoding only (no string match), or RLE to limit match distances to one (run-length encoding). Filtered data consists mostly of small values with a somewhat random distribution. In this case, the compression algorithm is tuned to compress them better. The effect of FILTERED is to force more Huffman coding and less string matching; it is somewhat intermediate between DEFAULT_STRATEGY and HUFFMAN_ONLY.  RLE is designed to be almost as fast as HUFFMAN_ONLY, but give better compression for PNG image data. The strategy parameter only affects the compression ratio but not the correctness of the compressed output even if it is not set appropriately. FIXED prevents the use of dynamic Huffman codes, allowing for a simpler decoder for special applications.
column_compression_zlib_wrap - Generate zlib header and trailer and compute adler32 check value. It can be used with storage engines that don't provide data integrity verification to detect data corruption.

Status variables added:
Column_compressions - incremented every time field data is compressed.
Column_decompressions - incremented every time field data is decompressed. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,10,,8,1,9,1,0,2,2,,0,850,1,0,0,2017-07-06 20:42:21,Big column compressed(innodb),"Some big columns(blob/text/varchar/varbinary) waste a lot of space, we introduce “compressed” into column definition when create or alter a table.
When a column was defined as a compressed column, the column data will be compressed using zlib (other compress algorithm not support yet).
We could get a better compression ratio and performance, more flexibility (vs compressed row format)
For example:
Create table tcompress (
C1 int,
C2 blob compressed,
C3 text compressed,
C4 text) engine = innodb

We achieve this 'big columns compress' function by following step:

# Support 'compressed' syntax, and save this attribute in .frm file
# Store the 'compressed' attribute in innodb layer, we add DATA_IS_COMPRESSED flag in prtype
# If needed , we do compress in row_mysql_store_col_in_innobase_format and do decompress in row_sel_store_mysql_field
# Use a compress header to control how to compress/decompress.
Compress Header is 1 Byte,
7 Bit: Always 1, mean compressed;
5-6 Bit: Compressed algorithm - Always 0, means zlib.It maybe support other compression algorithm in the future.
0-3 Bit: Bytes of ""Record Original Length""
Record Original Length: 1-4 Bytes*/

We add system global variable 'field_compress_min_len' was used to control that only compress the column if the data length exceeds 'field_compress_min_len'. Default 128.
Also we add 3 error num:
ER_FIELD_TYPE_NOT_ALLOWED_AS_COMPRESSED_FIELD: support text/blob/varchar/varbinary column has compress attribute only.
ER_FIELD_CAN_NOT_COMPRESSED_AND_INDEX: column has compress attribute can not be an index
ER_FIELD_CAN_NOT_COMPRESSED_IN_CURRENT_ENGINESS: column compress be supported in innodb only",,1,1,0,506,1.18143,"Big column compressed(innodb) $end$ Some big columns(blob/text/varchar/varbinary) waste a lot of space, we introduce “compressed” into column definition when create or alter a table.
When a column was defined as a compressed column, the column data will be compressed using zlib (other compress algorithm not support yet).
We could get a better compression ratio and performance, more flexibility (vs compressed row format)
For example:
Create table tcompress (
C1 int,
C2 blob compressed,
C3 text compressed,
C4 text) engine = innodb

We achieve this 'big columns compress' function by following step:

# Support 'compressed' syntax, and save this attribute in .frm file
# Store the 'compressed' attribute in innodb layer, we add DATA_IS_COMPRESSED flag in prtype
# If needed , we do compress in row_mysql_store_col_in_innobase_format and do decompress in row_sel_store_mysql_field
# Use a compress header to control how to compress/decompress.
Compress Header is 1 Byte,
7 Bit: Always 1, mean compressed;
5-6 Bit: Compressed algorithm - Always 0, means zlib.It maybe support other compression algorithm in the future.
0-3 Bit: Bytes of ""Record Original Length""
Record Original Length: 1-4 Bytes*/

We add system global variable 'field_compress_min_len' was used to control that only compress the column if the data length exceeds 'field_compress_min_len'. Default 128.
Also we add 3 error num:
ER_FIELD_TYPE_NOT_ALLOWED_AS_COMPRESSED_FIELD: support text/blob/varchar/varbinary column has compress attribute only.
ER_FIELD_CAN_NOT_COMPRESSED_AND_INDEX: column has compress attribute can not be an index
ER_FIELD_CAN_NOT_COMPRESSED_IN_CURRENT_ENGINESS: column compress be supported in innodb only $acceptance criteria:$",2,1,1,1,1,1,1,5268.92,18,1,0.0555556,0,0.0,0,0.0,0,0.0,0,0.0
591,MDEV-11424,Task,MDEV,2016-11-30 09:40:56,,0,Instant ALTER TABLE of failure-free record format changes,"This is an umbrella task for allowing {{ALTER TABLE}} to be instantaneous in cases that cannot fail due to existing records being incompatible with the altered table definition. Later, MDEV-16356 and MDEV-16291 could extend this to support {{ALGORITHM=NOCOPY}} for operations that can avoid rebuilding tables, but need to validate the data.

h2. Introduction

Traditionally, {{ALTER TABLE}} would be roughly equivalent to the following SQL statements:
{code:sql}
CREATE TABLE `#sql-…` (…);
INSERT INTO `#sql-…` SELECT … FROM t;
RENAME TABLE t TO `#sql2-…`, `#sql-…` TO t;
DROP TABLE `#sql2-…`;
{code}
This mode of operation is still available by specifying {{ALGORITHM=COPY}} or {{SET old_alter_table=1}} (or starting with MDEV-13134 in MariaDB 10.3, {{SET alter_algorithm=copy}}).

Copying a table and rebuilding all its indexes can be a very expensive operation. While InnoDB mostly allows tables to be rebuilt online since MariaDB Server version 10.0, the temporary files can occupy a significant amount of space and I/O capacity.

There are cases where the data format is not affected by the {{ALTER TABLE}} operation; only metadata needs to be updated. Examples include renaming columns, changing the default values of columns, and changing the maximum length of a {{VARCHAR}} column such that the storage format does not change.

A goal for MariaDB Server is to allow instantaneous execution of any {{ALTER TABLE}} operation where data conversions cannot fail, and indexes do not need to be rebuilt. Even in cases where some affected indexes have to be rebuilt, it will be more efficient to only rebuild some indexes than to copy the whole table.

The goal can be reformulated as: Avoid rebuilding the table.

How to avoid rebuilding the table, if the underlying storage format would be affected by the {{ALTER TABLE}} operation? By extending the storage format in a way that allows the data to be in ‘non-canonical’ format. The main examples of this are MDEV-11369 and MDEV-15562, which implement {{ADD COLUMN}}, {{DROP COLUMN}} and changing the order of columns.

The original InnoDB storage format (retroactively named {{ROW_FORMAT=REDUNDANT}}) is very generic, basically allowing {{NULL}} values and arbitrary length for every column. For it, MDEV-15562 would be the only storage format change needed to avoid unnecessary rebuild of the table.

Note: Whenever the {{PRIMARY KEY}} is changed, all indexes will have to be rebuilt. Likewise, some operations on indexed columns may require the indexes to be rebuilt.

The space-optimized row formats {{COMPACT}} and {{DYNAMIC}} omit ‘is null’ flags for {{NOT NULL}} columns and length information for fixed-length columns. MDEV-17520 could extend MDEV-15563 to these {{ROW_FORMAT}} by using a more flexible encoding for all clustered index pages that have been modified since the {{ALTER TABLE}} operation.

Operations that involve adding or dropping indexes (also {{DROP COLUMN}} can imply this) will not be supported for {{ALGORITHM=INSTANT}}; they will be supported with {{ALGORITHM=NOCOPY}}. {{ALTER TABLE…ADD \[UNIQUE\] INDEX}} supports concurrent modifications to the table since MariaDB 10.0. MDEV-16223 could defer {{ADD INDEX}} to a background operation.

Operations that will continue to be refused by {{ALGORITHM=INSTANT}} (and {{ALGORITHM=NOCOPY}} even after MDEV-16291) include:
* Changing {{ROW_FORMAT}} or {{ENGINE}}
* Altering a table that is in {{ROW_FORMAT=COMPRESSED}}
* Dropping, adding or changing {{PRIMARY KEY}} columns, or {{ADD}}/{{DROP PRIMARY KEY}}

Any {{ALTER TABLE}} that would be refused with {{ALGORITHM=NOCOPY}} (anything that rebuilds the clustered index) will drop any ‘instant {{ALTER TABLE}}’ metadata. The metadata would also be deleted if a rebuild is explicitly requested by the use of the {{FORCE}} keyword.

h2. Metadata format changes
In InnoDB, instant {{ALTER TABLE}} affects clustered index page leaf records only. The data dictionary will reflect the most recent table definition. Additional metadata for interpreting records that correspond to an earlier version of the table definition will be stored in the clustered index tree as follows.

* MDEV-11369 in MariaDB 10.3.4 changed the root page type code to {{FIL_PAGE_TYPE_INSTANT}} to indicate that instant {{ALTER TABLE}} has been used. It also introduced a hidden metadata record at the start of the clustered index, on the leftmost leaf page. The new page type code prevents older MariaDB versions from opening the table.
* MDEV-15562 slightly modifies the format of the metadata record to represent dropped and reordered index fields. (All columns will internally be added last in the user records in the clustered index leaf pages.) MariaDB 10.3 will refuse to open such tables, because new {{info_bits}} will be set in the metadata record.

h2. Data format changes
User records in the clustered index leaf pages will have to indicate which format they correspond to.
* For other than {{ROW_FORMAT=REDUNDANT}}, MDEV-11369 introduced {{REC_STATUS_COLUMNS_ADDED}} that indicates the presence of an optional record header that encodes the number of 'instantly added' columns that are present in the record.
* For {{ROW_FORMAT=REDUNDANT}}, MDEV-11369 simply stores the number of fields in the record header.
* In MDEV-11369 and MDEV-15562, any 'instantly added' columns whose values are missing from the end of the clustered index record will be substituted with the values stored in the metadata record.
* MDEV-15562 will not change the user record format in any way. Instantly added columns are always added as last fields in the clustered index leaf page records.
* MDEV-17520 would allow clustered index leaf pages to be in a format where the metadata version of each record is identified.

h2. A note on MVCC
Because {{ha_innobase::commit_inplace_alter_table()}} will be invoked while holding {{MDL_EXCLUSIVE}}, any transactions that read or modified the table must finish before the {{ALTER TABLE}} can commit. But it is possible that some old transaction tries to do its first access to the table after the {{ALTER TABLE}} committed. Such transactions may receive an error message 'table definition changed', as noted in [MySQL Bug#28432|https://bugs.mysql.com/bug.php?id=28432]. It would be too much effort to support MVCC if a transaction after {{ALTER}} modified a record (converting it to newer dictionary version) that would otherwise be visible to the old transaction.
Here is the scenario in SQL:
{code:SQL}
connection con1;
START TRANSACTION WITH CONSISTENT SNAPSHOT; -- creates read view
connection con2;
ALTER TABLE t CHANGE COLUMN b b INT NULL;
UPDATE t SET b=1 WHERE a=100;
connection con1;
SELECT * FROM t WHERE a=1; -- might be OK, if the record is still in old format
SELECT * FROM t WHERE a=100; -- error: record is in to new format
{code}
For simplicity and consistency, we could always return an error to the SELECT statements (after any {{ALTER TABLE}}).

h1. {{ALTER TABLE}} operations that potentially affect the format of a row
In MariaDB Server 10.2, the following {{alter_table_operations}} might require a table to be rebuilt:
||alter_table_operations||SQL||Remarks||
|ALTER_ADD_STORED_BASE_COLUMN|{color:green}ADD COLUMN{color}|MDEV-11369 (10.3)|
|ALTER_ADD_VIRTUAL_COLUMN|{color:green}ADD COLUMN…AS{color}|Virtual columns are always added instantly|
|ALTER_ADD_STORED_GENERATED_COLUMN|{color:red}ADD COLUMN…PERSISTENT AS{color}|cannot be instant; until MDEV-16354 requires rebuild with {{ALGORITHM=COPY}}|
|ALTER_ADD_PK_INDEX|{color:red}ADD PRIMARY KEY{color}|Requires all indexes to be rebuilt.|
|ALTER_DROP_PK_INDEX|{color:red}DROP PRIMARY KEY{color}|Requires all indexes to be rebuilt. Without {{ADD PRIMARY KEY}}, cannot even support online rebuilding ({{LOCK=NONE}}).|
|ALTER_CHANGE_CREATE_OPTION|{color:red}ROW_FORMAT, KEY_BLOCK_SIZE, encryption{color}|Requires rebuild; see MDEV-16291.|
|ALTER_CHANGE_CREATE_OPTION|{color:green}page_compressed, page_compression_level{color}|MDEV-16328|
|ALTER_COLUMN_NULLABLE|{color:green}NULL{color}|MDEV-15563 for {{ROW_FORMAT=REDUNDANT}}|
|ALTER_COLUMN_NULLABLE|{color:darkgreen}NULL{color}|MDEV-17520 for {{COMPACT}} and {{DYNAMIC}}|
|ALTER_COLUMN_NOT_NULLABLE|{color:blue}NOT NULL{color}|MDEV-16291|
|ALTER_STORED_COLUMN_ORDER|{color:green}FIRST, LAST, AFTER{color}|MDEV-15562|
|ALTER_DROP_STORED_COLUMN|{color:darkgreen}DROP COLUMN{color}|MDEV-15562|
|ALTER_RECREATE_TABLE|{color:red}FORCE{color}|the sole purpose of this keyword is to explicitly request rebuild|
|ALTER_STORED_COLUMN_TYPE|{color:darkgreen}CHANGE to wider type{color}|MDEV-15564,MDEV-17520,MDEV-27864|
|ALTER_STORED_COLUMN_TYPE|{color:blue}CHANGE type{color}|MDEV-16291|
|ALTER_VIRTUAL_COLUMN_TYPE|{color:blue}CHANGE type{color}|MDEV-16332|
|ALTER_STORED_GCOL_EXPR|{color:blue}CHANGE expr{color}|MDEV-17035|
|ALTER_COLUMN_UNVERSIONED|{color:green}CHANGE…WITH\[OUT\] SYSTEM VERSIONING{color}|MDEV-16330|
|ALTER_ADD_SYSTEM_VERSIONING|{color:red}ADD SYSTEM VERSIONING{color}|Must rebuild the {{PRIMARY KEY}} and thus the full table|
|ALTER_DROP_SYSTEM_VERSIONING|{color:red}DROP SYSTEM VERSIONING{color}|Must rebuild the {{PRIMARY KEY}} and thus the full table|
|ALTER_ADD_PERIOD|{color:red}ADD PERIOD FOR SYSTEM TIME{color}|must be combined with {{ADD SYSTEM VERSIONING}}|
|ALTER_DROP_PERIOD|{color:red}DROP PERIOD FOR SYSTEM TIME{color}|must be combined with {{DROP SYSTEM VERSIONING}}|
|ALTER_ADD_CHECK_CONSTRAINT|{color:blue}ADD \[CONSTRAINT\] CHECK{color}|MDEV-16356|
|ALTER_DROP_CHECK_CONSTRAINT|{color:green}DROP CONSTRAINT{color}|MDEV-16331|

Legend:
* {color:green}can be performed instantly{color}
* {color:darkgreen}can be performed instantly, except if any secondary indexes need to be rebuilt{color}
* {color:blue}not instantaneous; could later be performed without rebuild, with validation{color}
* {color:red}will continue to require full table rebuild{color}",,"Instant ALTER TABLE of failure-free record format changes $end$ This is an umbrella task for allowing {{ALTER TABLE}} to be instantaneous in cases that cannot fail due to existing records being incompatible with the altered table definition. Later, MDEV-16356 and MDEV-16291 could extend this to support {{ALGORITHM=NOCOPY}} for operations that can avoid rebuilding tables, but need to validate the data.

h2. Introduction

Traditionally, {{ALTER TABLE}} would be roughly equivalent to the following SQL statements:
{code:sql}
CREATE TABLE `#sql-…` (…);
INSERT INTO `#sql-…` SELECT … FROM t;
RENAME TABLE t TO `#sql2-…`, `#sql-…` TO t;
DROP TABLE `#sql2-…`;
{code}
This mode of operation is still available by specifying {{ALGORITHM=COPY}} or {{SET old_alter_table=1}} (or starting with MDEV-13134 in MariaDB 10.3, {{SET alter_algorithm=copy}}).

Copying a table and rebuilding all its indexes can be a very expensive operation. While InnoDB mostly allows tables to be rebuilt online since MariaDB Server version 10.0, the temporary files can occupy a significant amount of space and I/O capacity.

There are cases where the data format is not affected by the {{ALTER TABLE}} operation; only metadata needs to be updated. Examples include renaming columns, changing the default values of columns, and changing the maximum length of a {{VARCHAR}} column such that the storage format does not change.

A goal for MariaDB Server is to allow instantaneous execution of any {{ALTER TABLE}} operation where data conversions cannot fail, and indexes do not need to be rebuilt. Even in cases where some affected indexes have to be rebuilt, it will be more efficient to only rebuild some indexes than to copy the whole table.

The goal can be reformulated as: Avoid rebuilding the table.

How to avoid rebuilding the table, if the underlying storage format would be affected by the {{ALTER TABLE}} operation? By extending the storage format in a way that allows the data to be in ‘non-canonical’ format. The main examples of this are MDEV-11369 and MDEV-15562, which implement {{ADD COLUMN}}, {{DROP COLUMN}} and changing the order of columns.

The original InnoDB storage format (retroactively named {{ROW_FORMAT=REDUNDANT}}) is very generic, basically allowing {{NULL}} values and arbitrary length for every column. For it, MDEV-15562 would be the only storage format change needed to avoid unnecessary rebuild of the table.

Note: Whenever the {{PRIMARY KEY}} is changed, all indexes will have to be rebuilt. Likewise, some operations on indexed columns may require the indexes to be rebuilt.

The space-optimized row formats {{COMPACT}} and {{DYNAMIC}} omit ‘is null’ flags for {{NOT NULL}} columns and length information for fixed-length columns. MDEV-17520 could extend MDEV-15563 to these {{ROW_FORMAT}} by using a more flexible encoding for all clustered index pages that have been modified since the {{ALTER TABLE}} operation.

Operations that involve adding or dropping indexes (also {{DROP COLUMN}} can imply this) will not be supported for {{ALGORITHM=INSTANT}}; they will be supported with {{ALGORITHM=NOCOPY}}. {{ALTER TABLE…ADD \[UNIQUE\] INDEX}} supports concurrent modifications to the table since MariaDB 10.0. MDEV-16223 could defer {{ADD INDEX}} to a background operation.

Operations that will continue to be refused by {{ALGORITHM=INSTANT}} (and {{ALGORITHM=NOCOPY}} even after MDEV-16291) include:
* Changing {{ROW_FORMAT}} or {{ENGINE}}
* Altering a table that is in {{ROW_FORMAT=COMPRESSED}}
* Dropping, adding or changing {{PRIMARY KEY}} columns, or {{ADD}}/{{DROP PRIMARY KEY}}

Any {{ALTER TABLE}} that would be refused with {{ALGORITHM=NOCOPY}} (anything that rebuilds the clustered index) will drop any ‘instant {{ALTER TABLE}}’ metadata. The metadata would also be deleted if a rebuild is explicitly requested by the use of the {{FORCE}} keyword.

h2. Metadata format changes
In InnoDB, instant {{ALTER TABLE}} affects clustered index page leaf records only. The data dictionary will reflect the most recent table definition. Additional metadata for interpreting records that correspond to an earlier version of the table definition will be stored in the clustered index tree as follows.

* MDEV-11369 in MariaDB 10.3.4 changed the root page type code to {{FIL_PAGE_TYPE_INSTANT}} to indicate that instant {{ALTER TABLE}} has been used. It also introduced a hidden metadata record at the start of the clustered index, on the leftmost leaf page. The new page type code prevents older MariaDB versions from opening the table.
* MDEV-15562 slightly modifies the format of the metadata record to represent dropped and reordered index fields. (All columns will internally be added last in the user records in the clustered index leaf pages.) MariaDB 10.3 will refuse to open such tables, because new {{info_bits}} will be set in the metadata record.

h2. Data format changes
User records in the clustered index leaf pages will have to indicate which format they correspond to.
* For other than {{ROW_FORMAT=REDUNDANT}}, MDEV-11369 introduced {{REC_STATUS_COLUMNS_ADDED}} that indicates the presence of an optional record header that encodes the number of 'instantly added' columns that are present in the record.
* For {{ROW_FORMAT=REDUNDANT}}, MDEV-11369 simply stores the number of fields in the record header.
* In MDEV-11369 and MDEV-15562, any 'instantly added' columns whose values are missing from the end of the clustered index record will be substituted with the values stored in the metadata record.
* MDEV-15562 will not change the user record format in any way. Instantly added columns are always added as last fields in the clustered index leaf page records.
* MDEV-17520 would allow clustered index leaf pages to be in a format where the metadata version of each record is identified.

h2. A note on MVCC
Because {{ha_innobase::commit_inplace_alter_table()}} will be invoked while holding {{MDL_EXCLUSIVE}}, any transactions that read or modified the table must finish before the {{ALTER TABLE}} can commit. But it is possible that some old transaction tries to do its first access to the table after the {{ALTER TABLE}} committed. Such transactions may receive an error message 'table definition changed', as noted in [MySQL Bug#28432|https://bugs.mysql.com/bug.php?id=28432]. It would be too much effort to support MVCC if a transaction after {{ALTER}} modified a record (converting it to newer dictionary version) that would otherwise be visible to the old transaction.
Here is the scenario in SQL:
{code:SQL}
connection con1;
START TRANSACTION WITH CONSISTENT SNAPSHOT; -- creates read view
connection con2;
ALTER TABLE t CHANGE COLUMN b b INT NULL;
UPDATE t SET b=1 WHERE a=100;
connection con1;
SELECT * FROM t WHERE a=1; -- might be OK, if the record is still in old format
SELECT * FROM t WHERE a=100; -- error: record is in to new format
{code}
For simplicity and consistency, we could always return an error to the SELECT statements (after any {{ALTER TABLE}}).

h1. {{ALTER TABLE}} operations that potentially affect the format of a row
In MariaDB Server 10.2, the following {{alter_table_operations}} might require a table to be rebuilt:
||alter_table_operations||SQL||Remarks||
|ALTER_ADD_STORED_BASE_COLUMN|{color:green}ADD COLUMN{color}|MDEV-11369 (10.3)|
|ALTER_ADD_VIRTUAL_COLUMN|{color:green}ADD COLUMN…AS{color}|Virtual columns are always added instantly|
|ALTER_ADD_STORED_GENERATED_COLUMN|{color:red}ADD COLUMN…PERSISTENT AS{color}|cannot be instant; until MDEV-16354 requires rebuild with {{ALGORITHM=COPY}}|
|ALTER_ADD_PK_INDEX|{color:red}ADD PRIMARY KEY{color}|Requires all indexes to be rebuilt.|
|ALTER_DROP_PK_INDEX|{color:red}DROP PRIMARY KEY{color}|Requires all indexes to be rebuilt. Without {{ADD PRIMARY KEY}}, cannot even support online rebuilding ({{LOCK=NONE}}).|
|ALTER_CHANGE_CREATE_OPTION|{color:red}ROW_FORMAT, KEY_BLOCK_SIZE, encryption{color}|Requires rebuild; see MDEV-16291.|
|ALTER_CHANGE_CREATE_OPTION|{color:green}page_compressed, page_compression_level{color}|MDEV-16328|
|ALTER_COLUMN_NULLABLE|{color:green}NULL{color}|MDEV-15563 for {{ROW_FORMAT=REDUNDANT}}|
|ALTER_COLUMN_NULLABLE|{color:darkgreen}NULL{color}|MDEV-17520 for {{COMPACT}} and {{DYNAMIC}}|
|ALTER_COLUMN_NOT_NULLABLE|{color:blue}NOT NULL{color}|MDEV-16291|
|ALTER_STORED_COLUMN_ORDER|{color:green}FIRST, LAST, AFTER{color}|MDEV-15562|
|ALTER_DROP_STORED_COLUMN|{color:darkgreen}DROP COLUMN{color}|MDEV-15562|
|ALTER_RECREATE_TABLE|{color:red}FORCE{color}|the sole purpose of this keyword is to explicitly request rebuild|
|ALTER_STORED_COLUMN_TYPE|{color:darkgreen}CHANGE to wider type{color}|MDEV-15564,MDEV-17520,MDEV-27864|
|ALTER_STORED_COLUMN_TYPE|{color:blue}CHANGE type{color}|MDEV-16291|
|ALTER_VIRTUAL_COLUMN_TYPE|{color:blue}CHANGE type{color}|MDEV-16332|
|ALTER_STORED_GCOL_EXPR|{color:blue}CHANGE expr{color}|MDEV-17035|
|ALTER_COLUMN_UNVERSIONED|{color:green}CHANGE…WITH\[OUT\] SYSTEM VERSIONING{color}|MDEV-16330|
|ALTER_ADD_SYSTEM_VERSIONING|{color:red}ADD SYSTEM VERSIONING{color}|Must rebuild the {{PRIMARY KEY}} and thus the full table|
|ALTER_DROP_SYSTEM_VERSIONING|{color:red}DROP SYSTEM VERSIONING{color}|Must rebuild the {{PRIMARY KEY}} and thus the full table|
|ALTER_ADD_PERIOD|{color:red}ADD PERIOD FOR SYSTEM TIME{color}|must be combined with {{ADD SYSTEM VERSIONING}}|
|ALTER_DROP_PERIOD|{color:red}DROP PERIOD FOR SYSTEM TIME{color}|must be combined with {{DROP SYSTEM VERSIONING}}|
|ALTER_ADD_CHECK_CONSTRAINT|{color:blue}ADD \[CONSTRAINT\] CHECK{color}|MDEV-16356|
|ALTER_DROP_CHECK_CONSTRAINT|{color:green}DROP CONSTRAINT{color}|MDEV-16331|

Legend:
* {color:green}can be performed instantly{color}
* {color:darkgreen}can be performed instantly, except if any secondary indexes need to be rebuilt{color}
* {color:blue}not instantaneous; could later be performed without rebuild, with validation{color}
* {color:red}will continue to require full table rebuild{color} $acceptance criteria:$",,Marko Mäkelä,Marko Mäkelä,Critical,25,,14,5,21,1,0,13,0,,0,850,4,2,0,2018-05-29 07:53:07,Instant ALTER TABLE of failure-free record format changes,"Implement instant ALTER TABLE operations where the specification of non-indexed columns is changed, for the cases that cannot be handled more efficiently. The special cases with efficient handling are:
* MDEV-11369 Instant {{ADD COLUMN…LAST}}
* MDEV-15562 Instant {{DROP COLUMN}}, {{ADD COLUMN}} anywhere, changing the order of columns
* MDEV-15563 Instant {{NOT NULL}} removal and {{CHAR}} or {{VARCHAR}} extension for {{ROW_FORMAT=REDUNDANT}}
* MDEV-15564 Avoid table rebuild in {{ALTER TABLE}} on collation or charset changes

Examples of ‘generic’ failure-free record format change (covered by this work) include:
* Instant {{NOT NULL}} removal and {{CHAR}} or {{VARCHAR}} extension for other than {{ROW_FORMAT=REDUNDANT}}
* Changing a column from {{TINYINT}} to {{INT}} or {{INT}} to {{BIGINT}}

Some operations could avoid data changes, but would not be instantaneous due to the need to validate the data. MDEV-16291 can allow them with {{ALGORITHM=NOCOPY}}.

Operations that involve adding or dropping indexes (also {{DROP COLUMN}} can imply this) are outside the scope of this work; they are supported with {{ALGORITHM=NOCOPY}}. {{ALTER TABLE…ADD \[UNIQUE\] INDEX}} supports concurrent modifications to the table since MariaDB 10.0. MDEV-16223 would defer {{ADD INDEX}} to a background operation.

Operations that will continue to be refused by {{ALGORITHM=INSTANT}} and {{ALGORITHM=NOCOPY}} include:
* Changing {{ROW_FORMAT}} or {{ENGINE}}
* Altering a table that is in {{ROW_FORMAT=COMPRESSED}}
* Dropping, adding or changing {{PRIMARY KEY}} columns, or {{ADD}}/{{DROP PRIMARY KEY}}

Any {{ALTER TABLE}} that would be refused by {{ALGORITHM=NOCOPY}} (anything that rebuilds the clustered index) will drop any ‘instant {{ALTER TABLE}}’ history. The history would also be deleted if a rebuild is explicitly requested by the use of the {{FORCE}} keyword.

h2. Metadata format changes
In InnoDB, instant {{ALTER TABLE}} affects clustered index page leaf records only. The data dictionary will reflect the most recent table definition. Additional metadata for interpreting records that correspond to an earlier version of the table definition will be stored in the clustered index tree as follows.

* MDEV-11369 in MariaDB 10.3.4 changed the root page type code to {{FIL_PAGE_TYPE_INSTANT}} to indicate that instant {{ALTER TABLE}} has been used. It also introduced a hidden 'default row' record at the start of the clustered index, on the leftmost leaf page. The new page type code prevents older MariaDB versions from opening the table.
* MDEV-15562 will slightly modify the format of the 'default row' record to represent dropped and reordered index fields. (All columns will internally be added last in the user records in the clustered index leaf pages.) MariaDB 10.3 will refuse to open such tables, because new info_bits will be set in the 'default row' record.
* This work will extend the 'default row' record further, to store 'conversion recipes' that make it possible to process records that correspond to an earlier definition of the table. To indicate the presence of the recipes in the 'default row', a new bit in info_bits will be set, and an additional BLOB column is appended to the record for storing the recipes.

h2. Data format changes
User records in the clustered index leaf pages will have to indicate which format they correspond to.
* For other than ROW_FORMAT=REDUNDANT, MDEV-11369 introduced {{REC_STATUS_COLUMNS_ADDED}} that indicates the presence of an optional record header that encodes the number of 'instantly added' columns that are present in the record.
* For ROW_FORMAT=REDUNDANT, MDEV-11369 simply stores the number of fields in the record header.
* In MDEV-11369 and MDEV-15562, any 'instantly added' columns whose values are missing from the end of the clustered index record will be substituted with the values stored in the 'default row' record.
* MDEV-15562 will not change the user record format in any way.

In this work, we must store a 'conversion recipe number' in the record or page header.
* We can repurpose the 64-bit page header field {{PAGE_MAX_TRX_ID}}. MDEV-6076 repurposed it for {{PAGE_ROOT_AUTO_INC}} in the root page. If the table consists of a single root page or it later shrinks to a single page, we can simply convert the table to non-instant format. Having a per-page format would require all records of the page to be converted to the newest format on any write.
* In the record header, there are few spare bits; for {{ROW_FORMAT=REDUNDANT}} 3 bits, and for {{ROW_FORMAT=COMPACT}} (the MySQL 5.0 default) and {{ROW_FORMAT=DYNAMIC}} (MySQL 5.7/MariaDB 10.2 default) possibly 5 bits. We could use these to support a small maximum number of conversion recipes.

We may choose to limit the number of conversion recipes that can be supported. If an {{ALTER TABLE}} operation would require a conversion recipe, but there is no space to store one, the operation would be refused with {{ALGORITHM=NOCOPY}} or {{ALGORITHM=INSTANT}}. Once the table has been rebuilt, the maximum number of conversion recipes will be available again.

The special cases (MDEV-11369 {{ADD COLUMN}}, MDEV-15562 {{DROP COLUMN}}, and other operations where the interpretation of existing user records can be changed) will not generate conversion recipes.

h2. How it works
The 'default row' record needs to store the conversion recipes that allow each clustered index leaf page record format version to be converted to the latest table definition. These recipes will be cached in {{dict_table_t}}. The 'default row' record will only be read when the table definition is first loaded to the data dictionary cache.

Adding or dropping indexes or renaming columns or indexes would not touch the 'default row' record.

If we store recipes for converting from version 1 to latest, 2 to latest, and so on, then each instant {{ALTER}} will have to rewrite the old recipes, so that they will work from 1 to latest+1, 2 to latest+1 and so on, and finally add a recipe from latest to latest+1. Because we will store the recipes in a BLOB, and because BLOBs are copy-on-write in InnoDB, all conversion recipe pages will have to be rewritten in any case.

Read and write access to the clustered index would be affected as follows:
Writes would always use the latest dictionary version. If we use per-page identifier (instead of per-record) and the page that is being written to contains other old-format records, we could convert all of them while we are modifying the page.

Reads would have to be prepared to convert records from old format to the latest one.

h2. Possible future work: Cleaning up conversion recipes
There could be a separate online operation for clean-up, such as {{OPTIMIZE INDEX PRIMARY ON t1}}, which would also defragment the index. This would be useful if writes to the table are rare, because reads would not write back any converted records.

The clean-up operation could also remove old version history from the 'default row' record. After MDEV-16264, we could also have a background cleanup operation that would (with some granularity) remember the last processed {{PRIMARY KEY}} values, by storing them in the (otherwise ignored) keys of the 'default row' records.

h2. A note on MVCC
Because {{ha_innobase::commit_inplace_alter_table()}} will be invoked while holding {{MDL_EXCLUSIVE}}, any transactions that read or modified the table must finish before the {{ALTER TABLE}} can commit. But it is possible that some old transaction tries to do its first access to the table after the {{ALTER TABLE}} committed. Such transactions may receive an error message 'table definition changed', as noted in [MySQL Bug#28432|https://bugs.mysql.com/bug.php?id=28432]. It would be too much effort to support MVCC if a transaction after {{ALTER}} modified a record (converting it to newer dictionary version) that would otherwise be visible to the old transaction.
Here is the scenario in SQL:
{code:SQL}
connection con1;
START TRANSACTION WITH CONSISTENT SNAPSHOT; -- creates read view
connection con2;
ALTER TABLE t CHANGE COLUMN b b INT NULL;
UPDATE t SET b=1 WHERE a=100;
connection con1;
SELECT * FROM t WHERE a=1; -- might be OK, if the record is still in old format
SELECT * FROM t WHERE a=100; -- error: record is in to new format
{code}
For simplicity and consistency, we could always return an error to the SELECT statements (after any {{ALTER TABLE}}).",,0,11,0,1426,0.888302,"Instant ALTER TABLE of failure-free record format changes $end$ Implement instant ALTER TABLE operations where the specification of non-indexed columns is changed, for the cases that cannot be handled more efficiently. The special cases with efficient handling are:
* MDEV-11369 Instant {{ADD COLUMN…LAST}}
* MDEV-15562 Instant {{DROP COLUMN}}, {{ADD COLUMN}} anywhere, changing the order of columns
* MDEV-15563 Instant {{NOT NULL}} removal and {{CHAR}} or {{VARCHAR}} extension for {{ROW_FORMAT=REDUNDANT}}
* MDEV-15564 Avoid table rebuild in {{ALTER TABLE}} on collation or charset changes

Examples of ‘generic’ failure-free record format change (covered by this work) include:
* Instant {{NOT NULL}} removal and {{CHAR}} or {{VARCHAR}} extension for other than {{ROW_FORMAT=REDUNDANT}}
* Changing a column from {{TINYINT}} to {{INT}} or {{INT}} to {{BIGINT}}

Some operations could avoid data changes, but would not be instantaneous due to the need to validate the data. MDEV-16291 can allow them with {{ALGORITHM=NOCOPY}}.

Operations that involve adding or dropping indexes (also {{DROP COLUMN}} can imply this) are outside the scope of this work; they are supported with {{ALGORITHM=NOCOPY}}. {{ALTER TABLE…ADD \[UNIQUE\] INDEX}} supports concurrent modifications to the table since MariaDB 10.0. MDEV-16223 would defer {{ADD INDEX}} to a background operation.

Operations that will continue to be refused by {{ALGORITHM=INSTANT}} and {{ALGORITHM=NOCOPY}} include:
* Changing {{ROW_FORMAT}} or {{ENGINE}}
* Altering a table that is in {{ROW_FORMAT=COMPRESSED}}
* Dropping, adding or changing {{PRIMARY KEY}} columns, or {{ADD}}/{{DROP PRIMARY KEY}}

Any {{ALTER TABLE}} that would be refused by {{ALGORITHM=NOCOPY}} (anything that rebuilds the clustered index) will drop any ‘instant {{ALTER TABLE}}’ history. The history would also be deleted if a rebuild is explicitly requested by the use of the {{FORCE}} keyword.

h2. Metadata format changes
In InnoDB, instant {{ALTER TABLE}} affects clustered index page leaf records only. The data dictionary will reflect the most recent table definition. Additional metadata for interpreting records that correspond to an earlier version of the table definition will be stored in the clustered index tree as follows.

* MDEV-11369 in MariaDB 10.3.4 changed the root page type code to {{FIL_PAGE_TYPE_INSTANT}} to indicate that instant {{ALTER TABLE}} has been used. It also introduced a hidden 'default row' record at the start of the clustered index, on the leftmost leaf page. The new page type code prevents older MariaDB versions from opening the table.
* MDEV-15562 will slightly modify the format of the 'default row' record to represent dropped and reordered index fields. (All columns will internally be added last in the user records in the clustered index leaf pages.) MariaDB 10.3 will refuse to open such tables, because new info_bits will be set in the 'default row' record.
* This work will extend the 'default row' record further, to store 'conversion recipes' that make it possible to process records that correspond to an earlier definition of the table. To indicate the presence of the recipes in the 'default row', a new bit in info_bits will be set, and an additional BLOB column is appended to the record for storing the recipes.

h2. Data format changes
User records in the clustered index leaf pages will have to indicate which format they correspond to.
* For other than ROW_FORMAT=REDUNDANT, MDEV-11369 introduced {{REC_STATUS_COLUMNS_ADDED}} that indicates the presence of an optional record header that encodes the number of 'instantly added' columns that are present in the record.
* For ROW_FORMAT=REDUNDANT, MDEV-11369 simply stores the number of fields in the record header.
* In MDEV-11369 and MDEV-15562, any 'instantly added' columns whose values are missing from the end of the clustered index record will be substituted with the values stored in the 'default row' record.
* MDEV-15562 will not change the user record format in any way.

In this work, we must store a 'conversion recipe number' in the record or page header.
* We can repurpose the 64-bit page header field {{PAGE_MAX_TRX_ID}}. MDEV-6076 repurposed it for {{PAGE_ROOT_AUTO_INC}} in the root page. If the table consists of a single root page or it later shrinks to a single page, we can simply convert the table to non-instant format. Having a per-page format would require all records of the page to be converted to the newest format on any write.
* In the record header, there are few spare bits; for {{ROW_FORMAT=REDUNDANT}} 3 bits, and for {{ROW_FORMAT=COMPACT}} (the MySQL 5.0 default) and {{ROW_FORMAT=DYNAMIC}} (MySQL 5.7/MariaDB 10.2 default) possibly 5 bits. We could use these to support a small maximum number of conversion recipes.

We may choose to limit the number of conversion recipes that can be supported. If an {{ALTER TABLE}} operation would require a conversion recipe, but there is no space to store one, the operation would be refused with {{ALGORITHM=NOCOPY}} or {{ALGORITHM=INSTANT}}. Once the table has been rebuilt, the maximum number of conversion recipes will be available again.

The special cases (MDEV-11369 {{ADD COLUMN}}, MDEV-15562 {{DROP COLUMN}}, and other operations where the interpretation of existing user records can be changed) will not generate conversion recipes.

h2. How it works
The 'default row' record needs to store the conversion recipes that allow each clustered index leaf page record format version to be converted to the latest table definition. These recipes will be cached in {{dict_table_t}}. The 'default row' record will only be read when the table definition is first loaded to the data dictionary cache.

Adding or dropping indexes or renaming columns or indexes would not touch the 'default row' record.

If we store recipes for converting from version 1 to latest, 2 to latest, and so on, then each instant {{ALTER}} will have to rewrite the old recipes, so that they will work from 1 to latest+1, 2 to latest+1 and so on, and finally add a recipe from latest to latest+1. Because we will store the recipes in a BLOB, and because BLOBs are copy-on-write in InnoDB, all conversion recipe pages will have to be rewritten in any case.

Read and write access to the clustered index would be affected as follows:
Writes would always use the latest dictionary version. If we use per-page identifier (instead of per-record) and the page that is being written to contains other old-format records, we could convert all of them while we are modifying the page.

Reads would have to be prepared to convert records from old format to the latest one.

h2. Possible future work: Cleaning up conversion recipes
There could be a separate online operation for clean-up, such as {{OPTIMIZE INDEX PRIMARY ON t1}}, which would also defragment the index. This would be useful if writes to the table are rare, because reads would not write back any converted records.

The clean-up operation could also remove old version history from the 'default row' record. After MDEV-16264, we could also have a background cleanup operation that would (with some granularity) remember the last processed {{PRIMARY KEY}} values, by storing them in the (otherwise ignored) keys of the 'default row' records.

h2. A note on MVCC
Because {{ha_innobase::commit_inplace_alter_table()}} will be invoked while holding {{MDL_EXCLUSIVE}}, any transactions that read or modified the table must finish before the {{ALTER TABLE}} can commit. But it is possible that some old transaction tries to do its first access to the table after the {{ALTER TABLE}} committed. Such transactions may receive an error message 'table definition changed', as noted in [MySQL Bug#28432|https://bugs.mysql.com/bug.php?id=28432]. It would be too much effort to support MVCC if a transaction after {{ALTER}} modified a record (converting it to newer dictionary version) that would otherwise be visible to the old transaction.
Here is the scenario in SQL:
{code:SQL}
connection con1;
START TRANSACTION WITH CONSISTENT SNAPSHOT; -- creates read view
connection con2;
ALTER TABLE t CHANGE COLUMN b b INT NULL;
UPDATE t SET b=1 WHERE a=100;
connection con1;
SELECT * FROM t WHERE a=1; -- might be OK, if the record is still in old format
SELECT * FROM t WHERE a=100; -- error: record is in to new format
{code}
For simplicity and consistency, we could always return an error to the SELECT statements (after any {{ALTER TABLE}}). $acceptance criteria:$",11,1,1,1,1,1,1,13078.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
592,MDEV-11426,Task,MDEV,2016-11-30 12:06:09,,0,Remove InnoDB INFORMATION_SCHEMA.FILES implementation,"MySQL 5.7 introduced [WL#7943: InnoDB: Implement Information_Schema.Files|http://dev.mysql.com/worklog/task/?id=7943] to provide a long-term alternative for accessing tablespace metadata. The INFORMATION_SCHEMA.INNODB_* views are considered internal interfaces that are subject to change or removal between releases. So, users should refer to I_S.FILES instead of I_S.INNODB_SYS_TABLESPACES to fetch metadata about CREATE TABLESPACE.

Because MariaDB 10.2 does not support CREATE TABLESPACE or CREATE TABLE…TABLESPACE for InnoDB, it does not make sense to support I_S.FILES either. So, let MariaDB 10.2 omit the code that was added in MySQL 5.7. After this change, I_S.FILES will report the empty result, unless some other storage engine in MariaDB 10.2 implements the interface. (This interface was originally created for the NDB Cluster.)",,"Remove InnoDB INFORMATION_SCHEMA.FILES implementation $end$ MySQL 5.7 introduced [WL#7943: InnoDB: Implement Information_Schema.Files|http://dev.mysql.com/worklog/task/?id=7943] to provide a long-term alternative for accessing tablespace metadata. The INFORMATION_SCHEMA.INNODB_* views are considered internal interfaces that are subject to change or removal between releases. So, users should refer to I_S.FILES instead of I_S.INNODB_SYS_TABLESPACES to fetch metadata about CREATE TABLESPACE.

Because MariaDB 10.2 does not support CREATE TABLESPACE or CREATE TABLE…TABLESPACE for InnoDB, it does not make sense to support I_S.FILES either. So, let MariaDB 10.2 omit the code that was added in MySQL 5.7. After this change, I_S.FILES will report the empty result, unless some other storage engine in MariaDB 10.2 implements the interface. (This interface was originally created for the NDB Cluster.) $acceptance criteria:$",,Marko Mäkelä,Marko Mäkelä,Major,10,,0,3,3,1,0,1,0,,0,850,3,0,0,2016-11-30 13:57:44,Remove InnoDB INFORMATION_SCHEMA.FILES implementation,"MySQL 5.7 introduced [WL#7943|WL#7943: InnoDB: Implement Information_Schema.Files] to provide a long-term alternative for accessing tablespace metadata. The INFORMATION_SCHEMA.INNODB_* views are considered internal interfaces that are subject to change or removal between releases. So, users should refer to I_S.FILES instead of I_S.INNODB_SYS_TABLESPACES to fetch metadata about CREATE TABLESPACE.

Because MariaDB 10.2 does not support CREATE TABLESPACE or CREATE TABLE…TABLESPACE for InnoDB, it does not make sense to support I_S.FILES either. So, let MariaDB 10.2 omit the code that was added in MySQL 5.7. After this change, I_S.FILES will report the empty result, unless some other storage engine in MariaDB 10.2 implements the interface. (This interface was originally created for the NDB Cluster.)",,0,1,0,4,0.0169492,"Remove InnoDB INFORMATION_SCHEMA.FILES implementation $end$ MySQL 5.7 introduced [WL#7943|WL#7943: InnoDB: Implement Information_Schema.Files] to provide a long-term alternative for accessing tablespace metadata. The INFORMATION_SCHEMA.INNODB_* views are considered internal interfaces that are subject to change or removal between releases. So, users should refer to I_S.FILES instead of I_S.INNODB_SYS_TABLESPACES to fetch metadata about CREATE TABLESPACE.

Because MariaDB 10.2 does not support CREATE TABLESPACE or CREATE TABLE…TABLESPACE for InnoDB, it does not make sense to support I_S.FILES either. So, let MariaDB 10.2 omit the code that was added in MySQL 5.7. After this change, I_S.FILES will report the empty result, unless some other storage engine in MariaDB 10.2 implements the interface. (This interface was originally created for the NDB Cluster.) $acceptance criteria:$",1,1,0,0,0,0,0,1.85,1,1,1.0,1,1.0,1,1.0,1,1.0,1,1.0
593,MDEV-11427,Task,MDEV,2016-11-30 14:19:19,,0,10.1.20 merge,"* 10.0
* 10.0-galera",,"10.1.20 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,4,,1,0,1,1,0,0,0,,0,850,0,0,0,2016-12-02 08:24:59,10.1.20 merge,"* 10.0
* 10.0-galera",,0,0,0,0,0.0,"10.1.20 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",0,0,0,0,0,0,0,42.0833,41,19,0.463415,13,0.317073,11,0.268293,11,0.268293,10,0.243902
594,MDEV-11429,Task,MDEV,2016-11-30 14:39:51,,0,Increase number of max table_open_cache instances,"One user got a problem that one couldn't increase the table open cache big enough.
There is no logical reason why not allow more than 512K open tables.

This task is to increase the maximum possible number to 1024K tables.
",,"Increase number of max table_open_cache instances $end$ One user got a problem that one couldn't increase the table open cache big enough.
There is no logical reason why not allow more than 512K open tables.

This task is to increase the maximum possible number to 1024K tables.
 $acceptance criteria:$",,Michael Widenius,Michael Widenius,Minor,8,,0,3,0,1,0,0,0,,0,850,3,0,0,2016-12-02 08:10:05,Increase number of max table_open_cache instances,"One user got a problem that one couldn't increase the table open cache big enough.
There is no logical reason why not allow more than 512K open tables.

This task is to increase the maximum possible number to 1024K tables.
",,0,0,0,0,0.0,"Increase number of max table_open_cache instances $end$ One user got a problem that one couldn't increase the table open cache big enough.
There is no logical reason why not allow more than 512K open tables.

This task is to increase the maximum possible number to 1024K tables.
 $acceptance criteria:$",0,0,0,0,0,0,0,41.5,8,3,0.375,3,0.375,3,0.375,3,0.375,2,0.25
595,MDEV-11557,Task,MDEV,2016-12-13 11:09:43,,0,port MySQL-5.7 JSON tests to MariaDB,port MySQL-5.7 JSON tests to MariaDB,,port MySQL-5.7 JSON tests to MariaDB $end$ port MySQL-5.7 JSON tests to MariaDB $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Major,11,,0,1,0,3,0,0,0,,0,850,1,0,0,2016-12-21 11:46:13,port MySQL-5.7 JSON tests to MariaDB,port MySQL-5.7 JSON tests to MariaDB,,0,0,0,0,0.0,port MySQL-5.7 JSON tests to MariaDB $end$ port MySQL-5.7 JSON tests to MariaDB $acceptance criteria:$,0,0,0,0,0,0,1,192.6,42,19,0.452381,13,0.309524,11,0.261905,11,0.261905,10,0.238095
596,MDEV-11559,Task,MDEV,2016-12-13 14:20:00,,0,5.5.54 merge,"* mysql-5.5.54 (/)
*  xtradb-5.5 (/) _nothing to merge_",,"5.5.54 merge $end$ * mysql-5.5.54 (/)
*  xtradb-5.5 (/) _nothing to merge_ $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,6,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-12-15 08:54:00,5.5.54 merge,"* mysql-5.5.54
*  xtradb-5.5",,0,1,0,5,0.555556,"5.5.54 merge $end$ * mysql-5.5.54
*  xtradb-5.5 $acceptance criteria:$",1,1,1,0,0,0,1,42.5667,43,19,0.44186,13,0.302326,11,0.255814,11,0.255814,10,0.232558
597,MDEV-11692,Task,MDEV,2016-12-30 15:37:34,,0,Comparison data type aggregation for pluggable data types,"Comparison data type aggregation is done when we need to mix two or more expressions of different data types for comparison.
Examples:
{code:sql}
SELECT a=b FROM t1;
SELECT a BETWEEN b AND c FROM t1;
SELECT a IN (b,c) FROM t1;
SELECT CASE predicant WHEN expr1 THEN ... WHEN expr2 THEN ... END FROM t1;
{code}

Functions {{LEAST}} and {{GREATEST}} are out of scope of this task and will be done separately.

Comparing of any arbitrary general purpose data types (like {{INT}}, {{DOUBLE}}, {{VARCHAR}}) was OK. But as we'll be adding special purpose data types like {{INET6}}, some mixtures will be meaningless. For example, comparing {{INET6}} with {{TIME}}  does not seem to have any sense.

New data type implementations should be able to decide:
1. to which other data types it can be compared with, or cannot be compared with (with a possibility to raise an error if comparison is not possible)
2. which data type format (handler) is used for comparison, if comparison is possible

Under terms of this tasks we'll implement a special registry which will contain the above information. Later, when we implement data type plugins, the server will add mixing rules to this registry when loading a new data type plugin.

The patch for this task will not change behavior for the existing data types.

The only exception is {{GEOMETRY}}, which is not a general purpose data type. We will change {{GEOMETRY}} to use the new comparison data type aggregation registry. Meaningless mixtures of {{GEOMETRY}} with other data types will be disallowed.
",,"Comparison data type aggregation for pluggable data types $end$ Comparison data type aggregation is done when we need to mix two or more expressions of different data types for comparison.
Examples:
{code:sql}
SELECT a=b FROM t1;
SELECT a BETWEEN b AND c FROM t1;
SELECT a IN (b,c) FROM t1;
SELECT CASE predicant WHEN expr1 THEN ... WHEN expr2 THEN ... END FROM t1;
{code}

Functions {{LEAST}} and {{GREATEST}} are out of scope of this task and will be done separately.

Comparing of any arbitrary general purpose data types (like {{INT}}, {{DOUBLE}}, {{VARCHAR}}) was OK. But as we'll be adding special purpose data types like {{INET6}}, some mixtures will be meaningless. For example, comparing {{INET6}} with {{TIME}}  does not seem to have any sense.

New data type implementations should be able to decide:
1. to which other data types it can be compared with, or cannot be compared with (with a possibility to raise an error if comparison is not possible)
2. which data type format (handler) is used for comparison, if comparison is possible

Under terms of this tasks we'll implement a special registry which will contain the above information. Later, when we implement data type plugins, the server will add mixing rules to this registry when loading a new data type plugin.

The patch for this task will not change behavior for the existing data types.

The only exception is {{GEOMETRY}}, which is not a general purpose data type. We will change {{GEOMETRY}} to use the new comparison data type aggregation registry. Meaningless mixtures of {{GEOMETRY}} with other data types will be disallowed.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,12,,1,0,3,2,0,0,0,,0,850,0,0,0,2017-01-18 10:05:08,Comparison data type aggregation for pluggable data types,"Comparison data type aggregation is done when we need to mix two or more expressions of different data types for comparison.
Examples:
{code:sql}
SELECT a=b FROM t1;
SELECT a BETWEEN b AND c FROM t1;
SELECT a IN (b,c) FROM t1;
SELECT CASE predicant WHEN expr1 THEN ... WHEN expr2 THEN ... END FROM t1;
{code}

Functions {{LEAST}} and {{GREATEST}} are out of scope of this task and will be done separately.

Comparing of any arbitrary general purpose data types (like {{INT}}, {{DOUBLE}}, {{VARCHAR}}) was OK. But as we'll be adding special purpose data types like {{INET6}}, some mixtures will be meaningless. For example, comparing {{INET6}} with {{TIME}}  does not seem to have any sense.

New data type implementations should be able to decide:
1. to which other data types it can be compared with, or cannot be compared with (with a possibility to raise an error if comparison is not possible)
2. which data type format (handler) is used for comparison, if comparison is possible

Under terms of this tasks we'll implement a special registry which will contain the above information. Later, when we implement data type plugins, the server will add mixing rules to this registry when loading a new data type plugin.

The patch for this task will not change behavior for the existing data types.

The only exception is {{GEOMETRY}}, which is not a general purpose data type. We will change {{GEOMETRY}} to use the new comparison data type aggregation registry. Meaningless mixtures of {{GEOMETRY}} with other data types will be disallowed.
",,0,0,0,0,0.0,"Comparison data type aggregation for pluggable data types $end$ Comparison data type aggregation is done when we need to mix two or more expressions of different data types for comparison.
Examples:
{code:sql}
SELECT a=b FROM t1;
SELECT a BETWEEN b AND c FROM t1;
SELECT a IN (b,c) FROM t1;
SELECT CASE predicant WHEN expr1 THEN ... WHEN expr2 THEN ... END FROM t1;
{code}

Functions {{LEAST}} and {{GREATEST}} are out of scope of this task and will be done separately.

Comparing of any arbitrary general purpose data types (like {{INT}}, {{DOUBLE}}, {{VARCHAR}}) was OK. But as we'll be adding special purpose data types like {{INET6}}, some mixtures will be meaningless. For example, comparing {{INET6}} with {{TIME}}  does not seem to have any sense.

New data type implementations should be able to decide:
1. to which other data types it can be compared with, or cannot be compared with (with a possibility to raise an error if comparison is not possible)
2. which data type format (handler) is used for comparison, if comparison is possible

Under terms of this tasks we'll implement a special registry which will contain the above information. Later, when we implement data type plugins, the server will add mixing rules to this registry when loading a new data type plugin.

The patch for this task will not change behavior for the existing data types.

The only exception is {{GEOMETRY}}, which is not a general purpose data type. We will change {{GEOMETRY}} to use the new comparison data type aggregation registry. Meaningless mixtures of {{GEOMETRY}} with other data types will be disallowed.
 $acceptance criteria:$",0,0,0,0,0,0,1,450.45,41,27,0.658537,22,0.536585,22,0.536585,21,0.512195,21,0.512195
598,MDEV-11714,Task,MDEV,2017-01-03 14:35:03,,0,10.0.29 merge,"* 5.5 (/)
* InnoDB (/)
* XtraDB (/) No changes
* P_S (/)
* Connect (/)
* Spider (/) No changes
* PCRE (/) No changes
* Mroonga (/) No changes
* TokuDB (/) No changes",,"10.0.29 merge $end$ * 5.5 (/)
* InnoDB (/)
* XtraDB (/) No changes
* P_S (/)
* Connect (/)
* Spider (/) No changes
* PCRE (/) No changes
* Mroonga (/) No changes
* TokuDB (/) No changes $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,18,,0,1,0,1,0,10,0,,0,850,1,0,0,2017-01-04 10:25:04,10.0.29 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB",,0,10,0,19,0.826087,"10.0.29 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB $acceptance criteria:$",10,1,1,1,1,0,1,19.8333,44,20,0.454545,14,0.318182,11,0.25,11,0.25,10,0.227273
599,MDEV-11743,Task,MDEV,2017-01-08 16:03:55,,0,Add environment variables for unit test which uses server,"Add:
MYSQL_TEST_HOST, MYSQL_TEST_USER, MYSQL_TEST_PORT, MYSQL_TEST_DB",,"Add environment variables for unit test which uses server $end$ Add:
MYSQL_TEST_HOST, MYSQL_TEST_USER, MYSQL_TEST_PORT, MYSQL_TEST_DB $acceptance criteria:$",,Oleksandr Byelkin,Oleksandr Byelkin,Critical,12,,0,2,0,3,0,0,0,,0,850,2,0,0,2017-01-18 17:25:49,Add environment variables for unit test which uses server,"Add:
MYSQL_TEST_HOST, MYSQL_TEST_USER, MYSQL_TEST_PORT, MYSQL_TEST_DB",,0,0,0,0,0.0,"Add environment variables for unit test which uses server $end$ Add:
MYSQL_TEST_HOST, MYSQL_TEST_USER, MYSQL_TEST_PORT, MYSQL_TEST_DB $acceptance criteria:$",0,0,0,0,0,0,1,241.35,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
600,MDEV-11751,Task,MDEV,2017-01-10 08:08:10,,0,Merge new release of InnoDB MySQL 5.7.18 to 10.2,,,Merge new release of InnoDB MySQL 5.7.18 to 10.2 $end$ $acceptance criteria:$,,Jan Lindström,Jan Lindström,Critical,15,,4,4,5,3,0,1,0,,0,850,4,0,0,2017-03-22 09:58:17,Merge new release of InnoDB MySQL 5.7.17 to 10.2,,,1,0,0,2,0.0833333,Merge new release of InnoDB MySQL 5.7.17 to 10.2 $end$ $acceptance criteria:$,1,1,0,0,0,0,1,1705.83,2,1,0.5,0,0.0,0,0.0,0,0.0,0,0.0
601,MDEV-11755,Task,MDEV,2017-01-10 14:28:54,,0,10.1.21 merge,"* 5.5 (/)
* 10.0 (/)
* 10.0-galera (/)",,"10.1.21 merge $end$ * 5.5 (/)
* 10.0 (/)
* 10.0-galera (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,10,,0,0,0,1,0,3,0,,0,850,0,0,0,2017-01-11 08:14:48,10.1.21 merge,"* 10.0
* 10.0-galera",,0,3,0,5,0.555556,"10.1.21 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",3,1,1,0,0,0,1,17.75,45,21,0.466667,15,0.333333,12,0.266667,12,0.266667,10,0.222222
602,MDEV-11782,Task,MDEV,2017-01-12 10:40:17,,0,Redefine the innodb_encrypt_log format,"[WL#8845|http://dev.mysql.com/worklog/task/?id=8845] introduced a redo log format identifier in MySQL 5.7.9.

The MariaDB extension innodb_encrypt_log should distinguish encrypted redo logs from cleartext ones by the redo log format tag. In this way, an attempt to start up MySQL 5.7 after a crash of MariaDB with innodb_encrypt_log=1 will result in a clear error message rather than some strange-looking crash.",,"Redefine the innodb_encrypt_log format $end$ [WL#8845|http://dev.mysql.com/worklog/task/?id=8845] introduced a redo log format identifier in MySQL 5.7.9.

The MariaDB extension innodb_encrypt_log should distinguish encrypted redo logs from cleartext ones by the redo log format tag. In this way, an attempt to start up MySQL 5.7 after a crash of MariaDB with innodb_encrypt_log=1 will result in a clear error message rather than some strange-looking crash. $acceptance criteria:$",,Marko Mäkelä,Marko Mäkelä,Major,17,,8,12,12,2,0,1,0,,0,850,12,0,0,2017-01-18 10:00:30,Distinguish encrypted redo log by a format tag,"[WL#8845|http://dev.mysql.com/worklog/task/?id=8845] introduced a redo log format identifier in MySQL 5.7.9.

The MariaDB extension innodb_encrypt_log should distinguish encrypted redo logs from cleartext ones by the redo log format tag. In this way, an attempt to start up MySQL 5.7 after a crash of MariaDB with innodb_encrypt_log=1 will result in a clear error message rather than some strange-looking crash.",,1,0,0,10,0.102941,"Distinguish encrypted redo log by a format tag $end$ [WL#8845|http://dev.mysql.com/worklog/task/?id=8845] introduced a redo log format identifier in MySQL 5.7.9.

The MariaDB extension innodb_encrypt_log should distinguish encrypted redo logs from cleartext ones by the redo log format tag. In this way, an attempt to start up MySQL 5.7 after a crash of MariaDB with innodb_encrypt_log=1 will result in a clear error message rather than some strange-looking crash. $acceptance criteria:$",1,1,1,1,0,0,1,143.333,2,2,1.0,1,0.5,1,0.5,1,0.5,1,0.5
603,MDEV-11825,Task,MDEV,2017-01-17 13:41:52,,0,Make session variables TRACKING enabled by default,"Make by default tracking for:
autocommit,
Character_set_client,
Character_set_connection,
Character_set_results,
time_zone",,"Make session variables TRACKING enabled by default $end$ Make by default tracking for:
autocommit,
Character_set_client,
Character_set_connection,
Character_set_results,
time_zone $acceptance criteria:$",,Oleksandr Byelkin,Oleksandr Byelkin,Major,17,,0,4,0,2,0,0,0,,0,850,3,0,0,2017-01-18 17:27:28,Make session variables TRACKING enabled by default,"Make by default tracking for:
autocommit,
Character_set_client,
Character_set_connection,
Character_set_results,
time_zone",,0,0,0,0,0.0,"Make session variables TRACKING enabled by default $end$ Make by default tracking for:
autocommit,
Character_set_client,
Character_set_connection,
Character_set_results,
time_zone $acceptance criteria:$",0,0,0,0,0,0,1,27.75,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
604,MDEV-11837,Technical task,MDEV,2017-01-18 19:39:55,,0,MariaRocks Packaging Issues,"* Build dynamically (/)
* Run test suites with dynamic build (/)
* C++ 11 checks  -- don't build if C++11 is not supported (/)
* GCC 4.8 Regex (/)
* make tarball should put mariarocks && rocksdb sources in (/) (make dist generates tarball with sources)
* Compression libraries (/)
* Check how tests run and disable as necessary (/)
* Ship (/)
{noformat}
SET(ROCKSDB_TOOL_SOURCES
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_tool.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_cmd.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/sst_dump_tool.cc
{noformat}
* Some sort of backup (myrocks_hotbackup) (?)",,"MariaRocks Packaging Issues $end$ * Build dynamically (/)
* Run test suites with dynamic build (/)
* C++ 11 checks  -- don't build if C++11 is not supported (/)
* GCC 4.8 Regex (/)
* make tarball should put mariarocks && rocksdb sources in (/) (make dist generates tarball with sources)
* Compression libraries (/)
* Check how tests run and disable as necessary (/)
* Ship (/)
{noformat}
SET(ROCKSDB_TOOL_SOURCES
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_tool.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_cmd.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/sst_dump_tool.cc
{noformat}
* Some sort of backup (myrocks_hotbackup) (?) $acceptance criteria:$",,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,13,,0,0,0,3,0,4,0,,0,850,0,0,0,2017-01-18 19:39:55,MariaRocks Packaging Issues,"* Build dynamically
* Run test suites with dynamic build
* C++ 11 checks  -- don't build if C++11 is not supported
* GCC 4.8 Regex
* make tarball should put mariarocks && rocksdb sources in
* Compression libraries
* Check how tests run and disable as necessary
* Ship
{noformat}
SET(ROCKSDB_TOOL_SOURCES
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_tool.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_cmd.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/sst_dump_tool.cc
{noformat}
* Some sort of backup (myrocks_hotbackup) (?)",,0,4,0,14,0.202899,"MariaRocks Packaging Issues $end$ * Build dynamically
* Run test suites with dynamic build
* C++ 11 checks  -- don't build if C++11 is not supported
* GCC 4.8 Regex
* make tarball should put mariarocks && rocksdb sources in
* Compression libraries
* Check how tests run and disable as necessary
* Ship
{noformat}
SET(ROCKSDB_TOOL_SOURCES
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_tool.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/ldb_cmd.cc
   ${CMAKE_SOURCE_DIR}/storage/rocksdb/rocksdb/tools/sst_dump_tool.cc
{noformat}
* Some sort of backup (myrocks_hotbackup) (?) $acceptance criteria:$",4,1,1,1,0,0,1,0.0,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
605,MDEV-11880,Technical task,MDEV,2017-01-23 10:43:00,,0,sql_mode=ORACLE: Make the concatenation operator ignore NULL arguments,"The concatenation operator {{||}} in Oracle skips all {{NULL}} arguments and returns the result of concatenation of non-{{NULL}} arguments.
{{NULL}} is returned only if all arguments were NULL.

Under terms of this task we'll make the {{||}} operator work in Oracle style when {{sql_mode}} is {{ORACLE}}.

Note, the function {{CONCAT}} won't be changed by this task.


There is a contributed patch from Jérôme Brauge implementing this feature:

https://lists.launchpad.net/maria-developers/msg10288.html

It adds a new flag {{MODE_CONCAT_NULL_YIELDS_NULL_OFF}}.
We'll probably won't add the flag and just perform the Oracle style concatenation when {{sql_mode & MODE_ORACLE}} is set.

",,"sql_mode=ORACLE: Make the concatenation operator ignore NULL arguments $end$ The concatenation operator {{||}} in Oracle skips all {{NULL}} arguments and returns the result of concatenation of non-{{NULL}} arguments.
{{NULL}} is returned only if all arguments were NULL.

Under terms of this task we'll make the {{||}} operator work in Oracle style when {{sql_mode}} is {{ORACLE}}.

Note, the function {{CONCAT}} won't be changed by this task.


There is a contributed patch from Jérôme Brauge implementing this feature:

https://lists.launchpad.net/maria-developers/msg10288.html

It adds a new flag {{MODE_CONCAT_NULL_YIELDS_NULL_OFF}}.
We'll probably won't add the flag and just perform the Oracle style concatenation when {{sql_mode & MODE_ORACLE}} is set.

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,15,,1,2,4,5,0,2,0,,0,850,2,0,0,2017-01-23 10:43:00,sql_mode=ORACLE: Make the concatenation operator ignore NULL arguments,"The concatenation operator {{||}} in Oracle skips all {{NULL}} arguments and returns the result of concatenation of non-{{NULL}} arguments.
{{NULL}} is returned only if all arguments were NULL.

Under terms of this task we'll make the {{||}} operator work in Oracle style when {{sql_mode}} is {{ORACLE}}.

Note, the function {{CONCAT}} won't be changed by this task.
",,0,2,0,37,0.552239,"sql_mode=ORACLE: Make the concatenation operator ignore NULL arguments $end$ The concatenation operator {{||}} in Oracle skips all {{NULL}} arguments and returns the result of concatenation of non-{{NULL}} arguments.
{{NULL}} is returned only if all arguments were NULL.

Under terms of this task we'll make the {{||}} operator work in Oracle style when {{sql_mode}} is {{ORACLE}}.

Note, the function {{CONCAT}} won't be changed by this task.
 $acceptance criteria:$",2,1,1,1,1,1,1,0.0,42,27,0.642857,22,0.52381,22,0.52381,21,0.5,21,0.5
606,MDEV-11903,Task,MDEV,2017-01-24 14:43:57,,0,Windows : Support  innodb page sizes in the installer/mysql_install_db.exe,"Innodb page size cannot be changed after database is created. For different reasons (mostly to support effective compression using NTFS compression), it is beneficial to enable block size of 64K",,"Windows : Support  innodb page sizes in the installer/mysql_install_db.exe $end$ Innodb page size cannot be changed after database is created. For different reasons (mostly to support effective compression using NTFS compression), it is beneficial to enable block size of 64K $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-08 11:15:58,Windows : Support  innodb page sizes in the installer/mysql_install_db.exe,"Innodb page size cannot be changed after database is created. For different reasons (mostly to support effective compression using NTFS compression), it is beneficial to enable block size of 64K",,0,0,0,0,0.0,"Windows : Support  innodb page sizes in the installer/mysql_install_db.exe $end$ Innodb page size cannot be changed after database is created. For different reasons (mostly to support effective compression using NTFS compression), it is beneficial to enable block size of 64K $acceptance criteria:$",0,0,0,0,0,0,0,1028.53,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
607,MDEV-11934,Task,MDEV,2017-01-29 17:02:06,,0,MariaRocks: Group Commit with binlog,"MyRocks has group commit with the binary log based on MySQL API: 
https://github.com/facebook/mysql-5.6/commit/14a0d4a97c09b52fa7450e6a3d56ebe7ed193ab6

Inside MyRocks/RocksDB:
* One can set {{m_rocksdb_tx->GetWriteOptions()->sync}} to false to avoid flushing.
* One can flush WAL to disk with {{rdb->SyncWAL()}} call.
* RocksDB has its own group commit imlementation which ""just works"" and is not visible from outside of RocksDB API.

h2. == MySQL's Group Commit API ==

Here is a description of how it works when safe settings are ( sync_binlog=1, rocksdb_enable_2pc=ON, rocksdb_write_sync=ON)

=== Prepare ===
The storage engine checks `thd->durability_property == HA_IGNORE_DURABILITY`.
If true, it sets sync=false, which causes RocksDB not to persist the Prepare operation to disk.

=== Flush logs ===

Then SQL layer calls rocksdb_flush_wal() which makes the effect of
rocksdb_prepare() call persistent by calling SyncWAL().

If we crash at this point, recovery process will roll back the prepared
transaction in MyRocks.

Then, SQL layer writes and flushes the binlog. If we crash after that, recovery
will commit the prepared MyRocks' transaction.

As far as MyRocks is concerned, each SyncWAL() call is made individually.
RocksDB has its own Group Commit implementation under the hood.

=== Commit ===

Then SQL layer calls rocksdb_commit().

Commit writes to WAL too, but does not sync it.
(The effect of rocksdb_prepare() was flushed, the binlog has the information about whether the recovery should commit or roll back, the binlog has been flushed to disk)

h2. == MariaDB ==

MariaDB 10.2 has {{thd->durability_property}} but it is always equal to HA_REGULAR_DURABILITY

For actually doing Group Commit, MariaDB 10.0+ has new handlerton functions:

*  handlerton->prepare_ordered
*  handlerton->commit_ordered
*  (handlerton->commit is still there and still used also)
*  handlerton->commit_checkpoint_request

",,"MariaRocks: Group Commit with binlog $end$ MyRocks has group commit with the binary log based on MySQL API: 
https://github.com/facebook/mysql-5.6/commit/14a0d4a97c09b52fa7450e6a3d56ebe7ed193ab6

Inside MyRocks/RocksDB:
* One can set {{m_rocksdb_tx->GetWriteOptions()->sync}} to false to avoid flushing.
* One can flush WAL to disk with {{rdb->SyncWAL()}} call.
* RocksDB has its own group commit imlementation which ""just works"" and is not visible from outside of RocksDB API.

h2. == MySQL's Group Commit API ==

Here is a description of how it works when safe settings are ( sync_binlog=1, rocksdb_enable_2pc=ON, rocksdb_write_sync=ON)

=== Prepare ===
The storage engine checks `thd->durability_property == HA_IGNORE_DURABILITY`.
If true, it sets sync=false, which causes RocksDB not to persist the Prepare operation to disk.

=== Flush logs ===

Then SQL layer calls rocksdb_flush_wal() which makes the effect of
rocksdb_prepare() call persistent by calling SyncWAL().

If we crash at this point, recovery process will roll back the prepared
transaction in MyRocks.

Then, SQL layer writes and flushes the binlog. If we crash after that, recovery
will commit the prepared MyRocks' transaction.

As far as MyRocks is concerned, each SyncWAL() call is made individually.
RocksDB has its own Group Commit implementation under the hood.

=== Commit ===

Then SQL layer calls rocksdb_commit().

Commit writes to WAL too, but does not sync it.
(The effect of rocksdb_prepare() was flushed, the binlog has the information about whether the recovery should commit or roll back, the binlog has been flushed to disk)

h2. == MariaDB ==

MariaDB 10.2 has {{thd->durability_property}} but it is always equal to HA_REGULAR_DURABILITY

For actually doing Group Commit, MariaDB 10.0+ has new handlerton functions:

*  handlerton->prepare_ordered
*  handlerton->commit_ordered
*  (handlerton->commit is still there and still used also)
*  handlerton->commit_checkpoint_request

 $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,5,,0,35,2,1,0,0,0,,0,850,30,0,0,2017-03-08 11:11:11,MariaRocks: Group Commit with binlog,"MyRocks has group commit with the binary log based on MySQL API: 
https://github.com/facebook/mysql-5.6/commit/14a0d4a97c09b52fa7450e6a3d56ebe7ed193ab6

Inside MyRocks/RocksDB:
* One can set {{m_rocksdb_tx->GetWriteOptions()->sync}} to false to avoid flushing.
* One can flush WAL to disk with {{rdb->SyncWAL()}} call.
* RocksDB has its own group commit imlementation which ""just works"" and is not visible from outside of RocksDB API.

h2. == MySQL's Group Commit API ==

Here is a description of how it works when safe settings are ( sync_binlog=1, rocksdb_enable_2pc=ON, rocksdb_write_sync=ON)

=== Prepare ===
The storage engine checks `thd->durability_property == HA_IGNORE_DURABILITY`.
If true, it sets sync=false, which causes RocksDB not to persist the Prepare operation to disk.

=== Flush logs ===

Then SQL layer calls rocksdb_flush_wal() which makes the effect of
rocksdb_prepare() call persistent by calling SyncWAL().

If we crash at this point, recovery process will roll back the prepared
transaction in MyRocks.

Then, SQL layer writes and flushes the binlog. If we crash after that, recovery
will commit the prepared MyRocks' transaction.

As far as MyRocks is concerned, each SyncWAL() call is made individually.
RocksDB has its own Group Commit implementation under the hood.

=== Commit ===

Then SQL layer calls rocksdb_commit().

Commit writes to WAL too, but does not sync it.
(The effect of rocksdb_prepare() was flushed, the binlog has the information about whether the recovery should commit or roll back, the binlog has been flushed to disk)

h2. == MariaDB ==

MariaDB 10.2 has {{thd->durability_property}} but it is always equal to HA_REGULAR_DURABILITY

For actually doing Group Commit, MariaDB 10.0+ has new handlerton functions:

*  handlerton->prepare_ordered
*  handlerton->commit_ordered
*  (handlerton->commit is still there and still used also)
*  handlerton->commit_checkpoint_request

",,0,0,0,0,0.0,"MariaRocks: Group Commit with binlog $end$ MyRocks has group commit with the binary log based on MySQL API: 
https://github.com/facebook/mysql-5.6/commit/14a0d4a97c09b52fa7450e6a3d56ebe7ed193ab6

Inside MyRocks/RocksDB:
* One can set {{m_rocksdb_tx->GetWriteOptions()->sync}} to false to avoid flushing.
* One can flush WAL to disk with {{rdb->SyncWAL()}} call.
* RocksDB has its own group commit imlementation which ""just works"" and is not visible from outside of RocksDB API.

h2. == MySQL's Group Commit API ==

Here is a description of how it works when safe settings are ( sync_binlog=1, rocksdb_enable_2pc=ON, rocksdb_write_sync=ON)

=== Prepare ===
The storage engine checks `thd->durability_property == HA_IGNORE_DURABILITY`.
If true, it sets sync=false, which causes RocksDB not to persist the Prepare operation to disk.

=== Flush logs ===

Then SQL layer calls rocksdb_flush_wal() which makes the effect of
rocksdb_prepare() call persistent by calling SyncWAL().

If we crash at this point, recovery process will roll back the prepared
transaction in MyRocks.

Then, SQL layer writes and flushes the binlog. If we crash after that, recovery
will commit the prepared MyRocks' transaction.

As far as MyRocks is concerned, each SyncWAL() call is made individually.
RocksDB has its own Group Commit implementation under the hood.

=== Commit ===

Then SQL layer calls rocksdb_commit().

Commit writes to WAL too, but does not sync it.
(The effect of rocksdb_prepare() was flushed, the binlog has the information about whether the recovery should commit or roll back, the binlog has been flushed to disk)

h2. == MariaDB ==

MariaDB 10.2 has {{thd->durability_property}} but it is always equal to HA_REGULAR_DURABILITY

For actually doing Group Commit, MariaDB 10.0+ has new handlerton functions:

*  handlerton->prepare_ordered
*  handlerton->commit_ordered
*  (handlerton->commit is still there and still used also)
*  handlerton->commit_checkpoint_request

 $acceptance criteria:$",0,0,0,0,0,0,0,906.15,7,1,0.142857,1,0.142857,1,0.142857,1,0.142857,1,0.142857
608,MDEV-11946,Task,MDEV,2017-01-30 21:25:15,,0,MariaRocks: review the changes on SQL layer,"MariaRocks (current tree: {{bb-10.2-mariarocks}}) has some changes on the SQL layer (that is, outside of storage/rocksdb directory). 

The current diff is just 370 lines and can be found at:  https://gist.github.com/spetrunia/c7665515f9c5e714dd8869701fdabcc0 . 

[~serg], please review.",,"MariaRocks: review the changes on SQL layer $end$ MariaRocks (current tree: {{bb-10.2-mariarocks}}) has some changes on the SQL layer (that is, outside of storage/rocksdb directory). 

The current diff is just 370 lines and can be found at:  https://gist.github.com/spetrunia/c7665515f9c5e714dd8869701fdabcc0 . 

[~serg], please review. $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Critical,13,,0,0,1,1,0,0,0,,0,850,0,0,0,2017-03-08 11:11:31,MariaRocks: review the changes on SQL layer,"MariaRocks (current tree: {{bb-10.2-mariarocks}}) has some changes on the SQL layer (that is, outside of storage/rocksdb directory). 

The current diff is just 370 lines and can be found at:  https://gist.github.com/spetrunia/c7665515f9c5e714dd8869701fdabcc0 . 

[~serg], please review.",,0,0,0,0,0.0,"MariaRocks: review the changes on SQL layer $end$ MariaRocks (current tree: {{bb-10.2-mariarocks}}) has some changes on the SQL layer (that is, outside of storage/rocksdb directory). 

The current diff is just 370 lines and can be found at:  https://gist.github.com/spetrunia/c7665515f9c5e714dd8869701fdabcc0 . 

[~serg], please review. $acceptance criteria:$",0,0,0,0,0,0,0,877.767,8,1,0.125,1,0.125,1,0.125,1,0.125,1,0.125
609,MDEV-11953,Task,MDEV,2017-01-31 15:34:57,,0,support of brackets (parentheses) in UNION/EXCEPT/INTERSECT operations,"Add suport of brackets which mean priority of operation in the parser:

*(*(SELECT a from t1) union (SELECT b from t2)*)* intersect (select c from t3)",,"support of brackets (parentheses) in UNION/EXCEPT/INTERSECT operations $end$ Add suport of brackets which mean priority of operation in the parser:

*(*(SELECT a from t1) union (SELECT b from t2)*)* intersect (select c from t3) $acceptance criteria:$",,Oleksandr Byelkin,Oleksandr Byelkin,Critical,39,,7,4,9,3,0,1,0,,0,850,3,0,0,2017-05-24 13:24:54,support of brackets in UNION/EXCEPT/INTERSECT operations,"Add suport of brackets which mean priority of operation in the parser:

*(*(SELECT a from t1) union (SELECT b from t2)*)* intersect (select c from t3)",,1,0,0,1,0.0285714,"support of brackets in UNION/EXCEPT/INTERSECT operations $end$ Add suport of brackets which mean priority of operation in the parser:

*(*(SELECT a from t1) union (SELECT b from t2)*)* intersect (select c from t3) $acceptance criteria:$",1,1,0,0,0,0,1,2709.82,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
610,MDEV-11994,Task,MDEV,2017-02-04 21:49:25,,0,10.0.30 merge,"* 5.5 (/)
* InnoDB (/)
* XtraDB (/)
* P_S (/)
* Connect (/)
* Spider (/)
* PCRE (/)
* Mroonga (/)
* TokuDB (/) -> Needs review
* bb-10.0-monty (/)",,"10.0.30 merge $end$ * 5.5 (/)
* InnoDB (/)
* XtraDB (/)
* P_S (/)
* Connect (/)
* Spider (/)
* PCRE (/)
* Mroonga (/)
* TokuDB (/) -> Needs review
* bb-10.0-monty (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,14,,0,1,0,1,0,9,0,,0,850,1,0,0,2017-02-20 14:49:15,10.0.30 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
* bb-10.0-monty",,0,9,0,13,0.52,"10.0.30 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
* bb-10.0-monty $acceptance criteria:$",9,1,1,1,0,0,1,376.983,46,22,0.478261,16,0.347826,12,0.26087,12,0.26087,10,0.217391
611,MDEV-12007,Technical task,MDEV,2017-02-07 04:55:36,,0,Allow ROW variables as a cursor FETCH target,"We'll allow {{ROW}} type variables as {{FETCH}} targets:

{code:sql}
FETCH cur INTO rec;
{code}
where {{cur}} is a {{CURSOR}} and {{rec}} is a {{ROW}} type SP variable.
Note, currently an attempt to use {{FETCH}} for a {{ROW}} type variable returns this error:
{noformat}
ERROR 1328 (HY000): Incorrect number of FETCH variables
{noformat}

{{FETCH}} from a cursor {{cur}} into a {{ROW}} variable {{rec}} will work as follows:
- The number of fields in {{cur}} must match the number of fields in {{rec}}. Otherwise, an error is reported.
- Assignment is done from left to right. The first cursor field is assigned to the first variable field, the second cursor field is assigned to the second variable field, etc.
- Field names in {{rec}} are not important and can differ from field names in {{cur}}.


A complete example for {{sql_mode=ORACLE}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=oracle;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec ROW(a INT, b VARCHAR(32));
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT ('rec=(' || rec.a ||','|| rec.b||')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}


A complete example for {{sql_mode=DEFAULT}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=DEFAULT;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE done INT DEFAULT FALSE;
  DECLARE rec ROW(a INT, b VARCHAR(32));
  DECLARE c CURSOR FOR SELECT a,b FROM t1;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
  OPEN c;
read_loop:
  LOOP
    FETCH c INTO rec;
    IF done THEN
      LEAVE read_loop;
    END IF;
    SELECT CONCAT('rec=(',rec.a,',',rec.b,')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
",,"Allow ROW variables as a cursor FETCH target $end$ We'll allow {{ROW}} type variables as {{FETCH}} targets:

{code:sql}
FETCH cur INTO rec;
{code}
where {{cur}} is a {{CURSOR}} and {{rec}} is a {{ROW}} type SP variable.
Note, currently an attempt to use {{FETCH}} for a {{ROW}} type variable returns this error:
{noformat}
ERROR 1328 (HY000): Incorrect number of FETCH variables
{noformat}

{{FETCH}} from a cursor {{cur}} into a {{ROW}} variable {{rec}} will work as follows:
- The number of fields in {{cur}} must match the number of fields in {{rec}}. Otherwise, an error is reported.
- Assignment is done from left to right. The first cursor field is assigned to the first variable field, the second cursor field is assigned to the second variable field, etc.
- Field names in {{rec}} are not important and can differ from field names in {{cur}}.


A complete example for {{sql_mode=ORACLE}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=oracle;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec ROW(a INT, b VARCHAR(32));
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT ('rec=(' || rec.a ||','|| rec.b||')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}


A complete example for {{sql_mode=DEFAULT}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=DEFAULT;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE done INT DEFAULT FALSE;
  DECLARE rec ROW(a INT, b VARCHAR(32));
  DECLARE c CURSOR FOR SELECT a,b FROM t1;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
  OPEN c;
read_loop:
  LOOP
    FETCH c INTO rec;
    IF done THEN
      LEAVE read_loop;
    END IF;
    SELECT CONCAT('rec=(',rec.a,',',rec.b,')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,0,1,2,5,0,2,0,,0,850,1,0,0,2017-02-07 04:55:36,Allow ROW variables as a cursor FETCH target,"We'll allow {{ROW}} type variables as {{FETCH}} targets:

{code:sql}
FETCH cur INTO rec;
{code}
where {{cur}} is a {{CURSOR}} and {{rec}} is a {{ROW}} type SP variable.
Note, currently an attempt to use {{FETCH}} for a {{ROW}} type variable returns this error:
{noformat}
ERROR 1328 (HY000): Incorrect number of FETCH variables
{noformat}

{{FETCH}} from a cursor {{cur}} into a {{ROW}} variable {{rec}} will work as follows:
- The number of fields in {{cur}} must match the number of fields in {{rec}}. Otherwise, an error is reported.
- Assignment is done from left to right. The first cursor field is assigned to the first variable field, the second cursor field is assigned to the second variable field, etc.
- Field names in {{rec}} are not important and can differ from field names in {{cur}}.


A complete example for {{sql_mode=ORACLE}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=oracle;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec ROW(a INT, b VARCHAR(32));
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT ('rec=(' || rec.a ||','|| rec.b||')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}


A complete example for {{sql_mode=DEFAULT}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=DEFAULT;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE done INT DEFAULT FALSE;
  DECLARE rec ROW(a INT, b VARCHAR(32));
  DECLARE c CURSOR FOR SELECT a,b FROM t1;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
  OPEN c;
read_loop:
  LOOP
    FETCH c INTO rec;
    IF done THEN
      LEAVE read_loop;
    END IF;
    SELECT CONCAT('rec=(',rec.a,',',rec.b,')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
",,2,0,0,0,0.0,"Allow ROW variables as a cursor FETCH target $end$ We'll allow {{ROW}} type variables as {{FETCH}} targets:

{code:sql}
FETCH cur INTO rec;
{code}
where {{cur}} is a {{CURSOR}} and {{rec}} is a {{ROW}} type SP variable.
Note, currently an attempt to use {{FETCH}} for a {{ROW}} type variable returns this error:
{noformat}
ERROR 1328 (HY000): Incorrect number of FETCH variables
{noformat}

{{FETCH}} from a cursor {{cur}} into a {{ROW}} variable {{rec}} will work as follows:
- The number of fields in {{cur}} must match the number of fields in {{rec}}. Otherwise, an error is reported.
- Assignment is done from left to right. The first cursor field is assigned to the first variable field, the second cursor field is assigned to the second variable field, etc.
- Field names in {{rec}} are not important and can differ from field names in {{cur}}.


A complete example for {{sql_mode=ORACLE}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=oracle;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec ROW(a INT, b VARCHAR(32));
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT ('rec=(' || rec.a ||','|| rec.b||')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}


A complete example for {{sql_mode=DEFAULT}}:

{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');

SET sql_mode=DEFAULT;
DROP PROCEDURE IF EXISTS p1;
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE done INT DEFAULT FALSE;
  DECLARE rec ROW(a INT, b VARCHAR(32));
  DECLARE c CURSOR FOR SELECT a,b FROM t1;
  DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
  OPEN c;
read_loop:
  LOOP
    FETCH c INTO rec;
    IF done THEN
      LEAVE read_loop;
    END IF;
    SELECT CONCAT('rec=(',rec.a,',',rec.b,')');
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
 $acceptance criteria:$",2,0,0,0,0,0,1,0.0,43,28,0.651163,23,0.534884,23,0.534884,22,0.511628,22,0.511628
612,MDEV-12011,Technical task,MDEV,2017-02-07 10:10:27,,0,sql_mode=ORACLE: cursor%ROWTYPE in variable declarations,"This task will implement Oracle-stype %ROWTYPE declaration for cursors, for {{sql_mode=ORACLE}}.

Example:
{code:sql}
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
{code}

The record {{rec}} can store the entire row of data fetched from the cursor {{cur}}. There is no a need to specify column names and data types. They're automatically copied from the result set of the cursor {{cur}}.


A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    OPEN c;
    LOOP
      FETCH c INTO rec;
      EXIT WHEN c%NOTFOUND;
      SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
    END LOOP;
    CLOSE c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}


Note, in Oracle it's possible to use {{%ROWTYPE}} variables before opening the referenced cursor, or even without opening it. Also, record variables declared with {{%ROWTYPE}} can be initialized by the assignment operator instead of {{FETCH}}.
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT 10 AS a,20 AS b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    rec.a:= 10;
    rec.b:= 20;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}
%ROWTYPE can be used with open and closed cursors

Also, in MariaDB it's now not possible to declare a variable after cursors (see MDEV-10598). So this won't work:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
BEGIN
  -- statements
END;
{code}
One will have to use an additional nested block:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  DECLARE
    rec cur%ROWTYPE;
  BEGIN
    -- statements
  END;
END;
{code}

",,"sql_mode=ORACLE: cursor%ROWTYPE in variable declarations $end$ This task will implement Oracle-stype %ROWTYPE declaration for cursors, for {{sql_mode=ORACLE}}.

Example:
{code:sql}
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
{code}

The record {{rec}} can store the entire row of data fetched from the cursor {{cur}}. There is no a need to specify column names and data types. They're automatically copied from the result set of the cursor {{cur}}.


A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (30,'b30');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    OPEN c;
    LOOP
      FETCH c INTO rec;
      EXIT WHEN c%NOTFOUND;
      SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
    END LOOP;
    CLOSE c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}


Note, in Oracle it's possible to use {{%ROWTYPE}} variables before opening the referenced cursor, or even without opening it. Also, record variables declared with {{%ROWTYPE}} can be initialized by the assignment operator instead of {{FETCH}}.
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT 10 AS a,20 AS b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    rec.a:= 10;
    rec.b:= 20;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}
%ROWTYPE can be used with open and closed cursors

Also, in MariaDB it's now not possible to declare a variable after cursors (see MDEV-10598). So this won't work:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
BEGIN
  -- statements
END;
{code}
One will have to use an additional nested block:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  DECLARE
    rec cur%ROWTYPE;
  BEGIN
    -- statements
  END;
END;
{code}

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,23,,0,5,4,5,0,2,0,,0,850,5,0,0,2017-02-07 10:10:27,sql_mode=ORACLE: cursor%ROWTYPE in variable declarations,"This task will implement Oracle-stype %ROWTYPE declaration for cursors, for {{sql_mode=ORACLE}}.

Example:
{code:sql}
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
{code}

The record {{rec}} can store the entire row of data fetched from the cursor {{cur}}. There is no a need to specify column names and data types. They're automatically copied from the result set of the cursor {{cur}}.


A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (20,'b30');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    OPEN c;
    LOOP
      FETCH c INTO rec;
      EXIT WHEN c%NOTFOUND;
      SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
    END LOOP;
    CLOSE c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}


Note, in Oracle it's possible to use {{%ROWTYPE}} variables before opening the referenced cursor, or even without opening it. Also, record variables declared with {{%ROWTYPE}} can be initialized by the assignment operator instead of {{FETCH}}.
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT 10 AS a,20 AS b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    rec.a:= 10;
    rec.b:= 20;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}
As far as we currently need this task as a per-requisite to implement {{FOR LOOP}} for cursors (see MDEV-10581), we can have a limitation that it's only possible to read/write a %ROWTYPE variable after the cursor is opened. It will be decided later, depending on implementation efforts.

Also, in MariaDB it's now not possible to declare a variable after cursors (see MDEV-10598). So this won't work:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
BEGIN
  -- statements
END;
{code}
One will have to use an additional nested block:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  DECLARE
    rec cur%ROWTYPE;
  BEGIN
    -- statements
  END;
END;
{code}

",,0,2,0,56,0.132948,"sql_mode=ORACLE: cursor%ROWTYPE in variable declarations $end$ This task will implement Oracle-stype %ROWTYPE declaration for cursors, for {{sql_mode=ORACLE}}.

Example:
{code:sql}
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
{code}

The record {{rec}} can store the entire row of data fetched from the cursor {{cur}}. There is no a need to specify column names and data types. They're automatically copied from the result set of the cursor {{cur}}.


A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
INSERT INTO t1 VALUES (20,'b20');
INSERT INTO t1 VALUES (20,'b30');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT a,b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    OPEN c;
    LOOP
      FETCH c INTO rec;
      EXIT WHEN c%NOTFOUND;
      SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
    END LOOP;
    CLOSE c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}


Note, in Oracle it's possible to use {{%ROWTYPE}} variables before opening the referenced cursor, or even without opening it. Also, record variables declared with {{%ROWTYPE}} can be initialized by the assignment operator instead of {{FETCH}}.
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT 10 AS a,20 AS b FROM t1;
BEGIN
  DECLARE
    rec c%ROWTYPE; 
  BEGIN
    rec.a:= 10;
    rec.b:= 20;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c;
  END;
END;
$$
DELIMITER ;
CALL p1();
{code}
As far as we currently need this task as a per-requisite to implement {{FOR LOOP}} for cursors (see MDEV-10581), we can have a limitation that it's only possible to read/write a %ROWTYPE variable after the cursor is opened. It will be decided later, depending on implementation efforts.

Also, in MariaDB it's now not possible to declare a variable after cursors (see MDEV-10598). So this won't work:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  rec cur%ROWTYPE;
BEGIN
  -- statements
END;
{code}
One will have to use an additional nested block:
{code:sql}
DECLARE
  CURSOR cur IS SELECT a,b FROM t1;
  DECLARE
    rec cur%ROWTYPE;
  BEGIN
    -- statements
  END;
END;
{code}

 $acceptance criteria:$",2,1,1,1,1,1,1,0.0,44,28,0.636364,23,0.522727,23,0.522727,22,0.5,22,0.5
613,MDEV-12086,Technical task,MDEV,2017-02-20 08:35:38,,0,sql_mode=ORACLE: Allow SELECT UNIQUE as a synonym for SELECT DISTINCT,"Oracle supports {{SELECT UNIQUE}} as a synonym for {{SELECT DISTINCT}}.

{noformat}
SQL> SELECT UNIQUE a FROM t1;
	 A
----------
	30
	20
	10
{noformat}

We'll allow the same syntax when running with {{sql_mode=ORACLE}}.
",,"sql_mode=ORACLE: Allow SELECT UNIQUE as a synonym for SELECT DISTINCT $end$ Oracle supports {{SELECT UNIQUE}} as a synonym for {{SELECT DISTINCT}}.

{noformat}
SQL> SELECT UNIQUE a FROM t1;
	 A
----------
	30
	20
	10
{noformat}

We'll allow the same syntax when running with {{sql_mode=ORACLE}}.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,8,,0,1,0,5,0,1,0,,0,850,1,0,0,2017-02-20 08:35:38,sql_mode=ORACLE: allow SELECT UNIQUE as a synonym for SELECT DISTINCT,"Oracle supports {{SELECT UNIQUE}} as a synonym for {{SELECT DISTINCT}}.

{noformat}
SQL> SELECT UNIQUE a FROM t1;
	 A
----------
	30
	20
	10
{noformat}

We'll allow the same syntax when running with {{sql_mode=ORACLE}}.
",,1,0,0,2,0.0222222,"sql_mode=ORACLE: allow SELECT UNIQUE as a synonym for SELECT DISTINCT $end$ Oracle supports {{SELECT UNIQUE}} as a synonym for {{SELECT DISTINCT}}.

{noformat}
SQL> SELECT UNIQUE a FROM t1;
	 A
----------
	30
	20
	10
{noformat}

We'll allow the same syntax when running with {{sql_mode=ORACLE}}.
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,45,29,0.644444,24,0.533333,24,0.533333,23,0.511111,23,0.511111
614,MDEV-12088,Technical task,MDEV,2017-02-20 10:00:43,,0,sql_mode=ORACLE: Do not require BEGIN..END in multi-statement exception handlers in THEN clause,"When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) requires extra {{BEGIN..END}} in multi-statement exception handlers in {{THEN}} clause:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
  BEGIN
    NULL;
    NULL;
  END;
  WHEN OTHERS THEN
  BEGIN
    NULL;
    NULL;
  END;
END;
/
DELIMITER ;
{code}


For better Oracle compatibility, we'll fix it not to require {{BEGIN..END}}, to make this work:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
    NULL;
    NULL;
  WHEN OTHERS THEN
    NULL;
    NULL;
END;
/
DELIMITER ;
{code}

",,"sql_mode=ORACLE: Do not require BEGIN..END in multi-statement exception handlers in THEN clause $end$ When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) requires extra {{BEGIN..END}} in multi-statement exception handlers in {{THEN}} clause:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
  BEGIN
    NULL;
    NULL;
  END;
  WHEN OTHERS THEN
  BEGIN
    NULL;
    NULL;
  END;
END;
/
DELIMITER ;
{code}


For better Oracle compatibility, we'll fix it not to require {{BEGIN..END}}, to make this work:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
    NULL;
    NULL;
  WHEN OTHERS THEN
    NULL;
    NULL;
END;
/
DELIMITER ;
{code}

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,6,,0,1,0,5,0,0,0,,0,850,1,0,0,2017-02-20 10:00:43,sql_mode=ORACLE: Do not require BEGIN..END in multi-statement exception handlers in THEN clause,"When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) requires extra {{BEGIN..END}} in multi-statement exception handlers in {{THEN}} clause:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
  BEGIN
    NULL;
    NULL;
  END;
  WHEN OTHERS THEN
  BEGIN
    NULL;
    NULL;
  END;
END;
/
DELIMITER ;
{code}


For better Oracle compatibility, we'll fix it not to require {{BEGIN..END}}, to make this work:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
    NULL;
    NULL;
  WHEN OTHERS THEN
    NULL;
    NULL;
END;
/
DELIMITER ;
{code}

",,0,0,0,0,0.0,"sql_mode=ORACLE: Do not require BEGIN..END in multi-statement exception handlers in THEN clause $end$ When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) requires extra {{BEGIN..END}} in multi-statement exception handlers in {{THEN}} clause:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
  BEGIN
    NULL;
    NULL;
  END;
  WHEN OTHERS THEN
  BEGIN
    NULL;
    NULL;
  END;
END;
/
DELIMITER ;
{code}


For better Oracle compatibility, we'll fix it not to require {{BEGIN..END}}, to make this work:
{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
  INSERT INTO t1 (a) VALUES (10);
EXCEPTION
  WHEN DUP_VAL_ON_INDEX THEN
    NULL;
    NULL;
  WHEN OTHERS THEN
    NULL;
    NULL;
END;
/
DELIMITER ;
{code}

 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,46,30,0.652174,24,0.521739,24,0.521739,23,0.5,23,0.5
615,MDEV-12089,Technical task,MDEV,2017-02-20 10:08:58,,0,sql_mode=ORACLE: Understand optional routine name after the END keyword,"When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) does not understand the optional function or procedure name after the {{END}} keyword which ends the entire routine definition. We'll extend the parser to understand the optional name, so this script is parsed without syntax errors:

{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
END p1; -- Notice p1
/
DELIMITER ;
{code}
In case of stand-alone routines we'll also allow qualified names: {{END test.p1;}}

h2. Originally this task included changes for package routines, but it does not any more:
Optional routine names are also possible inside a package body definition:
{code:sql}
DELIMITER $$
CREATE PACKAGE BODY test2 AS
  FUNCTION f1 RETURN INT AS
  BEGIN
    RETURN 10;
  END f1;
  PROCEDURE p1 AS
  BEGIN
    NULL;
  END p1;
END test2;
$$
DELIMITER ;
DROP PACKAGE test2;
{code}
In case of package routines the name specified in {{END}} cannot be qualified.

If the name specified in {{CREATE}} does not match the name specified in {{END}}, an error will be reported.

Under terms of this patch we'll only fix traditional (stand-alone) stored functions and procedures.
Handling the name after the {{END}} for package routines will be done in package related tasks (e.g. MDEV-10591 or MDEV-11952)



",,"sql_mode=ORACLE: Understand optional routine name after the END keyword $end$ When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) does not understand the optional function or procedure name after the {{END}} keyword which ends the entire routine definition. We'll extend the parser to understand the optional name, so this script is parsed without syntax errors:

{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
END p1; -- Notice p1
/
DELIMITER ;
{code}
In case of stand-alone routines we'll also allow qualified names: {{END test.p1;}}

h2. Originally this task included changes for package routines, but it does not any more:
Optional routine names are also possible inside a package body definition:
{code:sql}
DELIMITER $$
CREATE PACKAGE BODY test2 AS
  FUNCTION f1 RETURN INT AS
  BEGIN
    RETURN 10;
  END f1;
  PROCEDURE p1 AS
  BEGIN
    NULL;
  END p1;
END test2;
$$
DELIMITER ;
DROP PACKAGE test2;
{code}
In case of package routines the name specified in {{END}} cannot be qualified.

If the name specified in {{CREATE}} does not match the name specified in {{END}}, an error will be reported.

Under terms of this patch we'll only fix traditional (stand-alone) stored functions and procedures.
Handling the name after the {{END}} for package routines will be done in package related tasks (e.g. MDEV-10591 or MDEV-11952)



 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,0,1,0,5,0,5,0,,0,850,1,0,0,2017-02-20 10:08:58,sql_mode=ORACLE: Understand optional routine name after the END keyword,"
When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) does not understand the optional function or procedure name after the {{END}} keyword which ends the entire routine definition. We'll extend the parser to understand the optional name, so this script is parsed without syntax errors:

{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
END p1; -- Notice p1
/
DELIMITER ;
{code}
",,0,5,0,138,1.76923,"sql_mode=ORACLE: Understand optional routine name after the END keyword $end$ 
When running with {{sql_mode=ORACLE}}, MariaDB (the {{bb-10.2-compatibility}} branch) does not understand the optional function or procedure name after the {{END}} keyword which ends the entire routine definition. We'll extend the parser to understand the optional name, so this script is parsed without syntax errors:

{code:sql}
SET sql_mode=ORACLE;
DROP PROCEDURE p1;
DELIMITER /
CREATE PROCEDURE p1 AS
BEGIN
END p1; -- Notice p1
/
DELIMITER ;
{code}
 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,47,30,0.638298,24,0.510638,24,0.510638,23,0.489362,23,0.489362
616,MDEV-12098,Technical task,MDEV,2017-02-20 17:48:40,,0,sql_mode=ORACLE: Implicit cursor FOR loop,"In addition to {{FOR}} loops on explicitly declared cursors (MDEV-10581), we'll implement {{FOR}} loops for implicit cursors.

The SQL query for the implicit cursor is declared directly inside the {{FOR}} loop:
{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b INT);
INSERT INTO t1 VALUES (10,20);

DROP PROCEDURE IF EXISTS p1;
DELIMITER /
CREATE PROCEDURE p1 AS 
BEGIN
  FOR rec IN (SELECT a, b FROM t1)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
/
DELIMITER ;
CALL p1();
{code}
",,"sql_mode=ORACLE: Implicit cursor FOR loop $end$ In addition to {{FOR}} loops on explicitly declared cursors (MDEV-10581), we'll implement {{FOR}} loops for implicit cursors.

The SQL query for the implicit cursor is declared directly inside the {{FOR}} loop:
{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b INT);
INSERT INTO t1 VALUES (10,20);

DROP PROCEDURE IF EXISTS p1;
DELIMITER /
CREATE PROCEDURE p1 AS 
BEGIN
  FOR rec IN (SELECT a, b FROM t1)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
/
DELIMITER ;
CALL p1();
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,1,1,4,5,0,0,0,,0,850,1,0,0,2017-02-20 17:48:40,sql_mode=ORACLE: Implicit cursor FOR loop,"In addition to {{FOR}} loops on explicitly declared cursors (MDEV-10581), we'll implement {{FOR}} loops for implicit cursors.

The SQL query for the implicit cursor is declared directly inside the {{FOR}} loop:
{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b INT);
INSERT INTO t1 VALUES (10,20);

DROP PROCEDURE IF EXISTS p1;
DELIMITER /
CREATE PROCEDURE p1 AS 
BEGIN
  FOR rec IN (SELECT a, b FROM t1)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
/
DELIMITER ;
CALL p1();
{code}
",,0,0,0,0,0.0,"sql_mode=ORACLE: Implicit cursor FOR loop $end$ In addition to {{FOR}} loops on explicitly declared cursors (MDEV-10581), we'll implement {{FOR}} loops for implicit cursors.

The SQL query for the implicit cursor is declared directly inside the {{FOR}} loop:
{code:sql}
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b INT);
INSERT INTO t1 VALUES (10,20);

DROP PROCEDURE IF EXISTS p1;
DELIMITER /
CREATE PROCEDURE p1 AS 
BEGIN
  FOR rec IN (SELECT a, b FROM t1)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
/
DELIMITER ;
CALL p1();
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,48,31,0.645833,25,0.520833,25,0.520833,24,0.5,24,0.5
617,MDEV-12107,Technical task,MDEV,2017-02-22 08:29:24,,0,sql_mode=ORACLE: Inside routines the CALL keywoard is optional,"Normally, a stored procedure call is done using this syntax:
{code:sql}
  CALL p1(10);
  CALL p2;
  CALL test.p1(10);
  CALL test.p2;
{code}

However, when we want to call a stored procedure from another routine or an anonymous block, the keyword {{CALL}} is optional. We'll fix the parser to understand this syntax.

Example:
{code:sql}
SET sql_mode=ORACLE;
DELIMITER /
CREATE OR REPLACE PROCEDURE p1(a INT) AS
BEGIN
  NULL;
END;
/
CREATE OR REPLACE PROCEDURE p2 AS
BEGIN
  NULL;
END;
/
BEGIN
  p1(10);
  p2;
  test.p1(10);
  test.p2(10);
END;
/
{code}
",,"sql_mode=ORACLE: Inside routines the CALL keywoard is optional $end$ Normally, a stored procedure call is done using this syntax:
{code:sql}
  CALL p1(10);
  CALL p2;
  CALL test.p1(10);
  CALL test.p2;
{code}

However, when we want to call a stored procedure from another routine or an anonymous block, the keyword {{CALL}} is optional. We'll fix the parser to understand this syntax.

Example:
{code:sql}
SET sql_mode=ORACLE;
DELIMITER /
CREATE OR REPLACE PROCEDURE p1(a INT) AS
BEGIN
  NULL;
END;
/
CREATE OR REPLACE PROCEDURE p2 AS
BEGIN
  NULL;
END;
/
BEGIN
  p1(10);
  p2;
  test.p1(10);
  test.p2(10);
END;
/
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,0,1,0,5,0,1,0,,0,850,1,0,0,2017-02-22 08:29:24,sql_mode=ORACLE: Inside routines the CALL keywoard is optional,"Normally, a stored procedure call is done using this syntax:
{code:sql}
  CALL p1(10);
  CALL p2;
{code}

However, when we want to call a stored procedure from another routine or an anonymous block, the keyword {{CALL}} is optional. We'll fix the parser to understand this syntax.

Example:
{code:sql}
SET sql_mode=ORACLE;
DELIMITER /
CREATE OR REPLACE PROCEDURE p1(a INT) AS
BEGIN
  NULL;
END;
/
CREATE OR REPLACE PROCEDURE p2 AS
BEGIN
  NULL;
END;
/
BEGIN
  p1(10);
  p2;
END;
/
{code}
",,0,1,0,6,0.0674157,"sql_mode=ORACLE: Inside routines the CALL keywoard is optional $end$ Normally, a stored procedure call is done using this syntax:
{code:sql}
  CALL p1(10);
  CALL p2;
{code}

However, when we want to call a stored procedure from another routine or an anonymous block, the keyword {{CALL}} is optional. We'll fix the parser to understand this syntax.

Example:
{code:sql}
SET sql_mode=ORACLE;
DELIMITER /
CREATE OR REPLACE PROCEDURE p1(a INT) AS
BEGIN
  NULL;
END;
/
CREATE OR REPLACE PROCEDURE p2 AS
BEGIN
  NULL;
END;
/
BEGIN
  p1(10);
  p2;
END;
/
{code}
 $acceptance criteria:$",1,1,1,0,0,0,1,0.0,49,31,0.632653,25,0.510204,25,0.510204,24,0.489796,24,0.489796
618,MDEV-12111,Task,MDEV,2017-02-22 13:56:41,,0,Merge 10.0.30 to 10.0-Galera,"* Merge 5.5-galera to 10.0-galera (naturally after we have both releases)
*  https://github.com/codership/mysql-wsrep/commits/5.6 Merge commits:
** 748ffbaa946
** d76a6012a34
** 2bf4384ddb5
** a09f9eb5f67
** 51cd60c1262
** d43dd570fad
** a59d98afbcf
** cb07e6329dc
** ea8affc1dfb
** 22610397fd1
** b1bbf4a5fb5
** 796d45d340d
** 9c3335331bb
** 607b8b2b3c5
** e49a55424f6
** 1325e00c524
** 007066255a8
** 1659e1d0bcf
** 97c9125fccf
** 1fee325cf3d
** e482bef7a79
** 49434d24a94
** dc0bf7ae817
** 38bbf586055
** 30ba89989a4
** 53c14f22711
** 4b0ec2936ce
** 2d599706c90
** c98ca240c96
** f623cf8443e
** 14301279075
** 5772735a9c5
** fbc71ce7844
** ded7834bfde
** a442b79209f
** ba647dc7b1e
** b9dd2df4d09
** 0f12a08923f
** 0babfb6ae4d
** 5d46632cadb
** de641f23a95
** 104cb35c709
** 16a6d4d3d8e
** 88b8fca03fd
** 21f3a5050e3
** 5361a649371
** ec69e1444f2
** cc54b12a874
** 0122a99b9d9
** 5667ab4a13f
** e27e4d6b99a
** 7c55962a7bf
** c47b8712a25
** 4a5d6d01a00
** c09b6140ce4
** c1407d7bd9a
** 75718ed06c7
** 1f9ae89ff9a
** 7516a6f90ae
** c1cbdc8dd62
** 10d85108404
** 687f068c203
** ebd02fc9711
** cb960d8309b
** ace3688592e
* Test and ask review
* https://github.com/MariaDB/server/tree/10.0 Merge tag mariadb-10.0.30 (when released)
* Test
* Create tag mariadb-galera-10.0.30
* Create todo for Daniel Bartholomew to release
",,"Merge 10.0.30 to 10.0-Galera $end$ * Merge 5.5-galera to 10.0-galera (naturally after we have both releases)
*  https://github.com/codership/mysql-wsrep/commits/5.6 Merge commits:
** 748ffbaa946
** d76a6012a34
** 2bf4384ddb5
** a09f9eb5f67
** 51cd60c1262
** d43dd570fad
** a59d98afbcf
** cb07e6329dc
** ea8affc1dfb
** 22610397fd1
** b1bbf4a5fb5
** 796d45d340d
** 9c3335331bb
** 607b8b2b3c5
** e49a55424f6
** 1325e00c524
** 007066255a8
** 1659e1d0bcf
** 97c9125fccf
** 1fee325cf3d
** e482bef7a79
** 49434d24a94
** dc0bf7ae817
** 38bbf586055
** 30ba89989a4
** 53c14f22711
** 4b0ec2936ce
** 2d599706c90
** c98ca240c96
** f623cf8443e
** 14301279075
** 5772735a9c5
** fbc71ce7844
** ded7834bfde
** a442b79209f
** ba647dc7b1e
** b9dd2df4d09
** 0f12a08923f
** 0babfb6ae4d
** 5d46632cadb
** de641f23a95
** 104cb35c709
** 16a6d4d3d8e
** 88b8fca03fd
** 21f3a5050e3
** 5361a649371
** ec69e1444f2
** cc54b12a874
** 0122a99b9d9
** 5667ab4a13f
** e27e4d6b99a
** 7c55962a7bf
** c47b8712a25
** 4a5d6d01a00
** c09b6140ce4
** c1407d7bd9a
** 75718ed06c7
** 1f9ae89ff9a
** 7516a6f90ae
** c1cbdc8dd62
** 10d85108404
** 687f068c203
** ebd02fc9711
** cb960d8309b
** ace3688592e
* Test and ask review
* https://github.com/MariaDB/server/tree/10.0 Merge tag mariadb-10.0.30 (when released)
* Test
* Create tag mariadb-galera-10.0.30
* Create todo for Daniel Bartholomew to release
 $acceptance criteria:$",,Jan Lindström,Jan Lindström,Blocker,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-02 11:33:47,Merge 10.0.30 to 10.0-Galera,"* Merge 5.5-galera to 10.0-galera (naturally after we have both releases)
*  https://github.com/codership/mysql-wsrep/commits/5.6 Merge commits:
** 748ffbaa946
** d76a6012a34
** 2bf4384ddb5
** a09f9eb5f67
** 51cd60c1262
** d43dd570fad
** a59d98afbcf
** cb07e6329dc
** ea8affc1dfb
** 22610397fd1
** b1bbf4a5fb5
** 796d45d340d
** 9c3335331bb
** 607b8b2b3c5
** e49a55424f6
** 1325e00c524
** 007066255a8
** 1659e1d0bcf
** 97c9125fccf
** 1fee325cf3d
** e482bef7a79
** 49434d24a94
** dc0bf7ae817
** 38bbf586055
** 30ba89989a4
** 53c14f22711
** 4b0ec2936ce
** 2d599706c90
** c98ca240c96
** f623cf8443e
** 14301279075
** 5772735a9c5
** fbc71ce7844
** ded7834bfde
** a442b79209f
** ba647dc7b1e
** b9dd2df4d09
** 0f12a08923f
** 0babfb6ae4d
** 5d46632cadb
** de641f23a95
** 104cb35c709
** 16a6d4d3d8e
** 88b8fca03fd
** 21f3a5050e3
** 5361a649371
** ec69e1444f2
** cc54b12a874
** 0122a99b9d9
** 5667ab4a13f
** e27e4d6b99a
** 7c55962a7bf
** c47b8712a25
** 4a5d6d01a00
** c09b6140ce4
** c1407d7bd9a
** 75718ed06c7
** 1f9ae89ff9a
** 7516a6f90ae
** c1cbdc8dd62
** 10d85108404
** 687f068c203
** ebd02fc9711
** cb960d8309b
** ace3688592e
* Test and ask review
* https://github.com/MariaDB/server/tree/10.0 Merge tag mariadb-10.0.30 (when released)
* Test
* Create tag mariadb-galera-10.0.30
* Create todo for Daniel Bartholomew to release
",,0,0,0,0,0.0,"Merge 10.0.30 to 10.0-Galera $end$ * Merge 5.5-galera to 10.0-galera (naturally after we have both releases)
*  https://github.com/codership/mysql-wsrep/commits/5.6 Merge commits:
** 748ffbaa946
** d76a6012a34
** 2bf4384ddb5
** a09f9eb5f67
** 51cd60c1262
** d43dd570fad
** a59d98afbcf
** cb07e6329dc
** ea8affc1dfb
** 22610397fd1
** b1bbf4a5fb5
** 796d45d340d
** 9c3335331bb
** 607b8b2b3c5
** e49a55424f6
** 1325e00c524
** 007066255a8
** 1659e1d0bcf
** 97c9125fccf
** 1fee325cf3d
** e482bef7a79
** 49434d24a94
** dc0bf7ae817
** 38bbf586055
** 30ba89989a4
** 53c14f22711
** 4b0ec2936ce
** 2d599706c90
** c98ca240c96
** f623cf8443e
** 14301279075
** 5772735a9c5
** fbc71ce7844
** ded7834bfde
** a442b79209f
** ba647dc7b1e
** b9dd2df4d09
** 0f12a08923f
** 0babfb6ae4d
** 5d46632cadb
** de641f23a95
** 104cb35c709
** 16a6d4d3d8e
** 88b8fca03fd
** 21f3a5050e3
** 5361a649371
** ec69e1444f2
** cc54b12a874
** 0122a99b9d9
** 5667ab4a13f
** e27e4d6b99a
** 7c55962a7bf
** c47b8712a25
** 4a5d6d01a00
** c09b6140ce4
** c1407d7bd9a
** 75718ed06c7
** 1f9ae89ff9a
** 7516a6f90ae
** c1cbdc8dd62
** 10d85108404
** 687f068c203
** ebd02fc9711
** cb960d8309b
** ace3688592e
* Test and ask review
* https://github.com/MariaDB/server/tree/10.0 Merge tag mariadb-10.0.30 (when released)
* Test
* Create tag mariadb-galera-10.0.30
* Create todo for Daniel Bartholomew to release
 $acceptance criteria:$",0,0,0,0,0,0,0,189.617,3,2,0.666667,0,0.0,0,0.0,0,0.0,0,0.0
619,MDEV-12133,Technical task,MDEV,2017-02-27 06:52:22,,0,sql_mode=ORACLE: table%ROWTYPE in variable declarations,"This task will implement Oracle-stype table%ROWTYPE declarations, for sql_mode=ORACLE.

Example:

{code:sql}
rec t1%ROWTYPE;
{code}

The record variable {{rec}} can store the entire row of the data fetched from the table t1. There is no a need to specify column names and data types. They're automatically copied from {{t1}}.

Under scope of this task, we'll implement table%ROWTYPE for routine variables. Using table%ROWTYPE for routine parameters and function return values will be done separately.


A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec t1%ROWTYPE; 
  CURSOR c IS SELECT * FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
rec=(10,b10)
{noformat}


Data types will be resolved at the very beginning of a routine execution, in {{sp_rcontext::create()}}. If the tables referenced in %ROWTYPE declarations are altered inside the routine, this will not affect structures of the referencing %ROWTYPE variables. In the below example the variable {{rec}} will have only two fields {{a}} and {{b}}, it will not have the field {{c}}. The fact that {{rec}} is declared after the {{ALTER}} statement does not matter. This implementations will be close to Oracle, who determines all data types at {{CREATE PROCEDURE}} time.


{{table%ROWTYPE}} and implicit {{ROW}} variables will be mutually assignable if they have the same number of fields. Note, Oracle has stricter rules, it also checks field names, but in a very strange way. See ""Oracle implementation details"". We won't check field names. Assignment will be done from left to right (the N'th source field is assigned to the N'th destination field).

In the below example all three variables {{rec0}}, {{rec1}}, {{rec2}} will be mutually assignable, because they have the same number of fields:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1, t2;
CREATE TABLE t1 (a INT, b VARCHAR(32));
CREATE TABLE t2 (c INT, d VARCHAR(32));
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec0 ROW(x INT,y INT);
  rec1 t1%ROWTYPE;
  rec2 t2%ROWTYPE;
BEGIN
  rec0:=rec1;
  rec0:=rec2;
  rec1:=rec0;
  rec1:=rec2;
  rec2:=rec0;
  rec2:=rec1;
END;
$$
DELIMITER ;
CALL p1();
{code}


It will be possible to pass a {{table%ROWTYPE}} variable into a routine with a compatible explicit {{ROW}} argument:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
DROP PROCEDURE IF EXISTS p1;
DROP PROCEDURE IF EXISTS p2;
DELIMITER $$
CREATE PROCEDURE p1(a ROW(a INT, b VARCHAR(20)))
AS
BEGIN
  SELECT a.a, a.b;
END;
$$
CREATE PROCEDURE p2 AS
  rec1 t1%ROWTYPE:=ROW(10,'bb');
BEGIN
  CALL p1(rec1);
END;
$$
DELIMITER ;
CALL p2();
{code}
{noformat}
+------+------+
| a.a  | a.b  |
+------+------+
| 10   | bb   |
+------+------+
{noformat}
",,"sql_mode=ORACLE: table%ROWTYPE in variable declarations $end$ This task will implement Oracle-stype table%ROWTYPE declarations, for sql_mode=ORACLE.

Example:

{code:sql}
rec t1%ROWTYPE;
{code}

The record variable {{rec}} can store the entire row of the data fetched from the table t1. There is no a need to specify column names and data types. They're automatically copied from {{t1}}.

Under scope of this task, we'll implement table%ROWTYPE for routine variables. Using table%ROWTYPE for routine parameters and function return values will be done separately.


A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec t1%ROWTYPE; 
  CURSOR c IS SELECT * FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
rec=(10,b10)
{noformat}


Data types will be resolved at the very beginning of a routine execution, in {{sp_rcontext::create()}}. If the tables referenced in %ROWTYPE declarations are altered inside the routine, this will not affect structures of the referencing %ROWTYPE variables. In the below example the variable {{rec}} will have only two fields {{a}} and {{b}}, it will not have the field {{c}}. The fact that {{rec}} is declared after the {{ALTER}} statement does not matter. This implementations will be close to Oracle, who determines all data types at {{CREATE PROCEDURE}} time.


{{table%ROWTYPE}} and implicit {{ROW}} variables will be mutually assignable if they have the same number of fields. Note, Oracle has stricter rules, it also checks field names, but in a very strange way. See ""Oracle implementation details"". We won't check field names. Assignment will be done from left to right (the N'th source field is assigned to the N'th destination field).

In the below example all three variables {{rec0}}, {{rec1}}, {{rec2}} will be mutually assignable, because they have the same number of fields:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1, t2;
CREATE TABLE t1 (a INT, b VARCHAR(32));
CREATE TABLE t2 (c INT, d VARCHAR(32));
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec0 ROW(x INT,y INT);
  rec1 t1%ROWTYPE;
  rec2 t2%ROWTYPE;
BEGIN
  rec0:=rec1;
  rec0:=rec2;
  rec1:=rec0;
  rec1:=rec2;
  rec2:=rec0;
  rec2:=rec1;
END;
$$
DELIMITER ;
CALL p1();
{code}


It will be possible to pass a {{table%ROWTYPE}} variable into a routine with a compatible explicit {{ROW}} argument:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
DROP PROCEDURE IF EXISTS p1;
DROP PROCEDURE IF EXISTS p2;
DELIMITER $$
CREATE PROCEDURE p1(a ROW(a INT, b VARCHAR(20)))
AS
BEGIN
  SELECT a.a, a.b;
END;
$$
CREATE PROCEDURE p2 AS
  rec1 t1%ROWTYPE:=ROW(10,'bb');
BEGIN
  CALL p1(rec1);
END;
$$
DELIMITER ;
CALL p2();
{code}
{noformat}
+------+------+
| a.a  | a.b  |
+------+------+
| 10   | bb   |
+------+------+
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,18,,0,5,4,5,0,2,0,,0,850,5,0,0,2017-02-27 06:52:22,sql_mode=ORACLE: table%ROWTYPE in variable declarations,"This task will implement Oracle-stype table%ROWTYPE declarations, for sql_mode=ORACLE.

Example:

{code:sql}
rec t1%ROWTYPE;
{code}

The record variable {{rec}} can store the entire row of the data fetched from the table t1. There is no a need to specify column names and data types. They're automatically copied from {{t1}}.

A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec t1%ROWTYPE; 
  CURSOR c IS SELECT * FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
rec=(10,b10)
{noformat}
",,0,2,0,339,2.58779,"sql_mode=ORACLE: table%ROWTYPE in variable declarations $end$ This task will implement Oracle-stype table%ROWTYPE declarations, for sql_mode=ORACLE.

Example:

{code:sql}
rec t1%ROWTYPE;
{code}

The record variable {{rec}} can store the entire row of the data fetched from the table t1. There is no a need to specify column names and data types. They're automatically copied from {{t1}}.

A complete working example:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DROP PROCEDURE p1;
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec t1%ROWTYPE; 
  CURSOR c IS SELECT * FROM t1;
BEGIN
  OPEN c;
  LOOP
    FETCH c INTO rec;
    EXIT WHEN c%NOTFOUND;
    SELECT 'rec=(' || rec.a ||','|| rec.b||')' AS c FROM dual;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
rec=(10,b10)
{noformat}
 $acceptance criteria:$",2,1,1,1,1,1,1,0.0,50,32,0.64,26,0.52,25,0.5,24,0.48,24,0.48
620,MDEV-12137,Technical task,MDEV,2017-02-27 10:11:35,,0,DELETE statement with the same source and target,"Example :
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);{code}
currently returns:
{noformat}
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'DELETE' and as a separate source for data
{noformat}

The same script works fine in Oracle.
",,"DELETE statement with the same source and target $end$ Example :
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);{code}
currently returns:
{noformat}
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'DELETE' and as a separate source for data
{noformat}

The same script works fine in Oracle.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,1,3,3,5,0,1,0,,0,850,3,0,0,2017-02-27 10:11:35,sql_mode=ORACLE: DELETE statement with the same source and target,"Example :
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);{code}
currently returns:
{noformat}
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'DELETE' and as a separate source for data
{noformat}

The same script works fine in Oracle.
",,1,0,0,1,0.0120482,"sql_mode=ORACLE: DELETE statement with the same source and target $end$ Example :
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);{code}
currently returns:
{noformat}
DELETE FROM t1 WHERE c1 IN (SELECT b.c1 FROM t1 b WHERE b.c2=0);
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'DELETE' and as a separate source for data
{noformat}

The same script works fine in Oracle.
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,51,33,0.647059,27,0.529412,26,0.509804,25,0.490196,25,0.490196
621,MDEV-12143,Technical task,MDEV,2017-02-27 12:47:14,,0,sql_mode=ORACLE: Make the CONCAT function ignore NULL arguments,"Under terms of MDEV-11880, we fixed the concatenation operator {{||}} to ignore NULL arguments.

In addition to the concatenation operator, Oracle has also the function {{CONCAT}} with two arguments, so these two queries should be equivalent:

{code:sql}
SELECT a||b FROM t1;
SELECT CONCAT(a,b) FROM t1;
{code}

The patch for MDEV-11880 changed only behavior of the concatenation operator {{||}}.
Under terms of this task we'll also change {{CONCAT}} to ignore NULL arguments.
",,"sql_mode=ORACLE: Make the CONCAT function ignore NULL arguments $end$ Under terms of MDEV-11880, we fixed the concatenation operator {{||}} to ignore NULL arguments.

In addition to the concatenation operator, Oracle has also the function {{CONCAT}} with two arguments, so these two queries should be equivalent:

{code:sql}
SELECT a||b FROM t1;
SELECT CONCAT(a,b) FROM t1;
{code}

The patch for MDEV-11880 changed only behavior of the concatenation operator {{||}}.
Under terms of this task we'll also change {{CONCAT}} to ignore NULL arguments.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,1,2,2,5,0,1,0,,0,850,2,0,0,2017-02-27 12:47:14,sql_mode=ORACLE: make the CONCAT function ignore NULL arguments,"Under terms of MDEV-11880, we fixed the concatenation operator {{||}} to ignore NULL arguments.

In addition to the concatenation operator, Oracle has also the function {{CONCAT}} with two arguments, so these two queries should be equivalent:

{code:sql}
SELECT a||b FROM t1;
SELECT CONCAT(a,b) FROM t1;
{code}

The patch for MDEV-11880 changed only behavior of the concatenation operator {{||}}.
Under terms of this task we'll also change {{CONCAT}} to ignore NULL arguments.
",,1,0,0,2,0.0121951,"sql_mode=ORACLE: make the CONCAT function ignore NULL arguments $end$ Under terms of MDEV-11880, we fixed the concatenation operator {{||}} to ignore NULL arguments.

In addition to the concatenation operator, Oracle has also the function {{CONCAT}} with two arguments, so these two queries should be equivalent:

{code:sql}
SELECT a||b FROM t1;
SELECT CONCAT(a,b) FROM t1;
{code}

The patch for MDEV-11880 changed only behavior of the concatenation operator {{||}}.
Under terms of this task we'll also change {{CONCAT}} to ignore NULL arguments.
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,52,34,0.653846,27,0.519231,26,0.5,25,0.480769,25,0.480769
622,MDEV-12160,Task,MDEV,2017-03-01 14:08:44,,0,Modern alternative to the SHA1 authentication plugin,"Authentication plugin that
* uses a crypto-hash that is considered secure nowadays
* does not allow to get the password even if {{mysql.user}} is read and the authentication exchange is intercepted
* as easy to use as native_mysql_authentication plugin, no public/private key files or anything
* pure plugin",,"Modern alternative to the SHA1 authentication plugin $end$ Authentication plugin that
* uses a crypto-hash that is considered secure nowadays
* does not allow to get the password even if {{mysql.user}} is read and the authentication exchange is intercepted
* as easy to use as native_mysql_authentication plugin, no public/private key files or anything
* pure plugin $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,14,,0,2,5,1,0,4,0,,0,850,2,3,0,2017-03-02 12:17:42,SHA-2 (or SHA-3) authentication plugin,"Authentication plugin that
* uses a crypto-hash that is considered secure nowadays
* does not allow to get the password even if {{mysql.user}} is read and the authentication exchange is intercepted
* as easy to use as native_mysql_authentication plugin, no public/private key files or anything
* pure plugin",,1,0,0,8,0.0892857,"SHA-2 (or SHA-3) authentication plugin $end$ Authentication plugin that
* uses a crypto-hash that is considered secure nowadays
* does not allow to get the password even if {{mysql.user}} is read and the authentication exchange is intercepted
* as easy to use as native_mysql_authentication plugin, no public/private key files or anything
* pure plugin $acceptance criteria:$",1,1,1,0,0,0,1,22.1333,47,23,0.489362,17,0.361702,13,0.276596,12,0.255319,10,0.212766
623,MDEV-12164,Task,MDEV,2017-03-02 12:12:04,,0,10.1.22 merge,"* 5.5 (/)  -> Done by [~marko]
* 10.0 (/)  -> Done by [~marko]
* 10.0-galera (/) -> No new release here.",,"10.1.22 merge $end$ * 5.5 (/)  -> Done by [~marko]
* 10.0 (/)  -> Done by [~marko]
* 10.0-galera (/) -> No new release here. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2017-03-02 12:15:22,10.1.22 merge,"* 5.5
* 10.0
* 10.0-galera",,0,1,0,16,1.45455,"10.1.22 merge $end$ * 5.5
* 10.0
* 10.0-galera $acceptance criteria:$",1,1,1,1,1,0,1,0.05,48,24,0.5,18,0.375,13,0.270833,12,0.25,10,0.208333
624,MDEV-12172,Task,MDEV,2017-03-03 18:01:42,,0,Implement tables specified by table value constructors.,"SQL Standards starting from SQL-1999 uses the construct <table value constructor> as
one of the alternatives for <simple table>:
{noformat}
<simple table> ::=
    <query specification>
  | <table value constructor>
  | <explicit table>
{noformat}
Currently MariaDB uses <table value constructor> only in insert statements:
{code:sql}
INSERT INTO t VALUES (1,'xx'), (5,'yyy'), (1,'zzz');
{code}
With <table value constructor> it will be possible to use such CTE specification:
{code:sql}
WITH t (a,c) AS  (SELECT * FROM VALUES (1,'xx'), (5,'yyy'), (1,'zzz')) ...
{code}
Now instead of this we have to use something like this:
{code:sql}
WITH t (a,c) AS (SELECT 1,  'xx' UNION ALL SELECT 5,  'yyy' UNION ALL 1, 'zzz') ...
{code}
Processing of the latter will take much more memory.

Besides with the possibility of using table value constructors in CTE specifications some optimization transformations for IN predicates will be possible.
",,"Implement tables specified by table value constructors. $end$ SQL Standards starting from SQL-1999 uses the construct <table value constructor> as
one of the alternatives for <simple table>:
{noformat}
<simple table> ::=
    <query specification>
  | <table value constructor>
  | <explicit table>
{noformat}
Currently MariaDB uses <table value constructor> only in insert statements:
{code:sql}
INSERT INTO t VALUES (1,'xx'), (5,'yyy'), (1,'zzz');
{code}
With <table value constructor> it will be possible to use such CTE specification:
{code:sql}
WITH t (a,c) AS  (SELECT * FROM VALUES (1,'xx'), (5,'yyy'), (1,'zzz')) ...
{code}
Now instead of this we have to use something like this:
{code:sql}
WITH t (a,c) AS (SELECT 1,  'xx' UNION ALL SELECT 5,  'yyy' UNION ALL 1, 'zzz') ...
{code}
Processing of the latter will take much more memory.

Besides with the possibility of using table value constructors in CTE specifications some optimization transformations for IN predicates will be possible.
 $acceptance criteria:$",,Igor Babaev,Igor Babaev,Major,8,,0,4,1,1,0,1,0,,0,850,0,1,0,2017-10-24 14:28:30,Implement tables specified by table value constructors.,"SQL Standards starting from SQL-1999 uses the construct <table value constructor> as
one of the alternatives for <simple table>:
{noformat}
<simple table> ::=
    <query specification>
  | <table value constructor>
  | <explicit table>
{noformat}
Currently MariaDB uses <table value constructor> only in insert statements:
{code:sql}
INSERT INTO t VALUES (1,'xx'), (5,'yyy'), (1,'zzz');
{code}
With <table value constructor> it will be possible to use such CTE specification:
{code:sql}
WITH t (a,c) AS  (SELECT * FROM VALUES (1,'xx'), (5,'yyy'), (1,'zzz')) ...
{code}
Now instead of this we have to use something like this:
{code:sql}
WITH t (a,c) AS (SELECT 1,  'xx' UNION ALL SELECT 5,  'yyy' UNION ALL 1, 'zzz') ...
{code}
Processing of the latter will take much more memory.

Besides with the possibility of using table value constructors in CTE specifications some optimization transformations for IN predicates will be possible.
",,0,0,0,0,0.0,"Implement tables specified by table value constructors. $end$ SQL Standards starting from SQL-1999 uses the construct <table value constructor> as
one of the alternatives for <simple table>:
{noformat}
<simple table> ::=
    <query specification>
  | <table value constructor>
  | <explicit table>
{noformat}
Currently MariaDB uses <table value constructor> only in insert statements:
{code:sql}
INSERT INTO t VALUES (1,'xx'), (5,'yyy'), (1,'zzz');
{code}
With <table value constructor> it will be possible to use such CTE specification:
{code:sql}
WITH t (a,c) AS  (SELECT * FROM VALUES (1,'xx'), (5,'yyy'), (1,'zzz')) ...
{code}
Now instead of this we have to use something like this:
{code:sql}
WITH t (a,c) AS (SELECT 1,  'xx' UNION ALL SELECT 5,  'yyy' UNION ALL 1, 'zzz') ...
{code}
Processing of the latter will take much more memory.

Besides with the possibility of using table value constructors in CTE specifications some optimization transformations for IN predicates will be possible.
 $acceptance criteria:$",0,0,0,0,0,0,0,5636.43,1,1,1.0,1,1.0,0,0.0,0,0.0,0,0.0
625,MDEV-12176,Task,MDEV,2017-03-03 18:39:57,,0,Transform [NOT] IN predicate with long list of values INTO [NOT] IN subquery. ,"Currently if MariaDB checks whether possibility to use range access based on [NOT] IN predicate with a long list of values it build SEL_TREEs taking huge amount of memory.
Meanwhile using CTE specified by table value constructor such a predicate could be transformed into an equivalent [NOT] IN subquery:
{noformat}
SELECT ... WHERE ... (expr1, ...) [NOT] IN (value_list) ...;
=>
WITH t(col1, ...) AS (SELECT * FROM VALUES value_list) 
SELECT ... WHERE ... (expr1, ...) [NOT] IN (SELECT * FROM t)...;
{noformat}
",,"Transform [NOT] IN predicate with long list of values INTO [NOT] IN subquery.  $end$ Currently if MariaDB checks whether possibility to use range access based on [NOT] IN predicate with a long list of values it build SEL_TREEs taking huge amount of memory.
Meanwhile using CTE specified by table value constructor such a predicate could be transformed into an equivalent [NOT] IN subquery:
{noformat}
SELECT ... WHERE ... (expr1, ...) [NOT] IN (value_list) ...;
=>
WITH t(col1, ...) AS (SELECT * FROM VALUES value_list) 
SELECT ... WHERE ... (expr1, ...) [NOT] IN (SELECT * FROM t)...;
{noformat}
 $acceptance criteria:$",,Igor Babaev,Igor Babaev,Major,10,,4,18,5,1,0,2,0,,0,850,17,2,0,2017-10-24 14:28:59,Transform [NOT] IN predicate with long list of values INTO [NOT] IN subquery. ,"Currently if MariaDB checks whether possibility to use range access based on [NOT] IN predicate with a long list of values it build SEL_TREEs taking huge amount of memory.
Meanwhile using CTE specified by table value constructor such a predicate could be transformed into an equivalent [NOT] IN subquery:
{noformat}
SELECT ... WHERE ... (expr1, ...) [NOT] IN (value_list) ...;
=>
WITH t(col1, ...) AS (SELECT * FROM VALUES value_list) 
SELECT ... WHERE ... (expr1, ...) [NOT] IN (SELECT * FROM t)...;
{noformat}
",,0,0,0,0,0.0,"Transform [NOT] IN predicate with long list of values INTO [NOT] IN subquery.  $end$ Currently if MariaDB checks whether possibility to use range access based on [NOT] IN predicate with a long list of values it build SEL_TREEs taking huge amount of memory.
Meanwhile using CTE specified by table value constructor such a predicate could be transformed into an equivalent [NOT] IN subquery:
{noformat}
SELECT ... WHERE ... (expr1, ...) [NOT] IN (value_list) ...;
=>
WITH t(col1, ...) AS (SELECT * FROM VALUES value_list) 
SELECT ... WHERE ... (expr1, ...) [NOT] IN (SELECT * FROM t)...;
{noformat}
 $acceptance criteria:$",0,0,0,0,0,0,0,5635.82,2,1,0.5,1,0.5,0,0.0,0,0.0,0,0.0
626,MDEV-12179,Task,MDEV,2017-03-06 09:10:40,,0,Per-engine mysql.gtid_slave_pos tables,"Implement that server reads/updates the GTID position using multiple tables
mysql.gtid_slave_pos_XXX, each using a different storage engine. Replicated
transactions update the version of the table in the same engine, if
available. This avoids the overhead of cross-engine transactions on servers
where multiple storage engines are in use at the same time (but not in the
same transactions).

See mailing list thread: https://lists.launchpad.net/maria-developers/msg10453.html",,"Per-engine mysql.gtid_slave_pos tables $end$ Implement that server reads/updates the GTID position using multiple tables
mysql.gtid_slave_pos_XXX, each using a different storage engine. Replicated
transactions update the version of the table in the same engine, if
available. This avoids the overhead of cross-engine transactions on servers
where multiple storage engines are in use at the same time (but not in the
same transactions).

See mailing list thread: https://lists.launchpad.net/maria-developers/msg10453.html $acceptance criteria:$",,Kristian Nielsen,Kristian Nielsen,Critical,9,,0,10,2,1,0,0,0,,0,850,8,0,0,2017-05-24 13:28:03,Per-engine mysql.gtid_slave_pos tables,"Implement that server reads/updates the GTID position using multiple tables
mysql.gtid_slave_pos_XXX, each using a different storage engine. Replicated
transactions update the version of the table in the same engine, if
available. This avoids the overhead of cross-engine transactions on servers
where multiple storage engines are in use at the same time (but not in the
same transactions).

See mailing list thread: https://lists.launchpad.net/maria-developers/msg10453.html",,0,0,0,0,0.0,"Per-engine mysql.gtid_slave_pos tables $end$ Implement that server reads/updates the GTID position using multiple tables
mysql.gtid_slave_pos_XXX, each using a different storage engine. Replicated
transactions update the version of the table in the same engine, if
available. This avoids the overhead of cross-engine transactions on servers
where multiple storage engines are in use at the same time (but not in the
same transactions).

See mailing list thread: https://lists.launchpad.net/maria-developers/msg10453.html $acceptance criteria:$",0,0,0,0,0,0,0,1900.28,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
627,MDEV-12189,Task,MDEV,2017-03-07 05:48:19,,0,MariaRocks packaging: disable x86 builds,"Building MariaRocks causes the compiler to hang on some x86 hosts. The hang looks like this:

{noformat}
[ 52%] Building CXX object storage/rocksdb/CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o
cd /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb && /usr/bin/c++   
-DHAVE_CONFIG_H -DHAVE_SCHED_GETCPU=1 -DHAVE_SYSTEMD -DOS_LINUX -DROCKSDB_FALLOCATE_PRESENT -DROCKSDB_LIB_IO_POSIX -
DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PLATFORM_POSIX -DZLIB -
I/home/buildbot/buildbot/build/mariadb-10.2.4/include -I/home/buildbot/buildbot/build/mariadb-10.2.4/sql -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb -
I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/include -isystem /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/third-party/gtest-
1.7.0/fused-src -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util  -pie -fPIC -Wl,-z,relro,-z,now -fstack-protector --param=ssp-buffer-size=4 -
DWITH_INNODB_DISALLOW_WRITES -fno-rtti -O3 -g -static-libgcc -fno-omit-frame-pointer 
-fno-strict-aliasing -Wno-uninitialized -D_FORTIFY_SOURCE=2 -DDBUG_OFF   -std=c++11 -fPIC -fno-builtin-memcmp -frtti -o CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o -c 
/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util/statistics.cc
{noformat}
command timed out: 7200 seconds without output, attempting to kill
process killed by signal 9

Happens on Fedora 24 and 25, x86:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-x86/builds/619/steps/compile/logs/stdio
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-x86/builds/154/steps/compile/logs/stdio

Doesn't happen on Fedora 24 or 25 with amd64:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-amd64/builds/154
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-amd64/builds/560

I don't think anybody in MariaRocks' target audience is running x86, so one can just disable building MariaRocks on 32-bit hosts (optionally, when using gcc 6.x on 32-bit hosts). ",,"MariaRocks packaging: disable x86 builds $end$ Building MariaRocks causes the compiler to hang on some x86 hosts. The hang looks like this:

{noformat}
[ 52%] Building CXX object storage/rocksdb/CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o
cd /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb && /usr/bin/c++   
-DHAVE_CONFIG_H -DHAVE_SCHED_GETCPU=1 -DHAVE_SYSTEMD -DOS_LINUX -DROCKSDB_FALLOCATE_PRESENT -DROCKSDB_LIB_IO_POSIX -
DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PLATFORM_POSIX -DZLIB -
I/home/buildbot/buildbot/build/mariadb-10.2.4/include -I/home/buildbot/buildbot/build/mariadb-10.2.4/sql -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb -
I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/include -isystem /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/third-party/gtest-
1.7.0/fused-src -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util  -pie -fPIC -Wl,-z,relro,-z,now -fstack-protector --param=ssp-buffer-size=4 -
DWITH_INNODB_DISALLOW_WRITES -fno-rtti -O3 -g -static-libgcc -fno-omit-frame-pointer 
-fno-strict-aliasing -Wno-uninitialized -D_FORTIFY_SOURCE=2 -DDBUG_OFF   -std=c++11 -fPIC -fno-builtin-memcmp -frtti -o CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o -c 
/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util/statistics.cc
{noformat}
command timed out: 7200 seconds without output, attempting to kill
process killed by signal 9

Happens on Fedora 24 and 25, x86:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-x86/builds/619/steps/compile/logs/stdio
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-x86/builds/154/steps/compile/logs/stdio

Doesn't happen on Fedora 24 or 25 with amd64:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-amd64/builds/154
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-amd64/builds/560

I don't think anybody in MariaRocks' target audience is running x86, so one can just disable building MariaRocks on 32-bit hosts (optionally, when using gcc 6.x on 32-bit hosts).  $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,7,,0,3,1,1,0,1,0,,0,850,3,1,0,2017-03-08 11:37:53,MariaRocks packaging: disable x86 builds,"Building MariaRocks causes the compiler to hang on some x86 hosts. The hang looks like this:

{noformat}
[ 52%] Building CXX object storage/rocksdb/CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o
cd /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb && /usr/bin/c++   
-DHAVE_CONFIG_H -DHAVE_SCHED_GETCPU=1 -DHAVE_SYSTEMD -DOS_LINUX -DROCKSDB_FALLOCATE_PRESENT -DROCKSDB_LIB_IO_POSIX -
DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PLATFORM_POSIX -DZLIB -
I/home/buildbot/buildbot/build/mariadb-10.2.4/include -I/home/buildbot/buildbot/build/mariadb-10.2.4/sql -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb -
I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/include -isystem /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/third-party/gtest-
1.7.0/fused-src -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util  -pie -fPIC -Wl,-z,relro,-z,now -fstack-protector --param=ssp-buffer-size=4 -
DWITH_INNODB_DISALLOW_WRITES -fno-rtti -O3 -g -static-libgcc -fno-omit-frame-pointer 
-fno-strict-aliasing -Wno-uninitialized -D_FORTIFY_SOURCE=2 -DDBUG_OFF   -std=c++11 -fPIC -fno-builtin-memcmp -frtti -o CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o -c 
/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util/statistics.cc
{noformat}
command timed out: 7200 seconds without output, attempting to kill
process killed by signal 9

Happens on Fedora 24 and 25, x86:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-x86/builds/619/steps/compile/logs/stdio
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-x86/builds/154/steps/compile/logs/stdio

Doesn't happen on Fedora 24 or 25 with amd64:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-amd64/builds/154
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-amd64/builds/560

I don't think anybody in MariaRocks' target audience is running x86, so one can just disable building MariaRocks on 32-bit hosts (optionally, when using gcc 6.x on 32-bit hosts). ",,0,0,0,0,0.0,"MariaRocks packaging: disable x86 builds $end$ Building MariaRocks causes the compiler to hang on some x86 hosts. The hang looks like this:

{noformat}
[ 52%] Building CXX object storage/rocksdb/CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o
cd /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb && /usr/bin/c++   
-DHAVE_CONFIG_H -DHAVE_SCHED_GETCPU=1 -DHAVE_SYSTEMD -DOS_LINUX -DROCKSDB_FALLOCATE_PRESENT -DROCKSDB_LIB_IO_POSIX -
DROCKSDB_MALLOC_USABLE_SIZE -DROCKSDB_PLATFORM_POSIX -DZLIB -
I/home/buildbot/buildbot/build/mariadb-10.2.4/include -I/home/buildbot/buildbot/build/mariadb-10.2.4/sql -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb -
I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/include -isystem /home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/third-party/gtest-
1.7.0/fused-src -I/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util  -pie -fPIC -Wl,-z,relro,-z,now -fstack-protector --param=ssp-buffer-size=4 -
DWITH_INNODB_DISALLOW_WRITES -fno-rtti -O3 -g -static-libgcc -fno-omit-frame-pointer 
-fno-strict-aliasing -Wno-uninitialized -D_FORTIFY_SOURCE=2 -DDBUG_OFF   -std=c++11 -fPIC -fno-builtin-memcmp -frtti -o CMakeFiles/rocksdblib.dir/rocksdb/util/statistics.cc.o -c 
/home/buildbot/buildbot/build/mariadb-10.2.4/storage/rocksdb/rocksdb/util/statistics.cc
{noformat}
command timed out: 7200 seconds without output, attempting to kill
process killed by signal 9

Happens on Fedora 24 and 25, x86:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-x86/builds/619/steps/compile/logs/stdio
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-x86/builds/154/steps/compile/logs/stdio

Doesn't happen on Fedora 24 or 25 with amd64:
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora25-amd64/builds/154
http://buildbot.askmonty.org/buildbot/builders/kvm-rpm-fedora24-amd64/builds/560

I don't think anybody in MariaRocks' target audience is running x86, so one can just disable building MariaRocks on 32-bit hosts (optionally, when using gcc 6.x on 32-bit hosts).  $acceptance criteria:$",0,0,0,0,0,0,0,29.8167,9,1,0.111111,1,0.111111,1,0.111111,1,0.111111,1,0.111111
628,MDEV-12191,Task,MDEV,2017-03-07 06:39:12,,0,Server error messages in Hindi,,,Server error messages in Hindi $end$ $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-09 12:18:03,Server error messages in Hindi,,,0,0,0,0,0.0,Server error messages in Hindi $end$ $acceptance criteria:$,0,0,0,0,0,0,0,53.6333,19,2,0.105263,1,0.0526316,1,0.0526316,1,0.0526316,1,0.0526316
629,MDEV-12194,Task,MDEV,2017-03-07 06:49:52,,0,Remove connect warnings in storage connect,,,Remove connect warnings in storage connect $end$ $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-09 12:16:22,Remove connect warnings in storage connect,,,0,0,0,0,0.0,Remove connect warnings in storage connect $end$ $acceptance criteria:$,0,0,0,0,0,0,0,53.4333,20,2,0.1,1,0.05,1,0.05,1,0.05,1,0.05
630,MDEV-12195,Task,MDEV,2017-03-07 07:31:36,,0,10.2 whitespace warnings + printf attributes in sql/log,,,10.2 whitespace warnings + printf attributes in sql/log $end$ $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-09 12:17:32,10.2 whitespace warnings + printf attributes in sql/log,,,0,0,0,0,0.0,10.2 whitespace warnings + printf attributes in sql/log $end$ $acceptance criteria:$,0,0,0,0,0,0,0,52.75,21,2,0.0952381,1,0.047619,1,0.047619,1,0.047619,1,0.047619
631,MDEV-12199,Task,MDEV,2017-03-07 14:11:05,,0,Split Item_func_{abs|neg|int_val}::fix_length_and_dec() into methods in Type_handler,"The following methods:
{code:cpp}
Item_func_neg::fix_length_and_dec()
Item_func_abs::fix_length_and_dec()
Item_func_int_val::fix_length_and_dec()
{code}
have switches on {{arg[0]->cast_to_int_type()}}.
This is not friendly to pluggable data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_abs_fix_length_and_dec(Item_func_abs *) const;
virtual bool Item_func_int_val_fix_length_and_dec(Item_func_int_val *) const;
virtual bool Item_func_neg_fix_length_and_dec(Item_func_neg *) const;
{code}
",,"Split Item_func_{abs|neg|int_val}::fix_length_and_dec() into methods in Type_handler $end$ The following methods:
{code:cpp}
Item_func_neg::fix_length_and_dec()
Item_func_abs::fix_length_and_dec()
Item_func_int_val::fix_length_and_dec()
{code}
have switches on {{arg[0]->cast_to_int_type()}}.
This is not friendly to pluggable data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_abs_fix_length_and_dec(Item_func_abs *) const;
virtual bool Item_func_int_val_fix_length_and_dec(Item_func_int_val *) const;
virtual bool Item_func_neg_fix_length_and_dec(Item_func_neg *) const;
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,10,,0,2,1,1,0,0,0,,0,850,2,0,0,2017-03-08 12:11:15,Split Item_func_{abs|neg|int_val}::fix_length_and_dec() into methods in Type_handler,"The following methods:
{code:cpp}
Item_func_neg::fix_length_and_dec()
Item_func_abs::fix_length_and_dec()
Item_func_int_val::fix_length_and_dec()
{code}
have switches on {{arg[0]->cast_to_int_type()}}.
This is not friendly to pluggable data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_abs_fix_length_and_dec(Item_func_abs *) const;
virtual bool Item_func_int_val_fix_length_and_dec(Item_func_int_val *) const;
virtual bool Item_func_neg_fix_length_and_dec(Item_func_neg *) const;
{code}
",,0,0,0,0,0.0,"Split Item_func_{abs|neg|int_val}::fix_length_and_dec() into methods in Type_handler $end$ The following methods:
{code:cpp}
Item_func_neg::fix_length_and_dec()
Item_func_abs::fix_length_and_dec()
Item_func_int_val::fix_length_and_dec()
{code}
have switches on {{arg[0]->cast_to_int_type()}}.
This is not friendly to pluggable data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_abs_fix_length_and_dec(Item_func_abs *) const;
virtual bool Item_func_int_val_fix_length_and_dec(Item_func_int_val *) const;
virtual bool Item_func_neg_fix_length_and_dec(Item_func_neg *) const;
{code}
 $acceptance criteria:$",0,0,0,0,0,0,0,22.0,53,35,0.660377,27,0.509434,26,0.490566,25,0.471698,25,0.471698
632,MDEV-12201,Task,MDEV,2017-03-07 20:19:50,,0,innodb_flush_method are not available on Windows,"The purpose of this task is to make Windows behave the same as Unix with respect to different flush methods, i.e innodb_flush_method=O_DSYNC, O_DIRECT, O_DIRECT_NO_FSYNC etc.

It probably makes sense to retain  the current default behavior on Windows, where both redo log and data are used without buffering,and  are flushed.

Windows has exact corresponding options to unix-ly O_DIRECT (FILE_FLAG_NO_BUFFERING) and O_SYNC (FILE_FLAG_WRITE_THROUGH), so the separation of the flushing methods is artificial.",,"innodb_flush_method are not available on Windows $end$ The purpose of this task is to make Windows behave the same as Unix with respect to different flush methods, i.e innodb_flush_method=O_DSYNC, O_DIRECT, O_DIRECT_NO_FSYNC etc.

It probably makes sense to retain  the current default behavior on Windows, where both redo log and data are used without buffering,and  are flushed.

Windows has exact corresponding options to unix-ly O_DIRECT (FILE_FLAG_NO_BUFFERING) and O_SYNC (FILE_FLAG_WRITE_THROUGH), so the separation of the flushing methods is artificial. $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,4,,0,0,1,1,0,0,0,,0,850,0,0,0,2017-03-08 11:16:14,innodb_flush_method are not available on Windows,"The purpose of this task is to make Windows behave the same as Unix with respect to different flush methods, i.e innodb_flush_method=O_DSYNC, O_DIRECT, O_DIRECT_NO_FSYNC etc.

It probably makes sense to retain  the current default behavior on Windows, where both redo log and data are used without buffering,and  are flushed.

Windows has exact corresponding options to unix-ly O_DIRECT (FILE_FLAG_NO_BUFFERING) and O_SYNC (FILE_FLAG_WRITE_THROUGH), so the separation of the flushing methods is artificial.",,0,0,0,0,0.0,"innodb_flush_method are not available on Windows $end$ The purpose of this task is to make Windows behave the same as Unix with respect to different flush methods, i.e innodb_flush_method=O_DSYNC, O_DIRECT, O_DIRECT_NO_FSYNC etc.

It probably makes sense to retain  the current default behavior on Windows, where both redo log and data are used without buffering,and  are flushed.

Windows has exact corresponding options to unix-ly O_DIRECT (FILE_FLAG_NO_BUFFERING) and O_SYNC (FILE_FLAG_WRITE_THROUGH), so the separation of the flushing methods is artificial. $acceptance criteria:$",0,0,0,0,0,0,0,14.9333,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
633,MDEV-12209,Technical task,MDEV,2017-03-08 17:02:47,,0,sql_mode=ORACLE: Syntax error in a OPEN cursor with parameters makes the server crash,"The below script makes the server crash. Notice a syntax error in {{OPEN(a+,b);}}.

{code:sql}
SET sql_mode=oracle;
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
INSERT INTO t1 VALUES (1,'A');
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
DELIMITER $$
CREATE PROCEDURE p1(a INT,b VARCHAR)
AS
  CURSOR c (p_a INT, p_b VARCHAR) IS SELECT * FROM t1 WHERE a=p_a;
BEGIN
  OPEN c(a+, b);
  LOOP
    FETCH c INTO a, b;
    EXIT WHEN c%NOTFOUND;
    SELECT a, b;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1(1,'a');
DROP TABLE t1;
{code}
",,"sql_mode=ORACLE: Syntax error in a OPEN cursor with parameters makes the server crash $end$ The below script makes the server crash. Notice a syntax error in {{OPEN(a+,b);}}.

{code:sql}
SET sql_mode=oracle;
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
INSERT INTO t1 VALUES (1,'A');
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
DELIMITER $$
CREATE PROCEDURE p1(a INT,b VARCHAR)
AS
  CURSOR c (p_a INT, p_b VARCHAR) IS SELECT * FROM t1 WHERE a=p_a;
BEGIN
  OPEN c(a+, b);
  LOOP
    FETCH c INTO a, b;
    EXIT WHEN c%NOTFOUND;
    SELECT a, b;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1(1,'a');
DROP TABLE t1;
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,10,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-03-08 17:02:47,sql_mode=ORACLE: Syntax error in a OPEN cursor with parameters makes the server crash,"The below script makes the server crash. Notice a syntax error in {{OPEN(a+,b);}}.

{code:sql}
SET sql_mode=oracle;
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
INSERT INTO t1 VALUES (1,'A');
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
DELIMITER $$
CREATE PROCEDURE p1(a INT,b VARCHAR)
AS
  CURSOR c (p_a INT, p_b VARCHAR) IS SELECT * FROM t1 WHERE a=p_a;
BEGIN
  OPEN c(a+, b);
  LOOP
    FETCH c INTO a, b;
    EXIT WHEN c%NOTFOUND;
    SELECT a, b;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1(1,'a');
DROP TABLE t1;
{code}
",,0,0,0,0,0.0,"sql_mode=ORACLE: Syntax error in a OPEN cursor with parameters makes the server crash $end$ The below script makes the server crash. Notice a syntax error in {{OPEN(a+,b);}}.

{code:sql}
SET sql_mode=oracle;
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
INSERT INTO t1 VALUES (1,'A');
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(10));
DELIMITER $$
CREATE PROCEDURE p1(a INT,b VARCHAR)
AS
  CURSOR c (p_a INT, p_b VARCHAR) IS SELECT * FROM t1 WHERE a=p_a;
BEGIN
  OPEN c(a+, b);
  LOOP
    FETCH c INTO a, b;
    EXIT WHEN c%NOTFOUND;
    SELECT a, b;
  END LOOP;
  CLOSE c;
END;
$$
DELIMITER ;
CALL p1(1,'a');
DROP TABLE t1;
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,54,35,0.648148,27,0.5,26,0.481481,25,0.462963,25,0.462963
634,MDEV-12238,Task,MDEV,2017-03-13 05:05:44,,0,Add Type_handler::Item_func_{plus|minus|mul|div|mod}_fix_length_and_dec(),"The following methods:
{code:cpp}
Item_func_plus::fix_length_and_dec()
Item_func_minus::fix_length_and_dec()
Item_func_mul::fix_length_and_dec()
Item_func_div::fix_length_and_dec()
Item_func_mod::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
This is not friendly to pluggable data types, as only covers the built-in data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_plus_fix_length_and_dec(Item_func_plus *) const;
virtual bool Item_func_minus_fix_length_and_dec(Item_func_minus *) const;
virtual bool Item_func_mul_fix_length_and_dec(Item_func_mul *) const;
virtual bool Item_func_div_fix_length_and_dec(Item_func_div *) const;
virtual bool Item_func_mod_fix_length_and_dec(Item_func_mod *) const;
{code}

Pluggable data types should be able to define their own fix_length_and_dec() logic.

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
",,"Add Type_handler::Item_func_{plus|minus|mul|div|mod}_fix_length_and_dec() $end$ The following methods:
{code:cpp}
Item_func_plus::fix_length_and_dec()
Item_func_minus::fix_length_and_dec()
Item_func_mul::fix_length_and_dec()
Item_func_div::fix_length_and_dec()
Item_func_mod::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
This is not friendly to pluggable data types, as only covers the built-in data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_plus_fix_length_and_dec(Item_func_plus *) const;
virtual bool Item_func_minus_fix_length_and_dec(Item_func_minus *) const;
virtual bool Item_func_mul_fix_length_and_dec(Item_func_mul *) const;
virtual bool Item_func_div_fix_length_and_dec(Item_func_div *) const;
virtual bool Item_func_mod_fix_length_and_dec(Item_func_mod *) const;
{code}

Pluggable data types should be able to define their own fix_length_and_dec() logic.

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,18,,0,2,1,2,0,0,0,,0,850,2,0,0,2017-03-15 14:58:17,Add Type_handler::Item_func_{plus|minus|mul|div|mod}_fix_length_and_dec(),"The following methods:
{code:cpp}
Item_func_plus::fix_length_and_dec()
Item_func_minus::fix_length_and_dec()
Item_func_mul::fix_length_and_dec()
Item_func_div::fix_length_and_dec()
Item_func_mod::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
This is not friendly to pluggable data types, as only covers the built-in data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_plus_fix_length_and_dec(Item_func_plus *) const;
virtual bool Item_func_minus_fix_length_and_dec(Item_func_minus *) const;
virtual bool Item_func_mul_fix_length_and_dec(Item_func_mul *) const;
virtual bool Item_func_div_fix_length_and_dec(Item_func_div *) const;
virtual bool Item_func_mod_fix_length_and_dec(Item_func_mod *) const;
{code}

Pluggable data types should be able to define their own fix_length_and_dec() logic.

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
",,0,0,0,0,0.0,"Add Type_handler::Item_func_{plus|minus|mul|div|mod}_fix_length_and_dec() $end$ The following methods:
{code:cpp}
Item_func_plus::fix_length_and_dec()
Item_func_minus::fix_length_and_dec()
Item_func_mul::fix_length_and_dec()
Item_func_div::fix_length_and_dec()
Item_func_mod::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
This is not friendly to pluggable data types, as only covers the built-in data types.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_func_plus_fix_length_and_dec(Item_func_plus *) const;
virtual bool Item_func_minus_fix_length_and_dec(Item_func_minus *) const;
virtual bool Item_func_mul_fix_length_and_dec(Item_func_mul *) const;
virtual bool Item_func_div_fix_length_and_dec(Item_func_div *) const;
virtual bool Item_func_mod_fix_length_and_dec(Item_func_mod *) const;
{code}

Pluggable data types should be able to define their own fix_length_and_dec() logic.

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
 $acceptance criteria:$",0,0,0,0,0,0,1,57.8667,55,35,0.636364,27,0.490909,26,0.472727,25,0.454545,25,0.454545
635,MDEV-12239,Task,MDEV,2017-03-13 05:09:54,,0,Add Type_handler::Item_sum_{sum|avg|variance}_fix_length_and_dec(),"The following methods:
{code:cpp}
Item_sum_sum::fix_length_and_dec()
Item_sum_avg::fix_length_and_dec()
Item_sum_variance::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
The is not friendly to pluggable data types.
Pluggable data types should be able to define their own fix_length_and_dec() logic.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_sum_sum_fix_length_and_dec(Item_sum_sum *) const;
virtual bool Item_sum_avg_fix_length_and_dec(Item_sum_avg *) const;
virtual bool Item_sum_variance_fix_length_and_dec(Item_sum_variance *) const;
{code}

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
",,"Add Type_handler::Item_sum_{sum|avg|variance}_fix_length_and_dec() $end$ The following methods:
{code:cpp}
Item_sum_sum::fix_length_and_dec()
Item_sum_avg::fix_length_and_dec()
Item_sum_variance::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
The is not friendly to pluggable data types.
Pluggable data types should be able to define their own fix_length_and_dec() logic.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_sum_sum_fix_length_and_dec(Item_sum_sum *) const;
virtual bool Item_sum_avg_fix_length_and_dec(Item_sum_avg *) const;
virtual bool Item_sum_variance_fix_length_and_dec(Item_sum_variance *) const;
{code}

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,11,,0,1,1,1,0,1,0,,0,850,1,0,0,2017-03-15 14:58:46,Add Type_handler::Item_sum_{sum|avg|variance}_mod_length_and_dec(),"The following methods:
{code:cpp}
Item_sum_sum::fix_length_and_dec()
Item_sum_avg::fix_length_and_dec()
Item_sum_variance::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
The is not friendly to pluggable data types.
Pluggable data types should be able to define their own fix_length_and_dec() logic.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_sum_sum_fix_length_and_dec(Item_sum_sum *) const;
virtual bool Item_sum_avg_fix_length_and_dec(Item_sum_avg *) const;
virtual bool Item_sum_variance_fix_length_and_dec(Item_sum_variance *) const;
{code}

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
",,1,0,0,2,0.0105263,"Add Type_handler::Item_sum_{sum|avg|variance}_mod_length_and_dec() $end$ The following methods:
{code:cpp}
Item_sum_sum::fix_length_and_dec()
Item_sum_avg::fix_length_and_dec()
Item_sum_variance::fix_length_and_dec()
{code}
use methods {{cmp_type()}} and {{result_type()}} of their arguments.
The is not friendly to pluggable data types.
Pluggable data types should be able to define their own fix_length_and_dec() logic.

Under term of this task we'll split implementations of these methods into new methods in Type_handler:
{code:cpp}
virtual bool Item_sum_sum_fix_length_and_dec(Item_sum_sum *) const;
virtual bool Item_sum_avg_fix_length_and_dec(Item_sum_avg *) const;
virtual bool Item_sum_variance_fix_length_and_dec(Item_sum_variance *) const;
{code}

We'll also make the server return an error when a GOMETRY type expression appears as an argument of the affected operations.
 $acceptance criteria:$",1,1,0,0,0,0,0,57.8,56,35,0.625,27,0.482143,26,0.464286,25,0.446429,25,0.446429
636,MDEV-12254,Task,MDEV,2017-03-14 09:53:26,,0,MariaRocks: check whether it gets into packages,"Need to check whether MariaRocks makes it into packages, in either source or binary form.",,"MariaRocks: check whether it gets into packages $end$ Need to check whether MariaRocks makes it into packages, in either source or binary form. $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Critical,8,,0,14,1,2,0,0,0,,0,850,7,0,0,2017-03-15 08:45:36,MariaRocks: check whether it gets into packages,"Need to check whether MariaRocks makes it into packages, in either source or binary form.",,0,0,0,0,0.0,"MariaRocks: check whether it gets into packages $end$ Need to check whether MariaRocks makes it into packages, in either source or binary form. $acceptance criteria:$",0,0,0,0,0,0,1,22.8667,10,1,0.1,1,0.1,1,0.1,1,0.1,1,0.1
637,MDEV-12266,Task,MDEV,2017-03-15 07:55:12,,0,Reduce the number of InnoDB tablespace lookups,"InnoDB used to hide the definition of {{fil_space_t}} and use numeric tablespace ID everywhere. This causes a large number of tablespace lookups and thus contention on the {{fil_system->mutex}}.

We should store {{fil_space_t*}} where possible to reduce the number of tablespace lookups:
* {{fil_system->sys_space}} for the {{innodb_system}} tablespace
* {{fil_system->temp_space}} for the {{innodb_temporary}} tablespace
* {{dict_table_t::space}} for user tables
* optionally, {{rseg_t::space}} for rollback segment header pages and and undo logs

Also, we should use atomic memory access in {{fil_space_release()}} and avoid acquiring the mutex there. Furthermore, some accessor functions, such as {{fil_space_get_type()}} and {{fil_space_get_first_path()}} must be removed.",,"Reduce the number of InnoDB tablespace lookups $end$ InnoDB used to hide the definition of {{fil_space_t}} and use numeric tablespace ID everywhere. This causes a large number of tablespace lookups and thus contention on the {{fil_system->mutex}}.

We should store {{fil_space_t*}} where possible to reduce the number of tablespace lookups:
* {{fil_system->sys_space}} for the {{innodb_system}} tablespace
* {{fil_system->temp_space}} for the {{innodb_temporary}} tablespace
* {{dict_table_t::space}} for user tables
* optionally, {{rseg_t::space}} for rollback segment header pages and and undo logs

Also, we should use atomic memory access in {{fil_space_release()}} and avoid acquiring the mutex there. Furthermore, some accessor functions, such as {{fil_space_get_type()}} and {{fil_space_get_first_path()}} must be removed. $acceptance criteria:$",,Rasmus Johansson,Rasmus Johansson,Major,20,,3,2,4,1,0,3,0,,0,850,2,1,0,2017-04-27 10:37:05,"port Bug #85304 - ""Reduce mutex contention of fil_system->mutex""",Port the contributed patch that was sent to [MySQL Bug #85304|https://bugs.mysql.com/bug.php?id=85304].,,1,1,0,118,4.43478,"port Bug #85304 - ""Reduce mutex contention of fil_system->mutex"" $end$ Port the contributed patch that was sent to [MySQL Bug #85304|https://bugs.mysql.com/bug.php?id=85304]. $acceptance criteria:$",2,1,1,1,1,1,1,1034.68,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
638,MDEV-12279,Task,MDEV,2017-03-16 09:12:49,,0,"rocksdb.tbl_opt_data_index_dir fails, wrong error code","Running 

{noformat}
./mtr  rocksdb.tbl_opt_data_index_dir
{noformat}

produces this error:

{noformat}
rocksdb.tbl_opt_data_index_dir           [ fail ]
        Test ended at 2017-03-16 11:11:07

CURRENT_TEST: rocksdb.tbl_opt_data_index_dir
mysqltest: At line 14: query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno 1005: 'Can't create table `test`.`t1` (errno: 198 ""Unknown error 198"")', instead of 1296...

The result from queries just before the failure was:
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';

 - saving '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/' to '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/'
--------------------------------------------------------------------------
The servers were restarted 0 times
Spent 0.000 of 4 seconds executing testcases

Failure: Failed 1/1 tests, 0.00% were successful.

Failing test(s): rocksdb.tbl_opt_data_index_dir
{noformat}",,"rocksdb.tbl_opt_data_index_dir fails, wrong error code $end$ Running 

{noformat}
./mtr  rocksdb.tbl_opt_data_index_dir
{noformat}

produces this error:

{noformat}
rocksdb.tbl_opt_data_index_dir           [ fail ]
        Test ended at 2017-03-16 11:11:07

CURRENT_TEST: rocksdb.tbl_opt_data_index_dir
mysqltest: At line 14: query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno 1005: 'Can't create table `test`.`t1` (errno: 198 ""Unknown error 198"")', instead of 1296...

The result from queries just before the failure was:
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';

 - saving '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/' to '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/'
--------------------------------------------------------------------------
The servers were restarted 0 times
Spent 0.000 of 4 seconds executing testcases

Failure: Failed 1/1 tests, 0.00% were successful.

Failing test(s): rocksdb.tbl_opt_data_index_dir
{noformat} $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,5,,0,5,1,1,0,0,0,,0,850,4,0,0,2017-03-22 10:10:08,"rocksdb.tbl_opt_data_index_dir fails, wrong error code","Running 

{noformat}
./mtr  rocksdb.tbl_opt_data_index_dir
{noformat}

produces this error:

{noformat}
rocksdb.tbl_opt_data_index_dir           [ fail ]
        Test ended at 2017-03-16 11:11:07

CURRENT_TEST: rocksdb.tbl_opt_data_index_dir
mysqltest: At line 14: query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno 1005: 'Can't create table `test`.`t1` (errno: 198 ""Unknown error 198"")', instead of 1296...

The result from queries just before the failure was:
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';

 - saving '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/' to '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/'
--------------------------------------------------------------------------
The servers were restarted 0 times
Spent 0.000 of 4 seconds executing testcases

Failure: Failed 1/1 tests, 0.00% were successful.

Failing test(s): rocksdb.tbl_opt_data_index_dir
{noformat}",,0,0,0,0,0.0,"rocksdb.tbl_opt_data_index_dir fails, wrong error code $end$ Running 

{noformat}
./mtr  rocksdb.tbl_opt_data_index_dir
{noformat}

produces this error:

{noformat}
rocksdb.tbl_opt_data_index_dir           [ fail ]
        Test ended at 2017-03-16 11:11:07

CURRENT_TEST: rocksdb.tbl_opt_data_index_dir
mysqltest: At line 14: query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno 1005: 'Can't create table `test`.`t1` (errno: 198 ""Unknown error 198"")', instead of 1296...

The result from queries just before the failure was:
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';

 - saving '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/' to '/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/log/rocksdb.tbl_opt_data_index_dir/'
--------------------------------------------------------------------------
The servers were restarted 0 times
Spent 0.000 of 4 seconds executing testcases

Failure: Failed 1/1 tests, 0.00% were successful.

Failing test(s): rocksdb.tbl_opt_data_index_dir
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,144.95,11,1,0.0909091,1,0.0909091,1,0.0909091,1,0.0909091,1,0.0909091
639,MDEV-12288,Task,MDEV,2017-03-17 07:00:37,,0,"Reset DB_TRX_ID when the history is removed, to speed up MVCC","The InnoDB clustered index record system columns DB_TRX_ID,DB_ROLL_PTR are used by multi-versioning and for determining if a record is implicitly locked. After the history is no longer needed, these columns can safely be reset to 0 and 1<<55 (to indicate a fresh insert).

When a reader sees 0 in the DB_TRX_ID column, it can instantly determine that the record is present the read view. There is no need to acquire the transaction system mutex to check if the transaction exists, because writes can never be conducted by a transaction whose ID is 0.

The persistent InnoDB undo log is split into two parts: insert_undo and update_undo. The insert_undo log is discarded at transaction commit or rollback, and the update_undo log is processed by the purge subsystem. As part of this change, we must merge the two types of undo logs into one, and the purge subsystem will reset the DB_TRX_ID whenever a clustered index record is ‘touched’.

h2. Upgrade considerations
This will change the persistent InnoDB file formats, not only in the undo log and redo log, but also in the data files. There are some debug assertions that would not allow any record to contain DB_TRX_ID=0.

A new redo log format tag must be introduced so that the writes of the system columns can be properly redo-logged. (See MDEV-11432, MDEV-11782.) This will prevent a startup of an older version with the new-version redo logs. We may also prevent a crash recovery of MariaDB 10.2 files with the newer version. (Crash recovery of files from 10.1 or earlier versions is already prevented in 10.2.)

The undo log format will be changed as well. To be able to get rid of legacy code, InnoDB startup should detect if any old-format undo logs are present. If yes, startup will be refused, and the user must perform a slow shutdown (SET GLOBAL innodb_fast_shutdown=0) with the old server in order to empty the undo logs.

[A proof-of-concept implementation for 10.2|https://github.com/MariaDB/server/commit/7d8714d70bbc9f7bc6aff641e8007413fa0e01cd] consists of 4 consecutive commits. It is missing any of the above-mentioned upgrade logic.",,"Reset DB_TRX_ID when the history is removed, to speed up MVCC $end$ The InnoDB clustered index record system columns DB_TRX_ID,DB_ROLL_PTR are used by multi-versioning and for determining if a record is implicitly locked. After the history is no longer needed, these columns can safely be reset to 0 and 1<<55 (to indicate a fresh insert).

When a reader sees 0 in the DB_TRX_ID column, it can instantly determine that the record is present the read view. There is no need to acquire the transaction system mutex to check if the transaction exists, because writes can never be conducted by a transaction whose ID is 0.

The persistent InnoDB undo log is split into two parts: insert_undo and update_undo. The insert_undo log is discarded at transaction commit or rollback, and the update_undo log is processed by the purge subsystem. As part of this change, we must merge the two types of undo logs into one, and the purge subsystem will reset the DB_TRX_ID whenever a clustered index record is ‘touched’.

h2. Upgrade considerations
This will change the persistent InnoDB file formats, not only in the undo log and redo log, but also in the data files. There are some debug assertions that would not allow any record to contain DB_TRX_ID=0.

A new redo log format tag must be introduced so that the writes of the system columns can be properly redo-logged. (See MDEV-11432, MDEV-11782.) This will prevent a startup of an older version with the new-version redo logs. We may also prevent a crash recovery of MariaDB 10.2 files with the newer version. (Crash recovery of files from 10.1 or earlier versions is already prevented in 10.2.)

The undo log format will be changed as well. To be able to get rid of legacy code, InnoDB startup should detect if any old-format undo logs are present. If yes, startup will be refused, and the user must perform a slow shutdown (SET GLOBAL innodb_fast_shutdown=0) with the old server in order to empty the undo logs.

[A proof-of-concept implementation for 10.2|https://github.com/MariaDB/server/commit/7d8714d70bbc9f7bc6aff641e8007413fa0e01cd] consists of 4 consecutive commits. It is missing any of the above-mentioned upgrade logic. $acceptance criteria:$",,Marko Mäkelä,Marko Mäkelä,Major,11,,25,10,31,1,0,0,0,,0,850,5,0,0,2017-07-06 20:22:12,"Reset DB_TRX_ID when the history is removed, to speed up MVCC","The InnoDB clustered index record system columns DB_TRX_ID,DB_ROLL_PTR are used by multi-versioning and for determining if a record is implicitly locked. After the history is no longer needed, these columns can safely be reset to 0 and 1<<55 (to indicate a fresh insert).

When a reader sees 0 in the DB_TRX_ID column, it can instantly determine that the record is present the read view. There is no need to acquire the transaction system mutex to check if the transaction exists, because writes can never be conducted by a transaction whose ID is 0.

The persistent InnoDB undo log is split into two parts: insert_undo and update_undo. The insert_undo log is discarded at transaction commit or rollback, and the update_undo log is processed by the purge subsystem. As part of this change, we must merge the two types of undo logs into one, and the purge subsystem will reset the DB_TRX_ID whenever a clustered index record is ‘touched’.

h2. Upgrade considerations
This will change the persistent InnoDB file formats, not only in the undo log and redo log, but also in the data files. There are some debug assertions that would not allow any record to contain DB_TRX_ID=0.

A new redo log format tag must be introduced so that the writes of the system columns can be properly redo-logged. (See MDEV-11432, MDEV-11782.) This will prevent a startup of an older version with the new-version redo logs. We may also prevent a crash recovery of MariaDB 10.2 files with the newer version. (Crash recovery of files from 10.1 or earlier versions is already prevented in 10.2.)

The undo log format will be changed as well. To be able to get rid of legacy code, InnoDB startup should detect if any old-format undo logs are present. If yes, startup will be refused, and the user must perform a slow shutdown (SET GLOBAL innodb_fast_shutdown=0) with the old server in order to empty the undo logs.

[A proof-of-concept implementation for 10.2|https://github.com/MariaDB/server/commit/7d8714d70bbc9f7bc6aff641e8007413fa0e01cd] consists of 4 consecutive commits. It is missing any of the above-mentioned upgrade logic.",,0,0,0,0,0.0,"Reset DB_TRX_ID when the history is removed, to speed up MVCC $end$ The InnoDB clustered index record system columns DB_TRX_ID,DB_ROLL_PTR are used by multi-versioning and for determining if a record is implicitly locked. After the history is no longer needed, these columns can safely be reset to 0 and 1<<55 (to indicate a fresh insert).

When a reader sees 0 in the DB_TRX_ID column, it can instantly determine that the record is present the read view. There is no need to acquire the transaction system mutex to check if the transaction exists, because writes can never be conducted by a transaction whose ID is 0.

The persistent InnoDB undo log is split into two parts: insert_undo and update_undo. The insert_undo log is discarded at transaction commit or rollback, and the update_undo log is processed by the purge subsystem. As part of this change, we must merge the two types of undo logs into one, and the purge subsystem will reset the DB_TRX_ID whenever a clustered index record is ‘touched’.

h2. Upgrade considerations
This will change the persistent InnoDB file formats, not only in the undo log and redo log, but also in the data files. There are some debug assertions that would not allow any record to contain DB_TRX_ID=0.

A new redo log format tag must be introduced so that the writes of the system columns can be properly redo-logged. (See MDEV-11432, MDEV-11782.) This will prevent a startup of an older version with the new-version redo logs. We may also prevent a crash recovery of MariaDB 10.2 files with the newer version. (Crash recovery of files from 10.1 or earlier versions is already prevented in 10.2.)

The undo log format will be changed as well. To be able to get rid of legacy code, InnoDB startup should detect if any old-format undo logs are present. If yes, startup will be refused, and the user must perform a slow shutdown (SET GLOBAL innodb_fast_shutdown=0) with the old server in order to empty the undo logs.

[A proof-of-concept implementation for 10.2|https://github.com/MariaDB/server/commit/7d8714d70bbc9f7bc6aff641e8007413fa0e01cd] consists of 4 consecutive commits. It is missing any of the above-mentioned upgrade logic. $acceptance criteria:$",0,0,0,0,0,0,0,2677.35,3,3,1.0,2,0.666667,2,0.666667,1,0.333333,1,0.333333
640,MDEV-12291,Technical task,MDEV,2017-03-17 13:38:00,,0,Allow ROW variables as SELECT INTO targets,"Under terms of this task we'll add support for {{ROW}} type variables as {{SELECT..INTO}} targets.

Example:
{code:sql}
SET sql_mode=DEFAULT;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE rec1 ROW(a INT, b VARCHAR(32));
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


Example for {{sql_mode=ORACLE}}:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 ROW(a INT, b VARCHAR(32));
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{table%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{cursor%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR cur1 IS SELECT * FROM t1;
  rec1 cur1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


An attempt to use multiple {{ROW}} variables in the {{SELECT..INTO}} list will report an error.

An attempt to use {{ROW}} variables with a different column count than in the {{SELECT..INTO}} list will report an error.
",,"Allow ROW variables as SELECT INTO targets $end$ Under terms of this task we'll add support for {{ROW}} type variables as {{SELECT..INTO}} targets.

Example:
{code:sql}
SET sql_mode=DEFAULT;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE rec1 ROW(a INT, b VARCHAR(32));
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


Example for {{sql_mode=ORACLE}}:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 ROW(a INT, b VARCHAR(32));
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{table%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{cursor%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR cur1 IS SELECT * FROM t1;
  rec1 cur1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


An attempt to use multiple {{ROW}} variables in the {{SELECT..INTO}} list will report an error.

An attempt to use {{ROW}} variables with a different column count than in the {{SELECT..INTO}} list will report an error.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,15,,0,1,3,5,0,2,0,,0,850,1,0,0,2017-03-17 13:38:00,Allow ROW variables as SELECT INTO targets,"Under terms of this task we'll add support for {{ROW}} type variables as {{SELECT..INTO}} targets.

Example:
{code:sql}
SET sql_mode=DEFAULT;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE rec1 ROW(a INT, b VARCHAR(32));
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


Example for {{sql_mode=ORACLE}}:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 ROW(a INT, b VARCHAR(32));
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{table%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{cursor%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR cur1 IS SELECT * FROM t1;
  rec1 cur1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


An attempt to use multiple {{ROW}} variables in the {{SELECT..INTO}} list will report an error.

An attempt to use {{ROW}} variables with a different column count than in the {{SELECT..INTO}} list will report an error.
",,2,0,0,0,0.0,"Allow ROW variables as SELECT INTO targets $end$ Under terms of this task we'll add support for {{ROW}} type variables as {{SELECT..INTO}} targets.

Example:
{code:sql}
SET sql_mode=DEFAULT;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1()
BEGIN
  DECLARE rec1 ROW(a INT, b VARCHAR(32));
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


Example for {{sql_mode=ORACLE}}:
{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 ROW(a INT, b VARCHAR(32));
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{table%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


{{sql_mode=ORACLE}} will additionally support {{cursor%ROWTYPE}} variables as {{SELECT..INTO}} targets:

{code:sql}
SET sql_mode=ORACLE;
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b10');
DELIMITER $$
CREATE PROCEDURE p1 AS
  CURSOR cur1 IS SELECT * FROM t1;
  rec1 cur1%ROWTYPE;
BEGIN
  SELECT * FROM t1 INTO rec1;
  SELECT rec1.a, rec1.b;
END;
$$
DELIMITER ;
CALL p1();
{code}
{noformat}
+--------+--------+
| rec1.a | rec1.b |
+--------+--------+
|     10 | b10    |
+--------+--------+
{noformat}


An attempt to use multiple {{ROW}} variables in the {{SELECT..INTO}} list will report an error.

An attempt to use {{ROW}} variables with a different column count than in the {{SELECT..INTO}} list will report an error.
 $acceptance criteria:$",2,0,0,0,0,0,1,0.0,57,36,0.631579,27,0.473684,26,0.45614,25,0.438596,25,0.438596
641,MDEV-12314,Technical task,MDEV,2017-03-21 07:18:55,,0,sql_mode=ORACLE: Implicit cursor FOR LOOP for cursors with parameters,"This is similar to MDEV-12098, but for cursors with parameters.

The below script demonstrates using of a {{FOR}} loop with a cursor {{cur}} that has two parameters.

{code:sql}
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
SET sql_mode=ORACLE;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b0');
INSERT INTO t1 VALUES (11,'b1');
INSERT INTO t1 VALUES (12,'b2');
DELIMITER $$
CREATE PROCEDURE p1(pa INT, pb VARCHAR(32)) AS 
  CURSOR cur(va INT, vb VARCHAR(32)) IS
    SELECT a, b FROM t1 WHERE a=va AND b=vb;
BEGIN
  FOR rec IN cur(pa,pb)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
$$
DELIMITER ;
CALL p1(10,'B0');
CALL p1(11,'B1');
CALL p1(12,'B2');
CALL p1(12,'non-existing');
DROP TABLE t1;
DROP PROCEDURE p1;
{code}


The expected output is:
{noformat}
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    10 | b0    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    11 | b1    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    12 | b2    |
+-------+-------+
{noformat}
",,"sql_mode=ORACLE: Implicit cursor FOR LOOP for cursors with parameters $end$ This is similar to MDEV-12098, but for cursors with parameters.

The below script demonstrates using of a {{FOR}} loop with a cursor {{cur}} that has two parameters.

{code:sql}
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
SET sql_mode=ORACLE;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b0');
INSERT INTO t1 VALUES (11,'b1');
INSERT INTO t1 VALUES (12,'b2');
DELIMITER $$
CREATE PROCEDURE p1(pa INT, pb VARCHAR(32)) AS 
  CURSOR cur(va INT, vb VARCHAR(32)) IS
    SELECT a, b FROM t1 WHERE a=va AND b=vb;
BEGIN
  FOR rec IN cur(pa,pb)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
$$
DELIMITER ;
CALL p1(10,'B0');
CALL p1(11,'B1');
CALL p1(12,'B2');
CALL p1(12,'non-existing');
DROP TABLE t1;
DROP PROCEDURE p1;
{code}


The expected output is:
{noformat}
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    10 | b0    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    11 | b1    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    12 | b2    |
+-------+-------+
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,0,1,2,5,0,1,0,,0,850,1,0,0,2017-03-21 07:18:55,Implicit cursor FOR LOOP for cursors with parameters,"This is similar to MDEV-12098, but for cursors with parameters.

The below script demonstrates using of a {{FOR}} loop with a cursor {{cur}} that has two parameters.

{code:sql}
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
SET sql_mode=ORACLE;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b0');
INSERT INTO t1 VALUES (11,'b1');
INSERT INTO t1 VALUES (12,'b2');
DELIMITER $$
CREATE PROCEDURE p1(pa INT, pb VARCHAR(32)) AS 
  CURSOR cur(va INT, vb VARCHAR(32)) IS
    SELECT a, b FROM t1 WHERE a=va AND b=vb;
BEGIN
  FOR rec IN cur(pa,pb)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
$$
DELIMITER ;
CALL p1(10,'B0');
CALL p1(11,'B1');
CALL p1(12,'B2');
CALL p1(12,'non-existing');
DROP TABLE t1;
DROP PROCEDURE p1;
{code}


The expected output is:
{noformat}
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    10 | b0    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    11 | b1    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    12 | b2    |
+-------+-------+
{noformat}
",,1,0,0,1,0.00581395,"Implicit cursor FOR LOOP for cursors with parameters $end$ This is similar to MDEV-12098, but for cursors with parameters.

The below script demonstrates using of a {{FOR}} loop with a cursor {{cur}} that has two parameters.

{code:sql}
DROP TABLE IF EXISTS t1;
DROP PROCEDURE IF EXISTS p1;
SET sql_mode=ORACLE;
CREATE TABLE t1 (a INT, b VARCHAR(32));
INSERT INTO t1 VALUES (10,'b0');
INSERT INTO t1 VALUES (11,'b1');
INSERT INTO t1 VALUES (12,'b2');
DELIMITER $$
CREATE PROCEDURE p1(pa INT, pb VARCHAR(32)) AS 
  CURSOR cur(va INT, vb VARCHAR(32)) IS
    SELECT a, b FROM t1 WHERE a=va AND b=vb;
BEGIN
  FOR rec IN cur(pa,pb)
  LOOP
    SELECT rec.a, rec.b;
  END LOOP;
END;
$$
DELIMITER ;
CALL p1(10,'B0');
CALL p1(11,'B1');
CALL p1(12,'B2');
CALL p1(12,'non-existing');
DROP TABLE t1;
DROP PROCEDURE p1;
{code}


The expected output is:
{noformat}
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    10 | b0    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    11 | b1    |
+-------+-------+
+-------+-------+
| rec.a | rec.b |
+-------+-------+
|    12 | b2    |
+-------+-------+
{noformat}
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,58,36,0.62069,27,0.465517,26,0.448276,25,0.431034,25,0.431034
642,MDEV-12328,Task,MDEV,2017-03-22 09:19:11,,0,Add building of AWS KMS,,,Add building of AWS KMS $end$ $acceptance criteria:$,,Rasmus Johansson,Rasmus Johansson,Major,9,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-03-22 09:19:48,Add building of AWS KMS,,,0,0,0,0,0.0,Add building of AWS KMS $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
643,MDEV-12386,Task,MDEV,2017-03-28 22:14:11,,0,Windows CI builds integrated with Github,"To stop some developers submitting obviously broken MariaDB code that doesn't build on Windows, using appveyor as a free Windows build service could be used.

A simple file like and the appropriately configured github can make this happen for pull requests and mainstream pushed code.

https://github.com/facebook/rocksdb/blob/master/appveyor.yml",,"Windows CI builds integrated with Github $end$ To stop some developers submitting obviously broken MariaDB code that doesn't build on Windows, using appveyor as a free Windows build service could be used.

A simple file like and the appropriately configured github can make this happen for pull requests and mainstream pushed code.

https://github.com/facebook/rocksdb/blob/master/appveyor.yml $acceptance criteria:$",,Daniel Black,Daniel Black,Critical,9,,0,0,1,1,0,0,0,,0,850,0,0,0,2017-12-20 16:52:30,Windows CI builds integrated with Github,"To stop some developers submitting obviously broken MariaDB code that doesn't build on Windows, using appveyor as a free Windows build service could be used.

A simple file like and the appropriately configured github can make this happen for pull requests and mainstream pushed code.

https://github.com/facebook/rocksdb/blob/master/appveyor.yml",,0,0,0,0,0.0,"Windows CI builds integrated with Github $end$ To stop some developers submitting obviously broken MariaDB code that doesn't build on Windows, using appveyor as a free Windows build service could be used.

A simple file like and the appropriately configured github can make this happen for pull requests and mainstream pushed code.

https://github.com/facebook/rocksdb/blob/master/appveyor.yml $acceptance criteria:$",0,0,0,0,0,0,0,6402.63,6,1,0.166667,0,0.0,0,0.0,0,0.0,0,0.0
644,MDEV-12387,Task,MDEV,2017-03-28 22:46:52,,0,Push conditions into materialized IN subqueries,"The subqueries with GROUP BY are always materialized in MariaDB/MySQL.
If such a subquery is an IN subquery
{noformat}
 (expr[1],...) IN (SELECT col[1], ...FROM ... GROUP BY...)
{noformat}
and is a conjunct of the WHERE condition of the main query then for every row in the result set the following is true:
{noformat}
expr[1]=col[1] AND ...
{noformat}
Let P be a comparison predicate over expr[i]. Then the condition P(expr[i]/col[i]) can be pushed into the subquery.  

This task basically has to repeat what was done for pushing conditions into materialized views/derived tables (see MDEV-9197).

*EDIT*
{{optimizer_switch}} flag name is  {{condition_pushdown_for_subquery}}",,"Push conditions into materialized IN subqueries $end$ The subqueries with GROUP BY are always materialized in MariaDB/MySQL.
If such a subquery is an IN subquery
{noformat}
 (expr[1],...) IN (SELECT col[1], ...FROM ... GROUP BY...)
{noformat}
and is a conjunct of the WHERE condition of the main query then for every row in the result set the following is true:
{noformat}
expr[1]=col[1] AND ...
{noformat}
Let P be a comparison predicate over expr[i]. Then the condition P(expr[i]/col[i]) can be pushed into the subquery.  

This task basically has to repeat what was done for pushing conditions into materialized views/derived tables (see MDEV-9197).

*EDIT*
{{optimizer_switch}} flag name is  {{condition_pushdown_for_subquery}} $acceptance criteria:$",,Igor Babaev,Igor Babaev,Major,21,,0,4,1,5,0,6,0,,0,850,2,4,0,2018-01-17 16:38:32,Push conditions into materialized subqueries,"The subqueries with GROUP BY are always materialized in MariaDB/MySQL.
If such a subquery is an IN subquery
{noformat}
 (expr[1],...) IN (SELECT col[1], ...FROM ... GROUP BY...)
{noformat}
and is a conjunct of the WHERE condition of the main query then for every row in the result set the following is true:
{noformat}
expr[1]=col[1] AND ...
{noformat}
Let P be a comparison predicate over expr[i]. Then the condition P(expr[i]/col[i]) can be pushed into the subquery.  

This task basically has to repeat what was done for pushing conditions into materialized views/derived tables (see MDEV-9197).",,1,1,0,7,0.0693069,"Push conditions into materialized subqueries $end$ The subqueries with GROUP BY are always materialized in MariaDB/MySQL.
If such a subquery is an IN subquery
{noformat}
 (expr[1],...) IN (SELECT col[1], ...FROM ... GROUP BY...)
{noformat}
and is a conjunct of the WHERE condition of the main query then for every row in the result set the following is true:
{noformat}
expr[1]=col[1] AND ...
{noformat}
Let P be a comparison predicate over expr[i]. Then the condition P(expr[i]/col[i]) can be pushed into the subquery.  

This task basically has to repeat what was done for pushing conditions into materialized views/derived tables (see MDEV-9197). $acceptance criteria:$",2,1,1,0,0,0,1,7073.85,3,1,0.333333,1,0.333333,0,0.0,0,0.0,0,0.0
645,MDEV-12392,Technical task,MDEV,2017-03-29 13:11:55,,0,Duplicate code cleanup: add function normalize_db_name(),"There is a duplicate code in {{sql_db.cc}}, in functions {{mysql_create_db_internal()}} and {{mysql_rm_db_internal()}}.

{code:cpp}
char db_tmp[SAFE_NAME_LEN], *dbnorm;
if (lower_case_table_names)
{
  strmake_buf(db_tmp, db);
  my_casedn_str(system_charset_info, db_tmp);
  dbnorm= db_tmp;
}
else
  dbnorm= db;
{code}

Will introduce a new function {{normalize_db_name()}} and reuse it.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.",,"Duplicate code cleanup: add function normalize_db_name() $end$ There is a duplicate code in {{sql_db.cc}}, in functions {{mysql_create_db_internal()}} and {{mysql_rm_db_internal()}}.

{code:cpp}
char db_tmp[SAFE_NAME_LEN], *dbnorm;
if (lower_case_table_names)
{
  strmake_buf(db_tmp, db);
  my_casedn_str(system_charset_info, db_tmp);
  dbnorm= db_tmp;
}
else
  dbnorm= db;
{code}

Will introduce a new function {{normalize_db_name()}} and reuse it.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change. $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,8,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-03-29 13:11:55,Duplicate code cleanup: add function normalize_db_name(),"There is a duplicate code in {{sql_db.cc}}, in functions {{mysql_create_db_internal()}} and {{mysql_rm_db_internal()}}.

{code:cpp}
char db_tmp[SAFE_NAME_LEN], *dbnorm;
if (lower_case_table_names)
{
  strmake_buf(db_tmp, db);
  my_casedn_str(system_charset_info, db_tmp);
  dbnorm= db_tmp;
}
else
  dbnorm= db;
{code}

Will introduce a new function {{normalize_db_name()}} and reuse it.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.",,0,0,0,0,0.0,"Duplicate code cleanup: add function normalize_db_name() $end$ There is a duplicate code in {{sql_db.cc}}, in functions {{mysql_create_db_internal()}} and {{mysql_rm_db_internal()}}.

{code:cpp}
char db_tmp[SAFE_NAME_LEN], *dbnorm;
if (lower_case_table_names)
{
  strmake_buf(db_tmp, db);
  my_casedn_str(system_charset_info, db_tmp);
  dbnorm= db_tmp;
}
else
  dbnorm= db;
{code}

Will introduce a new function {{normalize_db_name()}} and reuse it.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,59,37,0.627119,27,0.457627,26,0.440678,25,0.423729,25,0.423729
646,MDEV-12393,Technical task,MDEV,2017-03-29 13:12:42,,0,Add function mysql_create_routine(),"There is a huge piece of code in {{mysql_execute_command()}}:

{code:cpp}
case SQLCOM_CREATE_PROCEDURE:
case SQLCOM_CREATE_SPFUNCTION:
{
  // 120 lines of code
}
{code}

This code will be needed for MDEV-10591, to create package routines.

We'll move this code from {{mysql_execute_command()}} into a separate new function {{mysql_create_routine()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
",,"Add function mysql_create_routine() $end$ There is a huge piece of code in {{mysql_execute_command()}}:

{code:cpp}
case SQLCOM_CREATE_PROCEDURE:
case SQLCOM_CREATE_SPFUNCTION:
{
  // 120 lines of code
}
{code}

This code will be needed for MDEV-10591, to create package routines.

We'll move this code from {{mysql_execute_command()}} into a separate new function {{mysql_create_routine()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-03-29 13:12:42,Add function mysql_create_routine(),"There is a huge piece of code in {{mysql_execute_command()}}:

{code:cpp}
case SQLCOM_CREATE_PROCEDURE:
case SQLCOM_CREATE_SPFUNCTION:
{
  // 120 lines of code
}
{code}

This code will be needed for MDEV-10591, to create package routines.

We'll move this code from {{mysql_execute_command()}} into a separate new function {{mysql_create_routine()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
",,0,0,0,0,0.0,"Add function mysql_create_routine() $end$ There is a huge piece of code in {{mysql_execute_command()}}:

{code:cpp}
case SQLCOM_CREATE_PROCEDURE:
case SQLCOM_CREATE_SPFUNCTION:
{
  // 120 lines of code
}
{code}

This code will be needed for MDEV-10591, to create package routines.

We'll move this code from {{mysql_execute_command()}} into a separate new function {{mysql_create_routine()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,60,37,0.616667,27,0.45,26,0.433333,25,0.416667,25,0.416667
647,MDEV-12394,Technical task,MDEV,2017-03-29 13:13:24,,0,Add function is_native_function_with_warn(),"There is a huge piece of code with huge comments in {{sql_yacc.yy}}:

{code:sql}
if (is_native_function(thd, & sp->m_name))
{
  /*
   Huge comment
  */
  ...
}
{code}

responsible to display a warning when a user shadows a built-in function with a stored function.

The same warning will be needed for package routines (MDEV-10591).

We'll move this piece of code to {{sql_lex.cc}} into a new function {{is_native_function_with_warn()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
",,"Add function is_native_function_with_warn() $end$ There is a huge piece of code with huge comments in {{sql_yacc.yy}}:

{code:sql}
if (is_native_function(thd, & sp->m_name))
{
  /*
   Huge comment
  */
  ...
}
{code}

responsible to display a warning when a user shadows a built-in function with a stored function.

The same warning will be needed for package routines (MDEV-10591).

We'll move this piece of code to {{sql_lex.cc}} into a new function {{is_native_function_with_warn()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,8,,0,0,1,5,0,0,0,,0,850,0,0,0,2017-03-29 13:13:24,Add function is_native_function_with_warn(),"There is a huge piece of code with huge comments in {{sql_yacc.yy}}:

{code:sql}
if (is_native_function(thd, & sp->m_name))
{
  /*
   Huge comment
  */
  ...
}
{code}

responsible to display a warning when a user shadows a built-in function with a stored function.

The same warning will be needed for package routines (MDEV-10591).

We'll move this piece of code to {{sql_lex.cc}} into a new function {{is_native_function_with_warn()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
",,0,0,0,0,0.0,"Add function is_native_function_with_warn() $end$ There is a huge piece of code with huge comments in {{sql_yacc.yy}}:

{code:sql}
if (is_native_function(thd, & sp->m_name))
{
  /*
   Huge comment
  */
  ...
}
{code}

responsible to display a warning when a user shadows a built-in function with a stored function.

The same warning will be needed for package routines (MDEV-10591).

We'll move this piece of code to {{sql_lex.cc}} into a new function {{is_native_function_with_warn()}}.

This change was originally a part of MDEV-11952, but it was decided to commit it as a standalone change.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,61,37,0.606557,27,0.442623,26,0.42623,25,0.409836,25,0.409836
648,MDEV-12411,Technical task,MDEV,2017-03-30 18:14:26,,0,Remove Lex::text_string_is_7bit,"Our current grammar in {{sql_yacc.yy}} uses {{LEX_STRING}} to return {{TEXT_STRING}} and {{NCHAR_STRING}} terminal symbols from the tokenizer, and additionally uses {{Lex->text_string_is_7bit}} to know a difference between 7bit and 8bit strings (for optimization purposes).

This approach is error prone. Changes in the grammar that require more look-ahead can put {{Lex->text_string_is_7bit}} out of sync from bison variables ({{$1}}, {{$2}}, {{$3}} etc), so for example {{Lext->text_string_is_7bit}} already corresponds to {{$2}} instead of expected {{$1}}.

A safe approach would be to return the {{LEX_STRING}} and the corresponding 7/8 bit flag as a single structure like this:

{code:cpp}
struct Lex_string_with_metadata_st: public LEX_STRING
{
  bool m_is_8bit;
public:
  void set_8bit(bool is_8bit) { m_is_8bit= is_8bit; }
  // Get string repertoire by the 8-bit flag and the character set
  uint repertoire(CHARSET_INFO *cs) const
  {
    return !m_is_8bit && my_charset_is_ascii_based(cs) ?
           MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
  // Get string repertoire by the 8-bit flag, for ASCII-based character sets
  uint repertoire() const
  {
    return !m_is_8bit ? MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
};
{code}
and use this structure for {{TEXT_STRING}} and {{NCHAR_STRING}}.

The problem was revealed by valgrind in the {{bb-10.2-compatibility}} branch when extending this rule:

{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  ;
{noformat}
to
{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  | RETURN_SYM /* from a procedure */
  ;
{noformat}

Before making changes in the grammar we should fix this problem.
",,"Remove Lex::text_string_is_7bit $end$ Our current grammar in {{sql_yacc.yy}} uses {{LEX_STRING}} to return {{TEXT_STRING}} and {{NCHAR_STRING}} terminal symbols from the tokenizer, and additionally uses {{Lex->text_string_is_7bit}} to know a difference between 7bit and 8bit strings (for optimization purposes).

This approach is error prone. Changes in the grammar that require more look-ahead can put {{Lex->text_string_is_7bit}} out of sync from bison variables ({{$1}}, {{$2}}, {{$3}} etc), so for example {{Lext->text_string_is_7bit}} already corresponds to {{$2}} instead of expected {{$1}}.

A safe approach would be to return the {{LEX_STRING}} and the corresponding 7/8 bit flag as a single structure like this:

{code:cpp}
struct Lex_string_with_metadata_st: public LEX_STRING
{
  bool m_is_8bit;
public:
  void set_8bit(bool is_8bit) { m_is_8bit= is_8bit; }
  // Get string repertoire by the 8-bit flag and the character set
  uint repertoire(CHARSET_INFO *cs) const
  {
    return !m_is_8bit && my_charset_is_ascii_based(cs) ?
           MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
  // Get string repertoire by the 8-bit flag, for ASCII-based character sets
  uint repertoire() const
  {
    return !m_is_8bit ? MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
};
{code}
and use this structure for {{TEXT_STRING}} and {{NCHAR_STRING}}.

The problem was revealed by valgrind in the {{bb-10.2-compatibility}} branch when extending this rule:

{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  ;
{noformat}
to
{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  | RETURN_SYM /* from a procedure */
  ;
{noformat}

Before making changes in the grammar we should fix this problem.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,22,,0,1,1,5,0,1,0,,0,850,1,0,0,2017-03-30 18:14:26,Remove Lex::text_string_is_7bit,"Our current grammar in {{sql_yacc.yy}} uses {{LEX_STRING}} to return {{TEXT_STRING}} and {{NCHAR_STRING}} terminal symbols from the tokenizer, and additionally uses {{Lex->text_string_is_7bit}} to know a difference between 7bit and 8bit strings (for optimization purposes).

This approach is error prone. Changes in the grammar that require more look-ahead can put {{Lex->text_string_is_7bit}} out of sync from bison variables ({{$1}}, {{$2}}, {{$3}} etc), so for example {{Lext->text_string_is_7bit}} already corresponds to {{$2}} instead of expected {{$1}}.

A safe approach would be to return the {{LEX_STRING}} and the corresponding 7/8 bit flag as a single structure like this:

{code:cpp}
struct Lex_string_with_metadata_st: public LEX_STRING
{
  bool m_is_8bit;
public:
  void set_8bit(bool is_8bit) { m_is_8bit= is_8bit; }
  // Get string repertoire by the 8-bit flag and the character set
  uint repertoire(CHARSET_INFO *cs) const
  {
    return !m_is_8bit && my_charset_is_ascii_based(cs) ?
           MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
  // Get string repertoire by the 8-bit flag, for ASCII-based character sets
  uint repertoire() const
  {
    return !m_is_8bit ? MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
};
{code}
and use this structure for {{TEXT_STRING}} and {{NCHAR_STRING}}.

The problem was revealed by valgrind when extending this rule:

{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  ;
{noformat}
to
{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  | RETURN_SYM /* from a procedure */
  ;
{noformat}

Before making changes in the grammar we should fix this problem.
",,0,1,0,4,0.0187793,"Remove Lex::text_string_is_7bit $end$ Our current grammar in {{sql_yacc.yy}} uses {{LEX_STRING}} to return {{TEXT_STRING}} and {{NCHAR_STRING}} terminal symbols from the tokenizer, and additionally uses {{Lex->text_string_is_7bit}} to know a difference between 7bit and 8bit strings (for optimization purposes).

This approach is error prone. Changes in the grammar that require more look-ahead can put {{Lex->text_string_is_7bit}} out of sync from bison variables ({{$1}}, {{$2}}, {{$3}} etc), so for example {{Lext->text_string_is_7bit}} already corresponds to {{$2}} instead of expected {{$1}}.

A safe approach would be to return the {{LEX_STRING}} and the corresponding 7/8 bit flag as a single structure like this:

{code:cpp}
struct Lex_string_with_metadata_st: public LEX_STRING
{
  bool m_is_8bit;
public:
  void set_8bit(bool is_8bit) { m_is_8bit= is_8bit; }
  // Get string repertoire by the 8-bit flag and the character set
  uint repertoire(CHARSET_INFO *cs) const
  {
    return !m_is_8bit && my_charset_is_ascii_based(cs) ?
           MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
  // Get string repertoire by the 8-bit flag, for ASCII-based character sets
  uint repertoire() const
  {
    return !m_is_8bit ? MY_REPERTOIRE_ASCII : MY_REPERTOIRE_UNICODE30;
  }
};
{code}
and use this structure for {{TEXT_STRING}} and {{NCHAR_STRING}}.

The problem was revealed by valgrind when extending this rule:

{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  ;
{noformat}
to
{noformat}
sp_proc_stmt_return:
    RETURN_SYM expr
  | RETURN_SYM /* from a procedure */
  ;
{noformat}

Before making changes in the grammar we should fix this problem.
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,62,37,0.596774,27,0.435484,26,0.419355,25,0.403226,25,0.403226
649,MDEV-12528,Task,MDEV,2017-04-19 13:40:12,,0,"Run the engine-agnostic test suite on MyRocks, too","(Filing so that it is not forgotten)

There is a storage-agnostic test suite in MariaDB somewhere. We should run it with MyRocks.
",,"Run the engine-agnostic test suite on MyRocks, too $end$ (Filing so that it is not forgotten)

There is a storage-agnostic test suite in MariaDB somewhere. We should run it with MyRocks.
 $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,8,,0,5,1,1,0,0,0,,0,850,2,0,0,2017-06-21 17:06:51,"Run the engine-agnostic test suite on MyRocks, too","(Filing so that it is not forgotten)

There is a storage-agnostic test suite in MariaDB somewhere. We should run it with MyRocks.
",,0,0,0,0,0.0,"Run the engine-agnostic test suite on MyRocks, too $end$ (Filing so that it is not forgotten)

There is a storage-agnostic test suite in MariaDB somewhere. We should run it with MyRocks.
 $acceptance criteria:$",0,0,0,0,0,0,0,1515.43,12,1,0.0833333,1,0.0833333,1,0.0833333,1,0.0833333,1,0.0833333
650,MDEV-12533,Technical task,MDEV,2017-04-20 01:43:49,,0,sql_mode=ORACLE: Add support for database qualified sequence names in NEXTVAL and CURRVAL,"The SQL standard syntax for {{NEXT VALUE FOR}} works for both non-qualified and qualified sequence names:
{code:sql}
SET sql_mode=ORACLE;
DROP SEQUENCE IF EXISTS s1;
CREATE SEQUENCE s1;
SELECT NEXT VALUE FOR s1;
SELECT s1.nextval;
{code}
{noformat}
+-------------------+
| NEXT VALUE FOR s1 |
+-------------------+
|                 1 |
+-------------------+
+------------+
| s1.nextval |
+------------+
|          2 |
+------------+
{noformat}
{code:sql}
SELECT NEXT VALUE FOR test.s1;
{code}
{noformat}
+------------------------+
| NEXT VALUE FOR test.s1 |
+------------------------+
|                      3 |
+------------------------+
{noformat}

Qualified sequence names also work with IBM DB2 syntax for {{PREVIOUS VALUE FOR}}
{code:sql}
SELECT PREVIOUS VALUE FOR test.s1;
{code}
{noformat}
+----------------------------+
| PREVIOUS VALUE FOR test.s1 |
+----------------------------+
|                          3 |
+----------------------------+
{noformat}

Under terms of this task we'll add support for qualified sequence names for Oracle syntax, to make these queries work:
{code:sql}
SELECT test.s1.nextval;
{code}
{code:sql}
SELECT test.s1.currval;
{code}
",,"sql_mode=ORACLE: Add support for database qualified sequence names in NEXTVAL and CURRVAL $end$ The SQL standard syntax for {{NEXT VALUE FOR}} works for both non-qualified and qualified sequence names:
{code:sql}
SET sql_mode=ORACLE;
DROP SEQUENCE IF EXISTS s1;
CREATE SEQUENCE s1;
SELECT NEXT VALUE FOR s1;
SELECT s1.nextval;
{code}
{noformat}
+-------------------+
| NEXT VALUE FOR s1 |
+-------------------+
|                 1 |
+-------------------+
+------------+
| s1.nextval |
+------------+
|          2 |
+------------+
{noformat}
{code:sql}
SELECT NEXT VALUE FOR test.s1;
{code}
{noformat}
+------------------------+
| NEXT VALUE FOR test.s1 |
+------------------------+
|                      3 |
+------------------------+
{noformat}

Qualified sequence names also work with IBM DB2 syntax for {{PREVIOUS VALUE FOR}}
{code:sql}
SELECT PREVIOUS VALUE FOR test.s1;
{code}
{noformat}
+----------------------------+
| PREVIOUS VALUE FOR test.s1 |
+----------------------------+
|                          3 |
+----------------------------+
{noformat}

Under terms of this task we'll add support for qualified sequence names for Oracle syntax, to make these queries work:
{code:sql}
SELECT test.s1.nextval;
{code}
{code:sql}
SELECT test.s1.currval;
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,10,,1,1,2,5,0,0,0,,0,850,1,0,0,2017-04-20 01:43:49,sql_mode=ORACLE: Add support for database qualified sequence names in NEXTVAL and CURRVAL,"The SQL standard syntax for {{NEXT VALUE FOR}} works for both non-qualified and qualified sequence names:
{code:sql}
SET sql_mode=ORACLE;
DROP SEQUENCE IF EXISTS s1;
CREATE SEQUENCE s1;
SELECT NEXT VALUE FOR s1;
SELECT s1.nextval;
{code}
{noformat}
+-------------------+
| NEXT VALUE FOR s1 |
+-------------------+
|                 1 |
+-------------------+
+------------+
| s1.nextval |
+------------+
|          2 |
+------------+
{noformat}
{code:sql}
SELECT NEXT VALUE FOR test.s1;
{code}
{noformat}
+------------------------+
| NEXT VALUE FOR test.s1 |
+------------------------+
|                      3 |
+------------------------+
{noformat}

Qualified sequence names also work with IBM DB2 syntax for {{PREVIOUS VALUE FOR}}
{code:sql}
SELECT PREVIOUS VALUE FOR test.s1;
{code}
{noformat}
+----------------------------+
| PREVIOUS VALUE FOR test.s1 |
+----------------------------+
|                          3 |
+----------------------------+
{noformat}

Under terms of this task we'll add support for qualified sequence names for Oracle syntax, to make these queries work:
{code:sql}
SELECT test.s1.nextval;
{code}
{code:sql}
SELECT test.s1.currval;
{code}
",,0,0,0,0,0.0,"sql_mode=ORACLE: Add support for database qualified sequence names in NEXTVAL and CURRVAL $end$ The SQL standard syntax for {{NEXT VALUE FOR}} works for both non-qualified and qualified sequence names:
{code:sql}
SET sql_mode=ORACLE;
DROP SEQUENCE IF EXISTS s1;
CREATE SEQUENCE s1;
SELECT NEXT VALUE FOR s1;
SELECT s1.nextval;
{code}
{noformat}
+-------------------+
| NEXT VALUE FOR s1 |
+-------------------+
|                 1 |
+-------------------+
+------------+
| s1.nextval |
+------------+
|          2 |
+------------+
{noformat}
{code:sql}
SELECT NEXT VALUE FOR test.s1;
{code}
{noformat}
+------------------------+
| NEXT VALUE FOR test.s1 |
+------------------------+
|                      3 |
+------------------------+
{noformat}

Qualified sequence names also work with IBM DB2 syntax for {{PREVIOUS VALUE FOR}}
{code:sql}
SELECT PREVIOUS VALUE FOR test.s1;
{code}
{noformat}
+----------------------------+
| PREVIOUS VALUE FOR test.s1 |
+----------------------------+
|                          3 |
+----------------------------+
{noformat}

Under terms of this task we'll add support for qualified sequence names for Oracle syntax, to make these queries work:
{code:sql}
SELECT test.s1.nextval;
{code}
{code:sql}
SELECT test.s1.currval;
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,63,38,0.603175,27,0.428571,26,0.412698,25,0.396825,25,0.396825
651,MDEV-12542,Task,MDEV,2017-04-20 19:24:30,,0,Add bind_address system variable,"MySQL added it in 5.6:
https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_bind_address",,"Add bind_address system variable $end$ MySQL added it in 5.6:
https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_bind_address $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,5,,0,1,2,1,0,0,0,,0,850,1,0,0,2017-10-24 17:55:12,Add bind_address system variable,"MySQL added it in 5.6:
https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_bind_address",,0,0,0,0,0.0,"Add bind_address system variable $end$ MySQL added it in 5.6:
https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_bind_address $acceptance criteria:$",0,0,0,0,0,0,0,4486.5,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
652,MDEV-12548,Task,MDEV,2017-04-21 10:57:49,,0,Add maria backup tool to MariaDB 10.2,"InnoDB I/O and buffer pool interfaces and the redo log format have been changed between MariaDB 10.1 and 10.2, and the backup code has to be adjusted accordingly.

The Mariabackup code will been simplified, and many memory leaks will be fixed. Instead of the file name xtrabackup_logfile, the file name ib_logfile0 will be used for the copy of the redo log. Unnecessary InnoDB startup and shutdown and some unnecessary threads will be removed.

Parameters will be cleaned up and aligned with those of MariaDB 10.2.

By default, innodb_doublewrite=OFF, so that --prepare works faster. If more crash-safety for --prepare is needed, doublewrite can be enabled.

The parameter innodb_log_checksums=OFF can be used to ignore redo log checksums in --backup.

Some messages will be cleaned up. Unless --export is specified, Mariabackup will not deal with undo log. The InnoDB mini-transaction redo log is not only about user-level transactions; it is actually about mini-transactions. To avoid confusion, we will call it the redo log, not transaction log.

We disable any undo log processing in --prepare without --export. Because MariaDB 10.2 supports indexed virtual columns, the undo log processing would need to be able to evaluate virtual column expressions. To reduce the amount of code dependencies, --prepare by itself will only apply the redo log.

In addition to disabling any undo log processing, we will disable any further changes to data pages during --prepare, including the change buffer merge. This means that restoring incremental backups should reliably work even when change buffering is being used on the server. Because of this, preparing a backup will not generate any further redo log, and the redo log file can be safely deleted.

This means that the following options are redundant and have been removed:
            xtrabackup --apply-log-only
            innobackupex --redo-only

In the initial implementation, the --export option will be disabled. If it is enabled in the future, it must generate redo log when processing undo logs and buffered changes. It will also involve a full server restart.

For proper restore and handling of partial backups, we should consider fixing MDEV-11898 in the server. Because --prepare will not deal with anything else than the redo log, it would not drop any excluded tables either. Maybe we should do that in the server proper.",,"Add maria backup tool to MariaDB 10.2 $end$ InnoDB I/O and buffer pool interfaces and the redo log format have been changed between MariaDB 10.1 and 10.2, and the backup code has to be adjusted accordingly.

The Mariabackup code will been simplified, and many memory leaks will be fixed. Instead of the file name xtrabackup_logfile, the file name ib_logfile0 will be used for the copy of the redo log. Unnecessary InnoDB startup and shutdown and some unnecessary threads will be removed.

Parameters will be cleaned up and aligned with those of MariaDB 10.2.

By default, innodb_doublewrite=OFF, so that --prepare works faster. If more crash-safety for --prepare is needed, doublewrite can be enabled.

The parameter innodb_log_checksums=OFF can be used to ignore redo log checksums in --backup.

Some messages will be cleaned up. Unless --export is specified, Mariabackup will not deal with undo log. The InnoDB mini-transaction redo log is not only about user-level transactions; it is actually about mini-transactions. To avoid confusion, we will call it the redo log, not transaction log.

We disable any undo log processing in --prepare without --export. Because MariaDB 10.2 supports indexed virtual columns, the undo log processing would need to be able to evaluate virtual column expressions. To reduce the amount of code dependencies, --prepare by itself will only apply the redo log.

In addition to disabling any undo log processing, we will disable any further changes to data pages during --prepare, including the change buffer merge. This means that restoring incremental backups should reliably work even when change buffering is being used on the server. Because of this, preparing a backup will not generate any further redo log, and the redo log file can be safely deleted.

This means that the following options are redundant and have been removed:
            xtrabackup --apply-log-only
            innobackupex --redo-only

In the initial implementation, the --export option will be disabled. If it is enabled in the future, it must generate redo log when processing undo logs and buffered changes. It will also involve a full server restart.

For proper restore and handling of partial backups, we should consider fixing MDEV-11898 in the server. Because --prepare will not deal with anything else than the redo log, it would not drop any excluded tables either. Maybe we should do that in the server proper. $acceptance criteria:$",,Rasmus Johansson,Rasmus Johansson,Major,6,,5,0,9,1,0,0,0,,0,850,0,0,0,2017-06-22 04:38:27,Add maria backup tool to MariaDB 10.2,"InnoDB I/O and buffer pool interfaces and the redo log format have been changed between MariaDB 10.1 and 10.2, and the backup code has to be adjusted accordingly.

The Mariabackup code will been simplified, and many memory leaks will be fixed. Instead of the file name xtrabackup_logfile, the file name ib_logfile0 will be used for the copy of the redo log. Unnecessary InnoDB startup and shutdown and some unnecessary threads will be removed.

Parameters will be cleaned up and aligned with those of MariaDB 10.2.

By default, innodb_doublewrite=OFF, so that --prepare works faster. If more crash-safety for --prepare is needed, doublewrite can be enabled.

The parameter innodb_log_checksums=OFF can be used to ignore redo log checksums in --backup.

Some messages will be cleaned up. Unless --export is specified, Mariabackup will not deal with undo log. The InnoDB mini-transaction redo log is not only about user-level transactions; it is actually about mini-transactions. To avoid confusion, we will call it the redo log, not transaction log.

We disable any undo log processing in --prepare without --export. Because MariaDB 10.2 supports indexed virtual columns, the undo log processing would need to be able to evaluate virtual column expressions. To reduce the amount of code dependencies, --prepare by itself will only apply the redo log.

In addition to disabling any undo log processing, we will disable any further changes to data pages during --prepare, including the change buffer merge. This means that restoring incremental backups should reliably work even when change buffering is being used on the server. Because of this, preparing a backup will not generate any further redo log, and the redo log file can be safely deleted.

This means that the following options are redundant and have been removed:
            xtrabackup --apply-log-only
            innobackupex --redo-only

In the initial implementation, the --export option will be disabled. If it is enabled in the future, it must generate redo log when processing undo logs and buffered changes. It will also involve a full server restart.

For proper restore and handling of partial backups, we should consider fixing MDEV-11898 in the server. Because --prepare will not deal with anything else than the redo log, it would not drop any excluded tables either. Maybe we should do that in the server proper.",,0,0,0,0,0.0,"Add maria backup tool to MariaDB 10.2 $end$ InnoDB I/O and buffer pool interfaces and the redo log format have been changed between MariaDB 10.1 and 10.2, and the backup code has to be adjusted accordingly.

The Mariabackup code will been simplified, and many memory leaks will be fixed. Instead of the file name xtrabackup_logfile, the file name ib_logfile0 will be used for the copy of the redo log. Unnecessary InnoDB startup and shutdown and some unnecessary threads will be removed.

Parameters will be cleaned up and aligned with those of MariaDB 10.2.

By default, innodb_doublewrite=OFF, so that --prepare works faster. If more crash-safety for --prepare is needed, doublewrite can be enabled.

The parameter innodb_log_checksums=OFF can be used to ignore redo log checksums in --backup.

Some messages will be cleaned up. Unless --export is specified, Mariabackup will not deal with undo log. The InnoDB mini-transaction redo log is not only about user-level transactions; it is actually about mini-transactions. To avoid confusion, we will call it the redo log, not transaction log.

We disable any undo log processing in --prepare without --export. Because MariaDB 10.2 supports indexed virtual columns, the undo log processing would need to be able to evaluate virtual column expressions. To reduce the amount of code dependencies, --prepare by itself will only apply the redo log.

In addition to disabling any undo log processing, we will disable any further changes to data pages during --prepare, including the change buffer merge. This means that restoring incremental backups should reliably work even when change buffering is being used on the server. Because of this, preparing a backup will not generate any further redo log, and the redo log file can be safely deleted.

This means that the following options are redundant and have been removed:
            xtrabackup --apply-log-only
            innobackupex --redo-only

In the initial implementation, the --export option will be disabled. If it is enabled in the future, it must generate redo log when processing undo logs and buffered changes. It will also involve a full server restart.

For proper restore and handling of partial backups, we should consider fixing MDEV-11898 in the server. Because --prepare will not deal with anything else than the redo log, it would not drop any excluded tables either. Maybe we should do that in the server proper. $acceptance criteria:$",0,0,0,0,0,0,0,1481.67,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
653,MDEV-12685,Technical task,MDEV,2017-05-04 08:44:28,,0,Oracle-compatible function CHR(),"We'll add a new function {{CHR(num)}}.
It will be very similar to MariaDB's {{CHAR(num)}}, but
- will accept only one argument
- will return a VARCHAR(1) with character set and collation according to @@character_set_database and @@collation_database, rather than VARBINARY(N).

It will be available in all {{sql_mode}}'s.
",,"Oracle-compatible function CHR() $end$ We'll add a new function {{CHR(num)}}.
It will be very similar to MariaDB's {{CHAR(num)}}, but
- will accept only one argument
- will return a VARCHAR(1) with character set and collation according to @@character_set_database and @@collation_database, rather than VARBINARY(N).

It will be available in all {{sql_mode}}'s.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,8,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-05-04 08:44:28,Oracle-compatible function CHR(),"We'll add a new function {{CHR(num)}}.
It will be very similar to MariaDB's {{CHAR(num)}}, but
- will accept only one argument
- will return a VARCHAR(1) with character set and collation according to @@character_set_database and @@collation_database, rather than VARBINARY(N).

It will be available in all {{sql_mode}}'s.
",,0,0,0,0,0.0,"Oracle-compatible function CHR() $end$ We'll add a new function {{CHR(num)}}.
It will be very similar to MariaDB's {{CHAR(num)}}, but
- will accept only one argument
- will return a VARCHAR(1) with character set and collation according to @@character_set_database and @@collation_database, rather than VARBINARY(N).

It will be available in all {{sql_mode}}'s.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,64,38,0.59375,27,0.421875,26,0.40625,25,0.390625,25,0.390625
654,MDEV-12783,Technical task,MDEV,2017-05-11 12:25:29,,0,sql_mode=ORACLE: Functions LENGTH() and LENGTHB(),"MariaDB translates function {{LENGTH()}} to SQL Standard function {{OCTET_LENGTH()}}.
Oracle translates function {{LENGTH()}} to SQL Standard function {{CHAR_LENGTH()}}.

We'll change MariaDB so it also translates {{LENGTH()}} to {{CHAR_LENGTH()}} rather than {{OCTET_LENGTH()}} when running with {{sql_mode=ORACLE}}.

Additionally, we'll add {{LENGTHB()}} as a synonym for {{OCTET_LENGTH()}}. This synonym will be available in all {{sql_mode}}'s.
",,"sql_mode=ORACLE: Functions LENGTH() and LENGTHB() $end$ MariaDB translates function {{LENGTH()}} to SQL Standard function {{OCTET_LENGTH()}}.
Oracle translates function {{LENGTH()}} to SQL Standard function {{CHAR_LENGTH()}}.

We'll change MariaDB so it also translates {{LENGTH()}} to {{CHAR_LENGTH()}} rather than {{OCTET_LENGTH()}} when running with {{sql_mode=ORACLE}}.

Additionally, we'll add {{LENGTHB()}} as a synonym for {{OCTET_LENGTH()}}. This synonym will be available in all {{sql_mode}}'s.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,0,0,2,5,0,1,0,,0,850,0,0,0,2017-05-11 12:25:29,sql_mode=ORACLE: Functions LENGTH() and LENGTHB(),"
MariaDB translates function {{LENGTH()}} to SQL Standard function {{OCTET_LENGTH()}}.
Oracle translates function {{LENGTH()}} to SQL Standard function {{CHAR_LENGTH()}}.

We'll change MariaDB so it also translates {{LENGTH()}} to {{CHAR_LENGTH()}} rather than {{OCTET_LENGTH()}} when running with {{sql_mode=ORACLE}}.

Additionally, we'll add {{LENGTHB()}} as a synonym for {{OCTET_LENGTH()}}.",,0,1,0,8,0.153846,"sql_mode=ORACLE: Functions LENGTH() and LENGTHB() $end$ 
MariaDB translates function {{LENGTH()}} to SQL Standard function {{OCTET_LENGTH()}}.
Oracle translates function {{LENGTH()}} to SQL Standard function {{CHAR_LENGTH()}}.

We'll change MariaDB so it also translates {{LENGTH()}} to {{CHAR_LENGTH()}} rather than {{OCTET_LENGTH()}} when running with {{sql_mode=ORACLE}}.

Additionally, we'll add {{LENGTHB()}} as a synonym for {{OCTET_LENGTH()}}. $acceptance criteria:$",1,1,1,0,0,0,1,0.0,65,38,0.584615,27,0.415385,26,0.4,25,0.384615,25,0.384615
655,MDEV-12836,Task,MDEV,2017-05-18 12:28:07,,0,Avoid table rebuild when removing of auto_increment settings,"Basically, it's a request to implement upstream feature request:

https://bugs.mysql.com/bug.php?id=72109

Copying the table to just drop auto_increment attribute:

{noformat}
MariaDB [test]> create table t(id int auto_increment not null, key(id)) engine=InnoDB;
Query OK, 0 rows affected (0.22 sec)

MariaDB [test]> alter table t modify column id int not null, algorithm=inplace; 
ERROR 1846 (0A000): ALGORITHM=INPLACE is not supported. Reason: Cannot change column type INPLACE. Try ALGORITHM=COPY
{noformat}

is awful in production. Workarounds exist, but why not to do this in-place and without copying all the data?",,"Avoid table rebuild when removing of auto_increment settings $end$ Basically, it's a request to implement upstream feature request:

https://bugs.mysql.com/bug.php?id=72109

Copying the table to just drop auto_increment attribute:

{noformat}
MariaDB [test]> create table t(id int auto_increment not null, key(id)) engine=InnoDB;
Query OK, 0 rows affected (0.22 sec)

MariaDB [test]> alter table t modify column id int not null, algorithm=inplace; 
ERROR 1846 (0A000): ALGORITHM=INPLACE is not supported. Reason: Cannot change column type INPLACE. Try ALGORITHM=COPY
{noformat}

is awful in production. Workarounds exist, but why not to do this in-place and without copying all the data? $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Critical,20,,3,11,4,2,0,1,0,,0,850,10,0,0,2017-12-19 16:40:27,Avoid table rebuild when adding or removing of auto_increment settings,"Basically, it's a request to implement upstream feature request:

https://bugs.mysql.com/bug.php?id=72109

Copying the table to just drop auto_increment attribute:

{noformat}
MariaDB [test]> create table t(id int auto_increment not null, key(id)) engine=InnoDB;
Query OK, 0 rows affected (0.22 sec)

MariaDB [test]> alter table t modify column id int not null, algorithm=inplace; 
ERROR 1846 (0A000): ALGORITHM=INPLACE is not supported. Reason: Cannot change column type INPLACE. Try ALGORITHM=COPY
{noformat}

is awful in production. Workarounds exist, but why not to do this in-place and without copying all the data?",,1,0,0,2,0.0206186,"Avoid table rebuild when adding or removing of auto_increment settings $end$ Basically, it's a request to implement upstream feature request:

https://bugs.mysql.com/bug.php?id=72109

Copying the table to just drop auto_increment attribute:

{noformat}
MariaDB [test]> create table t(id int auto_increment not null, key(id)) engine=InnoDB;
Query OK, 0 rows affected (0.22 sec)

MariaDB [test]> alter table t modify column id int not null, algorithm=inplace; 
ERROR 1846 (0A000): ALGORITHM=INPLACE is not supported. Reason: Cannot change column type INPLACE. Try ALGORITHM=COPY
{noformat}

is awful in production. Workarounds exist, but why not to do this in-place and without copying all the data? $acceptance criteria:$",1,1,0,0,0,0,1,5164.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
656,MDEV-12874,Technical task,MDEV,2017-05-23 13:29:48,,0,UPDATE statements with the same source and target,"This script works fine in Oracle (and PostgreSQL):
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
INSERT INTO t1 VALUES (10,10);
INSERT INTO t1 VALUES (20,20);
UPDATE t1 SET c1=c1+1 WHERE c2=(SELECT MAX(c2) FROM t1);
SELECT * FROM t1;
{code}
{noformat}
SQL> SELECT * FROM t1; 

	C1	   C2
---------- ----------
	10	   10
	21	   20
{noformat}
Notice, the table {{t1}} is used twice in the {{UPDATE}} query, as a target and as a source.

Under term of this tasks will allow to use the same table as a source and as a target in {{UPDATE}} statements in MariaDB.

Currently its not allowed and returns this error:
{noformat}
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'UPDATE' and as a separate source for data
{noformat}
",,"UPDATE statements with the same source and target $end$ This script works fine in Oracle (and PostgreSQL):
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
INSERT INTO t1 VALUES (10,10);
INSERT INTO t1 VALUES (20,20);
UPDATE t1 SET c1=c1+1 WHERE c2=(SELECT MAX(c2) FROM t1);
SELECT * FROM t1;
{code}
{noformat}
SQL> SELECT * FROM t1; 

	C1	   C2
---------- ----------
	10	   10
	21	   20
{noformat}
Notice, the table {{t1}} is used twice in the {{UPDATE}} query, as a target and as a source.

Under term of this tasks will allow to use the same table as a source and as a target in {{UPDATE}} statements in MariaDB.

Currently its not allowed and returns this error:
{noformat}
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'UPDATE' and as a separate source for data
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Critical,12,,0,0,2,5,0,2,0,,0,850,0,0,0,2017-05-23 13:29:48,MDEV-12137 UPDATE statements with the same source and target,"This script works fine in Oracle (and PostgreSQL):
{code:sql} 
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
INSERT INTO t1 VALUES (10,10);
INSERT INTO t1 VALUES (20,20);
UPDATE t1 SET c1=c1+1 WHERE c2=(SELECT MAX(c2) FROM t1);
SELECT * FROM t1;
{code}
{noformat}
SQL> SELECT * FROM t1; 

	C1	   C2
---------- ----------
	10	   10
	21	   20
{noformat}
Notice, the table {{t1}} is used twice in the {{UPDATE}} query, as a target and as a source.

Under term of this tasks will allow to use the same table as a source and as a target in {{UPDATE}} statements in MariaDB.

Currently its not allowed and returns this error:
{noformat}
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'UPDATE' and as a separate source for data
{noformat}
",,1,1,0,1,0.00699301,"MDEV-12137 UPDATE statements with the same source and target $end$ This script works fine in Oracle (and PostgreSQL):
{code:sql} 
DROP TABLE t1;
CREATE TABLE t1 (c1 INT, c2 INT);
INSERT INTO t1 VALUES (10,10);
INSERT INTO t1 VALUES (20,20);
UPDATE t1 SET c1=c1+1 WHERE c2=(SELECT MAX(c2) FROM t1);
SELECT * FROM t1;
{code}
{noformat}
SQL> SELECT * FROM t1; 

	C1	   C2
---------- ----------
	10	   10
	21	   20
{noformat}
Notice, the table {{t1}} is used twice in the {{UPDATE}} query, as a target and as a source.

Under term of this tasks will allow to use the same table as a source and as a target in {{UPDATE}} statements in MariaDB.

Currently its not allowed and returns this error:
{noformat}
ERROR 1093 (HY000): Table 't1' is specified twice, both as a target for 'UPDATE' and as a separate source for data
{noformat}
 $acceptance criteria:$",2,1,0,0,0,0,1,0.0,66,39,0.590909,28,0.424242,26,0.393939,25,0.378788,25,0.378788
657,MDEV-12894,Task,MDEV,2017-05-24 13:16:14,,0,System-versioned tables,"Implement support for ""System-versioned tables"" as in SQL:2016.

Clauses {{PERIOD SYSTEM_TIME}}, {{WITH SYSTEM VERSIONING}}, {{FOR SYSTEM_TIME AS OF}}, {{FOR SYSTEM_TIME BETWEEN ... AND}}, {{FOR SYSTEM_TIME FROM ... TO}}.",,"System-versioned tables $end$ Implement support for ""System-versioned tables"" as in SQL:2016.

Clauses {{PERIOD SYSTEM_TIME}}, {{WITH SYSTEM VERSIONING}}, {{FOR SYSTEM_TIME AS OF}}, {{FOR SYSTEM_TIME BETWEEN ... AND}}, {{FOR SYSTEM_TIME FROM ... TO}}. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,17,,5,1,9,3,0,0,0,,0,850,1,0,0,2017-05-24 13:17:58,System-versioned tables,"Implement support for ""System-versioned tables"" as in SQL:2016.

Clauses {{PERIOD SYSTEM_TIME}}, {{WITH SYSTEM VERSIONING}}, {{FOR SYSTEM_TIME AS OF}}, {{FOR SYSTEM_TIME BETWEEN ... AND}}, {{FOR SYSTEM_TIME FROM ... TO}}.",,0,0,0,0,0.0,"System-versioned tables $end$ Implement support for ""System-versioned tables"" as in SQL:2016.

Clauses {{PERIOD SYSTEM_TIME}}, {{WITH SYSTEM VERSIONING}}, {{FOR SYSTEM_TIME AS OF}}, {{FOR SYSTEM_TIME BETWEEN ... AND}}, {{FOR SYSTEM_TIME FROM ... TO}}. $acceptance criteria:$",0,0,0,0,0,0,1,0.0166667,49,25,0.510204,19,0.387755,14,0.285714,13,0.265306,10,0.204082
658,MDEV-12913,Task,MDEV,2017-05-24 19:06:33,,0,SP stack trace,"PR:396 implements stored routine error stack trace.

h2. Example

This script creates two procedures. {{p2}} calls {{p1}}, and {{p1}} opens a cursor using a non-existing table:
{code:sql}
DELIMITER $$
CREATE OR REPLACE PROCEDURE p1()
BEGIN
  DECLARE c CURSOR FOR SELECT * FROM not_existing;
  OPEN c;
  CLOSE c;
END;
$$
CREATE OR REPLACE PROCEDURE p2()
BEGIN
  CALL p1;
END;
$$
DELIMITER ;
CALL p2;
{code}
{noformat}
ERROR 1146 (42S02): Table 'test.not_existing' doesn't exist
{noformat}
The script returns an error, as expected.

Now it's possible to get additional diagnostics:
{code:sql}
SHOW WARNINGS;
{code}
{noformat}
+-------+------+-----------------------------------------+
| Level | Code | Message                                 |
+-------+------+-----------------------------------------+
| Error | 1146 | Table 'test.not_existing' doesn't exist |
| Note  | 4070 | At line 4 in test.p1                    |
| Note  | 4070 | At line 3 in test.p2                    |
+-------+------+-----------------------------------------+
{noformat}
It displays stack trace, showing where the error actually happened:
- Line {{4}} in {{test.p1}} is the {{OPEN}} command which actually raised the error
- Line {{3}} in {{test.p2}} is the {{CALL}} statement, calling {{p1}} from {{p2}}.
",,"SP stack trace $end$ PR:396 implements stored routine error stack trace.

h2. Example

This script creates two procedures. {{p2}} calls {{p1}}, and {{p1}} opens a cursor using a non-existing table:
{code:sql}
DELIMITER $$
CREATE OR REPLACE PROCEDURE p1()
BEGIN
  DECLARE c CURSOR FOR SELECT * FROM not_existing;
  OPEN c;
  CLOSE c;
END;
$$
CREATE OR REPLACE PROCEDURE p2()
BEGIN
  CALL p1;
END;
$$
DELIMITER ;
CALL p2;
{code}
{noformat}
ERROR 1146 (42S02): Table 'test.not_existing' doesn't exist
{noformat}
The script returns an error, as expected.

Now it's possible to get additional diagnostics:
{code:sql}
SHOW WARNINGS;
{code}
{noformat}
+-------+------+-----------------------------------------+
| Level | Code | Message                                 |
+-------+------+-----------------------------------------+
| Error | 1146 | Table 'test.not_existing' doesn't exist |
| Note  | 4070 | At line 4 in test.p1                    |
| Note  | 4070 | At line 3 in test.p2                    |
+-------+------+-----------------------------------------+
{noformat}
It displays stack trace, showing where the error actually happened:
- Line {{4}} in {{test.p1}} is the {{OPEN}} command which actually raised the error
- Line {{3}} in {{test.p2}} is the {{CALL}} statement, calling {{p1}} from {{p2}}.
 $acceptance criteria:$",,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,11,,0,1,0,1,0,4,1,,0,850,1,0,0,2017-05-29 10:56:30,SP stack trace,PR:396,,0,4,0,171,24.4286,SP stack trace $end$ PR:396 $acceptance criteria:$,4,1,1,1,1,1,1,111.817,5,2,0.4,2,0.4,2,0.4,1,0.2,1,0.2
659,MDEV-12985,Task,MDEV,2017-06-02 20:14:00,,0,support percentile and median window functions,"The percentile_cont and percentile_disc window functions are available in columnstore and many other databases. These allow calculation of percentiles. Percentile_cont will average 2 rows if one is not identified while Percentile_disc picks the first row in the window. Finally a median function should exist which is equivalent to percentile_cont(0.5).

These have slightly different syntax than other window function to specify the column:

percentile_cont(0.5) within group (order by amount) over (partition by owner) pct_cont,
percentile_disc(0.5) within group (order by amount) over (partition by owner) pct_disc

h2. Some investigation
percentile_cont and percentile_disc are not specifically window functions. They originally are ""ordered-set aggregate functions"" (#1) which one can also use as window functions (#2):

h3. Ordered-set aggreates
The syntax for case #1:
{noformat}
  percentile_cont(fraction) WITHIN GROUP (ORDER BY sort_expression)
{noformat}
Note the lack of OVER clause. 
Ordered-set aggregate functions are supported by:
* https://www.postgresql.org/docs/current/static/functions-aggregate.html
* http://docs.aws.amazon.com/redshift/latest/dg/c_Aggregate_Functions.html
Neither MariaDB nor MySQL support any ""ordered-set aggregate functions"".

h3. Ordered-set aggregates as window functions
Syntax for case #2 (ordered-set aggregate, used as window function)

* http://docs.aws.amazon.com/redshift/latest/dg/r_WF_PERCENTILE_DISC.html
* https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions100.htm#i1000909

{noformat}
PERCENTILE_DISC ( percentile )
WITHIN GROUP (ORDER BY expr)
OVER (  [ PARTITION BY expr_list ]  )
{noformat}

(BTW: note that PostgreSQL doesn't support ordered-set-aggregates-as-window functions: https://www.postgresql.org/docs/current/static/functions-window.html , 
{quote}any built-in or user-defined normal aggregate function (but not ordered-set or hypothetical-set aggregates) can be used as a window function)
{quote}
",,"support percentile and median window functions $end$ The percentile_cont and percentile_disc window functions are available in columnstore and many other databases. These allow calculation of percentiles. Percentile_cont will average 2 rows if one is not identified while Percentile_disc picks the first row in the window. Finally a median function should exist which is equivalent to percentile_cont(0.5).

These have slightly different syntax than other window function to specify the column:

percentile_cont(0.5) within group (order by amount) over (partition by owner) pct_cont,
percentile_disc(0.5) within group (order by amount) over (partition by owner) pct_disc

h2. Some investigation
percentile_cont and percentile_disc are not specifically window functions. They originally are ""ordered-set aggregate functions"" (#1) which one can also use as window functions (#2):

h3. Ordered-set aggreates
The syntax for case #1:
{noformat}
  percentile_cont(fraction) WITHIN GROUP (ORDER BY sort_expression)
{noformat}
Note the lack of OVER clause. 
Ordered-set aggregate functions are supported by:
* https://www.postgresql.org/docs/current/static/functions-aggregate.html
* http://docs.aws.amazon.com/redshift/latest/dg/c_Aggregate_Functions.html
Neither MariaDB nor MySQL support any ""ordered-set aggregate functions"".

h3. Ordered-set aggregates as window functions
Syntax for case #2 (ordered-set aggregate, used as window function)

* http://docs.aws.amazon.com/redshift/latest/dg/r_WF_PERCENTILE_DISC.html
* https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions100.htm#i1000909

{noformat}
PERCENTILE_DISC ( percentile )
WITHIN GROUP (ORDER BY expr)
OVER (  [ PARTITION BY expr_list ]  )
{noformat}

(BTW: note that PostgreSQL doesn't support ordered-set-aggregates-as-window functions: https://www.postgresql.org/docs/current/static/functions-window.html , 
{quote}any built-in or user-defined normal aggregate function (but not ordered-set or hypothetical-set aggregates) can be used as a window function)
{quote}
 $acceptance criteria:$",,David Thompson,David Thompson,Major,16,,1,10,5,2,0,1,0,,0,850,3,1,0,2017-07-06 20:24:35,support percentile and median window functions,"The percentile_cont and percentile_disc window functions are available in columnstore and many other databases. These allow calculation of percentiles. Percentile_cont will average 2 rows if one is not identified while Percentile_disc picks the first row in the window. Finally a median function should exist which is equivalent to percentile_cont(0.5).

These have slightly different syntax than other window function to specify the column:

percentile_cont(0.5) within group (order by amount) over (partition by owner) pct_cont,
percentile_disc(0.5) within group (order by amount) over (partition by owner) pct_disc

h2. Some investigation
percentile_cont and percentile_disc are not specifically window functions. They originally are ""ordered-set aggregate functions"" (#1) which one can also use as window functions (#2):

h3. Ordered-set aggreates
The syntax for case #1:
{noformat}
  percentile_cont(fraction) WITHIN GROUP (ORDER BY sort_expression)
{noformat}
Note the lack of OVER clause. 
Ordered-set aggregate functions are supported by:
* https://www.postgresql.org/docs/current/static/functions-aggregate.html
* http://docs.aws.amazon.com/redshift/latest/dg/c_Aggregate_Functions.html
Neither MariaDB nor MySQL support any ""ordered-set aggregate functions"".

h3. Ordered-set aggregates as window functions
Syntax for case #2 (ordered-set aggregate, used as window function)

* http://docs.aws.amazon.com/redshift/latest/dg/r_WF_PERCENTILE_DISC.html
* https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions100.htm#i1000909

{noformat}
PERCENTILE_DISC ( percentile )
WITHIN GROUP (ORDER BY expr)
OVER (  [ PARTITION BY expr_list ]  )
{noformat}

(BTW: note that PostgreSQL doesn't support ordered-set-aggregates-as-window functions: https://www.postgresql.org/docs/current/static/functions-window.html , 
{quote}any built-in or user-defined normal aggregate function (but not ordered-set or hypothetical-set aggregates) can be used as a window function)
{quote}
",,0,0,0,0,0.0,"support percentile and median window functions $end$ The percentile_cont and percentile_disc window functions are available in columnstore and many other databases. These allow calculation of percentiles. Percentile_cont will average 2 rows if one is not identified while Percentile_disc picks the first row in the window. Finally a median function should exist which is equivalent to percentile_cont(0.5).

These have slightly different syntax than other window function to specify the column:

percentile_cont(0.5) within group (order by amount) over (partition by owner) pct_cont,
percentile_disc(0.5) within group (order by amount) over (partition by owner) pct_disc

h2. Some investigation
percentile_cont and percentile_disc are not specifically window functions. They originally are ""ordered-set aggregate functions"" (#1) which one can also use as window functions (#2):

h3. Ordered-set aggreates
The syntax for case #1:
{noformat}
  percentile_cont(fraction) WITHIN GROUP (ORDER BY sort_expression)
{noformat}
Note the lack of OVER clause. 
Ordered-set aggregate functions are supported by:
* https://www.postgresql.org/docs/current/static/functions-aggregate.html
* http://docs.aws.amazon.com/redshift/latest/dg/c_Aggregate_Functions.html
Neither MariaDB nor MySQL support any ""ordered-set aggregate functions"".

h3. Ordered-set aggregates as window functions
Syntax for case #2 (ordered-set aggregate, used as window function)

* http://docs.aws.amazon.com/redshift/latest/dg/r_WF_PERCENTILE_DISC.html
* https://docs.oracle.com/cd/B12037_01/server.101/b10759/functions100.htm#i1000909

{noformat}
PERCENTILE_DISC ( percentile )
WITHIN GROUP (ORDER BY expr)
OVER (  [ PARTITION BY expr_list ]  )
{noformat}

(BTW: note that PostgreSQL doesn't support ordered-set-aggregates-as-window functions: https://www.postgresql.org/docs/current/static/functions-window.html , 
{quote}any built-in or user-defined normal aggregate function (but not ordered-set or hypothetical-set aggregates) can be used as a window function)
{quote}
 $acceptance criteria:$",0,0,0,0,0,0,1,816.167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
660,MDEV-13073,Task,MDEV,2017-06-13 10:51:22,MDEV-11372,0,AliSQL: [Performance] Issue #40 Optimize performance of semisync,"{noformat}
Description:
------------
This diff includes:
    1. Make semisync buidin to completely remove overhead of plugin lock.
    2. Remove LOCK_log requirement from dump thread
    3. Now user threads can wait for ACK before innodb commit.
        add rpl_semi_sync_master_wait_point to control this behavior(AFTER_COMMIT/AFTER_SYNC)
    4. Add a new ACK thread to handle ACK from slave.
    5. The IO thread flushs master info only when ACK is needed
        add rpl_semi_sync_slave_delay_master to control this behavior
    6. Fix bug#70669, if sync_binlog = 1 ,then notify dump thread after fsync of binlog file.
{noformat}

https://github.com/alibaba/AliSQL/commit/348783276913b0ad73d7a473498c0f8ea42ee9b0

This patch makes the variable rpl_semi_sync_slave obsolete, which is why it was removed.",,"AliSQL: [Performance] Issue #40 Optimize performance of semisync $end$ {noformat}
Description:
------------
This diff includes:
    1. Make semisync buidin to completely remove overhead of plugin lock.
    2. Remove LOCK_log requirement from dump thread
    3. Now user threads can wait for ACK before innodb commit.
        add rpl_semi_sync_master_wait_point to control this behavior(AFTER_COMMIT/AFTER_SYNC)
    4. Add a new ACK thread to handle ACK from slave.
    5. The IO thread flushs master info only when ACK is needed
        add rpl_semi_sync_slave_delay_master to control this behavior
    6. Fix bug#70669, if sync_binlog = 1 ,then notify dump thread after fsync of binlog file.
{noformat}

https://github.com/alibaba/AliSQL/commit/348783276913b0ad73d7a473498c0f8ea42ee9b0

This patch makes the variable rpl_semi_sync_slave obsolete, which is why it was removed. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,16,,2,4,2,2,0,1,0,,0,850,1,0,0,2018-03-14 17:11:34,AliSQL: [Performance] Issue #40 Optimize performance of semisync,"{noformat}
Description:
------------
This diff includes:
    1. Make semisync buidin to completely remove overhead of plugin lock.
    2. Remove LOCK_log requirement from dump thread
    3. Now user threads can wait for ACK before innodb commit.
        add rpl_semi_sync_master_wait_point to control this behavior(AFTER_COMMIT/AFTER_SYNC)
    4. Add a new ACK thread to handle ACK from slave.
    5. The IO thread flushs master info only when ACK is needed
        add rpl_semi_sync_slave_delay_master to control this behavior
    6. Fix bug#70669, if sync_binlog = 1 ,then notify dump thread after fsync of binlog file.
{noformat}

https://github.com/alibaba/AliSQL/commit/348783276913b0ad73d7a473498c0f8ea42ee9b0",,0,1,0,13,0.131313,"AliSQL: [Performance] Issue #40 Optimize performance of semisync $end$ {noformat}
Description:
------------
This diff includes:
    1. Make semisync buidin to completely remove overhead of plugin lock.
    2. Remove LOCK_log requirement from dump thread
    3. Now user threads can wait for ACK before innodb commit.
        add rpl_semi_sync_master_wait_point to control this behavior(AFTER_COMMIT/AFTER_SYNC)
    4. Add a new ACK thread to handle ACK from slave.
    5. The IO thread flushs master info only when ACK is needed
        add rpl_semi_sync_slave_delay_master to control this behavior
    6. Fix bug#70669, if sync_binlog = 1 ,then notify dump thread after fsync of binlog file.
{noformat}

https://github.com/alibaba/AliSQL/commit/348783276913b0ad73d7a473498c0f8ea42ee9b0 $acceptance criteria:$",1,1,1,1,0,0,1,6582.33,22,2,0.0909091,1,0.0454545,1,0.0454545,1,0.0454545,1,0.0454545
661,MDEV-13095,Task,MDEV,2017-06-14 21:37:49,,0,Implement user account locking,"MariaDB should support locking or unlocking user accounts via the·                                                                                                                     
_ACCOUNT LOCK_ and _ACCOUNT UNLOCK_ options for the _CREATE USER_
and _ALTER USER_ statements.

Given MySQL 5.7 already has this feature, we should preserve
compatibility in terms of both API and datadir migration.

We should support the following use cases:
{noformat}
    MariaDB [(none)]> CREATE USER user@localhost ACCOUNT LOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> CREATE USER user@localhost ACCOUNT UNLOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> ALTER USER user@localhost ACCOUNT LOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> SHOW CREATE USER user@localhost;
    +---------------------------------------------+
    | CREATE USER for user@localhost              |   
    +---------------------------------------------+
    | CREATE USER 'user'@'localhost' ACCOUNT LOCK |
    +---------------------------------------------+
    1 row in set (0.000 sec)
{noformat}

{noformat}
    MariaDB [(none)]> ALTER USER user@localhost ACCOUNT UNLOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> SHOW CREATE USER user@localhost;
    +-----------------------------------------------+
    | CREATE USER for user@localhost                |   
    +-----------------------------------------------+
    | CREATE USER 'user'@'localhost' ACCOUNT UNLOCK |
    +-----------------------------------------------+
    1 row in set (0.000 sec)
{noformat}

When a new connection is attempted to a locked account, the server should
return an _ER_LOCKED_ACCOUNT_ error code.

Regarding the required privileges for user account locking, there should be
no additional privileges required except for what it is already required
by the _CREATE USER_ and _ALTER USER_ statements.

Note| The users are allowed to drop themselves or change their own password,
we should follow a similar behavior in user account locking.

*Implementation details*:
* The locking state of an account should be kept in the JSON Priv column of
  mysql.global_priv. The *User_table_json* class will be enriched with accessors
  for reading/writing from/to the *account_locked* JSON field.
{noformat}
    MariaDB [(none)]> select user, host, Priv from mysql.global_priv where user='user';
    +-------+-----------+------------------------------+
    | user  | host      | Priv                         |
    +-------+-----------+------------------------------+
    | user  | localhost | {..., ""account_locked"":true} |
    +-------+-----------+------------------------------+
    1 row in set (0.001 sec)
{noformat}

* To preserve the drop-in replacement property for MySQL 5.7 datadirs, we have to add
similar accessors with the ones above to the *User_table_tabular* class which
will read/write from/to the account_locked column in the mysql.user table.

References:
https://dev.mysql.com/doc/refman/5.7/en/account-locking.html   ",,"Implement user account locking $end$ MariaDB should support locking or unlocking user accounts via the·                                                                                                                     
_ACCOUNT LOCK_ and _ACCOUNT UNLOCK_ options for the _CREATE USER_
and _ALTER USER_ statements.

Given MySQL 5.7 already has this feature, we should preserve
compatibility in terms of both API and datadir migration.

We should support the following use cases:
{noformat}
    MariaDB [(none)]> CREATE USER user@localhost ACCOUNT LOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> CREATE USER user@localhost ACCOUNT UNLOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> ALTER USER user@localhost ACCOUNT LOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> SHOW CREATE USER user@localhost;
    +---------------------------------------------+
    | CREATE USER for user@localhost              |   
    +---------------------------------------------+
    | CREATE USER 'user'@'localhost' ACCOUNT LOCK |
    +---------------------------------------------+
    1 row in set (0.000 sec)
{noformat}

{noformat}
    MariaDB [(none)]> ALTER USER user@localhost ACCOUNT UNLOCK;
    Query OK, 0 rows affected (0.00 sec)
{noformat}

{noformat}
    MariaDB [(none)]> SHOW CREATE USER user@localhost;
    +-----------------------------------------------+
    | CREATE USER for user@localhost                |   
    +-----------------------------------------------+
    | CREATE USER 'user'@'localhost' ACCOUNT UNLOCK |
    +-----------------------------------------------+
    1 row in set (0.000 sec)
{noformat}

When a new connection is attempted to a locked account, the server should
return an _ER_LOCKED_ACCOUNT_ error code.

Regarding the required privileges for user account locking, there should be
no additional privileges required except for what it is already required
by the _CREATE USER_ and _ALTER USER_ statements.

Note| The users are allowed to drop themselves or change their own password,
we should follow a similar behavior in user account locking.

*Implementation details*:
* The locking state of an account should be kept in the JSON Priv column of
  mysql.global_priv. The *User_table_json* class will be enriched with accessors
  for reading/writing from/to the *account_locked* JSON field.
{noformat}
    MariaDB [(none)]> select user, host, Priv from mysql.global_priv where user='user';
    +-------+-----------+------------------------------+
    | user  | host      | Priv                         |
    +-------+-----------+------------------------------+
    | user  | localhost | {..., ""account_locked"":true} |
    +-------+-----------+------------------------------+
    1 row in set (0.001 sec)
{noformat}

* To preserve the drop-in replacement property for MySQL 5.7 datadirs, we have to add
similar accessors with the ones above to the *User_table_tabular* class which
will read/write from/to the account_locked column in the mysql.user table.

References:
https://dev.mysql.com/doc/refman/5.7/en/account-locking.html    $acceptance criteria:$",,Geoff Montee,Geoff Montee,Critical,29,,1,3,2,1,0,8,0,,0,850,3,0,0,2018-05-29 07:55:50,Implement user account locking,"MySQL 5.7 supports account locking:

https://dev.mysql.com/doc/refman/5.7/en/account-locking.html

e.g.:

{noformat}
ALTER USER myuser ACCOUNT LOCK;
{noformat}

Should MariaDB also implement this? There's some discussion on use cases here:

http://mysqlblog.fivefarmers.com/2015/04/21/locking-accounts-in-mysql-5-7/",,0,8,0,372,10.1176,"Implement user account locking $end$ MySQL 5.7 supports account locking:

https://dev.mysql.com/doc/refman/5.7/en/account-locking.html

e.g.:

{noformat}
ALTER USER myuser ACCOUNT LOCK;
{noformat}

Should MariaDB also implement this? There's some discussion on use cases here:

http://mysqlblog.fivefarmers.com/2015/04/21/locking-accounts-in-mysql-5-7/ $acceptance criteria:$",8,1,1,1,1,1,1,8362.3,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
662,MDEV-13122,Task,MDEV,2017-06-19 08:45:19,,0,Backup myrocks with mariabackup,,,Backup myrocks with mariabackup $end$ $acceptance criteria:$,,Vladislav Vaintroub,Vladislav Vaintroub,Major,6,,0,2,4,1,0,0,0,,0,850,0,0,0,2018-05-29 07:55:28,Backup myrocks with mariabackup,,,0,0,0,0,0.0,Backup myrocks with mariabackup $end$ $acceptance criteria:$,0,0,0,0,0,0,0,8255.17,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
663,MDEV-13141,Task,MDEV,2017-06-21 12:07:29,,0,Buildbot prototype with LatentDockerWorker,"Build MariaDB server (after each commit to the source) on a remote Ubuntu docker worker, using LatentDockerWorker, with bb04.mariadb.net as the master.",,"Buildbot prototype with LatentDockerWorker $end$ Build MariaDB server (after each commit to the source) on a remote Ubuntu docker worker, using LatentDockerWorker, with bb04.mariadb.net as the master. $acceptance criteria:$",,Vesa Pentti,Vesa Pentti,Major,4,,0,1,1,1,0,0,0,,0,850,1,0,0,2017-06-21 12:12:37,Buildbot prototype with LatentDockerWorker,"Build MariaDB server (after each commit to the source) on a remote Ubuntu docker worker, using LatentDockerWorker, with bb04.mariadb.net as the master.",,0,0,0,0,0.0,"Buildbot prototype with LatentDockerWorker $end$ Build MariaDB server (after each commit to the source) on a remote Ubuntu docker worker, using LatentDockerWorker, with bb04.mariadb.net as the master. $acceptance criteria:$",0,0,0,0,0,0,0,0.0833333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
664,MDEV-13197,Technical task,MDEV,2017-06-28 07:46:30,,0,"Parser refactoring for CREATE VIEW,TRIGGER,SP,UDF,EVENT","This task is intended to simplify the work for {{CREATE PACKAGE}} (see MDEV-10591), as well as to fix minor known problems in the underlying parser grammar.

There are a few difficulties in the grammar for {{CREATE}} for {{VIEW}}, {{TRIGGER}}, {{EVENT}}, {{PROCEDURE}}, {{FUNCTION}}.

- The grammar for the mentioned objects is implemented with help of the rule {{view_or_trigger_or_sp_or_event}}. The grammar for all other object types is implemented directly in the {{create:}} rule. This makes the grammar non-symmetric for various object types, which is hard to read and understand.

- We'll add {{CREATE PACKAGE}} soon. It will require the {{DEFINER}} clause. In the current grammar, it will have to go to {{view_or_trigger_or_sp_or_event}} again, and make the things even more complex.

- {{LEX::create_view_mode}}, {{LEX::create_view_algorithm}}, {{LEX::create_view_suid}} are initialized for all objects described in {{view_or_trigger_or_sp_or_event}}. For non-{{VIEW}} objects this initialization is redundant.
{code:sql}
        | create_or_replace
          {
            Lex->create_info.set($1);
            Lex->create_view_mode= ($1.or_replace() ? VIEW_CREATE_OR_REPLACE :
                                                      VIEW_CREATE_NEW);
            Lex->create_view_algorithm= DTYPE_ALGORITHM_UNDEFINED;
            Lex->create_view_suid= TRUE;
          }
          view_or_trigger_or_sp_or_event { }
{code}

- The grammar erroneously accepts these weird query:
{code:sql}
ALTER VIEW IF NOT EXISTS v1 AS SELECT 1;
{code}
It's really senseless to {{ALTER}} something that {{NOT EXISTS}}.
The {{IF NOT EXISTS}} clause is accepted because both {{CREATE VIEW}} and {{ALTER VIEW}} grammar reuses the {{view_tail}} rule.


Under terms of this task we'll do the following:

- For stricter data type control: introduce a new {{enum}} instead of pre-processor constants:

{code:sql}
enum enum_view_suid
{
  VIEW_SUID_INVOKER= 0,
  VIEW_SUID_DEFINER= 1,
  VIEW_SUID_DEFAULT= 2
};
{code}


- Introduce a new class:

{code:cpp}
class Create_view_info: public Sql_alloc
{
public:
  LEX_CSTRING select;              // The SELECT statement of CREATE VIEW
  enum enum_view_create_mode mode;
  uint16 algorithm;
  uint8 check;
  enum enum_view_suid suid;
  Create_view_info(enum_view_create_mode mode_arg,
                   uint16 algorithm_arg,
                   enum_view_suid suid_arg)
   :select(null_clex_str),
    mode(mode_arg),
    algorithm(algorithm_arg),
    check(VIEW_CHECK_NONE),  
    suid(suid_arg)
  { }
}; 
{code}

- Remove the same members from {{LEX}} and add a pointer to {{Create_view_info}} instead. {{Create_view_info}} will be allocated on {{THD}} memory root only for {{CREATE VIEW}} or {{ALTER VIEW}} queries.


- Reduce the amount of duplicate {{C++}} code used in {{.yy}} files by adding new methods in {{LEX}}:

{code:sql}
  bool add_alter_view(THD *thd, uint16 algorithm, enum_view_suid suid,
                      Table_ident *table_ident);
  bool add_create_view(THD *thd, DDL_options_st ddl,
                       uint16 algorithm, enum_view_suid suid,
                       Table_ident *table_ident);
{code}

- In {{sql_yacc.yy}}: remove grammar rules {{view_or_trigger_or_sp_or_event}}, {{definer_tail}}, {{no_definer_tail}}, {{view_tail}}

- In {{sql_yacc.yy}}: add new grammar branches directly into the {{create:}} rules, for {{VIEW}}, {{TRIGGER}}, {{PROCEDURE}}, {{FUNCTION}}, {{EVENT}}.
Note, when we add the {{CREATE PACKAGE}} statement later (see MDEV-10591), it will also be added directly to {{create:}}, together with its optional {{DEFINER}} clause.

- In {{sql_yacc.yy}}: fix the rules {{view_algorithm}}, {{view_suid}}, {{view_check_options}} to return values in {{$$}}, instead of setting {{thd->lex}} members directly. This is needed to pass these values to the new methods {{LEX::add_alter_view()}} and {{LEX::add_create_view()}}.

- Do corresponding changes in {{sql_yacc_ora.yy}}.
",,"Parser refactoring for CREATE VIEW,TRIGGER,SP,UDF,EVENT $end$ This task is intended to simplify the work for {{CREATE PACKAGE}} (see MDEV-10591), as well as to fix minor known problems in the underlying parser grammar.

There are a few difficulties in the grammar for {{CREATE}} for {{VIEW}}, {{TRIGGER}}, {{EVENT}}, {{PROCEDURE}}, {{FUNCTION}}.

- The grammar for the mentioned objects is implemented with help of the rule {{view_or_trigger_or_sp_or_event}}. The grammar for all other object types is implemented directly in the {{create:}} rule. This makes the grammar non-symmetric for various object types, which is hard to read and understand.

- We'll add {{CREATE PACKAGE}} soon. It will require the {{DEFINER}} clause. In the current grammar, it will have to go to {{view_or_trigger_or_sp_or_event}} again, and make the things even more complex.

- {{LEX::create_view_mode}}, {{LEX::create_view_algorithm}}, {{LEX::create_view_suid}} are initialized for all objects described in {{view_or_trigger_or_sp_or_event}}. For non-{{VIEW}} objects this initialization is redundant.
{code:sql}
        | create_or_replace
          {
            Lex->create_info.set($1);
            Lex->create_view_mode= ($1.or_replace() ? VIEW_CREATE_OR_REPLACE :
                                                      VIEW_CREATE_NEW);
            Lex->create_view_algorithm= DTYPE_ALGORITHM_UNDEFINED;
            Lex->create_view_suid= TRUE;
          }
          view_or_trigger_or_sp_or_event { }
{code}

- The grammar erroneously accepts these weird query:
{code:sql}
ALTER VIEW IF NOT EXISTS v1 AS SELECT 1;
{code}
It's really senseless to {{ALTER}} something that {{NOT EXISTS}}.
The {{IF NOT EXISTS}} clause is accepted because both {{CREATE VIEW}} and {{ALTER VIEW}} grammar reuses the {{view_tail}} rule.


Under terms of this task we'll do the following:

- For stricter data type control: introduce a new {{enum}} instead of pre-processor constants:

{code:sql}
enum enum_view_suid
{
  VIEW_SUID_INVOKER= 0,
  VIEW_SUID_DEFINER= 1,
  VIEW_SUID_DEFAULT= 2
};
{code}


- Introduce a new class:

{code:cpp}
class Create_view_info: public Sql_alloc
{
public:
  LEX_CSTRING select;              // The SELECT statement of CREATE VIEW
  enum enum_view_create_mode mode;
  uint16 algorithm;
  uint8 check;
  enum enum_view_suid suid;
  Create_view_info(enum_view_create_mode mode_arg,
                   uint16 algorithm_arg,
                   enum_view_suid suid_arg)
   :select(null_clex_str),
    mode(mode_arg),
    algorithm(algorithm_arg),
    check(VIEW_CHECK_NONE),  
    suid(suid_arg)
  { }
}; 
{code}

- Remove the same members from {{LEX}} and add a pointer to {{Create_view_info}} instead. {{Create_view_info}} will be allocated on {{THD}} memory root only for {{CREATE VIEW}} or {{ALTER VIEW}} queries.


- Reduce the amount of duplicate {{C++}} code used in {{.yy}} files by adding new methods in {{LEX}}:

{code:sql}
  bool add_alter_view(THD *thd, uint16 algorithm, enum_view_suid suid,
                      Table_ident *table_ident);
  bool add_create_view(THD *thd, DDL_options_st ddl,
                       uint16 algorithm, enum_view_suid suid,
                       Table_ident *table_ident);
{code}

- In {{sql_yacc.yy}}: remove grammar rules {{view_or_trigger_or_sp_or_event}}, {{definer_tail}}, {{no_definer_tail}}, {{view_tail}}

- In {{sql_yacc.yy}}: add new grammar branches directly into the {{create:}} rules, for {{VIEW}}, {{TRIGGER}}, {{PROCEDURE}}, {{FUNCTION}}, {{EVENT}}.
Note, when we add the {{CREATE PACKAGE}} statement later (see MDEV-10591), it will also be added directly to {{create:}}, together with its optional {{DEFINER}} clause.

- In {{sql_yacc.yy}}: fix the rules {{view_algorithm}}, {{view_suid}}, {{view_check_options}} to return values in {{$$}}, instead of setting {{thd->lex}} members directly. This is needed to pass these values to the new methods {{LEX::add_alter_view()}} and {{LEX::add_create_view()}}.

- Do corresponding changes in {{sql_yacc_ora.yy}}.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,13,,0,0,1,5,0,1,0,,0,850,0,0,0,2017-06-28 07:46:30,"Parser refactoring for CREATE VIEW,TRIGGER,SP,UDF,EVENT","This task is intended to simplify the work for {{CREATE PACKAGE}} (see MDEV-10591), as well as to fix minor known problems in the underlying parser grammar.

There are a few difficulties in the grammar for {{CREATE}} for {{VIEW}}, {{TRIGGER}}, {{EVENT}}, {{PROCEDURE}}, {{FUNCTION}}.

- The grammar for the mentioned objects is implemented with help of the rule {{view_or_trigger_or_sp_or_event}}. The grammar for all other object types is implemented directly in the {{create:}} rule. This makes the grammar non-symmetric for various object types, which is hard to read and understand.

- We'll add {{CREATE PACKAGE}} soon. It will require the {{DEFINER}} clause. In the current grammar, it will have to go to {{view_or_trigger_or_sp_or_event}} again, and make the things even more complex.

- {{LEX::create_view_mode}}, {{LEX::create_view_algorithm}}, {{LEX::create_view_suid}} are initialized for all objects described in {{view_or_trigger_or_sp_or_event}}. For non-{{VIEW}} objects this initialization is redundant.
{code:sql}
        | create_or_replace
          {
            Lex->create_info.set($1);
            Lex->create_view_mode= ($1.or_replace() ? VIEW_CREATE_OR_REPLACE :
                                                      VIEW_CREATE_NEW);
            Lex->create_view_algorithm= DTYPE_ALGORITHM_UNDEFINED;
            Lex->create_view_suid= TRUE;
          }
          view_or_trigger_or_sp_or_event { }
{code}

- The grammar erroneously accepts these weird query:
{code:sql}
ALTER VIEW IF NOT EXISTS v1 AS SELECT 1;
{code}
It's really senseless to {{ALTER}} something that {{NOT EXISTS}}.
The {{IF NOT EXISTS}} clause is accepted because both {{CREATE VIEW}} and {{ALTER VIEW}} grammar reuses the {{view_tail}} rule.


Under terms of this task we'll do the following:

- For stricter data type control: introduce a new {{enum}} instead of pre-processor constants:

{code:sql}
enum enum_view_suid
{
  VIEW_SUID_INVOKER= 0,
  VIEW_SUID_DEFINER= 1,
  VIEW_SUID_DEFAULT= 2
};
{code}


- Introduce a new class:

{code:cpp}
class Create_view_info: public Sql_alloc
{
public:
  LEX_CSTRING select;              // The SELECT statement of CREATE VIEW
  enum enum_view_create_mode mode;
  uint16 algorithm;
  uint8 check;
  enum enum_view_suid suid;
  Create_view_info(enum_view_create_mode mode_arg,
                   uint16 algorithm_arg,
                   enum_view_suid suid_arg)
   :select(null_clex_str),
    mode(mode_arg),
    algorithm(algorithm_arg),
    check(VIEW_CHECK_NONE),  
    suid(suid_arg)
  { }
}; 
{code}

- Remove the same members from {{LEX}} and add a pointer to {{Create_view_info}} instead. {{Create_view_info}} will be allocated on {{THD}} memory root only for {{CREATE VIEW}} or {{ALTER VIEW}} queries.


- Reduce the amount of duplicate {{C++}} code used in {{.yy}} files by adding new methods in {{LEX}}:

{code:sql}
  bool add_alter_view(THD *thd, uint16 algorithm, enum_view_suid suid,
                      Table_ident *table_ident);
  bool add_create_view(THD *thd, DDL_options_st ddl,
                       uint16 algorithm, enum_view_suid suid,
                       Table_ident *table_ident);
{code}

- In {{sql_yacc.yy}}: remove grammar rules {{view_or_trigger_or_sp_or_event}}, {{definer_tail}}, {{no_definer_tail}}, {{view_tail}}

- In {{sql_yacc.yy}}: add new grammar branches directly into the {{create:}} rules, for {{VIEW}}, {{TRIGGER}}, {{PROCEDURE}}, {{FUNCTION}}, {{EVENT}}.
Note, when we add the {{CREATE PACKAGE}} statement later (see MDEV-10591}}, it will also be added directly to {{create:}}, together with its optional {{DEFINER}} clause.

- In {{sql_yacc.yy}}: fix the rules {{view_algorithm}}, {{view_suid}}, {{view_check_options}} to return values in {{$$}}, instead of setting {{thd->lex}} members directly. This is needed to pass these values to the new methods {{LEX::add_alter_view()}} and {{LEX::add_create_view()}}.

- Do corresponding changes in {{sql_yacc_ora.yy}}.
",,0,1,0,2,0.00219298,"Parser refactoring for CREATE VIEW,TRIGGER,SP,UDF,EVENT $end$ This task is intended to simplify the work for {{CREATE PACKAGE}} (see MDEV-10591), as well as to fix minor known problems in the underlying parser grammar.

There are a few difficulties in the grammar for {{CREATE}} for {{VIEW}}, {{TRIGGER}}, {{EVENT}}, {{PROCEDURE}}, {{FUNCTION}}.

- The grammar for the mentioned objects is implemented with help of the rule {{view_or_trigger_or_sp_or_event}}. The grammar for all other object types is implemented directly in the {{create:}} rule. This makes the grammar non-symmetric for various object types, which is hard to read and understand.

- We'll add {{CREATE PACKAGE}} soon. It will require the {{DEFINER}} clause. In the current grammar, it will have to go to {{view_or_trigger_or_sp_or_event}} again, and make the things even more complex.

- {{LEX::create_view_mode}}, {{LEX::create_view_algorithm}}, {{LEX::create_view_suid}} are initialized for all objects described in {{view_or_trigger_or_sp_or_event}}. For non-{{VIEW}} objects this initialization is redundant.
{code:sql}
        | create_or_replace
          {
            Lex->create_info.set($1);
            Lex->create_view_mode= ($1.or_replace() ? VIEW_CREATE_OR_REPLACE :
                                                      VIEW_CREATE_NEW);
            Lex->create_view_algorithm= DTYPE_ALGORITHM_UNDEFINED;
            Lex->create_view_suid= TRUE;
          }
          view_or_trigger_or_sp_or_event { }
{code}

- The grammar erroneously accepts these weird query:
{code:sql}
ALTER VIEW IF NOT EXISTS v1 AS SELECT 1;
{code}
It's really senseless to {{ALTER}} something that {{NOT EXISTS}}.
The {{IF NOT EXISTS}} clause is accepted because both {{CREATE VIEW}} and {{ALTER VIEW}} grammar reuses the {{view_tail}} rule.


Under terms of this task we'll do the following:

- For stricter data type control: introduce a new {{enum}} instead of pre-processor constants:

{code:sql}
enum enum_view_suid
{
  VIEW_SUID_INVOKER= 0,
  VIEW_SUID_DEFINER= 1,
  VIEW_SUID_DEFAULT= 2
};
{code}


- Introduce a new class:

{code:cpp}
class Create_view_info: public Sql_alloc
{
public:
  LEX_CSTRING select;              // The SELECT statement of CREATE VIEW
  enum enum_view_create_mode mode;
  uint16 algorithm;
  uint8 check;
  enum enum_view_suid suid;
  Create_view_info(enum_view_create_mode mode_arg,
                   uint16 algorithm_arg,
                   enum_view_suid suid_arg)
   :select(null_clex_str),
    mode(mode_arg),
    algorithm(algorithm_arg),
    check(VIEW_CHECK_NONE),  
    suid(suid_arg)
  { }
}; 
{code}

- Remove the same members from {{LEX}} and add a pointer to {{Create_view_info}} instead. {{Create_view_info}} will be allocated on {{THD}} memory root only for {{CREATE VIEW}} or {{ALTER VIEW}} queries.


- Reduce the amount of duplicate {{C++}} code used in {{.yy}} files by adding new methods in {{LEX}}:

{code:sql}
  bool add_alter_view(THD *thd, uint16 algorithm, enum_view_suid suid,
                      Table_ident *table_ident);
  bool add_create_view(THD *thd, DDL_options_st ddl,
                       uint16 algorithm, enum_view_suid suid,
                       Table_ident *table_ident);
{code}

- In {{sql_yacc.yy}}: remove grammar rules {{view_or_trigger_or_sp_or_event}}, {{definer_tail}}, {{no_definer_tail}}, {{view_tail}}

- In {{sql_yacc.yy}}: add new grammar branches directly into the {{create:}} rules, for {{VIEW}}, {{TRIGGER}}, {{PROCEDURE}}, {{FUNCTION}}, {{EVENT}}.
Note, when we add the {{CREATE PACKAGE}} statement later (see MDEV-10591}}, it will also be added directly to {{create:}}, together with its optional {{DEFINER}} clause.

- In {{sql_yacc.yy}}: fix the rules {{view_algorithm}}, {{view_suid}}, {{view_check_options}} to return values in {{$$}}, instead of setting {{thd->lex}} members directly. This is needed to pass these values to the new methods {{LEX::add_alter_view()}} and {{LEX::add_create_view()}}.

- Do corresponding changes in {{sql_yacc_ora.yy}}.
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,67,40,0.597015,28,0.41791,26,0.38806,25,0.373134,25,0.373134
665,MDEV-13292,Technical task,MDEV,2017-07-11 11:08:36,,0,Move the code from sp_head::init() to sp_head::sp_head(),"This task is a part of MDEV-10591 Oracle-style packages.

In order for packages to reuse all routine features, such as:
- package-wide variables, exceptions, cursors, etc
- package executable initialization block
under terms of MDEV-10591 we'll introduce new classes (with proposed names {{Package}} and {{Package_body}})
and derived these classes from {{sp_head}}.

To simplify initialization of the {{Package*}} instances, we need to move most the code from {{sp_head::init}} to {{sp_head::sp_head}}.

The relevant code:
{code:cpp}
sp_head::init(LEX *lex)
{
  ...
  m_param_begin= NULL;
  m_param_end= NULL;

  m_body_begin= NULL ;

  m_qname.str= NULL;
  m_qname.length= 0;

  m_explicit_name= false;

  m_db.str= NULL;
  m_db.length= 0;

  m_name.str= NULL;
  m_name.length= 0;

  m_params.str= NULL;
  m_params.length= 0;

  m_body.str= NULL;
  m_body.length= 0;

  m_defstr.str= NULL;
  m_defstr.length= 0;

  m_return_field_def.charset= NULL;
  ...
}
{code}


Currently new instances of {{sp_head}} are created by {{LEX::make_sp_head}}, in this code:
{code:sql}
  ...
  if ((sp= new sp_head(type)))
  {
    sp->reset_thd_mem_root(thd);
    sp->init(this);
  ...
{code}
Notice, the constructor call in {{new}} is immediately followed by {{init(this)}}.
After this task, the above members will be initialized in the constructor rather than in {{init()}}.



",,"Move the code from sp_head::init() to sp_head::sp_head() $end$ This task is a part of MDEV-10591 Oracle-style packages.

In order for packages to reuse all routine features, such as:
- package-wide variables, exceptions, cursors, etc
- package executable initialization block
under terms of MDEV-10591 we'll introduce new classes (with proposed names {{Package}} and {{Package_body}})
and derived these classes from {{sp_head}}.

To simplify initialization of the {{Package*}} instances, we need to move most the code from {{sp_head::init}} to {{sp_head::sp_head}}.

The relevant code:
{code:cpp}
sp_head::init(LEX *lex)
{
  ...
  m_param_begin= NULL;
  m_param_end= NULL;

  m_body_begin= NULL ;

  m_qname.str= NULL;
  m_qname.length= 0;

  m_explicit_name= false;

  m_db.str= NULL;
  m_db.length= 0;

  m_name.str= NULL;
  m_name.length= 0;

  m_params.str= NULL;
  m_params.length= 0;

  m_body.str= NULL;
  m_body.length= 0;

  m_defstr.str= NULL;
  m_defstr.length= 0;

  m_return_field_def.charset= NULL;
  ...
}
{code}


Currently new instances of {{sp_head}} are created by {{LEX::make_sp_head}}, in this code:
{code:sql}
  ...
  if ((sp= new sp_head(type)))
  {
    sp->reset_thd_mem_root(thd);
    sp->init(this);
  ...
{code}
Notice, the constructor call in {{new}} is immediately followed by {{init(this)}}.
After this task, the above members will be initialized in the constructor rather than in {{init()}}.



 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,6,,0,1,2,5,0,0,0,,0,850,1,0,0,2017-07-11 11:08:36,Move the code from sp_head::init() to sp_head::sp_head(),"This task is a part of MDEV-10591 Oracle-style packages.

In order for packages to reuse all routine features, such as:
- package-wide variables, exceptions, cursors, etc
- package executable initialization block
under terms of MDEV-10591 we'll introduce new classes (with proposed names {{Package}} and {{Package_body}})
and derived these classes from {{sp_head}}.

To simplify initialization of the {{Package*}} instances, we need to move most the code from {{sp_head::init}} to {{sp_head::sp_head}}.

The relevant code:
{code:cpp}
sp_head::init(LEX *lex)
{
  ...
  m_param_begin= NULL;
  m_param_end= NULL;

  m_body_begin= NULL ;

  m_qname.str= NULL;
  m_qname.length= 0;

  m_explicit_name= false;

  m_db.str= NULL;
  m_db.length= 0;

  m_name.str= NULL;
  m_name.length= 0;

  m_params.str= NULL;
  m_params.length= 0;

  m_body.str= NULL;
  m_body.length= 0;

  m_defstr.str= NULL;
  m_defstr.length= 0;

  m_return_field_def.charset= NULL;
  ...
}
{code}


Currently new instances of {{sp_head}} are created by {{LEX::make_sp_head}}, in this code:
{code:sql}
  ...
  if ((sp= new sp_head(type)))
  {
    sp->reset_thd_mem_root(thd);
    sp->init(this);
  ...
{code}
Notice, the constructor call in {{new}} is immediately followed by {{init(this)}}.
After this task, the above members will be initialized in the constructor rather than in {{init()}}.



",,0,0,0,0,0.0,"Move the code from sp_head::init() to sp_head::sp_head() $end$ This task is a part of MDEV-10591 Oracle-style packages.

In order for packages to reuse all routine features, such as:
- package-wide variables, exceptions, cursors, etc
- package executable initialization block
under terms of MDEV-10591 we'll introduce new classes (with proposed names {{Package}} and {{Package_body}})
and derived these classes from {{sp_head}}.

To simplify initialization of the {{Package*}} instances, we need to move most the code from {{sp_head::init}} to {{sp_head::sp_head}}.

The relevant code:
{code:cpp}
sp_head::init(LEX *lex)
{
  ...
  m_param_begin= NULL;
  m_param_end= NULL;

  m_body_begin= NULL ;

  m_qname.str= NULL;
  m_qname.length= 0;

  m_explicit_name= false;

  m_db.str= NULL;
  m_db.length= 0;

  m_name.str= NULL;
  m_name.length= 0;

  m_params.str= NULL;
  m_params.length= 0;

  m_body.str= NULL;
  m_body.length= 0;

  m_defstr.str= NULL;
  m_defstr.length= 0;

  m_return_field_def.charset= NULL;
  ...
}
{code}


Currently new instances of {{sp_head}} are created by {{LEX::make_sp_head}}, in this code:
{code:sql}
  ...
  if ((sp= new sp_head(type)))
  {
    sp->reset_thd_mem_root(thd);
    sp->init(this);
  ...
{code}
Notice, the constructor call in {{new}} is immediately followed by {{init(this)}}.
After this task, the above members will be initialized in the constructor rather than in {{init()}}.



 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,68,41,0.602941,28,0.411765,26,0.382353,25,0.367647,25,0.367647
666,MDEV-13298,Technical task,MDEV,2017-07-12 07:54:40,,0,Change sp_head::m_chistics from a pointer to a structure,"In order to reuse {{sp_head}} for new classes behind Oracle-style packages (MDEV-10591), we'll fix some {{st_sp_chistics}} related design problems in {{sp_head}}.

1. {{sp_head}} has a member {{st_sp_chistics *m_chistics}}, which can point in different moments of time to memory of different nature:
- {{m_chistics}} can be {{NULL}}
- {{m_chistics}} can point to {{Lex->sp_chistics}}
- {{m_chistics}} can point to a memory allocated on {{sp_head::mem_root}}
- {{m_chistics.comment.str}} can be {{NULL}}
- {{m_chistics.comment.str}} can point to the currently parsed query fragment (the query itself is alloced on {{thd->mem_root}}).
- {{m_chistics.comment.str}} can point to a memory allocated on {{sp_head::mem_root}}

This is very hard to follow and is bug prone.
This is the reason of some redundant code (a developer can never be sure which memory type {{m_chistics}} or its component point to, and has to do extra safety initialization/copying).

2. Having {{st_sp_chistics *m_chistics}} (as a pointer) rather than {{st_sp_chistics m_chistics}} (as a structure) is simply pointless, because:
- Any {{sp_head}} instance sooneer or later ends up in allocating an instance of {{st_sp_chistics}} on its own {{mem_root}} and assigning a pointer to it to {{m_chistics}}.
- Life cycle of {{m_chistics[0]}} ends up in exactly the same point of time when the owner {{sp_head}} itself is freed.
- Non of {{sp_head}} instances share the same {{sp_sp_chistics}}.


We'll do the following:
1. Change the data type of {{sp_head::m_chistics}} from a pointer to a structure

2. Move {{m_chistics}} to the {{private}} section, to disallow initialization of its components to arbitrary memory types. {{sp_head::set_info()}} will make sure that {{m_chistics.comment.str}} is set either to {{NULL}}, or to a memory allocated on {{sp_head::mem_root}}.

3. Introduce a few methods to access {{m_chistics}} components for read and write. Btw, this will also make the calling code shorter and easier to read.

4. Fix the parser to set {{sp_head::m_chistics}} immediately after scanning the {{sp_c_chistics}} rule. This is much easier to read (comparing to a postponed initialization, from {{Lex->sp_chistics}}).

5. Add initialization of {{sp_head::m_created}} and {{sp_head::m_modified}} at contructor time. This is much reasier to follow (compating to a postponed initialization, e.g. by {{set_info())}}).

6. Remove redundant code.
- {{Event_job_data::execute()}} does not have to do full {{set_info()}}. It's enough to set {{m_sql_mode}} only. Other {{m_chistics}} components were re-assigned to the default values again.
- In the new design, {{LEX::make_sp_head()}} does not have to do the hack with assigning {{sp->m_chistics}} to {{&Lex->sp_chistics}}.
- The {{ev_sql_stmt}} rule does not have to assing {{lex->sp_chistics.suid}} to {{SP_IS_SUID}}. This value is never used later.

7. Introduce a new class {{Sp_chistics}}, derived from {{st_sp_chistics}}, with automatic initialization. Reuse this class instead of {{st_sp_chistics}} when applicable, and remove tones of duplicate {{bzero's}}.

8. Fix {{show_create_sp()}} to accept {{st_sp_chistics}} as a reference, rather than as a pointer:
- to reduce changes in the code passing {{sp_head::m_chistics}}
- for consistency with {{db_load_routine()}}
- to be able to pass temporary on-stack {{Sp_chistics}} instances, e.g. {{show_create_sp(.., Sp_chistics(), ..)}}.
",,"Change sp_head::m_chistics from a pointer to a structure $end$ In order to reuse {{sp_head}} for new classes behind Oracle-style packages (MDEV-10591), we'll fix some {{st_sp_chistics}} related design problems in {{sp_head}}.

1. {{sp_head}} has a member {{st_sp_chistics *m_chistics}}, which can point in different moments of time to memory of different nature:
- {{m_chistics}} can be {{NULL}}
- {{m_chistics}} can point to {{Lex->sp_chistics}}
- {{m_chistics}} can point to a memory allocated on {{sp_head::mem_root}}
- {{m_chistics.comment.str}} can be {{NULL}}
- {{m_chistics.comment.str}} can point to the currently parsed query fragment (the query itself is alloced on {{thd->mem_root}}).
- {{m_chistics.comment.str}} can point to a memory allocated on {{sp_head::mem_root}}

This is very hard to follow and is bug prone.
This is the reason of some redundant code (a developer can never be sure which memory type {{m_chistics}} or its component point to, and has to do extra safety initialization/copying).

2. Having {{st_sp_chistics *m_chistics}} (as a pointer) rather than {{st_sp_chistics m_chistics}} (as a structure) is simply pointless, because:
- Any {{sp_head}} instance sooneer or later ends up in allocating an instance of {{st_sp_chistics}} on its own {{mem_root}} and assigning a pointer to it to {{m_chistics}}.
- Life cycle of {{m_chistics[0]}} ends up in exactly the same point of time when the owner {{sp_head}} itself is freed.
- Non of {{sp_head}} instances share the same {{sp_sp_chistics}}.


We'll do the following:
1. Change the data type of {{sp_head::m_chistics}} from a pointer to a structure

2. Move {{m_chistics}} to the {{private}} section, to disallow initialization of its components to arbitrary memory types. {{sp_head::set_info()}} will make sure that {{m_chistics.comment.str}} is set either to {{NULL}}, or to a memory allocated on {{sp_head::mem_root}}.

3. Introduce a few methods to access {{m_chistics}} components for read and write. Btw, this will also make the calling code shorter and easier to read.

4. Fix the parser to set {{sp_head::m_chistics}} immediately after scanning the {{sp_c_chistics}} rule. This is much easier to read (comparing to a postponed initialization, from {{Lex->sp_chistics}}).

5. Add initialization of {{sp_head::m_created}} and {{sp_head::m_modified}} at contructor time. This is much reasier to follow (compating to a postponed initialization, e.g. by {{set_info())}}).

6. Remove redundant code.
- {{Event_job_data::execute()}} does not have to do full {{set_info()}}. It's enough to set {{m_sql_mode}} only. Other {{m_chistics}} components were re-assigned to the default values again.
- In the new design, {{LEX::make_sp_head()}} does not have to do the hack with assigning {{sp->m_chistics}} to {{&Lex->sp_chistics}}.
- The {{ev_sql_stmt}} rule does not have to assing {{lex->sp_chistics.suid}} to {{SP_IS_SUID}}. This value is never used later.

7. Introduce a new class {{Sp_chistics}}, derived from {{st_sp_chistics}}, with automatic initialization. Reuse this class instead of {{st_sp_chistics}} when applicable, and remove tones of duplicate {{bzero's}}.

8. Fix {{show_create_sp()}} to accept {{st_sp_chistics}} as a reference, rather than as a pointer:
- to reduce changes in the code passing {{sp_head::m_chistics}}
- for consistency with {{db_load_routine()}}
- to be able to pass temporary on-stack {{Sp_chistics}} instances, e.g. {{show_create_sp(.., Sp_chistics(), ..)}}.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-07-12 07:54:40,Change sp_head::m_chistics from a pointer to a structure,"In order to reuse {{sp_head}} for new classes behind Oracle-style packages (MDEV-10591), we'll fix some {{st_sp_chistics}} related design problems in {{sp_head}}.

1. {{sp_head}} has a member {{st_sp_chistics *m_chistics}}, which can point in different moments of time to memory of different nature:
- {{m_chistics}} can be {{NULL}}
- {{m_chistics}} can point to {{Lex->sp_chistics}}
- {{m_chistics}} can point to a memory allocated on {{sp_head::mem_root}}
- {{m_chistics.comment.str}} can be {{NULL}}
- {{m_chistics.comment.str}} can point to the currently parsed query fragment (the query itself is alloced on {{thd->mem_root}}).
- {{m_chistics.comment.str}} can point to a memory allocated on {{sp_head::mem_root}}

This is very hard to follow and is bug prone.
This is the reason of some redundant code (a developer can never be sure which memory type {{m_chistics}} or its component point to, and has to do extra safety initialization/copying).

2. Having {{st_sp_chistics *m_chistics}} (as a pointer) rather than {{st_sp_chistics m_chistics}} (as a structure) is simply pointless, because:
- Any {{sp_head}} instance sooneer or later ends up in allocating an instance of {{st_sp_chistics}} on its own {{mem_root}} and assigning a pointer to it to {{m_chistics}}.
- Life cycle of {{m_chistics[0]}} ends up in exactly the same point of time when the owner {{sp_head}} itself is freed.
- Non of {{sp_head}} instances share the same {{sp_sp_chistics}}.


We'll do the following:
1. Change the data type of {{sp_head::m_chistics}} from a pointer to a structure

2. Move {{m_chistics}} to the {{private}} section, to disallow initialization of its components to arbitrary memory types. {{sp_head::set_info()}} will make sure that {{m_chistics.comment.str}} is set either to {{NULL}}, or to a memory allocated on {{sp_head::mem_root}}.

3. Introduce a few methods to access {{m_chistics}} components for read and write. Btw, this will also make the calling code shorter and easier to read.

4. Fix the parser to set {{sp_head::m_chistics}} immediately after scanning the {{sp_c_chistics}} rule. This is much easier to read (comparing to a postponed initialization, from {{Lex->sp_chistics}}).

5. Add initialization of {{sp_head::m_created}} and {{sp_head::m_modified}} at contructor time. This is much reasier to follow (compating to a postponed initialization, e.g. by {{set_info())}}).

6. Remove redundant code.
- {{Event_job_data::execute()}} does not have to do full {{set_info()}}. It's enough to set {{m_sql_mode}} only. Other {{m_chistics}} components were re-assigned to the default values again.
- In the new design, {{LEX::make_sp_head()}} does not have to do the hack with assigning {{sp->m_chistics}} to {{&Lex->sp_chistics}}.
- The {{ev_sql_stmt}} rule does not have to assing {{lex->sp_chistics.suid}} to {{SP_IS_SUID}}. This value is never used later.

7. Introduce a new class {{Sp_chistics}}, derived from {{st_sp_chistics}}, with automatic initialization. Reuse this class instead of {{st_sp_chistics}} when applicable, and remove tones of duplicate {{bzero's}}.

8. Fix {{show_create_sp()}} to accept {{st_sp_chistics}} as a reference, rather than as a pointer:
- to reduce changes in the code passing {{sp_head::m_chistics}}
- for consistency with {{db_load_routine()}}
- to be able to pass temporary on-stack {{Sp_chistics}} instances, e.g. {{show_create_sp(.., Sp_chistics(), ..)}}.
",,0,0,0,0,0.0,"Change sp_head::m_chistics from a pointer to a structure $end$ In order to reuse {{sp_head}} for new classes behind Oracle-style packages (MDEV-10591), we'll fix some {{st_sp_chistics}} related design problems in {{sp_head}}.

1. {{sp_head}} has a member {{st_sp_chistics *m_chistics}}, which can point in different moments of time to memory of different nature:
- {{m_chistics}} can be {{NULL}}
- {{m_chistics}} can point to {{Lex->sp_chistics}}
- {{m_chistics}} can point to a memory allocated on {{sp_head::mem_root}}
- {{m_chistics.comment.str}} can be {{NULL}}
- {{m_chistics.comment.str}} can point to the currently parsed query fragment (the query itself is alloced on {{thd->mem_root}}).
- {{m_chistics.comment.str}} can point to a memory allocated on {{sp_head::mem_root}}

This is very hard to follow and is bug prone.
This is the reason of some redundant code (a developer can never be sure which memory type {{m_chistics}} or its component point to, and has to do extra safety initialization/copying).

2. Having {{st_sp_chistics *m_chistics}} (as a pointer) rather than {{st_sp_chistics m_chistics}} (as a structure) is simply pointless, because:
- Any {{sp_head}} instance sooneer or later ends up in allocating an instance of {{st_sp_chistics}} on its own {{mem_root}} and assigning a pointer to it to {{m_chistics}}.
- Life cycle of {{m_chistics[0]}} ends up in exactly the same point of time when the owner {{sp_head}} itself is freed.
- Non of {{sp_head}} instances share the same {{sp_sp_chistics}}.


We'll do the following:
1. Change the data type of {{sp_head::m_chistics}} from a pointer to a structure

2. Move {{m_chistics}} to the {{private}} section, to disallow initialization of its components to arbitrary memory types. {{sp_head::set_info()}} will make sure that {{m_chistics.comment.str}} is set either to {{NULL}}, or to a memory allocated on {{sp_head::mem_root}}.

3. Introduce a few methods to access {{m_chistics}} components for read and write. Btw, this will also make the calling code shorter and easier to read.

4. Fix the parser to set {{sp_head::m_chistics}} immediately after scanning the {{sp_c_chistics}} rule. This is much easier to read (comparing to a postponed initialization, from {{Lex->sp_chistics}}).

5. Add initialization of {{sp_head::m_created}} and {{sp_head::m_modified}} at contructor time. This is much reasier to follow (compating to a postponed initialization, e.g. by {{set_info())}}).

6. Remove redundant code.
- {{Event_job_data::execute()}} does not have to do full {{set_info()}}. It's enough to set {{m_sql_mode}} only. Other {{m_chistics}} components were re-assigned to the default values again.
- In the new design, {{LEX::make_sp_head()}} does not have to do the hack with assigning {{sp->m_chistics}} to {{&Lex->sp_chistics}}.
- The {{ev_sql_stmt}} rule does not have to assing {{lex->sp_chistics.suid}} to {{SP_IS_SUID}}. This value is never used later.

7. Introduce a new class {{Sp_chistics}}, derived from {{st_sp_chistics}}, with automatic initialization. Reuse this class instead of {{st_sp_chistics}} when applicable, and remove tones of duplicate {{bzero's}}.

8. Fix {{show_create_sp()}} to accept {{st_sp_chistics}} as a reference, rather than as a pointer:
- to reduce changes in the code passing {{sp_head::m_chistics}}
- for consistency with {{db_load_routine()}}
- to be able to pass temporary on-stack {{Sp_chistics}} instances, e.g. {{show_create_sp(.., Sp_chistics(), ..)}}.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,69,41,0.594203,28,0.405797,26,0.376812,25,0.362319,25,0.362319
667,MDEV-13302,Technical task,MDEV,2017-07-12 18:49:44,,0,Avoid using LEX::spname during CREATE PROCEDURE and CREATE FUNCTION,"The code in {{sp_create_routine()}} uses two ways to access the routine name:
- Via {{sp_head}}: {{sp->m_db}} and {{sp->m_name}}
- Via {{LEX}}: {{lex->spname}}

This is a fragment from {{sp_create_routine()}}:
{code:cpp}
  if (!(table= open_proc_table_for_update(thd)))
  {
    my_error(ER_SP_STORE_FAILED, MYF(0), SP_TYPE_STRING(type),sp->m_name.str)
    goto done;
  }
  else
  {   
    /* Checking if the routine already exists */
    if (db_find_routine_aux(thd, type, lex->spname, table) == SP_OK)
    {
{code}


The function {{mysql_create_routine()}} uses the same style:
{code:cpp}
  if (check_db_name((LEX_STRING*) &lex->sphead->m_db))  
  {
  ...
    if (lex->create_info.or_replace())
  {
    if (check_routine_access(thd, ALTER_PROC_ACL, lex->spname->m_db.str,
  ...
{code}


In the above code, both {{LEX::sp_name}} and {{sp_sphead}} point to copies of the same qualified routine name. Copying is done in sql_yacc.yy:
{code:cpp}
             if (!Lex->make_sp_head_no_recursive(thd, $1, $2,
                                                 TYPE_ENUM_FUNCTION))
               MYSQL_YYABORT;
             Lex->spname= $2;
{code}


We're going to reuse {{sp_head}} to store Oracle-style packages soon (see MDEV-10591).
To avoid duplicating of this redundancy, we should get rid of it before implementing packages.

Under terms of this task we will:
1. Fix the code responsible for {{CREATE PROCEDURE}} and {{CREATE FUNCTION}} not to use {{lex->spname}}, and to use {{sphead}} instead. This includes functions:
- {{mysql_create_routine()}}
- {{sp_create_routine()}}

2. Remove copying of the routine name to {{LEX::spname}}. The latter should stay {{NULL}} during {{CREATE PROCEDURE}} and {{CREATE FUNCTION}}.

",,"Avoid using LEX::spname during CREATE PROCEDURE and CREATE FUNCTION $end$ The code in {{sp_create_routine()}} uses two ways to access the routine name:
- Via {{sp_head}}: {{sp->m_db}} and {{sp->m_name}}
- Via {{LEX}}: {{lex->spname}}

This is a fragment from {{sp_create_routine()}}:
{code:cpp}
  if (!(table= open_proc_table_for_update(thd)))
  {
    my_error(ER_SP_STORE_FAILED, MYF(0), SP_TYPE_STRING(type),sp->m_name.str)
    goto done;
  }
  else
  {   
    /* Checking if the routine already exists */
    if (db_find_routine_aux(thd, type, lex->spname, table) == SP_OK)
    {
{code}


The function {{mysql_create_routine()}} uses the same style:
{code:cpp}
  if (check_db_name((LEX_STRING*) &lex->sphead->m_db))  
  {
  ...
    if (lex->create_info.or_replace())
  {
    if (check_routine_access(thd, ALTER_PROC_ACL, lex->spname->m_db.str,
  ...
{code}


In the above code, both {{LEX::sp_name}} and {{sp_sphead}} point to copies of the same qualified routine name. Copying is done in sql_yacc.yy:
{code:cpp}
             if (!Lex->make_sp_head_no_recursive(thd, $1, $2,
                                                 TYPE_ENUM_FUNCTION))
               MYSQL_YYABORT;
             Lex->spname= $2;
{code}


We're going to reuse {{sp_head}} to store Oracle-style packages soon (see MDEV-10591).
To avoid duplicating of this redundancy, we should get rid of it before implementing packages.

Under terms of this task we will:
1. Fix the code responsible for {{CREATE PROCEDURE}} and {{CREATE FUNCTION}} not to use {{lex->spname}}, and to use {{sphead}} instead. This includes functions:
- {{mysql_create_routine()}}
- {{sp_create_routine()}}

2. Remove copying of the routine name to {{LEX::spname}}. The latter should stay {{NULL}} during {{CREATE PROCEDURE}} and {{CREATE FUNCTION}}.

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-07-12 18:49:44,Avoid using LEX::spname during CREATE PROCEDURE and CREATE FUNCTION,"The code in {{sp_create_routine()}} uses two ways to access the routine name:
- Via {{sp_head}}: {{sp->m_db}} and {{sp->m_name}}
- Via {{LEX}}: {{lex->spname}}

This is a fragment from {{sp_create_routine()}}:
{code:cpp}
  if (!(table= open_proc_table_for_update(thd)))
  {
    my_error(ER_SP_STORE_FAILED, MYF(0), SP_TYPE_STRING(type),sp->m_name.str)
    goto done;
  }
  else
  {   
    /* Checking if the routine already exists */
    if (db_find_routine_aux(thd, type, lex->spname, table) == SP_OK)
    {
{code}


The function {{mysql_create_routine()}} uses the same style:
{code:cpp}
  if (check_db_name((LEX_STRING*) &lex->sphead->m_db))  
  {
  ...
    if (lex->create_info.or_replace())
  {
    if (check_routine_access(thd, ALTER_PROC_ACL, lex->spname->m_db.str,
  ...
{code}


In the above code, both {{LEX::sp_name}} and {{sp_sphead}} point to copies of the same qualified routine name. Copying is done in sql_yacc.yy:
{code:cpp}
             if (!Lex->make_sp_head_no_recursive(thd, $1, $2,
                                                 TYPE_ENUM_FUNCTION))
               MYSQL_YYABORT;
             Lex->spname= $2;
{code}


We're going to reuse {{sp_head}} to store Oracle-style packages soon (see MDEV-10591).
To avoid duplicating of this redundancy, we should get rid of it before implementing packages.

Under terms of this task we will:
1. Fix the code responsible for {{CREATE PROCEDURE}} and {{CREATE FUNCTION}} not to use {{lex->spname}}, and to use {{sphead}} instead. This includes functions:
- {{mysql_create_routine()}}
- {{sp_create_routine()}}

2. Remove copying of the routine name to {{LEX::spname}}. The latter should stay {{NULL}} during {{CREATE PROCEDURE}} and {{CREATE FUNCTION}}.

",,0,0,0,0,0.0,"Avoid using LEX::spname during CREATE PROCEDURE and CREATE FUNCTION $end$ The code in {{sp_create_routine()}} uses two ways to access the routine name:
- Via {{sp_head}}: {{sp->m_db}} and {{sp->m_name}}
- Via {{LEX}}: {{lex->spname}}

This is a fragment from {{sp_create_routine()}}:
{code:cpp}
  if (!(table= open_proc_table_for_update(thd)))
  {
    my_error(ER_SP_STORE_FAILED, MYF(0), SP_TYPE_STRING(type),sp->m_name.str)
    goto done;
  }
  else
  {   
    /* Checking if the routine already exists */
    if (db_find_routine_aux(thd, type, lex->spname, table) == SP_OK)
    {
{code}


The function {{mysql_create_routine()}} uses the same style:
{code:cpp}
  if (check_db_name((LEX_STRING*) &lex->sphead->m_db))  
  {
  ...
    if (lex->create_info.or_replace())
  {
    if (check_routine_access(thd, ALTER_PROC_ACL, lex->spname->m_db.str,
  ...
{code}


In the above code, both {{LEX::sp_name}} and {{sp_sphead}} point to copies of the same qualified routine name. Copying is done in sql_yacc.yy:
{code:cpp}
             if (!Lex->make_sp_head_no_recursive(thd, $1, $2,
                                                 TYPE_ENUM_FUNCTION))
               MYSQL_YYABORT;
             Lex->spname= $2;
{code}


We're going to reuse {{sp_head}} to store Oracle-style packages soon (see MDEV-10591).
To avoid duplicating of this redundancy, we should get rid of it before implementing packages.

Under terms of this task we will:
1. Fix the code responsible for {{CREATE PROCEDURE}} and {{CREATE FUNCTION}} not to use {{lex->spname}}, and to use {{sphead}} instead. This includes functions:
- {{mysql_create_routine()}}
- {{sp_create_routine()}}

2. Remove copying of the routine name to {{LEX::spname}}. The latter should stay {{NULL}} during {{CREATE PROCEDURE}} and {{CREATE FUNCTION}}.

 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,70,41,0.585714,28,0.4,26,0.371429,25,0.357143,25,0.357143
668,MDEV-13342,Technical task,MDEV,2017-07-18 14:17:09,,0,Testing for MDEV-11371 (Big column compressed),"Development tasks: MDEV-11371, MDEV-11381
Development tree (as of July 18th): bb-10.3-svoj
Tentative patch (as of July 18th): https://github.com/MariaDB/server/commit/79e055f407d34f195e3fde20401f39033dfce51d

Request before the second round of review (received by email):

On Thu, Jun 29, 2017 at 12:22:15PM +0400, Sergey Vojtovich wrote: 
{quote}
Elena: I added decent test for this feature, but it would be great if you could
       extend it, especially replication testing is missing. 
{quote}

_Note: Since there is no documentation for the feature, need to explore first._
_Note: Make sure it's documented before the release._
_Note: Have 'innodb' removed from MDEV-11371 subject._

Initial exploration:

- server restart (/)
- base for virt col (/)
- storage for dyncol (/)
- 2nd part of index (/)
- change column, alter column (/)
- views (/)
- analyze, optimize, check (/)
- create table .. select with unsupported engine (?) haven't found an unsupported engine yet
- partitions, partition by compressed col (x)
- timestamps, sets, enums (N/A)
- zerofill, unsigned (N/A)
- null/not null (/)
- default (/)
- charsets (/)
- compressed column + row_format=compressed (/)
- + table compressed (/)
- + encryption (/)
- aria, tokudb, rocksdb, connect, heap, federated (/)  
- binary log, replication (x)
- mysqldump (/)

--
(/) here does not mean ""tested"", it just means it appears to be supported and does not fail right away

Extra MTR tests needed:
- with partitions
** PARTITION BY KEY + SELECT .. WHERE col = 'something' etc. -- crashes
- with binlog in row format (easy to check with replication)
** some strange {{'\x00foo'}} shows up, length increases",,"Testing for MDEV-11371 (Big column compressed) $end$ Development tasks: MDEV-11371, MDEV-11381
Development tree (as of July 18th): bb-10.3-svoj
Tentative patch (as of July 18th): https://github.com/MariaDB/server/commit/79e055f407d34f195e3fde20401f39033dfce51d

Request before the second round of review (received by email):

On Thu, Jun 29, 2017 at 12:22:15PM +0400, Sergey Vojtovich wrote: 
{quote}
Elena: I added decent test for this feature, but it would be great if you could
       extend it, especially replication testing is missing. 
{quote}

_Note: Since there is no documentation for the feature, need to explore first._
_Note: Make sure it's documented before the release._
_Note: Have 'innodb' removed from MDEV-11371 subject._

Initial exploration:

- server restart (/)
- base for virt col (/)
- storage for dyncol (/)
- 2nd part of index (/)
- change column, alter column (/)
- views (/)
- analyze, optimize, check (/)
- create table .. select with unsupported engine (?) haven't found an unsupported engine yet
- partitions, partition by compressed col (x)
- timestamps, sets, enums (N/A)
- zerofill, unsigned (N/A)
- null/not null (/)
- default (/)
- charsets (/)
- compressed column + row_format=compressed (/)
- + table compressed (/)
- + encryption (/)
- aria, tokudb, rocksdb, connect, heap, federated (/)  
- binary log, replication (x)
- mysqldump (/)

--
(/) here does not mean ""tested"", it just means it appears to be supported and does not fail right away

Extra MTR tests needed:
- with partitions
** PARTITION BY KEY + SELECT .. WHERE col = 'something' etc. -- crashes
- with binlog in row format (easy to check with replication)
** some strange {{'\x00foo'}} shows up, length increases $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,14,,5,25,5,1,0,6,0,,0,850,25,0,0,2017-07-18 14:17:09,Testing for MDEV-11371 (Big column compressed),"Development tasks: MDEV-11371, MDEV-11381
Development tree (as of July 18th): bb-10.3-svoj
Tentative patch (as of July 18th): https://github.com/MariaDB/server/commit/79e055f407d34f195e3fde20401f39033dfce51d

Request before the second round of review (received by email):

On Thu, Jun 29, 2017 at 12:22:15PM +0400, Sergey Vojtovich wrote: 
{quote}
Elena: I added decent test for this feature, but it would be great if you could
       extend it, especially replication testing is missing. 
{quote}

_Note: Since there is no documentation for the feature, need to explore first._
_Note: Make sure it's documented before the release._

Initial exploration plan:

- server restart
- base for virt col
- storage for dyncol
- 2nd part of index
- change column, alter column
- views
- analyze, optimize, check
- create table .. select with unsupported engine
- partitions, partition by compressed col
- timestamps, sets, enums
- zerofill, unsigned, null/not null
- default
- charsets
- compressed column + row_format=compressed
- + table compressed
- + encryption
- aria, tokudb, rocksdb, connect, heap, federated, 
- binary log
- mysqldump
- replication

Questions:
- why MDEV-11371 says InnoDB, but the test has MyISAM and other engines?",,0,6,0,119,0.515625,"Testing for MDEV-11371 (Big column compressed) $end$ Development tasks: MDEV-11371, MDEV-11381
Development tree (as of July 18th): bb-10.3-svoj
Tentative patch (as of July 18th): https://github.com/MariaDB/server/commit/79e055f407d34f195e3fde20401f39033dfce51d

Request before the second round of review (received by email):

On Thu, Jun 29, 2017 at 12:22:15PM +0400, Sergey Vojtovich wrote: 
{quote}
Elena: I added decent test for this feature, but it would be great if you could
       extend it, especially replication testing is missing. 
{quote}

_Note: Since there is no documentation for the feature, need to explore first._
_Note: Make sure it's documented before the release._

Initial exploration plan:

- server restart
- base for virt col
- storage for dyncol
- 2nd part of index
- change column, alter column
- views
- analyze, optimize, check
- create table .. select with unsupported engine
- partitions, partition by compressed col
- timestamps, sets, enums
- zerofill, unsigned, null/not null
- default
- charsets
- compressed column + row_format=compressed
- + table compressed
- + encryption
- aria, tokudb, rocksdb, connect, heap, federated, 
- binary log
- mysqldump
- replication

Questions:
- why MDEV-11371 says InnoDB, but the test has MyISAM and other engines? $acceptance criteria:$",6,1,1,1,1,1,1,0.0,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
669,MDEV-13369,Task,MDEV,2017-07-21 20:27:29,,0,Optimization for equi-joins of derived tables with GROUP BY,"Consider the following tables
{code:sql}
create table t1 (a int);
insert into t1 values (5), (1), (2), (9), (7), (2), (7);
create table t2 (a int, b int, index idx(a));
insert into t2 values (7,10), (1,20), (2,23), (7,18), (1,30), (4,71), (3,15), (7,82);
{code}
and the query
{code:sql}
select t1.a,t.max,t.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 group by t2.a) t on t1.a=t.a;
{code}
The following re-writing could be applied to this query:
{code:sql}
=>
select t1.a,tl.max,tl.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 where  t1.a=t2.a) tl on 1=1;
{code}
The result of this re-writing is a query with so-called lateral derived table. This query requires refilling of the temporary table created the derived table for every new value t1.a. As the size of the derived table tl  usually is much smaller than the size of the derived table t this transformation will be always beneficial. Especially this transformation will be beneficial when the join operation that joins derived table is used in a multi-way join query where only a few records of t1 are selected.
Unfortunately now we do not support lateral derived tables on SQL level. This task will allow to use them internally for this transformation.

The difference of this task from the task MDEV-13225 that in this task the execution plan chosen for the original derived table will be just transformed into the one that uses a lateral derived table. As a result it might happen that the optimizer chooses not the best join order for the main query. The cost of using lateral derived table will be taken into account by the optimizer later.",,"Optimization for equi-joins of derived tables with GROUP BY $end$ Consider the following tables
{code:sql}
create table t1 (a int);
insert into t1 values (5), (1), (2), (9), (7), (2), (7);
create table t2 (a int, b int, index idx(a));
insert into t2 values (7,10), (1,20), (2,23), (7,18), (1,30), (4,71), (3,15), (7,82);
{code}
and the query
{code:sql}
select t1.a,t.max,t.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 group by t2.a) t on t1.a=t.a;
{code}
The following re-writing could be applied to this query:
{code:sql}
=>
select t1.a,tl.max,tl.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 where  t1.a=t2.a) tl on 1=1;
{code}
The result of this re-writing is a query with so-called lateral derived table. This query requires refilling of the temporary table created the derived table for every new value t1.a. As the size of the derived table tl  usually is much smaller than the size of the derived table t this transformation will be always beneficial. Especially this transformation will be beneficial when the join operation that joins derived table is used in a multi-way join query where only a few records of t1 are selected.
Unfortunately now we do not support lateral derived tables on SQL level. This task will allow to use them internally for this transformation.

The difference of this task from the task MDEV-13225 that in this task the execution plan chosen for the original derived table will be just transformed into the one that uses a lateral derived table. As a result it might happen that the optimizer chooses not the best join order for the main query. The cost of using lateral derived table will be taken into account by the optimizer later. $acceptance criteria:$",,Igor Babaev,Igor Babaev,Major,16,,0,4,3,1,0,4,0,,0,850,2,4,0,2017-11-16 05:41:55,Optimization for equi-joins of derived tables with GROUP BY,"Consider the following tables
{code:sql}
create table t1 (a int);
insert into t1 values (5), (1), (2), (9), (7), (2), (7);
create table t2 (a int, b int, index idx(a));
insert into t2 values (7,10), (1,20), (2,23), (7,18), (1,30), (4,71), (3,15), (7,82);
{code}
and the query
{code:sql}
select t1.a,t.max,t.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 group by t2.a) t on t1.a=t.a;
{code}
The following re-writing could be applied to this query:
{code:sql}
=>
select t1.a,tl.max,tl.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 where  t1.a=t2.a) tl on 1=1;
{code}
The result of this re-writing is a query with so-called lateral derived table. This query requires refilling of the temporary table created the derived table for every new value t1.a. As the size of the derived table tl  usually is much smaller than the size of the derived table t this transformation will be always beneficial. Especially this transformation will be beneficial when the join operation that joins derived table is used in a multi-way join query where only a few records of t1 are selected.
Unfortunately now we do not support lateral derived tables on SQL level. This task will allow to use them internally for this transformation.

The difference of this task from the task MDEV-13225 that in this task the execution plan chosen for the original derived table will be just transformed into the one that uses a lateral derived table. As a result it might happen that the optimizer chooses not the best join order for the main query. The cost of using lateral derived table will be taken into account by the optimizer later.",,0,0,0,0,0.0,"Optimization for equi-joins of derived tables with GROUP BY $end$ Consider the following tables
{code:sql}
create table t1 (a int);
insert into t1 values (5), (1), (2), (9), (7), (2), (7);
create table t2 (a int, b int, index idx(a));
insert into t2 values (7,10), (1,20), (2,23), (7,18), (1,30), (4,71), (3,15), (7,82);
{code}
and the query
{code:sql}
select t1.a,t.max,t.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 group by t2.a) t on t1.a=t.a;
{code}
The following re-writing could be applied to this query:
{code:sql}
=>
select t1.a,tl.max,tl.min from t1 left join (select a, max(t2.b) max, min(t2.b) min from t2 where  t1.a=t2.a) tl on 1=1;
{code}
The result of this re-writing is a query with so-called lateral derived table. This query requires refilling of the temporary table created the derived table for every new value t1.a. As the size of the derived table tl  usually is much smaller than the size of the derived table t this transformation will be always beneficial. Especially this transformation will be beneficial when the join operation that joins derived table is used in a multi-way join query where only a few records of t1 are selected.
Unfortunately now we do not support lateral derived tables on SQL level. This task will allow to use them internally for this transformation.

The difference of this task from the task MDEV-13225 that in this task the execution plan chosen for the original derived table will be just transformed into the one that uses a lateral derived table. As a result it might happen that the optimizer chooses not the best join order for the main query. The cost of using lateral derived table will be taken into account by the optimizer later. $acceptance criteria:$",0,0,0,0,0,0,0,2817.23,4,2,0.5,2,0.5,0,0.0,0,0.0,0,0.0
670,MDEV-13414,Technical task,MDEV,2017-07-31 12:46:18,,0,Fix the SP code to avoid excessive use of strlen,"Functions sp_load_for_information_schema() and show_create_sp() do a lot of strlen calls, e.g.:

{code:cpp}
  if (parse_user(definer, strlen(definer),
                 definer_user_name.str, &definer_user_name.length,
                 definer_host_name.str, &definer_host_name.length) &&
      definer_user_name.length && !definer_host_name.length)
{code}


{code:cpp}
  if (!show_create_sp(thd, &defstr,
                     type,
                     NULL, 0,
                     name->m_name.str, name->m_name.length,
                     params, strlen(params),
                     returns, strlen(returns),
                     body, strlen(body),
                     &chistics, definer_user_name, definer_host_name,
                     sql_mode))
{code}

{code:cpp}
  if (!show_create_sp(thd, &defstr, type,
                     sp_db_str.str, sp_db_str.length,
                     sp_name_obj.m_name.str, sp_name_obj.m_name.length,
                     params, strlen(params),
                     returns, strlen(returns), 
                     sp_body, strlen(sp_body),
                     &sp_chistics, &definer_user, &definer_host, sql_mode))
{code}


This code is going to be shared by Oracle-style packages soon (MDEV-10591). Before adding packages, it would be nice to get rid of {{strlen}} calls.

Under terms if this task we'll change API for these functions to accept {{LEX_CSTRING}} instead of {{const char *}} and fix the caller code accordingly.


",,"Fix the SP code to avoid excessive use of strlen $end$ Functions sp_load_for_information_schema() and show_create_sp() do a lot of strlen calls, e.g.:

{code:cpp}
  if (parse_user(definer, strlen(definer),
                 definer_user_name.str, &definer_user_name.length,
                 definer_host_name.str, &definer_host_name.length) &&
      definer_user_name.length && !definer_host_name.length)
{code}


{code:cpp}
  if (!show_create_sp(thd, &defstr,
                     type,
                     NULL, 0,
                     name->m_name.str, name->m_name.length,
                     params, strlen(params),
                     returns, strlen(returns),
                     body, strlen(body),
                     &chistics, definer_user_name, definer_host_name,
                     sql_mode))
{code}

{code:cpp}
  if (!show_create_sp(thd, &defstr, type,
                     sp_db_str.str, sp_db_str.length,
                     sp_name_obj.m_name.str, sp_name_obj.m_name.length,
                     params, strlen(params),
                     returns, strlen(returns), 
                     sp_body, strlen(sp_body),
                     &sp_chistics, &definer_user, &definer_host, sql_mode))
{code}


This code is going to be shared by Oracle-style packages soon (MDEV-10591). Before adding packages, it would be nice to get rid of {{strlen}} calls.

Under terms if this task we'll change API for these functions to accept {{LEX_CSTRING}} instead of {{const char *}} and fix the caller code accordingly.


 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,0,0,1,5,0,0,0,,0,850,0,0,0,2017-07-31 12:46:18,Fix the SP code to avoid excessive use of strlen,"Functions sp_load_for_information_schema() and show_create_sp() do a lot of strlen calls, e.g.:

{code:cpp}
  if (parse_user(definer, strlen(definer),
                 definer_user_name.str, &definer_user_name.length,
                 definer_host_name.str, &definer_host_name.length) &&
      definer_user_name.length && !definer_host_name.length)
{code}


{code:cpp}
  if (!show_create_sp(thd, &defstr,
                     type,
                     NULL, 0,
                     name->m_name.str, name->m_name.length,
                     params, strlen(params),
                     returns, strlen(returns),
                     body, strlen(body),
                     &chistics, definer_user_name, definer_host_name,
                     sql_mode))
{code}

{code:cpp}
  if (!show_create_sp(thd, &defstr, type,
                     sp_db_str.str, sp_db_str.length,
                     sp_name_obj.m_name.str, sp_name_obj.m_name.length,
                     params, strlen(params),
                     returns, strlen(returns), 
                     sp_body, strlen(sp_body),
                     &sp_chistics, &definer_user, &definer_host, sql_mode))
{code}


This code is going to be shared by Oracle-style packages soon (MDEV-10591). Before adding packages, it would be nice to get rid of {{strlen}} calls.

Under terms if this task we'll change API for these functions to accept {{LEX_CSTRING}} instead of {{const char *}} and fix the caller code accordingly.


",,0,0,0,0,0.0,"Fix the SP code to avoid excessive use of strlen $end$ Functions sp_load_for_information_schema() and show_create_sp() do a lot of strlen calls, e.g.:

{code:cpp}
  if (parse_user(definer, strlen(definer),
                 definer_user_name.str, &definer_user_name.length,
                 definer_host_name.str, &definer_host_name.length) &&
      definer_user_name.length && !definer_host_name.length)
{code}


{code:cpp}
  if (!show_create_sp(thd, &defstr,
                     type,
                     NULL, 0,
                     name->m_name.str, name->m_name.length,
                     params, strlen(params),
                     returns, strlen(returns),
                     body, strlen(body),
                     &chistics, definer_user_name, definer_host_name,
                     sql_mode))
{code}

{code:cpp}
  if (!show_create_sp(thd, &defstr, type,
                     sp_db_str.str, sp_db_str.length,
                     sp_name_obj.m_name.str, sp_name_obj.m_name.length,
                     params, strlen(params),
                     returns, strlen(returns), 
                     sp_body, strlen(sp_body),
                     &sp_chistics, &definer_user, &definer_host, sql_mode))
{code}


This code is going to be shared by Oracle-style packages soon (MDEV-10591). Before adding packages, it would be nice to get rid of {{strlen}} calls.

Under terms if this task we'll change API for these functions to accept {{LEX_CSTRING}} instead of {{const char *}} and fix the caller code accordingly.


 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,71,41,0.577465,28,0.394366,26,0.366197,25,0.352113,25,0.352113
671,MDEV-13415,Technical task,MDEV,2017-07-31 13:06:22,,0,Wrap the code in sp.cc into a class Sp_handler,"The code in {{sp.sp}}, {{sp_head.cc}}, {{sql_acl.cc}}, {{sql_lex.cc}}, {{sql_show.cc}}, {{sql_yacc*.yy}} uses a lot of conditions on the routine type, e.g.
{code:cpp}
  if (type == TYPE_ENUM_FUNCTION)
    do_func;
  else
    do_proc;
{code}

Oracle style packages (MDEV-10591) will introduce new SP object types: {{TYPE_ENUM_PACKAGE_SPEC}} and {{TYPE_ENUM_PACKAGE_BODY}}.
The conditional type-related code will get more complex.

Under terms of this code we'll create a new class {{Sp_handler}} with a number of virtual methods, to simplify handling of SP objects of different types and therefore simplify adding packages.

We'll add the be the top level abstract class {{Sp_handler}} with this approximate set of virtual methods:

{code:cpp}
class Sp_handler
{
public:
  virtual const char *show_create_routine_col1_caption() const;
  virtual const char *show_create_routine_col3_caption() const;
  virtual stored_procedure_type type() const;
  virtual LEX_CSTRING type_lex_cstring() const;
  virtual LEX_CSTRING empty_body_lex_cstring() const;
  virtual MDL_key::enum_mdl_namespace get_mdl_type() const;
  virtual sp_cache **get_cache(THD *) const;  
  virtual HASH *priv_hash() const;
  virtual ulong recursion_depth(THD *thd) const;
  virtual void recursion_level_error(THD *thd, const sp_head *sp) const;
};
{code}

and will add instantiable sub-classes for certain SP object types:
{noformat}
Sp_handler
  Sp_handler_procedure
  Sp_handler_trigger
  Sp_handler_function
{noformat}


MDEV-10591 will later add {{Sp_handler_package_spec}} and {{Sp_handler_package_body}} sub-classes.
",,"Wrap the code in sp.cc into a class Sp_handler $end$ The code in {{sp.sp}}, {{sp_head.cc}}, {{sql_acl.cc}}, {{sql_lex.cc}}, {{sql_show.cc}}, {{sql_yacc*.yy}} uses a lot of conditions on the routine type, e.g.
{code:cpp}
  if (type == TYPE_ENUM_FUNCTION)
    do_func;
  else
    do_proc;
{code}

Oracle style packages (MDEV-10591) will introduce new SP object types: {{TYPE_ENUM_PACKAGE_SPEC}} and {{TYPE_ENUM_PACKAGE_BODY}}.
The conditional type-related code will get more complex.

Under terms of this code we'll create a new class {{Sp_handler}} with a number of virtual methods, to simplify handling of SP objects of different types and therefore simplify adding packages.

We'll add the be the top level abstract class {{Sp_handler}} with this approximate set of virtual methods:

{code:cpp}
class Sp_handler
{
public:
  virtual const char *show_create_routine_col1_caption() const;
  virtual const char *show_create_routine_col3_caption() const;
  virtual stored_procedure_type type() const;
  virtual LEX_CSTRING type_lex_cstring() const;
  virtual LEX_CSTRING empty_body_lex_cstring() const;
  virtual MDL_key::enum_mdl_namespace get_mdl_type() const;
  virtual sp_cache **get_cache(THD *) const;  
  virtual HASH *priv_hash() const;
  virtual ulong recursion_depth(THD *thd) const;
  virtual void recursion_level_error(THD *thd, const sp_head *sp) const;
};
{code}

and will add instantiable sub-classes for certain SP object types:
{noformat}
Sp_handler
  Sp_handler_procedure
  Sp_handler_trigger
  Sp_handler_function
{noformat}


MDEV-10591 will later add {{Sp_handler_package_spec}} and {{Sp_handler_package_body}} sub-classes.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,0,0,1,5,0,2,0,,0,850,0,0,0,2017-07-31 13:06:22,Wrap the code in sp.cc into a class Sp_handler,"
The code in {{sp.sp}}, {{sp_head.cc}}, {{sql_acl.cc}}, {{sql_lex.cc}}, {{sql_show.cc}}, {{sql_yacc*.yy}} uses a lot of conditions on the routine type, e.g.
{code:cpp}
  if (type == TYPE_ENUM_FUNCTION)
    do_func;
  else
    do_proc;
{code}

Oracle style packages (MDEV-10591) will introduce new SP object types: {{TYPE_ENUM_PACKAGE_SPEC}} and {{TYPE_ENUM_PACKAGE_BODY}}.
The conditional type-related code will get more complex.

Under terms of this code we'll create a new class {{Sp_handler}} with a number of virtual methods, to simplify handling of SP objects of different types and therefore simplify adding packages.

We'll add the be the top level abstract class {{Sp_handler}} with this approximate set of virtual methods:

{code:cpp}
class Sp_handler
{
public:
  virtual const char *show_create_routine_col1_caption() const;
  virtual const char *show_create_routine_col3_caption() const;
  virtual stored_procedure_type type() const;
  virtual LEX_CSTRING type_lex_cstring() const;
  virtual LEX_CSTRING empty_body_lex_cstring() const;
  virtual MDL_key::enum_mdl_namespace get_mdl_type() const;
  virtual sp_cache **get_cache(THD *) const;  
  virtual HASH *priv_hash() const;
  virtual ulong recursion_depth(THD *thd) const;
  virtual void recursion_level_error(THD *thd, const sp_head *sp) const;
  virtual ulong recursion_depth(THD *thd) const;
  virtual void recursion_level_error(THD *thd, const sp_head *sp) const;
};
{code}

and will add instantiable sub-classes for certain SP object types:
{noformat}
Sp_handler
  Sp_handler_procedure
  Sp_handler_trigger
  Sp_handler_function
{noformat}


MDEV-10591 will later add {{Sp_handler_package_spec}} and {{Sp_handler_package_body}} sub-classes.
",,0,2,0,13,0.0646766,"Wrap the code in sp.cc into a class Sp_handler $end$ 
The code in {{sp.sp}}, {{sp_head.cc}}, {{sql_acl.cc}}, {{sql_lex.cc}}, {{sql_show.cc}}, {{sql_yacc*.yy}} uses a lot of conditions on the routine type, e.g.
{code:cpp}
  if (type == TYPE_ENUM_FUNCTION)
    do_func;
  else
    do_proc;
{code}

Oracle style packages (MDEV-10591) will introduce new SP object types: {{TYPE_ENUM_PACKAGE_SPEC}} and {{TYPE_ENUM_PACKAGE_BODY}}.
The conditional type-related code will get more complex.

Under terms of this code we'll create a new class {{Sp_handler}} with a number of virtual methods, to simplify handling of SP objects of different types and therefore simplify adding packages.

We'll add the be the top level abstract class {{Sp_handler}} with this approximate set of virtual methods:

{code:cpp}
class Sp_handler
{
public:
  virtual const char *show_create_routine_col1_caption() const;
  virtual const char *show_create_routine_col3_caption() const;
  virtual stored_procedure_type type() const;
  virtual LEX_CSTRING type_lex_cstring() const;
  virtual LEX_CSTRING empty_body_lex_cstring() const;
  virtual MDL_key::enum_mdl_namespace get_mdl_type() const;
  virtual sp_cache **get_cache(THD *) const;  
  virtual HASH *priv_hash() const;
  virtual ulong recursion_depth(THD *thd) const;
  virtual void recursion_level_error(THD *thd, const sp_head *sp) const;
  virtual ulong recursion_depth(THD *thd) const;
  virtual void recursion_level_error(THD *thd, const sp_head *sp) const;
};
{code}

and will add instantiable sub-classes for certain SP object types:
{noformat}
Sp_handler
  Sp_handler_procedure
  Sp_handler_trigger
  Sp_handler_function
{noformat}


MDEV-10591 will later add {{Sp_handler_package_spec}} and {{Sp_handler_package_body}} sub-classes.
 $acceptance criteria:$",2,1,1,1,0,0,1,0.0,72,41,0.569444,28,0.388889,26,0.361111,25,0.347222,25,0.347222
672,MDEV-13419,Technical task,MDEV,2017-08-01 07:51:35,,0,Cleanup for Sp_handler::show_create_sp,"In order to implement packages easier, we'll do the following changes in {{Sp_handler::show_create_sp()}}:
- Reverse the return value to the standard MariaDB notation: false on success, true on error
- Add a new parameter {{const DDL_options_st ddl_options}} and use it to process the {{OR REPLACE}} and {{IF NOT EXISTS}} clauses, instead of accessing {{thd->lex->create_info}}.
",,"Cleanup for Sp_handler::show_create_sp $end$ In order to implement packages easier, we'll do the following changes in {{Sp_handler::show_create_sp()}}:
- Reverse the return value to the standard MariaDB notation: false on success, true on error
- Add a new parameter {{const DDL_options_st ddl_options}} and use it to process the {{OR REPLACE}} and {{IF NOT EXISTS}} clauses, instead of accessing {{thd->lex->create_info}}.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,5,,0,0,1,5,0,0,0,,0,850,0,0,0,2017-08-01 07:51:35,Cleanup for Sp_handler::show_create_sp,"In order to implement packages easier, we'll do the following changes in {{Sp_handler::show_create_sp()}}:
- Reverse the return value to the standard MariaDB notation: false on success, true on error
- Add a new parameter {{const DDL_options_st ddl_options}} and use it to process the {{OR REPLACE}} and {{IF NOT EXISTS}} clauses, instead of accessing {{thd->lex->create_info}}.
",,0,0,0,0,0.0,"Cleanup for Sp_handler::show_create_sp $end$ In order to implement packages easier, we'll do the following changes in {{Sp_handler::show_create_sp()}}:
- Reverse the return value to the standard MariaDB notation: false on success, true on error
- Add a new parameter {{const DDL_options_st ddl_options}} and use it to process the {{OR REPLACE}} and {{IF NOT EXISTS}} clauses, instead of accessing {{thd->lex->create_info}}.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,73,42,0.575342,29,0.39726,27,0.369863,25,0.342466,25,0.342466
673,MDEV-13450,Technical task,MDEV,2017-08-04 12:36:14,,0,Cleanup SP code for packages,"In order to make the patch for Oracle-style packages smaller, we'll do the following changes as a separate commit:

- Add Sp_handler::db_find_and_cache_routine(), to avoid duplicate combinations of db_find_routine() followed by sp_cache_insert()
- Add append_suid() and append_comment(), helper functions to share between show for CREATE PROCEDURE and show for CREATE PACKAGE
- Move the code responsible to clone recursive SP routines from Sp_handler::sp_find_routine() into a new method Sp_handler::sp_clone_and_link_routine()
- Add ""const"" qualifier to LEX_CSTRING parameters to LEX::make_sp_name()
- Add LEX::set_user_variable(), to reduce code duplication in the grammar creating Item_func_set_user_var's
- Add LEX::call_statement_start(), to reduce code duplication in the grammar for ""call:""
- Add LEX::add_grant_command(), to reduce code duplication in the grammar for ""revoke_command:"" and ""grant_command:""
",,"Cleanup SP code for packages $end$ In order to make the patch for Oracle-style packages smaller, we'll do the following changes as a separate commit:

- Add Sp_handler::db_find_and_cache_routine(), to avoid duplicate combinations of db_find_routine() followed by sp_cache_insert()
- Add append_suid() and append_comment(), helper functions to share between show for CREATE PROCEDURE and show for CREATE PACKAGE
- Move the code responsible to clone recursive SP routines from Sp_handler::sp_find_routine() into a new method Sp_handler::sp_clone_and_link_routine()
- Add ""const"" qualifier to LEX_CSTRING parameters to LEX::make_sp_name()
- Add LEX::set_user_variable(), to reduce code duplication in the grammar creating Item_func_set_user_var's
- Add LEX::call_statement_start(), to reduce code duplication in the grammar for ""call:""
- Add LEX::add_grant_command(), to reduce code duplication in the grammar for ""revoke_command:"" and ""grant_command:""
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,4,,0,0,1,5,0,0,0,,0,850,0,0,0,2017-08-04 12:36:14,Cleanup SP code for packages,"In order to make the patch for Oracle-style packages smaller, we'll do the following changes as a separate commit:

- Add Sp_handler::db_find_and_cache_routine(), to avoid duplicate combinations of db_find_routine() followed by sp_cache_insert()
- Add append_suid() and append_comment(), helper functions to share between show for CREATE PROCEDURE and show for CREATE PACKAGE
- Move the code responsible to clone recursive SP routines from Sp_handler::sp_find_routine() into a new method Sp_handler::sp_clone_and_link_routine()
- Add ""const"" qualifier to LEX_CSTRING parameters to LEX::make_sp_name()
- Add LEX::set_user_variable(), to reduce code duplication in the grammar creating Item_func_set_user_var's
- Add LEX::call_statement_start(), to reduce code duplication in the grammar for ""call:""
- Add LEX::add_grant_command(), to reduce code duplication in the grammar for ""revoke_command:"" and ""grant_command:""
",,0,0,0,0,0.0,"Cleanup SP code for packages $end$ In order to make the patch for Oracle-style packages smaller, we'll do the following changes as a separate commit:

- Add Sp_handler::db_find_and_cache_routine(), to avoid duplicate combinations of db_find_routine() followed by sp_cache_insert()
- Add append_suid() and append_comment(), helper functions to share between show for CREATE PROCEDURE and show for CREATE PACKAGE
- Move the code responsible to clone recursive SP routines from Sp_handler::sp_find_routine() into a new method Sp_handler::sp_clone_and_link_routine()
- Add ""const"" qualifier to LEX_CSTRING parameters to LEX::make_sp_name()
- Add LEX::set_user_variable(), to reduce code duplication in the grammar creating Item_func_set_user_var's
- Add LEX::call_statement_start(), to reduce code duplication in the grammar for ""call:""
- Add LEX::add_grant_command(), to reduce code duplication in the grammar for ""revoke_command:"" and ""grant_command:""
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,74,42,0.567568,29,0.391892,27,0.364865,25,0.337838,25,0.337838
674,MDEV-13483,Task,MDEV,2017-08-09 10:27:44,,0,10.0.33 merge," * [5.5|https://github.com/mariadb/server/tree/5.5] (/) 5.5.58
* [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6] (/) 5.6.38
* [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6] (/) 5.6.38
* [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6] (/) 5.6.37-82.2
* [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6] (/) 5.6.37-82.2
* [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed* (/) 2566e67da80f291414f02c7dd6a8ca3557161d26 and later squashed c07064d31a4d7ee0533fec144648d93873c0dd17
* [Spider|https://github.com/Kentoku/MariaDB] (/) No update
* [Mroonga|https://github.com/Kentoku/MariaDB] (/) No update
* [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre] (/) No update

*Note:* when merging xtradb 5.6.37, revert commit d9bc5e03d788b958ce8c76e157239953db60adb2
See how innodb for the reference. (/)
*Note:* all code for Bug#23481444 should be reverted, and the follow-up fixes (such [Bug#23481444|https://github.com/mysql/mysql-server/commit/bce4b74f73f7d83b411d9cd3d667317f67bfe1b5] and [follow-up|https://github.com/mysql/mysql-server/commit/870674345522728dbd5350d08031f71cbe50cd98]) should be reverted from both InnoDB and XtraDB (/)",,"10.0.33 merge $end$  * [5.5|https://github.com/mariadb/server/tree/5.5] (/) 5.5.58
* [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6] (/) 5.6.38
* [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6] (/) 5.6.38
* [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6] (/) 5.6.37-82.2
* [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6] (/) 5.6.37-82.2
* [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed* (/) 2566e67da80f291414f02c7dd6a8ca3557161d26 and later squashed c07064d31a4d7ee0533fec144648d93873c0dd17
* [Spider|https://github.com/Kentoku/MariaDB] (/) No update
* [Mroonga|https://github.com/Kentoku/MariaDB] (/) No update
* [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre] (/) No update

*Note:* when merging xtradb 5.6.37, revert commit d9bc5e03d788b958ce8c76e157239953db60adb2
See how innodb for the reference. (/)
*Note:* all code for Bug#23481444 should be reverted, and the follow-up fixes (such [Bug#23481444|https://github.com/mysql/mysql-server/commit/bce4b74f73f7d83b411d9cd3d667317f67bfe1b5] and [follow-up|https://github.com/mysql/mysql-server/commit/870674345522728dbd5350d08031f71cbe50cd98]) should be reverted from both InnoDB and XtraDB (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,19,,1,0,1,1,0,13,0,,0,850,0,10,0,2017-10-24 15:49:48,10.0.33 merge,"* [5.5|https://github.com/mariadb/server/tree/5.5] (/) 5.5.57
* [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6] (/) No update
* [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6] (/) No update
* [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6] (/) 5.6.37-82.2
* [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6] (/) 5.6.37-82.2
* [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed* (/) 2566e67da80f291414f02c7dd6a8ca3557161d26
* [Spider|https://github.com/Kentoku/MariaDB] (/) No update
* [Mroonga|https://github.com/Kentoku/MariaDB] (/) No update
* [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre] (/) No update

*Note:* when merging xtradb 5.6.37, revert commit d9bc5e03d788b958ce8c76e157239953db60adb2
See how innodb for the reference. (/)
*Note:* all code for Bug#23481444 should be reverted, and the follow-up fixes (such [Bug#23481444|https://github.com/mysql/mysql-server/commit/bce4b74f73f7d83b411d9cd3d667317f67bfe1b5] and [follow-up|https://github.com/mysql/mysql-server/commit/870674345522728dbd5350d08031f71cbe50cd98]) should be reverted from both InnoDB and XtraDB (/)",,0,3,0,12,0.103448,"10.0.33 merge $end$ * [5.5|https://github.com/mariadb/server/tree/5.5] (/) 5.5.57
* [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6] (/) No update
* [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6] (/) No update
* [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6] (/) 5.6.37-82.2
* [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6] (/) 5.6.37-82.2
* [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed* (/) 2566e67da80f291414f02c7dd6a8ca3557161d26
* [Spider|https://github.com/Kentoku/MariaDB] (/) No update
* [Mroonga|https://github.com/Kentoku/MariaDB] (/) No update
* [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre] (/) No update

*Note:* when merging xtradb 5.6.37, revert commit d9bc5e03d788b958ce8c76e157239953db60adb2
See how innodb for the reference. (/)
*Note:* all code for Bug#23481444 should be reverted, and the follow-up fixes (such [Bug#23481444|https://github.com/mysql/mysql-server/commit/bce4b74f73f7d83b411d9cd3d667317f67bfe1b5] and [follow-up|https://github.com/mysql/mysql-server/commit/870674345522728dbd5350d08031f71cbe50cd98]) should be reverted from both InnoDB and XtraDB (/) $acceptance criteria:$",3,1,1,1,0,0,1,1829.37,50,25,0.5,19,0.38,14,0.28,13,0.26,10,0.2
675,MDEV-13491,Task,MDEV,2017-08-10 11:23:15,,0,benchmark SEQUENCE,Create and run a benchmark for SEQUENCE objects in MariaDB 10.3,,benchmark SEQUENCE $end$ Create and run a benchmark for SEQUENCE objects in MariaDB 10.3 $acceptance criteria:$,,Axel Schwenke,Axel Schwenke,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-01-24 09:36:09,benchmark SEQUENCE,Create and run a benchmark for SEQUENCE objects in MariaDB 10.3,,0,0,0,0,0.0,benchmark SEQUENCE $end$ Create and run a benchmark for SEQUENCE objects in MariaDB 10.3 $acceptance criteria:$,0,0,0,0,0,0,0,4006.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
676,MDEV-13528,Technical task,MDEV,2017-08-15 07:08:33,,0,Add LEX::sp_body_finalize_{procedure|function},"This change is done under terms of a separate MDEV, to reduce the patch size for MDEV-10591.

In order to save some duplicate code in sql_yacc.yy and sql_yacc_ora.yy, between
- FUNCTION
- PROCEDURE
- Package FUNCTION (coming soon in MDEV-10591)
- Package PROCEDURE (coming soon in MDEV-10591)
we'll introduce two new methods:


{code:cpp}
bool LEX::sp_body_finalize_procedure(THD *thd)
{
  if (sphead->check_unresolved_goto())
    return true;
  sphead->set_stmt_end(thd);
  sphead->restore_thd_mem_root(thd);
  return false;
}

bool LEX::sp_body_finalize_function(THD *thd)
{
  if (sphead->is_not_allowed_in_function(""function""))
    return true;
  if (!(sphead->m_flags & sp_head::HAS_RETURN))
  {
    my_error(ER_SP_NORETURN, MYF(0), ErrConvDQName(sphead).ptr());
    return true;
  }
  if (sp_body_finalize_procedure(thd))
    return true;
  (void) is_native_function_with_warn(thd, &sphead->m_name);
  return false;
}
{code}

and use these methods in *.yy files.
",,"Add LEX::sp_body_finalize_{procedure|function} $end$ This change is done under terms of a separate MDEV, to reduce the patch size for MDEV-10591.

In order to save some duplicate code in sql_yacc.yy and sql_yacc_ora.yy, between
- FUNCTION
- PROCEDURE
- Package FUNCTION (coming soon in MDEV-10591)
- Package PROCEDURE (coming soon in MDEV-10591)
we'll introduce two new methods:


{code:cpp}
bool LEX::sp_body_finalize_procedure(THD *thd)
{
  if (sphead->check_unresolved_goto())
    return true;
  sphead->set_stmt_end(thd);
  sphead->restore_thd_mem_root(thd);
  return false;
}

bool LEX::sp_body_finalize_function(THD *thd)
{
  if (sphead->is_not_allowed_in_function(""function""))
    return true;
  if (!(sphead->m_flags & sp_head::HAS_RETURN))
  {
    my_error(ER_SP_NORETURN, MYF(0), ErrConvDQName(sphead).ptr());
    return true;
  }
  if (sp_body_finalize_procedure(thd))
    return true;
  (void) is_native_function_with_warn(thd, &sphead->m_name);
  return false;
}
{code}

and use these methods in *.yy files.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,7,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-08-15 07:08:33,Add LEX::sp_body_finalize_{procedure|function},"This change is done under terms of a separate MDEV, to reduce the patch size for MDEV-10591.

In order to save some duplicate code in sql_yacc.yy and sql_yacc_ora.yy, between
- FUNCTION
- PROCEDURE
- Package FUNCTION (coming soon in MDEV-10591)
- Package PROCEDURE (coming soon in MDEV-10591)
we'll introduce two new methods:


{code:cpp}
bool LEX::sp_body_finalize_procedure(THD *thd)
{
  if (sphead->check_unresolved_goto())
    return true;
  sphead->set_stmt_end(thd);
  sphead->restore_thd_mem_root(thd);
  return false;
}

bool LEX::sp_body_finalize_function(THD *thd)
{
  if (sphead->is_not_allowed_in_function(""function""))
    return true;
  if (!(sphead->m_flags & sp_head::HAS_RETURN))
  {
    my_error(ER_SP_NORETURN, MYF(0), ErrConvDQName(sphead).ptr());
    return true;
  }
  if (sp_body_finalize_procedure(thd))
    return true;
  (void) is_native_function_with_warn(thd, &sphead->m_name);
  return false;
}
{code}

and use these methods in *.yy files.
",,0,0,0,0,0.0,"Add LEX::sp_body_finalize_{procedure|function} $end$ This change is done under terms of a separate MDEV, to reduce the patch size for MDEV-10591.

In order to save some duplicate code in sql_yacc.yy and sql_yacc_ora.yy, between
- FUNCTION
- PROCEDURE
- Package FUNCTION (coming soon in MDEV-10591)
- Package PROCEDURE (coming soon in MDEV-10591)
we'll introduce two new methods:


{code:cpp}
bool LEX::sp_body_finalize_procedure(THD *thd)
{
  if (sphead->check_unresolved_goto())
    return true;
  sphead->set_stmt_end(thd);
  sphead->restore_thd_mem_root(thd);
  return false;
}

bool LEX::sp_body_finalize_function(THD *thd)
{
  if (sphead->is_not_allowed_in_function(""function""))
    return true;
  if (!(sphead->m_flags & sp_head::HAS_RETURN))
  {
    my_error(ER_SP_NORETURN, MYF(0), ErrConvDQName(sphead).ptr());
    return true;
  }
  if (sp_body_finalize_procedure(thd))
    return true;
  (void) is_native_function_with_warn(thd, &sphead->m_name);
  return false;
}
{code}

and use these methods in *.yy files.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,75,42,0.56,29,0.386667,27,0.36,25,0.333333,25,0.333333
677,MDEV-13531,Technical task,MDEV,2017-08-15 10:37:38,,0,Add Database_qualified_name::copy(),"We'll move this code in {{sp_head::init_sp_name()}}:
{code:cpp}
  /* We have to copy strings to get them into the right memroot. */

  m_db.length= spname->m_db.length;
  m_db.str= strmake_root(thd->mem_root, spname->m_db.str, spname->m_db.length);

  m_name.length= spname->m_name.length;
  m_name.str= strmake_root(thd->mem_root, spname->m_name.str,
                           spname->m_name.length);
{code}


to a new method in {{Database_qualified_name}}:

{code:cpp}
void Database_qualified_name::copy(MEM_ROOT *mem_root,
                                   const LEX_CSTRING &db,
                                   const LEX_CSTRING &name)
{
  m_db.length= db.length;
  m_db.str= strmake_root(mem_root, db.str, db.length);
  m_name.length= name.length;
  m_name.str= strmake_root(mem_root, name.str, name.length);
}
{code}

It will be easier to reuse this code this way, e.g. for MDEV-10591.",,"Add Database_qualified_name::copy() $end$ We'll move this code in {{sp_head::init_sp_name()}}:
{code:cpp}
  /* We have to copy strings to get them into the right memroot. */

  m_db.length= spname->m_db.length;
  m_db.str= strmake_root(thd->mem_root, spname->m_db.str, spname->m_db.length);

  m_name.length= spname->m_name.length;
  m_name.str= strmake_root(thd->mem_root, spname->m_name.str,
                           spname->m_name.length);
{code}


to a new method in {{Database_qualified_name}}:

{code:cpp}
void Database_qualified_name::copy(MEM_ROOT *mem_root,
                                   const LEX_CSTRING &db,
                                   const LEX_CSTRING &name)
{
  m_db.length= db.length;
  m_db.str= strmake_root(mem_root, db.str, db.length);
  m_name.length= name.length;
  m_name.str= strmake_root(mem_root, name.str, name.length);
}
{code}

It will be easier to reuse this code this way, e.g. for MDEV-10591. $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,5,,0,0,1,5,0,0,0,,0,850,0,0,0,2017-08-15 10:37:38,Add Database_qualified_name::copy(),"We'll move this code in {{sp_head::init_sp_name()}}:
{code:cpp}
  /* We have to copy strings to get them into the right memroot. */

  m_db.length= spname->m_db.length;
  m_db.str= strmake_root(thd->mem_root, spname->m_db.str, spname->m_db.length);

  m_name.length= spname->m_name.length;
  m_name.str= strmake_root(thd->mem_root, spname->m_name.str,
                           spname->m_name.length);
{code}


to a new method in {{Database_qualified_name}}:

{code:cpp}
void Database_qualified_name::copy(MEM_ROOT *mem_root,
                                   const LEX_CSTRING &db,
                                   const LEX_CSTRING &name)
{
  m_db.length= db.length;
  m_db.str= strmake_root(mem_root, db.str, db.length);
  m_name.length= name.length;
  m_name.str= strmake_root(mem_root, name.str, name.length);
}
{code}

It will be easier to reuse this code this way, e.g. for MDEV-10591.",,0,0,0,0,0.0,"Add Database_qualified_name::copy() $end$ We'll move this code in {{sp_head::init_sp_name()}}:
{code:cpp}
  /* We have to copy strings to get them into the right memroot. */

  m_db.length= spname->m_db.length;
  m_db.str= strmake_root(thd->mem_root, spname->m_db.str, spname->m_db.length);

  m_name.length= spname->m_name.length;
  m_name.str= strmake_root(thd->mem_root, spname->m_name.str,
                           spname->m_name.length);
{code}


to a new method in {{Database_qualified_name}}:

{code:cpp}
void Database_qualified_name::copy(MEM_ROOT *mem_root,
                                   const LEX_CSTRING &db,
                                   const LEX_CSTRING &name)
{
  m_db.length= db.length;
  m_db.str= strmake_root(mem_root, db.str, db.length);
  m_name.length= name.length;
  m_name.str= strmake_root(mem_root, name.str, name.length);
}
{code}

It will be easier to reuse this code this way, e.g. for MDEV-10591. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,76,42,0.552632,29,0.381579,27,0.355263,25,0.328947,25,0.328947
678,MDEV-13533,Technical task,MDEV,2017-08-15 12:30:26,,0,Remove the THD parameter from sp_head::init_sp_name(),"{{sp_head}} has its own member {{MEM_ROOT main_mem_root}}.
The caller of {{sp_head::init_sp_name()}} currently makes sure to set {{THD::mem_root}} to {{&sphead::main_mem_root}}, by calling {{sphead::reset_thd_mem_root(THD*)}}. So the name is effectively created on {{main_mem_root}} anyway. 

There is no a need to pass a pointer to {{THD}} to {{init_sp_name()}}. It's much safer to create the name by allocating memory directly on {{sp_head::main_mem_root}}, instead of using its alias {{THD::mem_root}}.

Under terms of this task we'll do the following:
- Change:
{code:cpp}
void init_sp_name(THD *thd, const sp_name *spname);
{code}
to
{code:cpp}
void init_sp_name(const sp_name *spname);
{code}

- Change:
{code:cpp}
bool Database_qualified_name::make_qname(THD *thd, LEX_CSTRING *dst) const;
{code}
to
{code:cpp}
bool make_qname(MEM_ROOT *mem_root, LEX_CSTRING *dst) const;
{code}

After this change, the code will be more flexible, as it will be possible to call {{init_sp_name()}} without prior call for {{sp->reset_thd_mem_root(thd)}}.",,"Remove the THD parameter from sp_head::init_sp_name() $end$ {{sp_head}} has its own member {{MEM_ROOT main_mem_root}}.
The caller of {{sp_head::init_sp_name()}} currently makes sure to set {{THD::mem_root}} to {{&sphead::main_mem_root}}, by calling {{sphead::reset_thd_mem_root(THD*)}}. So the name is effectively created on {{main_mem_root}} anyway. 

There is no a need to pass a pointer to {{THD}} to {{init_sp_name()}}. It's much safer to create the name by allocating memory directly on {{sp_head::main_mem_root}}, instead of using its alias {{THD::mem_root}}.

Under terms of this task we'll do the following:
- Change:
{code:cpp}
void init_sp_name(THD *thd, const sp_name *spname);
{code}
to
{code:cpp}
void init_sp_name(const sp_name *spname);
{code}

- Change:
{code:cpp}
bool Database_qualified_name::make_qname(THD *thd, LEX_CSTRING *dst) const;
{code}
to
{code:cpp}
bool make_qname(MEM_ROOT *mem_root, LEX_CSTRING *dst) const;
{code}

After this change, the code will be more flexible, as it will be possible to call {{init_sp_name()}} without prior call for {{sp->reset_thd_mem_root(thd)}}. $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,11,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-08-15 12:30:26,Remove the THD parameter from sp_head::init_sp_name(),"{{sp_head}} has its own member {{MEM_ROOT main_mem_root}}.
The caller of {{sp_head::init_sp_name()}} currently makes sure to set {{THD::mem_root}} to {{&sphead::main_mem_root}}, by calling {{sphead::reset_thd_mem_root(THD*)}}. So the name is effectively created on {{main_mem_root}} anyway. 

There is no a need to pass a pointer to {{THD}} to {{init_sp_name()}}. It's much safer to create the name by allocating memory directly on {{sp_head::main_mem_root}}, instead of using its alias {{THD::mem_root}}.

Under terms of this task we'll do the following:
- Change:
{code:cpp}
void init_sp_name(THD *thd, const sp_name *spname);
{code}
to
{code:cpp}
void init_sp_name(const sp_name *spname);
{code}

- Change:
{code:cpp}
bool Database_qualified_name::make_qname(THD *thd, LEX_CSTRING *dst) const;
{code}
to
{code:cpp}
bool make_qname(MEM_ROOT *mem_root, LEX_CSTRING *dst) const;
{code}

After this change, the code will be more flexible, as it will be possible to call {{init_sp_name()}} without prior call for {{sp->reset_thd_mem_root(thd)}}.",,0,0,0,0,0.0,"Remove the THD parameter from sp_head::init_sp_name() $end$ {{sp_head}} has its own member {{MEM_ROOT main_mem_root}}.
The caller of {{sp_head::init_sp_name()}} currently makes sure to set {{THD::mem_root}} to {{&sphead::main_mem_root}}, by calling {{sphead::reset_thd_mem_root(THD*)}}. So the name is effectively created on {{main_mem_root}} anyway. 

There is no a need to pass a pointer to {{THD}} to {{init_sp_name()}}. It's much safer to create the name by allocating memory directly on {{sp_head::main_mem_root}}, instead of using its alias {{THD::mem_root}}.

Under terms of this task we'll do the following:
- Change:
{code:cpp}
void init_sp_name(THD *thd, const sp_name *spname);
{code}
to
{code:cpp}
void init_sp_name(const sp_name *spname);
{code}

- Change:
{code:cpp}
bool Database_qualified_name::make_qname(THD *thd, LEX_CSTRING *dst) const;
{code}
to
{code:cpp}
bool make_qname(MEM_ROOT *mem_root, LEX_CSTRING *dst) const;
{code}

After this change, the code will be more flexible, as it will be possible to call {{init_sp_name()}} without prior call for {{sp->reset_thd_mem_root(thd)}}. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,77,42,0.545455,29,0.376623,27,0.350649,25,0.324675,25,0.324675
679,MDEV-13760,Technical task,MDEV,2017-09-07 20:52:38,,0,Document column compression introduced in 10.3.2,"MDEV-11371 introduced column compression, with new syntax, variables etc. Please document it all -- and not just formal language additions, but also functionality as such -- when it works, which types it applies to, etc.",,"Document column compression introduced in 10.3.2 $end$ MDEV-11371 introduced column compression, with new syntax, variables etc. Please document it all -- and not just formal language additions, but also functionality as such -- when it works, which types it applies to, etc. $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,5,,0,3,0,1,0,0,0,,0,850,3,0,0,2017-09-07 20:52:38,Document column compression introduced in 10.3.2,"MDEV-11371 introduced column compression, with new syntax, variables etc. Please document it all -- and not just formal language additions, but also functionality as such -- when it works, which types it applies to, etc.",,0,0,0,0,0.0,"Document column compression introduced in 10.3.2 $end$ MDEV-11371 introduced column compression, with new syntax, variables etc. Please document it all -- and not just formal language additions, but also functionality as such -- when it works, which types it applies to, etc. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,7,2,0.285714,2,0.285714,2,0.285714,2,0.285714,2,0.285714
680,MDEV-13761,Task,MDEV,2017-09-07 20:59:53,,0,Document proxy protocol support,"MDEV-11159 introduced proxy protocol support, please document it in the KB.",,"Document proxy protocol support $end$ MDEV-11159 introduced proxy protocol support, please document it in the KB. $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,7,,0,2,0,1,0,0,0,,0,850,1,0,0,2018-01-24 11:59:16,Document proxy protocol support,"MDEV-11159 introduced proxy protocol support, please document it in the KB.",,0,0,0,0,0.0,"Document proxy protocol support $end$ MDEV-11159 introduced proxy protocol support, please document it in the KB. $acceptance criteria:$",0,0,0,0,0,0,0,3326.98,8,2,0.25,2,0.25,2,0.25,2,0.25,2,0.25
681,MDEV-13763,Technical task,MDEV,2017-09-07 21:30:54,,0,Document SP stack trace at SHOW WARNINGS page,"The feature doesn't need its own page, but is worth mentioning at [SHOW WARNINGS|https://mariadb.com/kb/en/the-mariadb-library/show-warnings] page.",,"Document SP stack trace at SHOW WARNINGS page $end$ The feature doesn't need its own page, but is worth mentioning at [SHOW WARNINGS|https://mariadb.com/kb/en/the-mariadb-library/show-warnings] page. $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,3,,0,2,0,1,0,0,0,,0,850,2,0,0,2017-09-07 21:30:54,Document SP stack trace at SHOW WARNINGS page,"The feature doesn't need its own page, but is worth mentioning at [SHOW WARNINGS|https://mariadb.com/kb/en/the-mariadb-library/show-warnings] page.",,0,0,0,0,0.0,"Document SP stack trace at SHOW WARNINGS page $end$ The feature doesn't need its own page, but is worth mentioning at [SHOW WARNINGS|https://mariadb.com/kb/en/the-mariadb-library/show-warnings] page. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,9,2,0.222222,2,0.222222,2,0.222222,2,0.222222,2,0.222222
682,MDEV-13812,Task,MDEV,2017-09-15 11:53:29,,0,Is it possible to get rid of Advance Toolchain when building MariaDB for ppc?,"Hi,

MariaDB dependency on Advance Toolchain for ppc is problematic (you can find some details in MDEV-13804 and MDEV-13811).

Dependency of Galera on Boost is also a problem for this architecture.

Is it possible to make MariaDB free of these dependencies?

This is just an inquiry. No pressure in any way. I'm trying to figure out a way of using this wonderful DB on ppc. But unfortunately currently I don't see a way which wouldn't be very painful and wouldn't break standard Linux support contracts.",,"Is it possible to get rid of Advance Toolchain when building MariaDB for ppc? $end$ Hi,

MariaDB dependency on Advance Toolchain for ppc is problematic (you can find some details in MDEV-13804 and MDEV-13811).

Dependency of Galera on Boost is also a problem for this architecture.

Is it possible to make MariaDB free of these dependencies?

This is just an inquiry. No pressure in any way. I'm trying to figure out a way of using this wonderful DB on ppc. But unfortunately currently I don't see a way which wouldn't be very painful and wouldn't break standard Linux support contracts. $acceptance criteria:$",,Andrey Nevolin,Andrey Nevolin,Major,9,,0,6,0,1,0,0,0,,0,850,5,0,0,2017-10-24 15:09:38,Is it possible to get rid of Advance Toolchain when building MariaDB for ppc?,"Hi,

MariaDB dependency on Advance Toolchain for ppc is problematic (you can find some details in MDEV-13804 and MDEV-13811).

Dependency of Galera on Boost is also a problem for this architecture.

Is it possible to make MariaDB free of these dependencies?

This is just an inquiry. No pressure in any way. I'm trying to figure out a way of using this wonderful DB on ppc. But unfortunately currently I don't see a way which wouldn't be very painful and wouldn't break standard Linux support contracts.",,0,0,0,0,0.0,"Is it possible to get rid of Advance Toolchain when building MariaDB for ppc? $end$ Hi,

MariaDB dependency on Advance Toolchain for ppc is problematic (you can find some details in MDEV-13804 and MDEV-13811).

Dependency of Galera on Boost is also a problem for this architecture.

Is it possible to make MariaDB free of these dependencies?

This is just an inquiry. No pressure in any way. I'm trying to figure out a way of using this wonderful DB on ppc. But unfortunately currently I don't see a way which wouldn't be very painful and wouldn't break standard Linux support contracts. $acceptance criteria:$",0,0,0,0,0,0,0,939.267,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
683,MDEV-13864,Technical task,MDEV,2017-09-22 05:19:40,,0,Change Item_func_case to store the predicant in args[0],"As of version {{10.3.1}}, {{Item_func_case}} (when handling a simple {{CASE}}) stores the predicant argument (in the {{args[]}} array) after the {{WHEN..THEN}} arguments and before the {{ELSE}} argument.

For example, this expression:
{code:sql}
CASE pred WHEN search1 THEN res1 WHEN search2 THEN res2 ELSE resE END
{code}
stores the arguments as follows:
- args[0]=search1
- args[1]=res1
- args[2]=search2
- args[3]=res2
- args[4]=pred
- args[6]=resE

Under terms of this task we'll do the following:
1. Change {{Item_func_case}} to store the arguments in the order of their appearance in the parser: the predicant argument in {{args[0]}}, followed by the {{WHEN..THEN..ELSE}} arguments
2. Split {{Item_func_case}} into two separate classes {{Item_func_case_simple}} and {{Item_func_case_searched}} (for {{CASE}} expressions with and without predicant respectively).
3. Change the constructors of the affected classes just to accept a {{List<Item>}} argument (without additional {{first_expr_arg}} and {{else_expr_arg}}.

Advantages:

a. #1 and #3 will help to simplify the code in {{sql_yacc_ora.yy}} 

{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    // 30 lines of the code, extracting {{else_expr_arg}} from the list.
  }
{code}
to something as simple as:
{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    $5->push_front($3, thd->mem_root);
    if (!($$= new (thd->mem_root) Item_func_case_simple(thd, *$5)))
      MYSQL_YYABORT;
  }
{code}

b. #1 and #3 will help to implement MDEV-13863 easier. MDEV-13836 will introduce a new class {{Item_func_decode_oracle}}. Without #1, we'd have to use the same complex code from {{sql_yacc_ora.yy}}, now for both {{DECODE()}} and {{DECODE_ORACLE()}}.
c. #2 will slightly reduce the memory size required to handle searched {{CASE}} expression, because only {{Item_func_case_simple}} will need to derive from {{Predicant_to_list_comparator}}
d. #2 will make the code more readable by removing a lot of {{if}} statements like this:
{code:cpp}
if (first_expr_num != -1)
{
  ...
}
else
{
  ...
}
{code}
as the branches for the searched and the simple {{CASE}} expressions will reside in methods of different classes.
e. #1 will make it easier to debug the code in {{gdb}}, as printing the arguments in their syntactic order will be much easier than now.

",,"Change Item_func_case to store the predicant in args[0] $end$ As of version {{10.3.1}}, {{Item_func_case}} (when handling a simple {{CASE}}) stores the predicant argument (in the {{args[]}} array) after the {{WHEN..THEN}} arguments and before the {{ELSE}} argument.

For example, this expression:
{code:sql}
CASE pred WHEN search1 THEN res1 WHEN search2 THEN res2 ELSE resE END
{code}
stores the arguments as follows:
- args[0]=search1
- args[1]=res1
- args[2]=search2
- args[3]=res2
- args[4]=pred
- args[6]=resE

Under terms of this task we'll do the following:
1. Change {{Item_func_case}} to store the arguments in the order of their appearance in the parser: the predicant argument in {{args[0]}}, followed by the {{WHEN..THEN..ELSE}} arguments
2. Split {{Item_func_case}} into two separate classes {{Item_func_case_simple}} and {{Item_func_case_searched}} (for {{CASE}} expressions with and without predicant respectively).
3. Change the constructors of the affected classes just to accept a {{List<Item>}} argument (without additional {{first_expr_arg}} and {{else_expr_arg}}.

Advantages:

a. #1 and #3 will help to simplify the code in {{sql_yacc_ora.yy}} 

{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    // 30 lines of the code, extracting {{else_expr_arg}} from the list.
  }
{code}
to something as simple as:
{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    $5->push_front($3, thd->mem_root);
    if (!($$= new (thd->mem_root) Item_func_case_simple(thd, *$5)))
      MYSQL_YYABORT;
  }
{code}

b. #1 and #3 will help to implement MDEV-13863 easier. MDEV-13836 will introduce a new class {{Item_func_decode_oracle}}. Without #1, we'd have to use the same complex code from {{sql_yacc_ora.yy}}, now for both {{DECODE()}} and {{DECODE_ORACLE()}}.
c. #2 will slightly reduce the memory size required to handle searched {{CASE}} expression, because only {{Item_func_case_simple}} will need to derive from {{Predicant_to_list_comparator}}
d. #2 will make the code more readable by removing a lot of {{if}} statements like this:
{code:cpp}
if (first_expr_num != -1)
{
  ...
}
else
{
  ...
}
{code}
as the branches for the searched and the simple {{CASE}} expressions will reside in methods of different classes.
e. #1 will make it easier to debug the code in {{gdb}}, as printing the arguments in their syntactic order will be much easier than now.

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,8,,1,1,1,5,0,0,0,,0,850,1,0,0,2017-09-22 05:19:40,Change Item_func_case to store the predicant in args[0],"As of version {{10.3.1}}, {{Item_func_case}} (when handling a simple {{CASE}}) stores the predicant argument (in the {{args[]}} array) after the {{WHEN..THEN}} arguments and before the {{ELSE}} argument.

For example, this expression:
{code:sql}
CASE pred WHEN search1 THEN res1 WHEN search2 THEN res2 ELSE resE END
{code}
stores the arguments as follows:
- args[0]=search1
- args[1]=res1
- args[2]=search2
- args[3]=res2
- args[4]=pred
- args[6]=resE

Under terms of this task we'll do the following:
1. Change {{Item_func_case}} to store the arguments in the order of their appearance in the parser: the predicant argument in {{args[0]}}, followed by the {{WHEN..THEN..ELSE}} arguments
2. Split {{Item_func_case}} into two separate classes {{Item_func_case_simple}} and {{Item_func_case_searched}} (for {{CASE}} expressions with and without predicant respectively).
3. Change the constructors of the affected classes just to accept a {{List<Item>}} argument (without additional {{first_expr_arg}} and {{else_expr_arg}}.

Advantages:

a. #1 and #3 will help to simplify the code in {{sql_yacc_ora.yy}} 

{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    // 30 lines of the code, extracting {{else_expr_arg}} from the list.
  }
{code}
to something as simple as:
{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    $5->push_front($3, thd->mem_root);
    if (!($$= new (thd->mem_root) Item_func_case_simple(thd, *$5)))
      MYSQL_YYABORT;
  }
{code}

b. #1 and #3 will help to implement MDEV-13863 easier. MDEV-13836 will introduce a new class {{Item_func_decode_oracle}}. Without #1, we'd have to use the same complex code from {{sql_yacc_ora.yy}}, now for both {{DECODE()}} and {{DECODE_ORACLE()}}.
c. #2 will slightly reduce the memory size required to handle searched {{CASE}} expression, because only {{Item_func_case_simple}} will need to derive from {{Predicant_to_list_comparator}}
d. #2 will make the code more readable by removing a lot of {{if}} statements like this:
{code:cpp}
if (first_expr_num != -1)
{
  ...
}
else
{
  ...
}
{code}
as the branches for the searched and the simple {{CASE}} expressions will reside in methods of different classes.
e. #1 will make it easier to debug the code in {{gdb}}, as printing the arguments in their syntactic order will be much easier than now.

",,0,0,0,0,0.0,"Change Item_func_case to store the predicant in args[0] $end$ As of version {{10.3.1}}, {{Item_func_case}} (when handling a simple {{CASE}}) stores the predicant argument (in the {{args[]}} array) after the {{WHEN..THEN}} arguments and before the {{ELSE}} argument.

For example, this expression:
{code:sql}
CASE pred WHEN search1 THEN res1 WHEN search2 THEN res2 ELSE resE END
{code}
stores the arguments as follows:
- args[0]=search1
- args[1]=res1
- args[2]=search2
- args[3]=res2
- args[4]=pred
- args[6]=resE

Under terms of this task we'll do the following:
1. Change {{Item_func_case}} to store the arguments in the order of their appearance in the parser: the predicant argument in {{args[0]}}, followed by the {{WHEN..THEN..ELSE}} arguments
2. Split {{Item_func_case}} into two separate classes {{Item_func_case_simple}} and {{Item_func_case_searched}} (for {{CASE}} expressions with and without predicant respectively).
3. Change the constructors of the affected classes just to accept a {{List<Item>}} argument (without additional {{first_expr_arg}} and {{else_expr_arg}}.

Advantages:

a. #1 and #3 will help to simplify the code in {{sql_yacc_ora.yy}} 

{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    // 30 lines of the code, extracting {{else_expr_arg}} from the list.
  }
{code}
to something as simple as:
{code:cpp}
| DECODE_SYM '(' expr ',' decode_when_list ')'
  {
    $5->push_front($3, thd->mem_root);
    if (!($$= new (thd->mem_root) Item_func_case_simple(thd, *$5)))
      MYSQL_YYABORT;
  }
{code}

b. #1 and #3 will help to implement MDEV-13863 easier. MDEV-13836 will introduce a new class {{Item_func_decode_oracle}}. Without #1, we'd have to use the same complex code from {{sql_yacc_ora.yy}}, now for both {{DECODE()}} and {{DECODE_ORACLE()}}.
c. #2 will slightly reduce the memory size required to handle searched {{CASE}} expression, because only {{Item_func_case_simple}} will need to derive from {{Predicant_to_list_comparator}}
d. #2 will make the code more readable by removing a lot of {{if}} statements like this:
{code:cpp}
if (first_expr_num != -1)
{
  ...
}
else
{
  ...
}
{code}
as the branches for the searched and the simple {{CASE}} expressions will reside in methods of different classes.
e. #1 will make it easier to debug the code in {{gdb}}, as printing the arguments in their syntactic order will be much easier than now.

 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,78,42,0.538462,29,0.371795,27,0.346154,25,0.320513,25,0.320513
684,MDEV-13919,Technical task,MDEV,2017-09-27 13:40:48,,0,sql_mode=ORACLE: Derive length of VARCHAR SP parameters with no length from actual parameters,"In sql_mode=oracle, when an SP parameter of the VARCHAR data type is defined without length,
the length should be inherited from the actual argument at call time. This is how Oracle works.

This script works fine in Oracle:
{code:sql}
CREATE OR REPLACE PROCEDURE p1(p OUT VARCHAR)
AS
BEGIN
  p:='0123456789';
END;
/
declare w varchar(10);
begin
  p1(w);
end;
/
{code}


This script:
{code:sql}
declare w varchar(8);
begin
  p1(w);
end;
/
{code}
fails with an error:
{noformat}
ERROR at line 1:
ORA-06502: PL/SQL: numeric or value error: character string buffer too small
{noformat}

Furthermore, since Oracle 9, VARCHAR datatype in PL/SQL is not limited to 4000 char but to 32k. 
It's the size of varchar column in a table that is limited to 4000 (until Oracle 12C which allow 32k when MAX_STRING_SIZE=EXTENDED).
",,"sql_mode=ORACLE: Derive length of VARCHAR SP parameters with no length from actual parameters $end$ In sql_mode=oracle, when an SP parameter of the VARCHAR data type is defined without length,
the length should be inherited from the actual argument at call time. This is how Oracle works.

This script works fine in Oracle:
{code:sql}
CREATE OR REPLACE PROCEDURE p1(p OUT VARCHAR)
AS
BEGIN
  p:='0123456789';
END;
/
declare w varchar(10);
begin
  p1(w);
end;
/
{code}


This script:
{code:sql}
declare w varchar(8);
begin
  p1(w);
end;
/
{code}
fails with an error:
{noformat}
ERROR at line 1:
ORA-06502: PL/SQL: numeric or value error: character string buffer too small
{noformat}

Furthermore, since Oracle 9, VARCHAR datatype in PL/SQL is not limited to 4000 char but to 32k. 
It's the size of varchar column in a table that is limited to 4000 (until Oracle 12C which allow 32k when MAX_STRING_SIZE=EXTENDED).
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,4,,0,1,0,5,0,0,0,,0,850,1,0,0,2017-09-27 13:40:48,sql_mode=ORACLE: Derive length of VARCHAR SP parameters with no length from actual parameters,"In sql_mode=oracle, when an SP parameter of the VARCHAR data type is defined without length,
the length should be inherited from the actual argument at call time. This is how Oracle works.

This script works fine in Oracle:
{code:sql}
CREATE OR REPLACE PROCEDURE p1(p OUT VARCHAR)
AS
BEGIN
  p:='0123456789';
END;
/
declare w varchar(10);
begin
  p1(w);
end;
/
{code}


This script:
{code:sql}
declare w varchar(8);
begin
  p1(w);
end;
/
{code}
fails with an error:
{noformat}
ERROR at line 1:
ORA-06502: PL/SQL: numeric or value error: character string buffer too small
{noformat}

Furthermore, since Oracle 9, VARCHAR datatype in PL/SQL is not limited to 4000 char but to 32k. 
It's the size of varchar column in a table that is limited to 4000 (until Oracle 12C which allow 32k when MAX_STRING_SIZE=EXTENDED).
",,0,0,0,0,0.0,"sql_mode=ORACLE: Derive length of VARCHAR SP parameters with no length from actual parameters $end$ In sql_mode=oracle, when an SP parameter of the VARCHAR data type is defined without length,
the length should be inherited from the actual argument at call time. This is how Oracle works.

This script works fine in Oracle:
{code:sql}
CREATE OR REPLACE PROCEDURE p1(p OUT VARCHAR)
AS
BEGIN
  p:='0123456789';
END;
/
declare w varchar(10);
begin
  p1(w);
end;
/
{code}


This script:
{code:sql}
declare w varchar(8);
begin
  p1(w);
end;
/
{code}
fails with an error:
{noformat}
ERROR at line 1:
ORA-06502: PL/SQL: numeric or value error: character string buffer too small
{noformat}

Furthermore, since Oracle 9, VARCHAR datatype in PL/SQL is not limited to 4000 char but to 32k. 
It's the size of varchar column in a table that is limited to 4000 (until Oracle 12C which allow 32k when MAX_STRING_SIZE=EXTENDED).
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,79,42,0.531646,29,0.367089,27,0.341772,25,0.316456,25,0.316456
685,MDEV-13997,Task,MDEV,2017-10-04 10:14:45,,0,Change Item_bool_rowready_func2 to cache const items at fix time rather than evaluation time,"Currently there is an asymmetry in how binary comparison operators (such as {{=}} or {{>}}, etc) are implemented for temporal vs other data types.

{{Item_bool_rowready_func2}} does not cache its constant arguments of temporal types at fix time. Instead, constant arguments are replaced to instances of {{Item_cache_temporal}} at evaluation time, when {{get_datetime_value()}} is executed for the first time.

Non-temporal types do it in a different way: constant arguments are cached at fix time, while the evaluation time code does not do any argument substitution or caching. Substitution is done in {{Arg_comparator::set_cmp_func_(string|int|real|decimals)()}}, with these two lines:
{code:sql}
  a= cache_converted_constant(thd, a, &a_cache, compare_type_handler());
  b= cache_converted_constant(thd, b, &b_cache, compare_type_handler());
{code}

Under terms of this task we will change the code for the temporal data types to cache constant arguments in fix time, using {{cache_converted_constant()}}, like it is done for the other data types. Caching during evaluation time will be removed.

Rationale:

- We're adding pluggable data types soon. It's better to have symmetric code, to add new data types easier.
- TIMESTAMP should be fixed to do comparison in my_time_t format internally, without conversion to MYSQL_TIME (such conversion can be lossy, see MDEV-13995). It would be nice to make the code symmetric before fixing bugs like MDEV-13995, for simplicity.


",,"Change Item_bool_rowready_func2 to cache const items at fix time rather than evaluation time $end$ Currently there is an asymmetry in how binary comparison operators (such as {{=}} or {{>}}, etc) are implemented for temporal vs other data types.

{{Item_bool_rowready_func2}} does not cache its constant arguments of temporal types at fix time. Instead, constant arguments are replaced to instances of {{Item_cache_temporal}} at evaluation time, when {{get_datetime_value()}} is executed for the first time.

Non-temporal types do it in a different way: constant arguments are cached at fix time, while the evaluation time code does not do any argument substitution or caching. Substitution is done in {{Arg_comparator::set_cmp_func_(string|int|real|decimals)()}}, with these two lines:
{code:sql}
  a= cache_converted_constant(thd, a, &a_cache, compare_type_handler());
  b= cache_converted_constant(thd, b, &b_cache, compare_type_handler());
{code}

Under terms of this task we will change the code for the temporal data types to cache constant arguments in fix time, using {{cache_converted_constant()}}, like it is done for the other data types. Caching during evaluation time will be removed.

Rationale:

- We're adding pluggable data types soon. It's better to have symmetric code, to add new data types easier.
- TIMESTAMP should be fixed to do comparison in my_time_t format internally, without conversion to MYSQL_TIME (such conversion can be lossy, see MDEV-13995). It would be nice to make the code symmetric before fixing bugs like MDEV-13995, for simplicity.


 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,20,,1,0,2,1,0,3,0,,0,850,0,3,0,2017-10-24 16:45:15,Change Item_bool_rowready_func2 to cache const items at fix time rather than evaluation time,"Currently there is an asymmetry in how binary comparison operators (such as {{=}} or {{>}}, etc) are implemented for temporal vs other data types.

{{Item_bool_rowready_func2}} does not cache its constant arguments of temporal types at fix time. Instead, constant arguments are replaced to instances of {{Item_cache_temporal}} at evaluation time, when {{get_datetime_value()}} is executed for the first time.

Non-temporal types do it in a different way: constant arguments are cached at fix time, while the evaluation time code does not do any argument substitution or caching. Substitution is done in {{Arg_comparator::set_cmp_func_(string|int|real|decimals)()}}, with these two lines:
{code:sql}
  a= cache_converted_constant(thd, a, &a_cache, compare_type_handler());
  b= cache_converted_constant(thd, b, &b_cache, compare_type_handler());
{code}

Under terms of this task we will change the code for the temporal data types to cache constant arguments in fix time, using {{cache_converted_constant()}}, like it is done for the other data types. Caching during evaluation time will be removed.

Rationale:

- We're adding pluggable data types soon. It's better to have symmetric code, to add new data types easier.
- TIMESTAMP should be fixed to do comparison in my_time_t format internally, without conversion to MYSQL_TIME (such conversion can be lossy, see MDEV-13995). It would be nice to make the code symmetric before fixing bugs like MDEV-13995, for simplicity.


",,0,0,0,0,0.0,"Change Item_bool_rowready_func2 to cache const items at fix time rather than evaluation time $end$ Currently there is an asymmetry in how binary comparison operators (such as {{=}} or {{>}}, etc) are implemented for temporal vs other data types.

{{Item_bool_rowready_func2}} does not cache its constant arguments of temporal types at fix time. Instead, constant arguments are replaced to instances of {{Item_cache_temporal}} at evaluation time, when {{get_datetime_value()}} is executed for the first time.

Non-temporal types do it in a different way: constant arguments are cached at fix time, while the evaluation time code does not do any argument substitution or caching. Substitution is done in {{Arg_comparator::set_cmp_func_(string|int|real|decimals)()}}, with these two lines:
{code:sql}
  a= cache_converted_constant(thd, a, &a_cache, compare_type_handler());
  b= cache_converted_constant(thd, b, &b_cache, compare_type_handler());
{code}

Under terms of this task we will change the code for the temporal data types to cache constant arguments in fix time, using {{cache_converted_constant()}}, like it is done for the other data types. Caching during evaluation time will be removed.

Rationale:

- We're adding pluggable data types soon. It's better to have symmetric code, to add new data types easier.
- TIMESTAMP should be fixed to do comparison in my_time_t format internally, without conversion to MYSQL_TIME (such conversion can be lossy, see MDEV-13995). It would be nice to make the code symmetric before fixing bugs like MDEV-13995, for simplicity.


 $acceptance criteria:$",0,0,0,0,0,0,0,486.5,80,42,0.525,29,0.3625,27,0.3375,25,0.3125,25,0.3125
686,MDEV-14012,Technical task,MDEV,2017-10-05 19:13:57,,0,sql_mode=Oracle: substr(): treat position 0 as position 1,"When running with sql_mode=ORACLE, MariaDB should treat the second argument (the position) to ""substr"" in Oracle way: If position is 0, then it is treated as 1.

Currently MariaDB returns an empty string in case when the position is 0:
{noformat}
select substr('abc',0,3);
+-------------------+
| substr('abc',0,3) |
+-------------------+
|                   |
+-------------------+
{noformat}

This screenshot from an Oracle session demonstrates that 0 is translated to 1:
{noformat}
SQL> select substr('abc',0,3) from dual;

SUB
---
abc
{noformat}
",,"sql_mode=Oracle: substr(): treat position 0 as position 1 $end$ When running with sql_mode=ORACLE, MariaDB should treat the second argument (the position) to ""substr"" in Oracle way: If position is 0, then it is treated as 1.

Currently MariaDB returns an empty string in case when the position is 0:
{noformat}
select substr('abc',0,3);
+-------------------+
| substr('abc',0,3) |
+-------------------+
|                   |
+-------------------+
{noformat}

This screenshot from an Oracle session demonstrates that 0 is translated to 1:
{noformat}
SQL> select substr('abc',0,3) from dual;

SUB
---
abc
{noformat}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,5,,0,0,1,5,0,0,0,,0,850,0,0,0,2017-10-05 19:13:57,sql_mode=Oracle: substr(): treat position 0 as position 1,"When running with sql_mode=ORACLE, MariaDB should treat the second argument (the position) to ""substr"" in Oracle way: If position is 0, then it is treated as 1.

Currently MariaDB returns an empty string in case when the position is 0:
{noformat}
select substr('abc',0,3);
+-------------------+
| substr('abc',0,3) |
+-------------------+
|                   |
+-------------------+
{noformat}

This screenshot from an Oracle session demonstrates that 0 is translated to 1:
{noformat}
SQL> select substr('abc',0,3) from dual;

SUB
---
abc
{noformat}
",,0,0,0,0,0.0,"sql_mode=Oracle: substr(): treat position 0 as position 1 $end$ When running with sql_mode=ORACLE, MariaDB should treat the second argument (the position) to ""substr"" in Oracle way: If position is 0, then it is treated as 1.

Currently MariaDB returns an empty string in case when the position is 0:
{noformat}
select substr('abc',0,3);
+-------------------+
| substr('abc',0,3) |
+-------------------+
|                   |
+-------------------+
{noformat}

This screenshot from an Oracle session demonstrates that 0 is translated to 1:
{noformat}
SQL> select substr('abc',0,3) from dual;

SUB
---
abc
{noformat}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,81,42,0.518519,29,0.358025,27,0.333333,25,0.308642,25,0.308642
687,MDEV-14013,Technical task,MDEV,2017-10-05 19:24:58,,0,sql_mode=EMPTY_STRING_IS_NULL,"In order to emulate Oracle's behavior in handling empty strings as NULL, we'll add a new sql_mode flag {{EMPTY_STRING_IS_NULL}}.
When this flag is set, we will:
- translate Item_string created in the parser to Item_null
- translate binding an empty string as prepared statement parameters to binding NULL

Note, more {{NULL}} handling flags will be added later under terms of separate patch:
- translating empty strings in function return values to NULL
- handling empty strings as equal to NULL in comparison operators

This task is only about literals and PS parameters.

The new flag will be disabled by default.
The new flag will NOT be a part of sql_mode=ORACLE.

In order to activate this behavior, one will have to do:
{code:sql}
SET sql_mode='ORACLE,EMPTY_STRING_IS_NULL';
{code}
",,"sql_mode=EMPTY_STRING_IS_NULL $end$ In order to emulate Oracle's behavior in handling empty strings as NULL, we'll add a new sql_mode flag {{EMPTY_STRING_IS_NULL}}.
When this flag is set, we will:
- translate Item_string created in the parser to Item_null
- translate binding an empty string as prepared statement parameters to binding NULL

Note, more {{NULL}} handling flags will be added later under terms of separate patch:
- translating empty strings in function return values to NULL
- handling empty strings as equal to NULL in comparison operators

This task is only about literals and PS parameters.

The new flag will be disabled by default.
The new flag will NOT be a part of sql_mode=ORACLE.

In order to activate this behavior, one will have to do:
{code:sql}
SET sql_mode='ORACLE,EMPTY_STRING_IS_NULL';
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,6,,3,6,3,5,0,0,0,,0,850,6,0,0,2017-10-05 19:24:58,sql_mode=EMPTY_STRING_IS_NULL,"In order to emulate Oracle's behavior in handling empty strings as NULL, we'll add a new sql_mode flag {{EMPTY_STRING_IS_NULL}}.
When this flag is set, we will:
- translate Item_string created in the parser to Item_null
- translate binding an empty string as prepared statement parameters to binding NULL

Note, more {{NULL}} handling flags will be added later under terms of separate patch:
- translating empty strings in function return values to NULL
- handling empty strings as equal to NULL in comparison operators

This task is only about literals and PS parameters.

The new flag will be disabled by default.
The new flag will NOT be a part of sql_mode=ORACLE.

In order to activate this behavior, one will have to do:
{code:sql}
SET sql_mode='ORACLE,EMPTY_STRING_IS_NULL';
{code}
",,0,0,0,0,0.0,"sql_mode=EMPTY_STRING_IS_NULL $end$ In order to emulate Oracle's behavior in handling empty strings as NULL, we'll add a new sql_mode flag {{EMPTY_STRING_IS_NULL}}.
When this flag is set, we will:
- translate Item_string created in the parser to Item_null
- translate binding an empty string as prepared statement parameters to binding NULL

Note, more {{NULL}} handling flags will be added later under terms of separate patch:
- translating empty strings in function return values to NULL
- handling empty strings as equal to NULL in comparison operators

This task is only about literals and PS parameters.

The new flag will be disabled by default.
The new flag will NOT be a part of sql_mode=ORACLE.

In order to activate this behavior, one will have to do:
{code:sql}
SET sql_mode='ORACLE,EMPTY_STRING_IS_NULL';
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,82,42,0.512195,29,0.353659,27,0.329268,25,0.304878,25,0.304878
688,MDEV-14024,Task,MDEV,2017-10-06 18:17:00,,0,PCRE2,"PCRE1 is in maintenance mode, some bugs aren't getting fixed (see https://bugs.exim.org/show_bug.cgi?id=2173). The current PCRE branch is PCRE2, we need to migrate to it to continue getting our reported bugs fixed.",,"PCRE2 $end$ PCRE1 is in maintenance mode, some bugs aren't getting fixed (see https://bugs.exim.org/show_bug.cgi?id=2173). The current PCRE branch is PCRE2, we need to migrate to it to continue getting our reported bugs fixed. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,26,,3,7,4,1,0,0,0,,0,850,7,0,0,2017-10-24 17:53:57,PCRE2,"PCRE1 is in maintenance mode, some bugs aren't getting fixed (see https://bugs.exim.org/show_bug.cgi?id=2173). The current PCRE branch is PCRE2, we need to migrate to it to continue getting our reported bugs fixed.",,0,0,0,0,0.0,"PCRE2 $end$ PCRE1 is in maintenance mode, some bugs aren't getting fixed (see https://bugs.exim.org/show_bug.cgi?id=2173). The current PCRE branch is PCRE2, we need to migrate to it to continue getting our reported bugs fixed. $acceptance criteria:$",0,0,0,0,0,0,0,431.6,51,26,0.509804,20,0.392157,15,0.294118,13,0.254902,10,0.196078
689,MDEV-14113,Task,MDEV,2017-10-24 07:25:58,,0,MS: Add linger option on TCP sockets,"{noformat}
From 04a3482b9c5772cd3a82a4aec51976cd2e3e686d Mon Sep 17 00:00:00 2001
From: Shuode Li <shuodl@microsoft.com>
Date: Tue, 19 Sep 2017 01:47:19 +0000
Subject: [PATCH] Merged PR 55490: Add linger option on TCP sockets.

Add linger option on TCP sockets.

Related work items: #73276
---
 CMakeLists.txt                       |  5 +++++
 include/mysql_com.h                  |  4 ++++
 mysql-test/r/mysqld--help-win.result |  6 ++++++
 sql/mysqld.cc                        | 10 ++++++++++
 sql/mysqld.h                         |  5 +++++
 sql/sys_vars.cc                      | 15 +++++++++++++++
 tools/cmake.cmd                      |  4 ++++
 7 files changed, 49 insertions(+)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 90a31a48303..aa3620a07c8 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -462,6 +462,11 @@ OPTION(DISABLE_AZURE_REPLICATION
 IF(DISABLE_AZURE_REPLICATION)
   ADD_DEFINITIONS(-DDISABLE_AZURE_REPLICATION)
 ENDIF()
+OPTION(DISABLE_TCP_LINGER
+  ""disable tcp linger options"" OFF)
+IF(DISABLE_TCP_LINGER)
+  ADD_DEFINITIONS(-DDISABLE_TCP_LINGER)
+ENDIF()

 OPTION(DISABLE_USER_NAME_CHECK
   ""disable user name check"" OFF)
diff --git a/include/mysql_com.h b/include/mysql_com.h
index 8f603d07322..8b527e91e92 100644
--- a/include/mysql_com.h
+++ b/include/mysql_com.h
@@ -309,6 +309,10 @@ enum enum_server_command
 #define NET_WRITE_TIMEOUT	60		/* Timeout on write */
 #define NET_WAIT_TIMEOUT	8*60*60		/* Wait for new query */

+#ifndef DISABLE_TCP_LINGER
+#define TCP_LINGER_TIMEOUT	10
+#endif
+
 #define ONLY_KILL_QUERY         1


diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index aee358bc2b2..351ea82a1f3 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -1026,6 +1026,10 @@ The following options may be given as the first argument:
  --tc-heuristic-recover=name
  Decision to use in heuristic recover process. Possible
  values are COMMIT or ROLLBACK.
+ --tcp-linger        The number of seconds the server tcp linger timeout
+ (Defaults to on; use --skip-tcp-linger to disable.)
+ --tcp-linger-timeout[=#]
+ The number of seconds the server tcp linger timeout
  --thread-cache-size=#
  How many threads we should keep in a cache for reuse
  --thread-handling=name
@@ -1362,6 +1366,8 @@ sync-relay-log-info 10000
 sysdate-is-now FALSE
 table-open-cache-instances 1
 tc-heuristic-recover COMMIT
+tcp-linger TRUE
+tcp-linger-timeout 10
 thread-cache-size 9
 thread-handling one-thread-per-connection
 thread-stack 262144
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index 151ffc5fcd2..0ceb4d037f2 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -588,6 +588,11 @@ ulong binlog_cache_use= 0, binlog_cache_disk_use= 0;
 ulong binlog_stmt_cache_use= 0, binlog_stmt_cache_disk_use= 0;
 ulong max_connections, max_connect_errors;

+#ifndef DISABLE_TCP_LINGER
+my_bool tcp_linger_onoff;
+uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 ulong max_super_acl_connections;    /* The variable define the max connection number of super user */
 #endif
@@ -6499,6 +6504,11 @@ void handle_connections_sockets()
         sleep(1);       // Give other threads some time
       continue;
     }
+
+#ifndef DISABLE_TCP_LINGER
+    struct linger ling = {tcp_linger_onoff, tcp_linger_timeout};
+    mysql_socket_setsockopt(new_sock, SOL_SOCKET, SO_LINGER, (char *)&ling, sizeof(ling));
+#endif

 #ifdef HAVE_LIBWRAP
     {
diff --git a/sql/mysqld.h b/sql/mysqld.h
index 9bd87bab9aa..b5f6b118f2d 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -222,6 +222,11 @@ extern MYSQL_PLUGIN_IMPORT ulong max_connections;
 extern ulong max_digest_length;
 extern ulong max_connect_errors, connect_timeout;

+#ifndef DISABLE_TCP_LINGER
+extern my_bool tcp_linger_onoff;
+extern uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 extern ulong max_super_acl_connections;
 #endif
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index eba5e9005aa..1753fa016ef 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -3205,6 +3205,21 @@ static Sys_var_ulong Sys_net_wait_timeout(
        VALID_RANGE(1, IF_WIN(INT_MAX32/1000, LONG_TIMEOUT)),
        DEFAULT(NET_WAIT_TIMEOUT), BLOCK_SIZE(1));

+#ifndef DISABLE_TCP_LINGER
+static Sys_var_mybool Sys_tcp_linger_onoff(
+       ""tcp_linger"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_onoff), CMD_LINE(OPT_ARG), DEFAULT(TRUE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG);
+
+static Sys_var_uint Sys_tcp_linger_timout(
+       ""tcp_linger_timeout"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_timeout), CMD_LINE(OPT_ARG),
+       VALID_RANGE(1, 65535U),
+       DEFAULT(TCP_LINGER_TIMEOUT), BLOCK_SIZE(1));
+#endif
+
 static Sys_var_plugin Sys_default_storage_engine(
        ""default_storage_engine"", ""The default storage engine for new tables"",
        SESSION_VAR(table_plugin), NO_CMD_LINE,
diff --git a/tools/cmake.cmd b/tools/cmake.cmd
index 0bcbc9db4fc..a48f447d257 100644
--- a/tools/cmake.cmd
+++ b/tools/cmake.cmd
@@ -68,6 +68,10 @@ IF NOT ""x%plugin:dazurerepl=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_AZURE_REPLICATION=ON
 )

+IF NOT ""x%plugin:etcplinger=%""==""x%plugin%"" (
+    SET cmakeargs=%cmakeargs% -DDISABLE_TCP_LINGER=ON
+)
+
 IF NOT ""x%plugin:dusernamecheck=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_USER_NAME_CHECK=ON
 )
{noformat}",,"MS: Add linger option on TCP sockets $end$ {noformat}
From 04a3482b9c5772cd3a82a4aec51976cd2e3e686d Mon Sep 17 00:00:00 2001
From: Shuode Li <shuodl@microsoft.com>
Date: Tue, 19 Sep 2017 01:47:19 +0000
Subject: [PATCH] Merged PR 55490: Add linger option on TCP sockets.

Add linger option on TCP sockets.

Related work items: #73276
---
 CMakeLists.txt                       |  5 +++++
 include/mysql_com.h                  |  4 ++++
 mysql-test/r/mysqld--help-win.result |  6 ++++++
 sql/mysqld.cc                        | 10 ++++++++++
 sql/mysqld.h                         |  5 +++++
 sql/sys_vars.cc                      | 15 +++++++++++++++
 tools/cmake.cmd                      |  4 ++++
 7 files changed, 49 insertions(+)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 90a31a48303..aa3620a07c8 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -462,6 +462,11 @@ OPTION(DISABLE_AZURE_REPLICATION
 IF(DISABLE_AZURE_REPLICATION)
   ADD_DEFINITIONS(-DDISABLE_AZURE_REPLICATION)
 ENDIF()
+OPTION(DISABLE_TCP_LINGER
+  ""disable tcp linger options"" OFF)
+IF(DISABLE_TCP_LINGER)
+  ADD_DEFINITIONS(-DDISABLE_TCP_LINGER)
+ENDIF()

 OPTION(DISABLE_USER_NAME_CHECK
   ""disable user name check"" OFF)
diff --git a/include/mysql_com.h b/include/mysql_com.h
index 8f603d07322..8b527e91e92 100644
--- a/include/mysql_com.h
+++ b/include/mysql_com.h
@@ -309,6 +309,10 @@ enum enum_server_command
 #define NET_WRITE_TIMEOUT	60		/* Timeout on write */
 #define NET_WAIT_TIMEOUT	8*60*60		/* Wait for new query */

+#ifndef DISABLE_TCP_LINGER
+#define TCP_LINGER_TIMEOUT	10
+#endif
+
 #define ONLY_KILL_QUERY         1


diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index aee358bc2b2..351ea82a1f3 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -1026,6 +1026,10 @@ The following options may be given as the first argument:
  --tc-heuristic-recover=name
  Decision to use in heuristic recover process. Possible
  values are COMMIT or ROLLBACK.
+ --tcp-linger        The number of seconds the server tcp linger timeout
+ (Defaults to on; use --skip-tcp-linger to disable.)
+ --tcp-linger-timeout[=#]
+ The number of seconds the server tcp linger timeout
  --thread-cache-size=#
  How many threads we should keep in a cache for reuse
  --thread-handling=name
@@ -1362,6 +1366,8 @@ sync-relay-log-info 10000
 sysdate-is-now FALSE
 table-open-cache-instances 1
 tc-heuristic-recover COMMIT
+tcp-linger TRUE
+tcp-linger-timeout 10
 thread-cache-size 9
 thread-handling one-thread-per-connection
 thread-stack 262144
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index 151ffc5fcd2..0ceb4d037f2 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -588,6 +588,11 @@ ulong binlog_cache_use= 0, binlog_cache_disk_use= 0;
 ulong binlog_stmt_cache_use= 0, binlog_stmt_cache_disk_use= 0;
 ulong max_connections, max_connect_errors;

+#ifndef DISABLE_TCP_LINGER
+my_bool tcp_linger_onoff;
+uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 ulong max_super_acl_connections;    /* The variable define the max connection number of super user */
 #endif
@@ -6499,6 +6504,11 @@ void handle_connections_sockets()
         sleep(1);       // Give other threads some time
       continue;
     }
+
+#ifndef DISABLE_TCP_LINGER
+    struct linger ling = {tcp_linger_onoff, tcp_linger_timeout};
+    mysql_socket_setsockopt(new_sock, SOL_SOCKET, SO_LINGER, (char *)&ling, sizeof(ling));
+#endif

 #ifdef HAVE_LIBWRAP
     {
diff --git a/sql/mysqld.h b/sql/mysqld.h
index 9bd87bab9aa..b5f6b118f2d 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -222,6 +222,11 @@ extern MYSQL_PLUGIN_IMPORT ulong max_connections;
 extern ulong max_digest_length;
 extern ulong max_connect_errors, connect_timeout;

+#ifndef DISABLE_TCP_LINGER
+extern my_bool tcp_linger_onoff;
+extern uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 extern ulong max_super_acl_connections;
 #endif
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index eba5e9005aa..1753fa016ef 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -3205,6 +3205,21 @@ static Sys_var_ulong Sys_net_wait_timeout(
        VALID_RANGE(1, IF_WIN(INT_MAX32/1000, LONG_TIMEOUT)),
        DEFAULT(NET_WAIT_TIMEOUT), BLOCK_SIZE(1));

+#ifndef DISABLE_TCP_LINGER
+static Sys_var_mybool Sys_tcp_linger_onoff(
+       ""tcp_linger"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_onoff), CMD_LINE(OPT_ARG), DEFAULT(TRUE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG);
+
+static Sys_var_uint Sys_tcp_linger_timout(
+       ""tcp_linger_timeout"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_timeout), CMD_LINE(OPT_ARG),
+       VALID_RANGE(1, 65535U),
+       DEFAULT(TCP_LINGER_TIMEOUT), BLOCK_SIZE(1));
+#endif
+
 static Sys_var_plugin Sys_default_storage_engine(
        ""default_storage_engine"", ""The default storage engine for new tables"",
        SESSION_VAR(table_plugin), NO_CMD_LINE,
diff --git a/tools/cmake.cmd b/tools/cmake.cmd
index 0bcbc9db4fc..a48f447d257 100644
--- a/tools/cmake.cmd
+++ b/tools/cmake.cmd
@@ -68,6 +68,10 @@ IF NOT ""x%plugin:dazurerepl=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_AZURE_REPLICATION=ON
 )

+IF NOT ""x%plugin:etcplinger=%""==""x%plugin%"" (
+    SET cmakeargs=%cmakeargs% -DDISABLE_TCP_LINGER=ON
+)
+
 IF NOT ""x%plugin:dusernamecheck=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_USER_NAME_CHECK=ON
 )
{noformat} $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,7,,0,11,0,1,0,0,0,,0,850,10,0,0,2017-10-24 16:55:38,MS: Add linger option on TCP sockets,"{noformat}
From 04a3482b9c5772cd3a82a4aec51976cd2e3e686d Mon Sep 17 00:00:00 2001
From: Shuode Li <shuodl@microsoft.com>
Date: Tue, 19 Sep 2017 01:47:19 +0000
Subject: [PATCH] Merged PR 55490: Add linger option on TCP sockets.

Add linger option on TCP sockets.

Related work items: #73276
---
 CMakeLists.txt                       |  5 +++++
 include/mysql_com.h                  |  4 ++++
 mysql-test/r/mysqld--help-win.result |  6 ++++++
 sql/mysqld.cc                        | 10 ++++++++++
 sql/mysqld.h                         |  5 +++++
 sql/sys_vars.cc                      | 15 +++++++++++++++
 tools/cmake.cmd                      |  4 ++++
 7 files changed, 49 insertions(+)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 90a31a48303..aa3620a07c8 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -462,6 +462,11 @@ OPTION(DISABLE_AZURE_REPLICATION
 IF(DISABLE_AZURE_REPLICATION)
   ADD_DEFINITIONS(-DDISABLE_AZURE_REPLICATION)
 ENDIF()
+OPTION(DISABLE_TCP_LINGER
+  ""disable tcp linger options"" OFF)
+IF(DISABLE_TCP_LINGER)
+  ADD_DEFINITIONS(-DDISABLE_TCP_LINGER)
+ENDIF()

 OPTION(DISABLE_USER_NAME_CHECK
   ""disable user name check"" OFF)
diff --git a/include/mysql_com.h b/include/mysql_com.h
index 8f603d07322..8b527e91e92 100644
--- a/include/mysql_com.h
+++ b/include/mysql_com.h
@@ -309,6 +309,10 @@ enum enum_server_command
 #define NET_WRITE_TIMEOUT	60		/* Timeout on write */
 #define NET_WAIT_TIMEOUT	8*60*60		/* Wait for new query */

+#ifndef DISABLE_TCP_LINGER
+#define TCP_LINGER_TIMEOUT	10
+#endif
+
 #define ONLY_KILL_QUERY         1


diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index aee358bc2b2..351ea82a1f3 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -1026,6 +1026,10 @@ The following options may be given as the first argument:
  --tc-heuristic-recover=name
  Decision to use in heuristic recover process. Possible
  values are COMMIT or ROLLBACK.
+ --tcp-linger        The number of seconds the server tcp linger timeout
+ (Defaults to on; use --skip-tcp-linger to disable.)
+ --tcp-linger-timeout[=#]
+ The number of seconds the server tcp linger timeout
  --thread-cache-size=#
  How many threads we should keep in a cache for reuse
  --thread-handling=name
@@ -1362,6 +1366,8 @@ sync-relay-log-info 10000
 sysdate-is-now FALSE
 table-open-cache-instances 1
 tc-heuristic-recover COMMIT
+tcp-linger TRUE
+tcp-linger-timeout 10
 thread-cache-size 9
 thread-handling one-thread-per-connection
 thread-stack 262144
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index 151ffc5fcd2..0ceb4d037f2 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -588,6 +588,11 @@ ulong binlog_cache_use= 0, binlog_cache_disk_use= 0;
 ulong binlog_stmt_cache_use= 0, binlog_stmt_cache_disk_use= 0;
 ulong max_connections, max_connect_errors;

+#ifndef DISABLE_TCP_LINGER
+my_bool tcp_linger_onoff;
+uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 ulong max_super_acl_connections;    /* The variable define the max connection number of super user */
 #endif
@@ -6499,6 +6504,11 @@ void handle_connections_sockets()
         sleep(1);       // Give other threads some time
       continue;
     }
+
+#ifndef DISABLE_TCP_LINGER
+    struct linger ling = {tcp_linger_onoff, tcp_linger_timeout};
+    mysql_socket_setsockopt(new_sock, SOL_SOCKET, SO_LINGER, (char *)&ling, sizeof(ling));
+#endif

 #ifdef HAVE_LIBWRAP
     {
diff --git a/sql/mysqld.h b/sql/mysqld.h
index 9bd87bab9aa..b5f6b118f2d 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -222,6 +222,11 @@ extern MYSQL_PLUGIN_IMPORT ulong max_connections;
 extern ulong max_digest_length;
 extern ulong max_connect_errors, connect_timeout;

+#ifndef DISABLE_TCP_LINGER
+extern my_bool tcp_linger_onoff;
+extern uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 extern ulong max_super_acl_connections;
 #endif
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index eba5e9005aa..1753fa016ef 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -3205,6 +3205,21 @@ static Sys_var_ulong Sys_net_wait_timeout(
        VALID_RANGE(1, IF_WIN(INT_MAX32/1000, LONG_TIMEOUT)),
        DEFAULT(NET_WAIT_TIMEOUT), BLOCK_SIZE(1));

+#ifndef DISABLE_TCP_LINGER
+static Sys_var_mybool Sys_tcp_linger_onoff(
+       ""tcp_linger"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_onoff), CMD_LINE(OPT_ARG), DEFAULT(TRUE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG);
+
+static Sys_var_uint Sys_tcp_linger_timout(
+       ""tcp_linger_timeout"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_timeout), CMD_LINE(OPT_ARG),
+       VALID_RANGE(1, 65535U),
+       DEFAULT(TCP_LINGER_TIMEOUT), BLOCK_SIZE(1));
+#endif
+
 static Sys_var_plugin Sys_default_storage_engine(
        ""default_storage_engine"", ""The default storage engine for new tables"",
        SESSION_VAR(table_plugin), NO_CMD_LINE,
diff --git a/tools/cmake.cmd b/tools/cmake.cmd
index 0bcbc9db4fc..a48f447d257 100644
--- a/tools/cmake.cmd
+++ b/tools/cmake.cmd
@@ -68,6 +68,10 @@ IF NOT ""x%plugin:dazurerepl=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_AZURE_REPLICATION=ON
 )

+IF NOT ""x%plugin:etcplinger=%""==""x%plugin%"" (
+    SET cmakeargs=%cmakeargs% -DDISABLE_TCP_LINGER=ON
+)
+
 IF NOT ""x%plugin:dusernamecheck=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_USER_NAME_CHECK=ON
 )
{noformat}",,0,0,0,0,0.0,"MS: Add linger option on TCP sockets $end$ {noformat}
From 04a3482b9c5772cd3a82a4aec51976cd2e3e686d Mon Sep 17 00:00:00 2001
From: Shuode Li <shuodl@microsoft.com>
Date: Tue, 19 Sep 2017 01:47:19 +0000
Subject: [PATCH] Merged PR 55490: Add linger option on TCP sockets.

Add linger option on TCP sockets.

Related work items: #73276
---
 CMakeLists.txt                       |  5 +++++
 include/mysql_com.h                  |  4 ++++
 mysql-test/r/mysqld--help-win.result |  6 ++++++
 sql/mysqld.cc                        | 10 ++++++++++
 sql/mysqld.h                         |  5 +++++
 sql/sys_vars.cc                      | 15 +++++++++++++++
 tools/cmake.cmd                      |  4 ++++
 7 files changed, 49 insertions(+)

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 90a31a48303..aa3620a07c8 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -462,6 +462,11 @@ OPTION(DISABLE_AZURE_REPLICATION
 IF(DISABLE_AZURE_REPLICATION)
   ADD_DEFINITIONS(-DDISABLE_AZURE_REPLICATION)
 ENDIF()
+OPTION(DISABLE_TCP_LINGER
+  ""disable tcp linger options"" OFF)
+IF(DISABLE_TCP_LINGER)
+  ADD_DEFINITIONS(-DDISABLE_TCP_LINGER)
+ENDIF()

 OPTION(DISABLE_USER_NAME_CHECK
   ""disable user name check"" OFF)
diff --git a/include/mysql_com.h b/include/mysql_com.h
index 8f603d07322..8b527e91e92 100644
--- a/include/mysql_com.h
+++ b/include/mysql_com.h
@@ -309,6 +309,10 @@ enum enum_server_command
 #define NET_WRITE_TIMEOUT	60		/* Timeout on write */
 #define NET_WAIT_TIMEOUT	8*60*60		/* Wait for new query */

+#ifndef DISABLE_TCP_LINGER
+#define TCP_LINGER_TIMEOUT	10
+#endif
+
 #define ONLY_KILL_QUERY         1


diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index aee358bc2b2..351ea82a1f3 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -1026,6 +1026,10 @@ The following options may be given as the first argument:
  --tc-heuristic-recover=name
  Decision to use in heuristic recover process. Possible
  values are COMMIT or ROLLBACK.
+ --tcp-linger        The number of seconds the server tcp linger timeout
+ (Defaults to on; use --skip-tcp-linger to disable.)
+ --tcp-linger-timeout[=#]
+ The number of seconds the server tcp linger timeout
  --thread-cache-size=#
  How many threads we should keep in a cache for reuse
  --thread-handling=name
@@ -1362,6 +1366,8 @@ sync-relay-log-info 10000
 sysdate-is-now FALSE
 table-open-cache-instances 1
 tc-heuristic-recover COMMIT
+tcp-linger TRUE
+tcp-linger-timeout 10
 thread-cache-size 9
 thread-handling one-thread-per-connection
 thread-stack 262144
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index 151ffc5fcd2..0ceb4d037f2 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -588,6 +588,11 @@ ulong binlog_cache_use= 0, binlog_cache_disk_use= 0;
 ulong binlog_stmt_cache_use= 0, binlog_stmt_cache_disk_use= 0;
 ulong max_connections, max_connect_errors;

+#ifndef DISABLE_TCP_LINGER
+my_bool tcp_linger_onoff;
+uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 ulong max_super_acl_connections;    /* The variable define the max connection number of super user */
 #endif
@@ -6499,6 +6504,11 @@ void handle_connections_sockets()
         sleep(1);       // Give other threads some time
       continue;
     }
+
+#ifndef DISABLE_TCP_LINGER
+    struct linger ling = {tcp_linger_onoff, tcp_linger_timeout};
+    mysql_socket_setsockopt(new_sock, SOL_SOCKET, SO_LINGER, (char *)&ling, sizeof(ling));
+#endif

 #ifdef HAVE_LIBWRAP
     {
diff --git a/sql/mysqld.h b/sql/mysqld.h
index 9bd87bab9aa..b5f6b118f2d 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -222,6 +222,11 @@ extern MYSQL_PLUGIN_IMPORT ulong max_connections;
 extern ulong max_digest_length;
 extern ulong max_connect_errors, connect_timeout;

+#ifndef DISABLE_TCP_LINGER
+extern my_bool tcp_linger_onoff;
+extern uint tcp_linger_timeout;
+#endif
+
 #ifndef DISABLE_SUPER_ACL_CONNECTION
 extern ulong max_super_acl_connections;
 #endif
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index eba5e9005aa..1753fa016ef 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -3205,6 +3205,21 @@ static Sys_var_ulong Sys_net_wait_timeout(
        VALID_RANGE(1, IF_WIN(INT_MAX32/1000, LONG_TIMEOUT)),
        DEFAULT(NET_WAIT_TIMEOUT), BLOCK_SIZE(1));

+#ifndef DISABLE_TCP_LINGER
+static Sys_var_mybool Sys_tcp_linger_onoff(
+       ""tcp_linger"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_onoff), CMD_LINE(OPT_ARG), DEFAULT(TRUE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG);
+
+static Sys_var_uint Sys_tcp_linger_timout(
+       ""tcp_linger_timeout"",
+       ""The number of seconds the server tcp linger timeout"",
+       GLOBAL_VAR(tcp_linger_timeout), CMD_LINE(OPT_ARG),
+       VALID_RANGE(1, 65535U),
+       DEFAULT(TCP_LINGER_TIMEOUT), BLOCK_SIZE(1));
+#endif
+
 static Sys_var_plugin Sys_default_storage_engine(
        ""default_storage_engine"", ""The default storage engine for new tables"",
        SESSION_VAR(table_plugin), NO_CMD_LINE,
diff --git a/tools/cmake.cmd b/tools/cmake.cmd
index 0bcbc9db4fc..a48f447d257 100644
--- a/tools/cmake.cmd
+++ b/tools/cmake.cmd
@@ -68,6 +68,10 @@ IF NOT ""x%plugin:dazurerepl=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_AZURE_REPLICATION=ON
 )

+IF NOT ""x%plugin:etcplinger=%""==""x%plugin%"" (
+    SET cmakeargs=%cmakeargs% -DDISABLE_TCP_LINGER=ON
+)
+
 IF NOT ""x%plugin:dusernamecheck=%""==""x%plugin%"" (
     SET cmakeargs=%cmakeargs% -DDISABLE_USER_NAME_CHECK=ON
 )
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,9.48333,23,3,0.130435,2,0.0869565,2,0.0869565,1,0.0434783,1,0.0434783
690,MDEV-14114,Task,MDEV,2017-10-24 08:47:27,,0,MS: Introduce a variable to init binlog cache size,"{noformat}
From f8ea0478200bf115cb0abdbf28cdf339e268a0f6 Mon Sep 17 00:00:00 2001
From: Hanzhi Wang <hanzhiwa@microsoft.com>
Date: Wed, 13 Sep 2017 07:13:49 +0000
Subject: [PATCH] Merged PR 60892: Introduce a variable to init binlog cache
 size

Introduce a variable to init binlog cache size

Related work items: #84138
---
 mysql-test/r/mysqld--help-win.result | 3 +++
 sql/binlog.cc                        | 2 +-
 sql/mysqld.cc                        | 1 +
 sql/mysqld.h                         | 1 +
 sql/sys_vars.cc                      | 8 ++++++++
 5 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index a6f2ae76188..e866f63c5a7 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -102,6 +102,8 @@ The following options may be given as the first argument:
  --binlog-ignore-db=name
  Tells the master that updates to the given database
  should not be logged to the binary log.
+ --binlog-init-cache-size=#
+ The size of init malloc cache for the binary log.
  --binlog-max-flush-queue-time=#
  The maximum time that the binary log group commit will
  keep reading transactions before it flush the
@@ -1084,6 +1086,7 @@ binlog-direct-non-transactional-updates FALSE
 binlog-error-action IGNORE_ERROR
 binlog-format STATEMENT
 binlog-gtid-simple-recovery FALSE
+binlog-init-cache-size 16384
 binlog-max-flush-queue-time 0
 binlog-order-commits TRUE
 binlog-row-event-max-size 8192
diff --git a/sql/binlog.cc b/sql/binlog.cc
index 8d3cb51b429..f092d283e73 100644
--- a/sql/binlog.cc
+++ b/sql/binlog.cc
@@ -1951,7 +1951,7 @@ File open_binlog_file(IO_CACHE *log, const char *log_file_name, const char **err
     *errmsg = ""Could not open log file"";
     goto err;
   }
-  if (init_io_cache(log, file, IO_SIZE*2, READ_CACHE, 0, 0,
+  if (init_io_cache(log, file, binlog_init_cache_size, READ_CACHE, 0, 0,
                     MYF(MY_WME|MY_DONT_CHECK_FILESIZE)))
   {
     sql_print_error(""Failed to create a cache on log (file '%s')"",
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index eba6fdec801..03839e6e742 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -562,6 +562,7 @@ ulonglong slave_rows_search_algorithms_options;
 #ifndef DBUG_OFF
 uint slave_rows_last_search_algorithm_used;
 #endif
+ulong binlog_init_cache_size=0;
 ulong binlog_cache_size=0;
 ulonglong  max_binlog_cache_size=0;
 ulong slave_max_allowed_packet= 0;
diff --git a/sql/mysqld.h b/sql/mysqld.h
index d814a8ff28a..e73c1c00230 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -236,6 +236,7 @@ extern my_bool log_bin_use_v1_row_events;
 extern ulong what_to_log,flush_time;
 extern ulong max_prepared_stmt_count, prepared_stmt_count;
 extern ulong open_files_limit;
+extern ulong binlog_init_cache_size;
 extern ulong binlog_cache_size, binlog_stmt_cache_size;
 extern ulonglong max_binlog_cache_size, max_binlog_stmt_cache_size;
 extern int32 opt_binlog_max_flush_queue_time;
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index 236afb2d356..3108cbfc5b5 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -602,6 +602,14 @@ static bool fix_binlog_stmt_cache_size(sys_var *self, THD *thd, enum_var_type ty
   return false;
 }

+static Sys_var_ulong Sys_binlog_init_cache_size(
+       ""binlog_init_cache_size"", ""The size of init malloc cache for the binary log."",
+       GLOBAL_VAR(binlog_init_cache_size),
+       CMD_LINE(REQUIRED_ARG),
+       VALID_RANGE(IO_SIZE*2, ULONG_MAX), DEFAULT(IO_SIZE*4), BLOCK_SIZE(IO_SIZE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG, ON_CHECK(0),
+       ON_UPDATE(0));
+
 static Sys_var_ulong Sys_binlog_cache_size(
        ""binlog_cache_size"", ""The size of the transactional cache for ""
        ""updates to transactional engines for the binary log. ""
{noformat}",,"MS: Introduce a variable to init binlog cache size $end$ {noformat}
From f8ea0478200bf115cb0abdbf28cdf339e268a0f6 Mon Sep 17 00:00:00 2001
From: Hanzhi Wang <hanzhiwa@microsoft.com>
Date: Wed, 13 Sep 2017 07:13:49 +0000
Subject: [PATCH] Merged PR 60892: Introduce a variable to init binlog cache
 size

Introduce a variable to init binlog cache size

Related work items: #84138
---
 mysql-test/r/mysqld--help-win.result | 3 +++
 sql/binlog.cc                        | 2 +-
 sql/mysqld.cc                        | 1 +
 sql/mysqld.h                         | 1 +
 sql/sys_vars.cc                      | 8 ++++++++
 5 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index a6f2ae76188..e866f63c5a7 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -102,6 +102,8 @@ The following options may be given as the first argument:
  --binlog-ignore-db=name
  Tells the master that updates to the given database
  should not be logged to the binary log.
+ --binlog-init-cache-size=#
+ The size of init malloc cache for the binary log.
  --binlog-max-flush-queue-time=#
  The maximum time that the binary log group commit will
  keep reading transactions before it flush the
@@ -1084,6 +1086,7 @@ binlog-direct-non-transactional-updates FALSE
 binlog-error-action IGNORE_ERROR
 binlog-format STATEMENT
 binlog-gtid-simple-recovery FALSE
+binlog-init-cache-size 16384
 binlog-max-flush-queue-time 0
 binlog-order-commits TRUE
 binlog-row-event-max-size 8192
diff --git a/sql/binlog.cc b/sql/binlog.cc
index 8d3cb51b429..f092d283e73 100644
--- a/sql/binlog.cc
+++ b/sql/binlog.cc
@@ -1951,7 +1951,7 @@ File open_binlog_file(IO_CACHE *log, const char *log_file_name, const char **err
     *errmsg = ""Could not open log file"";
     goto err;
   }
-  if (init_io_cache(log, file, IO_SIZE*2, READ_CACHE, 0, 0,
+  if (init_io_cache(log, file, binlog_init_cache_size, READ_CACHE, 0, 0,
                     MYF(MY_WME|MY_DONT_CHECK_FILESIZE)))
   {
     sql_print_error(""Failed to create a cache on log (file '%s')"",
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index eba6fdec801..03839e6e742 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -562,6 +562,7 @@ ulonglong slave_rows_search_algorithms_options;
 #ifndef DBUG_OFF
 uint slave_rows_last_search_algorithm_used;
 #endif
+ulong binlog_init_cache_size=0;
 ulong binlog_cache_size=0;
 ulonglong  max_binlog_cache_size=0;
 ulong slave_max_allowed_packet= 0;
diff --git a/sql/mysqld.h b/sql/mysqld.h
index d814a8ff28a..e73c1c00230 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -236,6 +236,7 @@ extern my_bool log_bin_use_v1_row_events;
 extern ulong what_to_log,flush_time;
 extern ulong max_prepared_stmt_count, prepared_stmt_count;
 extern ulong open_files_limit;
+extern ulong binlog_init_cache_size;
 extern ulong binlog_cache_size, binlog_stmt_cache_size;
 extern ulonglong max_binlog_cache_size, max_binlog_stmt_cache_size;
 extern int32 opt_binlog_max_flush_queue_time;
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index 236afb2d356..3108cbfc5b5 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -602,6 +602,14 @@ static bool fix_binlog_stmt_cache_size(sys_var *self, THD *thd, enum_var_type ty
   return false;
 }

+static Sys_var_ulong Sys_binlog_init_cache_size(
+       ""binlog_init_cache_size"", ""The size of init malloc cache for the binary log."",
+       GLOBAL_VAR(binlog_init_cache_size),
+       CMD_LINE(REQUIRED_ARG),
+       VALID_RANGE(IO_SIZE*2, ULONG_MAX), DEFAULT(IO_SIZE*4), BLOCK_SIZE(IO_SIZE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG, ON_CHECK(0),
+       ON_UPDATE(0));
+
 static Sys_var_ulong Sys_binlog_cache_size(
        ""binlog_cache_size"", ""The size of the transactional cache for ""
        ""updates to transactional engines for the binary log. ""
{noformat} $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,6,,0,4,0,1,0,0,0,,0,850,3,0,0,2017-10-24 16:55:50,MS: Introduce a variable to init binlog cache size,"{noformat}
From f8ea0478200bf115cb0abdbf28cdf339e268a0f6 Mon Sep 17 00:00:00 2001
From: Hanzhi Wang <hanzhiwa@microsoft.com>
Date: Wed, 13 Sep 2017 07:13:49 +0000
Subject: [PATCH] Merged PR 60892: Introduce a variable to init binlog cache
 size

Introduce a variable to init binlog cache size

Related work items: #84138
---
 mysql-test/r/mysqld--help-win.result | 3 +++
 sql/binlog.cc                        | 2 +-
 sql/mysqld.cc                        | 1 +
 sql/mysqld.h                         | 1 +
 sql/sys_vars.cc                      | 8 ++++++++
 5 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index a6f2ae76188..e866f63c5a7 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -102,6 +102,8 @@ The following options may be given as the first argument:
  --binlog-ignore-db=name
  Tells the master that updates to the given database
  should not be logged to the binary log.
+ --binlog-init-cache-size=#
+ The size of init malloc cache for the binary log.
  --binlog-max-flush-queue-time=#
  The maximum time that the binary log group commit will
  keep reading transactions before it flush the
@@ -1084,6 +1086,7 @@ binlog-direct-non-transactional-updates FALSE
 binlog-error-action IGNORE_ERROR
 binlog-format STATEMENT
 binlog-gtid-simple-recovery FALSE
+binlog-init-cache-size 16384
 binlog-max-flush-queue-time 0
 binlog-order-commits TRUE
 binlog-row-event-max-size 8192
diff --git a/sql/binlog.cc b/sql/binlog.cc
index 8d3cb51b429..f092d283e73 100644
--- a/sql/binlog.cc
+++ b/sql/binlog.cc
@@ -1951,7 +1951,7 @@ File open_binlog_file(IO_CACHE *log, const char *log_file_name, const char **err
     *errmsg = ""Could not open log file"";
     goto err;
   }
-  if (init_io_cache(log, file, IO_SIZE*2, READ_CACHE, 0, 0,
+  if (init_io_cache(log, file, binlog_init_cache_size, READ_CACHE, 0, 0,
                     MYF(MY_WME|MY_DONT_CHECK_FILESIZE)))
   {
     sql_print_error(""Failed to create a cache on log (file '%s')"",
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index eba6fdec801..03839e6e742 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -562,6 +562,7 @@ ulonglong slave_rows_search_algorithms_options;
 #ifndef DBUG_OFF
 uint slave_rows_last_search_algorithm_used;
 #endif
+ulong binlog_init_cache_size=0;
 ulong binlog_cache_size=0;
 ulonglong  max_binlog_cache_size=0;
 ulong slave_max_allowed_packet= 0;
diff --git a/sql/mysqld.h b/sql/mysqld.h
index d814a8ff28a..e73c1c00230 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -236,6 +236,7 @@ extern my_bool log_bin_use_v1_row_events;
 extern ulong what_to_log,flush_time;
 extern ulong max_prepared_stmt_count, prepared_stmt_count;
 extern ulong open_files_limit;
+extern ulong binlog_init_cache_size;
 extern ulong binlog_cache_size, binlog_stmt_cache_size;
 extern ulonglong max_binlog_cache_size, max_binlog_stmt_cache_size;
 extern int32 opt_binlog_max_flush_queue_time;
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index 236afb2d356..3108cbfc5b5 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -602,6 +602,14 @@ static bool fix_binlog_stmt_cache_size(sys_var *self, THD *thd, enum_var_type ty
   return false;
 }

+static Sys_var_ulong Sys_binlog_init_cache_size(
+       ""binlog_init_cache_size"", ""The size of init malloc cache for the binary log."",
+       GLOBAL_VAR(binlog_init_cache_size),
+       CMD_LINE(REQUIRED_ARG),
+       VALID_RANGE(IO_SIZE*2, ULONG_MAX), DEFAULT(IO_SIZE*4), BLOCK_SIZE(IO_SIZE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG, ON_CHECK(0),
+       ON_UPDATE(0));
+
 static Sys_var_ulong Sys_binlog_cache_size(
        ""binlog_cache_size"", ""The size of the transactional cache for ""
        ""updates to transactional engines for the binary log. ""
{noformat}",,0,0,0,0,0.0,"MS: Introduce a variable to init binlog cache size $end$ {noformat}
From f8ea0478200bf115cb0abdbf28cdf339e268a0f6 Mon Sep 17 00:00:00 2001
From: Hanzhi Wang <hanzhiwa@microsoft.com>
Date: Wed, 13 Sep 2017 07:13:49 +0000
Subject: [PATCH] Merged PR 60892: Introduce a variable to init binlog cache
 size

Introduce a variable to init binlog cache size

Related work items: #84138
---
 mysql-test/r/mysqld--help-win.result | 3 +++
 sql/binlog.cc                        | 2 +-
 sql/mysqld.cc                        | 1 +
 sql/mysqld.h                         | 1 +
 sql/sys_vars.cc                      | 8 ++++++++
 5 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/mysql-test/r/mysqld--help-win.result b/mysql-test/r/mysqld--help-win.result
index a6f2ae76188..e866f63c5a7 100644
--- a/mysql-test/r/mysqld--help-win.result
+++ b/mysql-test/r/mysqld--help-win.result
@@ -102,6 +102,8 @@ The following options may be given as the first argument:
  --binlog-ignore-db=name
  Tells the master that updates to the given database
  should not be logged to the binary log.
+ --binlog-init-cache-size=#
+ The size of init malloc cache for the binary log.
  --binlog-max-flush-queue-time=#
  The maximum time that the binary log group commit will
  keep reading transactions before it flush the
@@ -1084,6 +1086,7 @@ binlog-direct-non-transactional-updates FALSE
 binlog-error-action IGNORE_ERROR
 binlog-format STATEMENT
 binlog-gtid-simple-recovery FALSE
+binlog-init-cache-size 16384
 binlog-max-flush-queue-time 0
 binlog-order-commits TRUE
 binlog-row-event-max-size 8192
diff --git a/sql/binlog.cc b/sql/binlog.cc
index 8d3cb51b429..f092d283e73 100644
--- a/sql/binlog.cc
+++ b/sql/binlog.cc
@@ -1951,7 +1951,7 @@ File open_binlog_file(IO_CACHE *log, const char *log_file_name, const char **err
     *errmsg = ""Could not open log file"";
     goto err;
   }
-  if (init_io_cache(log, file, IO_SIZE*2, READ_CACHE, 0, 0,
+  if (init_io_cache(log, file, binlog_init_cache_size, READ_CACHE, 0, 0,
                     MYF(MY_WME|MY_DONT_CHECK_FILESIZE)))
   {
     sql_print_error(""Failed to create a cache on log (file '%s')"",
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index eba6fdec801..03839e6e742 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -562,6 +562,7 @@ ulonglong slave_rows_search_algorithms_options;
 #ifndef DBUG_OFF
 uint slave_rows_last_search_algorithm_used;
 #endif
+ulong binlog_init_cache_size=0;
 ulong binlog_cache_size=0;
 ulonglong  max_binlog_cache_size=0;
 ulong slave_max_allowed_packet= 0;
diff --git a/sql/mysqld.h b/sql/mysqld.h
index d814a8ff28a..e73c1c00230 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -236,6 +236,7 @@ extern my_bool log_bin_use_v1_row_events;
 extern ulong what_to_log,flush_time;
 extern ulong max_prepared_stmt_count, prepared_stmt_count;
 extern ulong open_files_limit;
+extern ulong binlog_init_cache_size;
 extern ulong binlog_cache_size, binlog_stmt_cache_size;
 extern ulonglong max_binlog_cache_size, max_binlog_stmt_cache_size;
 extern int32 opt_binlog_max_flush_queue_time;
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index 236afb2d356..3108cbfc5b5 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -602,6 +602,14 @@ static bool fix_binlog_stmt_cache_size(sys_var *self, THD *thd, enum_var_type ty
   return false;
 }

+static Sys_var_ulong Sys_binlog_init_cache_size(
+       ""binlog_init_cache_size"", ""The size of init malloc cache for the binary log."",
+       GLOBAL_VAR(binlog_init_cache_size),
+       CMD_LINE(REQUIRED_ARG),
+       VALID_RANGE(IO_SIZE*2, ULONG_MAX), DEFAULT(IO_SIZE*4), BLOCK_SIZE(IO_SIZE),
+       NO_MUTEX_GUARD, NOT_IN_BINLOG, ON_CHECK(0),
+       ON_UPDATE(0));
+
 static Sys_var_ulong Sys_binlog_cache_size(
        ""binlog_cache_size"", ""The size of the transactional cache for ""
        ""updates to transactional engines for the binary log. ""
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,8.13333,24,3,0.125,2,0.0833333,2,0.0833333,1,0.0416667,1,0.0416667
691,MDEV-14135,Task,MDEV,2017-10-26 09:29:49,,0,10.1.29 merge,"* 10.0 (/)
* 10.0-galera (/)
* ks-10.1-mroonga (/)",,"10.1.29 merge $end$ * 10.0 (/)
* 10.0-galera (/)
* ks-10.1-mroonga (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,7,,0,1,0,1,0,4,0,,0,850,0,2,0,2017-11-09 18:34:43,10.1.29 merge,"* 10.0
* 10.0-galera
* ks-10.1-mroonga",,0,2,0,3,0.272727,"10.1.29 merge $end$ * 10.0
* 10.0-galera
* ks-10.1-mroonga $acceptance criteria:$",2,1,0,0,0,0,0,345.067,52,26,0.5,20,0.384615,15,0.288462,13,0.25,10,0.192308
692,MDEV-14139,Technical task,MDEV,2017-10-26 10:47:01,,0,Anchored data types for variables,"Previously we added anchored data type declarations for table columns, tables, cursors:
{code:sql}
DECLARE a TYPE OF t1.a;   -- table column
DECLARE b ROW TYPE OF t1; -- table row
DECLARE c ROW TYPE OF c1; -- cursor row
{code}

Under terms of this tasks we'll add anchor references to other variables:
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
{code}

The following features will be supported:
- Nested anchor declarations (anchors to anchors)
- Anchors to implicit ROW variables
- Anchors to table and cursor ROW variables
- Only local SP variables will be supported (SP parameters and SP return values are out of scope of this task)
- Both TYPE OF (for sql_mode=default) and %TYPE (for sql_mode=ORACLE) will be supported

Example: Scalar declarations
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
DECLARE var3 TYPE OF var2;
{code}
Example: Scalar declarations for sql_mode=ORACLE
{code:sql}
DECLARE
  var1 INT;
  var2 var1%TYPE;
  var3 var2%TYPE;
{code}


Example: Implicit ROW variables
{code:sql}
DECLARE row1 ROW (a INT, b TEXT);
DECLARE row2 TYPE OF row1;
{code}
Example: Implicit ROW variables for sql_mode=ORACLE
{code:sql}
DECLARE
  row1 ROW (a INT, b TEXT);
  row2 row1%TYPE;
{code}


Example: Table ROW variables
{code:sql}
DECLARE row1 ROW TYPE OF table1;
DECLARE row2 TYPE OF row1;
{code}
Example: Table ROW variables for sql_mode=ORACLE
{code:sql}
DECLARE
  row1 ROW TYPE OF table1;
  row2 row1%TYPE;
{code}


Example: Cursor ROW variables
{code:sql}
DECLARE CURSOR cur1 AS SELECT 10 AS a, 'b' AS b;
DECLARE row1 ROW TYPE OF cur1;
DECLARE row2 TYPE OF row1;
{code}
Example: Cursor ROW variables for sql_mode=ORACLE
{code:sql}
DECLARE
  CURSOR cur1 AS SELECT 10 AS a, 'b' AS b;
  row1 cur1%ROWTYPE;
  row2 row1%TYPE;
{code}
",,"Anchored data types for variables $end$ Previously we added anchored data type declarations for table columns, tables, cursors:
{code:sql}
DECLARE a TYPE OF t1.a;   -- table column
DECLARE b ROW TYPE OF t1; -- table row
DECLARE c ROW TYPE OF c1; -- cursor row
{code}

Under terms of this tasks we'll add anchor references to other variables:
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
{code}

The following features will be supported:
- Nested anchor declarations (anchors to anchors)
- Anchors to implicit ROW variables
- Anchors to table and cursor ROW variables
- Only local SP variables will be supported (SP parameters and SP return values are out of scope of this task)
- Both TYPE OF (for sql_mode=default) and %TYPE (for sql_mode=ORACLE) will be supported

Example: Scalar declarations
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
DECLARE var3 TYPE OF var2;
{code}
Example: Scalar declarations for sql_mode=ORACLE
{code:sql}
DECLARE
  var1 INT;
  var2 var1%TYPE;
  var3 var2%TYPE;
{code}


Example: Implicit ROW variables
{code:sql}
DECLARE row1 ROW (a INT, b TEXT);
DECLARE row2 TYPE OF row1;
{code}
Example: Implicit ROW variables for sql_mode=ORACLE
{code:sql}
DECLARE
  row1 ROW (a INT, b TEXT);
  row2 row1%TYPE;
{code}


Example: Table ROW variables
{code:sql}
DECLARE row1 ROW TYPE OF table1;
DECLARE row2 TYPE OF row1;
{code}
Example: Table ROW variables for sql_mode=ORACLE
{code:sql}
DECLARE
  row1 ROW TYPE OF table1;
  row2 row1%TYPE;
{code}


Example: Cursor ROW variables
{code:sql}
DECLARE CURSOR cur1 AS SELECT 10 AS a, 'b' AS b;
DECLARE row1 ROW TYPE OF cur1;
DECLARE row2 TYPE OF row1;
{code}
Example: Cursor ROW variables for sql_mode=ORACLE
{code:sql}
DECLARE
  CURSOR cur1 AS SELECT 10 AS a, 'b' AS b;
  row1 cur1%ROWTYPE;
  row2 row1%TYPE;
{code}
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,0,0,3,5,0,7,0,,0,850,0,0,0,2017-10-26 10:47:01,Anchored data types for variables,"Previously we added anchored data type declarations for table columns, tables, cursors:
{code:sql}
DECLARE a TYPE OF t1.a;   -- table column
DECLARE b ROW TYPE OF t1; -- table row
DECLARE c ROW TYPE OF c1; -- cursor row
{code}

Under terms of this tasks we'll add anchor references to other variables:
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
{code}

The following features will be supported:
- Nested anchor declarations (anchors to anchors)
- Anchors to implicit ROW variables
- Anchors to table and cursor ROW variables
- Both local SP variables and SP parameters will be supported
- Both TYPE OF (for sql_mode=default) and %TYPE (for sql_mode=ORACLE) will be supported

Example: Scalar declarations
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
DECLARE var3 TYPE OF var2;
{code}

Example: Implicit ROW variables
{code:sql}
DECLARE row1 ROW (a INT, b TEXT);
DECLARE row2 TYPE OF var2;
{code}

Example: Table ROW variables
{code:sql}
DECLARE row1 ROW TYPE OF table1;
DECLARE row2 TYPE OF row1;
{code}


Example: Cursor ROW variables
{code:sql}
DECLARE CURSOR cur1 AS SELECT 10 AS a, 'b' AS b;
DECLARE row1 ROW TYPE OF cur1;
DECLARE row2 TYPE OF row1;
{code}


",,0,7,0,90,0.425743,"Anchored data types for variables $end$ Previously we added anchored data type declarations for table columns, tables, cursors:
{code:sql}
DECLARE a TYPE OF t1.a;   -- table column
DECLARE b ROW TYPE OF t1; -- table row
DECLARE c ROW TYPE OF c1; -- cursor row
{code}

Under terms of this tasks we'll add anchor references to other variables:
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
{code}

The following features will be supported:
- Nested anchor declarations (anchors to anchors)
- Anchors to implicit ROW variables
- Anchors to table and cursor ROW variables
- Both local SP variables and SP parameters will be supported
- Both TYPE OF (for sql_mode=default) and %TYPE (for sql_mode=ORACLE) will be supported

Example: Scalar declarations
{code:sql}
DECLARE var1 INT;
DECLARE var2 TYPE OF var1;
DECLARE var3 TYPE OF var2;
{code}

Example: Implicit ROW variables
{code:sql}
DECLARE row1 ROW (a INT, b TEXT);
DECLARE row2 TYPE OF var2;
{code}

Example: Table ROW variables
{code:sql}
DECLARE row1 ROW TYPE OF table1;
DECLARE row2 TYPE OF row1;
{code}


Example: Cursor ROW variables
{code:sql}
DECLARE CURSOR cur1 AS SELECT 10 AS a, 'b' AS b;
DECLARE row1 ROW TYPE OF cur1;
DECLARE row2 TYPE OF row1;
{code}


 $acceptance criteria:$",7,1,1,1,1,1,1,0.0,83,42,0.506024,29,0.349398,27,0.325301,25,0.301205,25,0.301205
693,MDEV-14212,Technical task,MDEV,2017-10-30 05:17:18,,0,Add Field_row for SP ROW variables,"We'll solve MDEV-13418 by translating:

{code:sql}
SELECT  1, a INTO a, b FROM dual;
{code}
into
{code:sql}
DECLARE a_tmp TYPE OF a;
DECLARE b_tmp TYPE OF b;
SELECT  1, a INTO a, b FROM dual;
SET a=a_tmp;
SET b=b_tmp;
{code}


Notice, this will need auto-generated assignments between SP variables ({{a=a_tmp}} and {{b=b_tmp}}).
Normally assignment is done via {{sp_instr_set}}, but it does not suite well here because it needs:
- an {{Item}} for the assignment source
- a {{LEX}} containing this {{Item}}

Instead of creating an {{Item}} and a {{LEX}} (whose {{sizeof}} are {{208}} and {{5896}} bytes respectively),
we'll introduce a new {{sp_instr_setvar}} which will copy between two variables directly, with no {{Item}} and {{LEX}}, using their offsets in {{sp_rcontext::m_var_table}}.

In order to implement this, we'll need some changes for {{ROW}}-type SP variables.
Currently {{ROW}} variables store their members inside {{Item_splocal_row::m_table}}.
We'll do the following:

- Create a new class {{Field_row}} and move {{m_table}} from {{Item_splocal_row}} to {{Field_row}}
{code:cpp}
class Field_row: public Field_null
{
  class Virtual_tmp_table *m_table;
public:
  Field_row(uchar *ptr_arg, const LEX_CSTRING *field_name_arg)
    :Field_null(ptr_arg, 0, Field::NONE, field_name_arg, &my_charset_bin),
     m_table(NULL)
    {}
};
{code}

- Remove the class {{Item_splocal_row}} and use {{Item_splocal}} instead by passing {{&type_handler_row}} to the constructor.

- Change {{Item_splocal::Item_splocal}} to accept {{const Type_handler *handler}} instead of {{enum_field_types sp_var_type}}.

- Remove the class {{Item_spvar_args}} and derive {{Item_field_row}} directly from {{Item_args}}

- Move {{row_create_items()}} and {{get_row_field()}} from {{Item_spvar_args}} to {{Item_field_row}} and fix them to use {{Field_row::m_table}} instead of {{Item_spvar_args::m_table}}.

- Replace {{DBUG_ASSERT}} in  {{Type_handler_row::Column_definition_fix_attributes()}}, {{Type_handler_row::Column_definition_prepare_stage1()}}, {{Type_handler_row::Column_definition_prepare_stage2()}} to real appropriate implementations.
",,"Add Field_row for SP ROW variables $end$ We'll solve MDEV-13418 by translating:

{code:sql}
SELECT  1, a INTO a, b FROM dual;
{code}
into
{code:sql}
DECLARE a_tmp TYPE OF a;
DECLARE b_tmp TYPE OF b;
SELECT  1, a INTO a, b FROM dual;
SET a=a_tmp;
SET b=b_tmp;
{code}


Notice, this will need auto-generated assignments between SP variables ({{a=a_tmp}} and {{b=b_tmp}}).
Normally assignment is done via {{sp_instr_set}}, but it does not suite well here because it needs:
- an {{Item}} for the assignment source
- a {{LEX}} containing this {{Item}}

Instead of creating an {{Item}} and a {{LEX}} (whose {{sizeof}} are {{208}} and {{5896}} bytes respectively),
we'll introduce a new {{sp_instr_setvar}} which will copy between two variables directly, with no {{Item}} and {{LEX}}, using their offsets in {{sp_rcontext::m_var_table}}.

In order to implement this, we'll need some changes for {{ROW}}-type SP variables.
Currently {{ROW}} variables store their members inside {{Item_splocal_row::m_table}}.
We'll do the following:

- Create a new class {{Field_row}} and move {{m_table}} from {{Item_splocal_row}} to {{Field_row}}
{code:cpp}
class Field_row: public Field_null
{
  class Virtual_tmp_table *m_table;
public:
  Field_row(uchar *ptr_arg, const LEX_CSTRING *field_name_arg)
    :Field_null(ptr_arg, 0, Field::NONE, field_name_arg, &my_charset_bin),
     m_table(NULL)
    {}
};
{code}

- Remove the class {{Item_splocal_row}} and use {{Item_splocal}} instead by passing {{&type_handler_row}} to the constructor.

- Change {{Item_splocal::Item_splocal}} to accept {{const Type_handler *handler}} instead of {{enum_field_types sp_var_type}}.

- Remove the class {{Item_spvar_args}} and derive {{Item_field_row}} directly from {{Item_args}}

- Move {{row_create_items()}} and {{get_row_field()}} from {{Item_spvar_args}} to {{Item_field_row}} and fix them to use {{Field_row::m_table}} instead of {{Item_spvar_args::m_table}}.

- Replace {{DBUG_ASSERT}} in  {{Type_handler_row::Column_definition_fix_attributes()}}, {{Type_handler_row::Column_definition_prepare_stage1()}}, {{Type_handler_row::Column_definition_prepare_stage2()}} to real appropriate implementations.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,6,,0,0,2,5,0,0,0,,0,850,0,0,0,2017-10-30 05:17:18,Add Field_row for SP ROW variables,"We'll solve MDEV-13418 by translating:

{code:sql}
SELECT  1, a INTO a, b FROM dual;
{code}
into
{code:sql}
DECLARE a_tmp TYPE OF a;
DECLARE b_tmp TYPE OF b;
SELECT  1, a INTO a, b FROM dual;
SET a=a_tmp;
SET b=b_tmp;
{code}


Notice, this will need auto-generated assignments between SP variables ({{a=a_tmp}} and {{b=b_tmp}}).
Normally assignment is done via {{sp_instr_set}}, but it does not suite well here because it needs:
- an {{Item}} for the assignment source
- a {{LEX}} containing this {{Item}}

Instead of creating an {{Item}} and a {{LEX}} (whose {{sizeof}} are {{208}} and {{5896}} bytes respectively),
we'll introduce a new {{sp_instr_setvar}} which will copy between two variables directly, with no {{Item}} and {{LEX}}, using their offsets in {{sp_rcontext::m_var_table}}.

In order to implement this, we'll need some changes for {{ROW}}-type SP variables.
Currently {{ROW}} variables store their members inside {{Item_splocal_row::m_table}}.
We'll do the following:

- Create a new class {{Field_row}} and move {{m_table}} from {{Item_splocal_row}} to {{Field_row}}
{code:cpp}
class Field_row: public Field_null
{
  class Virtual_tmp_table *m_table;
public:
  Field_row(uchar *ptr_arg, const LEX_CSTRING *field_name_arg)
    :Field_null(ptr_arg, 0, Field::NONE, field_name_arg, &my_charset_bin),
     m_table(NULL)
    {}
};
{code}

- Remove the class {{Item_splocal_row}} and use {{Item_splocal}} instead by passing {{&type_handler_row}} to the constructor.

- Change {{Item_splocal::Item_splocal}} to accept {{const Type_handler *handler}} instead of {{enum_field_types sp_var_type}}.

- Remove the class {{Item_spvar_args}} and derive {{Item_field_row}} directly from {{Item_args}}

- Move {{row_create_items()}} and {{get_row_field()}} from {{Item_spvar_args}} to {{Item_field_row}} and fix them to use {{Field_row::m_table}} instead of {{Item_spvar_args::m_table}}.

- Replace {{DBUG_ASSERT}} in  {{Type_handler_row::Column_definition_fix_attributes()}}, {{Type_handler_row::Column_definition_prepare_stage1()}}, {{Type_handler_row::Column_definition_prepare_stage2()}} to real appropriate implementations.
",,0,0,0,0,0.0,"Add Field_row for SP ROW variables $end$ We'll solve MDEV-13418 by translating:

{code:sql}
SELECT  1, a INTO a, b FROM dual;
{code}
into
{code:sql}
DECLARE a_tmp TYPE OF a;
DECLARE b_tmp TYPE OF b;
SELECT  1, a INTO a, b FROM dual;
SET a=a_tmp;
SET b=b_tmp;
{code}


Notice, this will need auto-generated assignments between SP variables ({{a=a_tmp}} and {{b=b_tmp}}).
Normally assignment is done via {{sp_instr_set}}, but it does not suite well here because it needs:
- an {{Item}} for the assignment source
- a {{LEX}} containing this {{Item}}

Instead of creating an {{Item}} and a {{LEX}} (whose {{sizeof}} are {{208}} and {{5896}} bytes respectively),
we'll introduce a new {{sp_instr_setvar}} which will copy between two variables directly, with no {{Item}} and {{LEX}}, using their offsets in {{sp_rcontext::m_var_table}}.

In order to implement this, we'll need some changes for {{ROW}}-type SP variables.
Currently {{ROW}} variables store their members inside {{Item_splocal_row::m_table}}.
We'll do the following:

- Create a new class {{Field_row}} and move {{m_table}} from {{Item_splocal_row}} to {{Field_row}}
{code:cpp}
class Field_row: public Field_null
{
  class Virtual_tmp_table *m_table;
public:
  Field_row(uchar *ptr_arg, const LEX_CSTRING *field_name_arg)
    :Field_null(ptr_arg, 0, Field::NONE, field_name_arg, &my_charset_bin),
     m_table(NULL)
    {}
};
{code}

- Remove the class {{Item_splocal_row}} and use {{Item_splocal}} instead by passing {{&type_handler_row}} to the constructor.

- Change {{Item_splocal::Item_splocal}} to accept {{const Type_handler *handler}} instead of {{enum_field_types sp_var_type}}.

- Remove the class {{Item_spvar_args}} and derive {{Item_field_row}} directly from {{Item_args}}

- Move {{row_create_items()}} and {{get_row_field()}} from {{Item_spvar_args}} to {{Item_field_row}} and fix them to use {{Field_row::m_table}} instead of {{Item_spvar_args::m_table}}.

- Replace {{DBUG_ASSERT}} in  {{Type_handler_row::Column_definition_fix_attributes()}}, {{Type_handler_row::Column_definition_prepare_stage1()}}, {{Type_handler_row::Column_definition_prepare_stage2()}} to real appropriate implementations.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,84,43,0.511905,30,0.357143,28,0.333333,26,0.309524,26,0.309524
694,MDEV-14286,Technical task,MDEV,2017-11-04 21:53:31,,0,Smoke test / sanity check for changes made in scope of MDEV-13328,"[~marko] wrote:
{quote}
DML before export, also DML before DISCARD TABLESPACE
at least some 100 megabytes of data in the table, with a big buffer pool, so that stale pages will stay around
maybe just import an empty table (that should be the worst exercise)
and in fact, you don’t necessarily have to copy large .ibd files during the test; we only need one .ibd file for the empty table to be imported (in a test loop)

so, basically:
- create table;
- loop:
** insert/replace a lot;
** alter table...discard tablespace;
** copy empty file
** alter table..import tablespace;
- goto loop

and of course, multiple threads in parallel, each using their own table
and lots of secondary indexes
{quote}

----

h2. Test description

The implemented test is limited to what's prescribed in the request above, with only one addition -- at the step ""copy empty file"", during the course of the test we alternate between using empty tablespaces and tablespaces with data.

Since the test is expected to use rather big tables, which take a lot of time to create, in order to speed up the process, the test is performed in a half-automated-half-manual mode.
One-time activity (manual):
- server is started with default parameters;
- tables are pre-created;
- tables are flushed for export;
- tablespaces are exported and stored manually;
- tables are unlocked;
- tables are populated;
- tables are flushed for export;
- tablespaces are exported and stored manually;
- tables are unlocked;
- server is shut down;
- the whole data directory is stored along with the exported/stored tablespaces.

Repeated activity;
- the stored data directory is copied back;
- server is started manually with parameters which are needed for the current test;
- an automated test is performed on the running server;
- server is shut down manually.

{code:sql|title=Data structures}
CREATE TABLE t1 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPACT ;
CREATE TABLE t2 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = REDUNDANT ;
set global innodb_file_format=Barracuda;
CREATE TABLE t3 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPRESSED ;
CREATE TABLE t4 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = DYNAMIC ;
CREATE TABLE t5 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = DYNAMIC PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=3;
CREATE TABLE t6 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = DYNAMIC PAGE_COMPRESSED=1;
CREATE TABLE t7 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPACT PAGE_COMPRESSED=1;
CREATE TABLE t8 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPACT PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=9;
{code}
{code:sql|title=Initial table contents}
INSERT INTO t1 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_10000;
INSERT INTO t2 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_50000;
INSERT INTO t3 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_100000;
INSERT INTO t4 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_200000;
INSERT INTO t5 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_300000;
INSERT INTO t6 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_30000;
INSERT INTO t7 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_80000;
INSERT INTO t8 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_800000;
{code}
{noformat:title=Stored tablespaces}
-rw-r----- 1 elenst elenst   26214400 Nov  6 13:28 data/test/t1.ibd.backup
-rw-r----- 1 elenst elenst  104857600 Nov  6 13:28 data/test/t2.ibd.backup
-rw-r----- 1 elenst elenst   96468992 Nov  6 13:28 data/test/t3.ibd.backup
-rw-r----- 1 elenst elenst  364904448 Nov  6 13:28 data/test/t4.ibd.backup
-rw-r----- 1 elenst elenst  515899392 Nov  6 13:29 data/test/t5.ibd.backup
-rw-r----- 1 elenst elenst   62914560 Nov  6 13:29 data/test/t6.ibd.backup
-rw-r----- 1 elenst elenst  146800640 Nov  6 13:29 data/test/t7.ibd.backup
-rw-r----- 1 elenst elenst 1430257664 Nov  6 13:29 data/test/t8.ibd.backup

-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t1.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t2.ibd.empty
-rw-r----- 1 elenst elenst  98304 Nov  6 13:28 data/test/t3.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t4.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t5.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t6.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t7.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t8.ibd.empty
{noformat}

h2. Test results

_Note: Any noticeable deviations and inaccuracies  in executing the scenario [caused various problems|https://jira.mariadb.org/browse/MDEV-14286?focusedCommentId=102766&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-102766]. These problems are considered to be outside the scope of this task and attributed to the general instability of export and {{DISCARD/IMPORT}} tablespace functionality. Analysis of such failures was not performed._

h3. Smoke tests for stability

Several runs of the described test were performed, all with the same data:

- 1-thread test on a server with {{innodb_buffer_pool_size=128M general_log=on}}, otherwise defaults, for table t1;
- 8-thread test on a server with {{innodb_buffer_pool_size=128M general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=2G general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=4G general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=8G general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=2G general_log=on ignore-builtin-innodb plugin-load-add=ha_innodb}}, otherwise defaults;

The duration of the test runs varied from 10 to 30 min. All test runs were performed on a debug build.

In all variations above, the test was considered a pass if the server didn't crash during the duration of the test.

On the initial implementation, bb-10.1-marko 1fb7ac18c417c8fbd95, the test caused failure described in [this comment|https://jira.mariadb.org/browse/MDEV-14286?focusedCommentId=102659&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-102659]. The failure was reproducible via a stress test and MTR.

{color:green}The failure stopped happening after a temporary patch and didn't re-appear after the real fix. No other crashes were observed.{color}

h3. Sanity check for performance regressions

The same test flow was converted into a performance comparison test. The tests were performed in an environment not tuned for benchmarks, results are not considered in any way accurate, and absolute values are irrelevant. The purpose of the test was to check for critical performance regressions. All tests were run on non-debug builds.

Several test runs were performed:

- 1-thread latency comparison* test on servers with  {{innodb_buffer_pool_size=4G general_log=on}}, otherwise defaults, table t3;
- 1-thread latency comparison* test on servers with  {{innodb_buffer_pool_size=4G general_log=on}}, otherwise defaults, table t8;
- 8-thread throughput comparison** tests on a servers with {{innodb_buffer_pool_size=8G general_log=on}}, otherwise defaults;
- 1-thread throughput comparison** tests on a servers with {{innodb_buffer_pool_size=8G general_log=on}}, otherwise defaults, table t5;
- 8-thread throughput comparison** tests on a servers with {{innodb_buffer_pool_size=128M general_log=on}}, otherwise defaults;

\* For the latency comparison test, two servers are running side-by-side. The test executes the same query on both servers one after another, compares the execution time, and reports differences which exceed a given threshold

\*\* For the throughput comparison test, only one server is started at a time. All test flow is run on the server for a given time interval, after that the overall number of all queries (including DML) per thread is calculated, and resulting numbers are compared.

Examples of results of throughput test:

*8-thread 10-min test, buffer pool 8G*
|| thread/table || 10.1 || bb-10.1-marko ||
| 1 | 451 | 478 |
| 2 | 283 | 286 |
| 3 | 345 | 425 |
| 4 | 271 | 320 |
| 5 | 233 | 287 |
| 6 | 418 | 418 |
| 7 | 322 | 314 |
| 8 | 381 | 348 |

*8-thread 10-min test, buffer pool 128M*
|| thread/table || 10.1 || bb-10.1-marko ||
| 1 | 392 | 452 |
| 2 | 286 | 378 |
| 3 | 349 | 349 |
| 4 | 325 | 340 |
| 5 | 302 | 244 |
| 6 | 391 | 468 |
| 7 | 322 | 402 |
| 8 | 381 | 348 |

*1-thread 10-min test, buffer pool 8G, table t5*
|| 10.1 || bb-10.1-marko ||
| 1043 | 1043 |
_yes, exactly the same, not a typo, weird coincidence_

{color:green}No differences which could not be explained by unreliability of the test environment have been noticed during tests.{color}",,"Smoke test / sanity check for changes made in scope of MDEV-13328 $end$ [~marko] wrote:
{quote}
DML before export, also DML before DISCARD TABLESPACE
at least some 100 megabytes of data in the table, with a big buffer pool, so that stale pages will stay around
maybe just import an empty table (that should be the worst exercise)
and in fact, you don’t necessarily have to copy large .ibd files during the test; we only need one .ibd file for the empty table to be imported (in a test loop)

so, basically:
- create table;
- loop:
** insert/replace a lot;
** alter table...discard tablespace;
** copy empty file
** alter table..import tablespace;
- goto loop

and of course, multiple threads in parallel, each using their own table
and lots of secondary indexes
{quote}

----

h2. Test description

The implemented test is limited to what's prescribed in the request above, with only one addition -- at the step ""copy empty file"", during the course of the test we alternate between using empty tablespaces and tablespaces with data.

Since the test is expected to use rather big tables, which take a lot of time to create, in order to speed up the process, the test is performed in a half-automated-half-manual mode.
One-time activity (manual):
- server is started with default parameters;
- tables are pre-created;
- tables are flushed for export;
- tablespaces are exported and stored manually;
- tables are unlocked;
- tables are populated;
- tables are flushed for export;
- tablespaces are exported and stored manually;
- tables are unlocked;
- server is shut down;
- the whole data directory is stored along with the exported/stored tablespaces.

Repeated activity;
- the stored data directory is copied back;
- server is started manually with parameters which are needed for the current test;
- an automated test is performed on the running server;
- server is shut down manually.

{code:sql|title=Data structures}
CREATE TABLE t1 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPACT ;
CREATE TABLE t2 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = REDUNDANT ;
set global innodb_file_format=Barracuda;
CREATE TABLE t3 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPRESSED ;
CREATE TABLE t4 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = DYNAMIC ;
CREATE TABLE t5 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = DYNAMIC PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=3;
CREATE TABLE t6 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = DYNAMIC PAGE_COMPRESSED=1;
CREATE TABLE t7 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPACT PAGE_COMPRESSED=1;
CREATE TABLE t8 ( pk INT AUTO_INCREMENT PRIMARY KEY, f1 INT, f2 INT NOT NULL, f3 BIGINT, f4 BIGINT NOT NULL, f5 VARCHAR(1024), f6 VARCHAR(2048) NOT NULL, f7 CHAR(255), f8 CHAR(128) NOT NULL, UNIQUE(f1), UNIQUE(f7), KEY(f2,f5(64)), KEY(f3,f6(128)), KEY(f4,f7), KEY(f1,f8) ) ENGINE=INNODB ROW_FORMAT = COMPACT PAGE_COMPRESSED=1 PAGE_COMPRESSION_LEVEL=9;
{code}
{code:sql|title=Initial table contents}
INSERT INTO t1 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_10000;
INSERT INTO t2 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_50000;
INSERT INTO t3 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_100000;
INSERT INTO t4 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_200000;
INSERT INTO t5 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_300000;
INSERT INTO t6 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_30000;
INSERT INTO t7 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_80000;
INSERT INTO t8 SELECT NULL, seq, seq%1000, seq%10, seq%10000, CONCAT('f5_',(seq%1000)), CONCAT('f6_',(seq%100)), CONCAT('f7_',seq), CONCAT('f8_',(seq%10)) FROM seq_1_to_800000;
{code}
{noformat:title=Stored tablespaces}
-rw-r----- 1 elenst elenst   26214400 Nov  6 13:28 data/test/t1.ibd.backup
-rw-r----- 1 elenst elenst  104857600 Nov  6 13:28 data/test/t2.ibd.backup
-rw-r----- 1 elenst elenst   96468992 Nov  6 13:28 data/test/t3.ibd.backup
-rw-r----- 1 elenst elenst  364904448 Nov  6 13:28 data/test/t4.ibd.backup
-rw-r----- 1 elenst elenst  515899392 Nov  6 13:29 data/test/t5.ibd.backup
-rw-r----- 1 elenst elenst   62914560 Nov  6 13:29 data/test/t6.ibd.backup
-rw-r----- 1 elenst elenst  146800640 Nov  6 13:29 data/test/t7.ibd.backup
-rw-r----- 1 elenst elenst 1430257664 Nov  6 13:29 data/test/t8.ibd.backup

-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t1.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t2.ibd.empty
-rw-r----- 1 elenst elenst  98304 Nov  6 13:28 data/test/t3.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t4.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t5.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t6.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t7.ibd.empty
-rw-r----- 1 elenst elenst 196608 Nov  6 13:28 data/test/t8.ibd.empty
{noformat}

h2. Test results

_Note: Any noticeable deviations and inaccuracies  in executing the scenario [caused various problems|https://jira.mariadb.org/browse/MDEV-14286?focusedCommentId=102766&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-102766]. These problems are considered to be outside the scope of this task and attributed to the general instability of export and {{DISCARD/IMPORT}} tablespace functionality. Analysis of such failures was not performed._

h3. Smoke tests for stability

Several runs of the described test were performed, all with the same data:

- 1-thread test on a server with {{innodb_buffer_pool_size=128M general_log=on}}, otherwise defaults, for table t1;
- 8-thread test on a server with {{innodb_buffer_pool_size=128M general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=2G general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=4G general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=8G general_log=on}}, otherwise defaults;
- 8-thread test on a server with {{innodb_buffer_pool_size=2G general_log=on ignore-builtin-innodb plugin-load-add=ha_innodb}}, otherwise defaults;

The duration of the test runs varied from 10 to 30 min. All test runs were performed on a debug build.

In all variations above, the test was considered a pass if the server didn't crash during the duration of the test.

On the initial implementation, bb-10.1-marko 1fb7ac18c417c8fbd95, the test caused failure described in [this comment|https://jira.mariadb.org/browse/MDEV-14286?focusedCommentId=102659&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-102659]. The failure was reproducible via a stress test and MTR.

{color:green}The failure stopped happening after a temporary patch and didn't re-appear after the real fix. No other crashes were observed.{color}

h3. Sanity check for performance regressions

The same test flow was converted into a performance comparison test. The tests were performed in an environment not tuned for benchmarks, results are not considered in any way accurate, and absolute values are irrelevant. The purpose of the test was to check for critical performance regressions. All tests were run on non-debug builds.

Several test runs were performed:

- 1-thread latency comparison* test on servers with  {{innodb_buffer_pool_size=4G general_log=on}}, otherwise defaults, table t3;
- 1-thread latency comparison* test on servers with  {{innodb_buffer_pool_size=4G general_log=on}}, otherwise defaults, table t8;
- 8-thread throughput comparison** tests on a servers with {{innodb_buffer_pool_size=8G general_log=on}}, otherwise defaults;
- 1-thread throughput comparison** tests on a servers with {{innodb_buffer_pool_size=8G general_log=on}}, otherwise defaults, table t5;
- 8-thread throughput comparison** tests on a servers with {{innodb_buffer_pool_size=128M general_log=on}}, otherwise defaults;

\* For the latency comparison test, two servers are running side-by-side. The test executes the same query on both servers one after another, compares the execution time, and reports differences which exceed a given threshold

\*\* For the throughput comparison test, only one server is started at a time. All test flow is run on the server for a given time interval, after that the overall number of all queries (including DML) per thread is calculated, and resulting numbers are compared.

Examples of results of throughput test:

*8-thread 10-min test, buffer pool 8G*
|| thread/table || 10.1 || bb-10.1-marko ||
| 1 | 451 | 478 |
| 2 | 283 | 286 |
| 3 | 345 | 425 |
| 4 | 271 | 320 |
| 5 | 233 | 287 |
| 6 | 418 | 418 |
| 7 | 322 | 314 |
| 8 | 381 | 348 |

*8-thread 10-min test, buffer pool 128M*
|| thread/table || 10.1 || bb-10.1-marko ||
| 1 | 392 | 452 |
| 2 | 286 | 378 |
| 3 | 349 | 349 |
| 4 | 325 | 340 |
| 5 | 302 | 244 |
| 6 | 391 | 468 |
| 7 | 322 | 402 |
| 8 | 381 | 348 |

*1-thread 10-min test, buffer pool 8G, table t5*
|| 10.1 || bb-10.1-marko ||
| 1043 | 1043 |
_yes, exactly the same, not a typo, weird coincidence_

{color:green}No differences which could not be explained by unreliability of the test environment have been noticed during tests.{color} $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Major,11,,0,3,0,1,0,8,0,,0,850,3,0,0,2017-11-04 21:53:31,Smoke test for changes made in scope of MDEV-132328,"[~marko] wrote:
{quote}
DML before export, also DML before DISCARD TABLESPACE
at least some 100 megabytes of data in the table, with a big buffer pool, so that stale pages will stay around
maybe just import an empty table (that should be the worst exercise)
and in fact, you don’t necessarily have to copy large .ibd files during the test; we only need one .ibd file for the empty table to be imported (in a test loop)

so, basically:
- create table;
- loop:
** insert/replace a lot;
** alter table...discard tablespace;
** copy empty file
** alter table..import tablespace;
- goto loop

and of course, multiple threads in parallel, each using their own table
and lots of secondary indexes
{quote}",,2,6,0,1455,10.9323,"Smoke test for changes made in scope of MDEV-132328 $end$ [~marko] wrote:
{quote}
DML before export, also DML before DISCARD TABLESPACE
at least some 100 megabytes of data in the table, with a big buffer pool, so that stale pages will stay around
maybe just import an empty table (that should be the worst exercise)
and in fact, you don’t necessarily have to copy large .ibd files during the test; we only need one .ibd file for the empty table to be imported (in a test loop)

so, basically:
- create table;
- loop:
** insert/replace a lot;
** alter table...discard tablespace;
** copy empty file
** alter table..import tablespace;
- goto loop

and of course, multiple threads in parallel, each using their own table
and lots of secondary indexes
{quote} $acceptance criteria:$",8,1,1,1,1,1,1,0.0,10,2,0.2,2,0.2,2,0.2,2,0.2,2,0.2
695,MDEV-14323,Task,MDEV,2017-11-08 09:31:48,,0,Example mtr test for 2 clusters,,,Example mtr test for 2 clusters $end$ $acceptance criteria:$,,Andrii Nikitin,Andrii Nikitin,Major,2,,0,1,1,1,0,0,0,,0,850,0,0,0,2017-11-09 18:28:40,Example mtr test for 2 clusters,,,0,0,0,0,0.0,Example mtr test for 2 clusters $end$ $acceptance criteria:$,0,0,0,0,0,0,0,32.9333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
696,MDEV-14533,Task,MDEV,2017-11-29 12:50:07,,0,Provide information_schema tables using which hardware information can be obtained.,"MaxScale should be able to react when some node of a cluster hits a disk-full situation.

For instance, if the master in a master-slave cluster hits a disk-full situation, MaxScale should promote some existing slave to master and remove the old master from the cluster. That way, a client connected to MaxScale would not necessarily detect anything special at all.

In order to do that, MaxScale must be able to detect that a disk-full situation has occurred. It's difficult and unreliable to do that using MariaDB itself, for instance, by trying to create a table and assuming it was a disk-full situation in case the creation fails or the attempt ends with a timeout. Further, that approach would not allow MaxScale to become aware of the situation _before_ the disk actually becomes full.

A more reliable way would be to actually monitor the disk-space situation on the node and use that for the decision making, but arranging that is rather complex and fragile.

A robust solution would be if there were a pseudo table or pseudo tables in {{information_schema}} using which information about the node could be found out. For instance as follows:

{code}
MariaDB [information_schema]> select * from information_schema.system;
+------+--------+-----+
| CPUS | MEMORY | ... |
+---------------------+
| 4    | 4096   | ... |
+---------------------+
1 row in set (0.00 sec)

MariaDB [information_schema]> select * from information_schema.disks;
+------------+------------+----------+-----+
| Filesystem |  1K-blocks |     Used | ... |
+------------+------------+----------+-----+
| udev       |    8156512 |        0 |     |
| tmpfs      |    1635388 |     9820 | ... |
| /dev/sda3  |   47929956 | 28150612 |     |
| ...        |        ... |      ... |     |
+------------+------------+----------+-----+
10 rows in set (0.00 sec)
{code}

With this information available, MaxScale could automatically take corrective action in case the amount of used disk-space grows above, say, 90%.",,"Provide information_schema tables using which hardware information can be obtained. $end$ MaxScale should be able to react when some node of a cluster hits a disk-full situation.

For instance, if the master in a master-slave cluster hits a disk-full situation, MaxScale should promote some existing slave to master and remove the old master from the cluster. That way, a client connected to MaxScale would not necessarily detect anything special at all.

In order to do that, MaxScale must be able to detect that a disk-full situation has occurred. It's difficult and unreliable to do that using MariaDB itself, for instance, by trying to create a table and assuming it was a disk-full situation in case the creation fails or the attempt ends with a timeout. Further, that approach would not allow MaxScale to become aware of the situation _before_ the disk actually becomes full.

A more reliable way would be to actually monitor the disk-space situation on the node and use that for the decision making, but arranging that is rather complex and fragile.

A robust solution would be if there were a pseudo table or pseudo tables in {{information_schema}} using which information about the node could be found out. For instance as follows:

{code}
MariaDB [information_schema]> select * from information_schema.system;
+------+--------+-----+
| CPUS | MEMORY | ... |
+---------------------+
| 4    | 4096   | ... |
+---------------------+
1 row in set (0.00 sec)

MariaDB [information_schema]> select * from information_schema.disks;
+------------+------------+----------+-----+
| Filesystem |  1K-blocks |     Used | ... |
+------------+------------+----------+-----+
| udev       |    8156512 |        0 |     |
| tmpfs      |    1635388 |     9820 | ... |
| /dev/sda3  |   47929956 | 28150612 |     |
| ...        |        ... |      ... |     |
+------------+------------+----------+-----+
10 rows in set (0.00 sec)
{code}

With this information available, MaxScale could automatically take corrective action in case the amount of used disk-space grows above, say, 90%. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Critical,7,,3,2,5,1,0,0,0,,0,850,1,0,0,2018-03-13 18:25:25,Provide information_schema tables using which hardware information can be obtained.,"MaxScale should be able to react when some node of a cluster hits a disk-full situation.

For instance, if the master in a master-slave cluster hits a disk-full situation, MaxScale should promote some existing slave to master and remove the old master from the cluster. That way, a client connected to MaxScale would not necessarily detect anything special at all.

In order to do that, MaxScale must be able to detect that a disk-full situation has occurred. It's difficult and unreliable to do that using MariaDB itself, for instance, by trying to create a table and assuming it was a disk-full situation in case the creation fails or the attempt ends with a timeout. Further, that approach would not allow MaxScale to become aware of the situation _before_ the disk actually becomes full.

A more reliable way would be to actually monitor the disk-space situation on the node and use that for the decision making, but arranging that is rather complex and fragile.

A robust solution would be if there were a pseudo table or pseudo tables in {{information_schema}} using which information about the node could be found out. For instance as follows:

{code}
MariaDB [information_schema]> select * from information_schema.system;
+------+--------+-----+
| CPUS | MEMORY | ... |
+---------------------+
| 4    | 4096   | ... |
+---------------------+
1 row in set (0.00 sec)

MariaDB [information_schema]> select * from information_schema.disks;
+------------+------------+----------+-----+
| Filesystem |  1K-blocks |     Used | ... |
+------------+------------+----------+-----+
| udev       |    8156512 |        0 |     |
| tmpfs      |    1635388 |     9820 | ... |
| /dev/sda3  |   47929956 | 28150612 |     |
| ...        |        ... |      ... |     |
+------------+------------+----------+-----+
10 rows in set (0.00 sec)
{code}

With this information available, MaxScale could automatically take corrective action in case the amount of used disk-space grows above, say, 90%.",,0,0,0,0,0.0,"Provide information_schema tables using which hardware information can be obtained. $end$ MaxScale should be able to react when some node of a cluster hits a disk-full situation.

For instance, if the master in a master-slave cluster hits a disk-full situation, MaxScale should promote some existing slave to master and remove the old master from the cluster. That way, a client connected to MaxScale would not necessarily detect anything special at all.

In order to do that, MaxScale must be able to detect that a disk-full situation has occurred. It's difficult and unreliable to do that using MariaDB itself, for instance, by trying to create a table and assuming it was a disk-full situation in case the creation fails or the attempt ends with a timeout. Further, that approach would not allow MaxScale to become aware of the situation _before_ the disk actually becomes full.

A more reliable way would be to actually monitor the disk-space situation on the node and use that for the decision making, but arranging that is rather complex and fragile.

A robust solution would be if there were a pseudo table or pseudo tables in {{information_schema}} using which information about the node could be found out. For instance as follows:

{code}
MariaDB [information_schema]> select * from information_schema.system;
+------+--------+-----+
| CPUS | MEMORY | ... |
+---------------------+
| 4    | 4096   | ... |
+---------------------+
1 row in set (0.00 sec)

MariaDB [information_schema]> select * from information_schema.disks;
+------------+------------+----------+-----+
| Filesystem |  1K-blocks |     Used | ... |
+------------+------------+----------+-----+
| udev       |    8156512 |        0 |     |
| tmpfs      |    1635388 |     9820 | ... |
| /dev/sda3  |   47929956 | 28150612 |     |
| ...        |        ... |      ... |     |
+------------+------------+----------+-----+
10 rows in set (0.00 sec)
{code}

With this information available, MaxScale could automatically take corrective action in case the amount of used disk-space grows above, say, 90%. $acceptance criteria:$",0,0,0,0,0,0,0,2501.58,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
697,MDEV-14592,Task,MDEV,2017-12-05 14:56:48,,0,Custom Aggregates Usage Status Variable,We should add a system variable to report number of queries which make use of Custom Aggregate Functions to get a feel as to how many users are using it.,,Custom Aggregates Usage Status Variable $end$ We should add a system variable to report number of queries which make use of Custom Aggregate Functions to get a feel as to how many users are using it. $acceptance criteria:$,,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,10,,0,2,0,1,0,0,0,,0,850,2,0,0,2018-03-21 20:41:05,Custom Aggregates Usage Status Variable,We should add a system variable to report number of queries which make use of Custom Aggregate Functions to get a feel as to how many users are using it.,,0,0,0,0,0.0,Custom Aggregates Usage Status Variable $end$ We should add a system variable to report number of queries which make use of Custom Aggregate Functions to get a feel as to how many users are using it. $acceptance criteria:$,0,0,0,0,0,0,0,2549.73,6,3,0.5,3,0.5,3,0.5,2,0.333333,2,0.333333
698,MDEV-14593,Task,MDEV,2017-12-05 15:04:21,,0,human-readable XA RECOVER,"{{XA RECOVER}} prints data in binary, because that format — raw XIDs — is most useful for the transaction coordinator.

But occasionally, a human intervention is necessary. For this use case it would be convenient if {{XA RECOVER}} could return XID in some format that can be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}.

Also, {{mysql}} command line client has a {{--binary-as-hex}} option, but it does not work for {{XA RECOVER}}. It should be fixed, if possible.

MySQL has the command {{XA RECOVER CONVERT XID}} that returns the value of XID in hex. But it cannot be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}.",,"human-readable XA RECOVER $end$ {{XA RECOVER}} prints data in binary, because that format — raw XIDs — is most useful for the transaction coordinator.

But occasionally, a human intervention is necessary. For this use case it would be convenient if {{XA RECOVER}} could return XID in some format that can be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}.

Also, {{mysql}} command line client has a {{--binary-as-hex}} option, but it does not work for {{XA RECOVER}}. It should be fixed, if possible.

MySQL has the command {{XA RECOVER CONVERT XID}} that returns the value of XID in hex. But it cannot be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,11,,3,3,3,1,0,1,0,,0,850,1,1,0,2017-12-12 15:44:46,human-readable XA RECOVER,"{{XA RECOVER}} prints data in binary, because that format — raw XIDs — is most useful for the transaction coordinator.

But occasionally, a human intervention is necessary. For this use case it would be convenient if {{XA RECOVER}} could return XID in some format that can be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}.

Also, {{mysql}} command line client has a {{--binary-as-hex}} option, but it does not work for {{XA RECOVER}}. It should be fixed, if possible.

MySQL has the command {{XA RECOVER CONVERT XID}} that returns the value of XID in hex. But it cannot be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}.",,0,0,0,0,0.0,"human-readable XA RECOVER $end$ {{XA RECOVER}} prints data in binary, because that format — raw XIDs — is most useful for the transaction coordinator.

But occasionally, a human intervention is necessary. For this use case it would be convenient if {{XA RECOVER}} could return XID in some format that can be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}.

Also, {{mysql}} command line client has a {{--binary-as-hex}} option, but it does not work for {{XA RECOVER}}. It should be fixed, if possible.

MySQL has the command {{XA RECOVER CONVERT XID}} that returns the value of XID in hex. But it cannot be directly copied into {{XA COMMIT}} or {{XA ROLLBACK}}. $acceptance criteria:$",0,0,0,0,0,0,0,168.667,53,27,0.509434,20,0.377358,15,0.283019,13,0.245283,10,0.188679
699,MDEV-14626,Task,MDEV,2017-12-12 09:23:18,,0,10.1.30 merge,"* 5.5→10.0 (/) Minimal conflicts, test case adjustments due to partition code changes.
* 10.0→10.1 (/) Minimal conflicts, only related to roles features.
* 10.0-galera→10.1 (/) Nothing new except VERSION BUMP",,"10.1.30 merge $end$ * 5.5→10.0 (/) Minimal conflicts, test case adjustments due to partition code changes.
* 10.0→10.1 (/) Minimal conflicts, only related to roles features.
* 10.0-galera→10.1 (/) Nothing new except VERSION BUMP $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,10,,0,0,0,1,0,4,0,,0,850,0,0,0,2017-12-12 11:22:00,10.1.30 merge,"* 5.5→10.0
* 10.0→10.1
* 10.0-galera→10.1",,0,4,0,25,2.27273,"10.1.30 merge $end$ * 5.5→10.0
* 10.0→10.1
* 10.0-galera→10.1 $acceptance criteria:$",4,1,1,1,1,1,1,1.96667,54,27,0.5,20,0.37037,15,0.277778,13,0.240741,10,0.185185
700,MDEV-14638,Task,MDEV,2017-12-13 11:17:06,MDEV-14442,0,Replace trx_sys_t::rw_trx_set with LF_HASH,"trx_sys_t::rw_trx_set implemented as std::set, which does a few quite expensive operations under trx_sys_t::mutex protection. E.g. malloc/free when adding/removing elements.

Aim of this task is to reduce trx_sys_t::mutex contention by replacing rw_trx_set with LF_HASH.",,"Replace trx_sys_t::rw_trx_set with LF_HASH $end$ trx_sys_t::rw_trx_set implemented as std::set, which does a few quite expensive operations under trx_sys_t::mutex protection. E.g. malloc/free when adding/removing elements.

Aim of this task is to reduce trx_sys_t::mutex contention by replacing rw_trx_set with LF_HASH. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,9,,3,7,5,1,0,0,0,,0,850,1,0,0,2018-01-10 11:31:26,Replace trx_sys_t::rw_trx_set with LF_HASH,"trx_sys_t::rw_trx_set implemented as std::set, which does a few quite expensive operations under trx_sys_t::mutex protection. E.g. malloc/free when adding/removing elements.

Aim of this task is to reduce trx_sys_t::mutex contention by replacing rw_trx_set with LF_HASH.",,0,0,0,0,0.0,"Replace trx_sys_t::rw_trx_set with LF_HASH $end$ trx_sys_t::rw_trx_set implemented as std::set, which does a few quite expensive operations under trx_sys_t::mutex protection. E.g. malloc/free when adding/removing elements.

Aim of this task is to reduce trx_sys_t::mutex contention by replacing rw_trx_set with LF_HASH. $acceptance criteria:$",0,0,0,0,0,0,0,672.233,25,3,0.12,2,0.08,2,0.08,1,0.04,1,0.04
701,MDEV-14705,Task,MDEV,2017-12-19 01:05:20,,0,systemd: EXTEND_TIMEOUT_USEC= to avoid startup and shutdown timeouts,"MDEV-9202, MDEV-8509 shows cases where the systemd timeout isn't sufficient to preform initialization/shut-down.

Since https://github.com/systemd/systemd/commit/a327431bd168b2f327f3cd422379e213c643f2a5 released in system v236 Type=notify service can now advice to the systemd service manager they are still working to avoid the service timing out.

The use of EXTEND_TIMEOUT_USEC= on older services has no effect and is therefore compatible.

This needs to be included in (feel free to correct/extend):
* buffer pool dump - buf_dump (storage/innobase/buf/buf0dump.cc)
* redo log recovery - log_group_read_log_seg (storage/innobase/log/log0log.cc)
* undo(?) recovery - recv_recover_page_func (storage/innobase/log/log0recv.cc)
* change buffer?
* merge buffer?

I was planning on making the 15 seconds of recv_sys_t->report() more general with respect to interval, and use a define INNODB_REPORT_INTERVAL (include/univ.i?) as the basis for this form of watchdog. I'd send notify messages of INNODB_REPORT_INTERVAL * 2 as an acceptable margin.

Anywhere else or other suggestions [~marko], [~jplindst]?
* galera SST scripts - donor and recipient

Any other server/engine slow points to account for?

Target 10.3 and then look at a backport?",,"systemd: EXTEND_TIMEOUT_USEC= to avoid startup and shutdown timeouts $end$ MDEV-9202, MDEV-8509 shows cases where the systemd timeout isn't sufficient to preform initialization/shut-down.

Since https://github.com/systemd/systemd/commit/a327431bd168b2f327f3cd422379e213c643f2a5 released in system v236 Type=notify service can now advice to the systemd service manager they are still working to avoid the service timing out.

The use of EXTEND_TIMEOUT_USEC= on older services has no effect and is therefore compatible.

This needs to be included in (feel free to correct/extend):
* buffer pool dump - buf_dump (storage/innobase/buf/buf0dump.cc)
* redo log recovery - log_group_read_log_seg (storage/innobase/log/log0log.cc)
* undo(?) recovery - recv_recover_page_func (storage/innobase/log/log0recv.cc)
* change buffer?
* merge buffer?

I was planning on making the 15 seconds of recv_sys_t->report() more general with respect to interval, and use a define INNODB_REPORT_INTERVAL (include/univ.i?) as the basis for this form of watchdog. I'd send notify messages of INNODB_REPORT_INTERVAL * 2 as an acceptable margin.

Anywhere else or other suggestions [~marko], [~jplindst]?
* galera SST scripts - donor and recipient

Any other server/engine slow points to account for?

Target 10.3 and then look at a backport? $acceptance criteria:$",,Daniel Black,Daniel Black,Critical,11,,15,12,16,1,0,0,0,,0,850,10,0,0,2018-03-14 10:06:06,systemd: EXTEND_TIMEOUT_USEC= to avoid startup and shutdown timeouts,"MDEV-9202, MDEV-8509 shows cases where the systemd timeout isn't sufficient to preform initialization/shut-down.

Since https://github.com/systemd/systemd/commit/a327431bd168b2f327f3cd422379e213c643f2a5 released in system v236 Type=notify service can now advice to the systemd service manager they are still working to avoid the service timing out.

The use of EXTEND_TIMEOUT_USEC= on older services has no effect and is therefore compatible.

This needs to be included in (feel free to correct/extend):
* buffer pool dump - buf_dump (storage/innobase/buf/buf0dump.cc)
* redo log recovery - log_group_read_log_seg (storage/innobase/log/log0log.cc)
* undo(?) recovery - recv_recover_page_func (storage/innobase/log/log0recv.cc)
* change buffer?
* merge buffer?

I was planning on making the 15 seconds of recv_sys_t->report() more general with respect to interval, and use a define INNODB_REPORT_INTERVAL (include/univ.i?) as the basis for this form of watchdog. I'd send notify messages of INNODB_REPORT_INTERVAL * 2 as an acceptable margin.

Anywhere else or other suggestions [~marko], [~jplindst]?
* galera SST scripts - donor and recipient

Any other server/engine slow points to account for?

Target 10.3 and then look at a backport?",,0,0,0,0,0.0,"systemd: EXTEND_TIMEOUT_USEC= to avoid startup and shutdown timeouts $end$ MDEV-9202, MDEV-8509 shows cases where the systemd timeout isn't sufficient to preform initialization/shut-down.

Since https://github.com/systemd/systemd/commit/a327431bd168b2f327f3cd422379e213c643f2a5 released in system v236 Type=notify service can now advice to the systemd service manager they are still working to avoid the service timing out.

The use of EXTEND_TIMEOUT_USEC= on older services has no effect and is therefore compatible.

This needs to be included in (feel free to correct/extend):
* buffer pool dump - buf_dump (storage/innobase/buf/buf0dump.cc)
* redo log recovery - log_group_read_log_seg (storage/innobase/log/log0log.cc)
* undo(?) recovery - recv_recover_page_func (storage/innobase/log/log0recv.cc)
* change buffer?
* merge buffer?

I was planning on making the 15 seconds of recv_sys_t->report() more general with respect to interval, and use a define INNODB_REPORT_INTERVAL (include/univ.i?) as the basis for this form of watchdog. I'd send notify messages of INNODB_REPORT_INTERVAL * 2 as an acceptable margin.

Anywhere else or other suggestions [~marko], [~jplindst]?
* galera SST scripts - donor and recipient

Any other server/engine slow points to account for?

Target 10.3 and then look at a backport? $acceptance criteria:$",0,0,0,0,0,0,0,2049.0,7,1,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
702,MDEV-14709,Task,MDEV,2017-12-19 09:07:57,,0,10.2.12 merge,"* 10.1 (/)
* InnoDB (already merged)
* Connect (up-to-date) (/)
* C/C (/)
* TokuDB (x) not merged, too complex but progress has been made",,"10.2.12 merge $end$ * 10.1 (/)
* InnoDB (already merged)
* Connect (up-to-date) (/)
* C/C (/)
* TokuDB (x) not merged, too complex but progress has been made $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,8,,0,0,0,1,0,4,0,,0,850,0,0,0,2017-12-19 09:08:17,10.2.12 merge,"* 10.1
* InnoDB
* Connect
* C/C
* TokuDB",,0,4,0,16,1.06667,"10.2.12 merge $end$ * 10.1
* InnoDB
* Connect
* C/C
* TokuDB $acceptance criteria:$",4,1,1,1,1,0,1,0.0,55,28,0.509091,21,0.381818,16,0.290909,14,0.254545,11,0.2
703,MDEV-14746,Task,MDEV,2017-12-22 19:41:33,,0,mariabackup doesn't read [mariabackup] option group in configuration file,"The mariabackup tool currently reads the [xtrabackup] option group from configuration files. It inherited this behavior from Percona XtraBackup, and it probably makes sense to keep it. However, wouldn't it also make sense for mariabackup to read the [mariabackup] option group? It doesn't currently seem to do so.

e.g. if someone wants to set a user name and password, they can currently do the following:

{noformat}
[xtrabackup]
user=sstuser
password=password
{noformat}

But maybe it would make sense to also support the following?:

{noformat}
[mariabackup]
user=sstuser
password=password
{noformat}",,"mariabackup doesn't read [mariabackup] option group in configuration file $end$ The mariabackup tool currently reads the [xtrabackup] option group from configuration files. It inherited this behavior from Percona XtraBackup, and it probably makes sense to keep it. However, wouldn't it also make sense for mariabackup to read the [mariabackup] option group? It doesn't currently seem to do so.

e.g. if someone wants to set a user name and password, they can currently do the following:

{noformat}
[xtrabackup]
user=sstuser
password=password
{noformat}

But maybe it would make sense to also support the following?:

{noformat}
[mariabackup]
user=sstuser
password=password
{noformat} $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,7,,5,1,11,1,0,0,0,,0,850,0,0,0,2018-01-18 12:46:19,mariabackup doesn't read [mariabackup] option group in configuration file,"The mariabackup tool currently reads the [xtrabackup] option group from configuration files. It inherited this behavior from Percona XtraBackup, and it probably makes sense to keep it. However, wouldn't it also make sense for mariabackup to read the [mariabackup] option group? It doesn't currently seem to do so.

e.g. if someone wants to set a user name and password, they can currently do the following:

{noformat}
[xtrabackup]
user=sstuser
password=password
{noformat}

But maybe it would make sense to also support the following?:

{noformat}
[mariabackup]
user=sstuser
password=password
{noformat}",,0,0,0,0,0.0,"mariabackup doesn't read [mariabackup] option group in configuration file $end$ The mariabackup tool currently reads the [xtrabackup] option group from configuration files. It inherited this behavior from Percona XtraBackup, and it probably makes sense to keep it. However, wouldn't it also make sense for mariabackup to read the [mariabackup] option group? It doesn't currently seem to do so.

e.g. if someone wants to set a user name and password, they can currently do the following:

{noformat}
[xtrabackup]
user=sstuser
password=password
{noformat}

But maybe it would make sense to also support the following?:

{noformat}
[mariabackup]
user=sstuser
password=password
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,641.067,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
704,MDEV-14756,Task,MDEV,2017-12-24 12:15:07,MDEV-14442,0,Remove trx_sys_t::rw_trx_list,"rw_trx_list contains ACTIVE, PREPARED and recovered COMMITTED transactions.
Move recovered COMMITTED transactions to purge_list.
ACTIVE and PREPARED transactions are already available through rw_trx_hash.

Aim of this task is to improve scalability.",,"Remove trx_sys_t::rw_trx_list $end$ rw_trx_list contains ACTIVE, PREPARED and recovered COMMITTED transactions.
Move recovered COMMITTED transactions to purge_list.
ACTIVE and PREPARED transactions are already available through rw_trx_hash.

Aim of this task is to improve scalability. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,9,,2,2,3,2,0,0,0,,0,850,2,0,0,2018-01-10 11:30:07,Remove trx_sys_t::rw_trx_list,"rw_trx_list contains ACTIVE, PREPARED and recovered COMMITTED transactions.
Move recovered COMMITTED transactions to purge_list.
ACTIVE and PREPARED transactions are already available through rw_trx_hash.

Aim of this task is to improve scalability.",,0,0,0,0,0.0,"Remove trx_sys_t::rw_trx_list $end$ rw_trx_list contains ACTIVE, PREPARED and recovered COMMITTED transactions.
Move recovered COMMITTED transactions to purge_list.
ACTIVE and PREPARED transactions are already available through rw_trx_hash.

Aim of this task is to improve scalability. $acceptance criteria:$",0,0,0,0,0,0,1,407.25,26,3,0.115385,2,0.0769231,2,0.0769231,1,0.0384615,1,0.0384615
705,MDEV-14908,Task,MDEV,2018-01-09 23:27:44,,0,5.5.59 merge,"mysql-5.5 → 5.5
xtradb → 5.5",,"5.5.59 merge $end$ mysql-5.5 → 5.5
xtradb → 5.5 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-01-10 11:36:14,5.5.59 merge,"mysql-5.5 → 5.5
xtradb → 5.5",,0,0,0,0,0.0,"5.5.59 merge $end$ mysql-5.5 → 5.5
xtradb → 5.5 $acceptance criteria:$",0,0,0,0,0,0,0,12.1333,56,29,0.517857,22,0.392857,17,0.303571,15,0.267857,11,0.196429
706,MDEV-14912,Task,MDEV,2018-01-10 12:51:57,,0,benchmark the effect of the KPTI kernel patch,benchmark the effect of the KPTI kernel patch,,benchmark the effect of the KPTI kernel patch $end$ benchmark the effect of the KPTI kernel patch $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-01-10 12:51:57,benchmark the effect of the KPTI kernel patch,benchmark the effect of the KPTI kernel patch,,0,0,0,0,0.0,benchmark the effect of the KPTI kernel patch $end$ benchmark the effect of the KPTI kernel patch $acceptance criteria:$,0,0,0,0,0,0,0,0.0,57,29,0.508772,22,0.385965,17,0.298246,15,0.263158,11,0.192982
707,MDEV-14958,Task,MDEV,2018-01-16 08:48:40,,0,Merge new release of InnoDB MySQL 5.7.21 to 10.2,"The following changes between MySQL 5.7.20 and MySQL 5.7.21 touch InnoDB files.

[Raise version number after cloning 5.7.20|https://github.com/mysql/mysql-server/commit/bfed7d5c35d9a031df7da38926a4fccccd60ffb9]
This one we should apply; MariaDB 10.2 will report an InnoDB version number.

[Backport some additional GenError dependencies|https://github.com/mysql/mysql-server/commit/a08a6a554e8d7664fec4597992fb4ae77024ad2b]
The MariaDB {{cmake}} files do use {{GenError}} outside InnoDB, so we could consider porting this.

[Bug #25729649 LOCK0LOCK.CC:NNN:ADD_POSITION != __NULL|https://github.com/mysql/mysql-server/commit/1b3da22c277b80bb3dd773f07fdb8fd9bc6044f3]
This appears to fix a bug in high-priority replication slave transactions that affects {{SPATIAL INDEX}}. The high-priority transactions (including {{TRX_STATE_FORCED_ROLLBACK}} must be dead code in MariaDB, because there is no {{tx_priority}} at the SQL layer, but we can apply the fix nevertheless. Only the test cannot be ported.

[Bug#26825211 BACKPORT FIX FOR #25643811 TO 5.7|https://github.com/mysql/mysql-server/commit/ba1a99c5cd7dd6204abbe8d177c7f71a48e53fe3]
These are compilation fixes for GCC 7, in InnoDB mostly fixing {{-Wimplicit-fallthrough}}. We can skip this, because MariaDB already got similar fixes.

[Bug #25076416  VIRTUAL COLUMN IS NOT CONSIDERED WHILE FETCHING THE AUTOINC COLUMN|https://github.com/mysql/mysql-server/commit/23228b9b835271e03fd975c44c5783705947151c]
There is no test case, and the code patch does not directly apply. MariaDB 10.2 thanks to MDEV-6204 already simplified this code, invoking {{row_search_autoinc_read_column()}} on the first field of the index, without bothering about columns or column names at all.

[Revert ""Bug #25076416 VIRTUAL COLUMN IS NOT CONSIDERED WHILE""|https://github.com/mysql/mysql-server/commit/5c312c43444024d91ce56050f4078b169f1d635a]
This appears to revert the above change.

[BUG#21625016: INNODB FULL TEXT CASE SENSITIVE NOT WORKING.|https://github.com/mysql/mysql-server/commit/4206a24fe8e06ab6f937855cba184b669e02270e]
This one we will get via the merge of MySQL 5.6.39 to MariaDB 10.0.
[The merge from 5.6.39 to 5.7.21|https://github.com/mysql/mysql-server/commit/9a0c517232c5da64f3aa0b52e7131de2ed227ee8] contains some conflict resolution due to different code formatting between 5.6 and 5.7, but apparently no functional change.

[Bug#26918439 ROW0SEL.CC:5178:32 ERROR: ISO C++ FORBIDS COMPARISON BETWEEN POINTER AND INTEGE|https://github.com/mysql/mysql-server/commit/ffdf388df94b240757af3a546efb2a035f2d41d3]
This is one more follow-up to fix the numerous problems introduced by
[Bug #23481444 OPTIMISER CALL ROW_SEARCH_MVCC() AND READ THE INDEX APPLIED BY UNCOMMITTED ROWS|https://github.com/mysql/mysql-server/commit/2f6741ef8e729e8ea6723ca434f95a40036e1d25] in MySQL 5.7.18. As noted in MDEV-11751, MariaDB did not include this change due to the perceived high risk and the questionable benefit.

[two trivial zero-initializations, to avoid ""may be used unitialized"" warnings|https://github.com/mysql/mysql-server/commit/7f9d06b6005b51a9edc8e31587b0f57c677dd162]
The InnoDB change applies to the above-mentioned code that we chose to skip in MDEV-11751. The change to {{Gis_multi_polygon::get_mbr()}} does not apply, because MariaDB is using more straightforward code (initializing the variable from a function return value, not by an output parameter).

[WL#10473: InnoDB: Deprecate innodb_undo_tablespaces in 5.7|https://github.com/mysql/mysql-server/commit/5017e6f5fb92d2c885996727956ffa8fc9ed21c3]
MariaDB will skip this. We will likely deprecate and remove parameters related to rollback segments and undo tablespaces in the course of MDEV-11657.

[Bug #26390658   RENAMING A PARTITIONED TABLE DOES NOT UPDATE MYSQL.INNODB_TABLE_STATS|https://github.com/mysql/mysql-server/commit/eba6a5fac02877df2076faf2b6235975f4c6d4ac]
The test case passes in MariaDB. The code changes only seem necessary due to the InnoDB native partitioning ({{ha_innopart}}) that was not merged to MariaDB.

[Backport patch for Bug#16877045 5.6-CLUSTER-7.3 WIN32 SQL_YACC.CC BUILD PROBLEM|https://github.com/mysql/mysql-server/commit/3a2f365bd2d65f8ea865113027125c947a6c9f3e]
This is a 5.6.39 patch that includes the above ‘two trivial zero-initializations’ patch from 5.7. If we need the {{cmake}} changes, we would get those via the merge from 5.6.39 to MariaDB 10.0.

[Bug #26191879 FOREIGN KEY CASCADES USE EXCESSIVE MEMORY|https://github.com/mysql/mysql-server/commit/7b26dc98a624d5cdf79cd5eee455cb03e3bccb5a]
The explicit recursion stack was replaced with procedural recursion, apparently to plug a memory leak. This fix will increase the stack memory usage of InnoDB, and to work around that, it will reduce the maximum depth of {{FOREIGN KEY}} cascade operations to 15 from the 255.
I think that MariaDB should try to fix the actual problem in MDEV-14222 instead of applying this patch.

[Bug#26152751: INNODB LEAKS MEMORY, PERFORMANCE_SCHEMA FILE_INSTANCES #SQL-IB3129987-252773.IBD|https://github.com/mysql/mysql-server/commit/6fa258c56c64f028ade6aaa73b11951e5a68c4a0]
This patch introduces {{PSI_FILE_CALL(end_file_rename_wait)}} in order to plug some kind of a race condition that can lead to a memory leak. The changes are mostly outside InnoDB. A test case is included. I filed MDEV-15179 to address this.

[BUG#26734457 BACKPORT BUG#22305994 TO 5.6 AND 5.7|https://github.com/mysql/mysql-server/commit/cd0b9f7de6561613338774f6253031a3bcd9db7e]
This 5.6.39 patch is replacing {{GetSystemTimeAsFileTime()}} with {{GetSystemTimePreciseAsFileTime()}} when it is available. The call is used in {{ut_gettimeofday()}}, which in turn is used by {{ut_usectime()}}, {{ut_time_us()}}, {{ut_time_ms()}}. The {{ut_usectime()}} in turn is used by {{lock_wait_suspend_thread()}} and {{os_event::wait_time_low()}}. The other calls are used in various other places, affecting both logic and diagnostics.
According to [~wlad], we do not need this.

[Merge branch 'mysql-5.6' into mysql-5.7|https://github.com/mysql/mysql-server/commit/f028fb0c2efbcdc057d882a9da6b836ed3dec25d]
The merge of the above, with a few conflict resolutions

[Bug #22486025   INNODB: FAILING ASSERTION: KEY_LEN != 0 || FIND_FLAG != HA_READ_KEY_EXACT|https://github.com/mysql/mysql-server/commit/1a13f7f98772da507b54018e05c4b88abbaac32c]
This bug only affects the MySQL 5.7 feature of using InnoDB for the temporary tables that are internally created during SQL execution. The code was never active in MariaDB, and it was removed in MDEV-11487.

[Bug #26256456   INNODB CRASHED WHEN MASTER THREAD EVICT DICT_TABLE_T OBJECT|https://github.com/mysql/mysql-server/commit/ed05c8786e6d4575afe9da3333d0f80be8649411]
This is the 5.6.39 version of a patch that seems to be a bug in the eviction of table definitions from the InnoDB dictionary cache, a feature that was introduced in MySQL 5.6. MDEV-13051 already fixed this in MariaDB.

[Bug #26492721   MYSQL/INNODB CRASHES DURING ALTER TABLE|https://github.com/mysql/mysql-server/commit/81753e747324436a91d4e8a4758293978ea7ea30]
This MySQL 5.6.39 change fixes a bug in the {{dict_sys->size}} bookkeeping when renaming columns. The whole field is somewhat questionable, because some memory allocations related to the InnoDB data dictionary cache are bypassing this bookkeeping altogether. MDEV-13325 removed {{dict_sys->size}}.

[Bug #24296076   INNODB REPORTS WARNING WHILE INNODB_UNDO_LOG_TRUNCATE IS ENABLED|https://github.com/mysql/mysql-server/commit/63a540c8c3e93b7f15aec6260c0a85f1cd07f6dc]
This is disabling a warning in non-debug builds. Apparently Oracle came to the same conclusion as we in MDEV-14916, because they reworded the message. MariaDB removed the message altogether.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/29d37f1b6abf0346c3de6b302650535e4ae9b248]
This introduces a warning message when the MySQL 5.7 buffer pool resizing cannot be done. MariaDB should introduce a similar warning, but with clearer wording.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/e0ad9ea241f038b197a68520336da3be02b3803b]
A follow-up fix to the above.

[Bug #27212129 INNODB: A RECORD LOCK WAIT HAPPENS IN A DICTIONARY OPERATION|https://github.com/mysql/mysql-server/commit/cf3e608fe5888e78121a4559f5edab4feb367546]
This fix, without a test case, fixes an InnoDB hang when the persistent statistics of partitioned tables are updated. It looks like the function {{row_rename_partitions_for_mysql()}} is only needed because of native partitioning, and should be removed from MariaDB 10.2.",,"Merge new release of InnoDB MySQL 5.7.21 to 10.2 $end$ The following changes between MySQL 5.7.20 and MySQL 5.7.21 touch InnoDB files.

[Raise version number after cloning 5.7.20|https://github.com/mysql/mysql-server/commit/bfed7d5c35d9a031df7da38926a4fccccd60ffb9]
This one we should apply; MariaDB 10.2 will report an InnoDB version number.

[Backport some additional GenError dependencies|https://github.com/mysql/mysql-server/commit/a08a6a554e8d7664fec4597992fb4ae77024ad2b]
The MariaDB {{cmake}} files do use {{GenError}} outside InnoDB, so we could consider porting this.

[Bug #25729649 LOCK0LOCK.CC:NNN:ADD_POSITION != __NULL|https://github.com/mysql/mysql-server/commit/1b3da22c277b80bb3dd773f07fdb8fd9bc6044f3]
This appears to fix a bug in high-priority replication slave transactions that affects {{SPATIAL INDEX}}. The high-priority transactions (including {{TRX_STATE_FORCED_ROLLBACK}} must be dead code in MariaDB, because there is no {{tx_priority}} at the SQL layer, but we can apply the fix nevertheless. Only the test cannot be ported.

[Bug#26825211 BACKPORT FIX FOR #25643811 TO 5.7|https://github.com/mysql/mysql-server/commit/ba1a99c5cd7dd6204abbe8d177c7f71a48e53fe3]
These are compilation fixes for GCC 7, in InnoDB mostly fixing {{-Wimplicit-fallthrough}}. We can skip this, because MariaDB already got similar fixes.

[Bug #25076416  VIRTUAL COLUMN IS NOT CONSIDERED WHILE FETCHING THE AUTOINC COLUMN|https://github.com/mysql/mysql-server/commit/23228b9b835271e03fd975c44c5783705947151c]
There is no test case, and the code patch does not directly apply. MariaDB 10.2 thanks to MDEV-6204 already simplified this code, invoking {{row_search_autoinc_read_column()}} on the first field of the index, without bothering about columns or column names at all.

[Revert ""Bug #25076416 VIRTUAL COLUMN IS NOT CONSIDERED WHILE""|https://github.com/mysql/mysql-server/commit/5c312c43444024d91ce56050f4078b169f1d635a]
This appears to revert the above change.

[BUG#21625016: INNODB FULL TEXT CASE SENSITIVE NOT WORKING.|https://github.com/mysql/mysql-server/commit/4206a24fe8e06ab6f937855cba184b669e02270e]
This one we will get via the merge of MySQL 5.6.39 to MariaDB 10.0.
[The merge from 5.6.39 to 5.7.21|https://github.com/mysql/mysql-server/commit/9a0c517232c5da64f3aa0b52e7131de2ed227ee8] contains some conflict resolution due to different code formatting between 5.6 and 5.7, but apparently no functional change.

[Bug#26918439 ROW0SEL.CC:5178:32 ERROR: ISO C++ FORBIDS COMPARISON BETWEEN POINTER AND INTEGE|https://github.com/mysql/mysql-server/commit/ffdf388df94b240757af3a546efb2a035f2d41d3]
This is one more follow-up to fix the numerous problems introduced by
[Bug #23481444 OPTIMISER CALL ROW_SEARCH_MVCC() AND READ THE INDEX APPLIED BY UNCOMMITTED ROWS|https://github.com/mysql/mysql-server/commit/2f6741ef8e729e8ea6723ca434f95a40036e1d25] in MySQL 5.7.18. As noted in MDEV-11751, MariaDB did not include this change due to the perceived high risk and the questionable benefit.

[two trivial zero-initializations, to avoid ""may be used unitialized"" warnings|https://github.com/mysql/mysql-server/commit/7f9d06b6005b51a9edc8e31587b0f57c677dd162]
The InnoDB change applies to the above-mentioned code that we chose to skip in MDEV-11751. The change to {{Gis_multi_polygon::get_mbr()}} does not apply, because MariaDB is using more straightforward code (initializing the variable from a function return value, not by an output parameter).

[WL#10473: InnoDB: Deprecate innodb_undo_tablespaces in 5.7|https://github.com/mysql/mysql-server/commit/5017e6f5fb92d2c885996727956ffa8fc9ed21c3]
MariaDB will skip this. We will likely deprecate and remove parameters related to rollback segments and undo tablespaces in the course of MDEV-11657.

[Bug #26390658   RENAMING A PARTITIONED TABLE DOES NOT UPDATE MYSQL.INNODB_TABLE_STATS|https://github.com/mysql/mysql-server/commit/eba6a5fac02877df2076faf2b6235975f4c6d4ac]
The test case passes in MariaDB. The code changes only seem necessary due to the InnoDB native partitioning ({{ha_innopart}}) that was not merged to MariaDB.

[Backport patch for Bug#16877045 5.6-CLUSTER-7.3 WIN32 SQL_YACC.CC BUILD PROBLEM|https://github.com/mysql/mysql-server/commit/3a2f365bd2d65f8ea865113027125c947a6c9f3e]
This is a 5.6.39 patch that includes the above ‘two trivial zero-initializations’ patch from 5.7. If we need the {{cmake}} changes, we would get those via the merge from 5.6.39 to MariaDB 10.0.

[Bug #26191879 FOREIGN KEY CASCADES USE EXCESSIVE MEMORY|https://github.com/mysql/mysql-server/commit/7b26dc98a624d5cdf79cd5eee455cb03e3bccb5a]
The explicit recursion stack was replaced with procedural recursion, apparently to plug a memory leak. This fix will increase the stack memory usage of InnoDB, and to work around that, it will reduce the maximum depth of {{FOREIGN KEY}} cascade operations to 15 from the 255.
I think that MariaDB should try to fix the actual problem in MDEV-14222 instead of applying this patch.

[Bug#26152751: INNODB LEAKS MEMORY, PERFORMANCE_SCHEMA FILE_INSTANCES #SQL-IB3129987-252773.IBD|https://github.com/mysql/mysql-server/commit/6fa258c56c64f028ade6aaa73b11951e5a68c4a0]
This patch introduces {{PSI_FILE_CALL(end_file_rename_wait)}} in order to plug some kind of a race condition that can lead to a memory leak. The changes are mostly outside InnoDB. A test case is included. I filed MDEV-15179 to address this.

[BUG#26734457 BACKPORT BUG#22305994 TO 5.6 AND 5.7|https://github.com/mysql/mysql-server/commit/cd0b9f7de6561613338774f6253031a3bcd9db7e]
This 5.6.39 patch is replacing {{GetSystemTimeAsFileTime()}} with {{GetSystemTimePreciseAsFileTime()}} when it is available. The call is used in {{ut_gettimeofday()}}, which in turn is used by {{ut_usectime()}}, {{ut_time_us()}}, {{ut_time_ms()}}. The {{ut_usectime()}} in turn is used by {{lock_wait_suspend_thread()}} and {{os_event::wait_time_low()}}. The other calls are used in various other places, affecting both logic and diagnostics.
According to [~wlad], we do not need this.

[Merge branch 'mysql-5.6' into mysql-5.7|https://github.com/mysql/mysql-server/commit/f028fb0c2efbcdc057d882a9da6b836ed3dec25d]
The merge of the above, with a few conflict resolutions

[Bug #22486025   INNODB: FAILING ASSERTION: KEY_LEN != 0 || FIND_FLAG != HA_READ_KEY_EXACT|https://github.com/mysql/mysql-server/commit/1a13f7f98772da507b54018e05c4b88abbaac32c]
This bug only affects the MySQL 5.7 feature of using InnoDB for the temporary tables that are internally created during SQL execution. The code was never active in MariaDB, and it was removed in MDEV-11487.

[Bug #26256456   INNODB CRASHED WHEN MASTER THREAD EVICT DICT_TABLE_T OBJECT|https://github.com/mysql/mysql-server/commit/ed05c8786e6d4575afe9da3333d0f80be8649411]
This is the 5.6.39 version of a patch that seems to be a bug in the eviction of table definitions from the InnoDB dictionary cache, a feature that was introduced in MySQL 5.6. MDEV-13051 already fixed this in MariaDB.

[Bug #26492721   MYSQL/INNODB CRASHES DURING ALTER TABLE|https://github.com/mysql/mysql-server/commit/81753e747324436a91d4e8a4758293978ea7ea30]
This MySQL 5.6.39 change fixes a bug in the {{dict_sys->size}} bookkeeping when renaming columns. The whole field is somewhat questionable, because some memory allocations related to the InnoDB data dictionary cache are bypassing this bookkeeping altogether. MDEV-13325 removed {{dict_sys->size}}.

[Bug #24296076   INNODB REPORTS WARNING WHILE INNODB_UNDO_LOG_TRUNCATE IS ENABLED|https://github.com/mysql/mysql-server/commit/63a540c8c3e93b7f15aec6260c0a85f1cd07f6dc]
This is disabling a warning in non-debug builds. Apparently Oracle came to the same conclusion as we in MDEV-14916, because they reworded the message. MariaDB removed the message altogether.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/29d37f1b6abf0346c3de6b302650535e4ae9b248]
This introduces a warning message when the MySQL 5.7 buffer pool resizing cannot be done. MariaDB should introduce a similar warning, but with clearer wording.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/e0ad9ea241f038b197a68520336da3be02b3803b]
A follow-up fix to the above.

[Bug #27212129 INNODB: A RECORD LOCK WAIT HAPPENS IN A DICTIONARY OPERATION|https://github.com/mysql/mysql-server/commit/cf3e608fe5888e78121a4559f5edab4feb367546]
This fix, without a test case, fixes an InnoDB hang when the persistent statistics of partitioned tables are updated. It looks like the function {{row_rename_partitions_for_mysql()}} is only needed because of native partitioning, and should be removed from MariaDB 10.2. $acceptance criteria:$",,Marko Mäkelä,Marko Mäkelä,Major,10,,6,1,7,2,0,1,0,,0,850,1,0,0,2018-01-24 14:05:04,Merge new release of InnoDB MySQL 5.7.21 to 10.2,"The following changes between MySQL 5.7.20 and MySQL 5.7.21 touch InnoDB files.

[Raise version number after cloning 5.7.20|https://github.com/mysql/mysql-server/commit/bfed7d5c35d9a031df7da38926a4fccccd60ffb9]
This one we should apply; MariaDB 10.2 will report an InnoDB version number.

[Backport some additional GenError dependencies|https://github.com/mysql/mysql-server/commit/a08a6a554e8d7664fec4597992fb4ae77024ad2b]
The MariaDB {{cmake}} files do use {{GenError}} outside InnoDB, so we should consider porting this.

[Bug #25729649 LOCK0LOCK.CC:NNN:ADD_POSITION != __NULL|https://github.com/mysql/mysql-server/commit/1b3da22c277b80bb3dd773f07fdb8fd9bc6044f3]
This appears to fix a bug in high-priority replication slave transactions that affects {{SPATIAL INDEX}}. The high-priority transactions could be dead code in MariaDB, but we should apply the fix nevertheless.

[Bug#26825211 BACKPORT FIX FOR #25643811 TO 5.7|https://github.com/mysql/mysql-server/commit/ba1a99c5cd7dd6204abbe8d177c7f71a48e53fe3]
These are compilation fixes for GCC 7, in InnoDB mostly fixing {{-Wimplicit-fallthrough}}. We can skip this, because MariaDB already got similar fixes.

[Bug #25076416  VIRTUAL COLUMN IS NOT CONSIDERED WHILE FETCHING THE AUTOINC COLUMN|https://github.com/mysql/mysql-server/commit/23228b9b835271e03fd975c44c5783705947151c]
There is no test case, and the code patch does not directly apply. Even though the impact is lower in MariaDB 10.2 thanks to MDEV-6204, we must add a test for this. (MariaDB 10.2 would only perform the equivalent of
{code:sql}
SELECT MAX(auto_inc_column) FROM t;
{code}
as part of
{code:sql}
ALTER TABLE t AUTO_INCREMENT=1, ALGORITHM=INPLACE;
{code}
to ensure that the value cannot be set lower than the actual maximum value in the table (because that is what happens with {{ALGORITHM=COPY}}). We are missing a test for that in the case when the table contains virtual columns.

[Revert ""Bug #25076416 VIRTUAL COLUMN IS NOT CONSIDERED WHILE""|https://github.com/mysql/mysql-server/commit/5c312c43444024d91ce56050f4078b169f1d635a]
This appears to revert the above change, again, with no test case included.
In any case, we must add a test for this scenario.

[BUG#21625016: INNODB FULL TEXT CASE SENSITIVE NOT WORKING.|https://github.com/mysql/mysql-server/commit/4206a24fe8e06ab6f937855cba184b669e02270e]
This one we will get via the merge of MySQL 5.6.39 to MariaDB 10.0.
[The merge from 5.6.39 to 5.7.21|https://github.com/mysql/mysql-server/commit/9a0c517232c5da64f3aa0b52e7131de2ed227ee8] contains some conflict resolution due to different code formatting between 5.6 and 5.7, but apparently no functional change.

[Bug#26918439 ROW0SEL.CC:5178:32 ERROR: ISO C++ FORBIDS COMPARISON BETWEEN POINTER AND INTEGE|https://github.com/mysql/mysql-server/commit/ffdf388df94b240757af3a546efb2a035f2d41d3]
This is one more follow-up to fix the numerous problems introduced by
[Bug #23481444 OPTIMISER CALL ROW_SEARCH_MVCC() AND READ THE INDEX APPLIED BY UNCOMMITTED ROWS|https://github.com/mysql/mysql-server/commit/2f6741ef8e729e8ea6723ca434f95a40036e1d25] in MySQL 5.7.18. As noted in MDEV-11751, MariaDB did not include this change due to the perceived high risk and the questionable benefit.

[two trivial zero-initializations, to avoid ""may be used unitialized"" warnings|https://github.com/mysql/mysql-server/commit/7f9d06b6005b51a9edc8e31587b0f57c677dd162]
The InnoDB change applies to the above-mentioned code that we chose to skip in MDEV-11751. The change to {{Gis_multi_polygon::get_mbr()}} does not apply, because MariaDB is using more straightforward code (initializing the variable from a function return value, not by an output parameter).

[WL#10473: InnoDB: Deprecate innodb_undo_tablespaces in 5.7|https://github.com/mysql/mysql-server/commit/5017e6f5fb92d2c885996727956ffa8fc9ed21c3]
MariaDB will skip this. We will likely deprecate and remove parameters related to rollback segments and undo tablespaces in the course of MDEV-11657.

[Bug #26390658   RENAMING A PARTITIONED TABLE DOES NOT UPDATE MYSQL.INNODB_TABLE_STATS|https://github.com/mysql/mysql-server/commit/eba6a5fac02877df2076faf2b6235975f4c6d4ac]
This looks like something that could be unique to the InnoDB native partitioning. Luckily there is a test case that we can import. In MariaDB, the code in this area was recently changed in MDEV-14511, and due to that, the code changes will not apply directly.

[Backport patch for Bug#16877045 5.6-CLUSTER-7.3 WIN32 SQL_YACC.CC BUILD PROBLEM|https://github.com/mysql/mysql-server/commit/3a2f365bd2d65f8ea865113027125c947a6c9f3e]
This is a 5.6.39 patch that includes the above ‘two trivial zero-initializations’ patch from 5.7. If we need the {{cmake}} changes, we would get those via the merge from 5.6.39 to MariaDB 10.0.

[Bug #26191879 FOREIGN KEY CASCADES USE EXCESSIVE MEMORY|https://github.com/mysql/mysql-server/commit/7b26dc98a624d5cdf79cd5eee455cb03e3bccb5a]
The explicit recursion stack was replaced with procedural recursion, apparently to plug a memory leak. This fix will increase the stack memory usage of InnoDB, and to work around that, it will reduce the maximum depth of {{FOREIGN KEY}} cascade operations to 15 from the 255.
I think that MariaDB should try to fix the actual problem instead of applying this patch.

[Bug#26152751: INNODB LEAKS MEMORY, PERFORMANCE_SCHEMA FILE_INSTANCES #SQL-IB3129987-252773.IBD|https://github.com/mysql/mysql-server/commit/6fa258c56c64f028ade6aaa73b11951e5a68c4a0]
This patch introduces {{PSI_FILE_CALL(end_file_rename_wait)}} in order to plug some kind of a race condition that can lead to a memory leak. The changes are mostly outside InnoDB. A test case is included.

[BUG#26734457 BACKPORT BUG#22305994 TO 5.6 AND 5.7|https://github.com/mysql/mysql-server/commit/cd0b9f7de6561613338774f6253031a3bcd9db7e]
This 5.6.39 patch is replacing {{GetSystemTimeAsFileTime()}} with {{GetSystemTimePreciseAsFileTime()}} when it is available. The call is used in {{ut_gettimeofday()}}, which in turn is used by {{ut_usectime()}}, {{ut_time_us()}}, {{ut_time_ms()}}. The {{ut_usectime()}} in turn is used by {{lock_wait_suspend_thread()}} and {{os_event::wait_time_low()}}. The other calls are used in various other places, affecting both logic and diagnostics.
[~wlad] should comment whether we need this. By default, we would get this from the merge of 5.6.39 to MariaDB 10.0.

[Merge branch 'mysql-5.6' into mysql-5.7|https://github.com/mysql/mysql-server/commit/f028fb0c2efbcdc057d882a9da6b836ed3dec25d]
The merge of the above, with a few conflict resolutions

[Bug #22486025   INNODB: FAILING ASSERTION: KEY_LEN != 0 || FIND_FLAG != HA_READ_KEY_EXACT|https://github.com/mysql/mysql-server/commit/1a13f7f98772da507b54018e05c4b88abbaac32c]
This bug only affects the MySQL 5.7 feature of using InnoDB for the temporary tables that are internally created during SQL execution. The code was never active in MariaDB, and it was removed in MDEV-11487.

[Bug #26256456   INNODB CRASHED WHEN MASTER THREAD EVICT DICT_TABLE_T OBJECT|https://github.com/mysql/mysql-server/commit/ed05c8786e6d4575afe9da3333d0f80be8649411]
This is the 5.6.39 version of a patch that seems to be a bug in the eviction of table definitions from the InnoDB dictionary cache, a feature that was introduced in MySQL 5.6. We will get it via MariaDB 10.0.

[Bug #26492721   MYSQL/INNODB CRASHES DURING ALTER TABLE|https://github.com/mysql/mysql-server/commit/81753e747324436a91d4e8a4758293978ea7ea30]
This MySQL 5.6.39 change fixes a bug in the {{dict_sys->size}} bookkeeping when renaming columns. The whole field is somewhat questionable, because some memory allocations related to the InnoDB data dictionary cache are bypassing this bookkeeping altogether.

[Bug #24296076   INNODB REPORTS WARNING WHILE INNODB_UNDO_LOG_TRUNCATE IS ENABLED|https://github.com/mysql/mysql-server/commit/63a540c8c3e93b7f15aec6260c0a85f1cd07f6dc]
This is disabling a warning in non-debug builds. Apparently Oracle came to the same conclusion as we in MDEV-14916, because they reworded the message. MariaDB removed the message altogether.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/29d37f1b6abf0346c3de6b302650535e4ae9b248]
This fixes a bug in the MySQL 5.7 buffer pool resizing. It is applicable to MariaDB.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/e0ad9ea241f038b197a68520336da3be02b3803b]
A follow-up fix to the above.

[Bug #27212129 INNODB: A RECORD LOCK WAIT HAPPENS IN A DICTIONARY OPERATION|https://github.com/mysql/mysql-server/commit/cf3e608fe5888e78121a4559f5edab4feb367546]
This is a follow-up fix to the bug#26390658 fix above. It does not apply to MariaDB, because MDEV-14511 cleaned up the locking around updates of persistent statistics.",,0,1,0,327,0.242871,"Merge new release of InnoDB MySQL 5.7.21 to 10.2 $end$ The following changes between MySQL 5.7.20 and MySQL 5.7.21 touch InnoDB files.

[Raise version number after cloning 5.7.20|https://github.com/mysql/mysql-server/commit/bfed7d5c35d9a031df7da38926a4fccccd60ffb9]
This one we should apply; MariaDB 10.2 will report an InnoDB version number.

[Backport some additional GenError dependencies|https://github.com/mysql/mysql-server/commit/a08a6a554e8d7664fec4597992fb4ae77024ad2b]
The MariaDB {{cmake}} files do use {{GenError}} outside InnoDB, so we should consider porting this.

[Bug #25729649 LOCK0LOCK.CC:NNN:ADD_POSITION != __NULL|https://github.com/mysql/mysql-server/commit/1b3da22c277b80bb3dd773f07fdb8fd9bc6044f3]
This appears to fix a bug in high-priority replication slave transactions that affects {{SPATIAL INDEX}}. The high-priority transactions could be dead code in MariaDB, but we should apply the fix nevertheless.

[Bug#26825211 BACKPORT FIX FOR #25643811 TO 5.7|https://github.com/mysql/mysql-server/commit/ba1a99c5cd7dd6204abbe8d177c7f71a48e53fe3]
These are compilation fixes for GCC 7, in InnoDB mostly fixing {{-Wimplicit-fallthrough}}. We can skip this, because MariaDB already got similar fixes.

[Bug #25076416  VIRTUAL COLUMN IS NOT CONSIDERED WHILE FETCHING THE AUTOINC COLUMN|https://github.com/mysql/mysql-server/commit/23228b9b835271e03fd975c44c5783705947151c]
There is no test case, and the code patch does not directly apply. Even though the impact is lower in MariaDB 10.2 thanks to MDEV-6204, we must add a test for this. (MariaDB 10.2 would only perform the equivalent of
{code:sql}
SELECT MAX(auto_inc_column) FROM t;
{code}
as part of
{code:sql}
ALTER TABLE t AUTO_INCREMENT=1, ALGORITHM=INPLACE;
{code}
to ensure that the value cannot be set lower than the actual maximum value in the table (because that is what happens with {{ALGORITHM=COPY}}). We are missing a test for that in the case when the table contains virtual columns.

[Revert ""Bug #25076416 VIRTUAL COLUMN IS NOT CONSIDERED WHILE""|https://github.com/mysql/mysql-server/commit/5c312c43444024d91ce56050f4078b169f1d635a]
This appears to revert the above change, again, with no test case included.
In any case, we must add a test for this scenario.

[BUG#21625016: INNODB FULL TEXT CASE SENSITIVE NOT WORKING.|https://github.com/mysql/mysql-server/commit/4206a24fe8e06ab6f937855cba184b669e02270e]
This one we will get via the merge of MySQL 5.6.39 to MariaDB 10.0.
[The merge from 5.6.39 to 5.7.21|https://github.com/mysql/mysql-server/commit/9a0c517232c5da64f3aa0b52e7131de2ed227ee8] contains some conflict resolution due to different code formatting between 5.6 and 5.7, but apparently no functional change.

[Bug#26918439 ROW0SEL.CC:5178:32 ERROR: ISO C++ FORBIDS COMPARISON BETWEEN POINTER AND INTEGE|https://github.com/mysql/mysql-server/commit/ffdf388df94b240757af3a546efb2a035f2d41d3]
This is one more follow-up to fix the numerous problems introduced by
[Bug #23481444 OPTIMISER CALL ROW_SEARCH_MVCC() AND READ THE INDEX APPLIED BY UNCOMMITTED ROWS|https://github.com/mysql/mysql-server/commit/2f6741ef8e729e8ea6723ca434f95a40036e1d25] in MySQL 5.7.18. As noted in MDEV-11751, MariaDB did not include this change due to the perceived high risk and the questionable benefit.

[two trivial zero-initializations, to avoid ""may be used unitialized"" warnings|https://github.com/mysql/mysql-server/commit/7f9d06b6005b51a9edc8e31587b0f57c677dd162]
The InnoDB change applies to the above-mentioned code that we chose to skip in MDEV-11751. The change to {{Gis_multi_polygon::get_mbr()}} does not apply, because MariaDB is using more straightforward code (initializing the variable from a function return value, not by an output parameter).

[WL#10473: InnoDB: Deprecate innodb_undo_tablespaces in 5.7|https://github.com/mysql/mysql-server/commit/5017e6f5fb92d2c885996727956ffa8fc9ed21c3]
MariaDB will skip this. We will likely deprecate and remove parameters related to rollback segments and undo tablespaces in the course of MDEV-11657.

[Bug #26390658   RENAMING A PARTITIONED TABLE DOES NOT UPDATE MYSQL.INNODB_TABLE_STATS|https://github.com/mysql/mysql-server/commit/eba6a5fac02877df2076faf2b6235975f4c6d4ac]
This looks like something that could be unique to the InnoDB native partitioning. Luckily there is a test case that we can import. In MariaDB, the code in this area was recently changed in MDEV-14511, and due to that, the code changes will not apply directly.

[Backport patch for Bug#16877045 5.6-CLUSTER-7.3 WIN32 SQL_YACC.CC BUILD PROBLEM|https://github.com/mysql/mysql-server/commit/3a2f365bd2d65f8ea865113027125c947a6c9f3e]
This is a 5.6.39 patch that includes the above ‘two trivial zero-initializations’ patch from 5.7. If we need the {{cmake}} changes, we would get those via the merge from 5.6.39 to MariaDB 10.0.

[Bug #26191879 FOREIGN KEY CASCADES USE EXCESSIVE MEMORY|https://github.com/mysql/mysql-server/commit/7b26dc98a624d5cdf79cd5eee455cb03e3bccb5a]
The explicit recursion stack was replaced with procedural recursion, apparently to plug a memory leak. This fix will increase the stack memory usage of InnoDB, and to work around that, it will reduce the maximum depth of {{FOREIGN KEY}} cascade operations to 15 from the 255.
I think that MariaDB should try to fix the actual problem instead of applying this patch.

[Bug#26152751: INNODB LEAKS MEMORY, PERFORMANCE_SCHEMA FILE_INSTANCES #SQL-IB3129987-252773.IBD|https://github.com/mysql/mysql-server/commit/6fa258c56c64f028ade6aaa73b11951e5a68c4a0]
This patch introduces {{PSI_FILE_CALL(end_file_rename_wait)}} in order to plug some kind of a race condition that can lead to a memory leak. The changes are mostly outside InnoDB. A test case is included.

[BUG#26734457 BACKPORT BUG#22305994 TO 5.6 AND 5.7|https://github.com/mysql/mysql-server/commit/cd0b9f7de6561613338774f6253031a3bcd9db7e]
This 5.6.39 patch is replacing {{GetSystemTimeAsFileTime()}} with {{GetSystemTimePreciseAsFileTime()}} when it is available. The call is used in {{ut_gettimeofday()}}, which in turn is used by {{ut_usectime()}}, {{ut_time_us()}}, {{ut_time_ms()}}. The {{ut_usectime()}} in turn is used by {{lock_wait_suspend_thread()}} and {{os_event::wait_time_low()}}. The other calls are used in various other places, affecting both logic and diagnostics.
[~wlad] should comment whether we need this. By default, we would get this from the merge of 5.6.39 to MariaDB 10.0.

[Merge branch 'mysql-5.6' into mysql-5.7|https://github.com/mysql/mysql-server/commit/f028fb0c2efbcdc057d882a9da6b836ed3dec25d]
The merge of the above, with a few conflict resolutions

[Bug #22486025   INNODB: FAILING ASSERTION: KEY_LEN != 0 || FIND_FLAG != HA_READ_KEY_EXACT|https://github.com/mysql/mysql-server/commit/1a13f7f98772da507b54018e05c4b88abbaac32c]
This bug only affects the MySQL 5.7 feature of using InnoDB for the temporary tables that are internally created during SQL execution. The code was never active in MariaDB, and it was removed in MDEV-11487.

[Bug #26256456   INNODB CRASHED WHEN MASTER THREAD EVICT DICT_TABLE_T OBJECT|https://github.com/mysql/mysql-server/commit/ed05c8786e6d4575afe9da3333d0f80be8649411]
This is the 5.6.39 version of a patch that seems to be a bug in the eviction of table definitions from the InnoDB dictionary cache, a feature that was introduced in MySQL 5.6. We will get it via MariaDB 10.0.

[Bug #26492721   MYSQL/INNODB CRASHES DURING ALTER TABLE|https://github.com/mysql/mysql-server/commit/81753e747324436a91d4e8a4758293978ea7ea30]
This MySQL 5.6.39 change fixes a bug in the {{dict_sys->size}} bookkeeping when renaming columns. The whole field is somewhat questionable, because some memory allocations related to the InnoDB data dictionary cache are bypassing this bookkeeping altogether.

[Bug #24296076   INNODB REPORTS WARNING WHILE INNODB_UNDO_LOG_TRUNCATE IS ENABLED|https://github.com/mysql/mysql-server/commit/63a540c8c3e93b7f15aec6260c0a85f1cd07f6dc]
This is disabling a warning in non-debug builds. Apparently Oracle came to the same conclusion as we in MDEV-14916, because they reworded the message. MariaDB removed the message altogether.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/29d37f1b6abf0346c3de6b302650535e4ae9b248]
This fixes a bug in the MySQL 5.7 buffer pool resizing. It is applicable to MariaDB.

[Bug #23590280   NO WARNING WHEN REDUCING INNODB_BUFFER_POOL_SIZE INSIZE THE FIRST CHUNK|https://github.com/mysql/mysql-server/commit/e0ad9ea241f038b197a68520336da3be02b3803b]
A follow-up fix to the above.

[Bug #27212129 INNODB: A RECORD LOCK WAIT HAPPENS IN A DICTIONARY OPERATION|https://github.com/mysql/mysql-server/commit/cf3e608fe5888e78121a4559f5edab4feb367546]
This is a follow-up fix to the bug#26390658 fix above. It does not apply to MariaDB, because MDEV-14511 cleaned up the locking around updates of persistent statistics. $acceptance criteria:$",1,1,1,1,1,1,1,197.267,4,3,0.75,2,0.5,2,0.5,1,0.25,1,0.25
708,MDEV-14962,Task,MDEV,2018-01-16 13:21:20,,0,10.0.34 merge,"*     [5.5|https://github.com/mariadb/server/tree/5.5] (/)
*     [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6] (/)
*     [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6] (/) *no changes*
*     [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6] (/)
*     [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6] (/)
*     [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed* (/)
*     [Spider|https://github.com/Kentoku/MariaDB] (/) *no changes*
*     [Mroonga|https://github.com/Kentoku/MariaDB] (/) *no changes*
*     [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre] (/) *no changes*
",,"10.0.34 merge $end$ *     [5.5|https://github.com/mariadb/server/tree/5.5] (/)
*     [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6] (/)
*     [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6] (/) *no changes*
*     [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6] (/)
*     [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6] (/)
*     [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed* (/)
*     [Spider|https://github.com/Kentoku/MariaDB] (/) *no changes*
*     [Mroonga|https://github.com/Kentoku/MariaDB] (/) *no changes*
*     [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre] (/) *no changes*
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,6,,0,0,0,1,0,2,0,,0,850,0,0,0,2018-01-16 13:21:38,10.0.34 merge,"*     [5.5|https://github.com/mariadb/server/tree/5.5]
*     [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6]
*     [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6]
*     [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6]
*     [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6]
*     [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed*
*     [Spider|https://github.com/Kentoku/MariaDB]
*     [Mroonga|https://github.com/Kentoku/MariaDB]
*     [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre]
",,0,2,0,17,0.708333,"10.0.34 merge $end$ *     [5.5|https://github.com/mariadb/server/tree/5.5]
*     [InnoDB|https://github.com/MariaDB/mergetrees/tree/merge-innodb-5.6]
*     [P_S|https://github.com/MariaDB/mergetrees/tree/merge-perfschema-5.6]
*     [XtraDB|https://github.com/MariaDB/mergetrees/tree/merge-xtradb-5.6]
*     [TokuDB|https://github.com/MariaDB/mergetrees/tree/merge-tokudb-5.6]
*     [Connect|https://github.com/Buggynours/MariaDB/tree/10.0] *squashed*
*     [Spider|https://github.com/Kentoku/MariaDB]
*     [Mroonga|https://github.com/Kentoku/MariaDB]
*     [PCRE|https://github.com/MariaDB/mergetrees/tree/merge-pcre]
 $acceptance criteria:$",2,1,1,1,1,0,1,0.0,58,29,0.5,22,0.37931,17,0.293103,15,0.258621,11,0.189655
709,MDEV-15053,Task,MDEV,2018-01-24 10:49:42,,0,Reduce buf_pool_t::mutex contention,"[MySQL 8.0.0 split the InnoDB {{buf_pool_t::mutex}}|https://github.com/mysql/mysql-server/commit/2bcc00d11f21fe43ba3c0e0f81d3d9cec44c44a0]. MariaDB should do something similar.

Instead of introducing more mutexes or radically changing the latching rules of various {{buf_pool_t}} and {{buf_block_t}} data members, I think that it is possible to reduce the contention on {{buf_pool.mutex}} by other means:
* Move more code to inline functions of {{buf_pool_t}} or {{buf_page_t}}.
* Reduce the amount of mutex release/reacquire dance in {{buf0flu.cc}} and {{buf0rea.cc}}.
* Avoid repeated calls to {{page_id_t::fold()}} or {{page_id_t::page_id_t()}}; use {{page_id_t}} directly as loop iterator.
* Move {{buf_page_t::flush_type}} to {{IORequest}}.
* Split {{buf_page_io_complete()}} into separate ‘read completion’ and ‘write completion’ callbacks.
* Avoid holding {{buf_pool.mutex}} during {{buf_pool.page_hash}} operations. Consider removing the debug field {{buf_page_t::in_page_hash}}.
* Split operations on {{buf_pool.watch[]}} into two parts. The allocation of {{buf_pool.watch[]}} should be only protected by {{buf_pool.mutex}}, and the {{buf_pool.page_hash}} only by the hash bucket latch.",,"Reduce buf_pool_t::mutex contention $end$ [MySQL 8.0.0 split the InnoDB {{buf_pool_t::mutex}}|https://github.com/mysql/mysql-server/commit/2bcc00d11f21fe43ba3c0e0f81d3d9cec44c44a0]. MariaDB should do something similar.

Instead of introducing more mutexes or radically changing the latching rules of various {{buf_pool_t}} and {{buf_block_t}} data members, I think that it is possible to reduce the contention on {{buf_pool.mutex}} by other means:
* Move more code to inline functions of {{buf_pool_t}} or {{buf_page_t}}.
* Reduce the amount of mutex release/reacquire dance in {{buf0flu.cc}} and {{buf0rea.cc}}.
* Avoid repeated calls to {{page_id_t::fold()}} or {{page_id_t::page_id_t()}}; use {{page_id_t}} directly as loop iterator.
* Move {{buf_page_t::flush_type}} to {{IORequest}}.
* Split {{buf_page_io_complete()}} into separate ‘read completion’ and ‘write completion’ callbacks.
* Avoid holding {{buf_pool.mutex}} during {{buf_pool.page_hash}} operations. Consider removing the debug field {{buf_page_t::in_page_hash}}.
* Split operations on {{buf_pool.watch[]}} into two parts. The allocation of {{buf_pool.watch[]}} should be only protected by {{buf_pool.mutex}}, and the {{buf_pool.page_hash}} only by the hash bucket latch. $acceptance criteria:$",,Marko Mäkelä,Marko Mäkelä,Critical,27,,21,13,23,2,0,2,0,,0,850,8,0,0,2018-03-20 15:49:56,Split buf_pool_t::mutex,[MySQL 8.0.0 split the InnoDB {{buf_pool_t::mutex}}|https://github.com/mysql/mysql-server/commit/2bcc00d11f21fe43ba3c0e0f81d3d9cec44c44a0]. MariaDB should do the same.,,1,1,0,131,8.0625,Split buf_pool_t::mutex $end$ [MySQL 8.0.0 split the InnoDB {{buf_pool_t::mutex}}|https://github.com/mysql/mysql-server/commit/2bcc00d11f21fe43ba3c0e0f81d3d9cec44c44a0]. MariaDB should do the same. $acceptance criteria:$,2,1,1,1,1,1,1,1325.0,5,4,0.8,3,0.6,3,0.6,2,0.4,2,0.4
710,MDEV-15055,Task,MDEV,2018-01-24 11:43:19,,0,10.1.31 merge,"* 10.0 (/)
* 10.0-galera (/)",,"10.1.31 merge $end$ * 10.0 (/)
* 10.0-galera (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,7,,0,0,0,1,0,2,0,,0,850,0,0,0,2018-01-24 11:43:19,10.1.31 merge,"* 10.0
* 10.0-galera",,0,2,0,2,0.222222,"10.1.31 merge $end$ * 10.0
* 10.0-galera $acceptance criteria:$",2,1,0,0,0,0,0,0.0,59,30,0.508475,23,0.38983,18,0.305085,16,0.271186,11,0.186441
711,MDEV-15104,Task,MDEV,2018-01-28 20:06:31,MDEV-14442,0,Remove trx_sys_t::rw_trx_ids and trx_sys_t::serialisation_list,"Take snapshot of registered read-write transaction identifiers directly
from rw_trx_hash. It immediately saves one trx_sys.mutex lock, reduces
size of another critical section protected by this mutex, and makes
further optimisations like removing trx_sys_t::serialisation_list
possible.

Downside of this approach is bigger overhead for view opening, because
iterating LF_HASH is more expensive compared to taking snapshot of an
array. However for low concurrency overhead difference is negligible,
while for high concurrency mutex is much bigger evil.

Currently we still take trx_sys.mutex to serialise ReadView creation.
This is required to keep serialisation_list ordered by trx->no as well
as not to let purge thread to create more recent snapshot while another
thread gets suspended during creation of older snapshot. This will
become completely mutex free along with serialisation_list removal.

Compared to previous implementation removing element from rw_trx_hash
and serialisation_list is not atomic. We disregard all possible bad
consequences (if there're any) since it will be solved along with
serialisation_list removal.

serialisation_list was supposed to instantly give minimum registered
transaction serialisation number. However maintaining and accessing
this list requires global mutex protection.

Since we already take MVCC snapshot by iterating trx_sys_t::rw_trx_hash,
it is cheap to integrate minimum registered transaction lookup into this
iteration.",,"Remove trx_sys_t::rw_trx_ids and trx_sys_t::serialisation_list $end$ Take snapshot of registered read-write transaction identifiers directly
from rw_trx_hash. It immediately saves one trx_sys.mutex lock, reduces
size of another critical section protected by this mutex, and makes
further optimisations like removing trx_sys_t::serialisation_list
possible.

Downside of this approach is bigger overhead for view opening, because
iterating LF_HASH is more expensive compared to taking snapshot of an
array. However for low concurrency overhead difference is negligible,
while for high concurrency mutex is much bigger evil.

Currently we still take trx_sys.mutex to serialise ReadView creation.
This is required to keep serialisation_list ordered by trx->no as well
as not to let purge thread to create more recent snapshot while another
thread gets suspended during creation of older snapshot. This will
become completely mutex free along with serialisation_list removal.

Compared to previous implementation removing element from rw_trx_hash
and serialisation_list is not atomic. We disregard all possible bad
consequences (if there're any) since it will be solved along with
serialisation_list removal.

serialisation_list was supposed to instantly give minimum registered
transaction serialisation number. However maintaining and accessing
this list requires global mutex protection.

Since we already take MVCC snapshot by iterating trx_sys_t::rw_trx_hash,
it is cheap to integrate minimum registered transaction lookup into this
iteration. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,12,,3,8,5,1,0,0,0,,0,850,4,0,0,2018-01-31 18:44:09,Remove trx_sys_t::rw_trx_ids and trx_sys_t::serialisation_list,"Take snapshot of registered read-write transaction identifiers directly
from rw_trx_hash. It immediately saves one trx_sys.mutex lock, reduces
size of another critical section protected by this mutex, and makes
further optimisations like removing trx_sys_t::serialisation_list
possible.

Downside of this approach is bigger overhead for view opening, because
iterating LF_HASH is more expensive compared to taking snapshot of an
array. However for low concurrency overhead difference is negligible,
while for high concurrency mutex is much bigger evil.

Currently we still take trx_sys.mutex to serialise ReadView creation.
This is required to keep serialisation_list ordered by trx->no as well
as not to let purge thread to create more recent snapshot while another
thread gets suspended during creation of older snapshot. This will
become completely mutex free along with serialisation_list removal.

Compared to previous implementation removing element from rw_trx_hash
and serialisation_list is not atomic. We disregard all possible bad
consequences (if there're any) since it will be solved along with
serialisation_list removal.

serialisation_list was supposed to instantly give minimum registered
transaction serialisation number. However maintaining and accessing
this list requires global mutex protection.

Since we already take MVCC snapshot by iterating trx_sys_t::rw_trx_hash,
it is cheap to integrate minimum registered transaction lookup into this
iteration.",,0,0,0,0,0.0,"Remove trx_sys_t::rw_trx_ids and trx_sys_t::serialisation_list $end$ Take snapshot of registered read-write transaction identifiers directly
from rw_trx_hash. It immediately saves one trx_sys.mutex lock, reduces
size of another critical section protected by this mutex, and makes
further optimisations like removing trx_sys_t::serialisation_list
possible.

Downside of this approach is bigger overhead for view opening, because
iterating LF_HASH is more expensive compared to taking snapshot of an
array. However for low concurrency overhead difference is negligible,
while for high concurrency mutex is much bigger evil.

Currently we still take trx_sys.mutex to serialise ReadView creation.
This is required to keep serialisation_list ordered by trx->no as well
as not to let purge thread to create more recent snapshot while another
thread gets suspended during creation of older snapshot. This will
become completely mutex free along with serialisation_list removal.

Compared to previous implementation removing element from rw_trx_hash
and serialisation_list is not atomic. We disregard all possible bad
consequences (if there're any) since it will be solved along with
serialisation_list removal.

serialisation_list was supposed to instantly give minimum registered
transaction serialisation number. However maintaining and accessing
this list requires global mutex protection.

Since we already take MVCC snapshot by iterating trx_sys_t::rw_trx_hash,
it is cheap to integrate minimum registered transaction lookup into this
iteration. $acceptance criteria:$",0,0,0,0,0,0,0,70.6167,27,3,0.111111,2,0.0740741,2,0.0740741,1,0.037037,1,0.037037
712,MDEV-15107,Technical task,MDEV,2018-01-29 06:27:50,,0,"Add virtual Field::sp_prepare_and_store_item(), make sp_rcontext symmetric for scalar and ROW","After MDEV-14212, the Virtual_tmp_table instance that stores a ROW variable elements is accessible from the underlying Field_row (rather than Item_field_row).

Under terms of this task we'll do some further changes by moving the code from sp_instr_xxx, sp_rcontext, Item_xxx to Virtual_tmp_table and Field_xxx:

- Move the data type specific code (scalar vs ROW) which stores a value to an SP variable into a new virtual method Field_xxx::sp_prepare_and_store_item().
- Make the the code in sp_rcontext::set_variable() and sp_eval_expr() symmetric for scalar and ROW values.
- Make the code in in sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row() symmetric for ROW elements (i.e. scalar and ROW elements inside a ROW).

Rationale:

Prepare the code to implement these tasks soon easier:

- MDEV-12252 ROW data type for stored function return values 
- MDEV-12307 ROW data type for built-in function return values 
- MDEV-6121 Data type: Array
- MDEV-10593 sql_mode=ORACLE: TYPE .. AS OBJECT: basic functionality
- ROW with ROW fields (no MDEV yet)
",,"Add virtual Field::sp_prepare_and_store_item(), make sp_rcontext symmetric for scalar and ROW $end$ After MDEV-14212, the Virtual_tmp_table instance that stores a ROW variable elements is accessible from the underlying Field_row (rather than Item_field_row).

Under terms of this task we'll do some further changes by moving the code from sp_instr_xxx, sp_rcontext, Item_xxx to Virtual_tmp_table and Field_xxx:

- Move the data type specific code (scalar vs ROW) which stores a value to an SP variable into a new virtual method Field_xxx::sp_prepare_and_store_item().
- Make the the code in sp_rcontext::set_variable() and sp_eval_expr() symmetric for scalar and ROW values.
- Make the code in in sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row() symmetric for ROW elements (i.e. scalar and ROW elements inside a ROW).

Rationale:

Prepare the code to implement these tasks soon easier:

- MDEV-12252 ROW data type for stored function return values 
- MDEV-12307 ROW data type for built-in function return values 
- MDEV-6121 Data type: Array
- MDEV-10593 sql_mode=ORACLE: TYPE .. AS OBJECT: basic functionality
- ROW with ROW fields (no MDEV yet)
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,6,,1,0,1,5,0,0,0,,0,850,0,0,0,2018-01-29 06:27:50,"Add virtual Field::sp_prepare_and_store_item(), make sp_rcontext symmetric for scalar and ROW","After MDEV-14212, the Virtual_tmp_table instance that stores a ROW variable elements is accessible from the underlying Field_row (rather than Item_field_row).

Under terms of this task we'll do some further changes by moving the code from sp_instr_xxx, sp_rcontext, Item_xxx to Virtual_tmp_table and Field_xxx:

- Move the data type specific code (scalar vs ROW) which stores a value to an SP variable into a new virtual method Field_xxx::sp_prepare_and_store_item().
- Make the the code in sp_rcontext::set_variable() and sp_eval_expr() symmetric for scalar and ROW values.
- Make the code in in sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row() symmetric for ROW elements (i.e. scalar and ROW elements inside a ROW).

Rationale:

Prepare the code to implement these tasks soon easier:

- MDEV-12252 ROW data type for stored function return values 
- MDEV-12307 ROW data type for built-in function return values 
- MDEV-6121 Data type: Array
- MDEV-10593 sql_mode=ORACLE: TYPE .. AS OBJECT: basic functionality
- ROW with ROW fields (no MDEV yet)
",,0,0,0,0,0.0,"Add virtual Field::sp_prepare_and_store_item(), make sp_rcontext symmetric for scalar and ROW $end$ After MDEV-14212, the Virtual_tmp_table instance that stores a ROW variable elements is accessible from the underlying Field_row (rather than Item_field_row).

Under terms of this task we'll do some further changes by moving the code from sp_instr_xxx, sp_rcontext, Item_xxx to Virtual_tmp_table and Field_xxx:

- Move the data type specific code (scalar vs ROW) which stores a value to an SP variable into a new virtual method Field_xxx::sp_prepare_and_store_item().
- Make the the code in sp_rcontext::set_variable() and sp_eval_expr() symmetric for scalar and ROW values.
- Make the code in in sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row_field(), sp_rcontext::set_variable_row() symmetric for ROW elements (i.e. scalar and ROW elements inside a ROW).

Rationale:

Prepare the code to implement these tasks soon easier:

- MDEV-12252 ROW data type for stored function return values 
- MDEV-12307 ROW data type for built-in function return values 
- MDEV-6121 Data type: Array
- MDEV-10593 sql_mode=ORACLE: TYPE .. AS OBJECT: basic functionality
- ROW with ROW fields (no MDEV yet)
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,85,43,0.505882,30,0.352941,28,0.329412,26,0.305882,26,0.305882
713,MDEV-15153,Task,MDEV,2018-01-31 17:38:04,,0,10.2.13 merge,"* 10.1 (/)
* Connect (/)
* C/C (/)
* TokuDB (x) Progress, but not ready",,"10.2.13 merge $end$ * 10.1 (/)
* Connect (/)
* C/C (/)
* TokuDB (x) Progress, but not ready $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,7,,0,0,1,1,0,3,0,,0,850,0,0,0,2018-01-31 17:38:04,10.2.13 merge,"* 10.1
* Connect
* C/C
* TokuDB ?",,0,3,0,9,0.571429,"10.2.13 merge $end$ * 10.1
* Connect
* C/C
* TokuDB ? $acceptance criteria:$",3,1,1,0,0,0,1,0.0,60,31,0.516667,23,0.383333,18,0.3,16,0.266667,11,0.183333
714,MDEV-15241,Task,MDEV,2018-02-07 18:07:49,,0,make SIGNAL maximum MESSAGE_TEXT length a larger value,"Currently when you raise a SIGNAL and set MESSAGE_TEXT you are limited to 128 characters. When you are dynamically constructing messages sometimes the 128 char limit is a a problem.

Would it be easy to make this longer or configurable to a longer length?
",,"make SIGNAL maximum MESSAGE_TEXT length a larger value $end$ Currently when you raise a SIGNAL and set MESSAGE_TEXT you are limited to 128 characters. When you are dynamically constructing messages sometimes the 128 char limit is a a problem.

Would it be easy to make this longer or configurable to a longer length?
 $acceptance criteria:$",,Robert Dyas,Robert Dyas,Major,10,,0,2,0,1,0,0,0,,0,850,2,0,0,2018-03-20 14:29:04,make SIGNAL maximum MESSAGE_TEXT length a larger value,"Currently when you raise a SIGNAL and set MESSAGE_TEXT you are limited to 128 characters. When you are dynamically constructing messages sometimes the 128 char limit is a a problem.

Would it be easy to make this longer or configurable to a longer length?
",,0,0,0,0,0.0,"make SIGNAL maximum MESSAGE_TEXT length a larger value $end$ Currently when you raise a SIGNAL and set MESSAGE_TEXT you are limited to 128 characters. When you are dynamically constructing messages sometimes the 128 char limit is a a problem.

Would it be easy to make this longer or configurable to a longer length?
 $acceptance criteria:$",0,0,0,0,0,0,0,980.35,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
715,MDEV-15253,Task,MDEV,2018-02-08 13:25:16,,0,Default optimizer setting changes for MariaDB 10.4,"The current default settings are:

{noformat}
optimizer_use_condition_selectivity=1
use_stat_tables=NEVER
{noformat}

This means many optimizations are not enabled.

During discussion on the optimizer call, figured that we should use defaults like so:
{noformat}
optimizer_use_condition_selectivity=4
use_stat_tables=PREFERABLY
{noformat}

The task is:
- change the defaults
- This will cause some MTR test result differences 
- Go through the failing test and see:
-- some tests specifically require old settings. Set the variables for these tests.
-- some tests dont' update test results for these.


cc [~igor], [~cvicentiu], [~sanja], [~varun_raiko], [~shagalla]",,"Default optimizer setting changes for MariaDB 10.4 $end$ The current default settings are:

{noformat}
optimizer_use_condition_selectivity=1
use_stat_tables=NEVER
{noformat}

This means many optimizations are not enabled.

During discussion on the optimizer call, figured that we should use defaults like so:
{noformat}
optimizer_use_condition_selectivity=4
use_stat_tables=PREFERABLY
{noformat}

The task is:
- change the defaults
- This will cause some MTR test result differences 
- Go through the failing test and see:
-- some tests specifically require old settings. Set the variables for these tests.
-- some tests dont' update test results for these.


cc [~igor], [~cvicentiu], [~sanja], [~varun_raiko], [~shagalla] $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Critical,25,,20,10,25,1,0,3,0,,0,850,7,0,0,2018-02-08 14:17:16,Default optimizer setting changes for MariaDB 10.3,"The current default settings are:

{noformat}
optimizer_use_condition_selectivity=1
join_cache_level=2
use_stat_tables=NEVER
{noformat}

This means many optimizations are not enabled.

During discussion on the optimizer call, figured that we should use defaults like so:
{noformat}
join_cache_level=4
optimizer_use_condition_selectivity=4
use_stat_tables=PREFERABLY
{noformat}

The task is:
- change the defaults
- This will cause some MTR test result differences 
- Go through the failing test and see:
-- some tests specifically require old settings. Set the variables for these tests.
-- some tests dont' update test results for these.


cc [~igor] [~cvicentiu] [[~sanja] [~varun_raiko][~shagalla]",,1,2,0,13,0.0824742,"Default optimizer setting changes for MariaDB 10.3 $end$ The current default settings are:

{noformat}
optimizer_use_condition_selectivity=1
join_cache_level=2
use_stat_tables=NEVER
{noformat}

This means many optimizations are not enabled.

During discussion on the optimizer call, figured that we should use defaults like so:
{noformat}
join_cache_level=4
optimizer_use_condition_selectivity=4
use_stat_tables=PREFERABLY
{noformat}

The task is:
- change the defaults
- This will cause some MTR test result differences 
- Go through the failing test and see:
-- some tests specifically require old settings. Set the variables for these tests.
-- some tests dont' update test results for these.


cc [~igor] [~cvicentiu] [[~sanja] [~varun_raiko][~shagalla] $acceptance criteria:$",3,1,1,1,0,0,1,0.866667,13,1,0.0769231,1,0.0769231,1,0.0769231,1,0.0769231,1,0.0769231
716,MDEV-15409,Task,MDEV,2018-02-24 18:41:30,,0,make sure every sst script is tested in buildbot,make sure every sst script is tested in buildbot,,make sure every sst script is tested in buildbot $end$ make sure every sst script is tested in buildbot $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Blocker,8,,4,8,4,1,0,0,0,,0,850,8,0,0,2018-03-07 19:13:33,make sure every sst script is tested in buildbot,make sure every sst script is tested in buildbot,,0,0,0,0,0.0,make sure every sst script is tested in buildbot $end$ make sure every sst script is tested in buildbot $acceptance criteria:$,0,0,0,0,0,0,0,264.533,61,32,0.52459,24,0.393443,18,0.295082,16,0.262295,11,0.180328
717,MDEV-15473,Task,MDEV,2018-03-05 19:19:47,,0,"Isolate/sandbox PAM modules, so that they can't crash the server","Buggy PAM modules can currently crash the server. See MDEV-10361 for example. Should auth_pam isolate PAM modules somehow to prevent problems like this from taking down the whole server? Is it feasible for auth_pam to use sandboxes for PAM modules, or would that cripple performance and slow down authentication too much?",,"Isolate/sandbox PAM modules, so that they can't crash the server $end$ Buggy PAM modules can currently crash the server. See MDEV-10361 for example. Should auth_pam isolate PAM modules somehow to prevent problems like this from taking down the whole server? Is it feasible for auth_pam to use sandboxes for PAM modules, or would that cripple performance and slow down authentication too much? $acceptance criteria:$",,Geoff Montee,Geoff Montee,Critical,39,,11,11,15,1,0,0,0,,0,850,8,0,0,2018-05-29 07:52:12,"Isolate/sandbox PAM modules, so that they can't crash the server","Buggy PAM modules can currently crash the server. See MDEV-10361 for example. Should auth_pam isolate PAM modules somehow to prevent problems like this from taking down the whole server? Is it feasible for auth_pam to use sandboxes for PAM modules, or would that cripple performance and slow down authentication too much?",,0,0,0,0,0.0,"Isolate/sandbox PAM modules, so that they can't crash the server $end$ Buggy PAM modules can currently crash the server. See MDEV-10361 for example. Should auth_pam isolate PAM modules somehow to prevent problems like this from taking down the whole server? Is it feasible for auth_pam to use sandboxes for PAM modules, or would that cripple performance and slow down authentication too much? $acceptance criteria:$",0,0,0,0,0,0,0,2028.53,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
718,MDEV-15501,Task,MDEV,2018-03-07 12:20:56,,0,Dynamic config `proxy_protocol_networks`,"Current Implication of proxy protocol is not very convenient，proxy_protocol_networks is read only, so when we need deploy a proxy which not belong to proxy_protocol_networks, then we need change the server's config and restart.

And for the aspect of safety, we cannot config a big subnet, maybe just add proxy's ip to proxy_protocol_networks",,"Dynamic config `proxy_protocol_networks` $end$ Current Implication of proxy protocol is not very convenient，proxy_protocol_networks is read only, so when we need deploy a proxy which not belong to proxy_protocol_networks, then we need change the server's config and restart.

And for the aspect of safety, we cannot config a big subnet, maybe just add proxy's ip to proxy_protocol_networks $acceptance criteria:$",,dapeng huang,dapeng huang,Major,6,,0,3,1,1,0,0,0,,0,850,0,0,0,2018-03-20 14:41:15,Dynamic config `proxy_protocol_networks`,"Current Implication of proxy protocol is not very convenient，proxy_protocol_networks is read only, so when we need deploy a proxy which not belong to proxy_protocol_networks, then we need change the server's config and restart.

And for the aspect of safety, we cannot config a big subnet, maybe just add proxy's ip to proxy_protocol_networks",,0,0,0,0,0.0,"Dynamic config `proxy_protocol_networks` $end$ Current Implication of proxy protocol is not very convenient，proxy_protocol_networks is read only, so when we need deploy a proxy which not belong to proxy_protocol_networks, then we need change the server's config and restart.

And for the aspect of safety, we cannot config a big subnet, maybe just add proxy's ip to proxy_protocol_networks $acceptance criteria:$",0,0,0,0,0,0,0,314.333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
719,MDEV-15560,Task,MDEV,2018-03-13 23:42:56,,0,10.2.14 merge,"* 5.5 (/)
* 10.0 (/)
* 10.1 (/)
* Connect (/)
* C/C (/)
* TokuDB (x) not this time",,"10.2.14 merge $end$ * 5.5 (/)
* 10.0 (/)
* 10.1 (/)
* Connect (/)
* C/C (/)
* TokuDB (x) not this time $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,10,,0,0,0,1,0,5,0,,0,850,0,0,0,2018-03-13 23:42:56,10.2.15 merge,"* 5.5
* 10.0
* 10.1
* Connect
* C/C
* TokuDB (x) not this time",,1,4,0,7,0.285714,"10.2.15 merge $end$ * 5.5
* 10.0
* 10.1
* Connect
* C/C
* TokuDB (x) not this time $acceptance criteria:$",5,1,1,0,0,0,1,0.0,62,32,0.516129,24,0.387097,18,0.290323,16,0.258065,11,0.177419
720,MDEV-15694,Task,MDEV,2018-03-27 18:09:32,,0,Windows : use GetSystemTimePreciseAsFileTime if available for high resolution time,"Currently, we're using GetSystemTimesFileTime for high resolution timers.
Unfortunately, it is using system timer with accuracy 10ms to 55ms according to https://blogs.msdn.microsoft.com/oldnewthing/20170921-00.

For system versioning, we need a more accurate timer, this is what GetSystemTimePreciseAsFileTime  provides, but this is not available
prior to Windows 8.1.  We can use dynamic loading and fallback to non-precise variation on
downlevel Windows.

Note: there is a higher cost associated with precise function. my_hrtime() is used in many places, that may be performance relevant, most notably in pthread_condition_timedwait() Windows port, where it is used twice per pthread_condition_timedwait call. Thus  pthread_cond_timedwait, and surrounded code should be reimplemented to use the cheap timer .",,"Windows : use GetSystemTimePreciseAsFileTime if available for high resolution time $end$ Currently, we're using GetSystemTimesFileTime for high resolution timers.
Unfortunately, it is using system timer with accuracy 10ms to 55ms according to https://blogs.msdn.microsoft.com/oldnewthing/20170921-00.

For system versioning, we need a more accurate timer, this is what GetSystemTimePreciseAsFileTime  provides, but this is not available
prior to Windows 8.1.  We can use dynamic loading and fallback to non-precise variation on
downlevel Windows.

Note: there is a higher cost associated with precise function. my_hrtime() is used in many places, that may be performance relevant, most notably in pthread_condition_timedwait() Windows port, where it is used twice per pthread_condition_timedwait call. Thus  pthread_cond_timedwait, and surrounded code should be reimplemented to use the cheap timer . $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,7,,0,0,0,1,0,1,0,,0,850,0,0,0,2018-03-27 18:09:32,Windows : use GetSystemTimePreciseAsFileTime if available for high resolution time,"Currently, we're using GetSystemTimesFileTime for high resolution timers.
Unfortunately, it is using system timer with accuracy 10ms to 55ms according to https://blogs.msdn.microsoft.com/oldnewthing/20170921-00.

For system versioning, we need a more accurate timer, this is what GetSystemTimePreciseAsFileTime  provides, but this is not available
prior to Windows 8.1.  We can use dynamic loading and fallback to non-precise variation on
downlevel Windows.

Note: there is a higher cost associated with precise variation. my_hrtime() is used in many places, that may be performance relevant, most notably in pthread_condition_timedwait() Windows port, where it is used twice per pthread_condition_timedwait call. Thus  pthread_cond_timedwait, and surrounded code should be reimplemented to use the cheap timer .",,0,1,0,2,0.00833333,"Windows : use GetSystemTimePreciseAsFileTime if available for high resolution time $end$ Currently, we're using GetSystemTimesFileTime for high resolution timers.
Unfortunately, it is using system timer with accuracy 10ms to 55ms according to https://blogs.msdn.microsoft.com/oldnewthing/20170921-00.

For system versioning, we need a more accurate timer, this is what GetSystemTimePreciseAsFileTime  provides, but this is not available
prior to Windows 8.1.  We can use dynamic loading and fallback to non-precise variation on
downlevel Windows.

Note: there is a higher cost associated with precise variation. my_hrtime() is used in many places, that may be performance relevant, most notably in pthread_condition_timedwait() Windows port, where it is used twice per pthread_condition_timedwait call. Thus  pthread_cond_timedwait, and surrounded code should be reimplemented to use the cheap timer . $acceptance criteria:$",1,1,0,0,0,0,0,0.0,7,1,0.142857,1,0.142857,1,0.142857,1,0.142857,1,0.142857
721,MDEV-20720,Task,MDEV,2016-08-31 15:49:52,,0,Galera: Replicate MariaDB GTID to other nodes in the cluster,"The MariaDB GTID is currently not transferred to other nodes in the cluster. As a result,
receiving nodes simply use the current gtid_domain_id (or wsrep_gitd_domain_id in 10.1)
and server id to tag the incoming transactions along with galera-assigned sequence number.",,"Galera: Replicate MariaDB GTID to other nodes in the cluster $end$ The MariaDB GTID is currently not transferred to other nodes in the cluster. As a result,
receiving nodes simply use the current gtid_domain_id (or wsrep_gitd_domain_id in 10.1)
and server id to tag the incoming transactions along with galera-assigned sequence number. $acceptance criteria:$",,Nirbhay Choubey,Nirbhay Choubey,Critical,80,,10,34,14,7,0,0,0,,0,850,31,0,0,2016-10-26 19:52:37,Galera: Replicate MariaDB GTID to other nodes in the cluster,"The MariaDB GTID is currently not transferred to other nodes in the cluster. As a result,
receiving nodes simply use the current gtid_domain_id (or wsrep_gitd_domain_id in 10.1)
and server id to tag the incoming transactions along with galera-assigned sequence number.",,0,0,0,0,0.0,"Galera: Replicate MariaDB GTID to other nodes in the cluster $end$ The MariaDB GTID is currently not transferred to other nodes in the cluster. As a result,
receiving nodes simply use the current gtid_domain_id (or wsrep_gitd_domain_id in 10.1)
and server id to tag the incoming transactions along with galera-assigned sequence number. $acceptance criteria:$",0,0,0,0,0,0,1,1348.03,3,1,0.333333,0,0.0,0,0.0,0,0.0,0,0.0
722,MDEV-3929,Task,MDEV,2012-12-10 22:05:00,,0,Add system variable explicit_defaults_for_timestamp for compatibility with MySQL,"As the description of the main task MDEV-452 already mentions, MySQL implementation has the variable {{explicit_defaults_for_timestamp}} which modifies the behavior of auto-updated temporal columns. Since the planned release approaches and we don't have the variable yet, we either need to add it now, or decide to release without it, and add it in further versions. Either is okay as long as it's not completely forgotten. 

The definition of MySQL's implementation can be found here:
http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp

What we should do in MariaDB:
- Add this new variable and have it work as MySQL when enabled.
- Have it disabled as default, to not cause incompatibilities with old applications
- Always keep the variable around (not make it deprecated) as we don't want to have it default.
- Add a MYSQL mode where, if enabled, this variable is set.

TODO: check how replication of this variable works in MySQL.
",,"Add system variable explicit_defaults_for_timestamp for compatibility with MySQL $end$ As the description of the main task MDEV-452 already mentions, MySQL implementation has the variable {{explicit_defaults_for_timestamp}} which modifies the behavior of auto-updated temporal columns. Since the planned release approaches and we don't have the variable yet, we either need to add it now, or decide to release without it, and add it in further versions. Either is okay as long as it's not completely forgotten. 

The definition of MySQL's implementation can be found here:
http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp

What we should do in MariaDB:
- Add this new variable and have it work as MySQL when enabled.
- Have it disabled as default, to not cause incompatibilities with old applications
- Always keep the variable around (not make it deprecated) as we don't want to have it default.
- Add a MYSQL mode where, if enabled, this variable is set.

TODO: check how replication of this variable works in MySQL.
 $acceptance criteria:$",,Elena Stepanova,Elena Stepanova,Critical,33,,5,9,8,3,0,4,0,,0,850,2,4,0,2015-07-08 12:57:39,Add system variable explicit_defaults_for_timestamp for compatibility with MySQL,"As the description of the main task MDEV-452 already mentions, MySQL implementation has the variable {{explicit_defaults_for_timestamp}} which modifies the behavior of auto-updated temporal columns. Since the planned release approaches and we don't have the variable yet, we either need to add it now, or decide to release without it, and add it in further versions. Either is okay as long as it's not completely forgotten. 

The definition of MySQL's implementation can be found here:
http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp

What we should do in MariaDB:
- Add this new variable and have it work as MySQL when enabled.
- Have it disabled as default, to not cause incompatibilities with old applications
- Always keep the variable around (not make it deprecated) as we don't want to have it default.
- Add a MYSQL mode where, if enabled, this variable is set.

TODO: check how replication of this variable works in MySQL.
",,0,0,0,0,0.0,"Add system variable explicit_defaults_for_timestamp for compatibility with MySQL $end$ As the description of the main task MDEV-452 already mentions, MySQL implementation has the variable {{explicit_defaults_for_timestamp}} which modifies the behavior of auto-updated temporal columns. Since the planned release approaches and we don't have the variable yet, we either need to add it now, or decide to release without it, and add it in further versions. Either is okay as long as it's not completely forgotten. 

The definition of MySQL's implementation can be found here:
http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp

What we should do in MariaDB:
- Add this new variable and have it work as MySQL when enabled.
- Have it disabled as default, to not cause incompatibilities with old applications
- Always keep the variable around (not make it deprecated) as we don't want to have it default.
- Add a MYSQL mode where, if enabled, this variable is set.

TODO: check how replication of this variable works in MySQL.
 $acceptance criteria:$",0,0,0,0,0,0,1,22550.9,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
723,MDEV-3944,Task,MDEV,2012-12-17 03:49:43,,0,Allow derived tables in VIEWS,"mysql> CREATE VIEW v3 AS SELECT * FROM (SELECT s1 FROM t1) AS x;
ERROR 1349 (HY000): View's SELECT contains a subquery in the FROM clause

See http://bugs.mysql.com/bug.php?id=16757
and http://bugs.mysql.com/bug.php?id=12755

specifically Eric Bergen's comments about the complexity of removing this restriction. It should be doable without too much hassle, and as we can see from the comments, it would resolve a lot of issues for people.",,"Allow derived tables in VIEWS $end$ mysql> CREATE VIEW v3 AS SELECT * FROM (SELECT s1 FROM t1) AS x;
ERROR 1349 (HY000): View's SELECT contains a subquery in the FROM clause

See http://bugs.mysql.com/bug.php?id=16757
and http://bugs.mysql.com/bug.php?id=12755

specifically Eric Bergen's comments about the complexity of removing this restriction. It should be doable without too much hassle, and as we can see from the comments, it would resolve a lot of issues for people. $acceptance criteria:$",,Arjen Lentz,Arjen Lentz,Major,36,,4,9,7,3,0,1,1,,0,850,3,0,0,2016-02-23 22:08:21,CanSubquery in FROM clause of a VIEW,"mysql> CREATE VIEW v3 AS SELECT * FROM (SELECT s1 FROM t1) AS x;
ERROR 1349 (HY000): View's SELECT contains a subquery in the FROM clause

See http://bugs.mysql.com/bug.php?id=16757
and http://bugs.mysql.com/bug.php?id=12755

specifically Eric Bergen's comments about the complexity of removing this restriction. It should be doable without too much hassle, and as we can see from the comments, it would resolve a lot of issues for people.",,1,0,0,10,0.0921053,"CanSubquery in FROM clause of a VIEW $end$ mysql> CREATE VIEW v3 AS SELECT * FROM (SELECT s1 FROM t1) AS x;
ERROR 1349 (HY000): View's SELECT contains a subquery in the FROM clause

See http://bugs.mysql.com/bug.php?id=16757
and http://bugs.mysql.com/bug.php?id=12755

specifically Eric Bergen's comments about the complexity of removing this restriction. It should be doable without too much hassle, and as we can see from the comments, it would resolve a lot of issues for people. $acceptance criteria:$",1,1,1,1,0,0,1,27930.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
724,MDEV-427,Task,MDEV,2012-08-02 10:48:20,,0,Provide a systemd script for MariaDB,"MySQL doesn't provide a systemd (http://www.freedesktop.org/wiki/Software/systemd/) script and thus distributions that are shipping systemd have to provide their own. Not many like this. So should we provide one inside the MariaDB package to be better for distributions than MySQL?

Some references:
* https://wiki.archlinux.org/index.php/Systemd/Services#mysqld
* http://en.gentoo-wiki.com/wiki/Systemd#MySQL
* https://bugzilla.redhat.com/show_bug.cgi?id=714426",,"Provide a systemd script for MariaDB $end$ MySQL doesn't provide a systemd (http://www.freedesktop.org/wiki/Software/systemd/) script and thus distributions that are shipping systemd have to provide their own. Not many like this. So should we provide one inside the MariaDB package to be better for distributions than MySQL?

Some references:
* https://wiki.archlinux.org/index.php/Systemd/Services#mysqld
* http://en.gentoo-wiki.com/wiki/Systemd#MySQL
* https://bugzilla.redhat.com/show_bug.cgi?id=714426 $acceptance criteria:$",,Colin Charles,Colin Charles,Critical,64,,2,21,6,5,0,0,0,,0,850,7,0,0,2015-06-02 17:31:46,Provide a systemd script for MariaDB,"MySQL doesn't provide a systemd (http://www.freedesktop.org/wiki/Software/systemd/) script and thus distributions that are shipping systemd have to provide their own. Not many like this. So should we provide one inside the MariaDB package to be better for distributions than MySQL?

Some references:
* https://wiki.archlinux.org/index.php/Systemd/Services#mysqld
* http://en.gentoo-wiki.com/wiki/Systemd#MySQL
* https://bugzilla.redhat.com/show_bug.cgi?id=714426",,0,0,0,0,0.0,"Provide a systemd script for MariaDB $end$ MySQL doesn't provide a systemd (http://www.freedesktop.org/wiki/Software/systemd/) script and thus distributions that are shipping systemd have to provide their own. Not many like this. So should we provide one inside the MariaDB package to be better for distributions than MySQL?

Some references:
* https://wiki.archlinux.org/index.php/Systemd/Services#mysqld
* http://en.gentoo-wiki.com/wiki/Systemd#MySQL
* https://bugzilla.redhat.com/show_bug.cgi?id=714426 $acceptance criteria:$",0,0,0,0,0,0,1,24822.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
725,MDEV-4537,Task,MDEV,2013-05-16 13:38:13,,0,innotop tracker,"I based this task on the one on mytop : MDEV-4476

As of this date, the innotop packaged on Debian/Ubuntu packages (and possibly other ones) of MariaDB is the 1.7.1 and the latest upstream version is the 1.9.0 from september 2012.

1. Document changes in innotop that are in MariaDB on KB
2. Submit patches back to upstream innotop or if no substantial modifications has been done on the MariaDB version adopt the upstream version
3. Potentially package innotop on its own package
4. Add MariaDB specific functionalities such as the query progress and fix bugs",,"innotop tracker $end$ I based this task on the one on mytop : MDEV-4476

As of this date, the innotop packaged on Debian/Ubuntu packages (and possibly other ones) of MariaDB is the 1.7.1 and the latest upstream version is the 1.9.0 from september 2012.

1. Document changes in innotop that are in MariaDB on KB
2. Submit patches back to upstream innotop or if no substantial modifications has been done on the MariaDB version adopt the upstream version
3. Potentially package innotop on its own package
4. Add MariaDB specific functionalities such as the query progress and fix bugs $acceptance criteria:$",,Jean Weisbuch,Jean Weisbuch,Major,13,,0,8,2,1,0,0,0,,0,850,0,0,0,2017-03-02 11:51:46,innotop tracker,"I based this task on the one on mytop : MDEV-4476

As of this date, the innotop packaged on Debian/Ubuntu packages (and possibly other ones) of MariaDB is the 1.7.1 and the latest upstream version is the 1.9.0 from september 2012.

1. Document changes in innotop that are in MariaDB on KB
2. Submit patches back to upstream innotop or if no substantial modifications has been done on the MariaDB version adopt the upstream version
3. Potentially package innotop on its own package
4. Add MariaDB specific functionalities such as the query progress and fix bugs",,0,0,0,0,0.0,"innotop tracker $end$ I based this task on the one on mytop : MDEV-4476

As of this date, the innotop packaged on Debian/Ubuntu packages (and possibly other ones) of MariaDB is the 1.7.1 and the latest upstream version is the 1.9.0 from september 2012.

1. Document changes in innotop that are in MariaDB on KB
2. Submit patches back to upstream innotop or if no substantial modifications has been done on the MariaDB version adopt the upstream version
3. Potentially package innotop on its own package
4. Add MariaDB specific functionalities such as the query progress and fix bugs $acceptance criteria:$",0,0,0,0,0,0,0,33262.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
726,MDEV-4608,Task,MDEV,2013-06-02 14:37:29,,0,deb packages for jessie,"The downloads.mariadb.org website is currenty missing Debian packages for the current testing - jessie.

Also, the project for this bug should probably not be ""MariaDB Development"" yet there is no ""MariaDB Web"" project available.",,"deb packages for jessie $end$ The downloads.mariadb.org website is currenty missing Debian packages for the current testing - jessie.

Also, the project for this bug should probably not be ""MariaDB Development"" yet there is no ""MariaDB Web"" project available. $acceptance criteria:$",,Martin Hradil,Martin Hradil,Major,15,,0,6,0,1,0,0,0,,0,850,1,0,0,2015-06-02 20:04:12,deb packages for jessie,"The downloads.mariadb.org website is currenty missing Debian packages for the current testing - jessie.

Also, the project for this bug should probably not be ""MariaDB Development"" yet there is no ""MariaDB Web"" project available.",,0,0,0,0,0.0,"deb packages for jessie $end$ The downloads.mariadb.org website is currenty missing Debian packages for the current testing - jessie.

Also, the project for this bug should probably not be ""MariaDB Development"" yet there is no ""MariaDB Web"" project available. $acceptance criteria:$",0,0,0,0,0,0,0,17525.4,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
727,MDEV-4646,Task,MDEV,2013-06-12 21:37:36,,0,No mysqld-debug or debuginfo in MariaDB-Server RPM,"Oracle MySQL-Server RPMs include a mysqld-debug binary, but MariaDB-Server RPMs do not. MariaDB should either include mysqld-debug in the MariaDB-Server RPM or should offer it in a separate RPM for situations when it is necessary to use the debug-enabled server.",,"No mysqld-debug or debuginfo in MariaDB-Server RPM $end$ Oracle MySQL-Server RPMs include a mysqld-debug binary, but MariaDB-Server RPMs do not. MariaDB should either include mysqld-debug in the MariaDB-Server RPM or should offer it in a separate RPM for situations when it is necessary to use the debug-enabled server. $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,24,,2,14,4,1,0,1,0,,0,850,8,1,0,2017-03-02 12:22:56,No mysqld-debug or debuginfo in MariaDB-Server RPM,"Oracle MySQL-Server RPMs include a mysqld-debug binary, but MariaDB-Server RPMs do not. MariaDB should either include mysqld-debug in the MariaDB-Server RPM or should offer it in a separate RPM for situations when it is necessary to use the debug-enabled server.",,0,0,0,0,0.0,"No mysqld-debug or debuginfo in MariaDB-Server RPM $end$ Oracle MySQL-Server RPMs include a mysqld-debug binary, but MariaDB-Server RPMs do not. MariaDB should either include mysqld-debug in the MariaDB-Server RPM or should offer it in a separate RPM for situations when it is necessary to use the debug-enabled server. $acceptance criteria:$",0,0,0,0,0,0,0,32606.8,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
728,MDEV-4682,Task,MDEV,2013-06-19 20:47:59,,0,QUERY CACHE - add STATISTICS per query and show this informations in qc_info plugin,"Change sql_cache.cc and sql_cache.h to add hit rates and statistics
usable with qc_info plugin

Statistics:
total hits / total time expent in query cache
hits in three categories:
1) low period category: period between the last hit and the current hit < query expent time 
2) high period category: period < query expent time * 10 and > query expent time
3) outlier category: period > 1 second, and period > query expent time * 10
3.1) in a future version instead of  * 10, we could use number of competitors (concurrent) when inserting this query at query cache
for example if we have 9 concurrent inserts of the same query at query cache this number *10 is an acceptable value for high period, if we have n concurrent inserts a (n+1) value is acceptable

These categories helps to understand what kind of query we have cached:
1) low period hits: in this case if we lose this query entry, we will have a performace problem, and users will tell ""the server is slow now""
2) high period hits: we have a query that is executed with a good frequency, removing it we will have a higher i/o in disks
3) outlier period hits: this query isn't a query that we should cache, or we can loose it and we don't see users reporting ""hey the server is slow, or the disks is spinning a lot""

this allow a better query cache control (MDEV-4584) and a better cache hit rate when optimizing queries with SQL_CACHE/SQL_NO_CACHE


---
a second part could be an variable (query_cache_stats=0/1) to turn off/on the statistics and use a block memory just to statistics instead of one big block sharing memory with queries flags

in this case:
turn on -> off, should clean this stats memory block
turn off -> on, should create stats with 0 value (maybe a problem with lock contention), or clean all query cache and start from zero

changing query cache queries (invalidate, table invalidate, reallocation, etc) should handle the stats block too


---

tables of this plugin:

query_cache_queries:
|| *QUERY_CACHE_ID* || *STATEMENT_SCHEMA* || *STATEMENT_TEXT* || *RESULT_FOUND_ROWS* || *QUERY_ROWS* || *SELECT_ROWS_READ* || *QUERY_HITS* || *QUERY_HITS_PERIOD_LOW* || *QUERY_HITS_PERIOD_HIGH* || *QUERY_HITS_PERIOD_OUTLIERS* || *QUERY_HITS_TOTAL_TIME_US* || *QUERY_HITS_MEAN_PERIOD_US* || *QUERY_HITS_MEAN_PERIOD_LOW_US* || *QUERY_HITS_MEAN_PERIOD_HIGH_US* || *QUERY_INSERT_TIME* || *QUERY_LAST_HIT_TIME* || *SELECT_EXPEND_TIME_US* || *SELECT_LOCK_TIME_US* || *TABLES_TYPE* || *RESULT_LENGTH* || *RESULT_BLOCKS_COUNT* || *RESULT_BLOCKS_SIZE* || *RESULT_BLOCKS_SIZE_USED* || *FLAGS_CLIENT_LONG_FLAG* || *FLAGS_CLIENT_PROTOCOL_41* || *FLAGS_PROTOCOL_TYPE* || *FLAGS_MORE_RESULTS_EXISTS* || *FLAGS_IN_TRANS* || *FLAGS_AUTOCOMMIT* || *FLAGS_PKT_NR* || *FLAGS_CHARACTER_SET_CLIENT* || *FLAGS_CHARACTER_SET_RESULTS* || *FLAGS_COLLATION_CONNECTION* || *FLAGS_LIMIT* || *FLAGS_TIME_ZONE* || *FLAGS_SQL_MODE* || *FLAGS_MAX_SORT_LENGTH* || *FLAGS_GROUP_CONCAT_MAX_LEN* || *FLAGS_DIV_PRECISION_INCREMENT* || *FLAGS_DEFAULT_WEEK_FORMAT* || *FLAGS_LC_TIME_NAMES* ||
|| 1 || teste || select * from t1 || 3 || 3 || 3 || 0 || 0 || 0 || 0 || 0 ||  ||  ||  || 1411247500.713750 ||  || 232 || 35 || NON TRANSACT || 78 || 1 || 512 || 142 || 1 || 1 || 0 || 0 || 0 || 1 || 1 || utf8 || utf8 || utf8_general_ci || -1 || SYSTEM || PIPES_AS_CONCAT,ALLOW_INVALID_DATES || 1024 || 1024 || 5 || 0 || en_US ||

query_cache_queries_tables:
|| *QUERY_CACHE_ID* || *SCHEMA* || *TABLE_NAME* || *TABLE_SUFFIX* ||
|| 1 || teste || t1 || (here we display partition name like P#p0) ||

query_cache_tables
|| *TABLE_SCHEMA* || *TABLE_NAME* || *TABLE_HASHED* || *TABLE_TYPE* || *QUERIES_IN_CACHE* ||
|| teste || t1 || 1 || NON_TRANSACT || 1 ||
",,"QUERY CACHE - add STATISTICS per query and show this informations in qc_info plugin $end$ Change sql_cache.cc and sql_cache.h to add hit rates and statistics
usable with qc_info plugin

Statistics:
total hits / total time expent in query cache
hits in three categories:
1) low period category: period between the last hit and the current hit < query expent time 
2) high period category: period < query expent time * 10 and > query expent time
3) outlier category: period > 1 second, and period > query expent time * 10
3.1) in a future version instead of  * 10, we could use number of competitors (concurrent) when inserting this query at query cache
for example if we have 9 concurrent inserts of the same query at query cache this number *10 is an acceptable value for high period, if we have n concurrent inserts a (n+1) value is acceptable

These categories helps to understand what kind of query we have cached:
1) low period hits: in this case if we lose this query entry, we will have a performace problem, and users will tell ""the server is slow now""
2) high period hits: we have a query that is executed with a good frequency, removing it we will have a higher i/o in disks
3) outlier period hits: this query isn't a query that we should cache, or we can loose it and we don't see users reporting ""hey the server is slow, or the disks is spinning a lot""

this allow a better query cache control (MDEV-4584) and a better cache hit rate when optimizing queries with SQL_CACHE/SQL_NO_CACHE


---
a second part could be an variable (query_cache_stats=0/1) to turn off/on the statistics and use a block memory just to statistics instead of one big block sharing memory with queries flags

in this case:
turn on -> off, should clean this stats memory block
turn off -> on, should create stats with 0 value (maybe a problem with lock contention), or clean all query cache and start from zero

changing query cache queries (invalidate, table invalidate, reallocation, etc) should handle the stats block too


---

tables of this plugin:

query_cache_queries:
|| *QUERY_CACHE_ID* || *STATEMENT_SCHEMA* || *STATEMENT_TEXT* || *RESULT_FOUND_ROWS* || *QUERY_ROWS* || *SELECT_ROWS_READ* || *QUERY_HITS* || *QUERY_HITS_PERIOD_LOW* || *QUERY_HITS_PERIOD_HIGH* || *QUERY_HITS_PERIOD_OUTLIERS* || *QUERY_HITS_TOTAL_TIME_US* || *QUERY_HITS_MEAN_PERIOD_US* || *QUERY_HITS_MEAN_PERIOD_LOW_US* || *QUERY_HITS_MEAN_PERIOD_HIGH_US* || *QUERY_INSERT_TIME* || *QUERY_LAST_HIT_TIME* || *SELECT_EXPEND_TIME_US* || *SELECT_LOCK_TIME_US* || *TABLES_TYPE* || *RESULT_LENGTH* || *RESULT_BLOCKS_COUNT* || *RESULT_BLOCKS_SIZE* || *RESULT_BLOCKS_SIZE_USED* || *FLAGS_CLIENT_LONG_FLAG* || *FLAGS_CLIENT_PROTOCOL_41* || *FLAGS_PROTOCOL_TYPE* || *FLAGS_MORE_RESULTS_EXISTS* || *FLAGS_IN_TRANS* || *FLAGS_AUTOCOMMIT* || *FLAGS_PKT_NR* || *FLAGS_CHARACTER_SET_CLIENT* || *FLAGS_CHARACTER_SET_RESULTS* || *FLAGS_COLLATION_CONNECTION* || *FLAGS_LIMIT* || *FLAGS_TIME_ZONE* || *FLAGS_SQL_MODE* || *FLAGS_MAX_SORT_LENGTH* || *FLAGS_GROUP_CONCAT_MAX_LEN* || *FLAGS_DIV_PRECISION_INCREMENT* || *FLAGS_DEFAULT_WEEK_FORMAT* || *FLAGS_LC_TIME_NAMES* ||
|| 1 || teste || select * from t1 || 3 || 3 || 3 || 0 || 0 || 0 || 0 || 0 ||  ||  ||  || 1411247500.713750 ||  || 232 || 35 || NON TRANSACT || 78 || 1 || 512 || 142 || 1 || 1 || 0 || 0 || 0 || 1 || 1 || utf8 || utf8 || utf8_general_ci || -1 || SYSTEM || PIPES_AS_CONCAT,ALLOW_INVALID_DATES || 1024 || 1024 || 5 || 0 || en_US ||

query_cache_queries_tables:
|| *QUERY_CACHE_ID* || *SCHEMA* || *TABLE_NAME* || *TABLE_SUFFIX* ||
|| 1 || teste || t1 || (here we display partition name like P#p0) ||

query_cache_tables
|| *TABLE_SCHEMA* || *TABLE_NAME* || *TABLE_HASHED* || *TABLE_TYPE* || *QUERIES_IN_CACHE* ||
|| teste || t1 || 1 || NON_TRANSACT || 1 ||
 $acceptance criteria:$",,roberto spadim,roberto spadim,Major,34,,1,15,7,1,0,7,0,,0,850,1,7,0,2015-09-09 10:51:07,QUERY CACHE - add STATISTICS per query and show this informations in qc_info plugin,"Change sql_cache.cc and sql_cache.h to add hit rates and statistics
usable with qc_info plugin

Statistics:
total hits / total time expent in query cache
hits in three categories:
1) low period category: period between the last hit and the current hit < query expent time 
2) high period category: period < query expent time * 10 and > query expent time
3) outlier category: period > 1 second, and period > query expent time * 10
3.1) in a future version instead of  * 10, we could use number of competitors (concurrent) when inserting this query at query cache
for example if we have 9 concurrent inserts of the same query at query cache this number *10 is an acceptable value for high period, if we have n concurrent inserts a (n+1) value is acceptable

These categories helps to understand what kind of query we have cached:
1) low period hits: in this case if we lose this query entry, we will have a performace problem, and users will tell ""the server is slow now""
2) high period hits: we have a query that is executed with a good frequency, removing it we will have a higher i/o in disks
3) outlier period hits: this query isn't a query that we should cache, or we can loose it and we don't see users reporting ""hey the server is slow, or the disks is spinning a lot""

this allow a better query cache control (MDEV-4584) and a better cache hit rate when optimizing queries with SQL_CACHE/SQL_NO_CACHE


---
a second part could be an variable (query_cache_stats=0/1) to turn off/on the statistics and use a block memory just to statistics instead of one big block sharing memory with queries flags

in this case:
turn on -> off, should clean this stats memory block
turn off -> on, should create stats with 0 value (maybe a problem with lock contention), or clean all query cache and start from zero

changing query cache queries (invalidate, table invalidate, reallocation, etc) should handle the stats block too


---

tables of this plugin:

query_cache_queries:
|| *QUERY_CACHE_ID* || *STATEMENT_SCHEMA* || *STATEMENT_TEXT* || *RESULT_FOUND_ROWS* || *QUERY_ROWS* || *SELECT_ROWS_READ* || *QUERY_HITS* || *QUERY_HITS_PERIOD_LOW* || *QUERY_HITS_PERIOD_HIGH* || *QUERY_HITS_PERIOD_OUTLIERS* || *QUERY_HITS_TOTAL_TIME_US* || *QUERY_HITS_MEAN_PERIOD_US* || *QUERY_HITS_MEAN_PERIOD_LOW_US* || *QUERY_HITS_MEAN_PERIOD_HIGH_US* || *QUERY_INSERT_TIME* || *QUERY_LAST_HIT_TIME* || *SELECT_EXPEND_TIME_US* || *SELECT_LOCK_TIME_US* || *TABLES_TYPE* || *RESULT_LENGTH* || *RESULT_BLOCKS_COUNT* || *RESULT_BLOCKS_SIZE* || *RESULT_BLOCKS_SIZE_USED* || *FLAGS_CLIENT_LONG_FLAG* || *FLAGS_CLIENT_PROTOCOL_41* || *FLAGS_PROTOCOL_TYPE* || *FLAGS_MORE_RESULTS_EXISTS* || *FLAGS_IN_TRANS* || *FLAGS_AUTOCOMMIT* || *FLAGS_PKT_NR* || *FLAGS_CHARACTER_SET_CLIENT* || *FLAGS_CHARACTER_SET_RESULTS* || *FLAGS_COLLATION_CONNECTION* || *FLAGS_LIMIT* || *FLAGS_TIME_ZONE* || *FLAGS_SQL_MODE* || *FLAGS_MAX_SORT_LENGTH* || *FLAGS_GROUP_CONCAT_MAX_LEN* || *FLAGS_DIV_PRECISION_INCREMENT* || *FLAGS_DEFAULT_WEEK_FORMAT* || *FLAGS_LC_TIME_NAMES* ||
|| 1 || teste || select * from t1 || 3 || 3 || 3 || 0 || 0 || 0 || 0 || 0 ||  ||  ||  || 1411247500.713750 ||  || 232 || 35 || NON TRANSACT || 78 || 1 || 512 || 142 || 1 || 1 || 0 || 0 || 0 || 1 || 1 || utf8 || utf8 || utf8_general_ci || -1 || SYSTEM || PIPES_AS_CONCAT,ALLOW_INVALID_DATES || 1024 || 1024 || 5 || 0 || en_US ||

query_cache_queries_tables:
|| *QUERY_CACHE_ID* || *SCHEMA* || *TABLE_NAME* || *TABLE_SUFFIX* ||
|| 1 || teste || t1 || (here we display partition name like P#p0) ||

query_cache_tables
|| *TABLE_SCHEMA* || *TABLE_NAME* || *TABLE_HASHED* || *TABLE_TYPE* || *QUERIES_IN_CACHE* ||
|| teste || t1 || 1 || NON_TRANSACT || 1 ||
",,0,0,0,0,0.0,"QUERY CACHE - add STATISTICS per query and show this informations in qc_info plugin $end$ Change sql_cache.cc and sql_cache.h to add hit rates and statistics
usable with qc_info plugin

Statistics:
total hits / total time expent in query cache
hits in three categories:
1) low period category: period between the last hit and the current hit < query expent time 
2) high period category: period < query expent time * 10 and > query expent time
3) outlier category: period > 1 second, and period > query expent time * 10
3.1) in a future version instead of  * 10, we could use number of competitors (concurrent) when inserting this query at query cache
for example if we have 9 concurrent inserts of the same query at query cache this number *10 is an acceptable value for high period, if we have n concurrent inserts a (n+1) value is acceptable

These categories helps to understand what kind of query we have cached:
1) low period hits: in this case if we lose this query entry, we will have a performace problem, and users will tell ""the server is slow now""
2) high period hits: we have a query that is executed with a good frequency, removing it we will have a higher i/o in disks
3) outlier period hits: this query isn't a query that we should cache, or we can loose it and we don't see users reporting ""hey the server is slow, or the disks is spinning a lot""

this allow a better query cache control (MDEV-4584) and a better cache hit rate when optimizing queries with SQL_CACHE/SQL_NO_CACHE


---
a second part could be an variable (query_cache_stats=0/1) to turn off/on the statistics and use a block memory just to statistics instead of one big block sharing memory with queries flags

in this case:
turn on -> off, should clean this stats memory block
turn off -> on, should create stats with 0 value (maybe a problem with lock contention), or clean all query cache and start from zero

changing query cache queries (invalidate, table invalidate, reallocation, etc) should handle the stats block too


---

tables of this plugin:

query_cache_queries:
|| *QUERY_CACHE_ID* || *STATEMENT_SCHEMA* || *STATEMENT_TEXT* || *RESULT_FOUND_ROWS* || *QUERY_ROWS* || *SELECT_ROWS_READ* || *QUERY_HITS* || *QUERY_HITS_PERIOD_LOW* || *QUERY_HITS_PERIOD_HIGH* || *QUERY_HITS_PERIOD_OUTLIERS* || *QUERY_HITS_TOTAL_TIME_US* || *QUERY_HITS_MEAN_PERIOD_US* || *QUERY_HITS_MEAN_PERIOD_LOW_US* || *QUERY_HITS_MEAN_PERIOD_HIGH_US* || *QUERY_INSERT_TIME* || *QUERY_LAST_HIT_TIME* || *SELECT_EXPEND_TIME_US* || *SELECT_LOCK_TIME_US* || *TABLES_TYPE* || *RESULT_LENGTH* || *RESULT_BLOCKS_COUNT* || *RESULT_BLOCKS_SIZE* || *RESULT_BLOCKS_SIZE_USED* || *FLAGS_CLIENT_LONG_FLAG* || *FLAGS_CLIENT_PROTOCOL_41* || *FLAGS_PROTOCOL_TYPE* || *FLAGS_MORE_RESULTS_EXISTS* || *FLAGS_IN_TRANS* || *FLAGS_AUTOCOMMIT* || *FLAGS_PKT_NR* || *FLAGS_CHARACTER_SET_CLIENT* || *FLAGS_CHARACTER_SET_RESULTS* || *FLAGS_COLLATION_CONNECTION* || *FLAGS_LIMIT* || *FLAGS_TIME_ZONE* || *FLAGS_SQL_MODE* || *FLAGS_MAX_SORT_LENGTH* || *FLAGS_GROUP_CONCAT_MAX_LEN* || *FLAGS_DIV_PRECISION_INCREMENT* || *FLAGS_DEFAULT_WEEK_FORMAT* || *FLAGS_LC_TIME_NAMES* ||
|| 1 || teste || select * from t1 || 3 || 3 || 3 || 0 || 0 || 0 || 0 || 0 ||  ||  ||  || 1411247500.713750 ||  || 232 || 35 || NON TRANSACT || 78 || 1 || 512 || 142 || 1 || 1 || 0 || 0 || 0 || 1 || 1 || utf8 || utf8 || utf8_general_ci || -1 || SYSTEM || PIPES_AS_CONCAT,ALLOW_INVALID_DATES || 1024 || 1024 || 5 || 0 || en_US ||

query_cache_queries_tables:
|| *QUERY_CACHE_ID* || *SCHEMA* || *TABLE_NAME* || *TABLE_SUFFIX* ||
|| 1 || teste || t1 || (here we display partition name like P#p0) ||

query_cache_tables
|| *TABLE_SCHEMA* || *TABLE_NAME* || *TABLE_HASHED* || *TABLE_TYPE* || *QUERIES_IN_CACHE* ||
|| teste || t1 || 1 || NON_TRANSACT || 1 ||
 $acceptance criteria:$",0,0,0,0,0,0,0,19478.1,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
729,MDEV-4691,Task,MDEV,2013-06-21 19:26:48,,0,Kerberize MariaDB -- add Kerberos authentication support to MariaDB,"This task identifies TODOs to add Kerberos authentication support to MariaDB.

Kerberos is a standardised network authentication protocol providing mutual authentication of potential users and network service. Many commercial relational database management systems have already internalized support for Kerberos authentication. The goal of this project aims to adding cross-platform Kerberos authentication support to MariaDB database.

h3. User Scenario
In this section, we give a normal use case.
Suppose user Adam what to authentication against his Kerberos principal.

*Step 1* Create a login user in MariaDB and specify the _kerberos_ as server side authentication plugin.
{code}
CREATE USER 'adam' IDENTIFIED VIA kerberos AS 'adam/mariadb@lp';
{code}
The gap between MariaDB username length and Kerberos principal name length make it error-prone to embedded a whole principal name into a MariaDB login name. We use a short name as MariaDB login name and identify the principal name with the *AS* clause. If the *AS* clause is absent when creating a user, the MariaDB login name is used as principal name implicitly.
*Step 2* At client side, Adam acquires a service ticket (or access token) to MariaDB.
*Step 3* Adam tries to login with short login name.
{code}
$ mysql -u adam
{code}
*Step 4* If _adam/mariadb@lp_ is a valid principal in Kerberos and the service ticket is not expired, Adam can login MariaDB passwordlessly; otherwise, he will receive an actionable error message.

h3. The Source Code
Source codes will be located in the _plugin/auth_kerberos_ directory.
*_mysql_declar_plugin/mysql_declare_plugin_end_* macros can be used to define server side plugin and *_mysql_declare_client_plugin/mysql_end_client_plugin_* macros for client side.

h3. Client-Server Communication
This section defines the message exchanges between client and server during Kerberos authentication.

*Step 1* Server sends the null-terminated SPN to the client (the SPN is given as server parameter, documented in yet another section).
*Step 2* Client receives SPN from server with *_vio->read_packet_*, creates a secure context with the SPN by *_gss_init_sec_context_* repeating until done.
*Step 3* Client writes the output token created by *_gss_init_sec_context_* to the server, using *_vio->write_packet_*.
*Step 4* Server acquire credential with *_gss_acquire_cred_* to get the initial server credential.
*Step 5* Server reads token created at *step 3*, checks the principal name enclosed in the token with its credential get in *step 4* and accepts the access requests if recognizable otherwise fails the connection.

h3. Figure-out SPN
This section describes the policy to identify a valid service principal name.

For the server side plugin, an SPN is requried. The principal name is an option in configuration file (e.g. _~/.my.cfn_) with {code}spn=primary/instance@realm{code} or {code}spn=primary@machine.domain{code} depending on its platform, where *primary* is the service name. If this option does not present, *mysql* is the default SPN.

h3. Cross-platform Requirements

GSSAPI based Kerberos authentication is widely used in *nix world, while Windows also provides an SSPI based Kerberos authentication process. The plugin will support Windows-SSPI in addition to GSSAPi to maximize the compatibility.

h3. Links
# Configure Kerberos authentication in Oracle RDBMS. http://docs.oracle.com/cd/A97630_01/network.920/a96573/asokerb.htm
# Using Kerberos authentication with SQL Server. http://msdn.microsoft.com/en-us/library/cc280745(v=sql.105).aspx
# Configuring Kerberos for Sybase. http://www.sybase.com.hk/content/1029260/1029260.pdf
# The Kerberos network authentication service (v5). http://tools.ietf.org/html/rfc4120
# GSS-API C-binding. http://tools.ietf.org/html/rfc2744
# The Kerberos version 5 Generic Security Service Application Program Interface (GSS-API) mechanism: Version 2. http://tools.ietf.org/html/rfc4121
# MariaDB Pluggable Authentication. https://kb.askmonty.org/en/development-pluggable-authentication/
# How the Kerberos Version 5 Authentication Protocol Works. http://technet.microsoft.com/en-us/library/cc772815(v=ws.10).aspx
# SSPI/Kerberos Interoperability with GSSAPI. http://msdn.microsoft.com/en-us/library/ms995352.aspx
# Best Practices for Integrating Kerberos into Your Application. http://www.kerberos.org/software/appskerberos.pdf",,"Kerberize MariaDB -- add Kerberos authentication support to MariaDB $end$ This task identifies TODOs to add Kerberos authentication support to MariaDB.

Kerberos is a standardised network authentication protocol providing mutual authentication of potential users and network service. Many commercial relational database management systems have already internalized support for Kerberos authentication. The goal of this project aims to adding cross-platform Kerberos authentication support to MariaDB database.

h3. User Scenario
In this section, we give a normal use case.
Suppose user Adam what to authentication against his Kerberos principal.

*Step 1* Create a login user in MariaDB and specify the _kerberos_ as server side authentication plugin.
{code}
CREATE USER 'adam' IDENTIFIED VIA kerberos AS 'adam/mariadb@lp';
{code}
The gap between MariaDB username length and Kerberos principal name length make it error-prone to embedded a whole principal name into a MariaDB login name. We use a short name as MariaDB login name and identify the principal name with the *AS* clause. If the *AS* clause is absent when creating a user, the MariaDB login name is used as principal name implicitly.
*Step 2* At client side, Adam acquires a service ticket (or access token) to MariaDB.
*Step 3* Adam tries to login with short login name.
{code}
$ mysql -u adam
{code}
*Step 4* If _adam/mariadb@lp_ is a valid principal in Kerberos and the service ticket is not expired, Adam can login MariaDB passwordlessly; otherwise, he will receive an actionable error message.

h3. The Source Code
Source codes will be located in the _plugin/auth_kerberos_ directory.
*_mysql_declar_plugin/mysql_declare_plugin_end_* macros can be used to define server side plugin and *_mysql_declare_client_plugin/mysql_end_client_plugin_* macros for client side.

h3. Client-Server Communication
This section defines the message exchanges between client and server during Kerberos authentication.

*Step 1* Server sends the null-terminated SPN to the client (the SPN is given as server parameter, documented in yet another section).
*Step 2* Client receives SPN from server with *_vio->read_packet_*, creates a secure context with the SPN by *_gss_init_sec_context_* repeating until done.
*Step 3* Client writes the output token created by *_gss_init_sec_context_* to the server, using *_vio->write_packet_*.
*Step 4* Server acquire credential with *_gss_acquire_cred_* to get the initial server credential.
*Step 5* Server reads token created at *step 3*, checks the principal name enclosed in the token with its credential get in *step 4* and accepts the access requests if recognizable otherwise fails the connection.

h3. Figure-out SPN
This section describes the policy to identify a valid service principal name.

For the server side plugin, an SPN is requried. The principal name is an option in configuration file (e.g. _~/.my.cfn_) with {code}spn=primary/instance@realm{code} or {code}spn=primary@machine.domain{code} depending on its platform, where *primary* is the service name. If this option does not present, *mysql* is the default SPN.

h3. Cross-platform Requirements

GSSAPI based Kerberos authentication is widely used in *nix world, while Windows also provides an SSPI based Kerberos authentication process. The plugin will support Windows-SSPI in addition to GSSAPi to maximize the compatibility.

h3. Links
# Configure Kerberos authentication in Oracle RDBMS. http://docs.oracle.com/cd/A97630_01/network.920/a96573/asokerb.htm
# Using Kerberos authentication with SQL Server. http://msdn.microsoft.com/en-us/library/cc280745(v=sql.105).aspx
# Configuring Kerberos for Sybase. http://www.sybase.com.hk/content/1029260/1029260.pdf
# The Kerberos network authentication service (v5). http://tools.ietf.org/html/rfc4120
# GSS-API C-binding. http://tools.ietf.org/html/rfc2744
# The Kerberos version 5 Generic Security Service Application Program Interface (GSS-API) mechanism: Version 2. http://tools.ietf.org/html/rfc4121
# MariaDB Pluggable Authentication. https://kb.askmonty.org/en/development-pluggable-authentication/
# How the Kerberos Version 5 Authentication Protocol Works. http://technet.microsoft.com/en-us/library/cc772815(v=ws.10).aspx
# SSPI/Kerberos Interoperability with GSSAPI. http://msdn.microsoft.com/en-us/library/ms995352.aspx
# Best Practices for Integrating Kerberos into Your Application. http://www.kerberos.org/software/appskerberos.pdf $acceptance criteria:$",,QIU Shuang,QIU Shuang,Critical,29,,0,29,2,3,0,0,0,,0,850,7,0,0,2015-11-17 17:51:35,Kerberize MariaDB -- add Kerberos authentication support to MariaDB,"This task identifies TODOs to add Kerberos authentication support to MariaDB.

Kerberos is a standardised network authentication protocol providing mutual authentication of potential users and network service. Many commercial relational database management systems have already internalized support for Kerberos authentication. The goal of this project aims to adding cross-platform Kerberos authentication support to MariaDB database.

h3. User Scenario
In this section, we give a normal use case.
Suppose user Adam what to authentication against his Kerberos principal.

*Step 1* Create a login user in MariaDB and specify the _kerberos_ as server side authentication plugin.
{code}
CREATE USER 'adam' IDENTIFIED VIA kerberos AS 'adam/mariadb@lp';
{code}
The gap between MariaDB username length and Kerberos principal name length make it error-prone to embedded a whole principal name into a MariaDB login name. We use a short name as MariaDB login name and identify the principal name with the *AS* clause. If the *AS* clause is absent when creating a user, the MariaDB login name is used as principal name implicitly.
*Step 2* At client side, Adam acquires a service ticket (or access token) to MariaDB.
*Step 3* Adam tries to login with short login name.
{code}
$ mysql -u adam
{code}
*Step 4* If _adam/mariadb@lp_ is a valid principal in Kerberos and the service ticket is not expired, Adam can login MariaDB passwordlessly; otherwise, he will receive an actionable error message.

h3. The Source Code
Source codes will be located in the _plugin/auth_kerberos_ directory.
*_mysql_declar_plugin/mysql_declare_plugin_end_* macros can be used to define server side plugin and *_mysql_declare_client_plugin/mysql_end_client_plugin_* macros for client side.

h3. Client-Server Communication
This section defines the message exchanges between client and server during Kerberos authentication.

*Step 1* Server sends the null-terminated SPN to the client (the SPN is given as server parameter, documented in yet another section).
*Step 2* Client receives SPN from server with *_vio->read_packet_*, creates a secure context with the SPN by *_gss_init_sec_context_* repeating until done.
*Step 3* Client writes the output token created by *_gss_init_sec_context_* to the server, using *_vio->write_packet_*.
*Step 4* Server acquire credential with *_gss_acquire_cred_* to get the initial server credential.
*Step 5* Server reads token created at *step 3*, checks the principal name enclosed in the token with its credential get in *step 4* and accepts the access requests if recognizable otherwise fails the connection.

h3. Figure-out SPN
This section describes the policy to identify a valid service principal name.

For the server side plugin, an SPN is requried. The principal name is an option in configuration file (e.g. _~/.my.cfn_) with {code}spn=primary/instance@realm{code} or {code}spn=primary@machine.domain{code} depending on its platform, where *primary* is the service name. If this option does not present, *mysql* is the default SPN.

h3. Cross-platform Requirements

GSSAPI based Kerberos authentication is widely used in *nix world, while Windows also provides an SSPI based Kerberos authentication process. The plugin will support Windows-SSPI in addition to GSSAPi to maximize the compatibility.

h3. Links
# Configure Kerberos authentication in Oracle RDBMS. http://docs.oracle.com/cd/A97630_01/network.920/a96573/asokerb.htm
# Using Kerberos authentication with SQL Server. http://msdn.microsoft.com/en-us/library/cc280745(v=sql.105).aspx
# Configuring Kerberos for Sybase. http://www.sybase.com.hk/content/1029260/1029260.pdf
# The Kerberos network authentication service (v5). http://tools.ietf.org/html/rfc4120
# GSS-API C-binding. http://tools.ietf.org/html/rfc2744
# The Kerberos version 5 Generic Security Service Application Program Interface (GSS-API) mechanism: Version 2. http://tools.ietf.org/html/rfc4121
# MariaDB Pluggable Authentication. https://kb.askmonty.org/en/development-pluggable-authentication/
# How the Kerberos Version 5 Authentication Protocol Works. http://technet.microsoft.com/en-us/library/cc772815(v=ws.10).aspx
# SSPI/Kerberos Interoperability with GSSAPI. http://msdn.microsoft.com/en-us/library/ms995352.aspx
# Best Practices for Integrating Kerberos into Your Application. http://www.kerberos.org/software/appskerberos.pdf",,0,0,0,0,0.0,"Kerberize MariaDB -- add Kerberos authentication support to MariaDB $end$ This task identifies TODOs to add Kerberos authentication support to MariaDB.

Kerberos is a standardised network authentication protocol providing mutual authentication of potential users and network service. Many commercial relational database management systems have already internalized support for Kerberos authentication. The goal of this project aims to adding cross-platform Kerberos authentication support to MariaDB database.

h3. User Scenario
In this section, we give a normal use case.
Suppose user Adam what to authentication against his Kerberos principal.

*Step 1* Create a login user in MariaDB and specify the _kerberos_ as server side authentication plugin.
{code}
CREATE USER 'adam' IDENTIFIED VIA kerberos AS 'adam/mariadb@lp';
{code}
The gap between MariaDB username length and Kerberos principal name length make it error-prone to embedded a whole principal name into a MariaDB login name. We use a short name as MariaDB login name and identify the principal name with the *AS* clause. If the *AS* clause is absent when creating a user, the MariaDB login name is used as principal name implicitly.
*Step 2* At client side, Adam acquires a service ticket (or access token) to MariaDB.
*Step 3* Adam tries to login with short login name.
{code}
$ mysql -u adam
{code}
*Step 4* If _adam/mariadb@lp_ is a valid principal in Kerberos and the service ticket is not expired, Adam can login MariaDB passwordlessly; otherwise, he will receive an actionable error message.

h3. The Source Code
Source codes will be located in the _plugin/auth_kerberos_ directory.
*_mysql_declar_plugin/mysql_declare_plugin_end_* macros can be used to define server side plugin and *_mysql_declare_client_plugin/mysql_end_client_plugin_* macros for client side.

h3. Client-Server Communication
This section defines the message exchanges between client and server during Kerberos authentication.

*Step 1* Server sends the null-terminated SPN to the client (the SPN is given as server parameter, documented in yet another section).
*Step 2* Client receives SPN from server with *_vio->read_packet_*, creates a secure context with the SPN by *_gss_init_sec_context_* repeating until done.
*Step 3* Client writes the output token created by *_gss_init_sec_context_* to the server, using *_vio->write_packet_*.
*Step 4* Server acquire credential with *_gss_acquire_cred_* to get the initial server credential.
*Step 5* Server reads token created at *step 3*, checks the principal name enclosed in the token with its credential get in *step 4* and accepts the access requests if recognizable otherwise fails the connection.

h3. Figure-out SPN
This section describes the policy to identify a valid service principal name.

For the server side plugin, an SPN is requried. The principal name is an option in configuration file (e.g. _~/.my.cfn_) with {code}spn=primary/instance@realm{code} or {code}spn=primary@machine.domain{code} depending on its platform, where *primary* is the service name. If this option does not present, *mysql* is the default SPN.

h3. Cross-platform Requirements

GSSAPI based Kerberos authentication is widely used in *nix world, while Windows also provides an SSPI based Kerberos authentication process. The plugin will support Windows-SSPI in addition to GSSAPi to maximize the compatibility.

h3. Links
# Configure Kerberos authentication in Oracle RDBMS. http://docs.oracle.com/cd/A97630_01/network.920/a96573/asokerb.htm
# Using Kerberos authentication with SQL Server. http://msdn.microsoft.com/en-us/library/cc280745(v=sql.105).aspx
# Configuring Kerberos for Sybase. http://www.sybase.com.hk/content/1029260/1029260.pdf
# The Kerberos network authentication service (v5). http://tools.ietf.org/html/rfc4120
# GSS-API C-binding. http://tools.ietf.org/html/rfc2744
# The Kerberos version 5 Generic Security Service Application Program Interface (GSS-API) mechanism: Version 2. http://tools.ietf.org/html/rfc4121
# MariaDB Pluggable Authentication. https://kb.askmonty.org/en/development-pluggable-authentication/
# How the Kerberos Version 5 Authentication Protocol Works. http://technet.microsoft.com/en-us/library/cc772815(v=ws.10).aspx
# SSPI/Kerberos Interoperability with GSSAPI. http://msdn.microsoft.com/en-us/library/ms995352.aspx
# Best Practices for Integrating Kerberos into Your Application. http://www.kerberos.org/software/appskerberos.pdf $acceptance criteria:$",0,0,0,0,0,0,1,21094.4,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
730,MDEV-4912,Epic,MDEV,2013-08-17 20:48:17,,0,Data type plugin API version 1,"Issues to solve:
* read/write: that's easy, the plugin provides some kind of a ""store"" method that serializes the data into something that can be memcmp-ed. And ""val"" methods as appropriate.
* indexing: not at issue, the engine thinks the column type is BINARY and indexes is accordingly
* don't forget that field definition may use parameters, like in VARCHAR(10)
* protocol
** text protocol - what to send as field metadata? variant: MYSQL_TYPE_PLUGIN and a string with the type name
** binary protocol - how to send the value to the client: variant: as a string. how the client send the value to the server? either way, the server converts it from number/string/etc just as for INSERTs
* replication, RBR
** similar, use MYSQL_TYPE_PLUGIN, and as additional metadata - field type as a string, parameters, whatever

So, a plugin would need to provide
* store() method from at least some of the basic types
* val() methods to at least some of the basic types
* description of whatever parameters a field definition takes
* informational methods, like store_length(), etc

*This task doesn't cover everything!* It is assumed that we can expand this API later to add more features. In particular, the following is not solved:
* data types that cannot be efficiently memcmp()'ed. For example, this proposal doesn't allow to implement a string type with charset support.
* data types that require special indexes, such as XML, spatial data, etc.

_see also the original issue description in the history_",,"Data type plugin API version 1 $end$ Issues to solve:
* read/write: that's easy, the plugin provides some kind of a ""store"" method that serializes the data into something that can be memcmp-ed. And ""val"" methods as appropriate.
* indexing: not at issue, the engine thinks the column type is BINARY and indexes is accordingly
* don't forget that field definition may use parameters, like in VARCHAR(10)
* protocol
** text protocol - what to send as field metadata? variant: MYSQL_TYPE_PLUGIN and a string with the type name
** binary protocol - how to send the value to the client: variant: as a string. how the client send the value to the server? either way, the server converts it from number/string/etc just as for INSERTs
* replication, RBR
** similar, use MYSQL_TYPE_PLUGIN, and as additional metadata - field type as a string, parameters, whatever

So, a plugin would need to provide
* store() method from at least some of the basic types
* val() methods to at least some of the basic types
* description of whatever parameters a field definition takes
* informational methods, like store_length(), etc

*This task doesn't cover everything!* It is assumed that we can expand this API later to add more features. In particular, the following is not solved:
* data types that cannot be efficiently memcmp()'ed. For example, this proposal doesn't allow to implement a string type with charset support.
* data types that require special indexes, such as XML, spatial data, etc.

_see also the original issue description in the history_ $acceptance criteria:$",,roberto spadim,roberto spadim,Minor,142,,89,2,263,13,0,5,0,,0,850,1,4,0,2015-11-17 17:52:35,Add a plugin to field types (column types),"Issues to solve:
* read/write: that's easy, the plugin provides some kind of a ""store"" method that serializes the data into something that can be memcmp-ed. And ""val"" methods as appropriate.
* indexing: not at issue, the engine thinks the column type is BINARY and indexes is accordingly
* don't forget that field definition may use parameters, like in VARCHAR(10)
* protocol
** text protocol - what to send as field metadata? variant: MYSQL_TYPE_PLUGIN and a string with the type name
** binary protocol - how to send the value to the client: variant: as a string. how the client send the value to the server? either way, the server converts it from number/string/etc just as for INSERTs
* replication, RBR
** similar, use MYSQL_TYPE_PLUGIN, and as additional metadata - field type as a string, parameters, whatever

So, a plugin would need to provide
* store() method from at least some of the basic types
* val() methods to at least some of the basic types
* description of whatever parameters a field definition takes
* informational methods, like store_length(), etc

*This task doesn't cover everything!* It is assumed that we can expand this API later to add more features. In particular, the following is not solved:
* data types that cannot be efficiently memcmp()'ed. For example, this proposal doesn't allow to implement a string type with charset support.
* data types that require special indexes, such as XML, spatial data, etc.

_see also the original issue description in the history_",,1,0,0,12,0.0267176,"Add a plugin to field types (column types) $end$ Issues to solve:
* read/write: that's easy, the plugin provides some kind of a ""store"" method that serializes the data into something that can be memcmp-ed. And ""val"" methods as appropriate.
* indexing: not at issue, the engine thinks the column type is BINARY and indexes is accordingly
* don't forget that field definition may use parameters, like in VARCHAR(10)
* protocol
** text protocol - what to send as field metadata? variant: MYSQL_TYPE_PLUGIN and a string with the type name
** binary protocol - how to send the value to the client: variant: as a string. how the client send the value to the server? either way, the server converts it from number/string/etc just as for INSERTs
* replication, RBR
** similar, use MYSQL_TYPE_PLUGIN, and as additional metadata - field type as a string, parameters, whatever

So, a plugin would need to provide
* store() method from at least some of the basic types
* val() methods to at least some of the basic types
* description of whatever parameters a field definition takes
* informational methods, like store_length(), etc

*This task doesn't cover everything!* It is assumed that we can expand this API later to add more features. In particular, the following is not solved:
* data types that cannot be efficiently memcmp()'ed. For example, this proposal doesn't allow to implement a string type with charset support.
* data types that require special indexes, such as XML, spatial data, etc.

_see also the original issue description in the history_ $acceptance criteria:$",1,1,1,1,0,0,1,19725.1,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
731,MDEV-505,Task,MDEV,2012-09-05 02:47:29,,0,feature request: add \H option for mysql client prompt,"Please add a \H option for the mysql client prompt, as discussed at http://blog.wl0.org/2009/08/mysql-hostname-prompt-when-host-is-localhost/

This should show the value of the server's ""hostname"" variable in the prompt of the client connected to the server.",,"feature request: add \H option for mysql client prompt $end$ Please add a \H option for the mysql client prompt, as discussed at http://blog.wl0.org/2009/08/mysql-hostname-prompt-when-host-is-localhost/

This should show the value of the server's ""hostname"" variable in the prompt of the client connected to the server. $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,29,,1,3,1,3,0,0,0,,0,850,3,0,0,2015-11-18 10:49:33,feature request: add \H option for mysql client prompt,"Please add a \H option for the mysql client prompt, as discussed at http://blog.wl0.org/2009/08/mysql-hostname-prompt-when-host-is-localhost/

This should show the value of the server's ""hostname"" variable in the prompt of the client connected to the server.",,0,0,0,0,0.0,"feature request: add \H option for mysql client prompt $end$ Please add a \H option for the mysql client prompt, as discussed at http://blog.wl0.org/2009/08/mysql-hostname-prompt-when-host-is-localhost/

This should show the value of the server's ""hostname"" variable in the prompt of the client connected to the server. $acceptance criteria:$",0,0,0,0,0,0,1,28064.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
732,MDEV-5535,Task,MDEV,2014-01-16 20:04:18,,0,cannot reopen temporary table,"It is a well-known and very old MySQL/MariaDB limitation that temporary tables can only be used once in any query; for example, one cannot join a temporary table to itself. This task is about removing this limitation.

*original bug report:*
{quote}
if I {{create temporary table a (id int);}} I cant {{select a1.* from a a1, a a2;}}

I understand, that http://bugs.mysql.com/bug.php?id=10327 is in ""feature"" state for 8(!!!) years.
But may be at least at mariadb someone to fix it.
It is not so extraordinary type of select.
I had bumped at almost all kind of problems about this, which are described about this bug. And any kind of rewrite selects/logic to avoid this error is a huge performance and logic issuse at hiload projects.
Can anyone tell me if it would be fixed? when? where?
Because I cant find any mention about it in mariadb.
Sorry if I missed something.
{quote}",,"cannot reopen temporary table $end$ It is a well-known and very old MySQL/MariaDB limitation that temporary tables can only be used once in any query; for example, one cannot join a temporary table to itself. This task is about removing this limitation.

*original bug report:*
{quote}
if I {{create temporary table a (id int);}} I cant {{select a1.* from a a1, a a2;}}

I understand, that http://bugs.mysql.com/bug.php?id=10327 is in ""feature"" state for 8(!!!) years.
But may be at least at mariadb someone to fix it.
It is not so extraordinary type of select.
I had bumped at almost all kind of problems about this, which are described about this bug. And any kind of rewrite selects/logic to avoid this error is a huge performance and logic issuse at hiload projects.
Can anyone tell me if it would be fixed? when? where?
Because I cant find any mention about it in mariadb.
Sorry if I missed something.
{quote} $acceptance criteria:$",,tem,tem,Major,56,,3,14,6,13,0,3,0,,0,850,6,2,0,2015-11-18 10:00:44,cant reopen temporary table,"It is a well-known and very old MySQL/MariaDB limitation that temporary tables can only be used once in any query; for example, one cannot join a temporary table to itself. This task is about removing this limitation.

*original bug report:*
{quote}
if I {{create temporary table a (id int);}} I cant {{select a1.* from a a1, a a2;}}

I understand, that http://bugs.mysql.com/bug.php?id=10327 is in ""feature"" state for 8(!!!) years.
But may be at least at mariadb someone to fix it.
It is not so extraordinary type of select.
I had bumped at almost all kind of problems about this, which are described about this bug. And any kind of rewrite selects/logic to avoid this error is a huge performance and logic issuse at hiload projects.
Can anyone tell me if it would be fixed? when? where?
Because I cant find any mention about it in mariadb.
Sorry if I missed something.
{quote}",,1,0,0,2,0.00628931,"cant reopen temporary table $end$ It is a well-known and very old MySQL/MariaDB limitation that temporary tables can only be used once in any query; for example, one cannot join a temporary table to itself. This task is about removing this limitation.

*original bug report:*
{quote}
if I {{create temporary table a (id int);}} I cant {{select a1.* from a a1, a a2;}}

I understand, that http://bugs.mysql.com/bug.php?id=10327 is in ""feature"" state for 8(!!!) years.
But may be at least at mariadb someone to fix it.
It is not so extraordinary type of select.
I had bumped at almost all kind of problems about this, which are described about this bug. And any kind of rewrite selects/logic to avoid this error is a huge performance and logic issuse at hiload projects.
Can anyone tell me if it would be fixed? when? where?
Because I cant find any mention about it in mariadb.
Sorry if I missed something.
{quote} $acceptance criteria:$",1,1,0,0,0,0,1,16093.9,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
733,MDEV-5536,Task,MDEV,2014-01-17 11:26:57,,0,Support systemd socket activation,"Supporting socket activation would make each of the following possible for admins:
 * Cleaner restarts (the listener socket stays open persistently)
 * Network namespace isolation, disallowing any network access beyond the inherited listener port (and connections accepted from it).
 * Lazy startup for densely hosted instances. (It's also possible with socket activation to start it eagerly, as usual.)
 * Running MariaDB on privileged ports without having to start it initially as root
 * Non-racy startup for services (like a PHP site) that depend on connecting to MariaDB. Because systemd opens listener sockets early in boot, they're available even while MariaDB is starting
 * Deeper integration into coming network support in future systemd releases

Some examples in C are here:
http://0pointer.de/blog/projects/socket-activation.html

I am willing to sponsor development of this feature.",,"Support systemd socket activation $end$ Supporting socket activation would make each of the following possible for admins:
 * Cleaner restarts (the listener socket stays open persistently)
 * Network namespace isolation, disallowing any network access beyond the inherited listener port (and connections accepted from it).
 * Lazy startup for densely hosted instances. (It's also possible with socket activation to start it eagerly, as usual.)
 * Running MariaDB on privileged ports without having to start it initially as root
 * Non-racy startup for services (like a PHP site) that depend on connecting to MariaDB. Because systemd opens listener sockets early in boot, they're available even while MariaDB is starting
 * Deeper integration into coming network support in future systemd releases

Some examples in C are here:
http://0pointer.de/blog/projects/socket-activation.html

I am willing to sponsor development of this feature. $acceptance criteria:$",,David Strauss,David Strauss,Minor,41,,1,31,7,4,0,0,0,,0,850,10,0,0,2015-06-09 16:57:52,Support systemd socket activation,"Supporting socket activation would make each of the following possible for admins:
 * Cleaner restarts (the listener socket stays open persistently)
 * Network namespace isolation, disallowing any network access beyond the inherited listener port (and connections accepted from it).
 * Lazy startup for densely hosted instances. (It's also possible with socket activation to start it eagerly, as usual.)
 * Running MariaDB on privileged ports without having to start it initially as root
 * Non-racy startup for services (like a PHP site) that depend on connecting to MariaDB. Because systemd opens listener sockets early in boot, they're available even while MariaDB is starting
 * Deeper integration into coming network support in future systemd releases

Some examples in C are here:
http://0pointer.de/blog/projects/socket-activation.html

I am willing to sponsor development of this feature.",,0,0,0,0,0.0,"Support systemd socket activation $end$ Supporting socket activation would make each of the following possible for admins:
 * Cleaner restarts (the listener socket stays open persistently)
 * Network namespace isolation, disallowing any network access beyond the inherited listener port (and connections accepted from it).
 * Lazy startup for densely hosted instances. (It's also possible with socket activation to start it eagerly, as usual.)
 * Running MariaDB on privileged ports without having to start it initially as root
 * Non-racy startup for services (like a PHP site) that depend on connecting to MariaDB. Because systemd opens listener sockets early in boot, they're available even while MariaDB is starting
 * Deeper integration into coming network support in future systemd releases

Some examples in C are here:
http://0pointer.de/blog/projects/socket-activation.html

I am willing to sponsor development of this feature. $acceptance criteria:$",0,0,0,0,0,0,1,12197.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
734,MDEV-5713,Task,MDEV,2014-02-21 10:34:55,,0,RFE: Add support for systemd notify feature,"Originally reported as http://bugs.mysql.com/bug.php?id=65809

Since systemd tries to start services parallel, it's not easy to ensure that mysql daemon is started before another service, which requires it. Currently, we have to use an arbitrary script, that checks if the daemon is ready.

However, systemd has a feature to notify daemon about a service status, which would help in this scenario a lot and packagers wouldn't need any additional scripts to test daemon status. MySQL daemon can simple send a message to systemd daemon, that startup actions have been done successfully and we are ready to accept connections.

Using this feature is really straightforward and doesn't make any issues if a user uses alternative init system. To be concrete, only one function call and some configuration checking is needed.

Please, consider adopting the following patch attached.",,"RFE: Add support for systemd notify feature $end$ Originally reported as http://bugs.mysql.com/bug.php?id=65809

Since systemd tries to start services parallel, it's not easy to ensure that mysql daemon is started before another service, which requires it. Currently, we have to use an arbitrary script, that checks if the daemon is ready.

However, systemd has a feature to notify daemon about a service status, which would help in this scenario a lot and packagers wouldn't need any additional scripts to test daemon status. MySQL daemon can simple send a message to systemd daemon, that startup actions have been done successfully and we are ready to accept connections.

Using this feature is really straightforward and doesn't make any issues if a user uses alternative init system. To be concrete, only one function call and some configuration checking is needed.

Please, consider adopting the following patch attached. $acceptance criteria:$",,Honza Horak,Honza Horak,Major,35,,1,7,5,5,0,0,0,,0,850,0,0,0,2015-06-09 16:58:51,RFE: Add support for systemd notify feature,"Originally reported as http://bugs.mysql.com/bug.php?id=65809

Since systemd tries to start services parallel, it's not easy to ensure that mysql daemon is started before another service, which requires it. Currently, we have to use an arbitrary script, that checks if the daemon is ready.

However, systemd has a feature to notify daemon about a service status, which would help in this scenario a lot and packagers wouldn't need any additional scripts to test daemon status. MySQL daemon can simple send a message to systemd daemon, that startup actions have been done successfully and we are ready to accept connections.

Using this feature is really straightforward and doesn't make any issues if a user uses alternative init system. To be concrete, only one function call and some configuration checking is needed.

Please, consider adopting the following patch attached.",,0,0,0,0,0.0,"RFE: Add support for systemd notify feature $end$ Originally reported as http://bugs.mysql.com/bug.php?id=65809

Since systemd tries to start services parallel, it's not easy to ensure that mysql daemon is started before another service, which requires it. Currently, we have to use an arbitrary script, that checks if the daemon is ready.

However, systemd has a feature to notify daemon about a service status, which would help in this scenario a lot and packagers wouldn't need any additional scripts to test daemon status. MySQL daemon can simple send a message to systemd daemon, that startup actions have been done successfully and we are ready to accept connections.

Using this feature is really straightforward and doesn't make any issues if a user uses alternative init system. To be concrete, only one function call and some configuration checking is needed.

Please, consider adopting the following patch attached. $acceptance criteria:$",0,0,0,0,0,0,1,11358.4,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
735,MDEV-5800,Task,MDEV,2014-03-06 00:25:51,MDEV-10872,0,indexes on virtual (not materialized) columns,"Currently to have a index on a virtual column, one has to materialized it. To support indexes on fully virtual columns, a storage engine must call back into the server to calculate the value of the virtual column.",,"indexes on virtual (not materialized) columns $end$ Currently to have a index on a virtual column, one has to materialized it. To support indexes on fully virtual columns, a storage engine must call back into the server to calculate the value of the virtual column. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,58,,51,3,57,14,0,0,0,,0,850,0,0,0,2015-11-17 17:53:53,indexes on virtual (not materialized) columns,"Currently to have a index on a virtual column, one has to materialized it. To support indexes on fully virtual columns, a storage engine must call back into the server to calculate the value of the virtual column.",,0,0,0,0,0.0,"indexes on virtual (not materialized) columns $end$ Currently to have a index on a virtual column, one has to materialized it. To support indexes on fully virtual columns, a storage engine must call back into the server to calculate the value of the virtual column. $acceptance criteria:$",0,0,0,0,0,0,1,14921.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
736,MDEV-6066,Task,MDEV,2014-04-10 10:26:30,,0,Merge new defaults from 5.6 and 5.7,Please merge new server defaults as described here: http://dev.mysql.com/doc/refman/5.6/en/server-default-changes.html,,Merge new defaults from 5.6 and 5.7 $end$ Please merge new server defaults as described here: http://dev.mysql.com/doc/refman/5.6/en/server-default-changes.html $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,32,,0,6,1,4,0,1,0,,0,850,6,1,0,2015-06-17 11:59:50,Merge new defaults from 5.6 and 5.7,Please merge new server defaults as described here: http://dev.mysql.com/doc/refman/5.6/en/server-default-changes.html,,0,0,0,0,0.0,Merge new defaults from 5.6 and 5.7 $end$ Please merge new server defaults as described here: http://dev.mysql.com/doc/refman/5.6/en/server-default-changes.html $acceptance criteria:$,0,0,0,0,0,0,1,10393.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
737,MDEV-6076,Task,MDEV,2014-04-11 16:26:42,,0,Persistent AUTO_INCREMENT for InnoDB,"The current auto_increment behavior in the InnoDB engine is sub-optimal. As it currently functions, the auto_increment value is stored until the server shuts down or resets, and then is rebuilt based on values in the table when it starts up again. Furthermore, in 5.6 this  ought to become even worse, because tables can be evicted from the InnoDB data dictionary cache. We may get a too low auto-increment value even without shutdown/restart. When a table is evicted, InnoDB will forget the current auto-increment value, and it will do SELECT MAX(auto_inc_column) next time when the table is accessed.


",,"Persistent AUTO_INCREMENT for InnoDB $end$ The current auto_increment behavior in the InnoDB engine is sub-optimal. As it currently functions, the auto_increment value is stored until the server shuts down or resets, and then is rebuilt based on values in the table when it starts up again. Furthermore, in 5.6 this  ought to become even worse, because tables can be evicted from the InnoDB data dictionary cache. We may get a too low auto-increment value even without shutdown/restart. When a table is evicted, InnoDB will forget the current auto-increment value, and it will do SELECT MAX(auto_inc_column) next time when the table is accessed.


 $acceptance criteria:$",,Jan Lindström,Jan Lindström,Major,31,,8,21,11,1,0,2,0,,0,850,13,1,0,2016-12-08 11:44:49,Persistent auto increment for InnoDB,"The current auto_increment behavior in the InnoDB engine is sub-optimal. As it currently functions, the auto_increment value is stored until the server shuts down or resets, and then is rebuilt based on values in the table when it starts up again. Furthermore, in 5.6 this  ought to become even worse, because tables can be evicted from the InnoDB data dictionary cache. We may get a too low auto-increment value even without shutdown/restart. When a table is evicted, InnoDB will forget the current auto-increment value, and it will do SELECT MAX(auto_inc_column) next time when the table is accessed.


",,1,0,0,3,0.0190476,"Persistent auto increment for InnoDB $end$ The current auto_increment behavior in the InnoDB engine is sub-optimal. As it currently functions, the auto_increment value is stored until the server shuts down or resets, and then is rebuilt based on values in the table when it starts up again. Furthermore, in 5.6 this  ought to become even worse, because tables can be evicted from the InnoDB data dictionary cache. We may get a too low auto-increment value even without shutdown/restart. When a table is evicted, InnoDB will forget the current auto-increment value, and it will do SELECT MAX(auto_inc_column) next time when the table is accessed.


 $acceptance criteria:$",1,1,0,0,0,0,0,23323.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
738,MDEV-6080,Task,MDEV,2014-04-11 22:46:15,,0,Allowing storage engine to shortcut group by queries,"This task is to allow storage engines that can execute GROUP BY
queries efficiently to intercept a full query or sub query from
MariaDB and deliver the result either to the client or to a temporary
table for further processing.

The interface is using a new 'group_by_handler' class. The new class
is needed as the original query may contain multiple tables and the
'result row' can contain fields from different tables.

h3. Overview

During prepare, call the storage engine handlerton to ask if the storage engine can execute the group by query.
If yes:
  - The handlerton returns a group_by_handler object.
  - Create a temporary table to store result rows.
  - Initialize the group_by_handler with the temporary table and other relevant objects
  - When doing 'optimize', don't optimize join order (not needed)
  
When {{do_select()}} is called, if we have a group_by_handler object, the following is done, instead of the normal procedure of reading things rows by row and joining tables:

   - Initialize group_by_handler
   - While {{get_next_row()}}, returns false:
      -- Depending on context, write {{temporary_table->record\[0]}} to the temporary table or return it to the next level (normally the end user).
   - finish group_by_handler

Note that the above loop can be executed many times, in case of
prepared statements or sub queries.

- When cleanup up SELECT_LEX, we will free the group_by_handler object.

h3. More details

Assumptions when this interface is used:
- The SELECT is a GROUP BY or summary query.
- All tables used in SELECT comes from the same storage engine.

h3. Suggested interface

New function for the handlerton class:
{code}
  group_by_handler*
  handerton::can_intercept_group_by(THD *thd, SELECT_LEX *,
			            List<Item> &fields,
				    TABLE_LIST *, ORDER *group_by,
                                    ORDER *order_by, Item *where,
                                    Item *having);
{code}
This function should return a group_by_handler object if the storage engine can resolve the query itself.

New group_by_handler class with the following data and virtual methods:
{code}
  TABLE *temporary_table;
  Item *having;
  ORDER *order_by;

  /*
    Store pointer to temporary table and objects modified to point to
    the temporary table.  This will happen during the prepare phase.
    Return 1 if the storage handler cannot handle the GROUP BY after all,
    in which case we fall back to normal query execution.
  */
  bool init(TABLE *temporary_table, Item *having, ORDER *order_by);

  /*
    Bit's of things the storage engine can do. Should be initialized on
    object creation.
  */
  #define GROUP_BY_ORDER_BY 1  /* Result data is sorted */
  uint flags;

  bool init_scan();
  /* Return next row result in temporary_table
  bool next_row();
  bool end_scan();
{code}

If the group_by_handler can't do the sorting, MariaDB will do this.  Note that we assume that the handler can filter out things not matching HAVING (by calling {{having->val_bool()}}).

In the future we will look at doing a more abstract interface so that the storage engine doesn't have to understand the SELECT_LEX and other structures.
",,"Allowing storage engine to shortcut group by queries $end$ This task is to allow storage engines that can execute GROUP BY
queries efficiently to intercept a full query or sub query from
MariaDB and deliver the result either to the client or to a temporary
table for further processing.

The interface is using a new 'group_by_handler' class. The new class
is needed as the original query may contain multiple tables and the
'result row' can contain fields from different tables.

h3. Overview

During prepare, call the storage engine handlerton to ask if the storage engine can execute the group by query.
If yes:
  - The handlerton returns a group_by_handler object.
  - Create a temporary table to store result rows.
  - Initialize the group_by_handler with the temporary table and other relevant objects
  - When doing 'optimize', don't optimize join order (not needed)
  
When {{do_select()}} is called, if we have a group_by_handler object, the following is done, instead of the normal procedure of reading things rows by row and joining tables:

   - Initialize group_by_handler
   - While {{get_next_row()}}, returns false:
      -- Depending on context, write {{temporary_table->record\[0]}} to the temporary table or return it to the next level (normally the end user).
   - finish group_by_handler

Note that the above loop can be executed many times, in case of
prepared statements or sub queries.

- When cleanup up SELECT_LEX, we will free the group_by_handler object.

h3. More details

Assumptions when this interface is used:
- The SELECT is a GROUP BY or summary query.
- All tables used in SELECT comes from the same storage engine.

h3. Suggested interface

New function for the handlerton class:
{code}
  group_by_handler*
  handerton::can_intercept_group_by(THD *thd, SELECT_LEX *,
			            List<Item> &fields,
				    TABLE_LIST *, ORDER *group_by,
                                    ORDER *order_by, Item *where,
                                    Item *having);
{code}
This function should return a group_by_handler object if the storage engine can resolve the query itself.

New group_by_handler class with the following data and virtual methods:
{code}
  TABLE *temporary_table;
  Item *having;
  ORDER *order_by;

  /*
    Store pointer to temporary table and objects modified to point to
    the temporary table.  This will happen during the prepare phase.
    Return 1 if the storage handler cannot handle the GROUP BY after all,
    in which case we fall back to normal query execution.
  */
  bool init(TABLE *temporary_table, Item *having, ORDER *order_by);

  /*
    Bit's of things the storage engine can do. Should be initialized on
    object creation.
  */
  #define GROUP_BY_ORDER_BY 1  /* Result data is sorted */
  uint flags;

  bool init_scan();
  /* Return next row result in temporary_table
  bool next_row();
  bool end_scan();
{code}

If the group_by_handler can't do the sorting, MariaDB will do this.  Note that we assume that the handler can filter out things not matching HAVING (by calling {{having->val_bool()}}).

In the future we will look at doing a more abstract interface so that the storage engine doesn't have to understand the SELECT_LEX and other structures.
 $acceptance criteria:$",,Michael Widenius,Michael Widenius,Blocker,21,,0,2,0,3,0,2,0,,0,850,0,1,0,2015-09-09 10:51:36,Allowing storage engine to shortcut group by queries,"This task is to allow storage engines that can execute GROUP BY
queries efficiently to intercept a full query or sub query from
MariaDB and deliver the result either to the client or to a temporary
table for further processing.

The interface is using a new 'group_by_handler' class. The new class
is needed as the original query may contain multiple tables and the
'result row' can contain fields from different tables.

Overview:

- During prepare, call the storage engine handlerton to ask if the storage engine can execute the group by query.
- If yes:
  - The handlerton returns a group_by_handler object.
  - Create a temporary table to store result rows.
  - Initialize the group_by_handler with the temporary table and other relevant objects-
  - When doing 'optimize', don't optimize join order (not needed)
  
   When {{do_select()}} is called, if we have a group_by_handler object,
   the following is done, instead of the normal procedure of reading
   things rows by row and joining tables:

   - Initialize group_by_handler
   - While {{get_next_row()}}, returns false:
      -- Depending on context, write {{temporary_table->record\[0]}} to the
        temporary table or return it to the next level (normally the end user).
   - finish group_by_handler

   Note that the above loop can be executed many times, in case of
   prepared statements or sub queries.

- When cleanup up SELECT_LEX, we will free the group_by_handler object.


More details:

Assumptions when this interface is used:
- The SELECT is a GROUP BY or summary query.
- All tables used in SELECT comes from the same storage engine.

Suggested interface:

- New function for the handlerton class:
{code}
  group_by_handler*
  handerton::can_intercept_group_by(THD *thd, SELECT_LEX *,
			            List<Item> &fields,
				    TABLE_LIST *, ORDER *group_by,
                                    ORDER *order_by, Item *where,
                                    Item *having);
{code}
  
This function should return a group_by_handler object if the storage engine can resolve the query itself.

- New group_by_handler class with the following data and virtual methods:
{code}
  TABLE *temporary_table;
  Item *having;
  ORDER *order_by;

  /*
    Store pointer to temporary table and objects modified to point to
    the temporary table.  This will happen during the prepare phase.
    Return 1 if the storage handler cannot handle the GROUP BY after all,
    in which case we fall back to normal query execution.
  */
  bool init(TABLE *temporary_table, Item *having, ORDER *order_by);

  /*
    Bit's of things the storage engine can do. Should be initialized on
    object creation.
  */
  #define GROUP_BY_ORDER_BY 1  /* Result data is sorted */
  uint flags;

  bool init_scan();
  /* Return next row result in temporary_table
  bool next_row();
  bool end_scan();
{code}

If the group_by_handler can't do the sorting, MariaDB will do this.
Note that we assume that the handler can filter out things not matching
HAVING (by calling {{having->val_bool()}}).

In the future we will look at doing a more abstract interface so that the storage engine doesn't have to understand the SELECT_LEX and other structures.
",,0,1,0,15,0.0211416,"Allowing storage engine to shortcut group by queries $end$ This task is to allow storage engines that can execute GROUP BY
queries efficiently to intercept a full query or sub query from
MariaDB and deliver the result either to the client or to a temporary
table for further processing.

The interface is using a new 'group_by_handler' class. The new class
is needed as the original query may contain multiple tables and the
'result row' can contain fields from different tables.

Overview:

- During prepare, call the storage engine handlerton to ask if the storage engine can execute the group by query.
- If yes:
  - The handlerton returns a group_by_handler object.
  - Create a temporary table to store result rows.
  - Initialize the group_by_handler with the temporary table and other relevant objects-
  - When doing 'optimize', don't optimize join order (not needed)
  
   When {{do_select()}} is called, if we have a group_by_handler object,
   the following is done, instead of the normal procedure of reading
   things rows by row and joining tables:

   - Initialize group_by_handler
   - While {{get_next_row()}}, returns false:
      -- Depending on context, write {{temporary_table->record\[0]}} to the
        temporary table or return it to the next level (normally the end user).
   - finish group_by_handler

   Note that the above loop can be executed many times, in case of
   prepared statements or sub queries.

- When cleanup up SELECT_LEX, we will free the group_by_handler object.


More details:

Assumptions when this interface is used:
- The SELECT is a GROUP BY or summary query.
- All tables used in SELECT comes from the same storage engine.

Suggested interface:

- New function for the handlerton class:
{code}
  group_by_handler*
  handerton::can_intercept_group_by(THD *thd, SELECT_LEX *,
			            List<Item> &fields,
				    TABLE_LIST *, ORDER *group_by,
                                    ORDER *order_by, Item *where,
                                    Item *having);
{code}
  
This function should return a group_by_handler object if the storage engine can resolve the query itself.

- New group_by_handler class with the following data and virtual methods:
{code}
  TABLE *temporary_table;
  Item *having;
  ORDER *order_by;

  /*
    Store pointer to temporary table and objects modified to point to
    the temporary table.  This will happen during the prepare phase.
    Return 1 if the storage handler cannot handle the GROUP BY after all,
    in which case we fall back to normal query execution.
  */
  bool init(TABLE *temporary_table, Item *having, ORDER *order_by);

  /*
    Bit's of things the storage engine can do. Should be initialized on
    object creation.
  */
  #define GROUP_BY_ORDER_BY 1  /* Result data is sorted */
  uint flags;

  bool init_scan();
  /* Return next row result in temporary_table
  bool next_row();
  bool end_scan();
{code}

If the group_by_handler can't do the sorting, MariaDB will do this.
Note that we assume that the handler can filter out things not matching
HAVING (by calling {{having->val_bool()}}).

In the future we will look at doing a more abstract interface so that the storage engine doesn't have to understand the SELECT_LEX and other structures.
 $acceptance criteria:$",1,1,1,1,1,0,1,12372.1,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
739,MDEV-6112,Task,MDEV,2014-04-15 20:15:44,MDEV-10872,0,multiple triggers per table,"Many triggers for the same event per table (e.g. many BEFORE DELETE triggerts).
This can be ported from 5.7

Looked at the MySQL 5.7 implementation. Too complex and too many not needed changed and moving of things to different files that make it very hard to follow code changes over time.

Will do this with new code, but will take test cases from MySQL 5.7

New functionality:
- ""Any"" amount of same events
- New syntax with { FOLLOWS | PRECEDES } trigger_name

CREATE [OR REPLACE]
    [DEFINER = { user | CURRENT_USER }]
    TRIGGER [IF NOT EXISTS] trigger_name trigger_time trigger_event
    ON tbl_name FOR EACH ROW 
    [{ FOLLOWS | PRECEDES } other_trigger_name ]
    trigger_stmt
",,"multiple triggers per table $end$ Many triggers for the same event per table (e.g. many BEFORE DELETE triggerts).
This can be ported from 5.7

Looked at the MySQL 5.7 implementation. Too complex and too many not needed changed and moving of things to different files that make it very hard to follow code changes over time.

Will do this with new code, but will take test cases from MySQL 5.7

New functionality:
- ""Any"" amount of same events
- New syntax with { FOLLOWS | PRECEDES } trigger_name

CREATE [OR REPLACE]
    [DEFINER = { user | CURRENT_USER }]
    TRIGGER [IF NOT EXISTS] trigger_name trigger_time trigger_event
    ON tbl_name FOR EACH ROW 
    [{ FOLLOWS | PRECEDES } other_trigger_name ]
    trigger_stmt
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,24,,6,6,8,3,0,3,0,,0,850,6,0,0,2016-08-31 18:06:21,multiple triggers per table,"Many triggers for the same event per table (e.g. many BEFORE DELETE triggerts).
This can be ported from 5.7",,0,3,0,94,3.61538,"multiple triggers per table $end$ Many triggers for the same event per table (e.g. many BEFORE DELETE triggerts).
This can be ported from 5.7 $acceptance criteria:$",3,1,1,1,1,1,1,20853.8,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
740,MDEV-6113,Task,MDEV,2014-04-15 20:32:28,,0,merge 5.7 innodb,"merge 5.7 innodb and xtradb, preferably both at the same time.",,"merge 5.7 innodb $end$ merge 5.7 innodb and xtradb, preferably both at the same time. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,39,,4,10,15,9,0,1,0,,0,850,7,0,0,2015-11-17 17:51:08,merge 5.7 innodb and xtradb,"merge 5.7 innodb and xtradb, preferably both at the same time.",,1,0,0,2,0.105263,"merge 5.7 innodb and xtradb $end$ merge 5.7 innodb and xtradb, preferably both at the same time. $acceptance criteria:$",1,1,0,0,0,0,1,13941.3,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
741,MDEV-6114,Task,MDEV,2014-04-15 20:33:01,,0,merge new P_S instrumentation and tables from 5.7,"Meta task to merge new P_S instrumentation and tables from 5.7

See linked tasks. After they're all closed, one still needs to compare tables and their structures with 5.7 and to implement whatever was not covered by linked tasks.

Done and released with MariaDB 10.5.2 everything except:
- sysvars instrumentation
- replication instrumentation
- group replication Instrumentation
",,"merge new P_S instrumentation and tables from 5.7 $end$ Meta task to merge new P_S instrumentation and tables from 5.7

See linked tasks. After they're all closed, one still needs to compare tables and their structures with 5.7 and to implement whatever was not covered by linked tasks.

Done and released with MariaDB 10.5.2 everything except:
- sysvars instrumentation
- replication instrumentation
- group replication Instrumentation
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,58,,1,5,17,3,0,6,0,,0,850,1,1,0,2016-08-24 09:58:46,merge new P_S instrumentation and tables from 5.7,merge new P_S instrumentation and tables from 5.7,,0,5,0,49,2.57895,merge new P_S instrumentation and tables from 5.7 $end$ merge new P_S instrumentation and tables from 5.7 $acceptance criteria:$,5,1,1,1,1,1,1,20677.4,3,2,0.666667,1,0.333333,1,0.333333,1,0.333333,1,0.333333
742,MDEV-6145,Task,MDEV,2014-04-21 16:38:07,,0,MariaDB 10 packages for Ubuntu 12.04 do not include TokuDB,The 12.04 MariaDB 10 packages do not currently include TokuDB.,,MariaDB 10 packages for Ubuntu 12.04 do not include TokuDB $end$ The 12.04 MariaDB 10 packages do not currently include TokuDB. $acceptance criteria:$,,Tim Callaghan,Tim Callaghan,Major,8,,0,1,0,1,0,0,0,,0,850,0,0,0,2014-05-27 14:43:09,MariaDB 10 packages for Ubuntu 12.04 do not include TokuDB,The 12.04 MariaDB 10 packages do not currently include TokuDB.,,0,0,0,0,0.0,MariaDB 10 packages for Ubuntu 12.04 do not include TokuDB $end$ The 12.04 MariaDB 10 packages do not currently include TokuDB. $acceptance criteria:$,0,0,0,0,0,0,0,862.083,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
743,MDEV-6150,Task,MDEV,2014-04-22 12:41:48,,0,Speed up connection speed by moving creation of THD to new thread,"One major bottleneck for creating a new connection is that THD is created in the thread that is handing all connections.

The fix for this is to move THD creation from the connection thread to the new thread that will handle queries.
",,"Speed up connection speed by moving creation of THD to new thread $end$ One major bottleneck for creating a new connection is that THD is created in the thread that is handing all connections.

The fix for this is to move THD creation from the connection thread to the new thread that will handle queries.
 $acceptance criteria:$",,Michael Widenius,Michael Widenius,Major,20,,0,5,1,1,0,0,0,,0,850,2,0,0,2015-07-09 16:10:00,Speed up connection speed by moving creation of THD to new thread,"One major bottleneck for creating a new connection is that THD is created in the thread that is handing all connections.

The fix for this is to move THD creation from the connection thread to the new thread that will handle queries.
",,0,0,0,0,0.0,"Speed up connection speed by moving creation of THD to new thread $end$ One major bottleneck for creating a new connection is that THD is created in the thread that is handing all connections.

The fix for this is to move THD creation from the connection thread to the new thread that will handle queries.
 $acceptance criteria:$",0,0,0,0,0,0,0,10635.5,1,1,1.0,1,1.0,1,1.0,1,1.0,0,0.0
744,MDEV-6152,Task,MDEV,2014-04-22 13:04:16,MDEV-7941,0,Remove calls to current_thd while creating Item,"current_thd() is called many times while creating Items (in many cases 2 or more calls per item).

The problem is not only the number of calls to current_thd(), but also that when we create items we cause more calls, like calling sql_alloc() that calls current_thd and then uses this to call alloc_root() with the right argumetns.

By adding thd as an argument to all functions that creates Items, we can avoid most calls to current_thd(). We can also call alloc_root() directly instead of calling it trough sql_alloc().

Changes in the code as part of this task:
- Add THD as an argument to all functions that creates an Item.
- Replace 'new Item_xxx' with 'new (thd->memroot) Item_xxx
- Add thd->memroot as an argument to push_back() and push_front() for lists.

Before adding this to 10.1 code base, we need to benchmark the result to see that this big change is really worth it.",,"Remove calls to current_thd while creating Item $end$ current_thd() is called many times while creating Items (in many cases 2 or more calls per item).

The problem is not only the number of calls to current_thd(), but also that when we create items we cause more calls, like calling sql_alloc() that calls current_thd and then uses this to call alloc_root() with the right argumetns.

By adding thd as an argument to all functions that creates Items, we can avoid most calls to current_thd(). We can also call alloc_root() directly instead of calling it trough sql_alloc().

Changes in the code as part of this task:
- Add THD as an argument to all functions that creates an Item.
- Replace 'new Item_xxx' with 'new (thd->memroot) Item_xxx
- Add thd->memroot as an argument to push_back() and push_front() for lists.

Before adding this to 10.1 code base, we need to benchmark the result to see that this big change is really worth it. $acceptance criteria:$",,Michael Widenius,Michael Widenius,Minor,12,,0,1,1,1,0,1,0,,0,850,1,1,0,2015-08-19 17:25:17,Remove calls to current_thd while creating Item,"current_thd() is called many times while creating Items (in many cases 2 or more calls per item).

The problem is not only the number of calls to current_thd(), but also that when we create items we cause more calls, like calling sql_alloc() that calls current_thd and then uses this to call alloc_root() with the right argumetns.

By adding thd as an argument to all functions that creates Items, we can avoid most calls to current_thd(). We can also call alloc_root() directly instead of calling it trough sql_alloc().

Changes in the code as part of this task:
- Add THD as an argument to all functions that creates an Item.
- Replace 'new Item_xxx' with 'new (thd->memroot) Item_xxx
- Add thd->memroot as an argument to push_back() and push_front() for lists.

Before adding this to 10.1 code base, we need to benchmark the result to see that this big change is really worth it.",,0,0,0,0,0.0,"Remove calls to current_thd while creating Item $end$ current_thd() is called many times while creating Items (in many cases 2 or more calls per item).

The problem is not only the number of calls to current_thd(), but also that when we create items we cause more calls, like calling sql_alloc() that calls current_thd and then uses this to call alloc_root() with the right argumetns.

By adding thd as an argument to all functions that creates Items, we can avoid most calls to current_thd(). We can also call alloc_root() directly instead of calling it trough sql_alloc().

Changes in the code as part of this task:
- Add THD as an argument to all functions that creates an Item.
- Replace 'new Item_xxx' with 'new (thd->memroot) Item_xxx
- Add thd->memroot as an argument to push_back() and push_front() for lists.

Before adding this to 10.1 code base, we need to benchmark the result to see that this big change is really worth it. $acceptance criteria:$",0,0,0,0,0,0,0,11620.3,2,1,0.5,1,0.5,1,0.5,1,0.5,0,0.0
745,MDEV-6271,Task,MDEV,2014-05-26 14:58:36,,0,update MSI installer to include latest Version of HeidiSQL (8.3.x.x),"Hello,

please include the latest Version of HeidiSQL (currently 8.3.0.4694)
in the MSI package. Thank you!

Best regards.",,"update MSI installer to include latest Version of HeidiSQL (8.3.x.x) $end$ Hello,

please include the latest Version of HeidiSQL (currently 8.3.0.4694)
in the MSI package. Thank you!

Best regards. $acceptance criteria:$",,Uwe Beierlein,Uwe Beierlein,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2014-05-27 14:44:23,update MSI installer to include latest Version of HeidiSQL (8.3.x.x),"Hello,

please include the latest Version of HeidiSQL (currently 8.3.0.4694)
in the MSI package. Thank you!

Best regards.",,0,0,0,0,0.0,"update MSI installer to include latest Version of HeidiSQL (8.3.x.x) $end$ Hello,

please include the latest Version of HeidiSQL (currently 8.3.0.4694)
in the MSI package. Thank you!

Best regards. $acceptance criteria:$",0,0,0,0,0,0,0,23.75,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
746,MDEV-6284,Task,MDEV,2014-05-30 21:56:04,,0,Merge downstream Debian/Ubuntu packaging into upstream MariaDB,"== Motivation ==

Merge downstream .deb-packaging into upstream MariaDB to unify, streamline and ease future release packaging. This will also fix some packaging errors in current MariaDB.org deb-packages.

== Patch description ==

List of notable changes:
* Completely new Debian packaging standard (3.0 quilt) including rules file, patch conventions and .files -> .install file listing conventions
* The will no longer be a debian/dist subfolder and the packaging of Debian an Ubuntu will be unified (= Debian will ship with AppArmor files included)
* Support for Debian releases before Wheezy (7) and Ubuntu releases before Precise (12.04) will be dropped

The upstream MariaDB contents of debian/* will not be fully identical to the downstream Debian/Ubuntu one.

List of intended differences between Debian master and MariaDB master regarding debian/ contents:
* control file: maintainer, uploaders, VCS and browser links
* omitted packages (not allowed in Debian): libmariadbclient18, libmysqlclient18, libmariadbclient-dev, mysql-common
* omitted plugins (problematic in Debian): Handler socket, TokuDB, Mroonga, Cassandra
* keep autobake.sh in mariadb.org repo

To close this bug please:

1) Merge selected parts of debian/* from https://anonscm.debian.org/gitweb/?p=pkg-mysql/mariadb-10.0.git to https://github.com/MariaDB/server via pull requests arriving from branch ok-debpkg at https://github.com/ottok/mariadb

2) Backport form MariaDB development head to 10.1, 10.0 and maybe even 5.5 commits that fix individual issues.

== Quality assurance ==

The new debian/* contents represents the state of the art of Debian packaging. The mariadb-5.5 and mariadb-10.0 has passed all Debian quality assurance and landed in the official Debian repositories. More details at https://wiki.debian.org/Teams/MySQL/MariaDB

These same packages have also gone downstream to Ubuntu, has passed Ubuntu QA and are included since the Ubuntu 14.04 release.

Buildbot test for ok-debpkg branch: https://buildbot.askmonty.org/buildbot/grid?branch=ok-debpkg&category=main

== Roll-out planning ==

This merge should be safe to release with 10.1. All new installs and upgrades using the MariaDB.org repo or mixing it with official Debian or Ubuntu repository contents is expected to work.

Below are the different scenarios how releasing the new packging at MariaDB.org will affect current installs/upgrades:

A) If users currently have a MariaDB.org repo enabled, those packages will have a + in their version number. Debian package manager will always consider e.g. 10.1.6+maria as more recent than 10.1.6 from the native repositories. Therefore people having MariaDB.org repositories will always get primarly the packages from MariaDB.org repositories. It is very unlikely that a Debian repository would ever have a more recent release than in MariaDB.org (e.g. 10.1.7 would override 10.1.6+maria, but that is not likely to take place as MariaDB.org always releases first).

B) The new packaging is designed and tested that it allows seamless upgrades from old deb packaging generation to new generation. Whether the new packaging is from MariaDB.org or distro repositories does not matter.

C) Backwards works only partially, but is a rare case (e.g. current Ubuntu users trying to upgrade from new style debs 10.0 to MariaDB.org repo old style debs 10.0 will see issues, e.g. MDEV-5977). As soon as this merge is done and MariaDB.org publishes packages using the new packaging this corner case is fixed.

D) If users remove the MariaDB.org from their install, they will automatically have their 10.1.6+maria updated to Debian/Ubuntu native 5.5.39 once it is released in Debian.

E) Like previously, upgrading from MySQL 5.5 to MariaDB 5.5 works seamlessly and backwards also. Updating from MariaDB 5.5 to 10.0 works seamlessly but backwards will have issues (downgrade flag, InnoDB log size mismatch etc) the package manager will not solve automatically. If users dump and import data manually, and in between completely purge and install the packages, then users can always migrate to whatever version.",,"Merge downstream Debian/Ubuntu packaging into upstream MariaDB $end$ == Motivation ==

Merge downstream .deb-packaging into upstream MariaDB to unify, streamline and ease future release packaging. This will also fix some packaging errors in current MariaDB.org deb-packages.

== Patch description ==

List of notable changes:
* Completely new Debian packaging standard (3.0 quilt) including rules file, patch conventions and .files -> .install file listing conventions
* The will no longer be a debian/dist subfolder and the packaging of Debian an Ubuntu will be unified (= Debian will ship with AppArmor files included)
* Support for Debian releases before Wheezy (7) and Ubuntu releases before Precise (12.04) will be dropped

The upstream MariaDB contents of debian/* will not be fully identical to the downstream Debian/Ubuntu one.

List of intended differences between Debian master and MariaDB master regarding debian/ contents:
* control file: maintainer, uploaders, VCS and browser links
* omitted packages (not allowed in Debian): libmariadbclient18, libmysqlclient18, libmariadbclient-dev, mysql-common
* omitted plugins (problematic in Debian): Handler socket, TokuDB, Mroonga, Cassandra
* keep autobake.sh in mariadb.org repo

To close this bug please:

1) Merge selected parts of debian/* from https://anonscm.debian.org/gitweb/?p=pkg-mysql/mariadb-10.0.git to https://github.com/MariaDB/server via pull requests arriving from branch ok-debpkg at https://github.com/ottok/mariadb

2) Backport form MariaDB development head to 10.1, 10.0 and maybe even 5.5 commits that fix individual issues.

== Quality assurance ==

The new debian/* contents represents the state of the art of Debian packaging. The mariadb-5.5 and mariadb-10.0 has passed all Debian quality assurance and landed in the official Debian repositories. More details at https://wiki.debian.org/Teams/MySQL/MariaDB

These same packages have also gone downstream to Ubuntu, has passed Ubuntu QA and are included since the Ubuntu 14.04 release.

Buildbot test for ok-debpkg branch: https://buildbot.askmonty.org/buildbot/grid?branch=ok-debpkg&category=main

== Roll-out planning ==

This merge should be safe to release with 10.1. All new installs and upgrades using the MariaDB.org repo or mixing it with official Debian or Ubuntu repository contents is expected to work.

Below are the different scenarios how releasing the new packging at MariaDB.org will affect current installs/upgrades:

A) If users currently have a MariaDB.org repo enabled, those packages will have a + in their version number. Debian package manager will always consider e.g. 10.1.6+maria as more recent than 10.1.6 from the native repositories. Therefore people having MariaDB.org repositories will always get primarly the packages from MariaDB.org repositories. It is very unlikely that a Debian repository would ever have a more recent release than in MariaDB.org (e.g. 10.1.7 would override 10.1.6+maria, but that is not likely to take place as MariaDB.org always releases first).

B) The new packaging is designed and tested that it allows seamless upgrades from old deb packaging generation to new generation. Whether the new packaging is from MariaDB.org or distro repositories does not matter.

C) Backwards works only partially, but is a rare case (e.g. current Ubuntu users trying to upgrade from new style debs 10.0 to MariaDB.org repo old style debs 10.0 will see issues, e.g. MDEV-5977). As soon as this merge is done and MariaDB.org publishes packages using the new packaging this corner case is fixed.

D) If users remove the MariaDB.org from their install, they will automatically have their 10.1.6+maria updated to Debian/Ubuntu native 5.5.39 once it is released in Debian.

E) Like previously, upgrading from MySQL 5.5 to MariaDB 5.5 works seamlessly and backwards also. Updating from MariaDB 5.5 to 10.0 works seamlessly but backwards will have issues (downgrade flag, InnoDB log size mismatch etc) the package manager will not solve automatically. If users dump and import data manually, and in between completely purge and install the packages, then users can always migrate to whatever version. $acceptance criteria:$",,Otto Kekäläinen,Otto Kekäläinen,Major,46,,26,26,32,1,0,5,0,,0,850,16,3,0,2016-08-31 07:43:58,Merge downstream Debian/Ubuntu packaging into upstream MariaDB,"== Motivation ==

Merge downstream .deb-packaging into upstream MariaDB to unify, streamline and ease future release packaging. This will also fix some packaging errors in current MariaDB.org deb-packages.

== Patch description ==

List of notable changes:
* Completely new Debian packaging standard (3.0 quilt) including rules file, patch conventions and .files -> .install file listing conventions
* The will no longer be a debian/dist subfolder and the packaging of Debian an Ubuntu will be unified (= Debian will ship with AppArmor files included)
* Support for Debian releases before Wheezy (7) and Ubuntu releases before Precise (12.04) will be dropped

The upstream MariaDB contents of debian/* will not be fully identical to the downstream Debian/Ubuntu one.

List of intended differences between Debian master and MariaDB master regarding debian/ contents:
* control file: maintainer, uploaders, VCS and browser links
* omitted packages (not allowed in Debian): libmariadbclient18, libmysqlclient18, libmariadbclient-dev, mysql-common
* omitted plugins (problematic in Debian): Handler socket, TokuDB, Mroonga, Cassandra
* keep autobake.sh in mariadb.org repo

To close this bug please:

1) Merge selected parts of debian/* from https://anonscm.debian.org/gitweb/?p=pkg-mysql/mariadb-10.0.git to https://github.com/MariaDB/server via pull requests arriving from branch ok-debpkg at https://github.com/ottok/mariadb

2) Backport form MariaDB development head to 10.1, 10.0 and maybe even 5.5 commits that fix individual issues.

== Quality assurance ==

The new debian/* contents represents the state of the art of Debian packaging. The mariadb-5.5 and mariadb-10.0 has passed all Debian quality assurance and landed in the official Debian repositories. More details at https://wiki.debian.org/Teams/MySQL/MariaDB

These same packages have also gone downstream to Ubuntu, has passed Ubuntu QA and are included since the Ubuntu 14.04 release.

Buildbot test for ok-debpkg branch: https://buildbot.askmonty.org/buildbot/grid?branch=ok-debpkg&category=main

== Roll-out planning ==

This merge should be safe to release with 10.1. All new installs and upgrades using the MariaDB.org repo or mixing it with official Debian or Ubuntu repository contents is expected to work.

Below are the different scenarios how releasing the new packging at MariaDB.org will affect current installs/upgrades:

A) If users currently have a MariaDB.org repo enabled, those packages will have a + in their version number. Debian package manager will always consider e.g. 10.1.6+maria as more recent than 10.1.6 from the native repositories. Therefore people having MariaDB.org repositories will always get primarly the packages from MariaDB.org repositories. It is very unlikely that a Debian repository would ever have a more recent release than in MariaDB.org (e.g. 10.1.7 would override 10.1.6+maria, but that is not likely to take place as MariaDB.org always releases first).

B) The new packaging is designed and tested that it allows seamless upgrades from old deb packaging generation to new generation. Whether the new packaging is from MariaDB.org or distro repositories does not matter.

C) Backwards works only partially, but is a rare case (e.g. current Ubuntu users trying to upgrade from new style debs 10.0 to MariaDB.org repo old style debs 10.0 will see issues, e.g. MDEV-5977). As soon as this merge is done and MariaDB.org publishes packages using the new packaging this corner case is fixed.

D) If users remove the MariaDB.org from their install, they will automatically have their 10.1.6+maria updated to Debian/Ubuntu native 5.5.39 once it is released in Debian.

E) Like previously, upgrading from MySQL 5.5 to MariaDB 5.5 works seamlessly and backwards also. Updating from MariaDB 5.5 to 10.0 works seamlessly but backwards will have issues (downgrade flag, InnoDB log size mismatch etc) the package manager will not solve automatically. If users dump and import data manually, and in between completely purge and install the packages, then users can always migrate to whatever version.",,0,2,0,0,0.0,"Merge downstream Debian/Ubuntu packaging into upstream MariaDB $end$ == Motivation ==

Merge downstream .deb-packaging into upstream MariaDB to unify, streamline and ease future release packaging. This will also fix some packaging errors in current MariaDB.org deb-packages.

== Patch description ==

List of notable changes:
* Completely new Debian packaging standard (3.0 quilt) including rules file, patch conventions and .files -> .install file listing conventions
* The will no longer be a debian/dist subfolder and the packaging of Debian an Ubuntu will be unified (= Debian will ship with AppArmor files included)
* Support for Debian releases before Wheezy (7) and Ubuntu releases before Precise (12.04) will be dropped

The upstream MariaDB contents of debian/* will not be fully identical to the downstream Debian/Ubuntu one.

List of intended differences between Debian master and MariaDB master regarding debian/ contents:
* control file: maintainer, uploaders, VCS and browser links
* omitted packages (not allowed in Debian): libmariadbclient18, libmysqlclient18, libmariadbclient-dev, mysql-common
* omitted plugins (problematic in Debian): Handler socket, TokuDB, Mroonga, Cassandra
* keep autobake.sh in mariadb.org repo

To close this bug please:

1) Merge selected parts of debian/* from https://anonscm.debian.org/gitweb/?p=pkg-mysql/mariadb-10.0.git to https://github.com/MariaDB/server via pull requests arriving from branch ok-debpkg at https://github.com/ottok/mariadb

2) Backport form MariaDB development head to 10.1, 10.0 and maybe even 5.5 commits that fix individual issues.

== Quality assurance ==

The new debian/* contents represents the state of the art of Debian packaging. The mariadb-5.5 and mariadb-10.0 has passed all Debian quality assurance and landed in the official Debian repositories. More details at https://wiki.debian.org/Teams/MySQL/MariaDB

These same packages have also gone downstream to Ubuntu, has passed Ubuntu QA and are included since the Ubuntu 14.04 release.

Buildbot test for ok-debpkg branch: https://buildbot.askmonty.org/buildbot/grid?branch=ok-debpkg&category=main

== Roll-out planning ==

This merge should be safe to release with 10.1. All new installs and upgrades using the MariaDB.org repo or mixing it with official Debian or Ubuntu repository contents is expected to work.

Below are the different scenarios how releasing the new packging at MariaDB.org will affect current installs/upgrades:

A) If users currently have a MariaDB.org repo enabled, those packages will have a + in their version number. Debian package manager will always consider e.g. 10.1.6+maria as more recent than 10.1.6 from the native repositories. Therefore people having MariaDB.org repositories will always get primarly the packages from MariaDB.org repositories. It is very unlikely that a Debian repository would ever have a more recent release than in MariaDB.org (e.g. 10.1.7 would override 10.1.6+maria, but that is not likely to take place as MariaDB.org always releases first).

B) The new packaging is designed and tested that it allows seamless upgrades from old deb packaging generation to new generation. Whether the new packaging is from MariaDB.org or distro repositories does not matter.

C) Backwards works only partially, but is a rare case (e.g. current Ubuntu users trying to upgrade from new style debs 10.0 to MariaDB.org repo old style debs 10.0 will see issues, e.g. MDEV-5977). As soon as this merge is done and MariaDB.org publishes packages using the new packaging this corner case is fixed.

D) If users remove the MariaDB.org from their install, they will automatically have their 10.1.6+maria updated to Debian/Ubuntu native 5.5.39 once it is released in Debian.

E) Like previously, upgrading from MySQL 5.5 to MariaDB 5.5 works seamlessly and backwards also. Updating from MariaDB 5.5 to 10.0 works seamlessly but backwards will have issues (downgrade flag, InnoDB log size mismatch etc) the package manager will not solve automatically. If users dump and import data manually, and in between completely purge and install the packages, then users can always migrate to whatever version. $acceptance criteria:$",2,0,0,0,0,0,0,19761.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
747,MDEV-6353,Task,MDEV,2014-06-17 17:07:29,,0,my_ismbchar() and my_mbcharlen() refactoring,"Currently MY_CHARSET_HANDLER has two functions to handle
multi-byte character lengths:

{code:c}
  uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

This API is not flexible enough.

1. Problems to detect invalid bytes sequences:

mbcharlen() reports invalid bytes as characters with length=1.
ismbchar() returns 0 both for single byte character and for invalid bytes.

It's not possible to detect invalid byte sequences using this API,
which makes it challenging to fix bugs like this:

MDEV-6218 Wrong result of CHAR_LENGTH(non-BMP-character) with 3-byte utf8

2. The first byte is not always enough to detect a character length.
For example, in gb18030, 0xFEFE is a two-byte character, while 
0xFE308130 is a four-byte character. Notice, both start with 0xFE.
Using mbcharlen(cs, first_byte) is useless in combination with gb18030,
because at least two bytes are needed to make a decision for 0xFE??.


This API should be changed into a single function:

{code:c}
  int (*charlen)(CHARSET_INFO *cs, const char *str, const char *strend);
{code}

For performance purposes, the caller must supply a string consisting of
at least one byte. Non-zero length will be asserted:
{code:c}
   DBUG_ASSERT(str <  strend);
{code}

The function will return the same codes that mb_wc() does:

1. Positive numbers on success:

1a. 1 in case of a one-byte character found starting at ""str""
1b. 2 in case of a two-byte character
1c. 3 in case of a three-byte character
1d. 4 in case of a four-byte character
1e. 5 in case of a five-byte character

2. Non-positive number on error:
2a. MY_CS_ILSEQ (0)      in case if a wrong byte sequence is met
2b. MY_CS_TOOSMALL(-101) in case if the supplied string is too short
    to detect a character length. The caller must append one more byte to the
    end of the tested string to make detection of the leading character
    length possible.
2c. MY_CS_TOOSMALL2...MY_CS_TOOSMALL4 (-102..-104).
    The same meaning as in the as previous one,
    but the caller must supply two..four more bytes.

Note, the function will ask the caller for as few more bytes as possible.
For example, for the character set gb18030 (which is not in MariaDB yet):

a. charlen(0xFE) will return MY_CS_TOOSMALL, asking for one more byte only.
   Note, although 0xFE can start both 2-byte and 4-byte sequences,
   charlen(0xFE) will ask for only one more byte,
   it will not ask for 3 more bytes immediately. This is to give a chance
   to the caller to read a 2-byte character from the incoming stream
   without having to read extra bytes when they are not really necessary.

b. charlen(0xFEFE) will return 2, meaning a two byte-character.

c. charlen(0xFE30) will return MY_CS_TOOSMALL2, asking for two more bytes.

d. charlen(0xFE3081) will return MY_CS_TOOSMALL, askibg for one more byte.

e. charlen(0xFE308130) will return 4, meaning a four-byte character.


Note, the affected charset handler functions are currently almost not used directly:
{code:c}
  cs->cset->ismbchar(cs, str, strend);
  cs->cset->mbcharlen(cs, ch);
{code}

Instead, they are used through the macros my_mbcharlen() and my_ismbchar():

{code:c}
#define my_ismbchar(s, a, b)          ((s)->cset->ismbchar((s), (a), (b)))
#define my_mbcharlen(s, a)            ((s)->cset->mbcharlen((s),(a)))
{code}

This is very fortunate.
To avoid major code changes, we'll rewrite these macros 
(either into new macros, or into functions) to mimic the old API.

my_ismbchar(cs, a, b) will call cs->cset->charlen(), but then will
change the return value as follows:

{code:c}
uint my_ismbchar(CHARSET_INFO *cs, const char *str, const char *strend)
{
  int rc= cs->cset->charlen(cs, str, strend);
  return (uint) (rc > 1 ? rc : 0);
}
{code}

This is to return 0 for all cases where the old macros my_ismbchar()
returned 0:
- single byte characters (return value 1)
- invalid byte sequences (return value 0)
- too short strings      (negative return values MY_CS_TOOSMALL*)


my_mbcharlen(cs, ch) will return 1 for single byte characters and for
unknown multi-byte heads. It will also convert MY_CS_TOOSMALL to 2,
MY_CS_TOOSMALL2 to 3 and so on.


Later, when adding gb18030, will replace some calls for my_ismbchar() and my_mbcharlen()
to direct calls for cs->cset->charlen(), in the places where it is important.


",,"my_ismbchar() and my_mbcharlen() refactoring $end$ Currently MY_CHARSET_HANDLER has two functions to handle
multi-byte character lengths:

{code:c}
  uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

This API is not flexible enough.

1. Problems to detect invalid bytes sequences:

mbcharlen() reports invalid bytes as characters with length=1.
ismbchar() returns 0 both for single byte character and for invalid bytes.

It's not possible to detect invalid byte sequences using this API,
which makes it challenging to fix bugs like this:

MDEV-6218 Wrong result of CHAR_LENGTH(non-BMP-character) with 3-byte utf8

2. The first byte is not always enough to detect a character length.
For example, in gb18030, 0xFEFE is a two-byte character, while 
0xFE308130 is a four-byte character. Notice, both start with 0xFE.
Using mbcharlen(cs, first_byte) is useless in combination with gb18030,
because at least two bytes are needed to make a decision for 0xFE??.


This API should be changed into a single function:

{code:c}
  int (*charlen)(CHARSET_INFO *cs, const char *str, const char *strend);
{code}

For performance purposes, the caller must supply a string consisting of
at least one byte. Non-zero length will be asserted:
{code:c}
   DBUG_ASSERT(str <  strend);
{code}

The function will return the same codes that mb_wc() does:

1. Positive numbers on success:

1a. 1 in case of a one-byte character found starting at ""str""
1b. 2 in case of a two-byte character
1c. 3 in case of a three-byte character
1d. 4 in case of a four-byte character
1e. 5 in case of a five-byte character

2. Non-positive number on error:
2a. MY_CS_ILSEQ (0)      in case if a wrong byte sequence is met
2b. MY_CS_TOOSMALL(-101) in case if the supplied string is too short
    to detect a character length. The caller must append one more byte to the
    end of the tested string to make detection of the leading character
    length possible.
2c. MY_CS_TOOSMALL2...MY_CS_TOOSMALL4 (-102..-104).
    The same meaning as in the as previous one,
    but the caller must supply two..four more bytes.

Note, the function will ask the caller for as few more bytes as possible.
For example, for the character set gb18030 (which is not in MariaDB yet):

a. charlen(0xFE) will return MY_CS_TOOSMALL, asking for one more byte only.
   Note, although 0xFE can start both 2-byte and 4-byte sequences,
   charlen(0xFE) will ask for only one more byte,
   it will not ask for 3 more bytes immediately. This is to give a chance
   to the caller to read a 2-byte character from the incoming stream
   without having to read extra bytes when they are not really necessary.

b. charlen(0xFEFE) will return 2, meaning a two byte-character.

c. charlen(0xFE30) will return MY_CS_TOOSMALL2, asking for two more bytes.

d. charlen(0xFE3081) will return MY_CS_TOOSMALL, askibg for one more byte.

e. charlen(0xFE308130) will return 4, meaning a four-byte character.


Note, the affected charset handler functions are currently almost not used directly:
{code:c}
  cs->cset->ismbchar(cs, str, strend);
  cs->cset->mbcharlen(cs, ch);
{code}

Instead, they are used through the macros my_mbcharlen() and my_ismbchar():

{code:c}
#define my_ismbchar(s, a, b)          ((s)->cset->ismbchar((s), (a), (b)))
#define my_mbcharlen(s, a)            ((s)->cset->mbcharlen((s),(a)))
{code}

This is very fortunate.
To avoid major code changes, we'll rewrite these macros 
(either into new macros, or into functions) to mimic the old API.

my_ismbchar(cs, a, b) will call cs->cset->charlen(), but then will
change the return value as follows:

{code:c}
uint my_ismbchar(CHARSET_INFO *cs, const char *str, const char *strend)
{
  int rc= cs->cset->charlen(cs, str, strend);
  return (uint) (rc > 1 ? rc : 0);
}
{code}

This is to return 0 for all cases where the old macros my_ismbchar()
returned 0:
- single byte characters (return value 1)
- invalid byte sequences (return value 0)
- too short strings      (negative return values MY_CS_TOOSMALL*)


my_mbcharlen(cs, ch) will return 1 for single byte characters and for
unknown multi-byte heads. It will also convert MY_CS_TOOSMALL to 2,
MY_CS_TOOSMALL2 to 3 and so on.


Later, when adding gb18030, will replace some calls for my_ismbchar() and my_mbcharlen()
to direct calls for cs->cset->charlen(), in the places where it is important.


 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Minor,36,,5,0,8,7,0,1,0,,0,850,0,0,0,2016-02-24 12:12:06,my_ismbchar() and my_mbcharlen() refactoring,"Currently MY_CHARSET_HANDLER has two functions to handle
multi-byte character lengths:

{code}
  uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

This API is not flexible enough.

1. Problems to detect invalid bytes sequences:

mbcharlen() reports invalid bytes as characters with length=1.
ismbchar() returns 0 both for single byte character and for invalid bytes.

It's not possible to detect invalid byte sequences using this API,
which makes it challenging to fix bugs like this:

MDEV-6218 Wrong result of CHAR_LENGTH(non-BMP-character) with 3-byte utf8

2. The first byte is not always enough to detect a character length.
For example, in gb18030, 0xFEFE is a two-byte character, while 
0xFE308130 is a four-byte character. Notice, both start with 0xFE.
Using mbcharlen(cs, first_byte) is useless in combination with gb18030,
because at least two bytes are needed to make a decision for 0xFE??.


This API should be changed into a single function:

{code}
  int (*charlen)(CHARSET_INFO *cs, const char *str, const char *strend);
{code}

For performance purposes, the caller must supply a string consisting of
at least one byte. Non-zero length will be asserted:
{code}
   DBUG_ASSERT(str <  strend);
{code}

The function will return the same codes that mb_wc() does:

1. Positive numbers on success:

1a. 1 in case of a one-byte character found starting at ""str""
1b. 2 in case of a two-byte character
1c. 3 in case of a three-byte character
1d. 4 in case of a four-byte character
1e. 5 in case of a five-byte character

2. Non-positive number on error:
2a. MY_CS_ILSEQ (0)      in case if a wrong byte sequence is met
2b. MY_CS_TOOSMALL(-101) in case if the supplied string is too short
    to detect a character length. The caller must append one more byte to the
    end of the tested string to make detection of the leading character
    length possible.
2c. MY_CS_TOOSMALL2...MY_CS_TOOSMALL4 (-102..-104).
    The same meaning as in the as previous one,
    but the caller must supply two..four more bytes.

Note, the function will ask the caller for as few more bytes as possible.
For example, for the character set gb18030 (which is not in MariaDB yet):

a. charlen(0xFE) will return MY_CS_TOOSMALL, asking for one more byte only.
   Note, although 0xFE can start both 2-byte and 4-byte sequences,
   charlen(0xFE) will ask for only one more byte,
   it will not ask for 3 more bytes immediately. This is to give a chance
   to the caller to read a 2-byte character from the incoming stream
   without having to read extra bytes when they are not really necessary.

b. charlen(0xFEFE) will return 2, meaning a two byte-character.

c. charlen(0xFE30) will return MY_CS_TOOSMALL2, asking for two more bytes.

d. charlen(0xFE3081) will return MY_CS_TOOSMALL, askibg for one more byte.

e. charlen(0xFE308130) will return 4, meaning a four-byte character.


Note, the affected charset handler functions are currently almost not used directly:
{code}
  cs->cset->ismbchar(cs, str, strend);
  cs->cset->mbcharlen(cs, ch);
{code}

Instead, they are used through the macros my_mbcharlen() and my_ismbchar():

{code}
#define my_ismbchar(s, a, b)          ((s)->cset->ismbchar((s), (a), (b)))
#define my_mbcharlen(s, a)            ((s)->cset->mbcharlen((s),(a)))
{code}

This is very fortunate.
To avoid major code changes, we'll rewrite these macros 
(either into new macros, or into functions) to mimic the old API.

my_ismbchar(cs, a, b) will call cs->cset->charlen(), but then will
change the return value as follows:

{code}
uint my_ismbchar(CHARSET_INFO *cs, const char *str, const char *strend)
{
  int rc= cs->cset->charlen(cs, str, strend);
  return (uint) (rc > 1 ? rc : 0);
}
{code}

This is to return 0 for all cases where the old macros my_ismbchar()
returned 0:
- single byte characters (return value 1)
- invalid byte sequences (return value 0)
- too short strings      (negative return values MY_CS_TOOSMALL*)


my_mbcharlen(cs, ch) will return 1 for single byte characters and for
unknown multi-byte heads. It will also convert MY_CS_TOOSMALL to 2,
MY_CS_TOOSMALL2 to 3 and so on.


Later, when adding gb18030, will replace some calls for my_ismbchar() and my_mbcharlen()
to direct calls for cs->cset->charlen(), in the places where it is important.


",,0,1,0,12,0.0091047,"my_ismbchar() and my_mbcharlen() refactoring $end$ Currently MY_CHARSET_HANDLER has two functions to handle
multi-byte character lengths:

{code}
  uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

This API is not flexible enough.

1. Problems to detect invalid bytes sequences:

mbcharlen() reports invalid bytes as characters with length=1.
ismbchar() returns 0 both for single byte character and for invalid bytes.

It's not possible to detect invalid byte sequences using this API,
which makes it challenging to fix bugs like this:

MDEV-6218 Wrong result of CHAR_LENGTH(non-BMP-character) with 3-byte utf8

2. The first byte is not always enough to detect a character length.
For example, in gb18030, 0xFEFE is a two-byte character, while 
0xFE308130 is a four-byte character. Notice, both start with 0xFE.
Using mbcharlen(cs, first_byte) is useless in combination with gb18030,
because at least two bytes are needed to make a decision for 0xFE??.


This API should be changed into a single function:

{code}
  int (*charlen)(CHARSET_INFO *cs, const char *str, const char *strend);
{code}

For performance purposes, the caller must supply a string consisting of
at least one byte. Non-zero length will be asserted:
{code}
   DBUG_ASSERT(str <  strend);
{code}

The function will return the same codes that mb_wc() does:

1. Positive numbers on success:

1a. 1 in case of a one-byte character found starting at ""str""
1b. 2 in case of a two-byte character
1c. 3 in case of a three-byte character
1d. 4 in case of a four-byte character
1e. 5 in case of a five-byte character

2. Non-positive number on error:
2a. MY_CS_ILSEQ (0)      in case if a wrong byte sequence is met
2b. MY_CS_TOOSMALL(-101) in case if the supplied string is too short
    to detect a character length. The caller must append one more byte to the
    end of the tested string to make detection of the leading character
    length possible.
2c. MY_CS_TOOSMALL2...MY_CS_TOOSMALL4 (-102..-104).
    The same meaning as in the as previous one,
    but the caller must supply two..four more bytes.

Note, the function will ask the caller for as few more bytes as possible.
For example, for the character set gb18030 (which is not in MariaDB yet):

a. charlen(0xFE) will return MY_CS_TOOSMALL, asking for one more byte only.
   Note, although 0xFE can start both 2-byte and 4-byte sequences,
   charlen(0xFE) will ask for only one more byte,
   it will not ask for 3 more bytes immediately. This is to give a chance
   to the caller to read a 2-byte character from the incoming stream
   without having to read extra bytes when they are not really necessary.

b. charlen(0xFEFE) will return 2, meaning a two byte-character.

c. charlen(0xFE30) will return MY_CS_TOOSMALL2, asking for two more bytes.

d. charlen(0xFE3081) will return MY_CS_TOOSMALL, askibg for one more byte.

e. charlen(0xFE308130) will return 4, meaning a four-byte character.


Note, the affected charset handler functions are currently almost not used directly:
{code}
  cs->cset->ismbchar(cs, str, strend);
  cs->cset->mbcharlen(cs, ch);
{code}

Instead, they are used through the macros my_mbcharlen() and my_ismbchar():

{code}
#define my_ismbchar(s, a, b)          ((s)->cset->ismbchar((s), (a), (b)))
#define my_mbcharlen(s, a)            ((s)->cset->mbcharlen((s),(a)))
{code}

This is very fortunate.
To avoid major code changes, we'll rewrite these macros 
(either into new macros, or into functions) to mimic the old API.

my_ismbchar(cs, a, b) will call cs->cset->charlen(), but then will
change the return value as follows:

{code}
uint my_ismbchar(CHARSET_INFO *cs, const char *str, const char *strend)
{
  int rc= cs->cset->charlen(cs, str, strend);
  return (uint) (rc > 1 ? rc : 0);
}
{code}

This is to return 0 for all cases where the old macros my_ismbchar()
returned 0:
- single byte characters (return value 1)
- invalid byte sequences (return value 0)
- too short strings      (negative return values MY_CS_TOOSMALL*)


my_mbcharlen(cs, ch) will return 1 for single byte characters and for
unknown multi-byte heads. It will also convert MY_CS_TOOSMALL to 2,
MY_CS_TOOSMALL2 to 3 and so on.


Later, when adding gb18030, will replace some calls for my_ismbchar() and my_mbcharlen()
to direct calls for cs->cset->charlen(), in the places where it is important.


 $acceptance criteria:$",1,1,1,1,0,0,1,14803.1,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
748,MDEV-6697,Task,MDEV,2014-09-04 20:28:20,,0,Improve foreign keys warnings/errors,"Hi guys, we should check if this should be merged or not from mysql
http://bugs.mysql.com/bug.php?id=73757

probably a document bug, or maybe not, just check to don't leave dba searching were is the erro, include a ""create table report errors at show warnings about foreign keys"" or something like this",,"Improve foreign keys warnings/errors $end$ Hi guys, we should check if this should be merged or not from mysql
http://bugs.mysql.com/bug.php?id=73757

probably a document bug, or maybe not, just check to don't leave dba searching were is the erro, include a ""create table report errors at show warnings about foreign keys"" or something like this $acceptance criteria:$",,roberto spadim,roberto spadim,Major,15,,0,2,1,2,0,1,0,,0,850,2,1,0,2015-07-28 16:53:59,Improve foreign keys warnings/errors,"Hi guys, we should check if this should be merged or not from mysql
http://bugs.mysql.com/bug.php?id=73757

probably a document bug, or maybe not, just check to don't leave dba searching were is the erro, include a ""create table report errors at show warnings about foreign keys"" or something like this",,0,0,0,0,0.0,"Improve foreign keys warnings/errors $end$ Hi guys, we should check if this should be merged or not from mysql
http://bugs.mysql.com/bug.php?id=73757

probably a document bug, or maybe not, just check to don't leave dba searching were is the erro, include a ""create table report errors at show warnings about foreign keys"" or something like this $acceptance criteria:$",0,0,0,0,0,0,1,7844.42,2,1,0.5,1,0.5,1,0.5,0,0.0,0,0.0
749,MDEV-6720,Task,MDEV,2014-09-10 14:19:18,,0,enable connection log in mysqltest by default,"mysqltest has commands
{noformat}
--enable_connect_log
--disable_connect_log
{noformat}

the default behavior is *disable*, we should make it *enable* by default.",,"enable connection log in mysqltest by default $end$ mysqltest has commands
{noformat}
--enable_connect_log
--disable_connect_log
{noformat}

the default behavior is *disable*, we should make it *enable* by default. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Minor,24,,0,5,0,5,0,0,0,,0,850,4,0,0,2015-11-18 10:59:28,enable connection log in mysqltest by default,"mysqltest has commands
{noformat}
--enable_connect_log
--disable_connect_log
{noformat}

the default behavior is *disable*, we should make it *enable* by default.",,0,0,0,0,0.0,"enable connection log in mysqltest by default $end$ mysqltest has commands
{noformat}
--enable_connect_log
--disable_connect_log
{noformat}

the default behavior is *disable*, we should make it *enable* by default. $acceptance criteria:$",0,0,0,0,0,0,1,10412.7,4,3,0.75,2,0.5,2,0.5,2,0.5,2,0.5
750,MDEV-6756,Task,MDEV,2014-09-18 21:18:48,,0,map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST,"We have have a multi-threaded program with multiple connections to the database.  While this program is running, we often get a single child process (pid) that just hangs.  This particular child process consumes 100% of any given CPU.  We are trying to troubleshoot this and trying to determine which child pid maps to the SHOW PROCESSLIST Id.

With that in mind, we would like to know if there is a way to map a linux child pid that's associated with the linux parent pid (ppid) of the mysqld process to the Id shown in the output of SHOW PROCESSLIST.

",,"map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST $end$ We have have a multi-threaded program with multiple connections to the database.  While this program is running, we often get a single child process (pid) that just hangs.  This particular child process consumes 100% of any given CPU.  We are trying to troubleshoot this and trying to determine which child pid maps to the SHOW PROCESSLIST Id.

With that in mind, we would like to know if there is a way to map a linux child pid that's associated with the linux parent pid (ppid) of the mysqld process to the Id shown in the output of SHOW PROCESSLIST.

 $acceptance criteria:$",,Harry Higgs,Harry Higgs,Major,30,,0,13,1,4,0,1,0,,0,850,6,1,0,2015-06-17 11:59:06,map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST,"We have have a multi-threaded program with multiple connections to the database.  While this program is running, we often get a single child process (pid) that just hangs.  This particular child process consumes 100% of any given CPU.  We are trying to troubleshoot this and trying to determine which child pid maps to the SHOW PROCESSLIST Id.

With that in mind, we would like to know if there is a way to map a linux child pid that's associated with the linux parent pid (ppid) of the mysqld process to the Id shown in the output of SHOW PROCESSLIST.

",,0,0,0,0,0.0,"map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST $end$ We have have a multi-threaded program with multiple connections to the database.  While this program is running, we often get a single child process (pid) that just hangs.  This particular child process consumes 100% of any given CPU.  We are trying to troubleshoot this and trying to determine which child pid maps to the SHOW PROCESSLIST Id.

With that in mind, we would like to know if there is a way to map a linux child pid that's associated with the linux parent pid (ppid) of the mysqld process to the Id shown in the output of SHOW PROCESSLIST.

 $acceptance criteria:$",0,0,0,0,0,0,1,6518.67,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
751,MDEV-6792,Task,MDEV,2014-09-26 01:14:06,,0,Build galera library through buildbot,"Enable buildbot to build galera library for all supported platforms.

https://github.com/codership/galera
",,"Build galera library through buildbot $end$ Enable buildbot to build galera library for all supported platforms.

https://github.com/codership/galera
 $acceptance criteria:$",,Nirbhay Choubey,Nirbhay Choubey,Major,20,,1,5,1,4,0,0,0,,0,850,3,0,0,2015-06-17 21:28:46,Build galera library through buildbot,"Enable buildbot to build galera library for all supported platforms.

https://github.com/codership/galera
",,0,0,0,0,0.0,"Build galera library through buildbot $end$ Enable buildbot to build galera library for all supported platforms.

https://github.com/codership/galera
 $acceptance criteria:$",0,0,0,0,0,0,1,6356.23,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
752,MDEV-6829,Task,MDEV,2014-10-02 21:03:00,,0,SELinux/AppArmor policies for Galera server,"Currently, there is no straight forward way to configure SELinux for MariaDB
Galera server. As a result, SELinux is normally disabled.
",,"SELinux/AppArmor policies for Galera server $end$ Currently, there is no straight forward way to configure SELinux for MariaDB
Galera server. As a result, SELinux is normally disabled.
 $acceptance criteria:$",,Nirbhay Choubey,Nirbhay Choubey,Major,8,,0,2,0,1,0,1,0,,0,850,1,0,0,2015-06-17 21:28:57,SELinux policy for Galera server,"Currently, there is no straight forward way to configure SELinux for MariaDB
Galera server. As a result, SELinux is normally disabled.
",,1,0,0,4,0.0689655,"SELinux policy for Galera server $end$ Currently, there is no straight forward way to configure SELinux for MariaDB
Galera server. As a result, SELinux is normally disabled.
 $acceptance criteria:$",1,1,0,0,0,0,0,6192.42,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
753,MDEV-6877,Task,MDEV,2014-10-16 05:16:18,,0,Merge binlog_row_image from MySQL 5.6,"Sadly, many production environments do have InnoDB tables without PRIMARY KEY. This happens for instance in Drupal deployments where either core or modules may not always have a PK defined in their structure.

This MySQL 5.6 optimisation would help MariaDB slaves execute certain queries faster.


Ref http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image heading ""Optimized Row-Based Replication"" quoted below
===
The final piece in improving replication performance is to introduce efficiencies to the binary log itself.

By only replicating those elements of the row image that have changed following INSERT, UPDATE and DELETE operations, replication throughput for both the master and slave(s) can be increased while binary log disk space, network resource and server memory footprint are all reduced.

This is especially useful when replicating across datacenters or cloud availability zones.

Another performance enhancements is the was Row-Based Replication events are handled on the slave against tables without Primary Keys. Updates would be made via multiple table scans. In MySQL 5.6, no matter size of the event, only one table scan is performed, significantly reducing the apply time.

You can learn more about Optimized Row Based Replication from the MySQL documentation (http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image)
===

",,"Merge binlog_row_image from MySQL 5.6 $end$ Sadly, many production environments do have InnoDB tables without PRIMARY KEY. This happens for instance in Drupal deployments where either core or modules may not always have a PK defined in their structure.

This MySQL 5.6 optimisation would help MariaDB slaves execute certain queries faster.


Ref http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image heading ""Optimized Row-Based Replication"" quoted below
===
The final piece in improving replication performance is to introduce efficiencies to the binary log itself.

By only replicating those elements of the row image that have changed following INSERT, UPDATE and DELETE operations, replication throughput for both the master and slave(s) can be increased while binary log disk space, network resource and server memory footprint are all reduced.

This is especially useful when replicating across datacenters or cloud availability zones.

Another performance enhancements is the was Row-Based Replication events are handled on the slave against tables without Primary Keys. Updates would be made via multiple table scans. In MySQL 5.6, no matter size of the event, only one table scan is performed, significantly reducing the apply time.

You can learn more about Optimized Row Based Replication from the MySQL documentation (http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image)
===

 $acceptance criteria:$",,Arjen Lentz,Arjen Lentz,Major,11,,5,6,6,1,0,0,0,,0,850,5,0,0,2015-06-17 17:53:14,Merge binlog_row_image from MySQL 5.6,"Sadly, many production environments do have InnoDB tables without PRIMARY KEY. This happens for instance in Drupal deployments where either core or modules may not always have a PK defined in their structure.

This MySQL 5.6 optimisation would help MariaDB slaves execute certain queries faster.


Ref http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image heading ""Optimized Row-Based Replication"" quoted below
===
The final piece in improving replication performance is to introduce efficiencies to the binary log itself.

By only replicating those elements of the row image that have changed following INSERT, UPDATE and DELETE operations, replication throughput for both the master and slave(s) can be increased while binary log disk space, network resource and server memory footprint are all reduced.

This is especially useful when replicating across datacenters or cloud availability zones.

Another performance enhancements is the was Row-Based Replication events are handled on the slave against tables without Primary Keys. Updates would be made via multiple table scans. In MySQL 5.6, no matter size of the event, only one table scan is performed, significantly reducing the apply time.

You can learn more about Optimized Row Based Replication from the MySQL documentation (http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image)
===

",,0,0,0,0,0.0,"Merge binlog_row_image from MySQL 5.6 $end$ Sadly, many production environments do have InnoDB tables without PRIMARY KEY. This happens for instance in Drupal deployments where either core or modules may not always have a PK defined in their structure.

This MySQL 5.6 optimisation would help MariaDB slaves execute certain queries faster.


Ref http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image heading ""Optimized Row-Based Replication"" quoted below
===
The final piece in improving replication performance is to introduce efficiencies to the binary log itself.

By only replicating those elements of the row image that have changed following INSERT, UPDATE and DELETE operations, replication throughput for both the master and slave(s) can be increased while binary log disk space, network resource and server memory footprint are all reduced.

This is especially useful when replicating across datacenters or cloud availability zones.

Another performance enhancements is the was Row-Based Replication events are handled on the slave against tables without Primary Keys. Updates would be made via multiple table scans. In MySQL 5.6, no matter size of the event, only one table scan is performed, significantly reducing the apply time.

You can learn more about Optimized Row Based Replication from the MySQL documentation (http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_binlog_row_image)
===

 $acceptance criteria:$",0,0,0,0,0,0,0,5868.6,1,1,1.0,1,1.0,1,1.0,0,0.0,0,0.0
754,MDEV-7032,Task,MDEV,2014-11-06 12:25:10,,0,new pam plugin with a suid wrapper,"PAM authentication in many cases only works if done by the root user or the user that is authenticating itself.

For example, to read {{/etc/shadow}} one has to be root. {{unix_chkpwd}} wrapper, created specifically to loosen this requirement, checks that user name matches the current UID. Google-authenticator PAM module reads the data from {{~user/}} home directory — again, can be only done as root or that user. And so on.

A solution to all these problems could be a small setuid wrapper that pam plugin invokes. Perhaps this wrapper should check that it is invoked as mysql user…",,"new pam plugin with a suid wrapper $end$ PAM authentication in many cases only works if done by the root user or the user that is authenticating itself.

For example, to read {{/etc/shadow}} one has to be root. {{unix_chkpwd}} wrapper, created specifically to loosen this requirement, checks that user name matches the current UID. Google-authenticator PAM module reads the data from {{~user/}} home directory — again, can be only done as root or that user. And so on.

A solution to all these problems could be a small setuid wrapper that pam plugin invokes. Perhaps this wrapper should check that it is invoked as mysql user… $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,21,,11,4,15,1,0,2,0,,0,850,2,2,0,2018-05-29 07:52:41,new pam plugin with a suid wrapper,"PAM authentication in many cases only works if done by the root user or the user that is authenticating itself.

For example, to read {{/etc/shadow}} one has to be root. {{unix_chkpwd}} wrapper, created specifically to loosen this requirement, checks that user name matches the current UID. Google-authenticator PAM module reads the data from {{~user/}} home directory — again, can be only done as root or that user. And so on.

A solution to all these problems could be a small setuid wrapper that pam plugin invokes. Perhaps this wrapper should check that it is invoked as mysql user…",,0,0,0,0,0.0,"new pam plugin with a suid wrapper $end$ PAM authentication in many cases only works if done by the root user or the user that is authenticating itself.

For example, to read {{/etc/shadow}} one has to be root. {{unix_chkpwd}} wrapper, created specifically to loosen this requirement, checks that user name matches the current UID. Google-authenticator PAM module reads the data from {{~user/}} home directory — again, can be only done as root or that user. And so on.

A solution to all these problems could be a small setuid wrapper that pam plugin invokes. Perhaps this wrapper should check that it is invoked as mysql user… $acceptance criteria:$",0,0,0,0,0,0,0,31195.4,5,3,0.6,2,0.4,2,0.4,2,0.4,2,0.4
755,MDEV-7145,Task,MDEV,2014-11-20 03:29:11,,0,Port/merge the CHANGE MASTER TO ... MASTER_DELAY option from MySQL 5.6,"This feature was earlier a port-in-progress in the OurDelta builds, but wasn't included in the work for 5.1 that Monty and I did for MariaDB.
It's in stock MySQL 5.6. It's a useful feature, and clients still sometimes need it. The pt-slave-delay tool does the job, but it's nicer to have it in the server - and heck, the code already exists.

If we can get this merged into (for instance) 10.1, that'd be great.
thanks",,"Port/merge the CHANGE MASTER TO ... MASTER_DELAY option from MySQL 5.6 $end$ This feature was earlier a port-in-progress in the OurDelta builds, but wasn't included in the work for 5.1 that Monty and I did for MariaDB.
It's in stock MySQL 5.6. It's a useful feature, and clients still sometimes need it. The pt-slave-delay tool does the job, but it's nicer to have it in the server - and heck, the code already exists.

If we can get this merged into (for instance) 10.1, that'd be great.
thanks $acceptance criteria:$",,Arjen Lentz,Arjen Lentz,Critical,11,,0,8,1,1,0,0,0,,0,850,3,0,0,2016-09-14 13:55:36,Port/merge the CHANGE MASTER TO ... MASTER_DELAY option from MySQL 5.6,"This feature was earlier a port-in-progress in the OurDelta builds, but wasn't included in the work for 5.1 that Monty and I did for MariaDB.
It's in stock MySQL 5.6. It's a useful feature, and clients still sometimes need it. The pt-slave-delay tool does the job, but it's nicer to have it in the server - and heck, the code already exists.

If we can get this merged into (for instance) 10.1, that'd be great.
thanks",,0,0,0,0,0.0,"Port/merge the CHANGE MASTER TO ... MASTER_DELAY option from MySQL 5.6 $end$ This feature was earlier a port-in-progress in the OurDelta builds, but wasn't included in the work for 5.1 that Monty and I did for MariaDB.
It's in stock MySQL 5.6. It's a useful feature, and clients still sometimes need it. The pt-slave-delay tool does the job, but it's nicer to have it in the server - and heck, the code already exists.

If we can get this merged into (for instance) 10.1, that'd be great.
thanks $acceptance criteria:$",0,0,0,0,0,0,0,15946.4,2,1,0.5,1,0.5,1,0.5,0,0.0,0,0.0
756,MDEV-7331,Task,MDEV,2014-12-15 23:22:34,,0,information_schema.user_variables,"Implement a information schema plugin, to report all user variables (name, value, and others informations)
",,"information_schema.user_variables $end$ Implement a information schema plugin, to report all user variables (name, value, and others informations)
 $acceptance criteria:$",,roberto spadim,roberto spadim,Major,18,,2,3,3,1,0,0,0,,0,850,3,0,0,2015-11-18 10:59:55,information_schema.user_variables,"Implement a information schema plugin, to report all user variables (name, value, and others informations)
",,0,0,0,0,0.0,"information_schema.user_variables $end$ Implement a information schema plugin, to report all user variables (name, value, and others informations)
 $acceptance criteria:$",0,0,0,0,0,0,0,8099.62,3,1,0.333333,1,0.333333,1,0.333333,0,0.0,0,0.0
757,MDEV-7376,Task,MDEV,2014-12-26 03:20:14,,0,"Removal of the tool ""mysql_zap""","MariaDB 10.1 still ships the tool ""_mysql\_zap.sh_"" on the ""_scripts/_"" directory.
This tool basically does what the ""_pkill_"" tool does with the ""_-f_"" switch : killing processes whose name match a certain string.

The way the script match the processes it needs to kill isnt clean (using the output of a ""_ps_"" command), it outputs an stty error message when executed, doesnt output any warning after trying to kill processes the user doesnt own and waits at least 2 seconds after each kills which renders it unpractical.

*The script doesnt seems to be called at any point on the MariaDB sources, doesnt do any MariaDB related job and is most probably not used anymore.*",,"Removal of the tool ""mysql_zap"" $end$ MariaDB 10.1 still ships the tool ""_mysql\_zap.sh_"" on the ""_scripts/_"" directory.
This tool basically does what the ""_pkill_"" tool does with the ""_-f_"" switch : killing processes whose name match a certain string.

The way the script match the processes it needs to kill isnt clean (using the output of a ""_ps_"" command), it outputs an stty error message when executed, doesnt output any warning after trying to kill processes the user doesnt own and waits at least 2 seconds after each kills which renders it unpractical.

*The script doesnt seems to be called at any point on the MariaDB sources, doesnt do any MariaDB related job and is most probably not used anymore.* $acceptance criteria:$",,Jean Weisbuch,Jean Weisbuch,Minor,12,,0,3,1,1,0,0,0,,0,850,3,0,0,2015-11-18 11:00:10,"Removal of the tool ""mysql_zap""","MariaDB 10.1 still ships the tool ""_mysql\_zap.sh_"" on the ""_scripts/_"" directory.
This tool basically does what the ""_pkill_"" tool does with the ""_-f_"" switch : killing processes whose name match a certain string.

The way the script match the processes it needs to kill isnt clean (using the output of a ""_ps_"" command), it outputs an stty error message when executed, doesnt output any warning after trying to kill processes the user doesnt own and waits at least 2 seconds after each kills which renders it unpractical.

*The script doesnt seems to be called at any point on the MariaDB sources, doesnt do any MariaDB related job and is most probably not used anymore.*",,0,0,0,0,0.0,"Removal of the tool ""mysql_zap"" $end$ MariaDB 10.1 still ships the tool ""_mysql\_zap.sh_"" on the ""_scripts/_"" directory.
This tool basically does what the ""_pkill_"" tool does with the ""_-f_"" switch : killing processes whose name match a certain string.

The way the script match the processes it needs to kill isnt clean (using the output of a ""_ps_"" command), it outputs an stty error message when executed, doesnt output any warning after trying to kill processes the user doesnt own and waits at least 2 seconds after each kills which renders it unpractical.

*The script doesnt seems to be called at any point on the MariaDB sources, doesnt do any MariaDB related job and is most probably not used anymore.* $acceptance criteria:$",0,0,0,0,0,0,0,7855.65,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
758,MDEV-7384,Task,MDEV,2014-12-27 05:55:32,,0,[PATCH] add PERSISENT FOR ALL option to mysqlanalyze/mysqlcheck,"Having a mysqlanalyze/check that can use PERSISTENT FOR ALL in the ANALYZE TABLES without enabling use_stats_tables globally will assist in engine-independent statistics gathering.

Patch to code and man pages attached to perform this.",,"[PATCH] add PERSISENT FOR ALL option to mysqlanalyze/mysqlcheck $end$ Having a mysqlanalyze/check that can use PERSISTENT FOR ALL in the ANALYZE TABLES without enabling use_stats_tables globally will assist in engine-independent statistics gathering.

Patch to code and man pages attached to perform this. $acceptance criteria:$",,Daniel Black,Daniel Black,Major,13,,0,3,0,1,0,0,0,,0,850,1,0,0,2015-06-09 17:14:49,[PATCH] add PERSISENT FOR ALL option to mysqlanalyze/mysqlcheck,"Having a mysqlanalyze/check that can use PERSISTENT FOR ALL in the ANALYZE TABLES without enabling use_stats_tables globally will assist in engine-independent statistics gathering.

Patch to code and man pages attached to perform this.",,0,0,0,0,0.0,"[PATCH] add PERSISENT FOR ALL option to mysqlanalyze/mysqlcheck $end$ Having a mysqlanalyze/check that can use PERSISTENT FOR ALL in the ANALYZE TABLES without enabling use_stats_tables globally will assist in engine-independent statistics gathering.

Patch to code and man pages attached to perform this. $acceptance criteria:$",0,0,0,0,0,0,0,3947.32,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
759,MDEV-7409,Task,MDEV,2015-01-05 14:44:27,,0,"On RBR, extend the PROCESSLIST info to include at least the name of the recently used table","When RBR is used, add the db.table name to the ""SHOW FULL PROCESSLIST"" command output.

Something like: table name - and the operation it is doing like: INSERT/UPDATE/DELETE/TRUNCATE will be good enough.

Note: there is similar MySQL request filled here : http://bugs.mysql.com/bug.php?id=62019

",,"On RBR, extend the PROCESSLIST info to include at least the name of the recently used table $end$ When RBR is used, add the db.table name to the ""SHOW FULL PROCESSLIST"" command output.

Something like: table name - and the operation it is doing like: INSERT/UPDATE/DELETE/TRUNCATE will be good enough.

Note: there is similar MySQL request filled here : http://bugs.mysql.com/bug.php?id=62019

 $acceptance criteria:$",,Stoykov,Stoykov,Major,28,,1,10,1,2,0,2,0,,0,850,9,2,0,2017-01-11 08:25:27,"On RBR, extend the PROCESSLIST info to include at least the name of the recently used table","When RBR is used, add the db.table name to the ""SHOW FULL PROCESSLIST"" command output.

Something like: table name - and the operation it is doing like: INSERT/UPDATE/DELETE/TRUNCATE will be good enough.

Note: there is similar MySQL request filled here : http://bugs.mysql.com/bug.php?id=62019

",,0,0,0,0,0.0,"On RBR, extend the PROCESSLIST info to include at least the name of the recently used table $end$ When RBR is used, add the db.table name to the ""SHOW FULL PROCESSLIST"" command output.

Something like: table name - and the operation it is doing like: INSERT/UPDATE/DELETE/TRUNCATE will be good enough.

Note: there is similar MySQL request filled here : http://bugs.mysql.com/bug.php?id=62019

 $acceptance criteria:$",0,0,0,0,0,0,1,17681.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
760,MDEV-7472,Task,MDEV,2015-01-15 15:50:17,,0,Implementation of user statements for handling the xtradb changed page bitmaps,"There are user statements available at Percona server for maintenance the  bitmap files that keep the changed pages.
These statements are not available in MariaDB.
reference:
http://www.percona.com/doc/percona-server/5.6/management/changed_page_tracking.html#changed-page-tracking-statements


",,"Implementation of user statements for handling the xtradb changed page bitmaps $end$ There are user statements available at Percona server for maintenance the  bitmap files that keep the changed pages.
These statements are not available in MariaDB.
reference:
http://www.percona.com/doc/percona-server/5.6/management/changed_page_tracking.html#changed-page-tracking-statements


 $acceptance criteria:$",,Stoykov,Stoykov,Major,7,,1,10,2,1,0,0,0,,0,850,6,0,0,2015-06-18 20:04:55,Implementation of user statements for handling the xtradb changed page bitmaps,"There are user statements available at Percona server for maintenance the  bitmap files that keep the changed pages.
These statements are not available in MariaDB.
reference:
http://www.percona.com/doc/percona-server/5.6/management/changed_page_tracking.html#changed-page-tracking-statements


",,0,0,0,0,0.0,"Implementation of user statements for handling the xtradb changed page bitmaps $end$ There are user statements available at Percona server for maintenance the  bitmap files that keep the changed pages.
These statements are not available in MariaDB.
reference:
http://www.percona.com/doc/percona-server/5.6/management/changed_page_tracking.html#changed-page-tracking-statements


 $acceptance criteria:$",0,0,0,0,0,0,0,3700.23,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
761,MDEV-7563,Task,MDEV,2015-02-09 12:09:39,MDEV-10872,0,Support CHECK constraint as in (or close to) SQL Standard,"Support CHECK constraint as in (or close to) SQL Standard.

The following syntaxes will be supported (they are already supported by the current MariaDB parser, but does nothing):

As part of column definition:
CREATE TABLE t1 (a int check (A > 10));

As an independent constraint:
CREATE TABLE t1 (a int, constraint `A` CHECK (A > 10));

ALTER TABLE DROP CONSTRAINT `A`;",,"Support CHECK constraint as in (or close to) SQL Standard $end$ Support CHECK constraint as in (or close to) SQL Standard.

The following syntaxes will be supported (they are already supported by the current MariaDB parser, but does nothing):

As part of column definition:
CREATE TABLE t1 (a int check (A > 10));

As an independent constraint:
CREATE TABLE t1 (a int, constraint `A` CHECK (A > 10));

ALTER TABLE DROP CONSTRAINT `A`; $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,10,,6,6,8,1,0,2,0,,0,850,0,2,0,2016-09-07 16:38:26,Support CHECK constraint as in (or close to) SQL Standard,"Support CHECK constraint as in (or close to) SQL Standard.

The following syntaxes will be supported (they are already supported by the current MariaDB parser, but does nothing):

As part of column definition:
CREATE TABLE t1 (a int check (A > 10));

As an independent constraint:
CREATE TABLE t1 (a int, constraint `A` CHECK (A > 10));

ALTER TABLE DROP CONSTRAINT `A`;",,0,0,0,0,0.0,"Support CHECK constraint as in (or close to) SQL Standard $end$ Support CHECK constraint as in (or close to) SQL Standard.

The following syntaxes will be supported (they are already supported by the current MariaDB parser, but does nothing):

As part of column definition:
CREATE TABLE t1 (a int check (A > 10));

As an independent constraint:
CREATE TABLE t1 (a int, constraint `A` CHECK (A > 10));

ALTER TABLE DROP CONSTRAINT `A`; $acceptance criteria:$",0,0,0,0,0,0,0,13828.5,6,3,0.5,2,0.333333,2,0.333333,2,0.333333,2,0.333333
762,MDEV-7635,Task,MDEV,2015-02-25 20:19:11,,0,update defaults and simplify mysqld config parameters ,"Poor MySQL has received some negative feedback over the years ^1^ ^2^. As entertaining as it is watching and listening to developers struggle and whine, I think it's about time we changed some of the defaults and made the config easier to understand.

Oracle are working on changing more defaults in 5.7 ^3^ ^4^ ^5^, I think we should too.

Something else they have done, is to radically change the simplicity of the default server config file, /etc/my.cnf:
https://raw.githubusercontent.com/mysql/mysql-server/5.7/support-files/my-default.cnf.sh

It's quite clear there, the sql_mode has been set, which will satisfy most developers and users looking for an ACID compliant database with little tweaking, and a rough note on increasing or decreasing RAM needed for InnoDB. MariaDB now probably needs something similar for Galera, CONNET and TokuDB.

# http://sql-info.de/mysql/gotchas.html
# http://blog.ionelmc.ro/2014/12/28/terrible-choices-mysql/
# http://www.tocker.ca/2015/01/23/proposal-to-change-additional-defaults-in-mysql-5-7.html
# http://www.tocker.ca/2015/01/14/proposal-to-change-replication-and-innodb-settings-in-mysql-5-7.html
# http://www.tocker.ca/2015/02/24/proposal-to-change-additional-defaults-in-mysql-5-7-february-edition.html


Proposed new defaults:
{code}
innodb_autoinc_lock_mode            = 2         (was: 1)
innodb_buffer_pool_dump_at_shutdown = ON        (was: OFF)
innodb_buffer_pool_dump_pct         = 25        (was: 100)
innodb_buffer_pool_load_at_startup  = ON        (was: OFF)
innodb_checksum_algorithm           = CRC32     (was: INNODB)
innodb_file_format                  = Barracuda (was: Antelope)
innodb_file_format_max              = Barracuda (was: Antelope)
innodb_large_prefix                 = ON        (was: OFF)
innodb_log_compressed_pages         = ON        (was: OFF)
innodb_purge_threads                = 4         (was: 1)
innodb_strict_mode                  = ON        (was: OFF)

binlog_annotate_row_events          = ON        (was: OFF)
binlog_format                       = MIXED     (was: STATEMENT)
group_concat_max_len                = 1M        (was: 1025)
lock_wait_timeout                   = 86400(1day)(was: 1year)
log_slow_admin_statements           = ON        (was: OFF)
log_slow_slave_statements           = ON        (was: OFF)
log_warnings                        = 2         (was: 1)
max_allowed_packet                  = 16M       (was: 4M)
replicate_annotate_row_events       = ON        (was: OFF)
slave_net_timeout                   = 60        (was: 3600)
sync_binlog                         = 1         (was: 0)
aria_recover                        = FORCE, BACKUP (was: NORMAL)
myisam_recover_options              = FORCE, BACKUP (was: OFF)
{code}
",,"update defaults and simplify mysqld config parameters  $end$ Poor MySQL has received some negative feedback over the years ^1^ ^2^. As entertaining as it is watching and listening to developers struggle and whine, I think it's about time we changed some of the defaults and made the config easier to understand.

Oracle are working on changing more defaults in 5.7 ^3^ ^4^ ^5^, I think we should too.

Something else they have done, is to radically change the simplicity of the default server config file, /etc/my.cnf:
https://raw.githubusercontent.com/mysql/mysql-server/5.7/support-files/my-default.cnf.sh

It's quite clear there, the sql_mode has been set, which will satisfy most developers and users looking for an ACID compliant database with little tweaking, and a rough note on increasing or decreasing RAM needed for InnoDB. MariaDB now probably needs something similar for Galera, CONNET and TokuDB.

# http://sql-info.de/mysql/gotchas.html
# http://blog.ionelmc.ro/2014/12/28/terrible-choices-mysql/
# http://www.tocker.ca/2015/01/23/proposal-to-change-additional-defaults-in-mysql-5-7.html
# http://www.tocker.ca/2015/01/14/proposal-to-change-replication-and-innodb-settings-in-mysql-5-7.html
# http://www.tocker.ca/2015/02/24/proposal-to-change-additional-defaults-in-mysql-5-7-february-edition.html


Proposed new defaults:
{code}
innodb_autoinc_lock_mode            = 2         (was: 1)
innodb_buffer_pool_dump_at_shutdown = ON        (was: OFF)
innodb_buffer_pool_dump_pct         = 25        (was: 100)
innodb_buffer_pool_load_at_startup  = ON        (was: OFF)
innodb_checksum_algorithm           = CRC32     (was: INNODB)
innodb_file_format                  = Barracuda (was: Antelope)
innodb_file_format_max              = Barracuda (was: Antelope)
innodb_large_prefix                 = ON        (was: OFF)
innodb_log_compressed_pages         = ON        (was: OFF)
innodb_purge_threads                = 4         (was: 1)
innodb_strict_mode                  = ON        (was: OFF)

binlog_annotate_row_events          = ON        (was: OFF)
binlog_format                       = MIXED     (was: STATEMENT)
group_concat_max_len                = 1M        (was: 1025)
lock_wait_timeout                   = 86400(1day)(was: 1year)
log_slow_admin_statements           = ON        (was: OFF)
log_slow_slave_statements           = ON        (was: OFF)
log_warnings                        = 2         (was: 1)
max_allowed_packet                  = 16M       (was: 4M)
replicate_annotate_row_events       = ON        (was: OFF)
slave_net_timeout                   = 60        (was: 3600)
sync_binlog                         = 1         (was: 0)
aria_recover                        = FORCE, BACKUP (was: NORMAL)
myisam_recover_options              = FORCE, BACKUP (was: OFF)
{code}
 $acceptance criteria:$",,Richard Bensley,Richard Bensley,Blocker,43,,32,29,39,8,0,5,0,,0,850,25,0,0,2016-08-03 10:11:02,update defaults and simplify mysqld config parameters ,"Poor MySQL has received some negative feedback over the years ^1^ ^2^. As entertaining as it is watching and listening to developers struggle and whine, I think it's about time we changed some of the defaults and made the config easier to understand.

Oracle are working on changing more defaults in 5.7 ^3^ ^4^ ^5^, I think we should too.

Something else they have done, is to radically change the simplicity of the default server config file, /etc/my.cnf:
https://raw.githubusercontent.com/mysql/mysql-server/5.7/support-files/my-default.cnf.sh

It's quite clear there, the sql_mode has been set, which will satisfy most developers and users looking for an ACID compliant database with little tweaking, and a rough note on increasing or decreasing RAM needed for InnoDB. MariaDB now probably needs something similar for Galera, CONNET and TokuDB.

# http://sql-info.de/mysql/gotchas.html
# http://blog.ionelmc.ro/2014/12/28/terrible-choices-mysql/
# http://www.tocker.ca/2015/01/23/proposal-to-change-additional-defaults-in-mysql-5-7.html
# http://www.tocker.ca/2015/01/14/proposal-to-change-replication-and-innodb-settings-in-mysql-5-7.html
# http://www.tocker.ca/2015/02/24/proposal-to-change-additional-defaults-in-mysql-5-7-february-edition.html",,0,5,0,126,0.857143,"update defaults and simplify mysqld config parameters  $end$ Poor MySQL has received some negative feedback over the years ^1^ ^2^. As entertaining as it is watching and listening to developers struggle and whine, I think it's about time we changed some of the defaults and made the config easier to understand.

Oracle are working on changing more defaults in 5.7 ^3^ ^4^ ^5^, I think we should too.

Something else they have done, is to radically change the simplicity of the default server config file, /etc/my.cnf:
https://raw.githubusercontent.com/mysql/mysql-server/5.7/support-files/my-default.cnf.sh

It's quite clear there, the sql_mode has been set, which will satisfy most developers and users looking for an ACID compliant database with little tweaking, and a rough note on increasing or decreasing RAM needed for InnoDB. MariaDB now probably needs something similar for Galera, CONNET and TokuDB.

# http://sql-info.de/mysql/gotchas.html
# http://blog.ionelmc.ro/2014/12/28/terrible-choices-mysql/
# http://www.tocker.ca/2015/01/23/proposal-to-change-additional-defaults-in-mysql-5-7.html
# http://www.tocker.ca/2015/01/14/proposal-to-change-replication-and-innodb-settings-in-mysql-5-7.html
# http://www.tocker.ca/2015/02/24/proposal-to-change-additional-defaults-in-mysql-5-7-february-edition.html $acceptance criteria:$",5,1,1,1,1,1,1,12589.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
763,MDEV-7660,Task,MDEV,2015-03-04 12:03:28,,0,"MySQL WL#6671 ""Improve scalability by not using thr_lock.c locks for InnoDB tables""",,,"MySQL WL#6671 ""Improve scalability by not using thr_lock.c locks for InnoDB tables"" $end$ $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Blocker,47,,5,16,6,13,0,0,0,,0,850,0,0,0,,"MySQL WL#6671 ""Improve scalability by not using thr_lock.c locks for InnoDB tables""",,,0,0,0,0,0.0,"MySQL WL#6671 ""Improve scalability by not using thr_lock.c locks for InnoDB tables"" $end$ $acceptance criteria:$",0,0,0,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
764,MDEV-7769,Task,MDEV,2015-03-13 06:11:49,,0,MY_CHARSET_INFO refactoring,"Some functions in MY_CHARSET_HANDLER are not good enough and new more powerful functions have been added as replacements. This task is to clean-up MY_CHARSET_HANDLER, to remove the functions that have replacements.

We'll try to preserve API as much as possible, in case some plugins use the old functions (but ABI will change!).

1. Remove ismbchar() from MY_CHARSET_HANDLER:
bq. (This part was done under terms of MDEV-6353 (task) and  MDEV-9665 (subtask))
{code}
uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
{code}

and fix the code to use a new function added in 10.1 instead:

{code}
 int (*charlen)(CHARSET_INFO *cs, const uchar *str, const uchar *end);
{code}

charlen() is a more powerful replacement for ismbchar(), as it can additionally:

- distinguish between a valid single byte (return value 1) character vs a broken byte (return value 0)
- report incomplete characters (premature end-of-line) with return values MY_CS_TOOSMALXXX

For API compatibility purposes, the macros my_ismbchar() can be restored as a wrapper
function around cs->cset->charlen() instead of cs->cset->ismbchar(), something like this:

{code}
uint my_ismbchar(CHARSET_INFO *cs, const uchar *str, const uchar *end)
{
  int rc= cs->cset->charlen(cs, str, end);
  return rc < 2 ? 0 : rc;
}
{code}

2. Remove mbcharlen() from MY_CHARSET_HANDLER:
bq. (This part was done under terms of MDEV-6353 (task) and  MDEV-9665 (subtask), without the function {{byte_property}} though)

{code}
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

and add a new function added in 10.1 instead:

{code}
  uint    (*byte_property)(CHARSET_INFO *, uint c);
{code}

Which will return a combination of flags, e.g.:

- the byte is a stand-anlone valid character
- the byte is a MB2 head
- the byte is a MB3 head
- the byte is a MB4 head
- the byte is a MB5 head
- the byte is a MB2 tail
- the byte is a MB3 tail
- the byte is a MB4 tail
- the byte is a MB5 tail
- the byte is MB23 continuation (e.g. the second byte in a 3-byte character)
- the byte is MB24 continuation (e.g. the second byte in a 4-byte character)
- the byte is MB34 continuation (e.g. the third byte in a 4-byte character)
- the byte is MBxy continuation (for all possible x and y combinations)
- and maybe some other flags

For API compatibility purposes, the old macros my_mbcharlen() can be rewritten as a wrapper around cs->cset->byte_property().

3. Remove {{well_formed_len}}:
{code}
  size_t  (*well_formed_len)(CHARSET_INFO *,
                             const char *b,const char *e,
                             size_t nchars, int *error); 
{code}
and use a new function added in 10.1 instead:
{code}
  size_t (*well_formed_char_length)(CHARSET_INFO *cs,
                                    const char *str, const char *end,
                                    size_t nchars,
                                    MY_STRCOPY_STATUS *status);
{code}
The new function is a replacement for well_formed_len() and numchars() at the same time, it can return:
a. ""number of characters"" as a return value
b. ""number of bytes"" which is directly calculated from status->m_source_end_pos.
c. ""there are bad bytes"" in status->m_well_formed_error_pos, or NULL if no bad bytes.
",,"MY_CHARSET_INFO refactoring $end$ Some functions in MY_CHARSET_HANDLER are not good enough and new more powerful functions have been added as replacements. This task is to clean-up MY_CHARSET_HANDLER, to remove the functions that have replacements.

We'll try to preserve API as much as possible, in case some plugins use the old functions (but ABI will change!).

1. Remove ismbchar() from MY_CHARSET_HANDLER:
bq. (This part was done under terms of MDEV-6353 (task) and  MDEV-9665 (subtask))
{code}
uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
{code}

and fix the code to use a new function added in 10.1 instead:

{code}
 int (*charlen)(CHARSET_INFO *cs, const uchar *str, const uchar *end);
{code}

charlen() is a more powerful replacement for ismbchar(), as it can additionally:

- distinguish between a valid single byte (return value 1) character vs a broken byte (return value 0)
- report incomplete characters (premature end-of-line) with return values MY_CS_TOOSMALXXX

For API compatibility purposes, the macros my_ismbchar() can be restored as a wrapper
function around cs->cset->charlen() instead of cs->cset->ismbchar(), something like this:

{code}
uint my_ismbchar(CHARSET_INFO *cs, const uchar *str, const uchar *end)
{
  int rc= cs->cset->charlen(cs, str, end);
  return rc < 2 ? 0 : rc;
}
{code}

2. Remove mbcharlen() from MY_CHARSET_HANDLER:
bq. (This part was done under terms of MDEV-6353 (task) and  MDEV-9665 (subtask), without the function {{byte_property}} though)

{code}
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

and add a new function added in 10.1 instead:

{code}
  uint    (*byte_property)(CHARSET_INFO *, uint c);
{code}

Which will return a combination of flags, e.g.:

- the byte is a stand-anlone valid character
- the byte is a MB2 head
- the byte is a MB3 head
- the byte is a MB4 head
- the byte is a MB5 head
- the byte is a MB2 tail
- the byte is a MB3 tail
- the byte is a MB4 tail
- the byte is a MB5 tail
- the byte is MB23 continuation (e.g. the second byte in a 3-byte character)
- the byte is MB24 continuation (e.g. the second byte in a 4-byte character)
- the byte is MB34 continuation (e.g. the third byte in a 4-byte character)
- the byte is MBxy continuation (for all possible x and y combinations)
- and maybe some other flags

For API compatibility purposes, the old macros my_mbcharlen() can be rewritten as a wrapper around cs->cset->byte_property().

3. Remove {{well_formed_len}}:
{code}
  size_t  (*well_formed_len)(CHARSET_INFO *,
                             const char *b,const char *e,
                             size_t nchars, int *error); 
{code}
and use a new function added in 10.1 instead:
{code}
  size_t (*well_formed_char_length)(CHARSET_INFO *cs,
                                    const char *str, const char *end,
                                    size_t nchars,
                                    MY_STRCOPY_STATUS *status);
{code}
The new function is a replacement for well_formed_len() and numchars() at the same time, it can return:
a. ""number of characters"" as a return value
b. ""number of bytes"" which is directly calculated from status->m_source_end_pos.
c. ""there are bad bytes"" in status->m_well_formed_error_pos, or NULL if no bad bytes.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,17,,0,0,2,4,0,4,0,,0,850,0,0,0,2016-02-24 11:59:03,MY_CHARSET_INFO refactoring,"Some functions in MY_CHARSET_HANDLER are not good enough and new more powerful functions have been added as replacements. This task is to clean-up MY_CHARSET_HANDLER, to remove the functions that have replacements.

We'll try to preserve API as much as possible, in case some plugins use the old functions (but ABI will change!).

1. Remove mbcharlen() from MY_CHARSET_HANDLER:

{code}
uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
{code}

and fix the code to use a new function added in 10.1 instead:

{code}
 int (*charlen)(CHARSET_INFO *cs, const uchar *str, const uchar *end);
{code}

charlen() is a more powerful replacement for ismbchar(), as it can additionally:

- distinguish between a valid single byte (return value 1) character vs a broken byte (return value 0)
- report incomplete characters (premature end-of-line) with return values MY_CS_TOOSMALXXX

For API compatibility purposes, the macros my_ismbchar() can be restored as a wrapper
function around cs->cset->charlen() instead of cs->cset->ismbchar(), something like this:

{code}
uint my_ismbchar(CHARSET_INFO *cs, const uchar *str, const uchar *end)
{
  int rc= cs->cset->charlen(cs, str, end);
  return rc < 2 ? 0 : rc;
}
{code}

2. Remove mbcharlen() from MY_CHARSET_HANDLER:

{code}
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

and add a new function added in 10.1 instead:

{code}
  uint    (*byte_property)(CHARSET_INFO *, uint c);
{code}

Which will return a combination of flags, e.g.:

- the byte is a stand-anlone valid character
- the byte is a MB2 head
- the byte is a MB3 head
- the byte is a MB4 head
- the byte is a MB5 head
- the byte is a MB2 tail
- the byte is a MB3 tail
- the byte is a MB4 tail
- the byte is a MB5 tail
- the byte is MB23 continuation (e.g. the second byte in a 3-byte character)
- the byte is MB24 continuation (e.g. the second byte in a 4-byte character)
- the byte is MB34 continuation (e.g. the third byte in a 4-byte character)
- the byte is MBxy continuation (for all possible x and y combinations)
- and maybe some other flags

For API compatibility purposes, the old macros my_mbcharlen() can be rewritten as a wrapper around cs->cset->byte_property().

3. Remove two functions:
{code}
  size_t  (*well_formed_len)(CHARSET_INFO *,
                             const char *b,const char *e,
                             size_t nchars, int *error); 
  size_t  (*numchars)(CHARSET_INFO *, const char *b, const char *e);
{code}
and use a new function added in 10.1 instead:
{code}
  size_t (*well_formed_char_length)(CHARSET_INFO *cs,
                                    const char *str, const char *end,
                                    size_t nchars,
                                    MY_STRCOPY_STATUS *status);
{code}
The new function is a replacement for well_formed_len() and numchars() at the same time, it can return:
a. ""number of characters"" as a return value
b. ""number of bytes"" which is directly calculated from status->m_source_end_pos.
c. ""there are bad bytes"" in status->m_well_formed_error_pos, or NULL if no bad bytes.

The old functions could return this information, but in separate calls with double loops on the string.
",,0,4,0,62,0.125523,"MY_CHARSET_INFO refactoring $end$ Some functions in MY_CHARSET_HANDLER are not good enough and new more powerful functions have been added as replacements. This task is to clean-up MY_CHARSET_HANDLER, to remove the functions that have replacements.

We'll try to preserve API as much as possible, in case some plugins use the old functions (but ABI will change!).

1. Remove mbcharlen() from MY_CHARSET_HANDLER:

{code}
uint    (*ismbchar)(CHARSET_INFO *, const char *, const char *);
{code}

and fix the code to use a new function added in 10.1 instead:

{code}
 int (*charlen)(CHARSET_INFO *cs, const uchar *str, const uchar *end);
{code}

charlen() is a more powerful replacement for ismbchar(), as it can additionally:

- distinguish between a valid single byte (return value 1) character vs a broken byte (return value 0)
- report incomplete characters (premature end-of-line) with return values MY_CS_TOOSMALXXX

For API compatibility purposes, the macros my_ismbchar() can be restored as a wrapper
function around cs->cset->charlen() instead of cs->cset->ismbchar(), something like this:

{code}
uint my_ismbchar(CHARSET_INFO *cs, const uchar *str, const uchar *end)
{
  int rc= cs->cset->charlen(cs, str, end);
  return rc < 2 ? 0 : rc;
}
{code}

2. Remove mbcharlen() from MY_CHARSET_HANDLER:

{code}
  uint    (*mbcharlen)(CHARSET_INFO *, uint c);
{code}

and add a new function added in 10.1 instead:

{code}
  uint    (*byte_property)(CHARSET_INFO *, uint c);
{code}

Which will return a combination of flags, e.g.:

- the byte is a stand-anlone valid character
- the byte is a MB2 head
- the byte is a MB3 head
- the byte is a MB4 head
- the byte is a MB5 head
- the byte is a MB2 tail
- the byte is a MB3 tail
- the byte is a MB4 tail
- the byte is a MB5 tail
- the byte is MB23 continuation (e.g. the second byte in a 3-byte character)
- the byte is MB24 continuation (e.g. the second byte in a 4-byte character)
- the byte is MB34 continuation (e.g. the third byte in a 4-byte character)
- the byte is MBxy continuation (for all possible x and y combinations)
- and maybe some other flags

For API compatibility purposes, the old macros my_mbcharlen() can be rewritten as a wrapper around cs->cset->byte_property().

3. Remove two functions:
{code}
  size_t  (*well_formed_len)(CHARSET_INFO *,
                             const char *b,const char *e,
                             size_t nchars, int *error); 
  size_t  (*numchars)(CHARSET_INFO *, const char *b, const char *e);
{code}
and use a new function added in 10.1 instead:
{code}
  size_t (*well_formed_char_length)(CHARSET_INFO *cs,
                                    const char *str, const char *end,
                                    size_t nchars,
                                    MY_STRCOPY_STATUS *status);
{code}
The new function is a replacement for well_formed_len() and numchars() at the same time, it can return:
a. ""number of characters"" as a return value
b. ""number of bytes"" which is directly calculated from status->m_source_end_pos.
c. ""there are bad bytes"" in status->m_well_formed_error_pos, or NULL if no bad bytes.

The old functions could return this information, but in separate calls with double loops on the string.
 $acceptance criteria:$",4,1,1,1,1,1,1,8357.78,1,1,1.0,1,1.0,1,1.0,0,0.0,0,0.0
765,MDEV-7773,Task,MDEV,2015-03-13 13:40:30,,0,Aggregate stored functions,"With {{CREATE FUNCTION}} one can create functions in SQL, but this syntax doesn't allow one to create an aggregate function (like {{SUM}}, {{AVG}}, etc). This task is to add support for aggregate stored functions.

h2. Syntax

is not decided yet. SQL Standard 2003 doesn't support aggregate stored functions (may be the newer standard does). [Oracle|http://docs.oracle.com/cd/B19306_01/appdev.102/b14289/dciaggfns.htm], [PostgreSQL|http://www.postgresql.org/docs/8.3/static/sql-createaggregate.html], [HSQL|http://hsqldb.org/doc/guide/sqlroutines-chapt.html#src_aggregate_functions] (may be more) all implement aggregate stored functions using their own incompatible syntax extensions. SQL Server and DB2 do not support creating new aggregate stored functions in SQL.

The syntax should at least allow for
* the code to return the function value
* the code to be called per row in a group, it accumulates data, does not return a value
* storage that is preserved between calls within a group

and possibly
* code to invoke between groups to reset the storage
* code to *remove* data from the group (useful for window functions)",,"Aggregate stored functions $end$ With {{CREATE FUNCTION}} one can create functions in SQL, but this syntax doesn't allow one to create an aggregate function (like {{SUM}}, {{AVG}}, etc). This task is to add support for aggregate stored functions.

h2. Syntax

is not decided yet. SQL Standard 2003 doesn't support aggregate stored functions (may be the newer standard does). [Oracle|http://docs.oracle.com/cd/B19306_01/appdev.102/b14289/dciaggfns.htm], [PostgreSQL|http://www.postgresql.org/docs/8.3/static/sql-createaggregate.html], [HSQL|http://hsqldb.org/doc/guide/sqlroutines-chapt.html#src_aggregate_functions] (may be more) all implement aggregate stored functions using their own incompatible syntax extensions. SQL Server and DB2 do not support creating new aggregate stored functions in SQL.

The syntax should at least allow for
* the code to return the function value
* the code to be called per row in a group, it accumulates data, does not return a value
* storage that is preserved between calls within a group

and possibly
* code to invoke between groups to reset the storage
* code to *remove* data from the group (useful for window functions) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,27,,3,21,7,3,0,4,0,,0,850,4,4,0,2017-05-24 13:09:38,Aggregate stored functions,"With {{CREATE FUNCTION}} one can create functions in SQL, but this syntax doesn't allow one to create an aggregate function (like {{SUM}}, {{AVG}}, etc). This task is to add support for aggregate stored functions.

h2. Syntax

is not decided yet. SQL Standard 2003 doesn't support aggregate stored functions (may be the newer standard does). [Oracle|http://docs.oracle.com/cd/B19306_01/appdev.102/b14289/dciaggfns.htm], [PostgreSQL|http://www.postgresql.org/docs/8.3/static/sql-createaggregate.html], [HSQL|http://hsqldb.org/doc/guide/sqlroutines-chapt.html#src_aggregate_functions] (may be more) all implement aggregate stored functions using their own incompatible syntax extensions. SQL Server and DB2 do not support creating new aggregate stored functions in SQL.

The syntax should at least allow for
* the code to return the function value
* the code to be called per row in a group, it accumulates data, does not return a value
* storage that is preserved between calls within a group

and possibly
* code to invoke between groups to reset the storage
* code to *remove* data from the group (useful for window functions)",,0,0,0,0,0.0,"Aggregate stored functions $end$ With {{CREATE FUNCTION}} one can create functions in SQL, but this syntax doesn't allow one to create an aggregate function (like {{SUM}}, {{AVG}}, etc). This task is to add support for aggregate stored functions.

h2. Syntax

is not decided yet. SQL Standard 2003 doesn't support aggregate stored functions (may be the newer standard does). [Oracle|http://docs.oracle.com/cd/B19306_01/appdev.102/b14289/dciaggfns.htm], [PostgreSQL|http://www.postgresql.org/docs/8.3/static/sql-createaggregate.html], [HSQL|http://hsqldb.org/doc/guide/sqlroutines-chapt.html#src_aggregate_functions] (may be more) all implement aggregate stored functions using their own incompatible syntax extensions. SQL Server and DB2 do not support creating new aggregate stored functions in SQL.

The syntax should at least allow for
* the code to return the function value
* the code to be called per row in a group, it accumulates data, does not return a value
* storage that is preserved between calls within a group

and possibly
* code to invoke between groups to reset the storage
* code to *remove* data from the group (useful for window functions) $acceptance criteria:$",0,0,0,0,0,0,1,19271.5,7,3,0.428571,2,0.285714,2,0.285714,2,0.285714,2,0.285714
766,MDEV-7780,Task,MDEV,2015-03-14 17:03:10,,0,Support for faking server version,"The official MySQL server returns version string like this:
5.5.25-log
While (latest stable) MariaDB:
10.0.17-MariaDB-1~wheezy-log

Some old and crappy CMS installations break with the MariaDB string above, like Joomla til API17:
{code}
  public function hasUTF()
  {
    $verParts = explode('.', $this->getVersion());
    return ($verParts[0] == 5 || ($verParts[0] == 4 && $verParts[1] == 1 && (int)$verParts[2] >= 2));
  }
{code}
MariaDB is told to be a drop-in replacement for MySQL. In this case it is unfortunately not true. Patching the already installed webapps is usually not an option for administrators.

For this reason I recommend to add a new configuration option, something like this into mysqld section of the config file:
fake_version=5.5.25-log

With this option set, MariaDB could return the faked version string. 
",,"Support for faking server version $end$ The official MySQL server returns version string like this:
5.5.25-log
While (latest stable) MariaDB:
10.0.17-MariaDB-1~wheezy-log

Some old and crappy CMS installations break with the MariaDB string above, like Joomla til API17:
{code}
  public function hasUTF()
  {
    $verParts = explode('.', $this->getVersion());
    return ($verParts[0] == 5 || ($verParts[0] == 4 && $verParts[1] == 1 && (int)$verParts[2] >= 2));
  }
{code}
MariaDB is told to be a drop-in replacement for MySQL. In this case it is unfortunately not true. Patching the already installed webapps is usually not an option for administrators.

For this reason I recommend to add a new configuration option, something like this into mysqld section of the config file:
fake_version=5.5.25-log

With this option set, MariaDB could return the faked version string. 
 $acceptance criteria:$",,Imre Rad,Imre Rad,Minor,21,,6,6,8,1,0,3,0,,0,850,5,3,0,2015-11-18 11:01:06,Support for faking server version,"The official MySQL server returns version string like this:
5.5.25-log
While (latest stable) MariaDB:
10.0.17-MariaDB-1~wheezy-log

Some old and crappy CMS installations break with the MariaDB string above, like Joomla til API17:
{code}
  public function hasUTF()
  {
    $verParts = explode('.', $this->getVersion());
    return ($verParts[0] == 5 || ($verParts[0] == 4 && $verParts[1] == 1 && (int)$verParts[2] >= 2));
  }
{code}
MariaDB is told to be a drop-in replacement for MySQL. In this case it is unfortunately not true. Patching the already installed webapps is usually not an option for administrators.

For this reason I recommend to add a new configuration option, something like this into mysqld section of the config file:
fake_version=5.5.25-log

With this option set, MariaDB could return the faked version string. 
",,0,0,0,0,0.0,"Support for faking server version $end$ The official MySQL server returns version string like this:
5.5.25-log
While (latest stable) MariaDB:
10.0.17-MariaDB-1~wheezy-log

Some old and crappy CMS installations break with the MariaDB string above, like Joomla til API17:
{code}
  public function hasUTF()
  {
    $verParts = explode('.', $this->getVersion());
    return ($verParts[0] == 5 || ($verParts[0] == 4 && $verParts[1] == 1 && (int)$verParts[2] >= 2));
  }
{code}
MariaDB is told to be a drop-in replacement for MySQL. In this case it is unfortunately not true. Patching the already installed webapps is usually not an option for administrators.

For this reason I recommend to add a new configuration option, something like this into mysqld section of the config file:
fake_version=5.5.25-log

With this option set, MariaDB could return the faked version string. 
 $acceptance criteria:$",0,0,0,0,0,0,0,5969.95,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
767,MDEV-7832,Task,MDEV,2015-03-25 14:58:08,,0,Add status variables to track CREATE TEMPORARY TABLE and DROP TEMPORARY TABLE,"Some users would find it helpful to track temporary tables created and dropped by their application (not those created internally to MariaDB). Can we add status variables like the following?:

Com_create_temp_table
Com_drop_temp_table",,"Add status variables to track CREATE TEMPORARY TABLE and DROP TEMPORARY TABLE $end$ Some users would find it helpful to track temporary tables created and dropped by their application (not those created internally to MariaDB). Can we add status variables like the following?:

Com_create_temp_table
Com_drop_temp_table $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,10,,0,4,0,1,0,0,0,,0,850,3,0,0,2015-06-17 20:10:44,Add status variables to track CREATE TEMPORARY TABLE and DROP TEMPORARY TABLE,"Some users would find it helpful to track temporary tables created and dropped by their application (not those created internally to MariaDB). Can we add status variables like the following?:

Com_create_temp_table
Com_drop_temp_table",,0,0,0,0,0.0,"Add status variables to track CREATE TEMPORARY TABLE and DROP TEMPORARY TABLE $end$ Some users would find it helpful to track temporary tables created and dropped by their application (not those created internally to MariaDB). Can we add status variables like the following?:

Com_create_temp_table
Com_drop_temp_table $acceptance criteria:$",0,0,0,0,0,0,0,2021.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
768,MDEV-7901,Task,MDEV,2015-04-03 08:01:31,,0,re-implement analyze table for low impact,"MDEV-7829 and MDEV-7363 where attempts to work around the implementation of analyze table because holding a read lock only on the entire table while gathers statistics. This can be up to an hour or so on larger tables as witnessed in production databases. This level of total read only on a table isn't acceptable.

MDEV-7196 suggested by [~jplindst] indicated that perhaps a full scan is more appropriate to give better statistics. Obviously this isn't going to help the locking scenario at all.

So perhaps, instead of locking the entire table, the analyze table takes a full table scan in incremental blocks. These block sizes should try to scale to something that keeps the lock time less that a goal time, a new variable max_analyze_table_lock_time (locks can be dropped in analyze table gets there too soon). Analyze table being a long running operation should drop to read committed (or uncommitted?) mode to keep the undo logs from growing because of the intensive read operations.",,"re-implement analyze table for low impact $end$ MDEV-7829 and MDEV-7363 where attempts to work around the implementation of analyze table because holding a read lock only on the entire table while gathers statistics. This can be up to an hour or so on larger tables as witnessed in production databases. This level of total read only on a table isn't acceptable.

MDEV-7196 suggested by [~jplindst] indicated that perhaps a full scan is more appropriate to give better statistics. Obviously this isn't going to help the locking scenario at all.

So perhaps, instead of locking the entire table, the analyze table takes a full table scan in incremental blocks. These block sizes should try to scale to something that keeps the lock time less that a goal time, a new variable max_analyze_table_lock_time (locks can be dropped in analyze table gets there too soon). Analyze table being a long running operation should drop to read committed (or uncommitted?) mode to keep the undo logs from growing because of the intensive read operations. $acceptance criteria:$",,Daniel Black,Daniel Black,Major,50,,4,13,6,6,0,0,0,,0,850,11,0,0,2015-06-17 11:59:32,re-implement analyze table for low impact,"MDEV-7829 and MDEV-7363 where attempts to work around the implementation of analyze table because holding a read lock only on the entire table while gathers statistics. This can be up to an hour or so on larger tables as witnessed in production databases. This level of total read only on a table isn't acceptable.

MDEV-7196 suggested by [~jplindst] indicated that perhaps a full scan is more appropriate to give better statistics. Obviously this isn't going to help the locking scenario at all.

So perhaps, instead of locking the entire table, the analyze table takes a full table scan in incremental blocks. These block sizes should try to scale to something that keeps the lock time less that a goal time, a new variable max_analyze_table_lock_time (locks can be dropped in analyze table gets there too soon). Analyze table being a long running operation should drop to read committed (or uncommitted?) mode to keep the undo logs from growing because of the intensive read operations.",,0,0,0,0,0.0,"re-implement analyze table for low impact $end$ MDEV-7829 and MDEV-7363 where attempts to work around the implementation of analyze table because holding a read lock only on the entire table while gathers statistics. This can be up to an hour or so on larger tables as witnessed in production databases. This level of total read only on a table isn't acceptable.

MDEV-7196 suggested by [~jplindst] indicated that perhaps a full scan is more appropriate to give better statistics. Obviously this isn't going to help the locking scenario at all.

So perhaps, instead of locking the entire table, the analyze table takes a full table scan in incremental blocks. These block sizes should try to scale to something that keeps the lock time less that a goal time, a new variable max_analyze_table_lock_time (locks can be dropped in analyze table gets there too soon). Analyze table being a long running operation should drop to read committed (or uncommitted?) mode to keep the undo logs from growing because of the intensive read operations. $acceptance criteria:$",0,0,0,0,0,0,1,1803.97,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
769,MDEV-7937,Task,MDEV,2015-04-08 20:54:04,,0,Enforce SSL when --ssl client option is used,"--ssl client options are currently ""advisory"".

Fixed in 5.7.3 : http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-3.html

https://bugzilla.suse.com/show_bug.cgi?id=924663",,"Enforce SSL when --ssl client option is used $end$ --ssl client options are currently ""advisory"".

Fixed in 5.7.3 : http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-3.html

https://bugzilla.suse.com/show_bug.cgi?id=924663 $acceptance criteria:$",,Nirbhay Choubey,Nirbhay Choubey,Critical,17,,1,4,1,1,0,0,0,,0,850,2,0,0,2015-06-02 17:32:46,Enforce SSL when --ssl client option is used,"--ssl client options are currently ""advisory"".

Fixed in 5.7.3 : http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-3.html

https://bugzilla.suse.com/show_bug.cgi?id=924663",,0,0,0,0,0.0,"Enforce SSL when --ssl client option is used $end$ --ssl client options are currently ""advisory"".

Fixed in 5.7.3 : http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-3.html

https://bugzilla.suse.com/show_bug.cgi?id=924663 $acceptance criteria:$",0,0,0,0,0,0,0,1316.63,2,1,0.5,0,0.0,0,0.0,0,0.0,0,0.0
770,MDEV-7978,Task,MDEV,2015-04-11 09:46:36,,0,CREATE/ALTER USER as in 5.7,"MySQL 5.7 has extended CREATE/ALTER USER syntax to make a better distinction between privilege management (GRANT/REVOKE) and account management (clauses like IDENTIFIED, REQUIRE SSL, MAX_QUERIES_PER_HOUR, etc). And there's SHOW CREATE USER.

We should support this syntax too.

Don't forget consistent use of IF [NOT] EXISTS",,"CREATE/ALTER USER as in 5.7 $end$ MySQL 5.7 has extended CREATE/ALTER USER syntax to make a better distinction between privilege management (GRANT/REVOKE) and account management (clauses like IDENTIFIED, REQUIRE SSL, MAX_QUERIES_PER_HOUR, etc). And there's SHOW CREATE USER.

We should support this syntax too.

Don't forget consistent use of IF [NOT] EXISTS $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,27,,1,2,2,6,0,2,0,,0,850,2,2,0,2015-11-17 17:55:18,CREATE/ALTER USER as in 5.7,"MySQL 5.7 has extended CREATE/ALTER USER syntax to make a better distinction between privilege management (GRANT/REVOKE) and account management (clauses like IDENTIFIED, REQUIRE SSL, MAX_QUERIES_PER_HOUR, etc). And there's SHOW CREATE USER.

We should support this syntax too.

Don't forget consistent use of IF [NOT] EXISTS",,0,0,0,0,0.0,"CREATE/ALTER USER as in 5.7 $end$ MySQL 5.7 has extended CREATE/ALTER USER syntax to make a better distinction between privilege management (GRANT/REVOKE) and account management (clauses like IDENTIFIED, REQUIRE SSL, MAX_QUERIES_PER_HOUR, etc). And there's SHOW CREATE USER.

We should support this syntax too.

Don't forget consistent use of IF [NOT] EXISTS $acceptance criteria:$",0,0,0,0,0,0,1,5288.13,8,3,0.375,2,0.25,2,0.25,2,0.25,2,0.25
771,MDEV-8010,Task,MDEV,2015-04-17 14:16:37,MDEV-7941,0,Avoid sql_alloc() in Items,"sql_alloc() has additional costs compared to direct mem_root allocation:
- function call: it is defined in a separate translation unit and can't be inlined
- it needs to call pthread_getspecific() to get THD::mem_root

It is called dozens of times implicitly by Items. Try to get rid of those calls.",,"Avoid sql_alloc() in Items $end$ sql_alloc() has additional costs compared to direct mem_root allocation:
- function call: it is defined in a separate translation unit and can't be inlined
- it needs to call pthread_getspecific() to get THD::mem_root

It is called dozens of times implicitly by Items. Try to get rid of those calls. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,13,,0,3,1,1,0,0,0,,0,850,0,0,0,2015-08-19 17:29:40,Avoid sql_alloc() in Items,"sql_alloc() has additional costs compared to direct mem_root allocation:
- function call: it is defined in a separate translation unit and can't be inlined
- it needs to call pthread_getspecific() to get THD::mem_root

It is called dozens of times implicitly by Items. Try to get rid of those calls.",,0,0,0,0,0.0,"Avoid sql_alloc() in Items $end$ sql_alloc() has additional costs compared to direct mem_root allocation:
- function call: it is defined in a separate translation unit and can't be inlined
- it needs to call pthread_getspecific() to get THD::mem_root

It is called dozens of times implicitly by Items. Try to get rid of those calls. $acceptance criteria:$",0,0,0,0,0,0,0,2979.22,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
772,MDEV-8091,Task,MDEV,2015-05-02 22:14:35,,0,Simple window functions,"There is a class of window functions that can be computed on the fly, after ordering. These functions are:
* -rank-  (DONE)
* -dense_rank- (DONE)
* -row_number- (DONE)
* -first_value (this is frame-based)- (DONE)

These functions can be computed directly. In order to do this we must:
# Sort the rows.
# Detect partition boundaries (on the fly as well)
# Given partition boundaries, compute the corresponding function value, for each row.

Two-pass window functions:
* -percent_rank (test implementation needs review)-
* -cume_dist  (see MDEV-9746)-
* -ntile-
* -nth_value  (this is frame-based)- (DONE)
* -last_value  (this is frame-based)- (DONE)
these require two passes over partition to compute. The extra information that we require is the number of rows in the partition. In order to find the number of rows, we must first detect partition boundaries and then we can compute the number of rows per partition.

Two-cursor window functions:
* -lag- (DONE)
* -lead- (DONE)
these require an additional cursor that is traveling n rows ahead/behind the current_row cursor.

Key pieces for implementing this:
* Make use of the filesort interface, since we do not need temporary tables for these functions.
* It is a very similar use case to the GROUP BY statement. An important task is figuring out partition boundaries. The classes used for computing GROUP BY, might prove useful.",,"Simple window functions $end$ There is a class of window functions that can be computed on the fly, after ordering. These functions are:
* -rank-  (DONE)
* -dense_rank- (DONE)
* -row_number- (DONE)
* -first_value (this is frame-based)- (DONE)

These functions can be computed directly. In order to do this we must:
# Sort the rows.
# Detect partition boundaries (on the fly as well)
# Given partition boundaries, compute the corresponding function value, for each row.

Two-pass window functions:
* -percent_rank (test implementation needs review)-
* -cume_dist  (see MDEV-9746)-
* -ntile-
* -nth_value  (this is frame-based)- (DONE)
* -last_value  (this is frame-based)- (DONE)
these require two passes over partition to compute. The extra information that we require is the number of rows in the partition. In order to find the number of rows, we must first detect partition boundaries and then we can compute the number of rows per partition.

Two-cursor window functions:
* -lag- (DONE)
* -lead- (DONE)
these require an additional cursor that is traveling n rows ahead/behind the current_row cursor.

Key pieces for implementing this:
* Make use of the filesort interface, since we do not need temporary tables for these functions.
* It is a very similar use case to the GROUP BY statement. An important task is figuring out partition boundaries. The classes used for computing GROUP BY, might prove useful. $acceptance criteria:$",,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,37,,0,3,1,8,0,12,1,,0,850,2,0,0,2015-11-17 17:51:20,Simple window functions,"There is a class of window functions that can be computed on the fly, after ordering. These functions are:
* rank
* dense_rank
* row_number

These functions can be computed directly. In order to do this we must:
# Sort the rows.
# Detect partition boundaries (on the fly as well)
# Given partition boundaries, compute the corresponding function value, for each row.

A second set of functions:
* percent_rank
* ntile
Can be computed given two passes. The extra information that we require is the number of rows in the partition. In order to find the number of rows, we must first detect partition boundaries and then we can compute the number of rows per partition.

Key pieces for implementing this:
* Make use of the filesort interface, since we do not need temporary tables for these functions.
* It is a very similar use case to the GROUP BY statement. An important task is figuring out partition boundaries. The classes used for computing GROUP BY, might prove useful.
",,0,12,0,80,0.380682,"Simple window functions $end$ There is a class of window functions that can be computed on the fly, after ordering. These functions are:
* rank
* dense_rank
* row_number

These functions can be computed directly. In order to do this we must:
# Sort the rows.
# Detect partition boundaries (on the fly as well)
# Given partition boundaries, compute the corresponding function value, for each row.

A second set of functions:
* percent_rank
* ntile
Can be computed given two passes. The extra information that we require is the number of rows in the partition. In order to find the number of rows, we must first detect partition boundaries and then we can compute the number of rows per partition.

Key pieces for implementing this:
* Make use of the filesort interface, since we do not need temporary tables for these functions.
* It is a very similar use case to the GROUP BY statement. An important task is figuring out partition boundaries. The classes used for computing GROUP BY, might prove useful.
 $acceptance criteria:$",12,1,1,1,1,1,1,4771.6,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
773,MDEV-8092,Task,MDEV,2015-05-04 07:13:41,,0,"Change Column_definition::field_name from ""const char *"" to LEX_CSTRING","This is needed as a pre-requisite for a few other tasks.
Create_field::field_name and Field::field_name will be changed from ""const char *"" to a LEX_CSTRNG-alike structure, so both pointer and length are stored.

This will help to:
1. Avoid strlen() in some cases
2. Put typical operations that are done on field names into method, to reuse the code (e.g. name comparision)
3. Improve performance of name comparison. Currently field names are compared as follows:
{code}
  if (!my_strcasecmp(system_charset_info, field->field_name, other->field_name))
  {
   // handle fields with equal names
  }
{code}
If we store length, the method comparing names could immediately return false if lengths are different.
4. Simplify sql_yacc.yy, so data in various formats (LEX_CSTRING, LEX_STRING, const char *) will be easier to reuse.
   (in separate tasks)
5. Make Field and Create_field closer to sp_variable, which is useful for the data type handlers. All these three classes could share some members and methods.

Note, under terms of this task we won't change Item::name. It still will be of type ""const char *"".
That means that we'll have to use strlen(item->name) when creating a Create_field or a Field from Item.
This is Okey for now, in long terms we'll change Item::name as well and get rid of strlen().
",,"Change Column_definition::field_name from ""const char *"" to LEX_CSTRING $end$ This is needed as a pre-requisite for a few other tasks.
Create_field::field_name and Field::field_name will be changed from ""const char *"" to a LEX_CSTRNG-alike structure, so both pointer and length are stored.

This will help to:
1. Avoid strlen() in some cases
2. Put typical operations that are done on field names into method, to reuse the code (e.g. name comparision)
3. Improve performance of name comparison. Currently field names are compared as follows:
{code}
  if (!my_strcasecmp(system_charset_info, field->field_name, other->field_name))
  {
   // handle fields with equal names
  }
{code}
If we store length, the method comparing names could immediately return false if lengths are different.
4. Simplify sql_yacc.yy, so data in various formats (LEX_CSTRING, LEX_STRING, const char *) will be easier to reuse.
   (in separate tasks)
5. Make Field and Create_field closer to sp_variable, which is useful for the data type handlers. All these three classes could share some members and methods.

Note, under terms of this task we won't change Item::name. It still will be of type ""const char *"".
That means that we'll have to use strlen(item->name) when creating a Create_field or a Field from Item.
This is Okey for now, in long terms we'll change Item::name as well and get rid of strlen().
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,21,,0,1,2,1,0,5,0,,0,850,1,0,0,2015-11-18 11:01:22,"Change Create_field::field from ""const char *"" to LEX_CSTRING","This is needed as a pre-requisite for a few other tasks.

Note, under terms of this task we won't change Field::field_name in the same way. It still will be of type ""const char *"".
That means that we'll have to use strlen(field->field_name) when creating a Create_field from a Field, in mysql_prepare_alter_table()  create_table_from_items().
But this is Okey for now, in long terms we'll change Field::field_name as well and get rid of strlen().
",,1,4,0,164,1.89024,"Change Create_field::field from ""const char *"" to LEX_CSTRING $end$ This is needed as a pre-requisite for a few other tasks.

Note, under terms of this task we won't change Field::field_name in the same way. It still will be of type ""const char *"".
That means that we'll have to use strlen(field->field_name) when creating a Create_field from a Field, in mysql_prepare_alter_table()  create_table_from_items().
But this is Okey for now, in long terms we'll change Field::field_name as well and get rid of strlen().
 $acceptance criteria:$",5,1,1,1,1,1,1,4755.78,2,2,1.0,2,1.0,2,1.0,1,0.5,1,0.5
774,MDEV-8111,Task,MDEV,2015-05-06 18:00:43,,0,"remove ""fast mutexes""","They aren't faster than normal mutexes. They're disabled by default for years, so de facto it's dead code, never used.",,"remove ""fast mutexes"" $end$ They aren't faster than normal mutexes. They're disabled by default for years, so de facto it's dead code, never used. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,9,,0,3,0,1,0,0,0,,0,850,3,0,0,2015-11-18 11:02:00,"remove ""fast mutexes""","They aren't faster than normal mutexes. They're disabled by default for years, so de facto it's dead code, never used.",,0,0,0,0,0.0,"remove ""fast mutexes"" $end$ They aren't faster than normal mutexes. They're disabled by default for years, so de facto it's dead code, never used. $acceptance criteria:$",0,0,0,0,0,0,0,4697.02,9,3,0.333333,2,0.222222,2,0.222222,2,0.222222,2,0.222222
775,MDEV-8183,Task,MDEV,2015-05-19 16:13:38,,0,Adding option mysqldump --no-data-med,"Some users have reported differences moving from FederatedX to Spider , mysqldump is not exporting rows from federated. 

{code}
if (!opt_no_data &&
       (!my_strcasecmp(&my_charset_latin1, table_type, ""MRG_MyISAM"") ||
        !strcmp(table_type,""MRG_ISAM"") ||
        !strcmp(table_type,""CONNECT"") ||
        !strcmp(table_type,""FEDERATED"")))
     result= IGNORE_DATA;
 } 
{code}

There is probably a better way to list all med engine directly in the code, can't we add a flag to storage engine API  

This task would add OQGRAPH and SPIDER test and an additional option  

'm' short option is not use 

{code}
 {""no-data-med"", 'm', ""No row information for MED storage."", &opt_no_data_med,
   &opt_no_data-med, 0, GET_BOOL, NO_ARG, 1, 0, 0, 0, 0, 0},
{code}

 The default would not fetch federated rows

",,"Adding option mysqldump --no-data-med $end$ Some users have reported differences moving from FederatedX to Spider , mysqldump is not exporting rows from federated. 

{code}
if (!opt_no_data &&
       (!my_strcasecmp(&my_charset_latin1, table_type, ""MRG_MyISAM"") ||
        !strcmp(table_type,""MRG_ISAM"") ||
        !strcmp(table_type,""CONNECT"") ||
        !strcmp(table_type,""FEDERATED"")))
     result= IGNORE_DATA;
 } 
{code}

There is probably a better way to list all med engine directly in the code, can't we add a flag to storage engine API  

This task would add OQGRAPH and SPIDER test and an additional option  

'm' short option is not use 

{code}
 {""no-data-med"", 'm', ""No row information for MED storage."", &opt_no_data_med,
   &opt_no_data-med, 0, GET_BOOL, NO_ARG, 1, 0, 0, 0, 0, 0},
{code}

 The default would not fetch federated rows

 $acceptance criteria:$",,VAROQUI Stephane,VAROQUI Stephane,Major,18,,0,1,0,1,0,1,0,,0,850,0,0,0,2015-06-09 17:01:37,Adding option mysqldump ---no-data-med,"Some users have reported differences moving from FederatedX to Spider , mysqldump is not exporting rows from federated. 

{code}
if (!opt_no_data &&
       (!my_strcasecmp(&my_charset_latin1, table_type, ""MRG_MyISAM"") ||
        !strcmp(table_type,""MRG_ISAM"") ||
        !strcmp(table_type,""CONNECT"") ||
        !strcmp(table_type,""FEDERATED"")))
     result= IGNORE_DATA;
 } 
{code}

There is probably a better way to list all med engine directly in the code, can't we add a flag to storage engine API  

This task would add OQGRAPH and SPIDER test and an additional option  

'm' short option is not use 

{code}
 {""no-data-med"", 'm', ""No row information for MED storage."", &opt_no_data_med,
   &opt_no_data-med, 0, GET_BOOL, NO_ARG, 1, 0, 0, 0, 0, 0},
{code}

 The default would not fetch federated rows

",,1,0,0,2,0.00892857,"Adding option mysqldump ---no-data-med $end$ Some users have reported differences moving from FederatedX to Spider , mysqldump is not exporting rows from federated. 

{code}
if (!opt_no_data &&
       (!my_strcasecmp(&my_charset_latin1, table_type, ""MRG_MyISAM"") ||
        !strcmp(table_type,""MRG_ISAM"") ||
        !strcmp(table_type,""CONNECT"") ||
        !strcmp(table_type,""FEDERATED"")))
     result= IGNORE_DATA;
 } 
{code}

There is probably a better way to list all med engine directly in the code, can't we add a flag to storage engine API  

This task would add OQGRAPH and SPIDER test and an additional option  

'm' short option is not use 

{code}
 {""no-data-med"", 'm', ""No row information for MED storage."", &opt_no_data_med,
   &opt_no_data-med, 0, GET_BOOL, NO_ARG, 1, 0, 0, 0, 0, 0},
{code}

 The default would not fetch federated rows

 $acceptance criteria:$",1,1,0,0,0,0,0,504.783,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
776,MDEV-8199,Task,MDEV,2015-05-21 11:30:24,MDEV-7941,0,first_breadth_first_tab() takes 0.07% in OLTP RO,"Even though first_breadth_first_tab() is trivial, it is not inlined (likely due to it's non-static status). Also it has excessive condition, which all callers may eliminate in advance.",,"first_breadth_first_tab() takes 0.07% in OLTP RO $end$ Even though first_breadth_first_tab() is trivial, it is not inlined (likely due to it's non-static status). Also it has excessive condition, which all callers may eliminate in advance. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,10,,0,3,0,1,0,0,0,,0,850,1,0,0,2015-06-17 09:36:04,first_breadth_first_tab() takes 0.07% in OLTP RO,"Even though first_breadth_first_tab() is trivial, it is not inlined (likely due to it's non-static status). Also it has excessive condition, which all callers may eliminate in advance.",,0,0,0,0,0.0,"first_breadth_first_tab() takes 0.07% in OLTP RO $end$ Even though first_breadth_first_tab() is trivial, it is not inlined (likely due to it's non-static status). Also it has excessive condition, which all callers may eliminate in advance. $acceptance criteria:$",0,0,0,0,0,0,0,646.083,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
777,MDEV-8214,Task,MDEV,2015-05-22 17:36:34,,0,"Asian MB2 charsets: compare broken bytes as ""greater than any non-broken character""","This is a subtask for  MDEV-8036 that covers Asian character sets that have mbmaxlen==2:
- big5
- cp932
- euckr
- gb2312
- gbk
- sjis
",,"Asian MB2 charsets: compare broken bytes as ""greater than any non-broken character"" $end$ This is a subtask for  MDEV-8036 that covers Asian character sets that have mbmaxlen==2:
- big5
- cp932
- euckr
- gb2312
- gbk
- sjis
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,14,,1,0,2,1,0,0,0,,0,850,0,0,0,2015-06-17 20:12:49,"Asian MB2 charsets: compare broken bytes as ""greater than any non-broken character""","This is a subtask for  MDEV-8036 that covers Asian character sets that have mbmaxlen==2:
- big5
- cp932
- euckr
- gb2312
- gbk
- sjis
",,0,0,0,0,0.0,"Asian MB2 charsets: compare broken bytes as ""greater than any non-broken character"" $end$ This is a subtask for  MDEV-8036 that covers Asian character sets that have mbmaxlen==2:
- big5
- cp932
- euckr
- gb2312
- gbk
- sjis
 $acceptance criteria:$",0,0,0,0,0,0,0,626.6,3,3,1.0,3,1.0,3,1.0,2,0.666667,2,0.666667
778,MDEV-8259,Task,MDEV,2015-06-02 17:38:21,,0,5.5.44 merge,"mysql-5.5 → 5.5
xtradb → 5.5
tokudb → 5.5",,"5.5.44 merge $end$ mysql-5.5 → 5.5
xtradb → 5.5
tokudb → 5.5 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,4,,1,0,1,1,0,0,0,,0,850,0,0,0,2015-06-02 18:28:20,5.5.44 merge,"mysql-5.5 → 5.5
xtradb → 5.5
tokudb → 5.5",,0,0,0,0,0.0,"5.5.44 merge $end$ mysql-5.5 → 5.5
xtradb → 5.5
tokudb → 5.5 $acceptance criteria:$",0,0,0,0,0,0,0,0.816667,10,3,0.3,2,0.2,2,0.2,2,0.2,2,0.2
779,MDEV-8264,Task,MDEV,2015-06-03 19:17:33,,0,encryption for binlog,,,encryption for binlog $end$ $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Blocker,14,,0,0,1,3,0,0,0,,0,850,0,0,0,2015-06-17 20:16:15,encryption for binlog,,,0,0,0,0,0.0,encryption for binlog $end$ $acceptance criteria:$,0,0,0,0,0,0,1,336.967,11,3,0.272727,2,0.181818,2,0.181818,2,0.181818,2,0.181818
780,MDEV-8290,Task,MDEV,2015-06-09 18:19:35,,0,10.0.20 merge,"* 5.5 (/) 5.5.44
* InnoDB (/) 5.6.25
* XtraDB (/) 5.6.24-72.2
* P_S (/) 5.6.25
* Connect (/)
* Spider (/) _nothing to do_
* pcre (/) _nothing to do_
* Mroonga (/) _nothing to do_",,"10.0.20 merge $end$ * 5.5 (/) 5.5.44
* InnoDB (/) 5.6.25
* XtraDB (/) 5.6.24-72.2
* P_S (/) 5.6.25
* Connect (/)
* Spider (/) _nothing to do_
* pcre (/) _nothing to do_
* Mroonga (/) _nothing to do_ $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,11,,1,0,1,1,0,4,0,,0,850,0,0,0,2015-06-09 18:19:52,10.0.20 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* pcre
* Mroonga

",,0,4,0,21,1.0,"10.0.20 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* pcre
* Mroonga

 $acceptance criteria:$",4,1,1,1,1,1,1,0.0,12,3,0.25,2,0.166667,2,0.166667,2,0.166667,2,0.166667
781,MDEV-8320,Task,MDEV,2015-06-16 09:24:58,,0,Allow index usage for DATE(datetime_column) = const,"Would it be possible to make DATE() on datetime column sargable in some cases?
Like rewrite ""DATE(col) = const"" to ""col BETWEEN concat(const, ' 00:00:00') AND concat(const, ' 23:59:59')""

Other similar cases:
- ""YEAR(col) = const"" is almost the same
- ""YEAR(col) = c1 AND MONTH(col) = c2"" and other such combinations may be too complex to be worth it.",,"Allow index usage for DATE(datetime_column) = const $end$ Would it be possible to make DATE() on datetime column sargable in some cases?
Like rewrite ""DATE(col) = const"" to ""col BETWEEN concat(const, ' 00:00:00') AND concat(const, ' 23:59:59')""

Other similar cases:
- ""YEAR(col) = const"" is almost the same
- ""YEAR(col) = c1 AND MONTH(col) = c2"" and other such combinations may be too complex to be worth it. $acceptance criteria:$",,Jiri Kavalik,Jiri Kavalik,Blocker,42,,3,32,6,8,0,1,0,,0,850,0,1,0,,Allow index usage for DATE(datetime_column) = const,"Would it be possible to make DATE() on datetime column sargable in some cases?
Like rewrite ""DATE(col) = const"" to ""col BETWEEN concat(const, ' 00:00:00') AND concat(const, ' 23:59:59')""

Other similar cases:
- ""YEAR(col) = const"" is almost the same
- ""YEAR(col) = c1 AND MONTH(col) = c2"" and other such combinations may be too complex to be worth it.",,0,0,0,0,0.0,"Allow index usage for DATE(datetime_column) = const $end$ Would it be possible to make DATE() on datetime column sargable in some cases?
Like rewrite ""DATE(col) = const"" to ""col BETWEEN concat(const, ' 00:00:00') AND concat(const, ' 23:59:59')""

Other similar cases:
- ""YEAR(col) = const"" is almost the same
- ""YEAR(col) = c1 AND MONTH(col) = c2"" and other such combinations may be too complex to be worth it. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
782,MDEV-8348,Task,MDEV,2015-06-22 07:49:46,MDEV-10872,0,Add catchall to all table partitioning for list partitions,"Not all table partitioning has the ability to add a so called ""catch all"". The catch all is present in range partitioning:
partition last values less than(MAXVALUE)

The proposal is to add similar statements to list partitioning: Proposed syntax:
partition last values not in other

Benefits: No more insert exceptions, giving the DBA more choices in applying this partitioning with less effort.

Oracle uses the following syntax for this:

CREATE TABLE h2 (c1 NUMBER,c2 NUMBER)
PARTITION BY LIST(c1) (
PARTITION p0 VALUES (1, 4, 7),
PARTITION p1 VALUES (2, 5, 8),
PARTITION p3 VALUES(DEFAULT)

We should probably use the DEFAULT syntax for this.
",,"Add catchall to all table partitioning for list partitions $end$ Not all table partitioning has the ability to add a so called ""catch all"". The catch all is present in range partitioning:
partition last values less than(MAXVALUE)

The proposal is to add similar statements to list partitioning: Proposed syntax:
partition last values not in other

Benefits: No more insert exceptions, giving the DBA more choices in applying this partitioning with less effort.

Oracle uses the following syntax for this:

CREATE TABLE h2 (c1 NUMBER,c2 NUMBER)
PARTITION BY LIST(c1) (
PARTITION p0 VALUES (1, 4, 7),
PARTITION p1 VALUES (2, 5, 8),
PARTITION p3 VALUES(DEFAULT)

We should probably use the DEFAULT syntax for this.
 $acceptance criteria:$",,Norbert van Nobelen,Norbert van Nobelen,Minor,21,,4,9,7,2,0,1,0,,0,850,9,1,0,2016-08-23 20:05:51,Add catchall to all table partitioning for list partitions,"Not all table partitioning has the ability to add a so called ""catch all"". The catch all is present in range partitioning:
partition last values less than(MAXVALUE)

The proposal is to add similar statements to list partitioning: Proposed syntax:
partition last values not in other

Benefits: No more insert exceptions, giving the DBA more choices in applying this partitioning with less effort.

Oracle uses the following syntax for this:

CREATE TABLE h2 (c1 NUMBER,c2 NUMBER)
PARTITION BY LIST(c1) (
PARTITION p0 VALUES (1, 4, 7),
PARTITION p1 VALUES (2, 5, 8),
PARTITION p3 VALUES(DEFAULT)

We should probably use the DEFAULT syntax for this.
",,0,0,0,0,0.0,"Add catchall to all table partitioning for list partitions $end$ Not all table partitioning has the ability to add a so called ""catch all"". The catch all is present in range partitioning:
partition last values less than(MAXVALUE)

The proposal is to add similar statements to list partitioning: Proposed syntax:
partition last values not in other

Benefits: No more insert exceptions, giving the DBA more choices in applying this partitioning with less effort.

Oracle uses the following syntax for this:

CREATE TABLE h2 (c1 NUMBER,c2 NUMBER)
PARTITION BY LIST(c1) (
PARTITION p0 VALUES (1, 4, 7),
PARTITION p1 VALUES (2, 5, 8),
PARTITION p3 VALUES(DEFAULT)

We should probably use the DEFAULT syntax for this.
 $acceptance criteria:$",0,0,0,0,0,0,1,10284.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
783,MDEV-8352,Task,MDEV,2015-06-22 15:17:41,,0,Increase Diffie-Helman modulus to 2048-bits,"Debian reported a bug in an older version of MariaDB relating to using a 512-bit modulus when
negotiating a Finite-Field Diffie-Hellman Ephemeral (FFDHE) handshake in TLS.

This was increased to 1024 in 10.0.18, but MySQL increased this to 2048 in their 5.7.7 release in Oct 2014, and the current consensus is that, while 1024 is currently sufficient, it's unlikely to be in the near to medium future.

Debian bug report: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=788905",,"Increase Diffie-Helman modulus to 2048-bits $end$ Debian reported a bug in an older version of MariaDB relating to using a 512-bit modulus when
negotiating a Finite-Field Diffie-Hellman Ephemeral (FFDHE) handshake in TLS.

This was increased to 1024 in 10.0.18, but MySQL increased this to 2048 in their 5.7.7 release in Oct 2014, and the current consensus is that, while 1024 is currently sufficient, it's unlikely to be in the near to medium future.

Debian bug report: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=788905 $acceptance criteria:$",,Ian Gilfillan,Ian Gilfillan,Critical,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2015-07-21 15:52:55,Increase Diffie-Helman modulus to 2048-bits,"Debian reported a bug in an older version of MariaDB relating to using a 512-bit modulus when
negotiating a Finite-Field Diffie-Hellman Ephemeral (FFDHE) handshake in TLS.

This was increased to 1024 in 10.0.18, but MySQL increased this to 2048 in their 5.7.7 release in Oct 2014, and the current consensus is that, while 1024 is currently sufficient, it's unlikely to be in the near to medium future.

Debian bug report: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=788905",,0,0,0,0,0.0,"Increase Diffie-Helman modulus to 2048-bits $end$ Debian reported a bug in an older version of MariaDB relating to using a 512-bit modulus when
negotiating a Finite-Field Diffie-Hellman Ephemeral (FFDHE) handshake in TLS.

This was increased to 1024 in 10.0.18, but MySQL increased this to 2048 in their 5.7.7 release in Oct 2014, and the current consensus is that, while 1024 is currently sufficient, it's unlikely to be in the near to medium future.

Debian bug report: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=788905 $acceptance criteria:$",0,0,0,0,0,0,0,696.583,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
784,MDEV-8360,Task,MDEV,2015-06-23 14:13:37,,0,Clean-up CHARSET_INFO: strnncollsp: diff_if_only_endspace_difference,"There is a function in MY_COLLATION_HANDLER:
{code}
  int     (*strnncollsp)(CHARSET_INFO *,
                         const uchar *, size_t, const uchar *, size_t,
                         my_bool diff_if_only_endspace_difference); 
{code}

The ""diff_if_only_endspace_difference"" parameter is never used and should be removed.
",,"Clean-up CHARSET_INFO: strnncollsp: diff_if_only_endspace_difference $end$ There is a function in MY_COLLATION_HANDLER:
{code}
  int     (*strnncollsp)(CHARSET_INFO *,
                         const uchar *, size_t, const uchar *, size_t,
                         my_bool diff_if_only_endspace_difference); 
{code}

The ""diff_if_only_endspace_difference"" parameter is never used and should be removed.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Minor,16,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-02-24 11:59:55,Clean-up CHARSET_INFO: strnncollsp: diff_if_only_endspace_difference,"There is a function in MY_COLLATION_HANDLER:
{code}
  int     (*strnncollsp)(CHARSET_INFO *,
                         const uchar *, size_t, const uchar *, size_t,
                         my_bool diff_if_only_endspace_difference); 
{code}

The ""diff_if_only_endspace_difference"" parameter is never used and should be removed.
",,0,0,0,0,0.0,"Clean-up CHARSET_INFO: strnncollsp: diff_if_only_endspace_difference $end$ There is a function in MY_COLLATION_HANDLER:
{code}
  int     (*strnncollsp)(CHARSET_INFO *,
                         const uchar *, size_t, const uchar *, size_t,
                         my_bool diff_if_only_endspace_difference); 
{code}

The ""diff_if_only_endspace_difference"" parameter is never used and should be removed.
 $acceptance criteria:$",0,0,0,0,0,0,1,5901.77,4,3,0.75,3,0.75,3,0.75,2,0.5,2,0.5
785,MDEV-8387,Task,MDEV,2015-06-26 16:52:02,,0,10.1.6 merge,"* 10.0 (/)
* 10.0-galera (/)
* spider and mroonga (/)
** there's a fixed mroonga that works with 10.1",,"10.1.6 merge $end$ * 10.0 (/)
* 10.0-galera (/)
* spider and mroonga (/)
** there's a fixed mroonga that works with 10.1 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,8,,1,0,1,1,0,4,0,,0,850,0,1,0,2015-06-28 11:32:23,10.1.6 merge,"* 10.0 (/)
* 10.0.-galera
* spider and mroonga
** there's a fixed mroonga that works with 10.1",,0,3,0,4,0.130435,"10.1.6 merge $end$ * 10.0 (/)
* 10.0.-galera
* spider and mroonga
** there's a fixed mroonga that works with 10.1 $acceptance criteria:$",3,1,0,0,0,0,0,42.6667,13,4,0.307692,3,0.230769,3,0.230769,3,0.230769,3,0.230769
786,MDEV-8491,Task,MDEV,2015-07-17 14:31:27,,0,"On shutdown, report the user and the host executed that.","example:
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown complete
{noformat}

Should be 
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown initiated by root at localhost
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown initiated by root at localhost complete
{noformat}
",,"On shutdown, report the user and the host executed that. $end$ example:
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown complete
{noformat}

Should be 
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown initiated by root at localhost
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown initiated by root at localhost complete
{noformat}
 $acceptance criteria:$",,Stoykov,Stoykov,Major,17,,1,5,2,2,0,0,0,,0,850,2,0,0,2015-11-18 11:04:08,"On shutdown, report the user and the host executed that.","example:
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown complete
{noformat}

Should be 
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown initiated by root at localhost
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown initiated by root at localhost complete
{noformat}
",,0,0,0,0,0.0,"On shutdown, report the user and the host executed that. $end$ example:
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown complete
{noformat}

Should be 
{noformat}
150716 13:17:27 [Note] /usr/sbin/mysqld: Normal shutdown initiated by root at localhost
.
..
150716 13:17:28 [Note] /usr/sbin/mysqld: Shutdown initiated by root at localhost complete
{noformat}
 $acceptance criteria:$",0,0,0,0,0,0,1,2972.53,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
787,MDEV-8542,Task,MDEV,2015-07-25 06:11:44,,0,"The ""aria_recover"" variable should be renamed ""aria_recover_options"" to match MyISAM","[According to the MySQL 5.5 documentation, the variable _myisam\_recover_ has been renamed to _myisam\_recover\_options_ on MySQL 5.5.3|https://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_myisam-recover].

The equivalent variable for Aria that accepts the same values and also accepts multiple options separated by a comma is _aria\_recover_.


It would be more logical for both options to have a similar name.",,"The ""aria_recover"" variable should be renamed ""aria_recover_options"" to match MyISAM $end$ [According to the MySQL 5.5 documentation, the variable _myisam\_recover_ has been renamed to _myisam\_recover\_options_ on MySQL 5.5.3|https://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_myisam-recover].

The equivalent variable for Aria that accepts the same values and also accepts multiple options separated by a comma is _aria\_recover_.


It would be more logical for both options to have a similar name. $acceptance criteria:$",,Jean Weisbuch,Jean Weisbuch,Minor,12,,1,1,1,1,0,0,0,,0,850,1,0,0,2015-11-18 11:04:56,"The ""aria_recover"" variable should be renamed ""aria_recover_options"" to match MyISAM","[According to the MySQL 5.5 documentation, the variable _myisam\_recover_ has been renamed to _myisam\_recover\_options_ on MySQL 5.5.3|https://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_myisam-recover].

The equivalent variable for Aria that accepts the same values and also accepts multiple options separated by a comma is _aria\_recover_.


It would be more logical for both options to have a similar name.",,0,0,0,0,0.0,"The ""aria_recover"" variable should be renamed ""aria_recover_options"" to match MyISAM $end$ [According to the MySQL 5.5 documentation, the variable _myisam\_recover_ has been renamed to _myisam\_recover\_options_ on MySQL 5.5.3|https://dev.mysql.com/doc/refman/5.5/en/server-options.html#option_mysqld_myisam-recover].

The equivalent variable for Aria that accepts the same values and also accepts multiple options separated by a comma is _aria\_recover_.


It would be more logical for both options to have a similar name. $acceptance criteria:$",0,0,0,0,0,0,0,2788.88,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
788,MDEV-8559,Task,MDEV,2015-07-29 17:36:46,,0,5.5.45 merge,"mysql-5.5 → 5.5 (/) 5.5.45
xtradb → 5.5 (/) 5.5.44-37.3
tokudb → 5.5 (/) not this time, ft-index repository had its history rewritten and doesn't merge anymore",,"5.5.45 merge $end$ mysql-5.5 → 5.5 (/) 5.5.45
xtradb → 5.5 (/) 5.5.44-37.3
tokudb → 5.5 (/) not this time, ft-index repository had its history rewritten and doesn't merge anymore $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,7,,0,0,0,1,0,4,0,,0,850,0,0,0,2015-07-29 17:36:46,5.5.45 merge,"mysql-5.5 → 5.5
xtradb → 5.5
tokudb → 5.5",,0,4,0,18,1.28571,"5.5.45 merge $end$ mysql-5.5 → 5.5
xtradb → 5.5
tokudb → 5.5 $acceptance criteria:$",4,1,1,1,1,0,1,0.0,14,5,0.357143,3,0.214286,3,0.214286,3,0.214286,3,0.214286
789,MDEV-8560,Task,MDEV,2015-07-29 17:38:09,,0,10.0.21 merge,"* 5.5 (/) 5.5.45
* InnoDB (/) 5.6.26
* XtraDB (/) 5.6.25-73.1
* P_S (/) 5.6.26
* Connect (/)
* Spider (/) nothing to do
* pcre (/) nothing to do
* Mroonga (/) nothing to do
",,"10.0.21 merge $end$ * 5.5 (/) 5.5.45
* InnoDB (/) 5.6.26
* XtraDB (/) 5.6.25-73.1
* P_S (/) 5.6.26
* Connect (/)
* Spider (/) nothing to do
* pcre (/) nothing to do
* Mroonga (/) nothing to do
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,8,,0,0,0,1,0,5,0,,0,850,0,0,0,2015-07-29 17:38:09,10.0.21 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* pcre
* Mroonga
",,0,5,0,21,1.0,"10.0.21 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* pcre
* Mroonga
 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,15,6,0.4,4,0.266667,4,0.266667,4,0.266667,3,0.2
790,MDEV-8646,Task,MDEV,2015-08-19 01:42:18,,0,Re-engineer the code for post-join operations,"<SanjaByelkin> instead of creating new JOIN_TABs array (i.e. new plan) to process results in temporary table (GROUPING ORDERING)
<SanjaByelkin> just additional JOIN_TABs added at the end which represent such operation
<SanjaByelkin> so you never rewrite it
<SanjaByelkin> it is backport from MySQL
<SanjaByelkin> so you can add any number of postprocessing operations
<SanjaByelkin> and he need it for window functions",,"Re-engineer the code for post-join operations $end$ <SanjaByelkin> instead of creating new JOIN_TABs array (i.e. new plan) to process results in temporary table (GROUPING ORDERING)
<SanjaByelkin> just additional JOIN_TABs added at the end which represent such operation
<SanjaByelkin> so you never rewrite it
<SanjaByelkin> it is backport from MySQL
<SanjaByelkin> so you can add any number of postprocessing operations
<SanjaByelkin> and he need it for window functions $acceptance criteria:$",,Igor Babaev,Igor Babaev,Major,9,,1,27,2,2,0,2,0,,0,850,14,0,0,2016-01-13 16:00:06,Re-factor the code for working tables ,"<SanjaByelkin> instead of creating new JOIN_TABs array (i.e. new plan) to process results in temporary table (GROUPING ORDERING)
<SanjaByelkin> just additional JOIN_TABs added at the end which represent such operation
<SanjaByelkin> so you never rewrite it
<SanjaByelkin> it is backport from MySQL
<SanjaByelkin> so you can add any number of postprocessing operations
<SanjaByelkin> and he need it for window functions",,2,0,0,6,0.0434783,"Re-factor the code for working tables  $end$ <SanjaByelkin> instead of creating new JOIN_TABs array (i.e. new plan) to process results in temporary table (GROUPING ORDERING)
<SanjaByelkin> just additional JOIN_TABs added at the end which represent such operation
<SanjaByelkin> so you never rewrite it
<SanjaByelkin> it is backport from MySQL
<SanjaByelkin> so you can add any number of postprocessing operations
<SanjaByelkin> and he need it for window functions $acceptance criteria:$",2,1,1,0,0,0,1,3542.28,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
791,MDEV-8654,Task,MDEV,2015-08-20 12:41:21,,0,Remove mysqlbug,"mysqlbug has been obsolete since MySQL 5.5 (https://dev.mysql.com/doc/refman/5.5/en/mysqlbug.html), and has been removed in MySQL 5.7 (https://dev.mysql.com/doc/refman/5.6/en/mysqlbug.html). It's also of no use for reporting MariaDB bugs. The script and the associated man page should be removed from MariaDB.",,"Remove mysqlbug $end$ mysqlbug has been obsolete since MySQL 5.5 (https://dev.mysql.com/doc/refman/5.5/en/mysqlbug.html), and has been removed in MySQL 5.7 (https://dev.mysql.com/doc/refman/5.6/en/mysqlbug.html). It's also of no use for reporting MariaDB bugs. The script and the associated man page should be removed from MariaDB. $acceptance criteria:$",,Ian Gilfillan,Ian Gilfillan,Minor,10,,0,2,1,1,0,2,0,,0,850,2,2,0,2015-11-18 11:05:12,Remove mysqlbug,"mysqlbug has been obsolete since MySQL 5.5 (https://dev.mysql.com/doc/refman/5.5/en/mysqlbug.html), and has been removed in MySQL 5.7 (https://dev.mysql.com/doc/refman/5.6/en/mysqlbug.html). It's also of no use for reporting MariaDB bugs. The script and the associated man page should be removed from MariaDB.",,0,0,0,0,0.0,"Remove mysqlbug $end$ mysqlbug has been obsolete since MySQL 5.5 (https://dev.mysql.com/doc/refman/5.5/en/mysqlbug.html), and has been removed in MySQL 5.7 (https://dev.mysql.com/doc/refman/5.6/en/mysqlbug.html). It's also of no use for reporting MariaDB bugs. The script and the associated man page should be removed from MariaDB. $acceptance criteria:$",0,0,0,0,0,0,0,2158.38,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
792,MDEV-8713,Task,MDEV,2015-09-01 04:43:24,,0,Add continuous binary log backup to mysqlbinlog,"This is a request to add continuous binary log backup to mysqlbinlog, such as exists in the MySQL mysqlbinlog as of 5.6.

""As of MySQL 5.6, mysqlbinlog can read binary log files and write new files containing the same content—that is, in binary format rather than text format. This capability enables you to easily back up a binary log in its original format. mysqlbinlog can make a static backup, backing up a set of log files and stopping when the end of the last file is reached. It can also make a continuous (“live”) backup, staying connected to the server when it reaches the end of the last log file and continuing to copy new events as they are generated. In continuous-backup operation, mysqlbinlog runs until the connection ends (for example, when the server exits) or mysqlbinlog is forcibly terminated. When the connection ends, mysqlbinlog does not wait and retry the connection, unlike a slave replication server. To continue a live backup after the server has been restarted, you must also restart mysqlbinlog.""

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog-backup.html

This also requires the addition of 2 new mysqlbinlog options:

--stop-never
--stop-never-slave-server-id=id

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never
https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never-slave-server-id",,"Add continuous binary log backup to mysqlbinlog $end$ This is a request to add continuous binary log backup to mysqlbinlog, such as exists in the MySQL mysqlbinlog as of 5.6.

""As of MySQL 5.6, mysqlbinlog can read binary log files and write new files containing the same content—that is, in binary format rather than text format. This capability enables you to easily back up a binary log in its original format. mysqlbinlog can make a static backup, backing up a set of log files and stopping when the end of the last file is reached. It can also make a continuous (“live”) backup, staying connected to the server when it reaches the end of the last log file and continuing to copy new events as they are generated. In continuous-backup operation, mysqlbinlog runs until the connection ends (for example, when the server exits) or mysqlbinlog is forcibly terminated. When the connection ends, mysqlbinlog does not wait and retry the connection, unlike a slave replication server. To continue a live backup after the server has been restarted, you must also restart mysqlbinlog.""

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog-backup.html

This also requires the addition of 2 new mysqlbinlog options:

--stop-never
--stop-never-slave-server-id=id

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never
https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never-slave-server-id $acceptance criteria:$",,Chris Calender,Chris Calender,Critical,27,,2,11,5,3,0,0,0,,0,850,7,0,0,2015-11-18 10:09:34,Add continuous binary log backup to mysqlbinlog,"This is a request to add continuous binary log backup to mysqlbinlog, such as exists in the MySQL mysqlbinlog as of 5.6.

""As of MySQL 5.6, mysqlbinlog can read binary log files and write new files containing the same content—that is, in binary format rather than text format. This capability enables you to easily back up a binary log in its original format. mysqlbinlog can make a static backup, backing up a set of log files and stopping when the end of the last file is reached. It can also make a continuous (“live”) backup, staying connected to the server when it reaches the end of the last log file and continuing to copy new events as they are generated. In continuous-backup operation, mysqlbinlog runs until the connection ends (for example, when the server exits) or mysqlbinlog is forcibly terminated. When the connection ends, mysqlbinlog does not wait and retry the connection, unlike a slave replication server. To continue a live backup after the server has been restarted, you must also restart mysqlbinlog.""

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog-backup.html

This also requires the addition of 2 new mysqlbinlog options:

--stop-never
--stop-never-slave-server-id=id

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never
https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never-slave-server-id",,0,0,0,0,0.0,"Add continuous binary log backup to mysqlbinlog $end$ This is a request to add continuous binary log backup to mysqlbinlog, such as exists in the MySQL mysqlbinlog as of 5.6.

""As of MySQL 5.6, mysqlbinlog can read binary log files and write new files containing the same content—that is, in binary format rather than text format. This capability enables you to easily back up a binary log in its original format. mysqlbinlog can make a static backup, backing up a set of log files and stopping when the end of the last file is reached. It can also make a continuous (“live”) backup, staying connected to the server when it reaches the end of the last log file and continuing to copy new events as they are generated. In continuous-backup operation, mysqlbinlog runs until the connection ends (for example, when the server exits) or mysqlbinlog is forcibly terminated. When the connection ends, mysqlbinlog does not wait and retry the connection, unlike a slave replication server. To continue a live backup after the server has been restarted, you must also restart mysqlbinlog.""

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog-backup.html

This also requires the addition of 2 new mysqlbinlog options:

--stop-never
--stop-never-slave-server-id=id

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never
https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_stop-never-slave-server-id $acceptance criteria:$",0,0,0,0,0,0,1,1877.43,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
793,MDEV-8715,Task,MDEV,2015-09-01 15:48:25,MDEV-7941,0,Obsolete sql_alloc() in favor of THD::alloc() and thd_alloc(),"Since sql_alloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::alloc() and modifying plugins to use thd_alloc().",,"Obsolete sql_alloc() in favor of THD::alloc() and thd_alloc() $end$ Since sql_alloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::alloc() and modifying plugins to use thd_alloc(). $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,8,,0,3,0,1,0,0,0,,0,850,3,0,0,2015-11-17 17:55:52,Obsolete sql_alloc() in favor of THD::alloc() and thd_alloc(),"Since sql_alloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::alloc() and modifying plugins to use thd_alloc().",,0,0,0,0,0.0,"Obsolete sql_alloc() in favor of THD::alloc() and thd_alloc() $end$ Since sql_alloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::alloc() and modifying plugins to use thd_alloc(). $acceptance criteria:$",0,0,0,0,0,0,0,1850.12,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
794,MDEV-8716,Task,MDEV,2015-09-01 15:49:41,MDEV-7941,0,Obsolete sql_calloc() in favor of THD::calloc() and thd_calloc(),"Since sql_calloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::calloc() and modifying plugins to use thd_calloc().",,"Obsolete sql_calloc() in favor of THD::calloc() and thd_calloc() $end$ Since sql_calloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::calloc() and modifying plugins to use thd_calloc(). $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,8,,0,2,0,1,0,0,0,,0,850,2,0,0,2015-11-17 17:55:57,Obsolete sql_calloc() in favor of THD::calloc() and thd_calloc(),"Since sql_calloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::calloc() and modifying plugins to use thd_calloc().",,0,0,0,0,0.0,"Obsolete sql_calloc() in favor of THD::calloc() and thd_calloc() $end$ Since sql_calloc() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::calloc() and modifying plugins to use thd_calloc(). $acceptance criteria:$",0,0,0,0,0,0,0,1850.1,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
795,MDEV-8717,Task,MDEV,2015-09-01 15:51:42,MDEV-7941,0,Obsolete sql_strdup() in favor of THD::strdup() and thd_strdup(),"Since sql_strdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strdup() and modifying plugins to use thd_strdup().",,"Obsolete sql_strdup() in favor of THD::strdup() and thd_strdup() $end$ Since sql_strdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strdup() and modifying plugins to use thd_strdup(). $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,8,,0,2,0,1,0,0,0,,0,850,2,0,0,2015-11-17 17:56:02,Obsolete sql_strdup() in favor of THD::strdup() and thd_strdup(),"Since sql_strdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strdup() and modifying plugins to use thd_strdup().",,0,0,0,0,0.0,"Obsolete sql_strdup() in favor of THD::strdup() and thd_strdup() $end$ Since sql_strdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strdup() and modifying plugins to use thd_strdup(). $acceptance criteria:$",0,0,0,0,0,0,0,1850.07,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
796,MDEV-8718,Task,MDEV,2015-09-01 15:52:18,MDEV-7941,0,Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake(),"Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake()

Since sql_strmake() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strmake() and modifying plugins to use thd_strmake().",,"Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake() $end$ Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake()

Since sql_strmake() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strmake() and modifying plugins to use thd_strmake(). $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,8,,0,2,0,1,0,0,0,,0,850,2,0,0,2015-11-17 17:56:08,Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake(),"Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake()

Since sql_strmake() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strmake() and modifying plugins to use thd_strmake().",,0,0,0,0,0.0,"Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake() $end$ Obsolete sql_strmake() in favor of THD::strmake() and thd_strmake()

Since sql_strmake() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::strmake() and modifying plugins to use thd_strmake(). $acceptance criteria:$",0,0,0,0,0,0,0,1850.05,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
797,MDEV-8719,Task,MDEV,2015-09-01 15:53:01,MDEV-7941,0,Obsolete sql_memdup() in favor of THD::memdup() and thd_memdup(),"Since sql_memdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::memdup() and modifying plugins to use thd_memdup().",,"Obsolete sql_memdup() in favor of THD::memdup() and thd_memdup() $end$ Since sql_memdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::memdup() and modifying plugins to use thd_memdup(). $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,8,,0,2,0,1,0,0,0,,0,850,2,0,0,2015-11-17 17:56:13,Obsolete sql_memdup() in favor of THD::memdup() and thd_memdup(),"Since sql_memdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::memdup() and modifying plugins to use thd_memdup().",,0,0,0,0,0.0,"Obsolete sql_memdup() in favor of THD::memdup() and thd_memdup() $end$ Since sql_memdup() doesn't have THD parameter, it has to retrieve THD::mem_root by calling relatively expensive pthread_getspecific(THR_MALLOC).

This can be optimized easily by modifying server to use THD::memdup() and modifying plugins to use thd_memdup(). $acceptance criteria:$",0,0,0,0,0,0,0,1850.05,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
798,MDEV-8739,Task,MDEV,2015-09-03 19:09:46,,0,add galera suites to the default suite list,add galera suites to the default suite list and fix all test failures,,add galera suites to the default suite list $end$ add galera suites to the default suite list and fix all test failures $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Major,19,,0,3,0,2,0,0,0,,0,850,3,0,0,2015-09-09 10:48:14,add galera suites to the default suite list,add galera suites to the default suite list and fix all test failures,,0,0,0,0,0.0,add galera suites to the default suite list $end$ add galera suites to the default suite list and fix all test failures $acceptance criteria:$,0,0,0,0,0,0,1,135.633,16,7,0.4375,5,0.3125,5,0.3125,5,0.3125,4,0.25
799,MDEV-8842,Task,MDEV,2015-09-24 21:47:00,,0,add group support to pam_user_map module,"Currently pam_user_map module ({{plugin/auth_pam/mapper/pam_user_map.c}}) can map user names based on the simple configuration file, like
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
{noformat}
Here the user ""john"" will be renamed to ""jack"".
This task is to add support for mapping, based on the user group, like:
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
@group_ro: readonly
{noformat}
Here any user in the ""group_ro"" group will be renamed to ""readonly"".
",,"add group support to pam_user_map module $end$ Currently pam_user_map module ({{plugin/auth_pam/mapper/pam_user_map.c}}) can map user names based on the simple configuration file, like
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
{noformat}
Here the user ""john"" will be renamed to ""jack"".
This task is to add support for mapping, based on the user group, like:
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
@group_ro: readonly
{noformat}
Here any user in the ""group_ro"" group will be renamed to ""readonly"".
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Critical,11,,0,1,0,1,0,0,0,,0,850,1,0,0,2015-09-30 10:48:11,add group support to pam_user_map module,"Currently pam_user_map module ({{plugin/auth_pam/mapper/pam_user_map.c}}) can map user names based on the simple configuration file, like
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
{noformat}
Here the user ""john"" will be renamed to ""jack"".
This task is to add support for mapping, based on the user group, like:
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
@group_ro: readonly
{noformat}
Here any user in the ""group_ro"" group will be renamed to ""readonly"".
",,0,0,0,0,0.0,"add group support to pam_user_map module $end$ Currently pam_user_map module ({{plugin/auth_pam/mapper/pam_user_map.c}}) can map user names based on the simple configuration file, like
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
{noformat}
Here the user ""john"" will be renamed to ""jack"".
This task is to add support for mapping, based on the user group, like:
{noformat}
#comments and emty lines are ignored
john: jack
bob:  admin
top:  accounting
@group_ro: readonly
{noformat}
Here any user in the ""group_ro"" group will be renamed to ""readonly"".
 $acceptance criteria:$",0,0,0,0,0,0,0,133.017,17,7,0.411765,5,0.294118,5,0.294118,5,0.294118,4,0.235294
800,MDEV-8909,Task,MDEV,2015-10-06 20:13:27,,0,union parser cleanup,"This is a spin-off from MDEV-8380. The corresponding parser changes are too big to go into 10.1

This task will remove a few ""Incorrect usage of ... and ..."" errors and will return ""syntax error"" instead.

h3. Example#1:
{code:sql}
SELECT * FROM t1 LIMIT 1 UNION SELECT * FROM t1 LIMIT 1;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}

h3. Example#2:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#3:
{code:sql}
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#4
{code:sql}
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

",,"union parser cleanup $end$ This is a spin-off from MDEV-8380. The corresponding parser changes are too big to go into 10.1

This task will remove a few ""Incorrect usage of ... and ..."" errors and will return ""syntax error"" instead.

h3. Example#1:
{code:sql}
SELECT * FROM t1 LIMIT 1 UNION SELECT * FROM t1 LIMIT 1;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}

h3. Example#2:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#3:
{code:sql}
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#4
{code:sql}
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Minor,23,,12,0,19,4,0,3,0,,0,850,0,3,0,2016-05-05 08:16:18,union parser cleanup,"This is a spin-off from MDEV-8380. The corresponding parser changes are too big to go into 10.1

This task will remove a few ""Incorrect usage of ... and ..."" errors and will return ""syntax error"" instead.

h3. Example#1:
{code:sql}
SELECT * FROM t1 LIMIT 1 UNION SELECT * FROM t1 LIMIT 1;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}

h3. Example#2:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#3:
{code:sql}
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#4
{code:sql}
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

",,0,0,0,0,0.0,"union parser cleanup $end$ This is a spin-off from MDEV-8380. The corresponding parser changes are too big to go into 10.1

This task will remove a few ""Incorrect usage of ... and ..."" errors and will return ""syntax error"" instead.

h3. Example#1:
{code:sql}
SELECT * FROM t1 LIMIT 1 UNION SELECT * FROM t1 LIMIT 1;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of UNION and LIMIT
{noformat}

h3. Example#2:
{code:sql}
SELECT * FROM (SELECT * FROM t1 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#3:
{code:sql}
SELECT * FROM t1 NATURAL JOIN (SELECT * FROM t2 PROCEDURE ANALYSE());
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

h3. Example#4
{code:sql}
SELECT (SELECT 1 FROM t1 PROCEDURE ANALYSE()) FROM t2;
{code}
{noformat}
ERROR 1221 (HY000): Incorrect usage of PROCEDURE and subquery
{noformat}

 $acceptance criteria:$",0,0,0,0,0,0,1,5076.03,18,7,0.388889,5,0.277778,5,0.277778,5,0.277778,4,0.222222
801,MDEV-8931,Task,MDEV,2015-10-12 11:29:43,,0,(server part of) session state tracking,"see, e.g. https://dev.mysql.com/doc/refman/5.7/en/mysql-session-track-get-first.html

this also deprecates the EOF packet. It will be only used for old clients that don't need session state tracking.",,"(server part of) session state tracking $end$ see, e.g. https://dev.mysql.com/doc/refman/5.7/en/mysql-session-track-get-first.html

this also deprecates the EOF packet. It will be only used for old clients that don't need session state tracking. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,61,,2,10,6,13,0,3,0,,0,850,0,3,0,,(server part of) session state tracking,"see, e.g. https://dev.mysql.com/doc/refman/5.7/en/mysql-session-track-get-first.html

this also deprecates the EOF packet. It will be only used for old clients that don't need session state tracking.",,0,0,0,0,0.0,"(server part of) session state tracking $end$ see, e.g. https://dev.mysql.com/doc/refman/5.7/en/mysql-session-track-get-first.html

this also deprecates the EOF packet. It will be only used for old clients that don't need session state tracking. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,19,7,0.368421,5,0.263158,5,0.263158,5,0.263158,4,0.210526
802,MDEV-8934,Task,MDEV,2015-10-13 12:23:46,,0,Library for spheric coordinates,"Implement the library library that performs spheric coordinates calculations
*	calculations on shapes
**		predicates as Intersects or Within
**		operations like Union and Buffer()
*	various functions that don't use the the 'common core'
**		length
**		convex_hull
",,"Library for spheric coordinates $end$ Implement the library library that performs spheric coordinates calculations
*	calculations on shapes
**		predicates as Intersects or Within
**		operations like Union and Buffer()
*	various functions that don't use the the 'common core'
**		length
**		convex_hull
 $acceptance criteria:$",,Alexey Botchkov,Alexey Botchkov,Major,34,,0,0,1,8,0,2,0,,0,850,0,1,0,2015-10-28 11:04:13,Library for spheric coordinates.,"Implement the library library that performs spheric coordinates calculations
*	calculations on shapes
**		predicates as Intersects or Within
**		operations like Union and Buffer()
*	various functions that don't use the the 'common core'
**		length
**		convex_hull
",,1,0,0,2,0.0217391,"Library for spheric coordinates. $end$ Implement the library library that performs spheric coordinates calculations
*	calculations on shapes
**		predicates as Intersects or Within
**		operations like Union and Buffer()
*	various functions that don't use the the 'common core'
**		length
**		convex_hull
 $acceptance criteria:$",1,1,0,0,0,0,1,358.667,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
803,MDEV-8971,Task,MDEV,2015-10-20 14:45:26,,0,10.1.9 merge,"10.0 → 10.1 (/)
10.0-galera → 10.1 (/)
Connect 10.1 → 10.1 (/) 1.04.0003",,"10.1.9 merge $end$ 10.0 → 10.1 (/)
10.0-galera → 10.1 (/)
Connect 10.1 → 10.1 (/) 1.04.0003 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,11,,0,1,0,1,0,3,0,,0,850,1,0,0,2015-11-10 16:44:03,10.1.9 merge,"10.0 → 10.1
10.0-galera → 10.1
Connect 10.1 → 10.1",,0,3,0,4,0.266667,"10.1.9 merge $end$ 10.0 → 10.1
10.0-galera → 10.1
Connect 10.1 → 10.1 $acceptance criteria:$",3,1,0,0,0,0,0,505.967,20,7,0.35,5,0.25,5,0.25,5,0.25,4,0.2
804,MDEV-8976,Task,MDEV,2015-10-21 00:58:25,,0,OQGraph should be disabled for POWER8 Ubuntu package builds,"On POWER8, the libjudydebian1 package is empty except for the copyright and changelog files:
{code}
u0016471@sys-74005:~$ dpkg -L libjudydebian1
/.
/usr
/usr/lib
/usr/share
/usr/share/doc
/usr/share/doc/libjudydebian1
/usr/share/doc/libjudydebian1/copyright
/usr/share/doc/libjudydebian1/changelog.Debian.gz
{code}

This leads to issues when compiling:
* If the libjudydebian1 and libjudy-dev packages *are not* installed, building MariaDB fails with the following failure: 
{code}
dpkg-checkbuilddeps: Unmet build dependencies: libjudy-dev
dpkg-buildpackage: warning: build dependencies/conflicts unsatisfied; aborting
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/616/steps/compile/logs/stdio 

* If the two packages *are* installed, building MariaDB fails with the following failure:

{code}
make[3]: *** No rule to make target `/usr/lib/libJudy.so', needed by `storage/oqgraph/ha_oqgraph.so'.  Stop.
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/615/steps/compile/logs/stdio 

We should disable building OQGraph on POWER8 and remove the dependency on the libjudy-dev package, at least until the libjudydebian1 package gets fixed upstream.",,"OQGraph should be disabled for POWER8 Ubuntu package builds $end$ On POWER8, the libjudydebian1 package is empty except for the copyright and changelog files:
{code}
u0016471@sys-74005:~$ dpkg -L libjudydebian1
/.
/usr
/usr/lib
/usr/share
/usr/share/doc
/usr/share/doc/libjudydebian1
/usr/share/doc/libjudydebian1/copyright
/usr/share/doc/libjudydebian1/changelog.Debian.gz
{code}

This leads to issues when compiling:
* If the libjudydebian1 and libjudy-dev packages *are not* installed, building MariaDB fails with the following failure: 
{code}
dpkg-checkbuilddeps: Unmet build dependencies: libjudy-dev
dpkg-buildpackage: warning: build dependencies/conflicts unsatisfied; aborting
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/616/steps/compile/logs/stdio 

* If the two packages *are* installed, building MariaDB fails with the following failure:

{code}
make[3]: *** No rule to make target `/usr/lib/libJudy.so', needed by `storage/oqgraph/ha_oqgraph.so'.  Stop.
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/615/steps/compile/logs/stdio 

We should disable building OQGraph on POWER8 and remove the dependency on the libjudy-dev package, at least until the libjudydebian1 package gets fixed upstream. $acceptance criteria:$",,Daniel Bartholomew,Daniel Bartholomew,Major,7,,2,5,2,2,0,0,0,,0,850,3,0,0,2015-11-10 21:00:29,OQGraph should be disabled for POWER8 Ubuntu package builds,"On POWER8, the libjudydebian1 package is empty except for the copyright and changelog files:
{code}
u0016471@sys-74005:~$ dpkg -L libjudydebian1
/.
/usr
/usr/lib
/usr/share
/usr/share/doc
/usr/share/doc/libjudydebian1
/usr/share/doc/libjudydebian1/copyright
/usr/share/doc/libjudydebian1/changelog.Debian.gz
{code}

This leads to issues when compiling:
* If the libjudydebian1 and libjudy-dev packages *are not* installed, building MariaDB fails with the following failure: 
{code}
dpkg-checkbuilddeps: Unmet build dependencies: libjudy-dev
dpkg-buildpackage: warning: build dependencies/conflicts unsatisfied; aborting
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/616/steps/compile/logs/stdio 

* If the two packages *are* installed, building MariaDB fails with the following failure:

{code}
make[3]: *** No rule to make target `/usr/lib/libJudy.so', needed by `storage/oqgraph/ha_oqgraph.so'.  Stop.
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/615/steps/compile/logs/stdio 

We should disable building OQGraph on POWER8 and remove the dependency on the libjudy-dev package, at least until the libjudydebian1 package gets fixed upstream.",,0,0,0,0,0.0,"OQGraph should be disabled for POWER8 Ubuntu package builds $end$ On POWER8, the libjudydebian1 package is empty except for the copyright and changelog files:
{code}
u0016471@sys-74005:~$ dpkg -L libjudydebian1
/.
/usr
/usr/lib
/usr/share
/usr/share/doc
/usr/share/doc/libjudydebian1
/usr/share/doc/libjudydebian1/copyright
/usr/share/doc/libjudydebian1/changelog.Debian.gz
{code}

This leads to issues when compiling:
* If the libjudydebian1 and libjudy-dev packages *are not* installed, building MariaDB fails with the following failure: 
{code}
dpkg-checkbuilddeps: Unmet build dependencies: libjudy-dev
dpkg-buildpackage: warning: build dependencies/conflicts unsatisfied; aborting
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/616/steps/compile/logs/stdio 

* If the two packages *are* installed, building MariaDB fails with the following failure:

{code}
make[3]: *** No rule to make target `/usr/lib/libJudy.so', needed by `storage/oqgraph/ha_oqgraph.so'.  Stop.
{code}

Log: http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/615/steps/compile/logs/stdio 

We should disable building OQGraph on POWER8 and remove the dependency on the libjudy-dev package, at least until the libjudydebian1 package gets fixed upstream. $acceptance criteria:$",0,0,0,0,0,0,1,500.033,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
805,MDEV-8980,Task,MDEV,2015-10-21 15:46:20,,0,10.0.22 merge,"* 5.5 (/) 5.5.46
* InnoDB (/) 5.6.27
* XtraDB (/) 5.6.26-74.0
* P_S (/) 5.6.27
* Connect (/)
* Spider (/) _nothing to do_
* PCRE (/) _nothing to do_
* Mroonga (/) _nothing to do_
* TokuDB (/) _cancelled, will do again in 10.0.23_
",,"10.0.22 merge $end$ * 5.5 (/) 5.5.46
* InnoDB (/) 5.6.27
* XtraDB (/) 5.6.26-74.0
* P_S (/) 5.6.27
* Connect (/)
* Spider (/) _nothing to do_
* PCRE (/) _nothing to do_
* Mroonga (/) _nothing to do_
* TokuDB (/) _cancelled, will do again in 10.0.23_
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,11,,0,1,0,1,0,6,0,,0,850,1,0,0,2015-10-21 15:46:28,10.0.22 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
",,0,6,0,28,1.21739,"10.0.22 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
 $acceptance criteria:$",6,1,1,1,1,1,1,0.0,21,8,0.380952,5,0.238095,5,0.238095,5,0.238095,4,0.190476
806,MDEV-9022,Task,MDEV,2015-10-27 14:53:17,,0,[PATCH] New authentication plugin for authentication via named pipe,"New authentication plugin for authentication via named pipe on Windows operating systems.

The plugin gets the sid of the client process and considers the user
authenticated if the given username matches the username of this sid.",,"[PATCH] New authentication plugin for authentication via named pipe $end$ New authentication plugin for authentication via named pipe on Windows operating systems.

The plugin gets the sid of the client process and considers the user
authenticated if the given username matches the username of this sid. $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,12,,1,1,1,3,0,0,0,,0,850,1,0,0,2015-11-10 16:57:02,[PATCH] New authentication plugin for authentication via named pipe,"New authentication plugin for authentication via named pipe on Windows operating systems.

The plugin gets the sid of the client process and considers the user
authenticated if the given username matches the username of this sid.",,0,0,0,0,0.0,"[PATCH] New authentication plugin for authentication via named pipe $end$ New authentication plugin for authentication via named pipe on Windows operating systems.

The plugin gets the sid of the client process and considers the user
authenticated if the given username matches the username of this sid. $acceptance criteria:$",0,0,0,0,0,0,1,338.05,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
807,MDEV-9058,Task,MDEV,2015-11-02 13:49:28,,0,protocol: COM_MULTI command,"Implement COM_MULTI protocol command. Just as COM_QUERY support multiple queries in one packet, COM_MULTI will bundle many COM_xxx commands in one packet.

_May be the server should only support COM_MULTI if CLIENT_MULTI_RESULTS flag is set? Probably not_

Packets included in one COM_MULTI probably do not need packet number as well as separate compression, so only 3 bytes of length for each.

For prepared statement to be able prepare / execute / deallocate the command in one COM_MULTI batch will be special prepared statement ID (0 or -1) which mean ID of previous command.

h2. Packet structure:
* 4 or 7 bytes: of packet header (read by my_real_read())
* 1 byte: COM_MULTI
** 3 bytes: length of subcommand 1
** N1 bytes: subcommand1
** 3 bytes: length of subcommand 2
** N2 bytes: subcommand 2
...
** 3 bytes: length of subcommand m
** Nm bytes: subcommand m

COM_MULTI should be numbered from the ""end""  i.e. 255

There should be capability flag for it.",,"protocol: COM_MULTI command $end$ Implement COM_MULTI protocol command. Just as COM_QUERY support multiple queries in one packet, COM_MULTI will bundle many COM_xxx commands in one packet.

_May be the server should only support COM_MULTI if CLIENT_MULTI_RESULTS flag is set? Probably not_

Packets included in one COM_MULTI probably do not need packet number as well as separate compression, so only 3 bytes of length for each.

For prepared statement to be able prepare / execute / deallocate the command in one COM_MULTI batch will be special prepared statement ID (0 or -1) which mean ID of previous command.

h2. Packet structure:
* 4 or 7 bytes: of packet header (read by my_real_read())
* 1 byte: COM_MULTI
** 3 bytes: length of subcommand 1
** N1 bytes: subcommand1
** 3 bytes: length of subcommand 2
** N2 bytes: subcommand 2
...
** 3 bytes: length of subcommand m
** Nm bytes: subcommand m

COM_MULTI should be numbered from the ""end""  i.e. 255

There should be capability flag for it. $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,42,,0,2,5,6,0,10,0,,0,850,2,10,0,2015-11-24 16:09:24,protocol: COM_MULTI command,"Implement COM_MULTI protocol command. Just as COM_QUERY support multiple queries in one packet, COM_MULTI will bundle many COM_xxx commands in one packet.

_May be the server should only support COM_MULTI if CLIENT_MULTI_RESULTS flag is set? Probably not_

Packets included in one COM_MULTI probably do not need packet number as well as separate compression, so only 3 bytes of length for each.

For prepared statement to be able prepare / execute / deallocate the command in one COM_MULTI batch will be special prepared statement ID (0 or -1) which mean ID of previous command.

h2. Packet structure:
* 4 or 7 bytes: of packet header (read by my_real_read())
* 1 byte: COM_MULTI
** 3 bytes: length of subcommand 1
** N1 bytes: subcommand1
** 3 bytes: length of subcommand 2
** N2 bytes: subcommand 2
...
** 3 bytes: length of subcommand m
** Nm bytes: subcommand m

COM_MULTI should be numbered from the ""end""  i.e. 255

There should be capability flag for it.",,0,0,0,0,0.0,"protocol: COM_MULTI command $end$ Implement COM_MULTI protocol command. Just as COM_QUERY support multiple queries in one packet, COM_MULTI will bundle many COM_xxx commands in one packet.

_May be the server should only support COM_MULTI if CLIENT_MULTI_RESULTS flag is set? Probably not_

Packets included in one COM_MULTI probably do not need packet number as well as separate compression, so only 3 bytes of length for each.

For prepared statement to be able prepare / execute / deallocate the command in one COM_MULTI batch will be special prepared statement ID (0 or -1) which mean ID of previous command.

h2. Packet structure:
* 4 or 7 bytes: of packet header (read by my_real_read())
* 1 byte: COM_MULTI
** 3 bytes: length of subcommand 1
** N1 bytes: subcommand1
** 3 bytes: length of subcommand 2
** N2 bytes: subcommand 2
...
** 3 bytes: length of subcommand m
** Nm bytes: subcommand m

COM_MULTI should be numbered from the ""end""  i.e. 255

There should be capability flag for it. $acceptance criteria:$",0,0,0,0,0,0,1,530.317,22,9,0.409091,6,0.272727,6,0.272727,6,0.272727,5,0.227273
808,MDEV-9114,Task,MDEV,2015-11-10 18:57:51,,0,Bulk operations (Array binding),"h3. Why?

To achieve better performance we need to support bulk operations: Instead of executing a prepared statement n times it shold be possible to bind an array of size N and execute the statement once.

h3.*What needs to be changed ?

h4.*Indicator variables

Since the length of an array element might change, an array element could be a NULL value or the column default should be used we need to introduce indicator variables. An Indicator can have the following values:

{code}
 STMT_INDICATOR_NONE
 STMT_INDICATOR_NULL     Null value
 STMT_INDICATOR_DEFAULT  use column default value
{code}
Indicator variable arrays will be stored in the MYSQL_BIND structure.

h4. Additional definitions
{code}
#define STMT_BUFFER_UNSIGNED  32768  
#define STMT:BUFFER_INDICATOR 16384 (specifies if an indicator variable is used)
{code}

h4. Protocol implementation

{code}1   MARIADB_COM_STMT_BULK_EXECUTE
4   Statement id
1   flags (cursor)
4   Iteration count (=n)

1   Send types to server{code}

h5. Parameter:

 if send_types_to_server && num_params > 0

{code} 2 bytes for each parameter
    1st byte: parameter type
    2nd byte: unsigned flag = 32768 (is part of type)
              indicator variable set= 16384{code}

h4. Data-Array:

       if (indicator variable set)

          {code} 1   indicator value:
               STMT_INDICATOR_NONE       no indicator
               STMT_INDICATOR_NULL       null value
               STMT_INDICATOR_DEFAULT    use default value
               
           n   value of each parameter{code}

h3. API changes

The function mysql_stmt_attr_set should support the following attributes:
{{STMT_ATTR_ARRAY_SIZE}} (specifies the size of bound array)
{{STMT_ATTR_BIND_TYPE}} (specifies the type of binding: column or row wise binding)
The API will now also support row wise binding, where the application defines the structure containing elements for each column. The application declares the size of the structure to the driver with the {{SQL_ATTR_BIND_TYPE}} statement attribute and binds the address of each member Thus, the connector can calculate the address of the data for a particular row and column as

Address = Bound Address + (Row Number * Structure Size)

In case the structure contains non fixed length members like char * (which are implicitly allocated) the address of this member has to be specified. {{BIND_IS_PTR}} flag in bind.flags needs to be set to indicate this special case.

h5. Example:
{code}
 struct st_my_data {
   int id;
   char *buffer;
  
   ....
 };

 struct st_my_data data[20];

 bind[0].buffer= &data[0].id;
 bind[0].buffer_type= MYSQL_TYPE_LONG;

 bind[1].buffer= &data[0].buffer;
 bind[1].flags|= SIND_IS_PTR;
 bind[1].lengths= length_array;
[edit] MYSQL_BIND structure:

offset member of MYSQL_BIND structure is used for fetch only, so we build an union around:

 union {
   unsigned long offset;           /* offset position for char/binary fetch */
   struct {
     uchar *indicator;            /* array of indicator flags */
   };
 }
 ...
{code}

h5.  Indicator variable flags:

{code}#define INDICATOR_NONE      0
#define INDICATOR_NULL      1    /* array element is a null value */
#define INDICATOR_DEFAULT   2    /* column default value should be used */
#define INDICATOR_NTS       4    /* Null terminated string: this will be used by client only
                                    to determine correct data length */{code}

h5. Checking server capabilities

We need to check if the server supports bulk operations, so we need an additional define
{code}
#define CLIENT_STMT_BULK_OPERATIONS,
{code}

h5. Fixed string lengths

{code}
char *str_test[]= {""Keremidarski"", NULL, ""Widenius"", NULL};
ulong lengths[]= {-1, -1, -1, -1};
uchar indicators[] = {0, STMT_INDICATOR_NULL, 0, 0, STMT_INDCATOR_DEFAULT};
ulong size= 4;
char *query= ""INSERT INTO names VALUES (?)"";

stmt= mysql_stmt_init(NULL);

mysql_stmt_prepare(stmt, query, strlen(query));

mysql_stmt_attr_set(stmt, STMT_ATTR_ARRAY_SIZE, &size);

/* Initialize bind structure */
bzero(&bind, sizeof(MYSQL_BIND));

bind[0].buffer_type= MYSQL_TYPE_STRING;
bind[0].buffer= str_test[0];
bind[0].length= lengths;

mysql_stmt_bind_param(stmt, bind);
mysql_stmt_execute(stmt);
{code}

h3. Server Side Optimizations

Open tables only in the beginning of the batch (the array) processing. (Limitation: all data should be in one packet i.e. less then max allowed packet).
No need full cleanup for Items between executions for different bindings of the batch (array) (Especially for Item_field: tables are opened and can't change).
Especially popular statement types could be optimized like multi-value INSERT, i.e. short loop over all parameters after all check and preparation/optimization to execute (SELECT-like could use subquery execution mechanism).

h3. Limitations

Bulk operations cannot be used in combination with mysql_stmt_send_long_data. See also Long Data and SQLSetPos and SQLBulkOperations
",,"Bulk operations (Array binding) $end$ h3. Why?

To achieve better performance we need to support bulk operations: Instead of executing a prepared statement n times it shold be possible to bind an array of size N and execute the statement once.

h3.*What needs to be changed ?

h4.*Indicator variables

Since the length of an array element might change, an array element could be a NULL value or the column default should be used we need to introduce indicator variables. An Indicator can have the following values:

{code}
 STMT_INDICATOR_NONE
 STMT_INDICATOR_NULL     Null value
 STMT_INDICATOR_DEFAULT  use column default value
{code}
Indicator variable arrays will be stored in the MYSQL_BIND structure.

h4. Additional definitions
{code}
#define STMT_BUFFER_UNSIGNED  32768  
#define STMT:BUFFER_INDICATOR 16384 (specifies if an indicator variable is used)
{code}

h4. Protocol implementation

{code}1   MARIADB_COM_STMT_BULK_EXECUTE
4   Statement id
1   flags (cursor)
4   Iteration count (=n)

1   Send types to server{code}

h5. Parameter:

 if send_types_to_server && num_params > 0

{code} 2 bytes for each parameter
    1st byte: parameter type
    2nd byte: unsigned flag = 32768 (is part of type)
              indicator variable set= 16384{code}

h4. Data-Array:

       if (indicator variable set)

          {code} 1   indicator value:
               STMT_INDICATOR_NONE       no indicator
               STMT_INDICATOR_NULL       null value
               STMT_INDICATOR_DEFAULT    use default value
               
           n   value of each parameter{code}

h3. API changes

The function mysql_stmt_attr_set should support the following attributes:
{{STMT_ATTR_ARRAY_SIZE}} (specifies the size of bound array)
{{STMT_ATTR_BIND_TYPE}} (specifies the type of binding: column or row wise binding)
The API will now also support row wise binding, where the application defines the structure containing elements for each column. The application declares the size of the structure to the driver with the {{SQL_ATTR_BIND_TYPE}} statement attribute and binds the address of each member Thus, the connector can calculate the address of the data for a particular row and column as

Address = Bound Address + (Row Number * Structure Size)

In case the structure contains non fixed length members like char * (which are implicitly allocated) the address of this member has to be specified. {{BIND_IS_PTR}} flag in bind.flags needs to be set to indicate this special case.

h5. Example:
{code}
 struct st_my_data {
   int id;
   char *buffer;
  
   ....
 };

 struct st_my_data data[20];

 bind[0].buffer= &data[0].id;
 bind[0].buffer_type= MYSQL_TYPE_LONG;

 bind[1].buffer= &data[0].buffer;
 bind[1].flags|= SIND_IS_PTR;
 bind[1].lengths= length_array;
[edit] MYSQL_BIND structure:

offset member of MYSQL_BIND structure is used for fetch only, so we build an union around:

 union {
   unsigned long offset;           /* offset position for char/binary fetch */
   struct {
     uchar *indicator;            /* array of indicator flags */
   };
 }
 ...
{code}

h5.  Indicator variable flags:

{code}#define INDICATOR_NONE      0
#define INDICATOR_NULL      1    /* array element is a null value */
#define INDICATOR_DEFAULT   2    /* column default value should be used */
#define INDICATOR_NTS       4    /* Null terminated string: this will be used by client only
                                    to determine correct data length */{code}

h5. Checking server capabilities

We need to check if the server supports bulk operations, so we need an additional define
{code}
#define CLIENT_STMT_BULK_OPERATIONS,
{code}

h5. Fixed string lengths

{code}
char *str_test[]= {""Keremidarski"", NULL, ""Widenius"", NULL};
ulong lengths[]= {-1, -1, -1, -1};
uchar indicators[] = {0, STMT_INDICATOR_NULL, 0, 0, STMT_INDCATOR_DEFAULT};
ulong size= 4;
char *query= ""INSERT INTO names VALUES (?)"";

stmt= mysql_stmt_init(NULL);

mysql_stmt_prepare(stmt, query, strlen(query));

mysql_stmt_attr_set(stmt, STMT_ATTR_ARRAY_SIZE, &size);

/* Initialize bind structure */
bzero(&bind, sizeof(MYSQL_BIND));

bind[0].buffer_type= MYSQL_TYPE_STRING;
bind[0].buffer= str_test[0];
bind[0].length= lengths;

mysql_stmt_bind_param(stmt, bind);
mysql_stmt_execute(stmt);
{code}

h3. Server Side Optimizations

Open tables only in the beginning of the batch (the array) processing. (Limitation: all data should be in one packet i.e. less then max allowed packet).
No need full cleanup for Items between executions for different bindings of the batch (array) (Especially for Item_field: tables are opened and can't change).
Especially popular statement types could be optimized like multi-value INSERT, i.e. short loop over all parameters after all check and preparation/optimization to execute (SELECT-like could use subquery execution mechanism).

h3. Limitations

Bulk operations cannot be used in combination with mysql_stmt_send_long_data. See also Long Data and SQLSetPos and SQLBulkOperations
 $acceptance criteria:$",,Oleksandr Byelkin,Oleksandr Byelkin,Blocker,40,,0,4,4,9,0,7,0,,0,850,0,7,0,,Bulk operations (Array binding),"h3. Why?

To achieve better performance we need to support bulk operations: Instead of executing a prepared statement n times it shold be possible to bind an array of size N and execute the statement once.

h3.*What needs to be changed ?

h4.*Indicator variables

Since the length of an array element might change, an array element could be a NULL value or the column default should be used we need to introduce indicator variables. An Indicator can have the following values:

{code}
 STMT_INDICATOR_NONE
 STMT_INDICATOR_NULL     Null value
 STMT_INDICATOR_DEFAULT  use column default value
{code}
Indicator variable arrays will be stored in the MYSQL_BIND structure.

h4. Additional definitions
{code}
#define STMT_BUFFER_UNSIGNED  32768  
#define STMT:BUFFER_INDICATOR 16384 (specifies if an indicator variable is used)
{code}

h4. Protocol implementation

{code}1   MARIADB_COM_STMT_BULK_EXECUTE
4   Statement id
1   flags (cursor)
4   Iteration count (=n)

1   Send types to server{code}

h5. Parameter:

 if send_types_to_server && num_params > 0

{code} 2 bytes for each parameter
    1st byte: parameter type
    2nd byte: unsigned flag = 32768 (is part of type)
              indicator variable set= 16384{code}

h4. Data-Array:

       if (indicator variable set)

          {code} 1   indicator value:
               STMT_INDICATOR_NONE       no indicator
               STMT_INDICATOR_NULL       null value
               STMT_INDICATOR_DEFAULT    use default value
               
           n   value of each parameter{code}

h3. API changes

The function mysql_stmt_attr_set should support the following attributes:
{{STMT_ATTR_ARRAY_SIZE}} (specifies the size of bound array)
{{STMT_ATTR_BIND_TYPE}} (specifies the type of binding: column or row wise binding)
The API will now also support row wise binding, where the application defines the structure containing elements for each column. The application declares the size of the structure to the driver with the {{SQL_ATTR_BIND_TYPE}} statement attribute and binds the address of each member Thus, the connector can calculate the address of the data for a particular row and column as

Address = Bound Address + (Row Number * Structure Size)

In case the structure contains non fixed length members like char * (which are implicitly allocated) the address of this member has to be specified. {{BIND_IS_PTR}} flag in bind.flags needs to be set to indicate this special case.

h5. Example:
{code}
 struct st_my_data {
   int id;
   char *buffer;
  
   ....
 };

 struct st_my_data data[20];

 bind[0].buffer= &data[0].id;
 bind[0].buffer_type= MYSQL_TYPE_LONG;

 bind[1].buffer= &data[0].buffer;
 bind[1].flags|= SIND_IS_PTR;
 bind[1].lengths= length_array;
[edit] MYSQL_BIND structure:

offset member of MYSQL_BIND structure is used for fetch only, so we build an union around:

 union {
   unsigned long offset;           /* offset position for char/binary fetch */
   struct {
     uchar *indicator;            /* array of indicator flags */
   };
 }
 ...
{code}

h5.  Indicator variable flags:

{code}#define INDICATOR_NONE      0
#define INDICATOR_NULL      1    /* array element is a null value */
#define INDICATOR_DEFAULT   2    /* column default value should be used */
#define INDICATOR_NTS       4    /* Null terminated string: this will be used by client only
                                    to determine correct data length */{code}

h5. Checking server capabilities

We need to check if the server supports bulk operations, so we need an additional define
{code}
#define CLIENT_STMT_BULK_OPERATIONS,
{code}

h5. Fixed string lengths

{code}
char *str_test[]= {""Keremidarski"", NULL, ""Widenius"", NULL};
ulong lengths[]= {-1, -1, -1, -1};
uchar indicators[] = {0, STMT_INDICATOR_NULL, 0, 0, STMT_INDCATOR_DEFAULT};
ulong size= 4;
char *query= ""INSERT INTO names VALUES (?)"";

stmt= mysql_stmt_init(NULL);

mysql_stmt_prepare(stmt, query, strlen(query));

mysql_stmt_attr_set(stmt, STMT_ATTR_ARRAY_SIZE, &size);

/* Initialize bind structure */
bzero(&bind, sizeof(MYSQL_BIND));

bind[0].buffer_type= MYSQL_TYPE_STRING;
bind[0].buffer= str_test[0];
bind[0].length= lengths;

mysql_stmt_bind_param(stmt, bind);
mysql_stmt_execute(stmt);
{code}

h3. Server Side Optimizations

Open tables only in the beginning of the batch (the array) processing. (Limitation: all data should be in one packet i.e. less then max allowed packet).
No need full cleanup for Items between executions for different bindings of the batch (array) (Especially for Item_field: tables are opened and can't change).
Especially popular statement types could be optimized like multi-value INSERT, i.e. short loop over all parameters after all check and preparation/optimization to execute (SELECT-like could use subquery execution mechanism).

h3. Limitations

Bulk operations cannot be used in combination with mysql_stmt_send_long_data. See also Long Data and SQLSetPos and SQLBulkOperations
",,0,0,0,0,0.0,"Bulk operations (Array binding) $end$ h3. Why?

To achieve better performance we need to support bulk operations: Instead of executing a prepared statement n times it shold be possible to bind an array of size N and execute the statement once.

h3.*What needs to be changed ?

h4.*Indicator variables

Since the length of an array element might change, an array element could be a NULL value or the column default should be used we need to introduce indicator variables. An Indicator can have the following values:

{code}
 STMT_INDICATOR_NONE
 STMT_INDICATOR_NULL     Null value
 STMT_INDICATOR_DEFAULT  use column default value
{code}
Indicator variable arrays will be stored in the MYSQL_BIND structure.

h4. Additional definitions
{code}
#define STMT_BUFFER_UNSIGNED  32768  
#define STMT:BUFFER_INDICATOR 16384 (specifies if an indicator variable is used)
{code}

h4. Protocol implementation

{code}1   MARIADB_COM_STMT_BULK_EXECUTE
4   Statement id
1   flags (cursor)
4   Iteration count (=n)

1   Send types to server{code}

h5. Parameter:

 if send_types_to_server && num_params > 0

{code} 2 bytes for each parameter
    1st byte: parameter type
    2nd byte: unsigned flag = 32768 (is part of type)
              indicator variable set= 16384{code}

h4. Data-Array:

       if (indicator variable set)

          {code} 1   indicator value:
               STMT_INDICATOR_NONE       no indicator
               STMT_INDICATOR_NULL       null value
               STMT_INDICATOR_DEFAULT    use default value
               
           n   value of each parameter{code}

h3. API changes

The function mysql_stmt_attr_set should support the following attributes:
{{STMT_ATTR_ARRAY_SIZE}} (specifies the size of bound array)
{{STMT_ATTR_BIND_TYPE}} (specifies the type of binding: column or row wise binding)
The API will now also support row wise binding, where the application defines the structure containing elements for each column. The application declares the size of the structure to the driver with the {{SQL_ATTR_BIND_TYPE}} statement attribute and binds the address of each member Thus, the connector can calculate the address of the data for a particular row and column as

Address = Bound Address + (Row Number * Structure Size)

In case the structure contains non fixed length members like char * (which are implicitly allocated) the address of this member has to be specified. {{BIND_IS_PTR}} flag in bind.flags needs to be set to indicate this special case.

h5. Example:
{code}
 struct st_my_data {
   int id;
   char *buffer;
  
   ....
 };

 struct st_my_data data[20];

 bind[0].buffer= &data[0].id;
 bind[0].buffer_type= MYSQL_TYPE_LONG;

 bind[1].buffer= &data[0].buffer;
 bind[1].flags|= SIND_IS_PTR;
 bind[1].lengths= length_array;
[edit] MYSQL_BIND structure:

offset member of MYSQL_BIND structure is used for fetch only, so we build an union around:

 union {
   unsigned long offset;           /* offset position for char/binary fetch */
   struct {
     uchar *indicator;            /* array of indicator flags */
   };
 }
 ...
{code}

h5.  Indicator variable flags:

{code}#define INDICATOR_NONE      0
#define INDICATOR_NULL      1    /* array element is a null value */
#define INDICATOR_DEFAULT   2    /* column default value should be used */
#define INDICATOR_NTS       4    /* Null terminated string: this will be used by client only
                                    to determine correct data length */{code}

h5. Checking server capabilities

We need to check if the server supports bulk operations, so we need an additional define
{code}
#define CLIENT_STMT_BULK_OPERATIONS,
{code}

h5. Fixed string lengths

{code}
char *str_test[]= {""Keremidarski"", NULL, ""Widenius"", NULL};
ulong lengths[]= {-1, -1, -1, -1};
uchar indicators[] = {0, STMT_INDICATOR_NULL, 0, 0, STMT_INDCATOR_DEFAULT};
ulong size= 4;
char *query= ""INSERT INTO names VALUES (?)"";

stmt= mysql_stmt_init(NULL);

mysql_stmt_prepare(stmt, query, strlen(query));

mysql_stmt_attr_set(stmt, STMT_ATTR_ARRAY_SIZE, &size);

/* Initialize bind structure */
bzero(&bind, sizeof(MYSQL_BIND));

bind[0].buffer_type= MYSQL_TYPE_STRING;
bind[0].buffer= str_test[0];
bind[0].length= lengths;

mysql_stmt_bind_param(stmt, bind);
mysql_stmt_execute(stmt);
{code}

h3. Server Side Optimizations

Open tables only in the beginning of the batch (the array) processing. (Limitation: all data should be in one packet i.e. less then max allowed packet).
No need full cleanup for Items between executions for different bindings of the batch (array) (Especially for Item_field: tables are opened and can't change).
Especially popular statement types could be optimized like multi-value INSERT, i.e. short loop over all parameters after all check and preparation/optimization to execute (SELECT-like could use subquery execution mechanism).

h3. Limitations

Bulk operations cannot be used in combination with mysql_stmt_send_long_data. See also Long Data and SQLSetPos and SQLBulkOperations
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
809,MDEV-9117,Task,MDEV,2015-11-11 17:18:41,,0,Client Server capability negotiation for MariaDB specific functionality,"Add yet another 4 bytes of capability flags for new MariaDB functionality:

* from server->client in handshake packet V1
** client parse version info to be sure that it is MariaDB server
** server put flags in last 4 of unused 10 (bytes 14-17)
* from client->server (only if server mariadb and version is >= 10.2) V1
** client sets 'client_long_password=0' and 'client_protocol_41=1'
** puts flag in last 4 of 23 unused

Move CLIENT_PROGRESS flag to the new flag space.",,"Client Server capability negotiation for MariaDB specific functionality $end$ Add yet another 4 bytes of capability flags for new MariaDB functionality:

* from server->client in handshake packet V1
** client parse version info to be sure that it is MariaDB server
** server put flags in last 4 of unused 10 (bytes 14-17)
* from client->server (only if server mariadb and version is >= 10.2) V1
** client sets 'client_long_password=0' and 'client_protocol_41=1'
** puts flag in last 4 of 23 unused

Move CLIENT_PROGRESS flag to the new flag space. $acceptance criteria:$",,Oleksandr Byelkin,Oleksandr Byelkin,Major,39,,0,6,1,2,0,8,0,,0,850,6,0,0,2015-11-11 17:18:41,expand client capability flags to 8 bytes,"Add yet another 4 bytes of capability flags:

* from server in handshake packet last 4 of unused 10
* from client (only if server mariadb and version is >= 10.2) last 4 of 23 unused
",,1,7,0,63,1.17391,"expand client capability flags to 8 bytes $end$ Add yet another 4 bytes of capability flags:

* from server in handshake packet last 4 of unused 10
* from client (only if server mariadb and version is >= 10.2) last 4 of 23 unused
 $acceptance criteria:$",8,1,1,1,1,1,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
810,MDEV-9118,Task,MDEV,2015-11-12 03:21:21,,0,ANALYZE TABLE for Engine independent status fetches blob/text columns without use,"From MDEV-7383, column stats of bob/text columns aren't implemented. They are however still retrieved during the process of ANALYZE TABLE. Removing the retrieval of blob columns during ANALYZE TABLE could potentially save a lot of IO on essentially an IO bound task.",,"ANALYZE TABLE for Engine independent status fetches blob/text columns without use $end$ From MDEV-7383, column stats of bob/text columns aren't implemented. They are however still retrieved during the process of ANALYZE TABLE. Removing the retrieval of blob columns during ANALYZE TABLE could potentially save a lot of IO on essentially an IO bound task. $acceptance criteria:$",,Daniel Black,Daniel Black,Major,25,,1,8,2,2,0,2,0,,0,850,8,1,0,2015-12-15 16:30:57,ANALYZE TABLE for Engine independent status fetchs blob/text columns without use,"From MDEV-7383, column stats of bob/text columns aren't implemented. They are however still retrieved during the process of ANALYZE TABLE. Removing the retrieval of blob columns during ANALYZE TABLE could potentially save a lot of IO on essentially an IO bound task.",,1,0,0,2,0.0178571,"ANALYZE TABLE for Engine independent status fetchs blob/text columns without use $end$ From MDEV-7383, column stats of bob/text columns aren't implemented. They are however still retrieved during the process of ANALYZE TABLE. Removing the retrieval of blob columns during ANALYZE TABLE could potentially save a lot of IO on essentially an IO bound task. $acceptance criteria:$",1,1,0,0,0,0,1,805.15,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
811,MDEV-9143,Task,MDEV,2015-11-17 16:37:36,,0,JSON_xxx functions,JSON_xxx functions as in 5.7 and SQL standard (\?),,JSON_xxx functions $end$ JSON_xxx functions as in 5.7 and SQL standard (\?) $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Blocker,61,,39,7,40,20,0,0,0,,0,850,0,0,0,,JSON_xxx functions,JSON_xxx functions as in 5.7 and SQL standard (\?),,0,0,0,0,0.0,JSON_xxx functions $end$ JSON_xxx functions as in 5.7 and SQL standard (\?) $acceptance criteria:$,0,0,0,0,0,0,1,0.0,23,9,0.391304,6,0.26087,6,0.26087,6,0.26087,5,0.217391
812,MDEV-9170,Task,MDEV,2015-11-23 15:11:36,,0,Get rid of LEX::length and LEX::dec,"Passing data in sql_yacc.yy through LEX members is hard to follow and is not recursively safe.
We'll get rid of LEX::length and LEX::dec and add a structure instead:
{code}
struct Lex_length_and_dec_st
{
  const char *length;
  const char *dec;
};
{code}
so it can be passed through the syntax rules in sql_yacc.yy as $$, $1, $2, etc.
This will help, for example, to change:
{code}
field_spec:
  field_ident { /* new Create_field is allocated here */}
  field_type { Lex->set_last_field_type($3); }
  field_def { ... }
  ;
{code}
into:
{code}
field_spec:
  field_ident field_type { /* new Create field is allocated here*/ }
  field_def { ... }
{code}

So we'll already know data type, length and dec when we do ""new Create_field"".
This will allow to call a method in Type_handler, to create a data-type specific ""Create_field"" replacement, instead of using the generic universal Create_field.
",,"Get rid of LEX::length and LEX::dec $end$ Passing data in sql_yacc.yy through LEX members is hard to follow and is not recursively safe.
We'll get rid of LEX::length and LEX::dec and add a structure instead:
{code}
struct Lex_length_and_dec_st
{
  const char *length;
  const char *dec;
};
{code}
so it can be passed through the syntax rules in sql_yacc.yy as $$, $1, $2, etc.
This will help, for example, to change:
{code}
field_spec:
  field_ident { /* new Create_field is allocated here */}
  field_type { Lex->set_last_field_type($3); }
  field_def { ... }
  ;
{code}
into:
{code}
field_spec:
  field_ident field_type { /* new Create field is allocated here*/ }
  field_def { ... }
{code}

So we'll already know data type, length and dec when we do ""new Create_field"".
This will allow to call a method in Type_handler, to create a data-type specific ""Create_field"" replacement, instead of using the generic universal Create_field.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,13,,0,0,1,1,0,0,0,,0,850,0,0,0,2015-11-23 17:00:16,Get rid of LEX::length and LEX::dec,"Passing data in sql_yacc.yy through LEX members is hard to follow and is not recursively safe.
We'll get rid of LEX::length and LEX::dec and add a structure instead:
{code}
struct Lex_length_and_dec_st
{
  const char *length;
  const char *dec;
};
{code}
so it can be passed through the syntax rules in sql_yacc.yy as $$, $1, $2, etc.
This will help, for example, to change:
{code}
field_spec:
  field_ident { /* new Create_field is allocated here */}
  field_type { Lex->set_last_field_type($3); }
  field_def { ... }
  ;
{code}
into:
{code}
field_spec:
  field_ident field_type { /* new Create field is allocated here*/ }
  field_def { ... }
{code}

So we'll already know data type, length and dec when we do ""new Create_field"".
This will allow to call a method in Type_handler, to create a data-type specific ""Create_field"" replacement, instead of using the generic universal Create_field.
",,0,0,0,0,0.0,"Get rid of LEX::length and LEX::dec $end$ Passing data in sql_yacc.yy through LEX members is hard to follow and is not recursively safe.
We'll get rid of LEX::length and LEX::dec and add a structure instead:
{code}
struct Lex_length_and_dec_st
{
  const char *length;
  const char *dec;
};
{code}
so it can be passed through the syntax rules in sql_yacc.yy as $$, $1, $2, etc.
This will help, for example, to change:
{code}
field_spec:
  field_ident { /* new Create_field is allocated here */}
  field_type { Lex->set_last_field_type($3); }
  field_def { ... }
  ;
{code}
into:
{code}
field_spec:
  field_ident field_type { /* new Create field is allocated here*/ }
  field_def { ... }
{code}

So we'll already know data type, length and dec when we do ""new Create_field"".
This will allow to call a method in Type_handler, to create a data-type specific ""Create_field"" replacement, instead of using the generic universal Create_field.
 $acceptance criteria:$",0,0,0,0,0,0,0,1.8,5,3,0.6,3,0.6,3,0.6,2,0.4,2,0.4
813,MDEV-9172,Task,MDEV,2015-11-23 20:47:12,,0,Analyze patches for IBM System z,Please analyze the patch offered by IBM for building MariaDB 10.1 on IBM z.,,Analyze patches for IBM System z $end$ Please analyze the patch offered by IBM for building MariaDB 10.1 on IBM z. $acceptance criteria:$,,Kolbe Kegel,Kolbe Kegel,Major,8,,0,3,0,1,0,0,0,,0,850,1,0,0,2015-12-09 13:13:53,Analyze patches for IBM System z,Please analyze the patch offered by IBM for building MariaDB 10.1 on IBM z.,,0,0,0,0,0.0,Analyze patches for IBM System z $end$ Please analyze the patch offered by IBM for building MariaDB 10.1 on IBM z. $acceptance criteria:$,0,0,0,0,0,0,0,376.433,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
814,MDEV-9185,Task,MDEV,2015-11-25 11:14:00,,0,Integrate with Travis-CI for easier and more automatic QA,"I simplified and refactored the original suggestion from Daniel Black regarding a .travis.yml file that enables the usage of the continous integration service by Travis, which is free for open source projects and easily available to anybody who wants to developd and work with the MariaDB server sources.

This Travis-CI file is not intended to replace the buildbots currently used by MariaDB core developers, but rather to provide an extra facility for occasional contributors who don't have their own buildbot trees.

Currently Travis-CI has a 50 minute limit on the build duration, so the tests are limited to the 'main' suite. Parallel Travis-CI jobs would be a way around that restriction, but it wastes resources, so I am working with Travis-CI dev to figure out another more effective solution.

The Travis build also includes building the Debian packages, which will help spot packaging breakages early on.

The code in this PR branch passes the test. Log visible at https://travis-ci.org/MariaDB/server/builds/92879843",,"Integrate with Travis-CI for easier and more automatic QA $end$ I simplified and refactored the original suggestion from Daniel Black regarding a .travis.yml file that enables the usage of the continous integration service by Travis, which is free for open source projects and easily available to anybody who wants to developd and work with the MariaDB server sources.

This Travis-CI file is not intended to replace the buildbots currently used by MariaDB core developers, but rather to provide an extra facility for occasional contributors who don't have their own buildbot trees.

Currently Travis-CI has a 50 minute limit on the build duration, so the tests are limited to the 'main' suite. Parallel Travis-CI jobs would be a way around that restriction, but it wastes resources, so I am working with Travis-CI dev to figure out another more effective solution.

The Travis build also includes building the Debian packages, which will help spot packaging breakages early on.

The code in this PR branch passes the test. Log visible at https://travis-ci.org/MariaDB/server/builds/92879843 $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,19,,0,10,0,2,0,3,0,,0,850,8,0,0,2016-05-11 07:34:06,[PATCH] Integrate with Travis-CI for easier and more automatic QA,"I simplified and refactored the original suggestion from Daniel Black regarding a .travis.yml file that enables the usage of the continous integration service by Travis, which is free for open source projects and easily available to anybody who wants to developd and work with the MariaDB server sources.

This Travis-CI file is not intended to replace the buildbots currently used by MariaDB core developers, but rather to provide an extra facility for occasional contributors who don't have their own buildbot trees.

Currently Travis-CI has a 50 minute limit on the build duration, so the tests are limited to the 'main' suite. Parallel Travis-CI jobs would be a way around that restriction, but it wastes resources, so I am working with Travis-CI dev to figure out another more effective solution.

The Travis build also includes building the Debian packages, which will help spot packaging breakages early on.

The code in this PR branch passes the test. Log visible at https://travis-ci.org/MariaDB/server/builds/92879843",,3,0,0,1,0.00581395,"[PATCH] Integrate with Travis-CI for easier and more automatic QA $end$ I simplified and refactored the original suggestion from Daniel Black regarding a .travis.yml file that enables the usage of the continous integration service by Travis, which is free for open source projects and easily available to anybody who wants to developd and work with the MariaDB server sources.

This Travis-CI file is not intended to replace the buildbots currently used by MariaDB core developers, but rather to provide an extra facility for occasional contributors who don't have their own buildbot trees.

Currently Travis-CI has a 50 minute limit on the build duration, so the tests are limited to the 'main' suite. Parallel Travis-CI jobs would be a way around that restriction, but it wastes resources, so I am working with Travis-CI dev to figure out another more effective solution.

The Travis build also includes building the Debian packages, which will help spot packaging breakages early on.

The code in this PR branch passes the test. Log visible at https://travis-ci.org/MariaDB/server/builds/92879843 $acceptance criteria:$",3,1,0,0,0,0,1,4028.33,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
815,MDEV-9197,Task,MDEV,2015-11-27 01:39:35,MDEV-10872,0,Pushdown conditions into non-mergeable views/derived tables,"It is possible to do condition pushdown into non-mergable VIEWs or derived tables.

Example:
{noformat}
select ...
from 
  (select col1, max(col2) as max_val 
   from t2 
   group by t1 ) TBL
where 
   col1 !='foo' and max_val > 100
{noformat}

here, both parts of top-level select's WHERE clause can be pushed into derived table's HAVING clause.

h2. Implementation challenges
Moving condition from one select into another changes the context which condition is used in. 
Table condition pushdown produces strictest-possible conditions, but has a property that certain part of condition can be attached to multiple tables.  
Doing the same thing here will cause trouble: if the same part of the condition is both in subquery's HAVING and in upper select's WHERE, it should e.g. automagically changes its used_tables() depending on what context we're looking at it from. 
The solution to this is to do limited pushdown: only push down expressions that we can completely remove from upper query's WHERE.

Pushdown may also require adjustments to {{ref_pointer_array}}. Sanja and Igor seem to understand the details of this. ",,"Pushdown conditions into non-mergeable views/derived tables $end$ It is possible to do condition pushdown into non-mergable VIEWs or derived tables.

Example:
{noformat}
select ...
from 
  (select col1, max(col2) as max_val 
   from t2 
   group by t1 ) TBL
where 
   col1 !='foo' and max_val > 100
{noformat}

here, both parts of top-level select's WHERE clause can be pushed into derived table's HAVING clause.

h2. Implementation challenges
Moving condition from one select into another changes the context which condition is used in. 
Table condition pushdown produces strictest-possible conditions, but has a property that certain part of condition can be attached to multiple tables.  
Doing the same thing here will cause trouble: if the same part of the condition is both in subquery's HAVING and in upper select's WHERE, it should e.g. automagically changes its used_tables() depending on what context we're looking at it from. 
The solution to this is to do limited pushdown: only push down expressions that we can completely remove from upper query's WHERE.

Pushdown may also require adjustments to {{ref_pointer_array}}. Sanja and Igor seem to understand the details of this.  $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,27,,2,7,5,2,0,0,0,,0,850,5,0,0,2016-08-31 18:06:55,Pushdown conditions into non-mergeable views/derived tables,"It is possible to do condition pushdown into non-mergable VIEWs or derived tables.

Example:
{noformat}
select ...
from 
  (select col1, max(col2) as max_val 
   from t2 
   group by t1 ) TBL
where 
   col1 !='foo' and max_val > 100
{noformat}

here, both parts of top-level select's WHERE clause can be pushed into derived table's HAVING clause.

h2. Implementation challenges
Moving condition from one select into another changes the context which condition is used in. 
Table condition pushdown produces strictest-possible conditions, but has a property that certain part of condition can be attached to multiple tables.  
Doing the same thing here will cause trouble: if the same part of the condition is both in subquery's HAVING and in upper select's WHERE, it should e.g. automagically changes its used_tables() depending on what context we're looking at it from. 
The solution to this is to do limited pushdown: only push down expressions that we can completely remove from upper query's WHERE.

Pushdown may also require adjustments to {{ref_pointer_array}}. Sanja and Igor seem to understand the details of this. ",,0,0,0,0,0.0,"Pushdown conditions into non-mergeable views/derived tables $end$ It is possible to do condition pushdown into non-mergable VIEWs or derived tables.

Example:
{noformat}
select ...
from 
  (select col1, max(col2) as max_val 
   from t2 
   group by t1 ) TBL
where 
   col1 !='foo' and max_val > 100
{noformat}

here, both parts of top-level select's WHERE clause can be pushed into derived table's HAVING clause.

h2. Implementation challenges
Moving condition from one select into another changes the context which condition is used in. 
Table condition pushdown produces strictest-possible conditions, but has a property that certain part of condition can be attached to multiple tables.  
Doing the same thing here will cause trouble: if the same part of the condition is both in subquery's HAVING and in upper select's WHERE, it should e.g. automagically changes its used_tables() depending on what context we're looking at it from. 
The solution to this is to do limited pushdown: only push down expressions that we can completely remove from upper query's WHERE.

Pushdown may also require adjustments to {{ref_pointer_array}}. Sanja and Igor seem to understand the details of this.  $acceptance criteria:$",0,0,0,0,0,0,1,6688.45,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
816,MDEV-9252,Task,MDEV,2015-12-09 11:51:31,,0,10.0.23 merge,"* 5.5 (/) 5.5.47
* InnoDB (/) 5.6.28
* XtraDB (/)     5.6.27-76.0
* P_S (/) 5.6.28
* Connect (/) 1.04.0005
* Spider (/) 3.2.37
* PCRE (/) 8.38
* Mroonga (/) _nothing to do_
* TokuDB (/) 5.6.26-74.0
",,"10.0.23 merge $end$ * 5.5 (/) 5.5.47
* InnoDB (/) 5.6.28
* XtraDB (/)     5.6.27-76.0
* P_S (/) 5.6.28
* Connect (/) 1.04.0005
* Spider (/) 3.2.37
* PCRE (/) 8.38
* Mroonga (/) _nothing to do_
* TokuDB (/) 5.6.26-74.0
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,13,,2,1,2,1,0,7,0,,0,850,1,0,0,2015-12-09 14:38:53,10.0.23 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
",,0,7,0,20,0.869565,"10.0.23 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
 $acceptance criteria:$",7,1,1,1,1,1,1,2.78333,24,9,0.375,6,0.25,6,0.25,6,0.25,5,0.208333
817,MDEV-9255,Task,MDEV,2015-12-09 20:33:04,,0,Add generation_expression to information_schema.columns,"In MySQL the information_schema.columns table has a generation_expression column containing the expression used for generated columns. In MariaDB this column is missing, which makes it more difficult for third party software to interplay with both MariaDB and MySQL by looking for this schema data in one location. Instead, it seems MariaDB only contains this information in the table's create statement which must be parsed — a non-trivial and fragile task.",,"Add generation_expression to information_schema.columns $end$ In MySQL the information_schema.columns table has a generation_expression column containing the expression used for generated columns. In MariaDB this column is missing, which makes it more difficult for third party software to interplay with both MariaDB and MySQL by looking for this schema data in one location. Instead, it seems MariaDB only contains this information in the table's create statement which must be parsed — a non-trivial and fragile task. $acceptance criteria:$",,Seth Willits,Seth Willits,Major,16,,1,3,2,1,0,0,0,,0,850,2,0,0,2017-03-15 20:19:53,Add generation_expression to information_schema.columns,"In MySQL the information_schema.columns table has a generation_expression column containing the expression used for generated columns. In MariaDB this column is missing, which makes it more difficult for third party software to interplay with both MariaDB and MySQL by looking for this schema data in one location. Instead, it seems MariaDB only contains this information in the table's create statement which must be parsed — a non-trivial and fragile task.",,0,0,0,0,0.0,"Add generation_expression to information_schema.columns $end$ In MySQL the information_schema.columns table has a generation_expression column containing the expression used for generated columns. In MariaDB this column is missing, which makes it more difficult for third party software to interplay with both MariaDB and MySQL by looking for this schema data in one location. Instead, it seems MariaDB only contains this information in the table's create statement which must be parsed — a non-trivial and fragile task. $acceptance criteria:$",0,0,0,0,0,0,0,11087.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
818,MDEV-9267,Task,MDEV,2015-12-11 17:03:51,,0,pull in Connector/C sources into the server tree on builds,"We need Connector/C sources to be present in the server source tree. There are (at least) two possibilities:
# download the source tarball using cmake {{FILE(DOWLOAD ...)}} command
# pull them in using git submodule feature
",,"pull in Connector/C sources into the server tree on builds $end$ We need Connector/C sources to be present in the server source tree. There are (at least) two possibilities:
# download the source tarball using cmake {{FILE(DOWLOAD ...)}} command
# pull them in using git submodule feature
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Major,15,,3,4,5,7,0,0,0,,0,850,4,0,0,2016-01-27 11:22:59,pull in Connector/C sources into the server tree on builds,"We need Connector/C sources to be present in the server source tree. There are (at least) two possibilities:
# download the source tarball using cmake {{FILE(DOWLOAD ...)}} command
# pull them in using git submodule feature
",,0,0,0,0,0.0,"pull in Connector/C sources into the server tree on builds $end$ We need Connector/C sources to be present in the server source tree. There are (at least) two possibilities:
# download the source tarball using cmake {{FILE(DOWLOAD ...)}} command
# pull them in using git submodule feature
 $acceptance criteria:$",0,0,0,0,0,0,1,1122.32,25,10,0.4,7,0.28,7,0.28,7,0.28,6,0.24
819,MDEV-9292,Task,MDEV,2015-12-15 22:35:17,,0,10.1.10 merge,"10.0 → 10.1 (/) 10.0.23+
10.0-galera → 10.1 (/) 10.0.22-galera+
Connect 10.1 → 10.1 (/) 1.04.0005",,"10.1.10 merge $end$ 10.0 → 10.1 (/) 10.0.23+
10.0-galera → 10.1 (/) 10.0.22-galera+
Connect 10.1 → 10.1 (/) 1.04.0005 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,6,,0,0,0,1,0,2,0,,0,850,0,0,0,2015-12-15 22:35:17,10.1.10 merge,"10.0 → 10.1
10.0-galera → 10.1
Connect 10.1 → 10.1",,0,2,0,6,0.4,"10.1.10 merge $end$ 10.0 → 10.1
10.0-galera → 10.1
Connect 10.1 → 10.1 $acceptance criteria:$",2,1,1,0,0,0,1,0.0,26,10,0.384615,7,0.269231,7,0.269231,7,0.269231,6,0.230769
820,MDEV-9293,Task,MDEV,2015-12-16 00:41:05,,0,change clients to use Connector/C,,,change clients to use Connector/C $end$ $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Major,28,,4,0,7,11,0,3,0,,0,850,0,3,0,,change clients to use Connector/C,,,0,0,0,0,0.0,change clients to use Connector/C $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,27,11,0.407407,8,0.296296,7,0.259259,7,0.259259,6,0.222222
821,MDEV-9438,Task,MDEV,2016-01-20 13:48:41,,0,backport feedback-http-proxy to 5.5 and 10.0,backport feedback-http-proxy to 5.5 and 10.0,,backport feedback-http-proxy to 5.5 and 10.0 $end$ backport feedback-http-proxy to 5.5 and 10.0 $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Critical,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-02-03 13:41:25,backport feedback-http-proxy to 5.5 and 10.0,backport feedback-http-proxy to 5.5 and 10.0,,0,0,0,0,0.0,backport feedback-http-proxy to 5.5 and 10.0 $end$ backport feedback-http-proxy to 5.5 and 10.0 $acceptance criteria:$,0,0,0,0,0,0,0,335.867,28,11,0.392857,8,0.285714,7,0.25,7,0.25,6,0.214286
822,MDEV-9460,Task,MDEV,2016-01-24 17:07:59,,0,10.1.11 merge,"5.5→10.0
10.0→10.1
10.0-galera→10.1
Connect 10.1→10.1",,"10.1.11 merge $end$ 5.5→10.0
10.0→10.1
10.0-galera→10.1
Connect 10.1→10.1 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-01-24 17:12:11,10.1.11 merge,"5.5→10.0
10.0→10.1
10.0-galera→10.1
Connect 10.1→10.1",,0,0,0,0,0.0,"10.1.11 merge $end$ 5.5→10.0
10.0→10.1
10.0-galera→10.1
Connect 10.1→10.1 $acceptance criteria:$",0,0,0,0,0,0,0,0.0666667,29,11,0.37931,8,0.275862,7,0.241379,7,0.241379,6,0.206897
823,MDEV-9508,Task,MDEV,2016-02-02 16:14:25,,0,5.5.48 merge,"* mysql-5.5.48 (/)
* xtradb (/) 5.5.47-37.7",,"5.5.48 merge $end$ * mysql-5.5.48 (/)
* xtradb (/) 5.5.47-37.7 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,6,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-02-03 15:02:05,5.5.48 merge,"* mysql-5.5.48
* xtradb",,0,1,0,3,0.333333,"5.5.48 merge $end$ * mysql-5.5.48
* xtradb $acceptance criteria:$",1,1,0,0,0,0,0,22.7833,30,11,0.366667,8,0.266667,7,0.233333,7,0.233333,6,0.2
824,MDEV-9526,Task,MDEV,2016-02-06 23:54:16,,0,Compute Aggregate functions as window functions,"One can use regular aggregate functions as window function. Something like

{noformat}
 SUM(col2) OVER (PARTITION BY col1 ORDER BY col2 
                 ROWS BETWEEN 10 PRECEDING AND 10 FOLLOWING)
{noformat}

h1. Task #1: Window frames

When the window frame moves, we have
* A position where rows enter the fame
* A position where rows leave the frame
* A position where the current row is.

That is, we need to maintain three positions.
Note that we're scanning the file by scanning filesort's result (a sequence of rowids). That is, these three positions are positions in filesort's result.


h1. Task# 2: Handle aggregate functions that support removal

The difference from computing an aggregate function in GROUP BY context is the use of window frame. Rows enter the window frame (this is not new) and leave the  window frame (this is new). 

Aggregate functions have code to update the function value after a row has been added.  There  is no code to update the function value after a row has been removed.  Some functions can update their value after a row has been removed. These are
* -SUM-
* -COUNT (need to check for NULL and then remove)-
* -AVG (this is SUM and COUNT)-
* -BIT_OR  (just make every bit a counter)-
* -BIT_AND  (can be reduced to BIT_OR)-
* -BIT_XOR  (removing the value is the same as adding it another time)-

h2. Aggregates that  maybe support removal #1 - numeric
I am not sure if it's possible to use removal
* -VAR_POP()-  (Implemented without removal so far)
* -VARIANCE()- (Implemented without removal so far)
* -VAR_SAMP()- (Implemented without removal so far)
These use a recursive formula to update the current value. It is possible reverse the computation and do the removal, but I am not sure whether this computation is numerically stable.

These functions are just SQRT() of their VARIANCE() counterpart:
* -STD()- (Implemented without removal so far)
* -STDDEV()- (Implemented without removal so far)
* -STDDEV_POP()- (Implemented without removal so far)
* -STDDEV_SAMP()- (Implemented without removal so far)

h2. Aggregates that  maybe support removal #2
* GROUP_CONCAT() 

The result is accumulated in {{String Item_func_group_concat::result}}. Generally, this allows for removal. 
There is a problem with {{@@group_concat_max_len}}.  When the length of the string exceeds this value, 
then a warning is produced, and the new value IS NOT ADDED to the string. Thus, the information about the new value is lost, and we will not be able to produce a correct result after remove() frees space.

Idea from Igor: if we run out of space at the end of the buffer, start re-using free space at the front.  So that the buffer is kind-of circular. 

h1. Task #3: Aggregate functions that don't support value removal
* -MIN()- (Implemented without removal)
* -MAX()- (Implemented without removal)

These functions do not support removal. (If we remove the value that is equal to min, what should be the next value?). Possible options
1. Use a custom solution that works for just MIN and MAX (something like a priority queue that spills to a Unique object?)
2. Use a generic O(N^2) algorithm.
",,"Compute Aggregate functions as window functions $end$ One can use regular aggregate functions as window function. Something like

{noformat}
 SUM(col2) OVER (PARTITION BY col1 ORDER BY col2 
                 ROWS BETWEEN 10 PRECEDING AND 10 FOLLOWING)
{noformat}

h1. Task #1: Window frames

When the window frame moves, we have
* A position where rows enter the fame
* A position where rows leave the frame
* A position where the current row is.

That is, we need to maintain three positions.
Note that we're scanning the file by scanning filesort's result (a sequence of rowids). That is, these three positions are positions in filesort's result.


h1. Task# 2: Handle aggregate functions that support removal

The difference from computing an aggregate function in GROUP BY context is the use of window frame. Rows enter the window frame (this is not new) and leave the  window frame (this is new). 

Aggregate functions have code to update the function value after a row has been added.  There  is no code to update the function value after a row has been removed.  Some functions can update their value after a row has been removed. These are
* -SUM-
* -COUNT (need to check for NULL and then remove)-
* -AVG (this is SUM and COUNT)-
* -BIT_OR  (just make every bit a counter)-
* -BIT_AND  (can be reduced to BIT_OR)-
* -BIT_XOR  (removing the value is the same as adding it another time)-

h2. Aggregates that  maybe support removal #1 - numeric
I am not sure if it's possible to use removal
* -VAR_POP()-  (Implemented without removal so far)
* -VARIANCE()- (Implemented without removal so far)
* -VAR_SAMP()- (Implemented without removal so far)
These use a recursive formula to update the current value. It is possible reverse the computation and do the removal, but I am not sure whether this computation is numerically stable.

These functions are just SQRT() of their VARIANCE() counterpart:
* -STD()- (Implemented without removal so far)
* -STDDEV()- (Implemented without removal so far)
* -STDDEV_POP()- (Implemented without removal so far)
* -STDDEV_SAMP()- (Implemented without removal so far)

h2. Aggregates that  maybe support removal #2
* GROUP_CONCAT() 

The result is accumulated in {{String Item_func_group_concat::result}}. Generally, this allows for removal. 
There is a problem with {{@@group_concat_max_len}}.  When the length of the string exceeds this value, 
then a warning is produced, and the new value IS NOT ADDED to the string. Thus, the information about the new value is lost, and we will not be able to produce a correct result after remove() frees space.

Idea from Igor: if we run out of space at the end of the buffer, start re-using free space at the front.  So that the buffer is kind-of circular. 

h1. Task #3: Aggregate functions that don't support value removal
* -MIN()- (Implemented without removal)
* -MAX()- (Implemented without removal)

These functions do not support removal. (If we remove the value that is equal to min, what should be the next value?). Possible options
1. Use a custom solution that works for just MIN and MAX (something like a priority queue that spills to a Unique object?)
2. Use a generic O(N^2) algorithm.
 $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,27,,0,3,3,2,0,16,0,,0,850,0,7,0,2016-03-03 08:17:07,Compute Aggregate functions as window functions,"One can use regular aggregate functions as window function. Something like

{noformat}
 SUM(col2) OVER (PARTITION BY col1 ORDER BY col2 
                 ROWS BETWEEN 10 PRECEDING AND 10 FOLLOWING)
{noformat}

h1. Task #1: Window frames

When the window frame moves, we have
* A position where rows enter the fame
* A position where rows leave the frame
* A position where the current row is.

That is, we need to maintain three positions.
Note that we're scanning the file by scanning filesort's result (a sequence of rowids). That is, these three positions are positions in filesort's result.


h1. Task# 2: Handle aggregate functions that support removal

The difference from computing an aggregate function in GROUP BY context is the use of window frame. Rows enter the window frame (this is not new) and leave the  window frame (this is new). 

Aggregate functions have code to update the function value after a row has been added.  There  is no code to update the function value after a row has been removed.  Some functions can update their value after a row has been removed. These are
* SUM
* COUNT (need to check for NULL and then remove)
* AVG (this is SUM and COUNT)
* BIT_OR  (just make every bit a counter)
* BIT_AND  (can be reduced to BIT_OR) 
* BIT_XOR  (removing the value is the same as adding it another time)

h2. Aggregates that  maybe support removal #1 - numeric
I am not sure if it's possible to use removal
* VAR_POP()
* VARIANCE()
* VAR_SAMP()
These use a recursive formula to update the current value. It is possible reverse the computation and do the removal, but I am not sure whether this computation is numerically stable.

These functions are just SQRT() of their VARIANCE() counterpart:
* STD()
* STDDEV()
* STDDEV_POP()
* STDDEV_SAMP()

h2. Aggregates that  maybe support removal #2
* GROUP_CONCAT() 

The result is accumulated in {{String Item_func_group_concat::result}}. Generally, this allows for removal. 
There is a problem with {{@@group_concat_max_len}}.  When the length of the string exceeds this value, 
then a warning is produced, and the new value IS NOT ADDED to the string. Thus, the information about the new value is lost, and we will not be able to produce a correct result after remove() frees space.

h1. Task #3: Aggregate functions that don't support value removal
* MIN()
* MAX()

These functions do not support removal. (If we remove the value that is equal to min, what should be the next value?). Possible options
1. Use a custom solution that works for just MIN and MAX (something like a priority queue that spills to a Unique object?)
2. Use a generic O(N^2) algorithm.
",,0,9,0,110,0.198238,"Compute Aggregate functions as window functions $end$ One can use regular aggregate functions as window function. Something like

{noformat}
 SUM(col2) OVER (PARTITION BY col1 ORDER BY col2 
                 ROWS BETWEEN 10 PRECEDING AND 10 FOLLOWING)
{noformat}

h1. Task #1: Window frames

When the window frame moves, we have
* A position where rows enter the fame
* A position where rows leave the frame
* A position where the current row is.

That is, we need to maintain three positions.
Note that we're scanning the file by scanning filesort's result (a sequence of rowids). That is, these three positions are positions in filesort's result.


h1. Task# 2: Handle aggregate functions that support removal

The difference from computing an aggregate function in GROUP BY context is the use of window frame. Rows enter the window frame (this is not new) and leave the  window frame (this is new). 

Aggregate functions have code to update the function value after a row has been added.  There  is no code to update the function value after a row has been removed.  Some functions can update their value after a row has been removed. These are
* SUM
* COUNT (need to check for NULL and then remove)
* AVG (this is SUM and COUNT)
* BIT_OR  (just make every bit a counter)
* BIT_AND  (can be reduced to BIT_OR) 
* BIT_XOR  (removing the value is the same as adding it another time)

h2. Aggregates that  maybe support removal #1 - numeric
I am not sure if it's possible to use removal
* VAR_POP()
* VARIANCE()
* VAR_SAMP()
These use a recursive formula to update the current value. It is possible reverse the computation and do the removal, but I am not sure whether this computation is numerically stable.

These functions are just SQRT() of their VARIANCE() counterpart:
* STD()
* STDDEV()
* STDDEV_POP()
* STDDEV_SAMP()

h2. Aggregates that  maybe support removal #2
* GROUP_CONCAT() 

The result is accumulated in {{String Item_func_group_concat::result}}. Generally, this allows for removal. 
There is a problem with {{@@group_concat_max_len}}.  When the length of the string exceeds this value, 
then a warning is produced, and the new value IS NOT ADDED to the string. Thus, the information about the new value is lost, and we will not be able to produce a correct result after remove() frees space.

h1. Task #3: Aggregate functions that don't support value removal
* MIN()
* MAX()

These functions do not support removal. (If we remove the value that is equal to min, what should be the next value?). Possible options
1. Use a custom solution that works for just MIN and MAX (something like a priority queue that spills to a Unique object?)
2. Use a generic O(N^2) algorithm.
 $acceptance criteria:$",9,1,1,1,1,1,1,608.367,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
825,MDEV-9537,Task,MDEV,2016-02-09 16:38:43,,0,10.0.24 merge,"* 5.5 (/) 5.5.48
* InnoDB (/) 5.6.29
* XtraDB (/) 5.6.28-76.1
* P_S (/) 5.6.29
* Connect (/)
* Spider (/) _nothing to do_
* PCRE (/) _nothing to do_
* Mroonga (/) _nothing to do_
----
* TokuDB (/) 5.6.28-76.1, in a separate branch *10.0-tokudb-merge*. See [lp:1546538|https://bugs.launchpad.net/bugs/1546538]",,"10.0.24 merge $end$ * 5.5 (/) 5.5.48
* InnoDB (/) 5.6.29
* XtraDB (/) 5.6.28-76.1
* P_S (/) 5.6.29
* Connect (/)
* Spider (/) _nothing to do_
* PCRE (/) _nothing to do_
* Mroonga (/) _nothing to do_
----
* TokuDB (/) 5.6.28-76.1, in a separate branch *10.0-tokudb-merge*. See [lp:1546538|https://bugs.launchpad.net/bugs/1546538] $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,13,,2,0,2,1,0,8,0,,0,850,0,0,0,2016-02-10 10:31:31,10.0.24 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB",,0,8,0,31,1.34783,"10.0.24 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB $acceptance criteria:$",8,1,1,1,1,1,1,17.8667,31,12,0.387097,8,0.258065,7,0.225806,7,0.225806,6,0.193548
826,MDEV-9566,Task,MDEV,2016-02-16 20:02:06,,0,Add xtrabackup tool to MariaDB 10.1,"Branch: bb-10.1-xtrabackup

- Done:
-- Added xtrabackup sources to storage/xtradb/xtrabackup.
- -Added cmake rules to find out support for gcrypt, gpg-error, libarchive, quicklz, posix_fadvise
-- Added conditional compilation of features requiring above libraries
-- Added conditional compilation of rest of posix functions
-- Added missing functions to xtradb (InnoDB) code
- Missing:
-- Do not remember if this compiles on Windows
-- Update 10.1 sources (git pull)
-- Update xtrabackup sources (not sure how to do that easily and repeatable)",,"Add xtrabackup tool to MariaDB 10.1 $end$ Branch: bb-10.1-xtrabackup

- Done:
-- Added xtrabackup sources to storage/xtradb/xtrabackup.
- -Added cmake rules to find out support for gcrypt, gpg-error, libarchive, quicklz, posix_fadvise
-- Added conditional compilation of features requiring above libraries
-- Added conditional compilation of rest of posix functions
-- Added missing functions to xtradb (InnoDB) code
- Missing:
-- Do not remember if this compiles on Windows
-- Update 10.1 sources (git pull)
-- Update xtrabackup sources (not sure how to do that easily and repeatable) $acceptance criteria:$",,Jan Lindström,Jan Lindström,Blocker,36,,3,0,9,8,0,0,0,,0,850,0,0,0,2016-04-27 06:59:21,Add xtrabackup tool to MariaDB 10.1,"Branch: bb-10.1-xtrabackup

- Done:
-- Added xtrabackup sources to storage/xtradb/xtrabackup.
- -Added cmake rules to find out support for gcrypt, gpg-error, libarchive, quicklz, posix_fadvise
-- Added conditional compilation of features requiring above libraries
-- Added conditional compilation of rest of posix functions
-- Added missing functions to xtradb (InnoDB) code
- Missing:
-- Do not remember if this compiles on Windows
-- Update 10.1 sources (git pull)
-- Update xtrabackup sources (not sure how to do that easily and repeatable)",,0,0,0,0,0.0,"Add xtrabackup tool to MariaDB 10.1 $end$ Branch: bb-10.1-xtrabackup

- Done:
-- Added xtrabackup sources to storage/xtradb/xtrabackup.
- -Added cmake rules to find out support for gcrypt, gpg-error, libarchive, quicklz, posix_fadvise
-- Added conditional compilation of features requiring above libraries
-- Added conditional compilation of rest of posix functions
-- Added missing functions to xtradb (InnoDB) code
- Missing:
-- Do not remember if this compiles on Windows
-- Update 10.1 sources (git pull)
-- Update xtrabackup sources (not sure how to do that easily and repeatable) $acceptance criteria:$",0,0,0,0,0,0,1,1690.95,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
827,MDEV-9583,Task,MDEV,2016-02-18 13:21:52,,0,10.1.12 merge,"* 10.0 (/)
* 10.0-galera (/)
* Connect 10.1 (/)",,"10.1.12 merge $end$ * 10.0 (/)
* 10.0-galera (/)
* Connect 10.1 (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,9,,0,0,0,1,0,2,0,,0,850,0,0,0,2016-02-18 13:22:13,10.1.12 merge,"* 10.0
* 10.0-galera
* Connect 10.1",,0,2,0,3,0.25,"10.1.12 merge $end$ * 10.0
* 10.0-galera
* Connect 10.1 $acceptance criteria:$",2,1,0,0,0,0,0,0.0,32,13,0.40625,9,0.28125,8,0.25,8,0.25,7,0.21875
828,MDEV-9651,Task,MDEV,2016-02-28 09:52:41,,0,Simplify audit event dispatching,"Currently audit event dispatching looks like:
{noformat}
mysql_audit_notify_connection_connect() -> mysql_audit_notify() -> connection_class_handler() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat}

This construct:
- adds unnecessary complexity when adding new event types
- performance overhead: 6 functions in chains, at least 3 can't be inlined
- shown to be vulnerable to programming mistakes

We should strive for:
{noformat}
mysql_audit_notify_connection_connect() -> plugin->event_notify()
{noformat}

If the latter is complex, at least the following should be trivial to implement:
{noformat}
mysql_audit_notify_connection_connect() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat}",,"Simplify audit event dispatching $end$ Currently audit event dispatching looks like:
{noformat}
mysql_audit_notify_connection_connect() -> mysql_audit_notify() -> connection_class_handler() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat}

This construct:
- adds unnecessary complexity when adding new event types
- performance overhead: 6 functions in chains, at least 3 can't be inlined
- shown to be vulnerable to programming mistakes

We should strive for:
{noformat}
mysql_audit_notify_connection_connect() -> plugin->event_notify()
{noformat}

If the latter is complex, at least the following should be trivial to implement:
{noformat}
mysql_audit_notify_connection_connect() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat} $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,8,,0,1,0,1,0,0,0,,0,850,0,0,0,2016-03-03 08:22:01,Simplify audit event dispatching,"Currently audit event dispatching looks like:
{noformat}
mysql_audit_notify_connection_connect() -> mysql_audit_notify() -> connection_class_handler() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat}

This construct:
- adds unnecessary complexity when adding new event types
- performance overhead: 6 functions in chains, at least 3 can't be inlined
- shown to be vulnerable to programming mistakes

We should strive for:
{noformat}
mysql_audit_notify_connection_connect() -> plugin->event_notify()
{noformat}

If the latter is complex, at least the following should be trivial to implement:
{noformat}
mysql_audit_notify_connection_connect() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat}",,0,0,0,0,0.0,"Simplify audit event dispatching $end$ Currently audit event dispatching looks like:
{noformat}
mysql_audit_notify_connection_connect() -> mysql_audit_notify() -> connection_class_handler() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat}

This construct:
- adds unnecessary complexity when adding new event types
- performance overhead: 6 functions in chains, at least 3 can't be inlined
- shown to be vulnerable to programming mistakes

We should strive for:
{noformat}
mysql_audit_notify_connection_connect() -> plugin->event_notify()
{noformat}

If the latter is complex, at least the following should be trivial to implement:
{noformat}
mysql_audit_notify_connection_connect() -> event_class_dispatch() -> plugins_dispatch() -> plugin->event_notify()
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,94.4833,11,1,0.0909091,0,0.0,0,0.0,0,0.0,0,0.0
829,MDEV-9657,Task,MDEV,2016-02-29 08:39:02,,0,Use /bin/sh,"Remove Linuxism.

Works fine with a POSIX-compat shell",,"Use /bin/sh $end$ Remove Linuxism.

Works fine with a POSIX-compat shell $acceptance criteria:$",,Sergey Vojtovich,Sergey Vojtovich,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-15 15:07:43,Use /bin/sh,"Remove Linuxism.

Works fine with a POSIX-compat shell",,0,0,0,0,0.0,"Use /bin/sh $end$ Remove Linuxism.

Works fine with a POSIX-compat shell $acceptance criteria:$",0,0,0,0,0,0,0,366.467,12,1,0.0833333,0,0.0,0,0.0,0,0.0,0,0.0
830,MDEV-9658,Task,MDEV,2016-02-29 09:30:56,,0,Make MyRocks in MariaDB stable,"This is the umbrella task for including MyRocks Storage Engine from https://github.com/facebook/mysql-5.6 (""FB tree"") into MariaDB. 

h2. Inclusion itself 

See MDEV-10965: Add MyRocks into MariaDB repository 

h2. Packaging 

See MDEV-10966 - Packaging for MariaRocks 

h2. Required changes at the SQL layer 

A list of changes in the FB tree that touch the SQL layer (based on {{git log}} output): 
[65501b5|https://github.com/facebook/mysql-5.6/commit/65501b5] Disable unique checks when replica is lagging 
[dd7eeae|https://github.com/facebook/mysql-5.6/commit/dd7eeae] Issue#250: MyRocks/Innodb different output from query with order ... 
[f0a2ded|https://github.com/facebook/mysql-5.6/commit/f0a2ded] Per database uuid 
[9b439ee|https://github.com/facebook/mysql-5.6/commit/9b439ee] Add option to exclude tables from gap lock check 
[5bcb50c|https://github.com/facebook/mysql-5.6/commit/5bcb50c] Per database uuid 
[e9ef099|https://github.com/facebook/mysql-5.6/commit/e9ef099] Improve code style for classes RDBSE_TABLE_DEF and RDBSE_KEYDEF 
[5a82f20|https://github.com/facebook/mysql-5.6/commit/5a82f20] Add gcc function attributes to MyRocks code with a sprinkle of assertions 
[6078c85|https://github.com/facebook/mysql-5.6/commit/6078c85] Preventing to write ""Got error 122.."" to err log on Gap Lock errors 
[782cc5a|https://github.com/facebook/mysql-5.6/commit/782cc5a] Optionally block/log queries relying on Gap Locks 
[c2dbe68|https://github.com/facebook/mysql-5.6/commit/c2dbe68] Add lock information to lock timeout error message 
[c1f1f0b|https://github.com/facebook/mysql-5.6/commit/c1f1f0b] Issue #108: Index-only scans do not work for partitioned tables and extended keys 
[3c0802f|https://github.com/facebook/mysql-5.6/commit/3c0802f] Improve singled thread replication performance 
[55622f1|https://github.com/facebook/mysql-5.6/commit/55622f1] Issue #75: Prefix bloom filter is not used for LinkBench style range scan 
[ee00797|https://github.com/facebook/mysql-5.6/commit/ee00797] Compile rocksdb library with mysql compiler settings 
[8098b78|https://github.com/facebook/mysql-5.6/commit/8098b78] Add support for reporting keys/deletes skipped in the extra slow query log 
[bcd7646|https://github.com/facebook/mysql-5.6/commit/bcd7646] Supporting START TRANSACTION WITH CONSISTENT \[ROCKSDB] SNAPSHOT 
[ff91c80|https://github.com/facebook/mysql-5.6/commit/ff91c80] Crash safe slave and master in RocksDB 


* also check if [da1d92fd|https://github.com/facebook/mysql-5.6/commit/da1d92fd] (set_end_range) is ported from mysql-5.6 because MyRocks' bloom prefix filter uses that. 
* MariaDB has just got NO_PAD collations: MDEV-9711. MyRocks actually had NO_PAD behaviour for PAD collations, and this was a bug
https://github.com/facebook/mysql-5.6/issues/257.  MyRocks will have to provide PAD/NO_PAD behaviour, accordingly to what charset is used.

* rdb_perf_context.cc uses my_io_perf_sum_atomic_helper() the function is not in MySQL or MariaDB, it comes from [bc4145e7|https://github.com/facebook/mysql-5.6/commit/bc4145e7]

* MDEV-10975: Merging of @@rocksdb_skip_unique_check
* MDEV-10976: Port MyRocks' Read Free Replication to MariaRocks
h2. Group Commit with Binlog (and Optimistic Parallel Replication). 

MyRocks (actually, RocksDB and MyRocks together) need support for high-priority transactions to get MariaDB's optimistic Parallel Replication to work. 

h2. Other issues 
* mysql-test-run should not require any additional parameters when running MyRocks tests.
* packaging for {{myrocks_hotbackup}}
* Do we include MyRocks' RQG tests? It has some MTR tests which actually just run the bundled RQG with a special grammar.
* TODO what is missing? 
",,"Make MyRocks in MariaDB stable $end$ This is the umbrella task for including MyRocks Storage Engine from https://github.com/facebook/mysql-5.6 (""FB tree"") into MariaDB. 

h2. Inclusion itself 

See MDEV-10965: Add MyRocks into MariaDB repository 

h2. Packaging 

See MDEV-10966 - Packaging for MariaRocks 

h2. Required changes at the SQL layer 

A list of changes in the FB tree that touch the SQL layer (based on {{git log}} output): 
[65501b5|https://github.com/facebook/mysql-5.6/commit/65501b5] Disable unique checks when replica is lagging 
[dd7eeae|https://github.com/facebook/mysql-5.6/commit/dd7eeae] Issue#250: MyRocks/Innodb different output from query with order ... 
[f0a2ded|https://github.com/facebook/mysql-5.6/commit/f0a2ded] Per database uuid 
[9b439ee|https://github.com/facebook/mysql-5.6/commit/9b439ee] Add option to exclude tables from gap lock check 
[5bcb50c|https://github.com/facebook/mysql-5.6/commit/5bcb50c] Per database uuid 
[e9ef099|https://github.com/facebook/mysql-5.6/commit/e9ef099] Improve code style for classes RDBSE_TABLE_DEF and RDBSE_KEYDEF 
[5a82f20|https://github.com/facebook/mysql-5.6/commit/5a82f20] Add gcc function attributes to MyRocks code with a sprinkle of assertions 
[6078c85|https://github.com/facebook/mysql-5.6/commit/6078c85] Preventing to write ""Got error 122.."" to err log on Gap Lock errors 
[782cc5a|https://github.com/facebook/mysql-5.6/commit/782cc5a] Optionally block/log queries relying on Gap Locks 
[c2dbe68|https://github.com/facebook/mysql-5.6/commit/c2dbe68] Add lock information to lock timeout error message 
[c1f1f0b|https://github.com/facebook/mysql-5.6/commit/c1f1f0b] Issue #108: Index-only scans do not work for partitioned tables and extended keys 
[3c0802f|https://github.com/facebook/mysql-5.6/commit/3c0802f] Improve singled thread replication performance 
[55622f1|https://github.com/facebook/mysql-5.6/commit/55622f1] Issue #75: Prefix bloom filter is not used for LinkBench style range scan 
[ee00797|https://github.com/facebook/mysql-5.6/commit/ee00797] Compile rocksdb library with mysql compiler settings 
[8098b78|https://github.com/facebook/mysql-5.6/commit/8098b78] Add support for reporting keys/deletes skipped in the extra slow query log 
[bcd7646|https://github.com/facebook/mysql-5.6/commit/bcd7646] Supporting START TRANSACTION WITH CONSISTENT \[ROCKSDB] SNAPSHOT 
[ff91c80|https://github.com/facebook/mysql-5.6/commit/ff91c80] Crash safe slave and master in RocksDB 


* also check if [da1d92fd|https://github.com/facebook/mysql-5.6/commit/da1d92fd] (set_end_range) is ported from mysql-5.6 because MyRocks' bloom prefix filter uses that. 
* MariaDB has just got NO_PAD collations: MDEV-9711. MyRocks actually had NO_PAD behaviour for PAD collations, and this was a bug
https://github.com/facebook/mysql-5.6/issues/257.  MyRocks will have to provide PAD/NO_PAD behaviour, accordingly to what charset is used.

* rdb_perf_context.cc uses my_io_perf_sum_atomic_helper() the function is not in MySQL or MariaDB, it comes from [bc4145e7|https://github.com/facebook/mysql-5.6/commit/bc4145e7]

* MDEV-10975: Merging of @@rocksdb_skip_unique_check
* MDEV-10976: Port MyRocks' Read Free Replication to MariaRocks
h2. Group Commit with Binlog (and Optimistic Parallel Replication). 

MyRocks (actually, RocksDB and MyRocks together) need support for high-priority transactions to get MariaDB's optimistic Parallel Replication to work. 

h2. Other issues 
* mysql-test-run should not require any additional parameters when running MyRocks tests.
* packaging for {{myrocks_hotbackup}}
* Do we include MyRocks' RQG tests? It has some MTR tests which actually just run the bundled RQG with a special grammar.
* TODO what is missing? 
 $acceptance criteria:$",,Colin Charles,Colin Charles,Critical,55,,23,39,47,8,0,26,0,,0,850,34,25,0,2016-10-13 06:32:39,Add MyRocks to MariaDB,"This is the umbrella task for including MyRocks Storage Engine from https://github.com/facebook/mysql-5.6 (""FB tree"") into MariaDB. 

h2. Inclusion itself 

See MDEV-10965: Add MyRocks into MariaDB repository 

h2. Packaging 

See MDEV-10966 - Packaging for MariaRocks 

h2. Required changes at the SQL layer 

A list of changes in the FB tree that touch the SQL layer (based on {{git log}} output): 
[65501b5|https://github.com/facebook/mysql-5.6/commit/65501b5] Disable unique checks when replica is lagging 
[dd7eeae|https://github.com/facebook/mysql-5.6/commit/dd7eeae] Issue#250: MyRocks/Innodb different output from query with order ... 
[f0a2ded|https://github.com/facebook/mysql-5.6/commit/f0a2ded] Per database uuid 
[9b439ee|https://github.com/facebook/mysql-5.6/commit/9b439ee] Add option to exclude tables from gap lock check 
[5bcb50c|https://github.com/facebook/mysql-5.6/commit/5bcb50c] Per database uuid 
[e9ef099|https://github.com/facebook/mysql-5.6/commit/e9ef099] Improve code style for classes RDBSE_TABLE_DEF and RDBSE_KEYDEF 
[5a82f20|https://github.com/facebook/mysql-5.6/commit/5a82f20] Add gcc function attributes to MyRocks code with a sprinkle of assertions 
[6078c85|https://github.com/facebook/mysql-5.6/commit/6078c85] Preventing to write ""Got error 122.."" to err log on Gap Lock errors 
[782cc5a|https://github.com/facebook/mysql-5.6/commit/782cc5a] Optionally block/log queries relying on Gap Locks 
[c2dbe68|https://github.com/facebook/mysql-5.6/commit/c2dbe68] Add lock information to lock timeout error message 
[c1f1f0b|https://github.com/facebook/mysql-5.6/commit/c1f1f0b] Issue #108: Index-only scans do not work for partitioned tables and extended keys 
[3c0802f|https://github.com/facebook/mysql-5.6/commit/3c0802f] Improve singled thread replication performance 
[55622f1|https://github.com/facebook/mysql-5.6/commit/55622f1] Issue #75: Prefix bloom filter is not used for LinkBench style range scan 
[ee00797|https://github.com/facebook/mysql-5.6/commit/ee00797] Compile rocksdb library with mysql compiler settings 
[8098b78|https://github.com/facebook/mysql-5.6/commit/8098b78] Add support for reporting keys/deletes skipped in the extra slow query log 
[bcd7646|https://github.com/facebook/mysql-5.6/commit/bcd7646] Supporting START TRANSACTION WITH CONSISTENT \[ROCKSDB] SNAPSHOT 
[ff91c80|https://github.com/facebook/mysql-5.6/commit/ff91c80] Crash safe slave and master in RocksDB 


* also check if [da1d92fd|https://github.com/facebook/mysql-5.6/commit/da1d92fd] (set_end_range) is ported from mysql-5.6 because MyRocks' bloom prefix filter uses that. 
* MariaDB has just got NO_PAD collations: MDEV-9711. MyRocks actually had NO_PAD behaviour for PAD collations, and this was a bug
https://github.com/facebook/mysql-5.6/issues/257.  MyRocks will have to provide PAD/NO_PAD behaviour, accordingly to what charset is used.

* rdb_perf_context.cc uses my_io_perf_sum_atomic_helper() the function is not in MySQL or MariaDB, it comes from [bc4145e7|https://github.com/facebook/mysql-5.6/commit/bc4145e7]

* MDEV-10975: Merging of @@rocksdb_skip_unique_check
* MDEV-10976: Port MyRocks' Read Free Replication to MariaRocks
h2. Group Commit with Binlog (and Optimistic Parallel Replication). 

MyRocks (actually, RocksDB and MyRocks together) need support for high-priority transactions to get MariaDB's optimistic Parallel Replication to work. 

h2. Other issues 
* mysql-test-run should not require any additional parameters when running MyRocks tests.
* packaging for {{myrocks_hotbackup}}
* Do we include MyRocks' RQG tests? It has some MTR tests which actually just run the bundled RQG with a special grammar.
* TODO what is missing? 
",,1,0,0,5,0.0078329,"Add MyRocks to MariaDB $end$ This is the umbrella task for including MyRocks Storage Engine from https://github.com/facebook/mysql-5.6 (""FB tree"") into MariaDB. 

h2. Inclusion itself 

See MDEV-10965: Add MyRocks into MariaDB repository 

h2. Packaging 

See MDEV-10966 - Packaging for MariaRocks 

h2. Required changes at the SQL layer 

A list of changes in the FB tree that touch the SQL layer (based on {{git log}} output): 
[65501b5|https://github.com/facebook/mysql-5.6/commit/65501b5] Disable unique checks when replica is lagging 
[dd7eeae|https://github.com/facebook/mysql-5.6/commit/dd7eeae] Issue#250: MyRocks/Innodb different output from query with order ... 
[f0a2ded|https://github.com/facebook/mysql-5.6/commit/f0a2ded] Per database uuid 
[9b439ee|https://github.com/facebook/mysql-5.6/commit/9b439ee] Add option to exclude tables from gap lock check 
[5bcb50c|https://github.com/facebook/mysql-5.6/commit/5bcb50c] Per database uuid 
[e9ef099|https://github.com/facebook/mysql-5.6/commit/e9ef099] Improve code style for classes RDBSE_TABLE_DEF and RDBSE_KEYDEF 
[5a82f20|https://github.com/facebook/mysql-5.6/commit/5a82f20] Add gcc function attributes to MyRocks code with a sprinkle of assertions 
[6078c85|https://github.com/facebook/mysql-5.6/commit/6078c85] Preventing to write ""Got error 122.."" to err log on Gap Lock errors 
[782cc5a|https://github.com/facebook/mysql-5.6/commit/782cc5a] Optionally block/log queries relying on Gap Locks 
[c2dbe68|https://github.com/facebook/mysql-5.6/commit/c2dbe68] Add lock information to lock timeout error message 
[c1f1f0b|https://github.com/facebook/mysql-5.6/commit/c1f1f0b] Issue #108: Index-only scans do not work for partitioned tables and extended keys 
[3c0802f|https://github.com/facebook/mysql-5.6/commit/3c0802f] Improve singled thread replication performance 
[55622f1|https://github.com/facebook/mysql-5.6/commit/55622f1] Issue #75: Prefix bloom filter is not used for LinkBench style range scan 
[ee00797|https://github.com/facebook/mysql-5.6/commit/ee00797] Compile rocksdb library with mysql compiler settings 
[8098b78|https://github.com/facebook/mysql-5.6/commit/8098b78] Add support for reporting keys/deletes skipped in the extra slow query log 
[bcd7646|https://github.com/facebook/mysql-5.6/commit/bcd7646] Supporting START TRANSACTION WITH CONSISTENT \[ROCKSDB] SNAPSHOT 
[ff91c80|https://github.com/facebook/mysql-5.6/commit/ff91c80] Crash safe slave and master in RocksDB 


* also check if [da1d92fd|https://github.com/facebook/mysql-5.6/commit/da1d92fd] (set_end_range) is ported from mysql-5.6 because MyRocks' bloom prefix filter uses that. 
* MariaDB has just got NO_PAD collations: MDEV-9711. MyRocks actually had NO_PAD behaviour for PAD collations, and this was a bug
https://github.com/facebook/mysql-5.6/issues/257.  MyRocks will have to provide PAD/NO_PAD behaviour, accordingly to what charset is used.

* rdb_perf_context.cc uses my_io_perf_sum_atomic_helper() the function is not in MySQL or MariaDB, it comes from [bc4145e7|https://github.com/facebook/mysql-5.6/commit/bc4145e7]

* MDEV-10975: Merging of @@rocksdb_skip_unique_check
* MDEV-10976: Port MyRocks' Read Free Replication to MariaRocks
h2. Group Commit with Binlog (and Optimistic Parallel Replication). 

MyRocks (actually, RocksDB and MyRocks together) need support for high-priority transactions to get MariaDB's optimistic Parallel Replication to work. 

h2. Other issues 
* mysql-test-run should not require any additional parameters when running MyRocks tests.
* packaging for {{myrocks_hotbackup}}
* Do we include MyRocks' RQG tests? It has some MTR tests which actually just run the bundled RQG with a special grammar.
* TODO what is missing? 
 $acceptance criteria:$",1,1,1,0,0,0,1,5445.02,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
831,MDEV-9659,Task,MDEV,2016-02-29 10:09:16,,0,Create encryption plugin that utilizes AWS Key Management Service,"* Whenever a new key or a key version is required (e.g {{CREATE TABLE ... ENCRYPTED=YES}}), plugins issues [GenerateDataKeyWithoutPlaintext AWS API call|http://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html]  to generate a new datakey, and stores ciphered key it in a file in the data directory. The file name for a key-number $key and version $ver will be aws-kms-key.$key.$ver
* Ciphered datakeys are decrypted(in memory) using [Decrypt API call | http://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html], and returned by {{get_key()}} encryption API calls.
* The data is encrypted with plain key, using AES-128 or AES-256 , depending on plain key length.
",,"Create encryption plugin that utilizes AWS Key Management Service $end$ * Whenever a new key or a key version is required (e.g {{CREATE TABLE ... ENCRYPTED=YES}}), plugins issues [GenerateDataKeyWithoutPlaintext AWS API call|http://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html]  to generate a new datakey, and stores ciphered key it in a file in the data directory. The file name for a key-number $key and version $ver will be aws-kms-key.$key.$ver
* Ciphered datakeys are decrypted(in memory) using [Decrypt API call | http://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html], and returned by {{get_key()}} encryption API calls.
* The data is encrypted with plain key, using AES-128 or AES-256 , depending on plain key length.
 $acceptance criteria:$",,Vladislav Vaintroub,Vladislav Vaintroub,Major,12,,0,0,0,2,0,1,0,,0,850,0,1,0,2016-03-03 08:17:24,Create encryption plugin that utilizes AWS Key Management Service,"* Whenever a new key or a key version is required (e.g {{CREATE TABLE ... ENCRYPTED=YES}}), plugins issues [GenerateDataKeyWithoutPlaintext AWS API call|http://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html]  to generate a new datakey, and stores ciphered key it in a file in the data directory. The file name for a key-number $key and version $ver will be aws-kms-key.$key.$ver
* Ciphered datakeys are decrypted(in memory) using [Decrypt API call | http://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html], and returned by {{get_key()}} encryption API calls.
* The data is encrypted with plain key, using AES-128 or AES-256 , depending on plain key length.
",,0,0,0,0,0.0,"Create encryption plugin that utilizes AWS Key Management Service $end$ * Whenever a new key or a key version is required (e.g {{CREATE TABLE ... ENCRYPTED=YES}}), plugins issues [GenerateDataKeyWithoutPlaintext AWS API call|http://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html]  to generate a new datakey, and stores ciphered key it in a file in the data directory. The file name for a key-number $key and version $ver will be aws-kms-key.$key.$ver
* Ciphered datakeys are decrypted(in memory) using [Decrypt API call | http://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html], and returned by {{get_key()}} encryption API calls.
* The data is encrypted with plain key, using AES-128 or AES-256 , depending on plain key length.
 $acceptance criteria:$",0,0,0,0,0,0,1,70.1333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
832,MDEV-9664,Technical task,MDEV,2016-03-01 01:45:37,,0,Test MDEV-3944 (Allow derived tables in views),,,Test MDEV-3944 (Allow derived tables in views) $end$ $acceptance criteria:$,,Elena Stepanova,Elena Stepanova,Critical,11,,3,3,4,3,0,0,0,,0,850,3,0,0,2016-03-01 01:45:37,Test MDEV-3944 (Allow derived tables in views),,,0,0,0,0,0.0,Test MDEV-3944 (Allow derived tables in views) $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
833,MDEV-9665,Task,MDEV,2016-03-01 04:45:06,,0,Remove cs->cset->ismbchar(),"This is a sub-task for MDEV-6353, to be done in a separate patch for simplicity.
We'll remove cs->cset->ismbchar() and change the callers to use cs->cset->charlen() instead.
",,"Remove cs->cset->ismbchar() $end$ This is a sub-task for MDEV-6353, to be done in a separate patch for simplicity.
We'll remove cs->cset->ismbchar() and change the callers to use cs->cset->charlen() instead.
 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,9,,0,1,1,1,0,0,0,,0,850,1,0,0,2016-03-09 13:06:34,Remove cs->cset->ismbchar(),"This is a sub-task for MDEV-6353, to be done in a separate patch for simplicity.
We'll remove cs->cset->ismbchar() and change the callers to use cs->cset->charlen() instead.
",,0,0,0,0,0.0,"Remove cs->cset->ismbchar() $end$ This is a sub-task for MDEV-6353, to be done in a separate patch for simplicity.
We'll remove cs->cset->ismbchar() and change the callers to use cs->cset->charlen() instead.
 $acceptance criteria:$",0,0,0,0,0,0,0,200.35,6,3,0.5,3,0.5,3,0.5,2,0.333333,2,0.333333
834,MDEV-9676,Task,MDEV,2016-03-02 16:33:05,,0,RANGE-type frames for window functions,"This is about supporting RANGE-type frames for window functions.

By support, I mean being able to provide Frame_xxxx range bound  classes which are able to ""follow"" the current row.  When the current row moves, range bounds move accordingly, calling sum_item->add() or sum_item->remove() for the rows that go in or out of the frame.

h2. Bound types
* {{UNBOUNDED FOLLOWING}} and {{UNBOUNDED PRECEDING}}  have the same meaning as with ROWS-type frames.
* CURRENT ROW has different meaning
* {{n PRECEDING|FOLLOWING}} have different meanings

h2. ""RANGE ... n PRECEDING|FOLLOWING"" bound

Location of the frame bound depends on the ordering parameter value of the current row.
We need to be able to do addition/subtraction on the current row value. 

{quote}
The declared type of SK shall be numeric, datetime, or interval. 
The declared type of UVS shall be numeric if the declared type of SK is numeric; 
otherwise, it shall be an interval type that may be added to or subtracted from the declared
type of SK according to the Syntax Rules of Subclause 6.30, “<datetime value expression>”, 
and Subclause 6.32, “<interval value expression>”...
{quote}

Q: is numeric just INT_RESULT in MySQL terms, or REAL_RESULT and DECIMAL_RESULT are allowed also?
A: yes, DECIMAL and DOUBLE are allowed. Anything that allows addition.

Q: what should we use for addition? (Items or add manually?)
A: We must use a different addition operation, depending on the datatypes of constants involved. So, it's better to use Items.

Q: what is ""interval type"" in MySQL codebase? Am I correct that date addition/subtraction is only used inside DATE_ADD() ?
A: Yes. DATE_ADD and DATE_SUB have date[time] INTERVALs as parameters.  Intervals are not supported outside these functions.

h2. RANGE ... CURRENT ROW bound

The standard draft says for bound #1:

bq. If WFB1 specifies CURRENT ROW, then remove from WF all rows that are not peers of the current row and that precede the current row in the window ordering defined by WD.

That is, the cursor should stop as soon as it reaches the first peer of the current row (the first peer itself is not included).

For bound #2: 
bq. If WFB2 specifies CURRENT ROW, then remove from WF all rows following the current row in the ordering defined by WD that are not peers of the current row.

That is, the cursor should be ahead of the current row, and stop as soon as it reaches the first non-peer of the current row.

h2. ""Current row"" problem
(this is orthoginal to CURRENT ROW frame bound) Consider
{noformat}
 SUM(col2) OVER (ORDER BY col1 RANGE BETWEEN 10 PRECEDING AND 5 FOLLOWING)
{noformat}

Suppose, we're moving to compute the sum function value for the next row.

1. We get the rowid for the next current_row
2. We need to read current_row
2.1 We compute top_bound_value = (current_row.col1-10) and this tells us where the first range bound should be.
2.2 We move the top bound to be at the last row that still has 
   row.col1 < top_bound_value, calling SUM->remove() for rows that go out of the frame.
2.3 We do the same for bottom_bound.
2.4 Now, we want to update the value for the current_row

The problem with operation 2.4 is that actions made in steps 2.2 and 2.3 have  touched temp.table's handler object. 
If we want to update the current row, we need to read it again. 
We can do that by calling temp_table->file->rnd_pos() but that's one extra row read per each row.
",,"RANGE-type frames for window functions $end$ This is about supporting RANGE-type frames for window functions.

By support, I mean being able to provide Frame_xxxx range bound  classes which are able to ""follow"" the current row.  When the current row moves, range bounds move accordingly, calling sum_item->add() or sum_item->remove() for the rows that go in or out of the frame.

h2. Bound types
* {{UNBOUNDED FOLLOWING}} and {{UNBOUNDED PRECEDING}}  have the same meaning as with ROWS-type frames.
* CURRENT ROW has different meaning
* {{n PRECEDING|FOLLOWING}} have different meanings

h2. ""RANGE ... n PRECEDING|FOLLOWING"" bound

Location of the frame bound depends on the ordering parameter value of the current row.
We need to be able to do addition/subtraction on the current row value. 

{quote}
The declared type of SK shall be numeric, datetime, or interval. 
The declared type of UVS shall be numeric if the declared type of SK is numeric; 
otherwise, it shall be an interval type that may be added to or subtracted from the declared
type of SK according to the Syntax Rules of Subclause 6.30, “<datetime value expression>”, 
and Subclause 6.32, “<interval value expression>”...
{quote}

Q: is numeric just INT_RESULT in MySQL terms, or REAL_RESULT and DECIMAL_RESULT are allowed also?
A: yes, DECIMAL and DOUBLE are allowed. Anything that allows addition.

Q: what should we use for addition? (Items or add manually?)
A: We must use a different addition operation, depending on the datatypes of constants involved. So, it's better to use Items.

Q: what is ""interval type"" in MySQL codebase? Am I correct that date addition/subtraction is only used inside DATE_ADD() ?
A: Yes. DATE_ADD and DATE_SUB have date[time] INTERVALs as parameters.  Intervals are not supported outside these functions.

h2. RANGE ... CURRENT ROW bound

The standard draft says for bound #1:

bq. If WFB1 specifies CURRENT ROW, then remove from WF all rows that are not peers of the current row and that precede the current row in the window ordering defined by WD.

That is, the cursor should stop as soon as it reaches the first peer of the current row (the first peer itself is not included).

For bound #2: 
bq. If WFB2 specifies CURRENT ROW, then remove from WF all rows following the current row in the ordering defined by WD that are not peers of the current row.

That is, the cursor should be ahead of the current row, and stop as soon as it reaches the first non-peer of the current row.

h2. ""Current row"" problem
(this is orthoginal to CURRENT ROW frame bound) Consider
{noformat}
 SUM(col2) OVER (ORDER BY col1 RANGE BETWEEN 10 PRECEDING AND 5 FOLLOWING)
{noformat}

Suppose, we're moving to compute the sum function value for the next row.

1. We get the rowid for the next current_row
2. We need to read current_row
2.1 We compute top_bound_value = (current_row.col1-10) and this tells us where the first range bound should be.
2.2 We move the top bound to be at the last row that still has 
   row.col1 < top_bound_value, calling SUM->remove() for rows that go out of the frame.
2.3 We do the same for bottom_bound.
2.4 Now, we want to update the value for the current_row

The problem with operation 2.4 is that actions made in steps 2.2 and 2.3 have  touched temp.table's handler object. 
If we want to update the current row, we need to read it again. 
We can do that by calling temp_table->file->rnd_pos() but that's one extra row read per each row.
 $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,15,,0,4,3,1,0,5,0,,0,850,4,5,0,2016-03-09 13:03:50,RANGE-type frames for window functions,"This is about supporting RANGE-type frames for window functions.

By support, I mean being able to provide Frame_xxxx range bound  classes which are able to ""follow"" the current row.  When the current row moves, range bounds move accordingly, calling sum_item->add() or sum_item->remove() for the rows that go in or out of the frame.

h2. Bound types
* {{UNBOUNDED FOLLOWING}} and {{UNBOUNDED PRECEDING}}  have the same meaning as with ROWS-type frames.
* CURRENT ROW has different meaning
* {{n PRECEDING|FOLLOWING}} have different meanings

h2. ""RANGE ... n PRECEDING|FOLLOWING"" bound

Location of the frame bound depends on the ordering parameter value of the current row.
We need to be able to do addition/subtraction on the current row value. 

{quote}
The declared type of SK shall be numeric, datetime, or interval. 
The declared type of UVS shall be numeric if the declared type of SK is numeric; 
otherwise, it shall be an interval type that may be added to or subtracted from the declared
type of SK according to the Syntax Rules of Subclause 6.30, “<datetime value expression>”, 
and Subclause 6.32, “<interval value expression>”...
{quote}

Q: is numeric just INT_RESULT in MySQL terms, or REAL_RESULT and DECIMAL_RESULT are allowed also?
A: yes, DECIMAL and DOUBLE are allowed. Anything that allows addition.

Q: what should we use for addition? (Items or add manually?)
A: We must use a different addition operation, depending on the datatypes of constants involved. So, it's better to use Items.

Q: what is ""interval type"" in MySQL codebase? Am I correct that date addition/subtraction is only used inside DATE_ADD() ?
A: Yes. DATE_ADD and DATE_SUB have date[time] INTERVALs as parameters.  Intervals are not supported outside these functions.

h2. RANGE ... CURRENT ROW bound

The standard draft says for bound #1:

bq. If WFB1 specifies CURRENT ROW, then remove from WF all rows that are not peers of the current row and that precede the current row in the window ordering defined by WD.

That is, the cursor should stop as soon as it reaches the first peer of the current row (the first peer itself is not included).

For bound #2: 
bq. If WFB2 specifies CURRENT ROW, then remove from WF all rows following the current row in the ordering defined by WD that are not peers of the current row.

That is, the cursor should be ahead of the current row, and stop as soon as it reaches the first non-peer of the current row.

h2. ""Current row"" problem
(this is orthoginal to CURRENT ROW frame bound) Consider
{noformat}
 SUM(col2) OVER (ORDER BY col1 RANGE BETWEEN 10 PRECEDING AND 5 FOLLOWING)
{noformat}

Suppose, we're moving to compute the sum function value for the next row.

1. We get the rowid for the next current_row
2. We need to read current_row
2.1 We compute top_bound_value = (current_row.col1-10) and this tells us where the first range bound should be.
2.2 We move the top bound to be at the last row that still has 
   row.col1 < top_bound_value, calling SUM->remove() for rows that go out of the frame.
2.3 We do the same for bottom_bound.
2.4 Now, we want to update the value for the current_row

The problem with operation 2.4 is that actions made in steps 2.2 and 2.3 have  touched temp.table's handler object. 
If we want to update the current row, we need to read it again. 
We can do that by calling temp_table->file->rnd_pos() but that's one extra row read per each row.
",,0,0,0,0,0.0,"RANGE-type frames for window functions $end$ This is about supporting RANGE-type frames for window functions.

By support, I mean being able to provide Frame_xxxx range bound  classes which are able to ""follow"" the current row.  When the current row moves, range bounds move accordingly, calling sum_item->add() or sum_item->remove() for the rows that go in or out of the frame.

h2. Bound types
* {{UNBOUNDED FOLLOWING}} and {{UNBOUNDED PRECEDING}}  have the same meaning as with ROWS-type frames.
* CURRENT ROW has different meaning
* {{n PRECEDING|FOLLOWING}} have different meanings

h2. ""RANGE ... n PRECEDING|FOLLOWING"" bound

Location of the frame bound depends on the ordering parameter value of the current row.
We need to be able to do addition/subtraction on the current row value. 

{quote}
The declared type of SK shall be numeric, datetime, or interval. 
The declared type of UVS shall be numeric if the declared type of SK is numeric; 
otherwise, it shall be an interval type that may be added to or subtracted from the declared
type of SK according to the Syntax Rules of Subclause 6.30, “<datetime value expression>”, 
and Subclause 6.32, “<interval value expression>”...
{quote}

Q: is numeric just INT_RESULT in MySQL terms, or REAL_RESULT and DECIMAL_RESULT are allowed also?
A: yes, DECIMAL and DOUBLE are allowed. Anything that allows addition.

Q: what should we use for addition? (Items or add manually?)
A: We must use a different addition operation, depending on the datatypes of constants involved. So, it's better to use Items.

Q: what is ""interval type"" in MySQL codebase? Am I correct that date addition/subtraction is only used inside DATE_ADD() ?
A: Yes. DATE_ADD and DATE_SUB have date[time] INTERVALs as parameters.  Intervals are not supported outside these functions.

h2. RANGE ... CURRENT ROW bound

The standard draft says for bound #1:

bq. If WFB1 specifies CURRENT ROW, then remove from WF all rows that are not peers of the current row and that precede the current row in the window ordering defined by WD.

That is, the cursor should stop as soon as it reaches the first peer of the current row (the first peer itself is not included).

For bound #2: 
bq. If WFB2 specifies CURRENT ROW, then remove from WF all rows following the current row in the ordering defined by WD that are not peers of the current row.

That is, the cursor should be ahead of the current row, and stop as soon as it reaches the first non-peer of the current row.

h2. ""Current row"" problem
(this is orthoginal to CURRENT ROW frame bound) Consider
{noformat}
 SUM(col2) OVER (ORDER BY col1 RANGE BETWEEN 10 PRECEDING AND 5 FOLLOWING)
{noformat}

Suppose, we're moving to compute the sum function value for the next row.

1. We get the rowid for the next current_row
2. We need to read current_row
2.1 We compute top_bound_value = (current_row.col1-10) and this tells us where the first range bound should be.
2.2 We move the top bound to be at the last row that still has 
   row.col1 < top_bound_value, calling SUM->remove() for rows that go out of the frame.
2.3 We do the same for bottom_bound.
2.4 Now, we want to update the value for the current_row

The problem with operation 2.4 is that actions made in steps 2.2 and 2.3 have  touched temp.table's handler object. 
If we want to update the current row, we need to read it again. 
We can do that by calling temp_table->file->rnd_pos() but that's one extra row read per each row.
 $acceptance criteria:$",0,0,0,0,0,0,0,164.5,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
835,MDEV-9695,Technical task,MDEV,2016-03-07 11:33:51,,0,Wrong window frame when using RANGE BETWEEN N FOLLOWING AND PRECEDING,"The sliding window defined by RANGE BETWEEN N FOLLOWING and N PRECEDING does not add/remove rows in the correct order.

Example:

{code:sql}
insert into t1 values
( 1 , 0, 1),
( 2 , 0, 2),
( 3 , 1, 4),
( 4 , 1, 8),
( 5 , 2, 32),
( 6 , 2, 64),
( 7 , 2, 128),
( 8 , 2, 16);

select pk, a, b,
bit_or(b) over (partition by a order by pk ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as bit_or
from t1;

pk	a	b	bit_or
1	0	1	3
2	0	2	3
3	1	4	12
4	1	8	12
5	2	32	96
6	2	64	224
7	2	128	176
8	2	16	48
{code}

The issue lies within the last partition. The removal of elements from the window is wrong. It removes an element 1 row after the one that should be removed.

{noformat}
.....
5	2	32	96     // OK (32 | 64) = 96
6	2	64	224   // OK (32 | 64 | 128) = 224
7	2	128	176   // NOT OK. Should be (64 | 128 | 16) = 208. Instead it is: (32 | 128 | 16).
8	2	16	48     // Again NOT OK. Should be (128 | 16) = 144. Instead it is: (32 | 16)
.....
{noformat}

The problem lies in the fact that the removal of elements from the sliding window is done incorectly.
from [32, 64, 128], we should add the row containing 16 and remove the row containing 32. Instead, when calling the remove() function for the window function, the current item points to row with the value of 64. Thus we remove 64 and get [32, 128, 16].

Again the same problem continues for the next row. Instead of removing the value 64, we are removing the value 128 (the row after the correct value).",,"Wrong window frame when using RANGE BETWEEN N FOLLOWING AND PRECEDING $end$ The sliding window defined by RANGE BETWEEN N FOLLOWING and N PRECEDING does not add/remove rows in the correct order.

Example:

{code:sql}
insert into t1 values
( 1 , 0, 1),
( 2 , 0, 2),
( 3 , 1, 4),
( 4 , 1, 8),
( 5 , 2, 32),
( 6 , 2, 64),
( 7 , 2, 128),
( 8 , 2, 16);

select pk, a, b,
bit_or(b) over (partition by a order by pk ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as bit_or
from t1;

pk	a	b	bit_or
1	0	1	3
2	0	2	3
3	1	4	12
4	1	8	12
5	2	32	96
6	2	64	224
7	2	128	176
8	2	16	48
{code}

The issue lies within the last partition. The removal of elements from the window is wrong. It removes an element 1 row after the one that should be removed.

{noformat}
.....
5	2	32	96     // OK (32 | 64) = 96
6	2	64	224   // OK (32 | 64 | 128) = 224
7	2	128	176   // NOT OK. Should be (64 | 128 | 16) = 208. Instead it is: (32 | 128 | 16).
8	2	16	48     // Again NOT OK. Should be (128 | 16) = 144. Instead it is: (32 | 16)
.....
{noformat}

The problem lies in the fact that the removal of elements from the sliding window is done incorectly.
from [32, 64, 128], we should add the row containing 16 and remove the row containing 32. Instead, when calling the remove() function for the window function, the current item points to row with the value of 64. Thus we remove 64 and get [32, 128, 16].

Again the same problem continues for the next row. Instead of removing the value 64, we are removing the value 128 (the row after the correct value). $acceptance criteria:$",,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,4,,0,1,0,8,0,0,0,,0,850,1,0,0,2016-03-07 11:33:51,Wrong window frame when using RANGE BETWEEN N FOLLOWING AND PRECEDING,"The sliding window defined by RANGE BETWEEN N FOLLOWING and N PRECEDING does not add/remove rows in the correct order.

Example:

{code:sql}
insert into t1 values
( 1 , 0, 1),
( 2 , 0, 2),
( 3 , 1, 4),
( 4 , 1, 8),
( 5 , 2, 32),
( 6 , 2, 64),
( 7 , 2, 128),
( 8 , 2, 16);

select pk, a, b,
bit_or(b) over (partition by a order by pk ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as bit_or
from t1;

pk	a	b	bit_or
1	0	1	3
2	0	2	3
3	1	4	12
4	1	8	12
5	2	32	96
6	2	64	224
7	2	128	176
8	2	16	48
{code}

The issue lies within the last partition. The removal of elements from the window is wrong. It removes an element 1 row after the one that should be removed.

{noformat}
.....
5	2	32	96     // OK (32 | 64) = 96
6	2	64	224   // OK (32 | 64 | 128) = 224
7	2	128	176   // NOT OK. Should be (64 | 128 | 16) = 208. Instead it is: (32 | 128 | 16).
8	2	16	48     // Again NOT OK. Should be (128 | 16) = 144. Instead it is: (32 | 16)
.....
{noformat}

The problem lies in the fact that the removal of elements from the sliding window is done incorectly.
from [32, 64, 128], we should add the row containing 16 and remove the row containing 32. Instead, when calling the remove() function for the window function, the current item points to row with the value of 64. Thus we remove 64 and get [32, 128, 16].

Again the same problem continues for the next row. Instead of removing the value 64, we are removing the value 128 (the row after the correct value).",,0,0,0,0,0.0,"Wrong window frame when using RANGE BETWEEN N FOLLOWING AND PRECEDING $end$ The sliding window defined by RANGE BETWEEN N FOLLOWING and N PRECEDING does not add/remove rows in the correct order.

Example:

{code:sql}
insert into t1 values
( 1 , 0, 1),
( 2 , 0, 2),
( 3 , 1, 4),
( 4 , 1, 8),
( 5 , 2, 32),
( 6 , 2, 64),
( 7 , 2, 128),
( 8 , 2, 16);

select pk, a, b,
bit_or(b) over (partition by a order by pk ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as bit_or
from t1;

pk	a	b	bit_or
1	0	1	3
2	0	2	3
3	1	4	12
4	1	8	12
5	2	32	96
6	2	64	224
7	2	128	176
8	2	16	48
{code}

The issue lies within the last partition. The removal of elements from the window is wrong. It removes an element 1 row after the one that should be removed.

{noformat}
.....
5	2	32	96     // OK (32 | 64) = 96
6	2	64	224   // OK (32 | 64 | 128) = 224
7	2	128	176   // NOT OK. Should be (64 | 128 | 16) = 208. Instead it is: (32 | 128 | 16).
8	2	16	48     // Again NOT OK. Should be (128 | 16) = 144. Instead it is: (32 | 16)
.....
{noformat}

The problem lies in the fact that the removal of elements from the sliding window is done incorectly.
from [32, 64, 128], we should add the row containing 16 and remove the row containing 32. Instead, when calling the remove() function for the window function, the current item points to row with the value of 64. Thus we remove 64 and get [32, 128, 16].

Again the same problem continues for the next row. Instead of removing the value 64, we are removing the value 128 (the row after the correct value). $acceptance criteria:$",0,0,0,0,0,0,1,0.0,1,1,1.0,1,1.0,1,1.0,1,1.0,1,1.0
836,MDEV-9711,Task,MDEV,2016-03-11 14:37:33,MDEV-10872,0,NO PAD collations,"MariaDB currently ignores trailing spaces when comparing values of the CHAR, VARCHAR or TEXT data types.

For example, these three scripts:
{code:sql}
CREATE TABLE t1 (a CHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a VARCHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a TEXT);
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
ignore trailing spaces in the column value and in the string constant and return two rows as a result.

This is correct. According to the standard, the behavior of trailing space comparison does not depend on the data type. It depends only on the collation, and namely on its PAD attribute, which can be PAD SPACE or NO PAD.

The SQL standard says:
{quote}
The comparison of two character string expressions depends on the collation used for the comparison (see Subclause 9.13, “Collation determination”). When values of unequal length are compared, if the collation for the comparison has the NO PAD characteristic and the shorter value is equal to some prefix of the longer value, then the shorter value is considered less than the longer value. If the collation for the comparison has the PAD SPACE characteristic, for the purposes of the comparison, the shorter value is effectively extended to the length of the longer by concatenation of <space>s on the right.
{quote}
MariaDB currently has PAD SPACE collations only, so trailing spaces are always ignored for the CHAR, VARCHAR and TEXT data types.

In certain cases it would be nice to take trailing spaces into account.

Under terms of this task, we want to add NO PAD variants for all default collations and for all _bin collations.

{quote}
Note: We eventually want to add NO PAD variants for all other collations. But this will be done separately. This task is only about adding NO PAD variants for default and _bin collations.
{quote}
New collation names will have the _nopad_ substring. The list of all new desired collations can be extracted using this SQL query:

{code:sql}
SELECT
  CHARACTER_SET_NAME,
  REGEXP_REPLACE(DEFAULT_COLLATE_NAME,'_[^_]*$','_nopad\\0') AS NEW_NAME1,
  CONCAT(CHARACTER_SET_NAME,'_nopad_bin') AS NEW_NAME2
FROM INFORMATION_SCHEMA.CHARACTER_SETS
WHERE CHARACTER_SET_NAME<>'binary';
{code}
{noformat}
+--------------------+---------------------------+--------------------+
| CHARACTER_SET_NAME | NEW_NAME1                 | NEW_NAME2          |
+--------------------+---------------------------+--------------------+
| big5               | big5_chinese_nopad_ci     | big5_nopad_bin     |
| dec8               | dec8_swedish_nopad_ci     | dec8_nopad_bin     |
| cp850              | cp850_general_nopad_ci    | cp850_nopad_bin    |
| hp8                | hp8_english_nopad_ci      | hp8_nopad_bin      |
| koi8r              | koi8r_general_nopad_ci    | koi8r_nopad_bin    |
| latin1             | latin1_swedish_nopad_ci   | latin1_nopad_bin   |
| latin2             | latin2_general_nopad_ci   | latin2_nopad_bin   |
| swe7               | swe7_swedish_nopad_ci     | swe7_nopad_bin     |
| ascii              | ascii_general_nopad_ci    | ascii_nopad_bin    |
| ujis               | ujis_japanese_nopad_ci    | ujis_nopad_bin     |
| sjis               | sjis_japanese_nopad_ci    | sjis_nopad_bin     |
| hebrew             | hebrew_general_nopad_ci   | hebrew_nopad_bin   |
| tis620             | tis620_thai_nopad_ci      | tis620_nopad_bin   |
| euckr              | euckr_korean_nopad_ci     | euckr_nopad_bin    |
| koi8u              | koi8u_general_nopad_ci    | koi8u_nopad_bin    |
| gb2312             | gb2312_chinese_nopad_ci   | gb2312_nopad_bin   |
| greek              | greek_general_nopad_ci    | greek_nopad_bin    |
| cp1250             | cp1250_general_nopad_ci   | cp1250_nopad_bin   |
| gbk                | gbk_chinese_nopad_ci      | gbk_nopad_bin      |
| latin5             | latin5_turkish_nopad_ci   | latin5_nopad_bin   |
| armscii8           | armscii8_general_nopad_ci | armscii8_nopad_bin |
| utf8               | utf8_general_nopad_ci     | utf8_nopad_bin     |
| ucs2               | ucs2_general_nopad_ci     | ucs2_nopad_bin     |
| cp866              | cp866_general_nopad_ci    | cp866_nopad_bin    |
| keybcs2            | keybcs2_general_nopad_ci  | keybcs2_nopad_bin  |
| macce              | macce_general_nopad_ci    | macce_nopad_bin    |
| macroman           | macroman_general_nopad_ci | macroman_nopad_bin |
| cp852              | cp852_general_nopad_ci    | cp852_nopad_bin    |
| latin7             | latin7_general_nopad_ci   | latin7_nopad_bin   |
| utf8mb4            | utf8mb4_general_nopad_ci  | utf8mb4_nopad_bin  |
| cp1251             | cp1251_general_nopad_ci   | cp1251_nopad_bin   |
| utf16              | utf16_general_nopad_ci    | utf16_nopad_bin    |
| utf16le            | utf16le_general_nopad_ci  | utf16le_nopad_bin  |
| cp1256             | cp1256_general_nopad_ci   | cp1256_nopad_bin   |
| cp1257             | cp1257_general_nopad_ci   | cp1257_nopad_bin   |
| utf32              | utf32_general_nopad_ci    | utf32_nopad_bin    |
| geostd8            | geostd8_general_nopad_ci  | geostd8_nopad_bin  |
| cp932              | cp932_japanese_nopad_ci   | cp932_nopad_bin    |
| eucjpms            | eucjpms_japanese_nopad_ci | eucjpms_nopad_bin  |
+--------------------+---------------------------+--------------------+
39 rows in set (0.00 sec)
{noformat}


h1. Implementation details


Suppose we need to add utf8_general_nopad_ci, which will be based on utf8_general_ci but will have the NO PAD attribute.
utf8_general_ci is implemented in strings/ctype-utf8.c.
Adding utf8_general_nopad_ci can be done in these three steps:

1. Add a new collation handler

Copy the collation handler from the existing my_collation_utf8_general_ci_handler which looks like this:

{code:c}
static MY_COLLATION_HANDLER my_collation_utf8_general_ci_handler =
{
    NULL,               /* init */
    my_strnncoll_utf8_general_ci,
    my_strnncollsp_utf8_general_ci,
    my_strnxfrm_unicode,
    my_strnxfrmlen_unicode,
    my_like_range_mb,
    my_wildcmp_utf8,
    my_strcasecmp_utf8,
    my_instr_mb,
    my_hash_sort_utf8,
    my_propagate_complex
};
{code}

and then replace these three virtual functions to new similar functions that will not ignore trailing spaces:

- my_strnncollsp_utf8_general_ci - this is used for BTREE indexes
- my_hash_sort_utf8              - this is used for HASH indexes
- my_strnxfrm_unicode            - this is used for filesort (non-indexed ORDER BY)

All other functions can be reused from the existing PAD SPACE collation.

So the new handler will look about like this:

{code:c|highlight=5,6,12}
static MY_COLLATION_HANDLER my_collation_utf8_general_nopad_ci_handler =
{
  NULL,               /* init */
  my_strnncoll_utf8_general_ci,
  my_strnncollsp_utf8_general_nopad_ci, /* a new function */
  my_strnxfrm_unicode_nopad,           /* a new function */
  my_strnxfrmlen_unicode,
  my_like_range_mb,
  my_wildcmp_utf8,
  my_strcasecmp_utf8,
  my_instr_mb,
  my_hash_sort_utf8_nopad,              /* a new function */
  my_propagate_complex
};
{code}

2. Add a new collation definition by copying it from my_charset_utf8_general_ci

The new definition will look like this:

{code:c|highlight=3,4,30}
struct charset_info_st my_charset_utf8_general_nopad_ci=
{
  333,0,0,             /* number       */
  MY_CS_COMPILED|MY_CS_STRNXFRM|MY_CS_UNICODE|MY_CS_NOPAD,  /* state  */
  ""utf8"",             /* cs name      */
  ""utf8_general_nopad_ci"",  /* name         */
  """",                 /* comment      */
  NULL,               /* tailoring    */
  ctype_utf8,         /* ctype        */
  to_lower_utf8,      /* to_lower     */
  to_upper_utf8,      /* to_upper     */
  to_upper_utf8,      /* sort_order   */
  NULL,               /* uca          */
  NULL,               /* tab_to_uni   */
  NULL,               /* tab_from_uni */
  &my_unicase_default,/* caseinfo     */
  NULL,               /* state_map    */
  NULL,               /* ident_map    */
  1,                  /* strxfrm_multiply */
  1,                  /* caseup_multiply  */
  1,                  /* casedn_multiply  */
  1,                  /* mbminlen     */
  3,                  /* mbmaxlen     */
  0,                  /* min_sort_char */
  0xFFFF,             /* max_sort_char */
  ' ',                /* pad char      */
  0,                  /* escape_with_backslash_is_dangerous */
  1,                  /* levels_for_order   */
  &my_charset_utf8_handler,
  &my_collation_utf8_general_nopad_ci_handler
};
{code}

Notice, it looks very similar to the definition of my_charset_utf8_general_ci but
- has a new distinct ID: 333 (the exact IDs for all new collations will be chosen later)
- has a new distinct name: utf8_general_nopad_ci
- has an extra {{MY_CS_NOPAD}} flag
- does not have {{MY_CS_PRIMARY}} in flags
- uses the new collation handler {{my_collation_utf8_general_nopad_ci_handler}} instead of {{my_collation_utf8_general_ci_handler}}

3. Add a collation initialization code

Open {{mysys/charset-def.c}} and add an initialization line like this:

{code:c|highlight=3}
#ifdef HAVE_CHARSET_utf8
  add_compiled_collation(&my_charset_utf8_general_ci);
  add_compiled_collation(&my_charset_utf8_general_nopad_ci); /* This is the new line */
  add_compiled_collation(&my_charset_utf8_bin);
  add_compiled_collation(&my_charset_utf8_general_mysql500_ci);
{code}

Collations for the other Unicode and Asian character sets and for latin1 are to be added using about the same steps (some minor details may differ though).
Collations for 8-bit character sets are to be done in a different way, see section 4.

h1. Testing
The task will include tests for the MariaDB ""mtr"" test infrastructure in the mysql-test source directory.
Tests will cover all new collations, in all parts of SQL queries involving comparison, including:
- Unique indexes
- UNION DISTINCT
- DISTINCT (with and without indexes)
- ORDER BY (with and without indexes)
- GROUP BY (with and without indexes)
- Mixing NO PAD and PAD collations
- Aggregate functions
-- MIIN(), MAX()
-- COUNT(DISTINCT)
-- GROUP_CONCAT(DISTINCT)
- SQL functions involving comparison
-- LEAST(), GREATEST()
-- IF()
-- NULLIF()
-- CASE WHEN a = b THEN ...
-- CASE a WHEN a0 THEN ...

Tests should cover MyISAM, HEAP and InnoDB tables.

To simplify adding test, we'll create a shared include file, say mysql-test/include/ctype_pad.inc,
which will be then included in all mysql-test/t/ctype_xxx.test files for individual character sets. See ctype_regex.inc as an example of such shared file.

h1. Proposed development order
As collations are quite stand-alone pieces of code and do not affect each other, it's easier to implement new collations one by one, consequently, in separate commits.
Every commit can include collations xxx_nopad_ci and xxx_bin for a single character set and should consist of:
- implementation, as described in ""Implementation details"" (for Unicode and Asian character sets and for latin1) and in section 4 (for 8-bit character sets)
- Tests, as described in ""Testing""

h2. 1. Collations for the Unicode character sets:
- utf8
- utf8mb4
- utf16
- utf16le
- utf32

h2. 2. Collations for the Asian character sets
- big5
- cp932
- eucjpms
- euckr
- gb2312
- gbk
- sjis
- ujis

h2. 3. Collations for latin1
Unlike all other 8-bit character sets, latin1 has a special implementation in ctype-latin1.c, so it's easier to have it in a separate step.
This step will start with introducing new shared handlers:
- my_collation_8bit_simple_nopad_ci_handler
- my_collation_8bit_nopad_bin_handler

and use these handlers to actually add new collations for latin1.
Note, the handlers added at this step will be reused for all other 8-bit character sets at the next step.

h2. 4. Collations for the other 8-bit character sets:
- dec8
- cp850
- hp8
- koi8r
- latin2
- swe7
- ascii
- hebrew
- tis620
- koi8u
- greek
- cp1250
- latin5
- armscii8
- cp866
- keybcs2
- macce
- macroman
- cp852
- latin7
- cp1251
- cp1256
- cp1257
- geostd8

8-bit collations are defined in ctype-extra.c.
Unlike all other ctype-xxxx.c files, this file is not manually written. It's generated from collation definition files sql/share/charsets/*.xml.

Whenever we modify the collation definition files, we do the following procedure to regenerate ctype-extra.c:
{noformat}
cd strings
make conf_to_src
./conf_to_src ../sql/share/charsets/ >ctype-extra2.c
# now make sure that ctype-extra2.c is OK, e.g. ""diff ctype-extra.c ctype-extra2.c""
mv ctype-extra2.c ctype-extra.c
{noformat}
New 8-bit collations should also be generated from the collation definition files.

We'll need the following changes:
h3. 4a. Make collation definition loader understand a new ""nopad"" flag.
The collation definition loader (implemented in ctype.c) should be extended to handle a new flag ""nopad"", so Index.xml can look like this (notice a new line highlighted):
{code:xml|highlight=5}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_nopad_bin""     id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
h3. 4b. Make collation definition loader reuse weight tables
New _ci collations should use exactly the same collating weights with their PAD SPACE counterparts.
For example, a collating weight table for greek_general_ci resides in greek.xml and looks like this:
{code:xml}
<collation name=""greek_general_ci"">
<map>
 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
 10 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F
 20 21 22 23 24 25 26 27 28 29 2A 2B 2C 2D 2E 2F
 30 31 32 33 34 35 36 37 38 39 3A 3B 3C 3D 3E 3F
 40 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 5B 5C 5D 5E 5F
 60 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 7B 7C 7D 7E 7F
 80 81 82 83 84 85 86 87 88 89 8A 8B 8C 8D 8E 8F
 90 91 92 93 94 95 96 97 98 99 9A 9B 9C 9D 9E 9F
 A0 A1 A2 A3 A4 A5 A6 A7 A8 A9 AA AB AC AD AE AF
 B0 B1 B2 B3 B4 B5 C1 B7 C5 C7 C9 BB CF BD D5 D9
 C9 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D2 D3 D4 D5 D6 D7 D8 D9 C9 D5 C1 C5 C7 C9
 D5 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D3 D3 D4 D5 D6 D7 D8 D9 C9 D5 CF D5 D9 FF
</map>
</collation>
{code}
It consists of 256 weights, one weight per code. The first weight corresponds the character with the code 0x00, the last weight corresponds to the character with the code 0xFF.
Notice, the characters 0x41 and 0x61 (Latin letters 'a' and 'A') have the same weight of 0x41. This makes the collation case insensitive.

The easiest way would be just to copy-and-paste the weight table definition and use the copy with a new name greek_general_nopad_ci.
But it would be nice to avoid duplication of definitions. We'll extend ctype.c to understand references to other collations instead of explicit weight table definitions, so a new block for the ""greek"" character set can look like this:

{code:xml|highlight=5,6}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_general_nopad_ci""    id=""xxx""  flag=""nopad"" map=""greek_general_ci""/>
  <collation name=""greek_nopad_bin""           id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
Notice, the ""map"" attribute has another collation name rather than a full weight table definition.

h3. 4c. Add new 8-bit collations
At this sub-step we'll use the changes made in the collation definition loader mentioned in 4a and 4b, and do the following:
- Edit Index.xml:
-- Add new _nopad_bin collations to all 8-bit character sets, as described in the example for ""greek"" in 4a (one collation per character set)
-- Add new _nopad_ci collations to all 8-bit character sets, as described in the example for ""greek"" in 4b (one collation per character set)
- Extend this block in conf_to_src.c:
{code:c}
  if (cs->state & MY_CS_BINSORT)
    fprintf(f,""  &my_collation_8bit_bin_handler,\n"");
  else
    fprintf(f,""  &my_collation_8bit_simple_ci_handler,\n"");
{code}
to check the {{MY_CS_NOPAD}} flag and print {{my_collation_8bit_nopad_bin_handler}} and {{my_collation_8bit_simple_nopad_ci_handler}} (which we added at step 3) when {{MY_CS_NOPAD}} is set.
 

- Regenerate ctype-extra.c, as described in the beginning of the section 4.
- Add tests for the new 8bit collations.

",,"NO PAD collations $end$ MariaDB currently ignores trailing spaces when comparing values of the CHAR, VARCHAR or TEXT data types.

For example, these three scripts:
{code:sql}
CREATE TABLE t1 (a CHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a VARCHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a TEXT);
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
ignore trailing spaces in the column value and in the string constant and return two rows as a result.

This is correct. According to the standard, the behavior of trailing space comparison does not depend on the data type. It depends only on the collation, and namely on its PAD attribute, which can be PAD SPACE or NO PAD.

The SQL standard says:
{quote}
The comparison of two character string expressions depends on the collation used for the comparison (see Subclause 9.13, “Collation determination”). When values of unequal length are compared, if the collation for the comparison has the NO PAD characteristic and the shorter value is equal to some prefix of the longer value, then the shorter value is considered less than the longer value. If the collation for the comparison has the PAD SPACE characteristic, for the purposes of the comparison, the shorter value is effectively extended to the length of the longer by concatenation of <space>s on the right.
{quote}
MariaDB currently has PAD SPACE collations only, so trailing spaces are always ignored for the CHAR, VARCHAR and TEXT data types.

In certain cases it would be nice to take trailing spaces into account.

Under terms of this task, we want to add NO PAD variants for all default collations and for all _bin collations.

{quote}
Note: We eventually want to add NO PAD variants for all other collations. But this will be done separately. This task is only about adding NO PAD variants for default and _bin collations.
{quote}
New collation names will have the _nopad_ substring. The list of all new desired collations can be extracted using this SQL query:

{code:sql}
SELECT
  CHARACTER_SET_NAME,
  REGEXP_REPLACE(DEFAULT_COLLATE_NAME,'_[^_]*$','_nopad\\0') AS NEW_NAME1,
  CONCAT(CHARACTER_SET_NAME,'_nopad_bin') AS NEW_NAME2
FROM INFORMATION_SCHEMA.CHARACTER_SETS
WHERE CHARACTER_SET_NAME<>'binary';
{code}
{noformat}
+--------------------+---------------------------+--------------------+
| CHARACTER_SET_NAME | NEW_NAME1                 | NEW_NAME2          |
+--------------------+---------------------------+--------------------+
| big5               | big5_chinese_nopad_ci     | big5_nopad_bin     |
| dec8               | dec8_swedish_nopad_ci     | dec8_nopad_bin     |
| cp850              | cp850_general_nopad_ci    | cp850_nopad_bin    |
| hp8                | hp8_english_nopad_ci      | hp8_nopad_bin      |
| koi8r              | koi8r_general_nopad_ci    | koi8r_nopad_bin    |
| latin1             | latin1_swedish_nopad_ci   | latin1_nopad_bin   |
| latin2             | latin2_general_nopad_ci   | latin2_nopad_bin   |
| swe7               | swe7_swedish_nopad_ci     | swe7_nopad_bin     |
| ascii              | ascii_general_nopad_ci    | ascii_nopad_bin    |
| ujis               | ujis_japanese_nopad_ci    | ujis_nopad_bin     |
| sjis               | sjis_japanese_nopad_ci    | sjis_nopad_bin     |
| hebrew             | hebrew_general_nopad_ci   | hebrew_nopad_bin   |
| tis620             | tis620_thai_nopad_ci      | tis620_nopad_bin   |
| euckr              | euckr_korean_nopad_ci     | euckr_nopad_bin    |
| koi8u              | koi8u_general_nopad_ci    | koi8u_nopad_bin    |
| gb2312             | gb2312_chinese_nopad_ci   | gb2312_nopad_bin   |
| greek              | greek_general_nopad_ci    | greek_nopad_bin    |
| cp1250             | cp1250_general_nopad_ci   | cp1250_nopad_bin   |
| gbk                | gbk_chinese_nopad_ci      | gbk_nopad_bin      |
| latin5             | latin5_turkish_nopad_ci   | latin5_nopad_bin   |
| armscii8           | armscii8_general_nopad_ci | armscii8_nopad_bin |
| utf8               | utf8_general_nopad_ci     | utf8_nopad_bin     |
| ucs2               | ucs2_general_nopad_ci     | ucs2_nopad_bin     |
| cp866              | cp866_general_nopad_ci    | cp866_nopad_bin    |
| keybcs2            | keybcs2_general_nopad_ci  | keybcs2_nopad_bin  |
| macce              | macce_general_nopad_ci    | macce_nopad_bin    |
| macroman           | macroman_general_nopad_ci | macroman_nopad_bin |
| cp852              | cp852_general_nopad_ci    | cp852_nopad_bin    |
| latin7             | latin7_general_nopad_ci   | latin7_nopad_bin   |
| utf8mb4            | utf8mb4_general_nopad_ci  | utf8mb4_nopad_bin  |
| cp1251             | cp1251_general_nopad_ci   | cp1251_nopad_bin   |
| utf16              | utf16_general_nopad_ci    | utf16_nopad_bin    |
| utf16le            | utf16le_general_nopad_ci  | utf16le_nopad_bin  |
| cp1256             | cp1256_general_nopad_ci   | cp1256_nopad_bin   |
| cp1257             | cp1257_general_nopad_ci   | cp1257_nopad_bin   |
| utf32              | utf32_general_nopad_ci    | utf32_nopad_bin    |
| geostd8            | geostd8_general_nopad_ci  | geostd8_nopad_bin  |
| cp932              | cp932_japanese_nopad_ci   | cp932_nopad_bin    |
| eucjpms            | eucjpms_japanese_nopad_ci | eucjpms_nopad_bin  |
+--------------------+---------------------------+--------------------+
39 rows in set (0.00 sec)
{noformat}


h1. Implementation details


Suppose we need to add utf8_general_nopad_ci, which will be based on utf8_general_ci but will have the NO PAD attribute.
utf8_general_ci is implemented in strings/ctype-utf8.c.
Adding utf8_general_nopad_ci can be done in these three steps:

1. Add a new collation handler

Copy the collation handler from the existing my_collation_utf8_general_ci_handler which looks like this:

{code:c}
static MY_COLLATION_HANDLER my_collation_utf8_general_ci_handler =
{
    NULL,               /* init */
    my_strnncoll_utf8_general_ci,
    my_strnncollsp_utf8_general_ci,
    my_strnxfrm_unicode,
    my_strnxfrmlen_unicode,
    my_like_range_mb,
    my_wildcmp_utf8,
    my_strcasecmp_utf8,
    my_instr_mb,
    my_hash_sort_utf8,
    my_propagate_complex
};
{code}

and then replace these three virtual functions to new similar functions that will not ignore trailing spaces:

- my_strnncollsp_utf8_general_ci - this is used for BTREE indexes
- my_hash_sort_utf8              - this is used for HASH indexes
- my_strnxfrm_unicode            - this is used for filesort (non-indexed ORDER BY)

All other functions can be reused from the existing PAD SPACE collation.

So the new handler will look about like this:

{code:c|highlight=5,6,12}
static MY_COLLATION_HANDLER my_collation_utf8_general_nopad_ci_handler =
{
  NULL,               /* init */
  my_strnncoll_utf8_general_ci,
  my_strnncollsp_utf8_general_nopad_ci, /* a new function */
  my_strnxfrm_unicode_nopad,           /* a new function */
  my_strnxfrmlen_unicode,
  my_like_range_mb,
  my_wildcmp_utf8,
  my_strcasecmp_utf8,
  my_instr_mb,
  my_hash_sort_utf8_nopad,              /* a new function */
  my_propagate_complex
};
{code}

2. Add a new collation definition by copying it from my_charset_utf8_general_ci

The new definition will look like this:

{code:c|highlight=3,4,30}
struct charset_info_st my_charset_utf8_general_nopad_ci=
{
  333,0,0,             /* number       */
  MY_CS_COMPILED|MY_CS_STRNXFRM|MY_CS_UNICODE|MY_CS_NOPAD,  /* state  */
  ""utf8"",             /* cs name      */
  ""utf8_general_nopad_ci"",  /* name         */
  """",                 /* comment      */
  NULL,               /* tailoring    */
  ctype_utf8,         /* ctype        */
  to_lower_utf8,      /* to_lower     */
  to_upper_utf8,      /* to_upper     */
  to_upper_utf8,      /* sort_order   */
  NULL,               /* uca          */
  NULL,               /* tab_to_uni   */
  NULL,               /* tab_from_uni */
  &my_unicase_default,/* caseinfo     */
  NULL,               /* state_map    */
  NULL,               /* ident_map    */
  1,                  /* strxfrm_multiply */
  1,                  /* caseup_multiply  */
  1,                  /* casedn_multiply  */
  1,                  /* mbminlen     */
  3,                  /* mbmaxlen     */
  0,                  /* min_sort_char */
  0xFFFF,             /* max_sort_char */
  ' ',                /* pad char      */
  0,                  /* escape_with_backslash_is_dangerous */
  1,                  /* levels_for_order   */
  &my_charset_utf8_handler,
  &my_collation_utf8_general_nopad_ci_handler
};
{code}

Notice, it looks very similar to the definition of my_charset_utf8_general_ci but
- has a new distinct ID: 333 (the exact IDs for all new collations will be chosen later)
- has a new distinct name: utf8_general_nopad_ci
- has an extra {{MY_CS_NOPAD}} flag
- does not have {{MY_CS_PRIMARY}} in flags
- uses the new collation handler {{my_collation_utf8_general_nopad_ci_handler}} instead of {{my_collation_utf8_general_ci_handler}}

3. Add a collation initialization code

Open {{mysys/charset-def.c}} and add an initialization line like this:

{code:c|highlight=3}
#ifdef HAVE_CHARSET_utf8
  add_compiled_collation(&my_charset_utf8_general_ci);
  add_compiled_collation(&my_charset_utf8_general_nopad_ci); /* This is the new line */
  add_compiled_collation(&my_charset_utf8_bin);
  add_compiled_collation(&my_charset_utf8_general_mysql500_ci);
{code}

Collations for the other Unicode and Asian character sets and for latin1 are to be added using about the same steps (some minor details may differ though).
Collations for 8-bit character sets are to be done in a different way, see section 4.

h1. Testing
The task will include tests for the MariaDB ""mtr"" test infrastructure in the mysql-test source directory.
Tests will cover all new collations, in all parts of SQL queries involving comparison, including:
- Unique indexes
- UNION DISTINCT
- DISTINCT (with and without indexes)
- ORDER BY (with and without indexes)
- GROUP BY (with and without indexes)
- Mixing NO PAD and PAD collations
- Aggregate functions
-- MIIN(), MAX()
-- COUNT(DISTINCT)
-- GROUP_CONCAT(DISTINCT)
- SQL functions involving comparison
-- LEAST(), GREATEST()
-- IF()
-- NULLIF()
-- CASE WHEN a = b THEN ...
-- CASE a WHEN a0 THEN ...

Tests should cover MyISAM, HEAP and InnoDB tables.

To simplify adding test, we'll create a shared include file, say mysql-test/include/ctype_pad.inc,
which will be then included in all mysql-test/t/ctype_xxx.test files for individual character sets. See ctype_regex.inc as an example of such shared file.

h1. Proposed development order
As collations are quite stand-alone pieces of code and do not affect each other, it's easier to implement new collations one by one, consequently, in separate commits.
Every commit can include collations xxx_nopad_ci and xxx_bin for a single character set and should consist of:
- implementation, as described in ""Implementation details"" (for Unicode and Asian character sets and for latin1) and in section 4 (for 8-bit character sets)
- Tests, as described in ""Testing""

h2. 1. Collations for the Unicode character sets:
- utf8
- utf8mb4
- utf16
- utf16le
- utf32

h2. 2. Collations for the Asian character sets
- big5
- cp932
- eucjpms
- euckr
- gb2312
- gbk
- sjis
- ujis

h2. 3. Collations for latin1
Unlike all other 8-bit character sets, latin1 has a special implementation in ctype-latin1.c, so it's easier to have it in a separate step.
This step will start with introducing new shared handlers:
- my_collation_8bit_simple_nopad_ci_handler
- my_collation_8bit_nopad_bin_handler

and use these handlers to actually add new collations for latin1.
Note, the handlers added at this step will be reused for all other 8-bit character sets at the next step.

h2. 4. Collations for the other 8-bit character sets:
- dec8
- cp850
- hp8
- koi8r
- latin2
- swe7
- ascii
- hebrew
- tis620
- koi8u
- greek
- cp1250
- latin5
- armscii8
- cp866
- keybcs2
- macce
- macroman
- cp852
- latin7
- cp1251
- cp1256
- cp1257
- geostd8

8-bit collations are defined in ctype-extra.c.
Unlike all other ctype-xxxx.c files, this file is not manually written. It's generated from collation definition files sql/share/charsets/*.xml.

Whenever we modify the collation definition files, we do the following procedure to regenerate ctype-extra.c:
{noformat}
cd strings
make conf_to_src
./conf_to_src ../sql/share/charsets/ >ctype-extra2.c
# now make sure that ctype-extra2.c is OK, e.g. ""diff ctype-extra.c ctype-extra2.c""
mv ctype-extra2.c ctype-extra.c
{noformat}
New 8-bit collations should also be generated from the collation definition files.

We'll need the following changes:
h3. 4a. Make collation definition loader understand a new ""nopad"" flag.
The collation definition loader (implemented in ctype.c) should be extended to handle a new flag ""nopad"", so Index.xml can look like this (notice a new line highlighted):
{code:xml|highlight=5}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_nopad_bin""     id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
h3. 4b. Make collation definition loader reuse weight tables
New _ci collations should use exactly the same collating weights with their PAD SPACE counterparts.
For example, a collating weight table for greek_general_ci resides in greek.xml and looks like this:
{code:xml}
<collation name=""greek_general_ci"">
<map>
 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
 10 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F
 20 21 22 23 24 25 26 27 28 29 2A 2B 2C 2D 2E 2F
 30 31 32 33 34 35 36 37 38 39 3A 3B 3C 3D 3E 3F
 40 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 5B 5C 5D 5E 5F
 60 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 7B 7C 7D 7E 7F
 80 81 82 83 84 85 86 87 88 89 8A 8B 8C 8D 8E 8F
 90 91 92 93 94 95 96 97 98 99 9A 9B 9C 9D 9E 9F
 A0 A1 A2 A3 A4 A5 A6 A7 A8 A9 AA AB AC AD AE AF
 B0 B1 B2 B3 B4 B5 C1 B7 C5 C7 C9 BB CF BD D5 D9
 C9 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D2 D3 D4 D5 D6 D7 D8 D9 C9 D5 C1 C5 C7 C9
 D5 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D3 D3 D4 D5 D6 D7 D8 D9 C9 D5 CF D5 D9 FF
</map>
</collation>
{code}
It consists of 256 weights, one weight per code. The first weight corresponds the character with the code 0x00, the last weight corresponds to the character with the code 0xFF.
Notice, the characters 0x41 and 0x61 (Latin letters 'a' and 'A') have the same weight of 0x41. This makes the collation case insensitive.

The easiest way would be just to copy-and-paste the weight table definition and use the copy with a new name greek_general_nopad_ci.
But it would be nice to avoid duplication of definitions. We'll extend ctype.c to understand references to other collations instead of explicit weight table definitions, so a new block for the ""greek"" character set can look like this:

{code:xml|highlight=5,6}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_general_nopad_ci""    id=""xxx""  flag=""nopad"" map=""greek_general_ci""/>
  <collation name=""greek_nopad_bin""           id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
Notice, the ""map"" attribute has another collation name rather than a full weight table definition.

h3. 4c. Add new 8-bit collations
At this sub-step we'll use the changes made in the collation definition loader mentioned in 4a and 4b, and do the following:
- Edit Index.xml:
-- Add new _nopad_bin collations to all 8-bit character sets, as described in the example for ""greek"" in 4a (one collation per character set)
-- Add new _nopad_ci collations to all 8-bit character sets, as described in the example for ""greek"" in 4b (one collation per character set)
- Extend this block in conf_to_src.c:
{code:c}
  if (cs->state & MY_CS_BINSORT)
    fprintf(f,""  &my_collation_8bit_bin_handler,\n"");
  else
    fprintf(f,""  &my_collation_8bit_simple_ci_handler,\n"");
{code}
to check the {{MY_CS_NOPAD}} flag and print {{my_collation_8bit_nopad_bin_handler}} and {{my_collation_8bit_simple_nopad_ci_handler}} (which we added at step 3) when {{MY_CS_NOPAD}} is set.
 

- Regenerate ctype-extra.c, as described in the beginning of the section 4.
- Add tests for the new 8bit collations.

 $acceptance criteria:$",,Alexander Barkov,Alexander Barkov,Major,84,,6,1,10,2,0,62,3,,0,850,1,62,0,2016-08-31 07:25:15,NO PAD collations,"MariaDB currently ignores trailing spaces when comparing values of the CHAR, VARCHAR or TEXT data types.

For example, these three scripts:
{code:sql}
CREATE TABLE t1 (a CHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a VARCHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a TEXT);
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
ignore trailing spaces in the column value and in the string constant and return two rows as a result.

This is correct. According to the standard, the behavior of trailing space comparison does not depend on the data type. It depends only on the collation, and namely on its PAD attribute, which can be PAD SPACE or NO PAD.

The SQL standard says:
{quote}
The comparison of two character string expressions depends on the collation used for the comparison (see Subclause 9.13, “Collation determination”). When values of unequal length are compared, if the collation for the comparison has the NO PAD characteristic and the shorter value is equal to some prefix of the longer value, then the shorter value is considered less than the longer value. If the collation for the comparison has the PAD SPACE characteristic, for the purposes of the comparison, the shorter value is effectively extended to the length of the longer by concatenation of <space>s on the right.
{quote}
MariaDB currently has PAD SPACE collations only, so trailing spaces are always ignored for the CHAR, VARCHAR and TEXT data types.

In certain cases it would be nice to take trailing spaces into account.

Under terms of this task, we want to add NO PAD variants for all default collations and for all _bin collations.

{quote}
Note: We eventually want to add NO PAD variants for all other collations. But this will be done separately. This task is only about adding NO PAD variants for default and _bin collations.
{quote}
New collation names will have the _nopad_ substring. The list of all new desired collations can be extracted using this SQL query:

{code:sql}
SELECT
  CHARACTER_SET_NAME,
  REGEXP_REPLACE(DEFAULT_COLLATE_NAME,'_[^_]*$','_nopad\\0') AS NEW_NAME1,
  CONCAT(CHARACTER_SET_NAME,'_nopad_bin') AS NEW_NAME2
FROM INFORMATION_SCHEMA.CHARACTER_SETS
WHERE CHARACTER_SET_NAME<>'binary';
{code}
{noformat}
+--------------------+---------------------------+--------------------+
| CHARACTER_SET_NAME | NEW_NAME1                 | NEW_NAME2          |
+--------------------+---------------------------+--------------------+
| big5               | big5_chinese_nopad_ci     | big5_nopad_bin     |
| dec8               | dec8_swedish_nopad_ci     | dec8_nopad_bin     |
| cp850              | cp850_general_nopad_ci    | cp850_nopad_bin    |
| hp8                | hp8_english_nopad_ci      | hp8_nopad_bin      |
| koi8r              | koi8r_general_nopad_ci    | koi8r_nopad_bin    |
| latin1             | latin1_swedish_nopad_ci   | latin1_nopad_bin   |
| latin2             | latin2_general_nopad_ci   | latin2_nopad_bin   |
| swe7               | swe7_swedish_nopad_ci     | swe7_nopad_bin     |
| ascii              | ascii_general_nopad_ci    | ascii_nopad_bin    |
| ujis               | ujis_japanese_nopad_ci    | ujis_nopad_bin     |
| sjis               | sjis_japanese_nopad_ci    | sjis_nopad_bin     |
| hebrew             | hebrew_general_nopad_ci   | hebrew_nopad_bin   |
| tis620             | tis620_thai_nopad_ci      | tis620_nopad_bin   |
| euckr              | euckr_korean_nopad_ci     | euckr_nopad_bin    |
| koi8u              | koi8u_general_nopad_ci    | koi8u_nopad_bin    |
| gb2312             | gb2312_chinese_nopad_ci   | gb2312_nopad_bin   |
| greek              | greek_general_nopad_ci    | greek_nopad_bin    |
| cp1250             | cp1250_general_nopad_ci   | cp1250_nopad_bin   |
| gbk                | gbk_chinese_nopad_ci      | gbk_nopad_bin      |
| latin5             | latin5_turkish_nopad_ci   | latin5_nopad_bin   |
| armscii8           | armscii8_general_nopad_ci | armscii8_nopad_bin |
| utf8               | utf8_general_nopad_ci     | utf8_nopad_bin     |
| ucs2               | ucs2_general_nopad_ci     | ucs2_nopad_bin     |
| cp866              | cp866_general_nopad_ci    | cp866_nopad_bin    |
| keybcs2            | keybcs2_general_nopad_ci  | keybcs2_nopad_bin  |
| macce              | macce_general_nopad_ci    | macce_nopad_bin    |
| macroman           | macroman_general_nopad_ci | macroman_nopad_bin |
| cp852              | cp852_general_nopad_ci    | cp852_nopad_bin    |
| latin7             | latin7_general_nopad_ci   | latin7_nopad_bin   |
| utf8mb4            | utf8mb4_general_nopad_ci  | utf8mb4_nopad_bin  |
| cp1251             | cp1251_general_nopad_ci   | cp1251_nopad_bin   |
| utf16              | utf16_general_nopad_ci    | utf16_nopad_bin    |
| utf16le            | utf16le_general_nopad_ci  | utf16le_nopad_bin  |
| cp1256             | cp1256_general_nopad_ci   | cp1256_nopad_bin   |
| cp1257             | cp1257_general_nopad_ci   | cp1257_nopad_bin   |
| utf32              | utf32_general_nopad_ci    | utf32_nopad_bin    |
| geostd8            | geostd8_general_nopad_ci  | geostd8_nopad_bin  |
| cp932              | cp932_japanese_nopad_ci   | cp932_nopad_bin    |
| eucjpms            | eucjpms_japanese_nopad_ci | eucjpms_nopad_bin  |
+--------------------+---------------------------+--------------------+
39 rows in set (0.00 sec)
{noformat}


h1. Implementation details


Suppose we need to add utf8_general_nopad_ci, which will be based on utf8_general_ci but will have the NO PAD attribute.
utf8_general_ci is implemented in strings/ctype-utf8.c.
Adding utf8_general_nopad_ci can be done in these three steps:

1. Add a new collation handler

Copy the collation handler from the existing my_collation_utf8_general_ci_handler which looks like this:

{code:c}
static MY_COLLATION_HANDLER my_collation_utf8_general_ci_handler =
{
    NULL,               /* init */
    my_strnncoll_utf8_general_ci,
    my_strnncollsp_utf8_general_ci,
    my_strnxfrm_unicode,
    my_strnxfrmlen_unicode,
    my_like_range_mb,
    my_wildcmp_utf8,
    my_strcasecmp_utf8,
    my_instr_mb,
    my_hash_sort_utf8,
    my_propagate_complex
};
{code}

and then replace these three virtual functions to new similar functions that will not ignore trailing spaces:

- my_strnncollsp_utf8_general_ci - this is used for BTREE indexes
- my_hash_sort_utf8              - this is used for HASH indexes
- my_strnxfrm_unicode            - this is used for filesort (non-indexed ORDER BY)

All other functions can be reused from the existing PAD SPACE collation.

So the new handler will look about like this:

{code:c|highlight=5,6,12}
static MY_COLLATION_HANDLER my_collation_utf8_general_nopad_ci_handler =
{
  NULL,               /* init */
  my_strnncoll_utf8_general_ci,
  my_strnncollsp_utf8_general_nopad_ci, /* a new function */
  my_strnxfrm_unicode_nopad,           /* a new function */
  my_strnxfrmlen_unicode,
  my_like_range_mb,
  my_wildcmp_utf8,
  my_strcasecmp_utf8,
  my_instr_mb,
  my_hash_sort_utf8_nopad,              /* a new function */
  my_propagate_complex
};
{code}

2. Add a new collation definition by copying it from my_charset_utf8_general_ci

The new definition will look like this:

{code:c|highlight=3,4,30}
struct charset_info_st my_charset_utf8_general_nopad_ci=
{
  333,0,0,             /* number       */
  MY_CS_COMPILED|MY_CS_STRNXFRM|MY_CS_UNICODE|MY_CS_NOPAD,  /* state  */
  ""utf8"",             /* cs name      */
  ""utf8_general_nopad_ci"",  /* name         */
  """",                 /* comment      */
  NULL,               /* tailoring    */
  ctype_utf8,         /* ctype        */
  to_lower_utf8,      /* to_lower     */
  to_upper_utf8,      /* to_upper     */
  to_upper_utf8,      /* sort_order   */
  NULL,               /* uca          */
  NULL,               /* tab_to_uni   */
  NULL,               /* tab_from_uni */
  &my_unicase_default,/* caseinfo     */
  NULL,               /* state_map    */
  NULL,               /* ident_map    */
  1,                  /* strxfrm_multiply */
  1,                  /* caseup_multiply  */
  1,                  /* casedn_multiply  */
  1,                  /* mbminlen     */
  3,                  /* mbmaxlen     */
  0,                  /* min_sort_char */
  0xFFFF,             /* max_sort_char */
  ' ',                /* pad char      */
  0,                  /* escape_with_backslash_is_dangerous */
  1,                  /* levels_for_order   */
  &my_charset_utf8_handler,
  &my_collation_utf8_general_nopad_ci_handler
};
{code}

Notice, it looks very similar to the definition of my_charset_utf8_general_ci but
- has a new distinct ID: 333 (the exact IDs for all new collations will be chosen later)
- has a new distinct name: utf8_general_nopad_ci
- has an extra {{MY_CS_NOPAD}} flag
- does not have {{MY_CS_PRIMARY}} in flags
- uses the new collation handler {{my_collation_utf8_general_nopad_ci_handler}} instead of {{my_collation_utf8_general_ci_handler}}

3. Add a collation initialization code

Open {{mysys/charset-def.c}} and add an initialization line like this:

{code:c|highlight=3}
#ifdef HAVE_CHARSET_utf8
  add_compiled_collation(&my_charset_utf8_general_ci);
  add_compiled_collation(&my_charset_utf8_general_nopad_ci); /* This is the new line */
  add_compiled_collation(&my_charset_utf8_bin);
  add_compiled_collation(&my_charset_utf8_general_mysql500_ci);
{code}

Collations for the other Unicode and Asian character sets and for latin1 are to be added using about the same steps (some minor details may differ though).
Collations for 8-bit character sets are to be done in a different way, see section 4.

h1. Testing
The task will include tests for the MariaDB ""mtr"" test infrastructure in the mysql-test source directory.
Tests will cover all new collations, in all parts of SQL queries involving comparison, including:
- Unique indexes
- UNION DISTINCT
- DISTINCT (with and without indexes)
- ORDER BY (with and without indexes)
- GROUP BY (with and without indexes)
- Mixing NO PAD and PAD collations
- Aggregate functions
-- MIIN(), MAX()
-- COUNT(DISTINCT)
-- GROUP_CONCAT(DISTINCT)
- SQL functions involving comparison
-- LEAST(), GREATEST()
-- IF()
-- NULLIF()
-- CASE WHEN a = b THEN ...
-- CASE a WHEN a0 THEN ...

Tests should cover MyISAM, HEAP and InnoDB tables.

To simplify adding test, we'll create a shared include file, say mysql-test/include/ctype_pad.inc,
which will be then included in all mysql-test/t/ctype_xxx.test files for individual character sets. See ctype_regex.inc as an example of such shared file.

h1. Proposed development order
As collations are quite stand-alone pieces of code and do not affect each other, it's easier to implement new collations one by one, consequently, in separate commits.
Every commit can include collations xxx_nopad_ci and xxx_bin for a single character set and should consist of:
- implementation, as described in ""Implementation details"" (for Unicode and Asian character sets and for latin1) and in section 4 (for 8-bit character sets)
- Tests, as described in ""Testing""

h2. 1. Collations for the Unicode character sets:
- utf8
- utf8mb4
- utf16
- utf16le
- utf32

h2. 2. Collations for the Asian character sets
- big5
- cp932
- eucjpms
- euckr
- gb2312
- gbk
- sjis
- ujis

h2. 3. Collations for latin1
Unlike all other 8-bit character sets, latin1 has a special implementation in ctype-latin1.c, so it's easier to have it in a separate step.
This step will start with introducing new shared handlers:
- my_collation_8bit_simple_nopad_ci_handler
- my_collation_8bit_nopad_bin_handler

and use these handlers to actually add new collations for latin1.
Note, the handlers added at this step will be reused for all other 8-bit character sets at the next step.

h2. 4. Collations for the other 8-bit character sets:
- dec8
- cp850
- hp8
- koi8r
- latin2
- swe7
- ascii
- hebrew
- tis620
- koi8u
- greek
- cp1250
- latin5
- armscii8
- cp866
- keybcs2
- macce
- macroman
- cp852
- latin7
- cp1251
- cp1256
- cp1257
- geostd8

8-bit collations are defined in ctype-extra.c.
Unlike all other ctype-xxxx.c files, this file is not manually written. It's generated from collation definition files sql/share/charsets/*.xml.

Whenever we modify the collation definition files, we do the following procedure to regenerate ctype-extra.c:
{noformat}
cd strings
make conf_to_src
./conf_to_src ../sql/share/charsets/ >ctype-extra2.c
# now make sure that ctype-extra2.c is OK, e.g. ""diff ctype-extra.c ctype-extra2.c""
mv ctype-extra2.c ctype-extra.c
{noformat}
New 8-bit collations should also be generated from the collation definition files.

We'll need the following changes:
h3. 4a. Make collation definition loader understand a new ""nopad"" flag.
The collation definition loader (implemented in ctype.c) should be extended to handle a new flag ""nopad"", so Index.xml can look like this (notice a new line highlighted):
{code:xml|highlight=5}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_nopad_bin""     id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
h3. 4b. Make collation definition loader reuse weight tables
New _ci collations should use exactly the same collating weights with their PAD SPACE counterparts.
For example, a collating weight table for greek_general_ci resides in greek.xml and looks like this:
{code:xml}
<collation name=""greek_general_ci"">
<map>
 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
 10 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F
 20 21 22 23 24 25 26 27 28 29 2A 2B 2C 2D 2E 2F
 30 31 32 33 34 35 36 37 38 39 3A 3B 3C 3D 3E 3F
 40 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 5B 5C 5D 5E 5F
 60 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 7B 7C 7D 7E 7F
 80 81 82 83 84 85 86 87 88 89 8A 8B 8C 8D 8E 8F
 90 91 92 93 94 95 96 97 98 99 9A 9B 9C 9D 9E 9F
 A0 A1 A2 A3 A4 A5 A6 A7 A8 A9 AA AB AC AD AE AF
 B0 B1 B2 B3 B4 B5 C1 B7 C5 C7 C9 BB CF BD D5 D9
 C9 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D2 D3 D4 D5 D6 D7 D8 D9 C9 D5 C1 C5 C7 C9
 D5 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D3 D3 D4 D5 D6 D7 D8 D9 C9 D5 CF D5 D9 FF
</map>
</collation>
{code}
It consists of 256 weights, one weight per code. The first weight corresponds the character with the code 0x00, the last weight corresponds to the character with the code 0xFF.
Notice, the characters 0x41 and 0x61 (Latin letters 'a' and 'A') have the same weight of 0x41. This makes the collation case insensitive.

The easiest way would be just to copy-and-paste the weight table definition and use the copy with a new name greek_general_nopad_ci.
But it would be nice to avoid duplication of definitions. We'll extend ctype.c to understand references to other collations instead of explicit weight table definitions, so a new block for the ""greek"" character set can look like this:

{code:xml|highlight=5,6}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_general_nopad_ci""    id=""xxx""  flag=""nopad"" map=""greek_general_ci""/>
  <collation name=""greek_nopad_bin""           id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
Notice, the ""map"" attribute has another collation name rather than a full weight table definition.

h3. 4c. Add new 8-bit collations
At this sub-step we'll use the changes made in the collation definition loader mentioned in 4a and 4b, and do the following:
- Edit Index.xml:
-- Add new _nopad_bin collations to all 8-bit character sets, as described in the example for ""greek"" in 4a (one collation per character set)
-- Add new _nopad_ci collations to all 8-bit character sets, as described in the example for ""greek"" in 4b (one collation per character set)
- Extend this block in conf_to_src.c:
{code:c}
  if (cs->state & MY_CS_BINSORT)
    fprintf(f,""  &my_collation_8bit_bin_handler,\n"");
  else
    fprintf(f,""  &my_collation_8bit_simple_ci_handler,\n"");
{code}
to check the {{MY_CS_NOPAD}} flag and print {{my_collation_8bit_nopad_bin_handler}} and {{my_collation_8bit_simple_nopad_ci_handler}} (which we added at step 3) when {{MY_CS_NOPAD}} is set.
 

- Regenerate ctype-extra.c, as described in the beginning of the section 4.
- Add tests for the new 8bit collations.

",,0,0,0,0,0.0,"NO PAD collations $end$ MariaDB currently ignores trailing spaces when comparing values of the CHAR, VARCHAR or TEXT data types.

For example, these three scripts:
{code:sql}
CREATE TABLE t1 (a CHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a VARCHAR(5));
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
{code:sql}
CREATE TABLE t1 (a TEXT);
INSERT INTO t1 VALUES ('a'),('a     ');
SELECT * FROM t1 WHERE a='a ';
{code}
ignore trailing spaces in the column value and in the string constant and return two rows as a result.

This is correct. According to the standard, the behavior of trailing space comparison does not depend on the data type. It depends only on the collation, and namely on its PAD attribute, which can be PAD SPACE or NO PAD.

The SQL standard says:
{quote}
The comparison of two character string expressions depends on the collation used for the comparison (see Subclause 9.13, “Collation determination”). When values of unequal length are compared, if the collation for the comparison has the NO PAD characteristic and the shorter value is equal to some prefix of the longer value, then the shorter value is considered less than the longer value. If the collation for the comparison has the PAD SPACE characteristic, for the purposes of the comparison, the shorter value is effectively extended to the length of the longer by concatenation of <space>s on the right.
{quote}
MariaDB currently has PAD SPACE collations only, so trailing spaces are always ignored for the CHAR, VARCHAR and TEXT data types.

In certain cases it would be nice to take trailing spaces into account.

Under terms of this task, we want to add NO PAD variants for all default collations and for all _bin collations.

{quote}
Note: We eventually want to add NO PAD variants for all other collations. But this will be done separately. This task is only about adding NO PAD variants for default and _bin collations.
{quote}
New collation names will have the _nopad_ substring. The list of all new desired collations can be extracted using this SQL query:

{code:sql}
SELECT
  CHARACTER_SET_NAME,
  REGEXP_REPLACE(DEFAULT_COLLATE_NAME,'_[^_]*$','_nopad\\0') AS NEW_NAME1,
  CONCAT(CHARACTER_SET_NAME,'_nopad_bin') AS NEW_NAME2
FROM INFORMATION_SCHEMA.CHARACTER_SETS
WHERE CHARACTER_SET_NAME<>'binary';
{code}
{noformat}
+--------------------+---------------------------+--------------------+
| CHARACTER_SET_NAME | NEW_NAME1                 | NEW_NAME2          |
+--------------------+---------------------------+--------------------+
| big5               | big5_chinese_nopad_ci     | big5_nopad_bin     |
| dec8               | dec8_swedish_nopad_ci     | dec8_nopad_bin     |
| cp850              | cp850_general_nopad_ci    | cp850_nopad_bin    |
| hp8                | hp8_english_nopad_ci      | hp8_nopad_bin      |
| koi8r              | koi8r_general_nopad_ci    | koi8r_nopad_bin    |
| latin1             | latin1_swedish_nopad_ci   | latin1_nopad_bin   |
| latin2             | latin2_general_nopad_ci   | latin2_nopad_bin   |
| swe7               | swe7_swedish_nopad_ci     | swe7_nopad_bin     |
| ascii              | ascii_general_nopad_ci    | ascii_nopad_bin    |
| ujis               | ujis_japanese_nopad_ci    | ujis_nopad_bin     |
| sjis               | sjis_japanese_nopad_ci    | sjis_nopad_bin     |
| hebrew             | hebrew_general_nopad_ci   | hebrew_nopad_bin   |
| tis620             | tis620_thai_nopad_ci      | tis620_nopad_bin   |
| euckr              | euckr_korean_nopad_ci     | euckr_nopad_bin    |
| koi8u              | koi8u_general_nopad_ci    | koi8u_nopad_bin    |
| gb2312             | gb2312_chinese_nopad_ci   | gb2312_nopad_bin   |
| greek              | greek_general_nopad_ci    | greek_nopad_bin    |
| cp1250             | cp1250_general_nopad_ci   | cp1250_nopad_bin   |
| gbk                | gbk_chinese_nopad_ci      | gbk_nopad_bin      |
| latin5             | latin5_turkish_nopad_ci   | latin5_nopad_bin   |
| armscii8           | armscii8_general_nopad_ci | armscii8_nopad_bin |
| utf8               | utf8_general_nopad_ci     | utf8_nopad_bin     |
| ucs2               | ucs2_general_nopad_ci     | ucs2_nopad_bin     |
| cp866              | cp866_general_nopad_ci    | cp866_nopad_bin    |
| keybcs2            | keybcs2_general_nopad_ci  | keybcs2_nopad_bin  |
| macce              | macce_general_nopad_ci    | macce_nopad_bin    |
| macroman           | macroman_general_nopad_ci | macroman_nopad_bin |
| cp852              | cp852_general_nopad_ci    | cp852_nopad_bin    |
| latin7             | latin7_general_nopad_ci   | latin7_nopad_bin   |
| utf8mb4            | utf8mb4_general_nopad_ci  | utf8mb4_nopad_bin  |
| cp1251             | cp1251_general_nopad_ci   | cp1251_nopad_bin   |
| utf16              | utf16_general_nopad_ci    | utf16_nopad_bin    |
| utf16le            | utf16le_general_nopad_ci  | utf16le_nopad_bin  |
| cp1256             | cp1256_general_nopad_ci   | cp1256_nopad_bin   |
| cp1257             | cp1257_general_nopad_ci   | cp1257_nopad_bin   |
| utf32              | utf32_general_nopad_ci    | utf32_nopad_bin    |
| geostd8            | geostd8_general_nopad_ci  | geostd8_nopad_bin  |
| cp932              | cp932_japanese_nopad_ci   | cp932_nopad_bin    |
| eucjpms            | eucjpms_japanese_nopad_ci | eucjpms_nopad_bin  |
+--------------------+---------------------------+--------------------+
39 rows in set (0.00 sec)
{noformat}


h1. Implementation details


Suppose we need to add utf8_general_nopad_ci, which will be based on utf8_general_ci but will have the NO PAD attribute.
utf8_general_ci is implemented in strings/ctype-utf8.c.
Adding utf8_general_nopad_ci can be done in these three steps:

1. Add a new collation handler

Copy the collation handler from the existing my_collation_utf8_general_ci_handler which looks like this:

{code:c}
static MY_COLLATION_HANDLER my_collation_utf8_general_ci_handler =
{
    NULL,               /* init */
    my_strnncoll_utf8_general_ci,
    my_strnncollsp_utf8_general_ci,
    my_strnxfrm_unicode,
    my_strnxfrmlen_unicode,
    my_like_range_mb,
    my_wildcmp_utf8,
    my_strcasecmp_utf8,
    my_instr_mb,
    my_hash_sort_utf8,
    my_propagate_complex
};
{code}

and then replace these three virtual functions to new similar functions that will not ignore trailing spaces:

- my_strnncollsp_utf8_general_ci - this is used for BTREE indexes
- my_hash_sort_utf8              - this is used for HASH indexes
- my_strnxfrm_unicode            - this is used for filesort (non-indexed ORDER BY)

All other functions can be reused from the existing PAD SPACE collation.

So the new handler will look about like this:

{code:c|highlight=5,6,12}
static MY_COLLATION_HANDLER my_collation_utf8_general_nopad_ci_handler =
{
  NULL,               /* init */
  my_strnncoll_utf8_general_ci,
  my_strnncollsp_utf8_general_nopad_ci, /* a new function */
  my_strnxfrm_unicode_nopad,           /* a new function */
  my_strnxfrmlen_unicode,
  my_like_range_mb,
  my_wildcmp_utf8,
  my_strcasecmp_utf8,
  my_instr_mb,
  my_hash_sort_utf8_nopad,              /* a new function */
  my_propagate_complex
};
{code}

2. Add a new collation definition by copying it from my_charset_utf8_general_ci

The new definition will look like this:

{code:c|highlight=3,4,30}
struct charset_info_st my_charset_utf8_general_nopad_ci=
{
  333,0,0,             /* number       */
  MY_CS_COMPILED|MY_CS_STRNXFRM|MY_CS_UNICODE|MY_CS_NOPAD,  /* state  */
  ""utf8"",             /* cs name      */
  ""utf8_general_nopad_ci"",  /* name         */
  """",                 /* comment      */
  NULL,               /* tailoring    */
  ctype_utf8,         /* ctype        */
  to_lower_utf8,      /* to_lower     */
  to_upper_utf8,      /* to_upper     */
  to_upper_utf8,      /* sort_order   */
  NULL,               /* uca          */
  NULL,               /* tab_to_uni   */
  NULL,               /* tab_from_uni */
  &my_unicase_default,/* caseinfo     */
  NULL,               /* state_map    */
  NULL,               /* ident_map    */
  1,                  /* strxfrm_multiply */
  1,                  /* caseup_multiply  */
  1,                  /* casedn_multiply  */
  1,                  /* mbminlen     */
  3,                  /* mbmaxlen     */
  0,                  /* min_sort_char */
  0xFFFF,             /* max_sort_char */
  ' ',                /* pad char      */
  0,                  /* escape_with_backslash_is_dangerous */
  1,                  /* levels_for_order   */
  &my_charset_utf8_handler,
  &my_collation_utf8_general_nopad_ci_handler
};
{code}

Notice, it looks very similar to the definition of my_charset_utf8_general_ci but
- has a new distinct ID: 333 (the exact IDs for all new collations will be chosen later)
- has a new distinct name: utf8_general_nopad_ci
- has an extra {{MY_CS_NOPAD}} flag
- does not have {{MY_CS_PRIMARY}} in flags
- uses the new collation handler {{my_collation_utf8_general_nopad_ci_handler}} instead of {{my_collation_utf8_general_ci_handler}}

3. Add a collation initialization code

Open {{mysys/charset-def.c}} and add an initialization line like this:

{code:c|highlight=3}
#ifdef HAVE_CHARSET_utf8
  add_compiled_collation(&my_charset_utf8_general_ci);
  add_compiled_collation(&my_charset_utf8_general_nopad_ci); /* This is the new line */
  add_compiled_collation(&my_charset_utf8_bin);
  add_compiled_collation(&my_charset_utf8_general_mysql500_ci);
{code}

Collations for the other Unicode and Asian character sets and for latin1 are to be added using about the same steps (some minor details may differ though).
Collations for 8-bit character sets are to be done in a different way, see section 4.

h1. Testing
The task will include tests for the MariaDB ""mtr"" test infrastructure in the mysql-test source directory.
Tests will cover all new collations, in all parts of SQL queries involving comparison, including:
- Unique indexes
- UNION DISTINCT
- DISTINCT (with and without indexes)
- ORDER BY (with and without indexes)
- GROUP BY (with and without indexes)
- Mixing NO PAD and PAD collations
- Aggregate functions
-- MIIN(), MAX()
-- COUNT(DISTINCT)
-- GROUP_CONCAT(DISTINCT)
- SQL functions involving comparison
-- LEAST(), GREATEST()
-- IF()
-- NULLIF()
-- CASE WHEN a = b THEN ...
-- CASE a WHEN a0 THEN ...

Tests should cover MyISAM, HEAP and InnoDB tables.

To simplify adding test, we'll create a shared include file, say mysql-test/include/ctype_pad.inc,
which will be then included in all mysql-test/t/ctype_xxx.test files for individual character sets. See ctype_regex.inc as an example of such shared file.

h1. Proposed development order
As collations are quite stand-alone pieces of code and do not affect each other, it's easier to implement new collations one by one, consequently, in separate commits.
Every commit can include collations xxx_nopad_ci and xxx_bin for a single character set and should consist of:
- implementation, as described in ""Implementation details"" (for Unicode and Asian character sets and for latin1) and in section 4 (for 8-bit character sets)
- Tests, as described in ""Testing""

h2. 1. Collations for the Unicode character sets:
- utf8
- utf8mb4
- utf16
- utf16le
- utf32

h2. 2. Collations for the Asian character sets
- big5
- cp932
- eucjpms
- euckr
- gb2312
- gbk
- sjis
- ujis

h2. 3. Collations for latin1
Unlike all other 8-bit character sets, latin1 has a special implementation in ctype-latin1.c, so it's easier to have it in a separate step.
This step will start with introducing new shared handlers:
- my_collation_8bit_simple_nopad_ci_handler
- my_collation_8bit_nopad_bin_handler

and use these handlers to actually add new collations for latin1.
Note, the handlers added at this step will be reused for all other 8-bit character sets at the next step.

h2. 4. Collations for the other 8-bit character sets:
- dec8
- cp850
- hp8
- koi8r
- latin2
- swe7
- ascii
- hebrew
- tis620
- koi8u
- greek
- cp1250
- latin5
- armscii8
- cp866
- keybcs2
- macce
- macroman
- cp852
- latin7
- cp1251
- cp1256
- cp1257
- geostd8

8-bit collations are defined in ctype-extra.c.
Unlike all other ctype-xxxx.c files, this file is not manually written. It's generated from collation definition files sql/share/charsets/*.xml.

Whenever we modify the collation definition files, we do the following procedure to regenerate ctype-extra.c:
{noformat}
cd strings
make conf_to_src
./conf_to_src ../sql/share/charsets/ >ctype-extra2.c
# now make sure that ctype-extra2.c is OK, e.g. ""diff ctype-extra.c ctype-extra2.c""
mv ctype-extra2.c ctype-extra.c
{noformat}
New 8-bit collations should also be generated from the collation definition files.

We'll need the following changes:
h3. 4a. Make collation definition loader understand a new ""nopad"" flag.
The collation definition loader (implemented in ctype.c) should be extended to handle a new flag ""nopad"", so Index.xml can look like this (notice a new line highlighted):
{code:xml|highlight=5}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_nopad_bin""     id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
h3. 4b. Make collation definition loader reuse weight tables
New _ci collations should use exactly the same collating weights with their PAD SPACE counterparts.
For example, a collating weight table for greek_general_ci resides in greek.xml and looks like this:
{code:xml}
<collation name=""greek_general_ci"">
<map>
 00 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F
 10 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F
 20 21 22 23 24 25 26 27 28 29 2A 2B 2C 2D 2E 2F
 30 31 32 33 34 35 36 37 38 39 3A 3B 3C 3D 3E 3F
 40 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 5B 5C 5D 5E 5F
 60 41 42 43 44 45 46 47 48 49 4A 4B 4C 4D 4E 4F
 50 51 52 53 54 55 56 57 58 59 5A 7B 7C 7D 7E 7F
 80 81 82 83 84 85 86 87 88 89 8A 8B 8C 8D 8E 8F
 90 91 92 93 94 95 96 97 98 99 9A 9B 9C 9D 9E 9F
 A0 A1 A2 A3 A4 A5 A6 A7 A8 A9 AA AB AC AD AE AF
 B0 B1 B2 B3 B4 B5 C1 B7 C5 C7 C9 BB CF BD D5 D9
 C9 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D2 D3 D4 D5 D6 D7 D8 D9 C9 D5 C1 C5 C7 C9
 D5 C1 C2 C3 C4 C5 C6 C7 C8 C9 CA CB CC CD CE CF
 D0 D1 D3 D3 D4 D5 D6 D7 D8 D9 C9 D5 CF D5 D9 FF
</map>
</collation>
{code}
It consists of 256 weights, one weight per code. The first weight corresponds the character with the code 0x00, the last weight corresponds to the character with the code 0xFF.
Notice, the characters 0x41 and 0x61 (Latin letters 'a' and 'A') have the same weight of 0x41. This makes the collation case insensitive.

The easiest way would be just to copy-and-paste the weight table definition and use the copy with a new name greek_general_nopad_ci.
But it would be nice to avoid duplication of definitions. We'll extend ctype.c to understand references to other collations instead of explicit weight table definitions, so a new block for the ""greek"" character set can look like this:

{code:xml|highlight=5,6}
<charset name=""greek"">
  ...
  <collation name=""greek_general_ci""    id=""25"" order=""Greek""   flag=""primary""/>
  <collation name=""greek_bin""           id=""70"" order=""Binary""  flag=""binary""/>
  <collation name=""greek_general_nopad_ci""    id=""xxx""  flag=""nopad"" map=""greek_general_ci""/>
  <collation name=""greek_nopad_bin""           id=""xxx""  flag=""binary"" flag=""nopad""/>
</charset>
{code}
Notice, the ""map"" attribute has another collation name rather than a full weight table definition.

h3. 4c. Add new 8-bit collations
At this sub-step we'll use the changes made in the collation definition loader mentioned in 4a and 4b, and do the following:
- Edit Index.xml:
-- Add new _nopad_bin collations to all 8-bit character sets, as described in the example for ""greek"" in 4a (one collation per character set)
-- Add new _nopad_ci collations to all 8-bit character sets, as described in the example for ""greek"" in 4b (one collation per character set)
- Extend this block in conf_to_src.c:
{code:c}
  if (cs->state & MY_CS_BINSORT)
    fprintf(f,""  &my_collation_8bit_bin_handler,\n"");
  else
    fprintf(f,""  &my_collation_8bit_simple_ci_handler,\n"");
{code}
to check the {{MY_CS_NOPAD}} flag and print {{my_collation_8bit_nopad_bin_handler}} and {{my_collation_8bit_simple_nopad_ci_handler}} (which we added at step 3) when {{MY_CS_NOPAD}} is set.
 

- Regenerate ctype-extra.c, as described in the beginning of the section 4.
- Add tests for the new 8bit collations.

 $acceptance criteria:$",0,0,0,0,0,0,1,4144.78,7,3,0.428571,3,0.428571,3,0.428571,2,0.285714,2,0.285714
837,MDEV-9731,Task,MDEV,2016-03-15 13:42:15,,0,Fix a memory leak in handlersocket and a cleanup bug,this patch fixes two bugs in handlersocket,,Fix a memory leak in handlersocket and a cleanup bug $end$ this patch fixes two bugs in handlersocket $acceptance criteria:$,,Sergey Vojtovich,Sergey Vojtovich,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-16 09:00:50,Fix a memory leak in handlersocket and a cleanup bug,this patch fixes two bugs in handlersocket,,0,0,0,0,0.0,Fix a memory leak in handlersocket and a cleanup bug $end$ this patch fixes two bugs in handlersocket $acceptance criteria:$,0,0,0,0,0,0,0,19.3,13,1,0.0769231,0,0.0,0,0.0,0,0.0,0,0.0
838,MDEV-9732,Task,MDEV,2016-03-15 15:04:55,,0,10.1.13 merge,"* 10.0 (/)
* 10.0-galera (/) _nothing to do_
* Connect 10.1 (/) 1.04.0006",,"10.1.13 merge $end$ * 10.0 (/)
* 10.0-galera (/) _nothing to do_
* Connect 10.1 (/) 1.04.0006 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,9,,0,0,0,1,0,2,0,,0,850,0,0,0,2016-03-15 15:06:56,10.1.13 merge,"* 10.0
* 10.0-galera
* Connect 10.1",,0,2,0,7,0.583333,"10.1.13 merge $end$ * 10.0
* 10.0-galera
* Connect 10.1 $acceptance criteria:$",2,1,1,0,0,0,1,0.0333333,33,14,0.424242,9,0.272727,8,0.242424,8,0.242424,7,0.212121
839,MDEV-9746,Task,MDEV,2016-03-16 12:26:32,,0,Window functions: CUME_DIST function,"CUME_DIST is a bit special. It needs to either 

1. remember a value for each peer group somewhere,
2. three cursors.

Details:
{noformat}
create table t1 (
  pk int, 
  a int
);

insert into t1 values
(1, 20),
(2, 20),
(3, 20),
(4, 20),
(5, 30),
(6, 30),
(7, 30),
(8, 40);
{noformat}

{noformat}
select pk,a,cume_dist() over (order by a) from t1;
+------+------+
| pk   | a    |
+------+------+
|    1 |   20 | 0.5
|    2 |   20 | 0.5
|    3 |   20 | 0.5
|    4 |   20 | 0.5
|    5 |   30 | 0.875
|    6 |   30 | 0.875
|    7 |   30 | 0.875
|    8 |   40 | 1
+------+------+
{noformat}

How to compute these?
1. The first pass needs to find how many rows are in the partition.
 (Q: BTW, is it true that ""any two-pass window function needs the first pass
  only to find #rows in the partition""?)

2. On the second pass, we need
  2.1 find how many peers are in the peer group (5 rows have a=20)
  2.2 compute the value and set it for every peer member.

Possible ways to do step#2:
A: make it in two passes: 
  - first, let the first row of the group store #values in the group
  - second, propagate this value to all group members.

B: Use two cursors. 
- The front cursor reaches the end of the peer group, remembers how many values
  it saw and then stays there.
- The back cursor walks through the peer group and updates the values.


",,"Window functions: CUME_DIST function $end$ CUME_DIST is a bit special. It needs to either 

1. remember a value for each peer group somewhere,
2. three cursors.

Details:
{noformat}
create table t1 (
  pk int, 
  a int
);

insert into t1 values
(1, 20),
(2, 20),
(3, 20),
(4, 20),
(5, 30),
(6, 30),
(7, 30),
(8, 40);
{noformat}

{noformat}
select pk,a,cume_dist() over (order by a) from t1;
+------+------+
| pk   | a    |
+------+------+
|    1 |   20 | 0.5
|    2 |   20 | 0.5
|    3 |   20 | 0.5
|    4 |   20 | 0.5
|    5 |   30 | 0.875
|    6 |   30 | 0.875
|    7 |   30 | 0.875
|    8 |   40 | 1
+------+------+
{noformat}

How to compute these?
1. The first pass needs to find how many rows are in the partition.
 (Q: BTW, is it true that ""any two-pass window function needs the first pass
  only to find #rows in the partition""?)

2. On the second pass, we need
  2.1 find how many peers are in the peer group (5 rows have a=20)
  2.2 compute the value and set it for every peer member.

Possible ways to do step#2:
A: make it in two passes: 
  - first, let the first row of the group store #values in the group
  - second, propagate this value to all group members.

B: Use two cursors. 
- The front cursor reaches the end of the peer group, remembers how many values
  it saw and then stays there.
- The back cursor walks through the peer group and updates the values.


 $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,6,,0,1,1,1,0,0,0,,0,850,0,0,0,2016-03-23 10:53:58,Window functions: CUME_DIST function,"CUME_DIST is a bit special. It needs to either 

1. remember a value for each peer group somewhere,
2. three cursors.

Details:
{noformat}
create table t1 (
  pk int, 
  a int
);

insert into t1 values
(1, 20),
(2, 20),
(3, 20),
(4, 20),
(5, 30),
(6, 30),
(7, 30),
(8, 40);
{noformat}

{noformat}
select pk,a,cume_dist() over (order by a) from t1;
+------+------+
| pk   | a    |
+------+------+
|    1 |   20 | 0.5
|    2 |   20 | 0.5
|    3 |   20 | 0.5
|    4 |   20 | 0.5
|    5 |   30 | 0.875
|    6 |   30 | 0.875
|    7 |   30 | 0.875
|    8 |   40 | 1
+------+------+
{noformat}

How to compute these?
1. The first pass needs to find how many rows are in the partition.
 (Q: BTW, is it true that ""any two-pass window function needs the first pass
  only to find #rows in the partition""?)

2. On the second pass, we need
  2.1 find how many peers are in the peer group (5 rows have a=20)
  2.2 compute the value and set it for every peer member.

Possible ways to do step#2:
A: make it in two passes: 
  - first, let the first row of the group store #values in the group
  - second, propagate this value to all group members.

B: Use two cursors. 
- The front cursor reaches the end of the peer group, remembers how many values
  it saw and then stays there.
- The back cursor walks through the peer group and updates the values.


",,0,0,0,0,0.0,"Window functions: CUME_DIST function $end$ CUME_DIST is a bit special. It needs to either 

1. remember a value for each peer group somewhere,
2. three cursors.

Details:
{noformat}
create table t1 (
  pk int, 
  a int
);

insert into t1 values
(1, 20),
(2, 20),
(3, 20),
(4, 20),
(5, 30),
(6, 30),
(7, 30),
(8, 40);
{noformat}

{noformat}
select pk,a,cume_dist() over (order by a) from t1;
+------+------+
| pk   | a    |
+------+------+
|    1 |   20 | 0.5
|    2 |   20 | 0.5
|    3 |   20 | 0.5
|    4 |   20 | 0.5
|    5 |   30 | 0.875
|    6 |   30 | 0.875
|    7 |   30 | 0.875
|    8 |   40 | 1
+------+------+
{noformat}

How to compute these?
1. The first pass needs to find how many rows are in the partition.
 (Q: BTW, is it true that ""any two-pass window function needs the first pass
  only to find #rows in the partition""?)

2. On the second pass, we need
  2.1 find how many peers are in the peer group (5 rows have a=20)
  2.2 compute the value and set it for every peer member.

Possible ways to do step#2:
A: make it in two passes: 
  - first, let the first row of the group store #values in the group
  - second, propagate this value to all group members.

B: Use two cursors. 
- The front cursor reaches the end of the peer group, remembers how many values
  it saw and then stays there.
- The back cursor walks through the peer group and updates the values.


 $acceptance criteria:$",0,0,0,0,0,0,0,166.45,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
840,MDEV-9758,Task,MDEV,2016-03-18 02:15:53,,0,CHECKSUM TABLE - optimize calling my_checksum with larger chunks,"Checksum implementations contain optimizations for calculating
checksums of larger blocks of memory.

This optimization calls my_checksum on a larger block of memory
rather than calling on multiple adjacent memory as its going though
the table columns for each table row.

Note: I am planning to submit optimized crc32 code soon.",,"CHECKSUM TABLE - optimize calling my_checksum with larger chunks $end$ Checksum implementations contain optimizations for calculating
checksums of larger blocks of memory.

This optimization calls my_checksum on a larger block of memory
rather than calling on multiple adjacent memory as its going though
the table columns for each table row.

Note: I am planning to submit optimized crc32 code soon. $acceptance criteria:$",,Daniel Black,Daniel Black,Major,9,,0,2,0,1,0,0,0,,0,850,2,0,0,2016-04-27 07:01:18,CHECKSUM TABLE - optimize calling my_checksum with larger chunks,"Checksum implementations contain optimizations for calculating
checksums of larger blocks of memory.

This optimization calls my_checksum on a larger block of memory
rather than calling on multiple adjacent memory as its going though
the table columns for each table row.

Note: I am planning to submit optimized crc32 code soon.",,0,0,0,0,0.0,"CHECKSUM TABLE - optimize calling my_checksum with larger chunks $end$ Checksum implementations contain optimizations for calculating
checksums of larger blocks of memory.

This optimization calls my_checksum on a larger block of memory
rather than calling on multiple adjacent memory as its going though
the table columns for each table row.

Note: I am planning to submit optimized crc32 code soon. $acceptance criteria:$",0,0,0,0,0,0,0,964.75,3,1,0.333333,0,0.0,0,0.0,0,0.0,0,0.0
841,MDEV-9775,Task,MDEV,2016-03-22 14:48:35,,0,10.2 merge,merge 10.1 → 10.2,,10.2 merge $end$ merge 10.1 → 10.2 $acceptance criteria:$,,Sergei Golubchik,Sergei Golubchik,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:48:40,10.2 merge,merge 10.1 → 10.2,,0,0,0,0,0.0,10.2 merge $end$ merge 10.1 → 10.2 $acceptance criteria:$,0,0,0,0,0,0,0,20.0,34,15,0.441176,10,0.294118,8,0.235294,8,0.235294,7,0.205882
842,MDEV-9780,Task,MDEV,2016-03-23 11:09:09,,0,Window functions: interplay between window function and other constructs,"Study (and fix) the interplay between window functions and other SQL constructs, like ORDER BY ... LIMIT,  DISTINCT, etc.

Known things:
* LIMIT should be applied AFTER the window functions are computed (that is, window function computation should see rows that are cut off by LIMIT)
* DISTINCT must not be converted into GROUP BY when window functions are present
* need to check window functions inside derived table and/or union. Item_window_func::update_field() is not implemented, is this ok?",,"Window functions: interplay between window function and other constructs $end$ Study (and fix) the interplay between window functions and other SQL constructs, like ORDER BY ... LIMIT,  DISTINCT, etc.

Known things:
* LIMIT should be applied AFTER the window functions are computed (that is, window function computation should see rows that are cut off by LIMIT)
* DISTINCT must not be converted into GROUP BY when window functions are present
* need to check window functions inside derived table and/or union. Item_window_func::update_field() is not implemented, is this ok? $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,11,,0,5,1,3,0,0,0,,0,850,5,0,0,2016-03-23 11:45:34,Window functions: interplay between window function and other constructs,"Study (and fix) the interplay between window functions and other SQL constructs, like ORDER BY ... LIMIT,  DISTINCT, etc.

Known things:
* LIMIT should be applied AFTER the window functions are computed (that is, window function computation should see rows that are cut off by LIMIT)
* DISTINCT must not be converted into GROUP BY when window functions are present
* need to check window functions inside derived table and/or union. Item_window_func::update_field() is not implemented, is this ok?",,0,0,0,0,0.0,"Window functions: interplay between window function and other constructs $end$ Study (and fix) the interplay between window functions and other SQL constructs, like ORDER BY ... LIMIT,  DISTINCT, etc.

Known things:
* LIMIT should be applied AFTER the window functions are computed (that is, window function computation should see rows that are cut off by LIMIT)
* DISTINCT must not be converted into GROUP BY when window functions are present
* need to check window functions inside derived table and/or union. Item_window_func::update_field() is not implemented, is this ok? $acceptance criteria:$",0,0,0,0,0,0,1,0.6,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
843,MDEV-9787,Task,MDEV,2016-03-24 15:02:39,,0,Window functions: HAVING and GROUP BY,"{noformat}
create table ten(a int);
insert into ten values (0),(1),(2),(3),(4),(5),(6),(7),(8),(9);
create table t1 (a  int, b int, c int); 
insert into t1 select a,a,a from ten;
{noformat}

{noformat}
select b,max(a) as MX from t1 group by b having MX in (3,5,7);
+------+------+
| b    | MX   |
+------+------+
|    3 |    3 |
|    5 |    5 |
|    7 |    7 |
+------+------+
3 rows in set (0.00 sec)
{noformat}
Now, add window function, and see that HAVING is not honored anymore:

{noformat}
select b,max(a) as MX, rank() over (order by b) from t1 group by b having MX in (3,5,7);
+------+------+--------------------------+
| b    | MX   | rank() over (order by b) |
+------+------+--------------------------+
|    0 |    0 |                        1 |
|    1 |    1 |                        2 |
|    2 |    2 |                        3 |
|    3 |    3 |                        4 |
|    4 |    4 |                        5 |
|    5 |    5 |                        6 |
|    6 |    6 |                        7 |
|    7 |    7 |                        8 |
|    8 |    8 |                        9 |
|    9 |    9 |                       10 |
+------+------+--------------------------+
10 rows in set (0.00 sec)
{noformat}",,"Window functions: HAVING and GROUP BY $end$ {noformat}
create table ten(a int);
insert into ten values (0),(1),(2),(3),(4),(5),(6),(7),(8),(9);
create table t1 (a  int, b int, c int); 
insert into t1 select a,a,a from ten;
{noformat}

{noformat}
select b,max(a) as MX from t1 group by b having MX in (3,5,7);
+------+------+
| b    | MX   |
+------+------+
|    3 |    3 |
|    5 |    5 |
|    7 |    7 |
+------+------+
3 rows in set (0.00 sec)
{noformat}
Now, add window function, and see that HAVING is not honored anymore:

{noformat}
select b,max(a) as MX, rank() over (order by b) from t1 group by b having MX in (3,5,7);
+------+------+--------------------------+
| b    | MX   | rank() over (order by b) |
+------+------+--------------------------+
|    0 |    0 |                        1 |
|    1 |    1 |                        2 |
|    2 |    2 |                        3 |
|    3 |    3 |                        4 |
|    4 |    4 |                        5 |
|    5 |    5 |                        6 |
|    6 |    6 |                        7 |
|    7 |    7 |                        8 |
|    8 |    8 |                        9 |
|    9 |    9 |                       10 |
+------+------+--------------------------+
10 rows in set (0.00 sec)
{noformat} $acceptance criteria:$",,Sergei Petrunia,Sergei Petrunia,Major,7,,0,3,1,2,0,0,0,,0,850,1,0,0,2016-03-30 08:14:32,Window functions: HAVING and GROUP BY,"{noformat}
create table ten(a int);
insert into ten values (0),(1),(2),(3),(4),(5),(6),(7),(8),(9);
create table t1 (a  int, b int, c int); 
insert into t1 select a,a,a from ten;
{noformat}

{noformat}
select b,max(a) as MX from t1 group by b having MX in (3,5,7);
+------+------+
| b    | MX   |
+------+------+
|    3 |    3 |
|    5 |    5 |
|    7 |    7 |
+------+------+
3 rows in set (0.00 sec)
{noformat}
Now, add window function, and see that HAVING is not honored anymore:

{noformat}
select b,max(a) as MX, rank() over (order by b) from t1 group by b having MX in (3,5,7);
+------+------+--------------------------+
| b    | MX   | rank() over (order by b) |
+------+------+--------------------------+
|    0 |    0 |                        1 |
|    1 |    1 |                        2 |
|    2 |    2 |                        3 |
|    3 |    3 |                        4 |
|    4 |    4 |                        5 |
|    5 |    5 |                        6 |
|    6 |    6 |                        7 |
|    7 |    7 |                        8 |
|    8 |    8 |                        9 |
|    9 |    9 |                       10 |
+------+------+--------------------------+
10 rows in set (0.00 sec)
{noformat}",,0,0,0,0,0.0,"Window functions: HAVING and GROUP BY $end$ {noformat}
create table ten(a int);
insert into ten values (0),(1),(2),(3),(4),(5),(6),(7),(8),(9);
create table t1 (a  int, b int, c int); 
insert into t1 select a,a,a from ten;
{noformat}

{noformat}
select b,max(a) as MX from t1 group by b having MX in (3,5,7);
+------+------+
| b    | MX   |
+------+------+
|    3 |    3 |
|    5 |    5 |
|    7 |    7 |
+------+------+
3 rows in set (0.00 sec)
{noformat}
Now, add window function, and see that HAVING is not honored anymore:

{noformat}
select b,max(a) as MX, rank() over (order by b) from t1 group by b having MX in (3,5,7);
+------+------+--------------------------+
| b    | MX   | rank() over (order by b) |
+------+------+--------------------------+
|    0 |    0 |                        1 |
|    1 |    1 |                        2 |
|    2 |    2 |                        3 |
|    3 |    3 |                        4 |
|    4 |    4 |                        5 |
|    5 |    5 |                        6 |
|    6 |    6 |                        7 |
|    7 |    7 |                        8 |
|    8 |    8 |                        9 |
|    9 |    9 |                       10 |
+------+------+--------------------------+
10 rows in set (0.00 sec)
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,137.183,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
844,MDEV-9792,Task,MDEV,2016-03-25 19:37:22,,0,Backport MDEV-8713 to 10.1,Backport the continuous binary log backup in mysqlbinlog to 10.1. It was added to 10.2 in MDEV-8713.,,Backport MDEV-8713 to 10.1 $end$ Backport the continuous binary log backup in mysqlbinlog to 10.1. It was added to 10.2 in MDEV-8713. $acceptance criteria:$,,Rasmus Johansson,Rasmus Johansson,Major,6,,0,1,1,1,0,0,0,,0,850,1,0,0,2016-04-27 07:02:44,Backport MDEV-8713 to 10.1,Backport the continuous binary log backup in mysqlbinlog to 10.1. It was added to 10.2 in MDEV-8713.,,0,0,0,0,0.0,Backport MDEV-8713 to 10.1 $end$ Backport the continuous binary log backup in mysqlbinlog to 10.1. It was added to 10.2 in MDEV-8713. $acceptance criteria:$,0,0,0,0,0,0,0,779.417,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
845,MDEV-9831,Task,MDEV,2016-03-30 08:24:07,,0,Implement additional window functions,"Following CUME_DIST's implementation, we need to implement as many related functions as possible.

This represents a subtask related to those functions.",,"Implement additional window functions $end$ Following CUME_DIST's implementation, we need to implement as many related functions as possible.

This represents a subtask related to those functions. $acceptance criteria:$",,Vicențiu Ciorbaru,Vicențiu Ciorbaru,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-03-30 09:59:49,Implement additional window functions,"Following CUME_DIST's implementation, we need to implement as many related functions as possible.

This represents a subtask related to those functions.",,0,0,0,0,0.0,"Implement additional window functions $end$ Following CUME_DIST's implementation, we need to implement as many related functions as possible.

This represents a subtask related to those functions. $acceptance criteria:$",0,0,0,0,0,0,1,1.58333,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
846,MDEV-9857,Task,MDEV,2016-04-01 02:05:05,,0,CACHE_LINE_SIZE in innodb should be 128 on POWER,"MDEV-6533 didn't fully contain all CACHE_LINE_SIZE elements.

The forthcoming github pull request will. Also includes patches in Performance Schema.",,"CACHE_LINE_SIZE in innodb should be 128 on POWER $end$ MDEV-6533 didn't fully contain all CACHE_LINE_SIZE elements.

The forthcoming github pull request will. Also includes patches in Performance Schema. $acceptance criteria:$",,Daniel Black,Daniel Black,Major,17,,0,7,2,2,0,0,0,,0,850,7,0,0,2016-04-27 07:01:29,CACHE_LINE_SIZE in innodb should be 128 on POWER,"MDEV-6533 didn't fully contain all CACHE_LINE_SIZE elements.

The forthcoming github pull request will. Also includes patches in Performance Schema.",,0,0,0,0,0.0,"CACHE_LINE_SIZE in innodb should be 128 on POWER $end$ MDEV-6533 didn't fully contain all CACHE_LINE_SIZE elements.

The forthcoming github pull request will. Also includes patches in Performance Schema. $acceptance criteria:$",0,0,0,0,0,0,1,628.933,4,1,0.25,0,0.0,0,0.0,0,0.0,0,0.0
847,MDEV-9872,Task,MDEV,2016-04-06 05:47:35,,0,Add common optimized CRC32 function interface,"patch as per https://github.com/MariaDB/server/pull/170 adds optimized Power8 implemention for CRC32-IEEE to complement existing CRC32C.


While I haven't included an implementation of an optimized CRC32-(IEEE) for intel, there is the following links that would enable this:
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-paper.pdf
* http://web.archive.org/web/20140101100538/http://mail.madler.net/pipermail/zlib-devel_madler.net/2013-November/003096.html
* https://golang.org/src/hash/crc32/

Note the documentation mentions patents on CRC32 folding (which I haven't done).",,"Add common optimized CRC32 function interface $end$ patch as per https://github.com/MariaDB/server/pull/170 adds optimized Power8 implemention for CRC32-IEEE to complement existing CRC32C.


While I haven't included an implementation of an optimized CRC32-(IEEE) for intel, there is the following links that would enable this:
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-paper.pdf
* http://web.archive.org/web/20140101100538/http://mail.madler.net/pipermail/zlib-devel_madler.net/2013-November/003096.html
* https://golang.org/src/hash/crc32/

Note the documentation mentions patents on CRC32 folding (which I haven't done). $acceptance criteria:$",,Daniel Black,Daniel Black,Major,20,,0,9,1,3,0,0,0,,0,850,9,0,0,2016-04-27 07:01:43,Add common optimized CRC32 function interface,"patch as per https://github.com/MariaDB/server/pull/170 adds optimized Power8 implemention for CRC32-IEEE to complement existing CRC32C.


While I haven't included an implementation of an optimized CRC32-(IEEE) for intel, there is the following links that would enable this:
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-paper.pdf
* http://web.archive.org/web/20140101100538/http://mail.madler.net/pipermail/zlib-devel_madler.net/2013-November/003096.html
* https://golang.org/src/hash/crc32/

Note the documentation mentions patents on CRC32 folding (which I haven't done).",,0,0,0,0,0.0,"Add common optimized CRC32 function interface $end$ patch as per https://github.com/MariaDB/server/pull/170 adds optimized Power8 implemention for CRC32-IEEE to complement existing CRC32C.


While I haven't included an implementation of an optimized CRC32-(IEEE) for intel, there is the following links that would enable this:
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf
* http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-paper.pdf
* http://web.archive.org/web/20140101100538/http://mail.madler.net/pipermail/zlib-devel_madler.net/2013-November/003096.html
* https://golang.org/src/hash/crc32/

Note the documentation mentions patents on CRC32 folding (which I haven't done). $acceptance criteria:$",0,0,0,0,0,0,1,505.233,5,1,0.2,0,0.0,0,0.0,0,0.0,0,0.0
848,MDEV-9942,Task,MDEV,2016-04-19 07:14:41,,0,10.0.25 merge,"* 5.5 (/) 5.5.49
* InnoDB (/) 5.6.30
* XtraDB (/) 5/6/29-76/2
* P_S (/) _nothing to do_
* Connect (/) 1.04.0006
* Spider (/) _nothing to do_
* PCRE (/) _nothing to do_
* Mroonga (/) _nothing to do_
* TokuDB (x) [lp:1546538|https://bugs.launchpad.net/bugs/1546538] is not fixed yet
",,"10.0.25 merge $end$ * 5.5 (/) 5.5.49
* InnoDB (/) 5.6.30
* XtraDB (/) 5/6/29-76/2
* P_S (/) _nothing to do_
* Connect (/) 1.04.0006
* Spider (/) _nothing to do_
* PCRE (/) _nothing to do_
* Mroonga (/) _nothing to do_
* TokuDB (x) [lp:1546538|https://bugs.launchpad.net/bugs/1546538] is not fixed yet
 $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,7,,0,1,0,1,0,3,0,,0,850,1,0,0,2016-04-19 12:33:43,10.0.25 merge,"* 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
",,0,3,0,30,1.30435,"10.0.25 merge $end$ * 5.5
* InnoDB
* XtraDB
* P_S
* Connect
* Spider
* PCRE
* Mroonga
* TokuDB
 $acceptance criteria:$",3,1,1,1,1,1,1,5.31667,35,15,0.428571,10,0.285714,8,0.228571,8,0.228571,7,0.2
849,MDEV-9947,Task,MDEV,2016-04-19 12:02:50,,0,COM_MULTI united response,"Put all OK packets in one physical packet.

",,"COM_MULTI united response $end$ Put all OK packets in one physical packet.

 $acceptance criteria:$",,Oleksandr Byelkin,Oleksandr Byelkin,Major,20,,0,5,2,2,0,2,0,,0,850,5,2,0,2016-05-04 13:30:32,COM_MULTI united response,"Put all OK packets in one physical packet.

",,0,0,0,0,0.0,"COM_MULTI united response $end$ Put all OK packets in one physical packet.

 $acceptance criteria:$",0,0,0,0,0,0,1,361.45,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
850,MDEV-9995,Task,MDEV,2016-04-26 13:21:01,,0,10.1.14 merge,"* 10.0 (/)
* 10.0-galera (/)
* Connect 10.1 (/)",,"10.1.14 merge $end$ * 10.0 (/)
* 10.0-galera (/)
* Connect 10.1 (/) $acceptance criteria:$",,Sergei Golubchik,Sergei Golubchik,Blocker,11,,0,0,0,1,0,2,0,,0,850,0,0,0,2016-04-26 14:36:27,10.1.14 merge,"* 10.0
* 10.0-galera
* Connect 10.1",,0,2,0,3,0.25,"10.1.14 merge $end$ * 10.0
* 10.0-galera
* Connect 10.1 $acceptance criteria:$",2,1,0,0,0,0,0,1.25,36,16,0.444444,11,0.305556,9,0.25,9,0.25,8,0.222222
851,MXS-1004,Task,MXS,2016-11-18 19:35:30,,0,Document basic mdbci use-cases,,,Document basic mdbci use-cases $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,11,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-01-02 10:27:14,Document basic mdbci use-cases,,,0,0,0,0,0.0,Document basic mdbci use-cases $end$ $acceptance criteria:$,0,0,0,0,0,0,1,9830.85,32,1,0.03125,0,0.0,0,0.0,0,0.0,0,0.0
852,MXS-1022,Sub-Task,MXS,2016-11-22 12:00:48,,0,Creating/Deleting a Monitor,"Make it possible to dynamically create/delete a monitor via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,"Creating/Deleting a Monitor $end$ Make it possible to dynamically create/delete a monitor via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-11-22 12:00:48,Creating/Deleting a Monitor,"Make it possible to dynamically create/delete a monitor via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,0,0,0,0,0.0,"Creating/Deleting a Monitor $end$ Make it possible to dynamically create/delete a monitor via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,96,16,0.166667,5,0.0520833,3,0.03125,1,0.0104167,1,0.0104167
853,MXS-1023,Sub-Task,MXS,2016-11-22 12:02:20,,0,Creating/Deleting a Listener,"Make it possible to dynamically create/delete a listener via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,"Creating/Deleting a Listener $end$ Make it possible to dynamically create/delete a listener via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-11-22 12:02:20,Creating/Deleting a Listener,"Make it possible to dynamically create/delete a listener via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,0,0,0,0,0.0,"Creating/Deleting a Listener $end$ Make it possible to dynamically create/delete a listener via MaxAdmin.
The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,97,16,0.164948,5,0.0515464,3,0.0309278,1,0.0103093,1,0.0103093
854,MXS-1029,Sub-Task,MXS,2016-11-30 08:43:12,,0,Add 16MB support to binlog encryption,"The encryption of events in binlogserver cannot handle the 16MB events right now.

This task will add the missing feature",,"Add 16MB support to binlog encryption $end$ The encryption of events in binlogserver cannot handle the 16MB events right now.

This task will add the missing feature $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,5,,0,1,0,8,0,0,0,,0,850,1,0,0,2016-11-30 08:43:12,Add 16MB support to binlog encryption,"The encryption of events in binlogserver cannot handle the 16MB events right now.

This task will add the missing feature",,0,0,0,0,0.0,"Add 16MB support to binlog encryption $end$ The encryption of events in binlogserver cannot handle the 16MB events right now.

This task will add the missing feature $acceptance criteria:$",0,0,0,0,0,0,1,0.0,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
855,MXS-1030,Task,MXS,2016-11-30 08:51:16,,0,Add maxadmin commands to the cache.,"For testing and other purposes it is valuable to be able to get runtime statistics from the cache. For instance, how much data it contains, how many items, the hitrate, etc.
",,"Add maxadmin commands to the cache. $end$ For testing and other purposes it is valuable to be able to get runtime statistics from the cache. For instance, how much data it contains, how many items, the hitrate, etc.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-11-30 08:51:16,Add maxadmin commands to the cache.,"For testing and other purposes it is valuable to be able to get runtime statistics from the cache. For instance, how much data it contains, how many items, the hitrate, etc.
",,0,0,0,0,0.0,"Add maxadmin commands to the cache. $end$ For testing and other purposes it is valuable to be able to get runtime statistics from the cache. For instance, how much data it contains, how many items, the hitrate, etc.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,98,16,0.163265,5,0.0510204,3,0.0306122,1,0.0102041,1,0.0102041
856,MXS-1031,New Feature,MXS,2016-11-30 08:51:41,,0,MaxRows test scenario,MaxRows test scenario,,MaxRows test scenario $end$ MaxRows test scenario $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,11,,0,0,0,3,0,0,1,,0,850,0,0,0,2016-11-30 10:26:26,MaxRows test scenario,MaxRows test scenario,,0,0,0,0,0.0,MaxRows test scenario $end$ MaxRows test scenario $acceptance criteria:$,0,0,0,0,0,0,1,1.56667,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
857,MXS-1038,Task,MXS,2016-12-01 12:55:00,,0,Make Jenkins work with tests residing in MaxScale repo.,,,Make Jenkins work with tests residing in MaxScale repo. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-05-18 09:35:00,Make Jenkins work with tests residing in MaxScale repo.,,,0,0,0,0,0.0,Make Jenkins work with tests residing in MaxScale repo. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,4028.67,99,16,0.161616,5,0.050505,3,0.030303,1,0.010101,1,0.010101
858,MXS-1061,New Feature,MXS,2016-12-14 09:44:29,,0,Binlog Encryption test scenario,,,Binlog Encryption test scenario $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,11,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-12-14 10:20:43,Binlog Encryption test scenario,,,0,0,0,0,0.0,Binlog Encryption test scenario $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.6,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
859,MXS-1062,Task,MXS,2016-12-14 10:30:30,,0,Create more and improve existing tests,,,Create more and improve existing tests $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-12-14 10:30:30,Create more and improve existing tests,,,0,0,0,0,0.0,Create more and improve existing tests $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,100,16,0.16,5,0.05,3,0.03,1,0.01,1,0.01
860,MXS-1063,Task,MXS,2016-12-14 10:32:11,,0,Fix the tee filter.,,,Fix the tee filter. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-12-14 10:32:11,Fix the tee filter.,,,0,0,0,0,0.0,Fix the tee filter. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,101,16,0.158416,5,0.049505,3,0.029703,1,0.00990099,1,0.00990099
861,MXS-1064,New Feature,MXS,2016-12-14 10:35:55,,0,"Add Encryption options in ""show service $binlog_service""","Add Encryption options in ""show service $binlog_service""",,"Add Encryption options in ""show service $binlog_service"" $end$ Add Encryption options in ""show service $binlog_service"" $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-12-14 10:36:17,"Add Encryption options in ""show service $binlog_service""","Add Encryption options in ""show service $binlog_service""",,0,0,0,0,0.0,"Add Encryption options in ""show service $binlog_service"" $end$ Add Encryption options in ""show service $binlog_service"" $acceptance criteria:$",0,0,0,0,0,0,0,0.0,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
862,MXS-1065,New Feature,MXS,2016-12-16 07:47:41,,0,wildcards for source option in namedfilter,"Hi,

to make it possible to route many IPs to one target and many other IPs to another target.

One usecase would be to route to different slave on westcoast and eastcoast.

{code}
[MyService]
type=service
router=readwritesplit
servers=server1,eastslave,westslave
user=myuser
passwd=mypasswd
filters=eastcoastfilter | westcoastfilter 

[eastcoast]
type=filter
module=namedserverfilter
source=  65.*.*.*
server=server1

[westcoast]
type=filter
module=namedserverfilter
source= 69.*.*.*
server=server2
{code}",,"wildcards for source option in namedfilter $end$ Hi,

to make it possible to route many IPs to one target and many other IPs to another target.

One usecase would be to route to different slave on westcoast and eastcoast.

{code}
[MyService]
type=service
router=readwritesplit
servers=server1,eastslave,westslave
user=myuser
passwd=mypasswd
filters=eastcoastfilter | westcoastfilter 

[eastcoast]
type=filter
module=namedserverfilter
source=  65.*.*.*
server=server1

[westcoast]
type=filter
module=namedserverfilter
source= 69.*.*.*
server=server2
{code} $acceptance criteria:$",,Richard Stracke,Richard Stracke,Minor,8,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-01-04 11:13:57,wildcards for source option in namedfilter,"Hi,

to make it possible to route many IPs to one target and many other IPs to another target.

One usecase would be to route to different slave on westcoast and eastcoast.

{code}
[MyService]
type=service
router=readwritesplit
servers=server1,eastslave,westslave
user=myuser
passwd=mypasswd
filters=eastcoastfilter | westcoastfilter 

[eastcoast]
type=filter
module=namedserverfilter
source=  65.*.*.*
server=server1

[westcoast]
type=filter
module=namedserverfilter
source= 69.*.*.*
server=server2
{code}",,0,0,0,0,0.0,"wildcards for source option in namedfilter $end$ Hi,

to make it possible to route many IPs to one target and many other IPs to another target.

One usecase would be to route to different slave on westcoast and eastcoast.

{code}
[MyService]
type=service
router=readwritesplit
servers=server1,eastslave,westslave
user=myuser
passwd=mypasswd
filters=eastcoastfilter | westcoastfilter 

[eastcoast]
type=filter
module=namedserverfilter
source=  65.*.*.*
server=server1

[westcoast]
type=filter
module=namedserverfilter
source= 69.*.*.*
server=server2
{code} $acceptance criteria:$",0,0,0,0,0,0,1,459.433,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
863,MXS-1067,New Feature,MXS,2016-12-19 16:08:17,,0,Query hint to influence Consistent Critical Read Filter,"The Consistent Critical Read Filter has ""ignore"" and ""match"" settings for the config. Sometimes it might be handy or easier to specify which queries should be matched/ignored directly in the code. Query hints could enable this.

 For example:

{code}
-- maxscale ccr=match
{code}

would tell maxscale to apply the ccr filter to the query, and

{code}
-- maxscale ccr=ignore
{code}

would tell maxscale to not apply the ccr filter
",,"Query hint to influence Consistent Critical Read Filter $end$ The Consistent Critical Read Filter has ""ignore"" and ""match"" settings for the config. Sometimes it might be handy or easier to specify which queries should be matched/ignored directly in the code. Query hints could enable this.

 For example:

{code}
-- maxscale ccr=match
{code}

would tell maxscale to apply the ccr filter to the query, and

{code}
-- maxscale ccr=ignore
{code}

would tell maxscale to not apply the ccr filter
 $acceptance criteria:$",,Heinz Wiesinger,Heinz Wiesinger,Minor,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-06-06 09:58:10,Query hint to influence Consistent Critical Read Filter,"The Consistent Critical Read Filter has ""ignore"" and ""match"" settings for the config. Sometimes it might be handy or easier to specify which queries should be matched/ignored directly in the code. Query hints could enable this.

 For example:

{code}
-- maxscale ccr=match
{code}

would tell maxscale to apply the ccr filter to the query, and

{code}
-- maxscale ccr=ignore
{code}

would tell maxscale to not apply the ccr filter
",,0,0,0,0,0.0,"Query hint to influence Consistent Critical Read Filter $end$ The Consistent Critical Read Filter has ""ignore"" and ""match"" settings for the config. Sometimes it might be handy or easier to specify which queries should be matched/ignored directly in the code. Query hints could enable this.

 For example:

{code}
-- maxscale ccr=match
{code}

would tell maxscale to apply the ccr filter to the query, and

{code}
-- maxscale ccr=ignore
{code}

would tell maxscale to not apply the ccr filter
 $acceptance criteria:$",0,0,0,0,0,0,0,4049.82,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
864,MXS-1070,Task,MXS,2017-01-02 15:46:08,,0,Add functionality for getting function usage information of a statement,,,Add functionality for getting function usage information of a statement $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,1,1,0,0,0,,0,850,0,0,0,2017-01-04 09:36:03,Add functionality for getting function usage information of a statement,,,0,0,0,0,0.0,Add functionality for getting function usage information of a statement $end$ $acceptance criteria:$,0,0,0,0,0,0,0,41.8167,102,16,0.156863,5,0.0490196,3,0.0294118,1,0.00980392,1,0.00980392
865,MXS-1071,Sub-Task,MXS,2017-01-03 20:16:28,,0,MaxRows test implementation,,,MaxRows test implementation $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-01-03 20:16:28,MaxRows test implementation,,,0,0,0,0,0.0,MaxRows test implementation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,33,1,0.030303,0,0.0,0,0.0,0,0.0,0,0.0
866,MXS-1073,Task,MXS,2017-01-04 10:00:24,,0,binlog encryption tests implementation,test scenarion https://jira.mariadb.org/browse/MXS-1061,,binlog encryption tests implementation $end$ test scenarion https://jira.mariadb.org/browse/MXS-1061 $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-04 10:52:57,binlog encryption tests implementation,test scenarion https://jira.mariadb.org/browse/MXS-1061,,0,0,0,0,0.0,binlog encryption tests implementation $end$ test scenarion https://jira.mariadb.org/browse/MXS-1061 $acceptance criteria:$,0,0,0,0,0,0,0,0.866667,34,1,0.0294118,0,0.0,0,0.0,0,0.0,0,0.0
867,MXS-1074,New Feature,MXS,2017-01-04 11:17:15,,0,Move configuration processing into the core,The configuration validation and processing should be done by the MaxScale core. This would make the initialization of the modules a lot simpler.,,Move configuration processing into the core $end$ The configuration validation and processing should be done by the MaxScale core. This would make the initialization of the modules a lot simpler. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-01-04 11:17:15,Move configuration processing into the core,The configuration validation and processing should be done by the MaxScale core. This would make the initialization of the modules a lot simpler.,,0,0,0,0,0.0,Move configuration processing into the core $end$ The configuration validation and processing should be done by the MaxScale core. This would make the initialization of the modules a lot simpler. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,13,3,0.230769,3,0.230769,3,0.230769,3,0.230769,3,0.230769
868,MXS-1075,Task,MXS,2017-01-12 12:25:05,,0,Support MariaDB GTID in binlog replication handshake with MariaDB 10 Slaves,"(1) Use GTID in handshake with MariaDB Master server 
When 
CHANGE MASTER TO master_host='<host-ip>', master_user='<usr>', master_password='<password>', master_use_gtid=slave_pos
is executed on MaxScale, request the master to send binlog starting from GTID corresponding to slave_pos; where slave_gtid_pos is the last 

(2) Accept GTID in handshake from a MariaDB Slave. When Slave requests MaxScale to send binlog starting from slave_pos - start sending binlog record to the particular slave from the GTID corresponding to requested slave_pos ",,"Support MariaDB GTID in binlog replication handshake with MariaDB 10 Slaves $end$ (1) Use GTID in handshake with MariaDB Master server 
When 
CHANGE MASTER TO master_host='<host-ip>', master_user='<usr>', master_password='<password>', master_use_gtid=slave_pos
is executed on MaxScale, request the master to send binlog starting from GTID corresponding to slave_pos; where slave_gtid_pos is the last 

(2) Accept GTID in handshake from a MariaDB Slave. When Slave requests MaxScale to send binlog starting from slave_pos - start sending binlog record to the particular slave from the GTID corresponding to requested slave_pos  $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,11,,0,4,1,5,0,1,0,,0,850,3,0,0,2017-02-01 10:38:25,Support MariaDB GTID in binlog replication handshake with  MariaDB Master and MariaDB Slave,"(1) Use GTID in handshake with MariaDB Master server 
When 
CHANGE MASTER TO master_host='<host-ip>', master_user='<usr>', master_password='<password>', master_use_gtid=slave_pos
is executed on MaxScale, request the master to send binlog starting from GTID corresponding to slave_pos; where slave_gtid_pos is the last 

(2) Accept GTID in handshake from a MariaDB Slave. When Slave requests MaxScale to send binlog starting from slave_pos - start sending binlog record to the particular slave from the GTID corresponding to requested slave_pos ",,1,0,0,6,0.0449438,"Support MariaDB GTID in binlog replication handshake with  MariaDB Master and MariaDB Slave $end$ (1) Use GTID in handshake with MariaDB Master server 
When 
CHANGE MASTER TO master_host='<host-ip>', master_user='<usr>', master_password='<password>', master_use_gtid=slave_pos
is executed on MaxScale, request the master to send binlog starting from GTID corresponding to slave_pos; where slave_gtid_pos is the last 

(2) Accept GTID in handshake from a MariaDB Slave. When Slave requests MaxScale to send binlog starting from slave_pos - start sending binlog record to the particular slave from the GTID corresponding to requested slave_pos  $acceptance criteria:$",1,1,1,0,0,0,1,478.217,23,4,0.173913,1,0.0434783,0,0.0,0,0.0,0,0.0
869,MXS-1078,Task,MXS,2017-01-13 13:14:11,,0,Refactor dbusers.c,"Background: When a client connects to MaxScale, he's authorized by comparing his username, host, password etc to the values in the user database (dbusers.c). The database is filled with user accounts using backend data, when MaxScale starts. The host-field in the userdb can have various forms: IP-address, host name (text), or a wildcard form of one of the former. If possible, host names are converted to IP-addresses when read. If the host name contains wildcards, this is impossible. This is why host names with wildcards were not allowed previously by MaxScale, although they are allowed by the sql-server. A recent commit added support for these to MaxScale aswell, but the implementation is unoptimal.

The problem: Currently, the various ways of finding the correct user@host-combination from the userDB-hashmap have been integrated into the hashmap itself through its function pointers (cmpfun). To match wildcard hostnames, a query needs to be made since the client ip-address needs to be converted to a text-form host name. This is very slow compared to any typical hashmap operations, and will also keep the map read-locked for some while. Also, this type of logic shouldn't really be inside the container's own functions.

Goal: Take the logic out of the hashmap and only use short and simple hash- and compare-functions inside the hashmap. The element type should be changed to a list/array that contains the username once and then all the possible hostname (IP or text) and password combinations for that user. The hashmap only handles these elements as a whole, the details are taken care of by other functions.

How: Here's some ideas.
1. Use only username as hash key, use inbuilt string hashing (hashtable_item_strhash).

2. The element type should be like
/**
 * MySQL hashtable entry. The entry contains the username (used for hashing as it
 * contains no wildcards) and a list of possible hosts for that user. The hosts
 * are divided to resolved hosts and hosts with wildcards.
 */
typedef struct mysql_user_table_entry
{
    char *user_name;
    MYSQL_USER_HOST *fq_hosts;
    MYSQL_USER_HOST *wc_hosts;
    int lock;
    anything else?
} MYSQL_USER_ENTRY;

3. Remove multiple lookups of the map in gw_find_mysql_user_password_sha1. Do just one lookup and then check the different hosts. May have to lock the table entry while doing it. Requires modifying all hashtable-related functions.
",,"Refactor dbusers.c $end$ Background: When a client connects to MaxScale, he's authorized by comparing his username, host, password etc to the values in the user database (dbusers.c). The database is filled with user accounts using backend data, when MaxScale starts. The host-field in the userdb can have various forms: IP-address, host name (text), or a wildcard form of one of the former. If possible, host names are converted to IP-addresses when read. If the host name contains wildcards, this is impossible. This is why host names with wildcards were not allowed previously by MaxScale, although they are allowed by the sql-server. A recent commit added support for these to MaxScale aswell, but the implementation is unoptimal.

The problem: Currently, the various ways of finding the correct user@host-combination from the userDB-hashmap have been integrated into the hashmap itself through its function pointers (cmpfun). To match wildcard hostnames, a query needs to be made since the client ip-address needs to be converted to a text-form host name. This is very slow compared to any typical hashmap operations, and will also keep the map read-locked for some while. Also, this type of logic shouldn't really be inside the container's own functions.

Goal: Take the logic out of the hashmap and only use short and simple hash- and compare-functions inside the hashmap. The element type should be changed to a list/array that contains the username once and then all the possible hostname (IP or text) and password combinations for that user. The hashmap only handles these elements as a whole, the details are taken care of by other functions.

How: Here's some ideas.
1. Use only username as hash key, use inbuilt string hashing (hashtable_item_strhash).

2. The element type should be like
/**
 * MySQL hashtable entry. The entry contains the username (used for hashing as it
 * contains no wildcards) and a list of possible hosts for that user. The hosts
 * are divided to resolved hosts and hosts with wildcards.
 */
typedef struct mysql_user_table_entry
{
    char *user_name;
    MYSQL_USER_HOST *fq_hosts;
    MYSQL_USER_HOST *wc_hosts;
    int lock;
    anything else?
} MYSQL_USER_ENTRY;

3. Remove multiple lookups of the map in gw_find_mysql_user_password_sha1. Do just one lookup and then check the different hosts. May have to lock the table entry while doing it. Requires modifying all hashtable-related functions.
 $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Minor,6,,0,1,1,1,0,1,0,,0,850,1,0,0,2017-02-01 10:36:21,Refactor dbusers.c,"Background: When a client connects to MaxScale, he's authorized by comparing his uname, host, password etc to the values in the user database (dbusers.c). The database is filled with user accounts using backend data, when MaxScale starts. The host-field in the userdb can have various forms: IP-address, host name (text), or a wildcard form of one of the former. If possible, host names are converted to IP-addresses when read. If the host name contains wildcards, this is impossible. This is why host names with wildcards were not allowed previously by MaxScale, although they are allowed by the sql-server. A recent commit added support for these to MaxScale aswell, but the implementation is unoptimal.

The problem: Currently, the various ways of finding the correct user@host-combination from the userDB-hashmap have been integrated into the hashmap itself through its function pointers (cmpfun). To match wildcard hostnames, a query needs to be made since the client ip-address needs to be converted to a text-form host name. This is very slow compared to any typical hashmap operations, and will also keep the map read-locked for some while. Also, this type of logic shouldn't really be inside the container's own functions.

Goal: Take the logic out of the hashmap and only use short and simple hash- and compare-functions inside the hashmap. The element type should be changed to a list/array that contains the username once and then all the possible hostname (IP or text) and password combinations for that user. The hashmap only handles these elements as a whole, the details are taken care of by other functions.

How: Here's some ideas.
1. Use only username as hash key, use inbuilt string hashing (hashtable_item_strhash).

2. The element type should be like
/**
 * MySQL hashtable entry. The entry contains the username (used for hashing as it
 * contains no wildcards) and a list of possible hosts for that user. The hosts
 * are divided to resolved hosts and hosts with wildcards.
 */
typedef struct mysql_user_table_entry
{
    char *user_name;
    MYSQL_USER_HOST *fq_hosts;
    MYSQL_USER_HOST *wc_hosts;
    int lock;
    anything else?
} MYSQL_USER_ENTRY;

3. Remove multiple lookups of the map in gw_find_mysql_user_password_sha1. Do just one lookup and then check the different hosts. May have to lock the table entry while doing it. Requires modifying all hashtable-related functions.
",,0,1,0,2,0.00262467,"Refactor dbusers.c $end$ Background: When a client connects to MaxScale, he's authorized by comparing his uname, host, password etc to the values in the user database (dbusers.c). The database is filled with user accounts using backend data, when MaxScale starts. The host-field in the userdb can have various forms: IP-address, host name (text), or a wildcard form of one of the former. If possible, host names are converted to IP-addresses when read. If the host name contains wildcards, this is impossible. This is why host names with wildcards were not allowed previously by MaxScale, although they are allowed by the sql-server. A recent commit added support for these to MaxScale aswell, but the implementation is unoptimal.

The problem: Currently, the various ways of finding the correct user@host-combination from the userDB-hashmap have been integrated into the hashmap itself through its function pointers (cmpfun). To match wildcard hostnames, a query needs to be made since the client ip-address needs to be converted to a text-form host name. This is very slow compared to any typical hashmap operations, and will also keep the map read-locked for some while. Also, this type of logic shouldn't really be inside the container's own functions.

Goal: Take the logic out of the hashmap and only use short and simple hash- and compare-functions inside the hashmap. The element type should be changed to a list/array that contains the username once and then all the possible hostname (IP or text) and password combinations for that user. The hashmap only handles these elements as a whole, the details are taken care of by other functions.

How: Here's some ideas.
1. Use only username as hash key, use inbuilt string hashing (hashtable_item_strhash).

2. The element type should be like
/**
 * MySQL hashtable entry. The entry contains the username (used for hashing as it
 * contains no wildcards) and a list of possible hosts for that user. The hosts
 * are divided to resolved hosts and hosts with wildcards.
 */
typedef struct mysql_user_table_entry
{
    char *user_name;
    MYSQL_USER_HOST *fq_hosts;
    MYSQL_USER_HOST *wc_hosts;
    int lock;
    anything else?
} MYSQL_USER_ENTRY;

3. Remove multiple lookups of the map in gw_find_mysql_user_password_sha1. Do just one lookup and then check the different hosts. May have to lock the table entry while doing it. Requires modifying all hashtable-related functions.
 $acceptance criteria:$",1,1,0,0,0,0,0,453.367,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
870,MXS-1083,Task,MXS,2017-01-18 09:42:23,,0,Arrange benchmarking of MaxScale,,,Arrange benchmarking of MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-01-18 09:42:23,Arrange benchmarking of MaxScale,,,0,0,0,0,0.0,Arrange benchmarking of MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,103,16,0.15534,5,0.0485437,3,0.0291262,1,0.00970874,1,0.00970874
871,MXS-1084,Task,MXS,2017-01-18 09:48:32,,0,Cleanup Headers,"* Split headers into one in {{include/maxscale}} containing public declarations, intended for use by modules and one in {{server/core/maxscale}} that includes the one in {{include/maxscale}} and in addition declares things intended for use by the MaxScale core.
* Add the _mxs_ prefix to public types and values (enums, typedefs, structs, defines, etc.)",,"Cleanup Headers $end$ * Split headers into one in {{include/maxscale}} containing public declarations, intended for use by modules and one in {{server/core/maxscale}} that includes the one in {{include/maxscale}} and in addition declares things intended for use by the MaxScale core.
* Add the _mxs_ prefix to public types and values (enums, typedefs, structs, defines, etc.) $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-01-18 09:48:32,Cleanup Headers,"* Split headers into one in {{include/maxscale}} containing public declarations, intended for use by modules and one in {{server/core/maxscale}} that includes the one in {{include/maxscale}} and in addition declares things intended for use by the MaxScale core.
* Add the _mxs_ prefix to public types and values (enums, typedefs, structs, defines, etc.)",,0,0,0,0,0.0,"Cleanup Headers $end$ * Split headers into one in {{include/maxscale}} containing public declarations, intended for use by modules and one in {{server/core/maxscale}} that includes the one in {{include/maxscale}} and in addition declares things intended for use by the MaxScale core.
* Add the _mxs_ prefix to public types and values (enums, typedefs, structs, defines, etc.) $acceptance criteria:$",0,0,0,0,0,0,1,0.0,104,16,0.153846,5,0.0480769,3,0.0288462,1,0.00961538,1,0.00961538
872,MXS-1087,Task,MXS,2017-01-18 10:36:08,,0,Create tests for masking filter,,,Create tests for masking filter $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-01-18 10:36:08,Create tests for masking filter,,,0,0,0,0,0.0,Create tests for masking filter $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,105,16,0.152381,5,0.047619,3,0.0285714,1,0.00952381,1,0.00952381
873,MXS-1088,New Feature,MXS,2017-01-25 12:31:28,,0,Can't hint queries to slaves,You can hint queries to master but not to slave. This should be possible: {{SELECT 1; -- maxscale route to slave}},,Can't hint queries to slaves $end$ You can hint queries to master but not to slave. This should be possible: {{SELECT 1; -- maxscale route to slave}} $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-02-15 10:54:06,Can't hint queries to slaves,You can hint queries to master but not to slave. This should be possible: {{SELECT 1; -- maxscale route to slave}},,0,0,0,0,0.0,Can't hint queries to slaves $end$ You can hint queries to master but not to slave. This should be possible: {{SELECT 1; -- maxscale route to slave}} $acceptance criteria:$,0,0,0,0,0,0,0,502.367,14,3,0.214286,3,0.214286,3,0.214286,3,0.214286,3,0.214286
874,MXS-1089,Task,MXS,2017-01-25 12:33:28,,0,Prepare 1.4.5,,,Prepare 1.4.5 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,7,,0,850,0,0,0,2017-02-01 11:12:21,Prepare 1.4.5,,,0,0,0,0,0.0,Prepare 1.4.5 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,166.633,106,16,0.150943,5,0.0471698,3,0.0283019,1,0.00943396,1,0.00943396
875,MXS-109,New Feature,MXS,2015-04-22 21:50:19,,0,Galera monitor: Minimum cluster size and/or refuse two cluster IDS in 1 cluster,"I think 2 extra features in the Galera monitor module would be a good addition to MaxScale.

If we have a 3 node galera cluster and somebody accidently bootstrap each one (or 2) of them, MaxScale would currently send traffic to all nodes allthough they are not in a cluster together. 

I think there are 2 ways to avoid this in the MaxScale monitor, option 1 is easy and option 2 is elegant.
1: Config parameter with the minimum cluster size. If any node is up but the minimum cluster size is too low, consider the node down.

2a: If there are several cluster identifiers present in the list of servers MaxScale knows about, only consider the servers with cluster identifier that occurs at least 1 time more then the other identifier(s)
2b: If there are several cluster identifiers present in the servers MaxScale knows about do not consider any of the nodes up
",,"Galera monitor: Minimum cluster size and/or refuse two cluster IDS in 1 cluster $end$ I think 2 extra features in the Galera monitor module would be a good addition to MaxScale.

If we have a 3 node galera cluster and somebody accidently bootstrap each one (or 2) of them, MaxScale would currently send traffic to all nodes allthough they are not in a cluster together. 

I think there are 2 ways to avoid this in the MaxScale monitor, option 1 is easy and option 2 is elegant.
1: Config parameter with the minimum cluster size. If any node is up but the minimum cluster size is too low, consider the node down.

2a: If there are several cluster identifiers present in the list of servers MaxScale knows about, only consider the servers with cluster identifier that occurs at least 1 time more then the other identifier(s)
2b: If there are several cluster identifiers present in the servers MaxScale knows about do not consider any of the nodes up
 $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,14,,0,4,0,2,0,1,0,,0,850,3,1,0,2017-01-18 10:30:21,Galera monitor: Minimum cluster size and/or refuse two cluster IDS in 1 cluster,"I think 2 extra features in the Galera monitor module would be a good addition to MaxScale.

If we have a 3 node galera cluster and somebody accidently bootstrap each one (or 2) of them, MaxScale would currently send traffic to all nodes allthough they are not in a cluster together. 

I think there are 2 ways to avoid this in the MaxScale monitor, option 1 is easy and option 2 is elegant.
1: Config parameter with the minimum cluster size. If any node is up but the minimum cluster size is too low, consider the node down.

2a: If there are several cluster identifiers present in the list of servers MaxScale knows about, only consider the servers with cluster identifier that occurs at least 1 time more then the other identifier(s)
2b: If there are several cluster identifiers present in the servers MaxScale knows about do not consider any of the nodes up
",,0,0,0,0,0.0,"Galera monitor: Minimum cluster size and/or refuse two cluster IDS in 1 cluster $end$ I think 2 extra features in the Galera monitor module would be a good addition to MaxScale.

If we have a 3 node galera cluster and somebody accidently bootstrap each one (or 2) of them, MaxScale would currently send traffic to all nodes allthough they are not in a cluster together. 

I think there are 2 ways to avoid this in the MaxScale monitor, option 1 is easy and option 2 is elegant.
1: Config parameter with the minimum cluster size. If any node is up but the minimum cluster size is too low, consider the node down.

2a: If there are several cluster identifiers present in the list of servers MaxScale knows about, only consider the servers with cluster identifier that occurs at least 1 time more then the other identifier(s)
2b: If there are several cluster identifiers present in the servers MaxScale knows about do not consider any of the nodes up
 $acceptance criteria:$",0,0,0,0,0,0,1,15276.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
876,MXS-1090,Sub-Task,MXS,2017-01-25 12:35:16,,0,Create branch,,,Create branch $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:35:16,Create branch,,,0,0,0,0,0.0,Create branch $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,107,16,0.149533,5,0.046729,3,0.0280374,1,0.00934579,1,0.00934579
877,MXS-1091,Sub-Task,MXS,2017-01-25 12:35:28,,0,Change log,,,Change log $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:35:28,Change log,,,0,0,0,0,0.0,Change log $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,108,16,0.148148,5,0.0462963,3,0.0277778,1,0.00925926,1,0.00925926
878,MXS-1092,Sub-Task,MXS,2017-01-25 12:35:36,,0,Upgrading to,,,Upgrading to $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:35:36,Upgrading to,,,0,0,0,0,0.0,Upgrading to $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,109,16,0.146789,5,0.0458716,3,0.0275229,1,0.00917431,1,0.00917431
879,MXS-1093,Sub-Task,MXS,2017-01-25 12:35:42,,0,Release notes,,,Release notes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:35:42,Release notes,,,0,0,0,0,0.0,Release notes $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,110,16,0.145455,5,0.0454545,3,0.0272727,1,0.00909091,1,0.00909091
880,MXS-1094,Sub-Task,MXS,2017-01-25 12:36:04,,0,Build and upload binaries,,,Build and upload binaries $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:36:04,Build and upload binaries,,,0,0,0,0,0.0,Build and upload binaries $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,111,16,0.144144,5,0.045045,3,0.027027,1,0.00900901,1,0.00900901
881,MXS-1095,Sub-Task,MXS,2017-01-25 12:36:23,,0,Regression testing,,,Regression testing $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:36:23,Regression testing,,,0,0,0,0,0.0,Regression testing $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,112,16,0.142857,5,0.0446429,3,0.0267857,1,0.00892857,1,0.00892857
882,MXS-1096,Sub-Task,MXS,2017-01-25 12:36:39,,0,Sync documentation to KB,,,Sync documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 12:36:39,Sync documentation to KB,,,0,0,0,0,0.0,Sync documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,113,16,0.141593,5,0.0442478,3,0.0265487,1,0.00884956,1,0.00884956
883,MXS-1098,Task,MXS,2017-01-25 13:04:19,,0,Prepare 2.0.4,,,Prepare 2.0.4 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,7,,0,850,0,0,0,2017-02-01 11:12:30,Prepare 2.0.4,,,0,0,0,0,0.0,Prepare 2.0.4 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,166.133,114,16,0.140351,5,0.0438596,3,0.0263158,1,0.00877193,1,0.00877193
884,MXS-1099,Sub-Task,MXS,2017-01-25 13:04:59,,0,Create branch,,,Create branch $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:04:59,Create branch,,,0,0,0,0,0.0,Create branch $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,115,16,0.13913,5,0.0434783,3,0.026087,1,0.00869565,1,0.00869565
885,MXS-1100,Sub-Task,MXS,2017-01-25 13:05:06,,0,Change log,,,Change log $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:05:06,Change log,,,0,0,0,0,0.0,Change log $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,116,16,0.137931,5,0.0431034,3,0.0258621,1,0.00862069,1,0.00862069
886,MXS-1101,Sub-Task,MXS,2017-01-25 13:05:14,,0,Upgrading to,,,Upgrading to $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:05:14,Upgrading to,,,0,0,0,0,0.0,Upgrading to $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,117,16,0.136752,5,0.042735,3,0.025641,1,0.00854701,1,0.00854701
887,MXS-1102,Sub-Task,MXS,2017-01-25 13:05:21,,0,Release notes,,,Release notes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:05:21,Release notes,,,0,0,0,0,0.0,Release notes $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,118,16,0.135593,5,0.0423729,3,0.0254237,1,0.00847458,1,0.00847458
888,MXS-1103,Sub-Task,MXS,2017-01-25 13:05:51,,0,Build and upload binaries,,,Build and upload binaries $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:05:51,Build and upload binaries,,,0,0,0,0,0.0,Build and upload binaries $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,119,16,0.134454,5,0.0420168,3,0.0252101,1,0.00840336,1,0.00840336
889,MXS-1104,Sub-Task,MXS,2017-01-25 13:06:00,,0,Regression testing,,,Regression testing $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:06:00,Regression testing,,,0,0,0,0,0.0,Regression testing $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,120,16,0.133333,5,0.0416667,3,0.025,1,0.00833333,1,0.00833333
890,MXS-1105,Sub-Task,MXS,2017-01-25 13:06:13,,0,Sync documentation to KB,,,Sync documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-01-25 13:06:13,Sync documentation to KB,,,0,0,0,0,0.0,Sync documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,121,16,0.132231,5,0.0413223,3,0.0247934,1,0.00826446,1,0.00826446
891,MXS-1110,Task,MXS,2017-01-27 10:50:20,,0,Generic >16MB insert/select test.,"> 16MB packets need to be handled specifically by filters and routers.

A test that
* Insert large data and ensures that there are > 16MB packets, and
* selects that same data and compares that is is identical with what was sent

could be used for testing the 16MB behaviour of all filters and routers.
",,"Generic >16MB insert/select test. $end$ > 16MB packets need to be handled specifically by filters and routers.

A test that
* Insert large data and ensures that there are > 16MB packets, and
* selects that same data and compares that is is identical with what was sent

could be used for testing the 16MB behaviour of all filters and routers.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-02-01 10:49:18,Generic >16MB insert/select test.,"> 16MB packets need to be handled specifically by filters and routers.

A test that
* Insert large data and ensures that there are > 16MB packets, and
* selects that same data and compares that is is identical with what was sent

could be used for testing the 16MB behaviour of all filters and routers.
",,0,0,0,0,0.0,"Generic >16MB insert/select test. $end$ > 16MB packets need to be handled specifically by filters and routers.

A test that
* Insert large data and ensures that there are > 16MB packets, and
* selects that same data and compares that is is identical with what was sent

could be used for testing the 16MB behaviour of all filters and routers.
 $acceptance criteria:$",0,0,0,0,0,0,1,119.967,122,16,0.131148,5,0.0409836,3,0.0245902,1,0.00819672,1,0.00819672
892,MXS-1113,Task,MXS,2017-01-31 16:12:19,,0,support of prepared statement (stmt) for schemarouter,"here is a comment from Mr. Markus Mäkelä, Software Engineer , from MariaDB Corporation.

https://groups.google.com/forum/#!topic/maxscale/4TxBg0ukA00




issue :  
schemarouter doesn't support prepared statement (stmt) queries,  it will route the query to all servers . 

maxscale execute every query on both servers (serv_A & serv_B) , it should redirect the query to one where selected database is .
i use schemarouter, it should work as proxy (high availability) .
i see this error only if i used $mysqli->prepare() . it work fine if when i use mysqli_query();
this is what i see on every query (with $mysqli->prepare() ) :
---------------------
2017-01-31 09:35:37   error  : Failed to execute select listid,eml  from max_test_A.table_1 where userid  in serv_B:3306. 42S02 Table 'max_test_A.table_1' doesn't exist
---------------------------

maxscale.cnf & script of stmt of test are in attachments .",,"support of prepared statement (stmt) for schemarouter $end$ here is a comment from Mr. Markus Mäkelä, Software Engineer , from MariaDB Corporation.

https://groups.google.com/forum/#!topic/maxscale/4TxBg0ukA00




issue :  
schemarouter doesn't support prepared statement (stmt) queries,  it will route the query to all servers . 

maxscale execute every query on both servers (serv_A & serv_B) , it should redirect the query to one where selected database is .
i use schemarouter, it should work as proxy (high availability) .
i see this error only if i used $mysqli->prepare() . it work fine if when i use mysqli_query();
this is what i see on every query (with $mysqli->prepare() ) :
---------------------
2017-01-31 09:35:37   error  : Failed to execute select listid,eml  from max_test_A.table_1 where userid  in serv_B:3306. 42S02 Table 'max_test_A.table_1' doesn't exist
---------------------------

maxscale.cnf & script of stmt of test are in attachments . $acceptance criteria:$",,imad el kholti,imad el kholti,Major,10,,0,1,0,1,0,0,0,,0,850,0,0,0,2018-07-10 09:10:56,support of prepared statement (stmt) for schemarouter,"here is a comment from Mr. Markus Mäkelä, Software Engineer , from MariaDB Corporation.

https://groups.google.com/forum/#!topic/maxscale/4TxBg0ukA00




issue :  
schemarouter doesn't support prepared statement (stmt) queries,  it will route the query to all servers . 

maxscale execute every query on both servers (serv_A & serv_B) , it should redirect the query to one where selected database is .
i use schemarouter, it should work as proxy (high availability) .
i see this error only if i used $mysqli->prepare() . it work fine if when i use mysqli_query();
this is what i see on every query (with $mysqli->prepare() ) :
---------------------
2017-01-31 09:35:37   error  : Failed to execute select listid,eml  from max_test_A.table_1 where userid  in serv_B:3306. 42S02 Table 'max_test_A.table_1' doesn't exist
---------------------------

maxscale.cnf & script of stmt of test are in attachments .",,0,0,0,0,0.0,"support of prepared statement (stmt) for schemarouter $end$ here is a comment from Mr. Markus Mäkelä, Software Engineer , from MariaDB Corporation.

https://groups.google.com/forum/#!topic/maxscale/4TxBg0ukA00




issue :  
schemarouter doesn't support prepared statement (stmt) queries,  it will route the query to all servers . 

maxscale execute every query on both servers (serv_A & serv_B) , it should redirect the query to one where selected database is .
i use schemarouter, it should work as proxy (high availability) .
i see this error only if i used $mysqli->prepare() . it work fine if when i use mysqli_query();
this is what i see on every query (with $mysqli->prepare() ) :
---------------------
2017-01-31 09:35:37   error  : Failed to execute select listid,eml  from max_test_A.table_1 where userid  in serv_B:3306. 42S02 Table 'max_test_A.table_1' doesn't exist
---------------------------

maxscale.cnf & script of stmt of test are in attachments . $acceptance criteria:$",0,0,0,0,0,0,0,12593.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
893,MXS-1115,Task,MXS,2017-02-01 08:41:15,,0,2.1 Blog,,,2.1 Blog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-02-01 08:41:15,2.1 Blog,,,0,0,0,0,0.0,2.1 Blog $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,123,16,0.130081,5,0.0406504,3,0.0243902,1,0.00813008,1,0.00813008
894,MXS-1116,Task,MXS,2017-02-01 08:42:02,,0,2.1 Blog,,,2.1 Blog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-02-01 08:42:02,2.1 Blog,,,0,0,0,0,0.0,2.1 Blog $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,124,16,0.129032,5,0.0403226,3,0.0241935,1,0.00806452,1,0.00806452
895,MXS-1117,Task,MXS,2017-02-01 09:03:34,,0,MaxScale 2.1.0 preparations,,,MaxScale 2.1.0 preparations $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,1,6,,0,850,0,0,0,2017-02-01 09:03:34,2.1.0 Release Preparations,,,1,0,0,4,0.5,2.1.0 Release Preparations $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,125,16,0.128,5,0.04,3,0.024,1,0.008,1,0.008
896,MXS-1118,Task,MXS,2017-02-01 09:42:51,,0,Run sysbench oltp tests with and without cache.,,,Run sysbench oltp tests with and without cache. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,4,0,0,0,,0,850,0,0,0,2017-02-01 09:42:51,Run sysbench oltp tests with and without cache.,,,0,0,0,0,0.0,Run sysbench oltp tests with and without cache. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,126,17,0.134921,5,0.0396825,3,0.0238095,1,0.00793651,1,0.00793651
897,MXS-1119,Task,MXS,2017-02-01 10:50:21,,0,Create simple snapshot job for testing branch (minimal configuring needed),"Create a job with the following parameters:
* value
* source
* test_branch

The job build MaxScale from the branch pointed by _value_ and runs a snapshot test on the package created by that build. The snapshot test should use the same VMs for testing to speed it up and only one build should run at a time. The job should (maybe?) run on _master_ server only. ",,"Create simple snapshot job for testing branch (minimal configuring needed) $end$ Create a job with the following parameters:
* value
* source
* test_branch

The job build MaxScale from the branch pointed by _value_ and runs a snapshot test on the package created by that build. The snapshot test should use the same VMs for testing to speed it up and only one build should run at a time. The job should (maybe?) run on _master_ server only.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-02-01 10:50:21,Create simple snapshot job for testing branch (minimal configuring needed),"Create a job with the following parameters:
* value
* source
* test_branch

The job build MaxScale from the branch pointed by _value_ and runs a snapshot test on the package created by that build. The snapshot test should use the same VMs for testing to speed it up and only one build should run at a time. The job should (maybe?) run on _master_ server only. ",,0,0,0,0,0.0,"Create simple snapshot job for testing branch (minimal configuring needed) $end$ Create a job with the following parameters:
* value
* source
* test_branch

The job build MaxScale from the branch pointed by _value_ and runs a snapshot test on the package created by that build. The snapshot test should use the same VMs for testing to speed it up and only one build should run at a time. The job should (maybe?) run on _master_ server only.  $acceptance criteria:$",0,0,0,0,0,0,0,0.0,127,17,0.133858,5,0.0393701,3,0.023622,1,0.00787401,1,0.00787401
898,MXS-1121,Task,MXS,2017-02-02 18:28:48,,0,Create Test Cases for 10.2 bulk insert,The new method of inserting large batches of data needs to be tested. This was introduced with 10.2 and C/C 3.0.,,Create Test Cases for 10.2 bulk insert $end$ The new method of inserting large batches of data needs to be tested. This was introduced with 10.2 and C/C 3.0. $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,1,0,1,0,1,0,,0,850,1,1,0,2017-02-15 10:46:15,Create Test Cases for 10.2 bulk insert,The new method of inserting large batches of data needs to be tested. This was introduced with 10.2 and C/C 3.0.,,0,0,0,0,0.0,Create Test Cases for 10.2 bulk insert $end$ The new method of inserting large batches of data needs to be tested. This was introduced with 10.2 and C/C 3.0. $acceptance criteria:$,0,0,0,0,0,0,0,304.283,24,5,0.208333,2,0.0833333,0,0.0,0,0.0,0,0.0
899,MXS-1124,Sub-Task,MXS,2017-02-07 06:41:57,,0,Change Log,,,Change Log $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-02-07 06:41:57,Change Log,,,0,0,0,0,0.0,Change Log $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,128,17,0.132812,5,0.0390625,3,0.0234375,1,0.0078125,1,0.0078125
900,MXS-1125,Sub-Task,MXS,2017-02-07 06:42:11,,0,Release Notes,,,Release Notes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-02-07 06:42:11,Release Notes,,,0,0,0,0,0.0,Release Notes $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,129,17,0.131783,5,0.0387597,3,0.0232558,1,0.00775194,1,0.00775194
901,MXS-1126,Sub-Task,MXS,2017-02-07 06:42:31,,0,Upgrade To,,,Upgrade To $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-02-07 06:42:31,Upgrade To,,,0,0,0,0,0.0,Upgrade To $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,130,17,0.130769,5,0.0384615,3,0.0230769,1,0.00769231,1,0.00769231
902,MXS-1127,Sub-Task,MXS,2017-02-07 06:42:59,,0,Update release data and sync documentation to KB,,,Update release data and sync documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-02-07 06:42:59,Update release data and sync documentation to KB,,,0,0,0,0,0.0,Update release data and sync documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,131,17,0.129771,5,0.0381679,3,0.0229008,1,0.00763359,1,0.00763359
903,MXS-1128,Sub-Task,MXS,2017-02-07 06:43:33,,0,Build packages and upload.,,,Build packages and upload. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-02-07 06:43:33,Build packages and upload.,,,0,0,0,0,0.0,Build packages and upload. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,132,17,0.128788,5,0.0378788,3,0.0227273,1,0.00757576,1,0.00757576
904,MXS-1129,Sub-Task,MXS,2017-02-07 06:43:49,,0,Create branch and tag,,,Create branch and tag $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-02-07 06:43:49,Create branch and tag,,,0,0,0,0,0.0,Create branch and tag $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,133,17,0.12782,5,0.037594,3,0.0225564,1,0.0075188,1,0.0075188
905,MXS-1134,New Feature,MXS,2017-02-10 10:52:45,MXS-1133,0,Cache Invalidation,"Currently there is no cache invalidation, but for _time-to-live_.

It would be relatively straightforward to invalidate cache entries as the result of updates, inserts and deletes.",,"Cache Invalidation $end$ Currently there is no cache invalidation, but for _time-to-live_.

It would be relatively straightforward to invalidate cache entries as the result of updates, inserts and deletes. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,26,,1,0,3,2,0,1,0,,0,850,0,1,0,2019-10-07 09:47:50,Cache Invalidation,"Currently there is no cache invalidation, but for _time-to-live_.

It would be relatively straightforward to invalidate cache entries as the result of updates, inserts and deletes.",,0,0,0,0,0.0,"Cache Invalidation $end$ Currently there is no cache invalidation, but for _time-to-live_.

It would be relatively straightforward to invalidate cache entries as the result of updates, inserts and deletes. $acceptance criteria:$",0,0,0,0,0,0,1,23254.9,134,17,0.126866,5,0.0373134,3,0.0223881,1,0.00746269,1,0.00746269
906,MXS-1136,Task,MXS,2017-02-10 11:05:46,MXS-1133,0,Cache: Smarter handling of transactions,"Currently the cache is used and populated only in conjunction with explicitly read only transactions, or if an explicit transaction is not used and autocommit is on.

During a non-read only transaction It would be possible to use and populate the cache until the first non-SELECT. ",,"Cache: Smarter handling of transactions $end$ Currently the cache is used and populated only in conjunction with explicitly read only transactions, or if an explicit transaction is not used and autocommit is on.

During a non-read only transaction It would be possible to use and populate the cache until the first non-SELECT.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-02-15 10:48:46,Cache: Smarter handling of transactions,"Currently the cache is used and populated only in conjunction with explicitly read only transactions, or if an explicit transaction is not used and autocommit is on.

During a non-read only transaction It would be possible to use and populate the cache until the first non-SELECT. ",,0,0,0,0,0.0,"Cache: Smarter handling of transactions $end$ Currently the cache is used and populated only in conjunction with explicitly read only transactions, or if an explicit transaction is not used and autocommit is on.

During a non-read only transaction It would be possible to use and populate the cache until the first non-SELECT.  $acceptance criteria:$",0,0,0,0,0,0,0,119.717,135,17,0.125926,5,0.037037,3,0.0222222,1,0.00740741,1,0.00740741
907,MXS-1137,Task,MXS,2017-02-10 11:11:38,MXS-1133,0,Cache: Handle variables,"_The cache key is effectively the entire SELECT statement. However, the value of any variables used in the select is not considered. For instance, if a variable is used in the WHERE clause of the select, a subsequent identical select will return the wrong result, if the value of the variable has been changed in between._

The value of variables should either be tracked or then if variables are used, the statement should not be cached.",,"Cache: Handle variables $end$ _The cache key is effectively the entire SELECT statement. However, the value of any variables used in the select is not considered. For instance, if a variable is used in the WHERE clause of the select, a subsequent identical select will return the wrong result, if the value of the variable has been changed in between._

The value of variables should either be tracked or then if variables are used, the statement should not be cached. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-01 09:49:21,Cache: Handle variables,"_The cache key is effectively the entire SELECT statement. However, the value of any variables used in the select is not considered. For instance, if a variable is used in the WHERE clause of the select, a subsequent identical select will return the wrong result, if the value of the variable has been changed in between._

The value of variables should either be tracked or then if variables are used, the statement should not be cached.",,0,0,0,0,0.0,"Cache: Handle variables $end$ _The cache key is effectively the entire SELECT statement. However, the value of any variables used in the select is not considered. For instance, if a variable is used in the WHERE clause of the select, a subsequent identical select will return the wrong result, if the value of the variable has been changed in between._

The value of variables should either be tracked or then if variables are used, the statement should not be cached. $acceptance criteria:$",0,0,0,0,0,0,0,454.617,136,17,0.125,5,0.0367647,3,0.0220588,1,0.00735294,1,0.00735294
908,MXS-1138,New Feature,MXS,2017-02-10 11:12:53,MXS-1133,0,User specific Cache,"The cache is not aware of grants.

The implication is that unless the cache has been explicitly configured who the caching should apply to, the presence of the cache may provide a user with access to data he should not have access to.",,"User specific Cache $end$ The cache is not aware of grants.

The implication is that unless the cache has been explicitly configured who the caching should apply to, the presence of the cache may provide a user with access to data he should not have access to. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,25,,0,0,0,1,0,2,0,,0,850,0,0,0,2019-10-28 08:54:11,Cache: Grant awareness,"_The cache is not aware of grants.

The implication is that unless the cache has been explicitly configured who the caching should apply to, the presence of the cache may provide a user with access to data he should not have access to._

The grants could be figured out automatically and taken into account.",,1,1,0,21,0.266667,"Cache: Grant awareness $end$ _The cache is not aware of grants.

The implication is that unless the cache has been explicitly configured who the caching should apply to, the presence of the cache may provide a user with access to data he should not have access to._

The grants could be figured out automatically and taken into account. $acceptance criteria:$",2,1,1,1,1,1,1,23757.7,137,17,0.124088,5,0.0364963,3,0.0218978,1,0.00729927,1,0.00729927
909,MXS-1140,Task,MXS,2017-02-15 08:50:28,,0,Test MaxScale HA Active/Standby & Active/Active Configuration,"Please see slides 5 through 8 of [this|https://docs.google.com/presentation/d/1NkiYSB82mN0LFmXP6voPiczFoBCVe1optndlE6Y4AIY/edit#slide=id.g21317f74fb_0_5] for the system setup.
",,"Test MaxScale HA Active/Standby & Active/Active Configuration $end$ Please see slides 5 through 8 of [this|https://docs.google.com/presentation/d/1NkiYSB82mN0LFmXP6voPiczFoBCVe1optndlE6Y4AIY/edit#slide=id.g21317f74fb_0_5] for the system setup.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,38,,0,2,0,16,0,3,0,,0,850,2,0,0,2017-02-15 10:30:01,Test MaxScale HA Active/Standby Configuration,"[~dshjoshi] Please provide detailed system-setup so that the right stuff is tested.
",,1,2,0,22,0.7,"Test MaxScale HA Active/Standby Configuration $end$ [~dshjoshi] Please provide detailed system-setup so that the right stuff is tested.
 $acceptance criteria:$",3,1,1,1,1,1,1,1.65,138,18,0.130435,6,0.0434783,4,0.0289855,2,0.0144928,2,0.0144928
910,MXS-1141,New Feature,MXS,2017-02-15 10:31:45,,0,maxbinlogcheck to replace an event with ignorable event,"Hi,

MaxScale binlog server is a great tool for point in time recovery. We use maxscale to continuously copy the binlogs to another machine.
Let's say a disaster occurs, somebody executes DROP TABLE databse.table;

We could do point in time recovery by restoring a backup and making the backup slave to MaxScale. It would replicate and replicate the disaster as well.

It would be great if we can nullify the event (make it ignorable by the replication slave). This way, we can remove the DROP TABLE from the binlog file and replicate all other events from MaxScale.

For example --remove-event=start_pos to remove the even from start_pos until the end of the event.

Thank you!",,"maxbinlogcheck to replace an event with ignorable event $end$ Hi,

MaxScale binlog server is a great tool for point in time recovery. We use maxscale to continuously copy the binlogs to another machine.
Let's say a disaster occurs, somebody executes DROP TABLE databse.table;

We could do point in time recovery by restoring a backup and making the backup slave to MaxScale. It would replicate and replicate the disaster as well.

It would be great if we can nullify the event (make it ignorable by the replication slave). This way, we can remove the DROP TABLE from the binlog file and replicate all other events from MaxScale.

For example --remove-event=start_pos to remove the even from start_pos until the end of the event.

Thank you! $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,6,,0,1,2,1,0,0,0,,0,850,1,0,0,2017-02-15 10:47:28,maxbinlogcheck to replace an event with ignorable event,"Hi,

MaxScale binlog server is a great tool for point in time recovery. We use maxscale to continuously copy the binlogs to another machine.
Let's say a disaster occurs, somebody executes DROP TABLE databse.table;

We could do point in time recovery by restoring a backup and making the backup slave to MaxScale. It would replicate and replicate the disaster as well.

It would be great if we can nullify the event (make it ignorable by the replication slave). This way, we can remove the DROP TABLE from the binlog file and replicate all other events from MaxScale.

For example --remove-event=start_pos to remove the even from start_pos until the end of the event.

Thank you!",,0,0,0,0,0.0,"maxbinlogcheck to replace an event with ignorable event $end$ Hi,

MaxScale binlog server is a great tool for point in time recovery. We use maxscale to continuously copy the binlogs to another machine.
Let's say a disaster occurs, somebody executes DROP TABLE databse.table;

We could do point in time recovery by restoring a backup and making the backup slave to MaxScale. It would replicate and replicate the disaster as well.

It would be great if we can nullify the event (make it ignorable by the replication slave). This way, we can remove the DROP TABLE from the binlog file and replicate all other events from MaxScale.

For example --remove-event=start_pos to remove the even from start_pos until the end of the event.

Thank you! $acceptance criteria:$",0,0,0,0,0,0,0,0.25,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
911,MXS-1142,New Feature,MXS,2017-02-15 10:35:13,,0,maxbinlogcheck to remove transaction from binlog,"Hi,

After being able to nullify one event from the binary log, I think it would be great if the maxbinlogcheck utitlity could remove/nullify/make ignorable a whole transaction from the binlog.

So, for example: maxbinlogcheck --remove-transaction=pos
Could look in the binlog to find the previous BEGIN statement and then look further until it finds the next COMMIT. Then, when the positions are known, make everything in between ignorable.

Thank you!
",,"maxbinlogcheck to remove transaction from binlog $end$ Hi,

After being able to nullify one event from the binary log, I think it would be great if the maxbinlogcheck utitlity could remove/nullify/make ignorable a whole transaction from the binlog.

So, for example: maxbinlogcheck --remove-transaction=pos
Could look in the binlog to find the previous BEGIN statement and then look further until it finds the next COMMIT. Then, when the positions are known, make everything in between ignorable.

Thank you!
 $acceptance criteria:$",,Michaël de groot,Michaël de groot,Minor,6,,0,1,2,1,0,0,0,,0,850,1,0,0,2017-02-15 10:47:40,maxbinlogcheck to remove transaction from binlog,"Hi,

After being able to nullify one event from the binary log, I think it would be great if the maxbinlogcheck utitlity could remove/nullify/make ignorable a whole transaction from the binlog.

So, for example: maxbinlogcheck --remove-transaction=pos
Could look in the binlog to find the previous BEGIN statement and then look further until it finds the next COMMIT. Then, when the positions are known, make everything in between ignorable.

Thank you!
",,0,0,0,0,0.0,"maxbinlogcheck to remove transaction from binlog $end$ Hi,

After being able to nullify one event from the binary log, I think it would be great if the maxbinlogcheck utitlity could remove/nullify/make ignorable a whole transaction from the binlog.

So, for example: maxbinlogcheck --remove-transaction=pos
Could look in the binlog to find the previous BEGIN statement and then look further until it finds the next COMMIT. Then, when the positions are known, make everything in between ignorable.

Thank you!
 $acceptance criteria:$",0,0,0,0,0,0,0,0.2,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
912,MXS-1149,Task,MXS,2017-02-28 08:07:47,,0,maxrows should return empty resultset instead of OK,"To minimize the risk for client confusion, maxrows should return an empty resultset instead of OK when any of its limits have been reached. As the client expects a resultset, it may not be prepared to handle an OK.",,"maxrows should return empty resultset instead of OK $end$ To minimize the risk for client confusion, maxrows should return an empty resultset instead of OK when any of its limits have been reached. As the client expects a resultset, it may not be prepared to handle an OK. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-02-28 08:07:47,maxrows should return empty resultset instead of OK,"To minimize the risk for client confusion, maxrows should return an empty resultset instead of OK when any of its limits have been reached. As the client expects a resultset, it may not be prepared to handle an OK.",,0,0,0,0,0.0,"maxrows should return empty resultset instead of OK $end$ To minimize the risk for client confusion, maxrows should return an empty resultset instead of OK when any of its limits have been reached. As the client expects a resultset, it may not be prepared to handle an OK. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,139,19,0.136691,7,0.0503597,5,0.0359712,3,0.0215827,3,0.0215827
913,MXS-1150,Task,MXS,2017-03-01 09:15:56,MXS-1133,0,Cache: Do not cache SELECTs using certain functions.,"SELECTs using e.g. {{LAST_INSERT_ID()}} or {{CURRENT_TIMESTAMP()}} should not be cached, as the result inherently changes.",,"Cache: Do not cache SELECTs using certain functions. $end$ SELECTs using e.g. {{LAST_INSERT_ID()}} or {{CURRENT_TIMESTAMP()}} should not be cached, as the result inherently changes. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-01 09:49:28,Cache: Do not cache SELECTs using certain functions.,"SELECTs using e.g. {{LAST_INSERT_ID()}} or {{CURRENT_TIMESTAMP()}} should not be cached, as the result inherently changes.",,0,0,0,0,0.0,"Cache: Do not cache SELECTs using certain functions. $end$ SELECTs using e.g. {{LAST_INSERT_ID()}} or {{CURRENT_TIMESTAMP()}} should not be cached, as the result inherently changes. $acceptance criteria:$",0,0,0,0,0,0,0,0.55,140,19,0.135714,7,0.05,5,0.0357143,3,0.0214286,3,0.0214286
914,MXS-1152,Task,MXS,2017-03-01 11:07:59,,0,namedserverfilter: Switch to PCRE2 regexes,"* Take into use and accept only PCRE2 regexes.
* Put note note in the release notes to that effect. ",,"namedserverfilter: Switch to PCRE2 regexes $end$ * Take into use and accept only PCRE2 regexes.
* Put note note in the release notes to that effect.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-01 11:07:59,namedserverfilter: Switch to PCRE2 regexes,"* Take into use and accept only PCRE2 regexes.
* Put note note in the release notes to that effect. ",,0,0,0,0,0.0,"namedserverfilter: Switch to PCRE2 regexes $end$ * Take into use and accept only PCRE2 regexes.
* Put note note in the release notes to that effect.  $acceptance criteria:$",0,0,0,0,0,0,0,0.0,141,19,0.134752,7,0.0496454,5,0.035461,3,0.0212766,3,0.0212766
915,MXS-1153,Task,MXS,2017-03-01 11:45:45,,0,Extend namedserverfilter,"Extend _namedserverfilter_ so that it is possible to use as target
* master,
* slave,
* uptodate server, and
* a specific set of servers
in addition to the current possibility of specifying a _particular_ server.

Also make it possible to specify several regexes and targets for a single filter.

The current behaviour of
{code}
match=some string
server=server2
{code}
should be retained but can be mutually exclusive with a new way of specifying several regexes and targets, for instance, like:
{code}
match1=some string
target1=server1
match2=some string
target2=->master
match3=some string
target3=->slave
match4=some string
target4=server1,server2,server3
{code}
",,"Extend namedserverfilter $end$ Extend _namedserverfilter_ so that it is possible to use as target
* master,
* slave,
* uptodate server, and
* a specific set of servers
in addition to the current possibility of specifying a _particular_ server.

Also make it possible to specify several regexes and targets for a single filter.

The current behaviour of
{code}
match=some string
server=server2
{code}
should be retained but can be mutually exclusive with a new way of specifying several regexes and targets, for instance, like:
{code}
match1=some string
target1=server1
match2=some string
target2=->master
match3=some string
target3=->slave
match4=some string
target4=server1,server2,server3
{code}
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-03-01 11:45:55,Extend namedserverfilter,"Extend _namedserverfilter_ so that it is possible to use as target
* master,
* slave,
* uptodate server, and
* a specific set of servers
in addition to the current possibility of specifying a _particular_ server.

Also make it possible to specify several regexes and targets for a single filter.

The current behaviour of
{code}
match=some string
server=server2
{code}
should be retained but can be mutually exclusive with a new way of specifying several regexes and targets, for instance, like:
{code}
match1=some string
target1=server1
match2=some string
target2=->master
match3=some string
target3=->slave
match4=some string
target4=server1,server2,server3
{code}
",,0,0,0,0,0.0,"Extend namedserverfilter $end$ Extend _namedserverfilter_ so that it is possible to use as target
* master,
* slave,
* uptodate server, and
* a specific set of servers
in addition to the current possibility of specifying a _particular_ server.

Also make it possible to specify several regexes and targets for a single filter.

The current behaviour of
{code}
match=some string
server=server2
{code}
should be retained but can be mutually exclusive with a new way of specifying several regexes and targets, for instance, like:
{code}
match1=some string
target1=server1
match2=some string
target2=->master
match3=some string
target3=->slave
match4=some string
target4=server1,server2,server3
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,142,19,0.133803,7,0.0492958,5,0.0352113,3,0.0211268,3,0.0211268
916,MXS-1156,Task,MXS,2017-03-02 10:07:14,,0,Support heartbeat and retry syntax for Binlog Router,"The best practice for defining a slave is to set short heartbeat checks , 
this task would enable to do 
CHANGE MASTER TO master_connect_retry=%d, master_heartbeat_period=%d as 
documented in the the manuals and KB ",,"Support heartbeat and retry syntax for Binlog Router $end$ The best practice for defining a slave is to set short heartbeat checks , 
this task would enable to do 
CHANGE MASTER TO master_connect_retry=%d, master_heartbeat_period=%d as 
documented in the the manuals and KB  $acceptance criteria:$",,VAROQUI Stephane,VAROQUI Stephane,Major,14,,0,4,1,2,0,1,0,,0,850,4,1,0,2017-08-09 09:49:36,Support heartbeat and retry syntax for Binlog Router,"The best practice for defining a slave is to set short heartbeat checks , 
this task would enable to do 
CHANGE MASTER TO master_connect_retry=%d, master_heartbeat_period=%d as 
documented in the the manuals and KB ",,0,0,0,0,0.0,"Support heartbeat and retry syntax for Binlog Router $end$ The best practice for defining a slave is to set short heartbeat checks , 
this task would enable to do 
CHANGE MASTER TO master_connect_retry=%d, master_heartbeat_period=%d as 
documented in the the manuals and KB  $acceptance criteria:$",0,0,0,0,0,0,1,3839.7,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
917,MXS-1177,New Feature,MXS,2017-03-08 14:23:01,,0,Configurable monitor check failures,"It would be great to have configurable monitor check failures in MaxScale, especially when we want to put the monitor_interval to a low value. Right now if a monitor fails a check it marks the server as down as soon as the check is failed.

I envision something similar as ProxySQL has:

If a host misses mysql-monitor_ping_max_failures pings in a row, MySQL_Monitor informs MySQL_Hostgroup_Manager that the node is unreacheable and that should immediately kill all connections. It is important to note that in case a connection to the backend is not available, MySQL_Monitor will first try to connect in order to ping, therefore the time to detect a node down could be one of the two:

mysql-monitor_ping_max_failures * mysql-monitor_connect_timeout
mysql-monitor_ping_max_failures * mysql-monitor_ping_timeout

A monitor_max_failures parameter with a default of 1 would be an improvement of this feature.",,"Configurable monitor check failures $end$ It would be great to have configurable monitor check failures in MaxScale, especially when we want to put the monitor_interval to a low value. Right now if a monitor fails a check it marks the server as down as soon as the check is failed.

I envision something similar as ProxySQL has:

If a host misses mysql-monitor_ping_max_failures pings in a row, MySQL_Monitor informs MySQL_Hostgroup_Manager that the node is unreacheable and that should immediately kill all connections. It is important to note that in case a connection to the backend is not available, MySQL_Monitor will first try to connect in order to ping, therefore the time to detect a node down could be one of the two:

mysql-monitor_ping_max_failures * mysql-monitor_connect_timeout
mysql-monitor_ping_max_failures * mysql-monitor_ping_timeout

A monitor_max_failures parameter with a default of 1 would be an improvement of this feature. $acceptance criteria:$",,Guillaume Lefranc,Guillaume Lefranc,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-03-29 09:29:22,Configurable monitor check failures,"It would be great to have configurable monitor check failures in MaxScale, especially when we want to put the monitor_interval to a low value. Right now if a monitor fails a check it marks the server as down as soon as the check is failed.

I envision something similar as ProxySQL has:

If a host misses mysql-monitor_ping_max_failures pings in a row, MySQL_Monitor informs MySQL_Hostgroup_Manager that the node is unreacheable and that should immediately kill all connections. It is important to note that in case a connection to the backend is not available, MySQL_Monitor will first try to connect in order to ping, therefore the time to detect a node down could be one of the two:

mysql-monitor_ping_max_failures * mysql-monitor_connect_timeout
mysql-monitor_ping_max_failures * mysql-monitor_ping_timeout

A monitor_max_failures parameter with a default of 1 would be an improvement of this feature.",,0,0,0,0,0.0,"Configurable monitor check failures $end$ It would be great to have configurable monitor check failures in MaxScale, especially when we want to put the monitor_interval to a low value. Right now if a monitor fails a check it marks the server as down as soon as the check is failed.

I envision something similar as ProxySQL has:

If a host misses mysql-monitor_ping_max_failures pings in a row, MySQL_Monitor informs MySQL_Hostgroup_Manager that the node is unreacheable and that should immediately kill all connections. It is important to note that in case a connection to the backend is not available, MySQL_Monitor will first try to connect in order to ping, therefore the time to detect a node down could be one of the two:

mysql-monitor_ping_max_failures * mysql-monitor_connect_timeout
mysql-monitor_ping_max_failures * mysql-monitor_ping_timeout

A monitor_max_failures parameter with a default of 1 would be an improvement of this feature. $acceptance criteria:$",0,0,0,0,0,0,0,499.1,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
918,MXS-1181,Task,MXS,2017-03-14 06:34:08,,0,Add IPv6 Support to MaxScale,"Add IPv6 support to MaxScale:
* Recognize and accept IPv6 addresses in the configuration file.
* Connect to servers using IPv6 if IPV6 address specified.
etc.
",,"Add IPv6 Support to MaxScale $end$ Add IPv6 support to MaxScale:
* Recognize and accept IPv6 addresses in the configuration file.
* Connect to servers using IPv6 if IPV6 address specified.
etc.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-03-15 10:32:45,Add IPv6 Support to MaxScale,"Add IPv6 support to MaxScale:
* Recognize and accept IPv6 addresses in the configuration file.
* Connect to servers using IPv6 if IPV6 address specified.
etc.
",,0,0,0,0,0.0,"Add IPv6 Support to MaxScale $end$ Add IPv6 support to MaxScale:
* Recognize and accept IPv6 addresses in the configuration file.
* Connect to servers using IPv6 if IPV6 address specified.
etc.
 $acceptance criteria:$",0,0,0,0,0,0,0,27.9667,143,19,0.132867,7,0.048951,5,0.034965,3,0.020979,3,0.020979
919,MXS-1184,Task,MXS,2017-03-15 09:48:15,,0,Detect transaction boundaries efficiently,"Detecting transaction boundaries using the query classifier is costly, if that is the only reason a statement is parsed and classified.

The detection should be performed in a more efficient manner.
",,"Detect transaction boundaries efficiently $end$ Detecting transaction boundaries using the query classifier is costly, if that is the only reason a statement is parsed and classified.

The detection should be performed in a more efficient manner.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-15 10:21:40,Detect transaction boundaries efficiently,"Detecting transaction boundaries using the query classifier is costly, if that is the only reason a statement is parsed and classified.

The detection should be performed in a more efficient manner.
",,0,0,0,0,0.0,"Detect transaction boundaries efficiently $end$ Detecting transaction boundaries using the query classifier is costly, if that is the only reason a statement is parsed and classified.

The detection should be performed in a more efficient manner.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.55,144,19,0.131944,7,0.0486111,5,0.0347222,3,0.0208333,3,0.0208333
920,MXS-1185,Task,MXS,2017-03-15 10:29:37,,0,Allow modules to express what information they are interested in,"Most modules are interested only in query type and/or query operation.

Consequently, e.g. the field and function information should not be collected by default.
",,"Allow modules to express what information they are interested in $end$ Most modules are interested only in query type and/or query operation.

Consequently, e.g. the field and function information should not be collected by default.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-15 10:29:37,Allow modules to express what information they are interested in,"Most modules are interested only in query type and/or query operation.

Consequently, e.g. the field and function information should not be collected by default.
",,0,0,0,0,0.0,"Allow modules to express what information they are interested in $end$ Most modules are interested only in query type and/or query operation.

Consequently, e.g. the field and function information should not be collected by default.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,145,19,0.131034,7,0.0482759,5,0.0344828,3,0.0206897,3,0.0206897
921,MXS-1186,Task,MXS,2017-03-15 10:30:04,,0,Use more efficient keys in cache,,,Use more efficient keys in cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-15 10:30:04,Use more efficient keys in cache,,,0,0,0,0,0.0,Use more efficient keys in cache $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,146,19,0.130137,7,0.0479452,5,0.0342466,3,0.0205479,3,0.0205479
922,MXS-1193,Task,MXS,2017-03-21 11:21:18,,0,Add 10.2 as a regularly tested backend,,,Add 10.2 as a regularly tested backend $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-04-19 09:50:05,Add 10.2 as a regularly tested backend,,,0,0,0,0,0.0,Add 10.2 as a regularly tested backend $end$ $acceptance criteria:$,0,0,0,0,0,0,0,694.467,147,19,0.129252,7,0.047619,5,0.0340136,3,0.0204082,3,0.0204082
923,MXS-1194,Task,MXS,2017-03-21 11:30:32,,0,Check what 10.2 language extensions are not supported by which filters,"We need to document any issues with introduced functionality in 10.2 and MaxScale. E.g. how are windoing functions handled by various filters.

* Window functions
* CTE (common table expression)
* JSON functions",,"Check what 10.2 language extensions are not supported by which filters $end$ We need to document any issues with introduced functionality in 10.2 and MaxScale. E.g. how are windoing functions handled by various filters.

* Window functions
* CTE (common table expression)
* JSON functions $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-03-29 09:27:09,Check what 10.2 language extensions are not supported by which filters,"We need to document any issues with introduced functionality in 10.2 and MaxScale. E.g. how are windoing functions handled by various filters.

* Window functions
* CTE (common table expression)
* JSON functions",,0,0,0,0,0.0,"Check what 10.2 language extensions are not supported by which filters $end$ We need to document any issues with introduced functionality in 10.2 and MaxScale. E.g. how are windoing functions handled by various filters.

* Window functions
* CTE (common table expression)
* JSON functions $acceptance criteria:$",0,0,0,0,0,0,1,189.933,148,19,0.128378,7,0.0472973,5,0.0337838,3,0.0202703,3,0.0202703
924,MXS-1196,Task,MXS,2017-03-21 12:04:25,,0,Parser compatibility,,,Parser compatibility $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,2,,0,850,0,0,0,2017-05-03 07:19:38,Parser compatibility,,,0,0,0,0,0.0,Parser compatibility $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1027.25,149,19,0.127517,7,0.0469799,5,0.033557,3,0.0201342,3,0.0201342
925,MXS-1206,Task,MXS,2017-03-29 08:52:32,,0,Prepare M17 talk on MaxScale,,,Prepare M17 talk on MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-29 08:52:32,Prepare M17 talk on MaxScale,,,0,0,0,0,0.0,Prepare M17 talk on MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,150,19,0.126667,7,0.0466667,5,0.0333333,3,0.02,3,0.02
926,MXS-1207,Task,MXS,2017-03-29 08:52:48,,0,Prepare M17 talk on MaxScale Security,,,Prepare M17 talk on MaxScale Security $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-03-29 08:52:48,Prepare M17 talk on MaxScale Security,,,0,0,0,0,0.0,Prepare M17 talk on MaxScale Security $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,151,19,0.125828,7,0.0463576,5,0.0331126,3,0.0198675,3,0.0198675
927,MXS-1208,Task,MXS,2017-03-29 08:53:18,,0,Rearrange Polling Mechanism,"Rearrange the Polling Mechanism
* Add cross-thread epoll based messaging mechanism
* Introduce separate thread for admin tasks
",,"Rearrange Polling Mechanism $end$ Rearrange the Polling Mechanism
* Add cross-thread epoll based messaging mechanism
* Introduce separate thread for admin tasks
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-03-29 08:53:18,Rearrange Polling Mechanism,"Rearrange the Polling Mechanism
* Add cross-thread epoll based messaging mechanism
* Introduce separate thread for admin tasks
",,0,0,0,0,0.0,"Rearrange Polling Mechanism $end$ Rearrange the Polling Mechanism
* Add cross-thread epoll based messaging mechanism
* Introduce separate thread for admin tasks
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,152,19,0.125,7,0.0460526,5,0.0328947,3,0.0197368,3,0.0197368
928,MXS-1209,New Feature,MXS,2017-03-29 09:16:44,,0,Support MariaDB GTID in binlog replication handshake with MariaDB 10 Master,,,Support MariaDB GTID in binlog replication handshake with MariaDB 10 Master $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,9,,0,2,0,3,0,0,0,,0,850,2,0,0,2017-03-29 09:17:09,Support MariaDB GTID in binlog replication handshake with MariaDB 10 Master,,,0,0,0,0,0.0,Support MariaDB GTID in binlog replication handshake with MariaDB 10 Master $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
929,MXS-1215,Task,MXS,2017-03-30 07:02:53,,0,The MaxScale configuration file should have a specific regex type.,"In some cases, the value of a key in the MaxScale configuration file is a regular expression. As such, every character of the value is significant.

Currently, it is quite error prone as there is no specific starting and terminating character (e.g. quote) for the regular expression. For instance, in the following

{{match= *from *users}}

the space after the equal sign is significant and as will any trailing spaces be, however invisible they might be.

To reduce the risk for confusion there should be a specific _regex_ type that requires the regex string to be enclosed in some specific character. E.g.

{{match="" *from *users ""}}

Note that whatever the quote character is there must be a way for escaping it to allow its use in the regex itself, or then the requirement must simply be that the first non whitespace character after the equal sign and the last character of the line must be the quote character, which are then simply ignored.",,"The MaxScale configuration file should have a specific regex type. $end$ In some cases, the value of a key in the MaxScale configuration file is a regular expression. As such, every character of the value is significant.

Currently, it is quite error prone as there is no specific starting and terminating character (e.g. quote) for the regular expression. For instance, in the following

{{match= *from *users}}

the space after the equal sign is significant and as will any trailing spaces be, however invisible they might be.

To reduce the risk for confusion there should be a specific _regex_ type that requires the regex string to be enclosed in some specific character. E.g.

{{match="" *from *users ""}}

Note that whatever the quote character is there must be a way for escaping it to allow its use in the regex itself, or then the requirement must simply be that the first non whitespace character after the equal sign and the last character of the line must be the quote character, which are then simply ignored. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-05-18 09:44:30,The MaxScale configuration file should have a specific regex type.,"In some cases, the value of a key in the MaxScale configuration file is a regular expression. As such, every character of the value is significant.

Currently, it is quite error prone as there is no specific starting and terminating character (e.g. quote) for the regular expression. For instance, in the following

{{match= *from *users}}

the space after the equal sign is significant and as will any trailing spaces be, however invisible they might be.

To reduce the risk for confusion there should be a specific _regex_ type that requires the regex string to be enclosed in some specific character. E.g.

{{match="" *from *users ""}}

Note that whatever the quote character is there must be a way for escaping it to allow its use in the regex itself, or then the requirement must simply be that the first non whitespace character after the equal sign and the last character of the line must be the quote character, which are then simply ignored.",,0,0,0,0,0.0,"The MaxScale configuration file should have a specific regex type. $end$ In some cases, the value of a key in the MaxScale configuration file is a regular expression. As such, every character of the value is significant.

Currently, it is quite error prone as there is no specific starting and terminating character (e.g. quote) for the regular expression. For instance, in the following

{{match= *from *users}}

the space after the equal sign is significant and as will any trailing spaces be, however invisible they might be.

To reduce the risk for confusion there should be a specific _regex_ type that requires the regex string to be enclosed in some specific character. E.g.

{{match="" *from *users ""}}

Note that whatever the quote character is there must be a way for escaping it to allow its use in the regex itself, or then the requirement must simply be that the first non whitespace character after the equal sign and the last character of the line must be the quote character, which are then simply ignored. $acceptance criteria:$",0,0,0,0,0,0,1,1178.68,153,19,0.124183,7,0.0457516,5,0.0326797,3,0.0196078,3,0.0196078
930,MXS-1220,New Feature,MXS,2017-04-14 01:31:39,,0,REST API,"h1. REST API for Managing MaxScale
h2. Overview
The REST API in MaxScale should be implement as a HTTP based administrative interface that manages resources in MaxScale in a stateless (i.e. RESTful) manner. This allows transactional communication with MaxScale.
h2. Requirements
The API should implement the API documented in the [MaxScale REST API documentation|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md]. 
h2. Implementation details
The current resources closely follow the maxadmin output of the previous versions.

Currently, all object references, e.g. servers in services, are expressed as relative pathnames. This means that nested resources in services, filters and monitors need to be queried separately. The required linkage of the nested resources could be done inside MaxScale so that only one query is needed to fully display an object.

The layout of the resources follow the JSON API specification: http://jsonapi.org/",,"REST API $end$ h1. REST API for Managing MaxScale
h2. Overview
The REST API in MaxScale should be implement as a HTTP based administrative interface that manages resources in MaxScale in a stateless (i.e. RESTful) manner. This allows transactional communication with MaxScale.
h2. Requirements
The API should implement the API documented in the [MaxScale REST API documentation|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md]. 
h2. Implementation details
The current resources closely follow the maxadmin output of the previous versions.

Currently, all object references, e.g. servers in services, are expressed as relative pathnames. This means that nested resources in services, filters and monitors need to be queried separately. The required linkage of the nested resources could be done inside MaxScale so that only one query is needed to fully display an object.

The layout of the resources follow the JSON API specification: http://jsonapi.org/ $acceptance criteria:$",,markus makela,markus makela,Major,16,,0,0,0,4,0,7,5,,0,850,0,5,0,2017-04-19 09:57:18,REST API,"h1. REST API for Managing MaxScale
h2. Overview
The REST API in MaxScale should be implement as a HTTP based administrative interface that manages resources in MaxScale in a stateless (i.e. RESTful) manner. This allows transactional communication with MaxScale.
h2. Requirements
The API should implement the API documented in the [MaxScale REST API documentation|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md]. 
h2. Implementation details
The current resources closely follow the maxadmin output of the previous versions.

Currently, all object references, e.g. servers in services, are expressed as relative pathnames. This means that nested resources in services, filters and monitors need to be queried separately. The required linkage of the nested resources could be done inside MaxScale so that only one query is needed to fully display an object.

Here is an example output of the {{/services/RW-Split-Router}} resource with relative paths:
{code}
{
    ""name"": ""RW-Split-Router"",
    ""router"": ""readwritesplit"",
    ""state"": ""Started"",
    ""started"": ""Mon Apr 17 20:52:40 2017"",
    ""enable_root"": false,
    ""servers"": [
        ""/servers/server1"",
        ""/servers/server2"",
        ""/servers/server3"",
        ""/servers/server4""
    ],
    ""total_connections"": 1,
    ""connections"": 1
}
{code}

The nested resources can also be expanded into actual objects with relative ease. This allows the client to retrieve more information with a smaller amound of requests:

{code}
{
    ""name"": ""RW-Split-Router"",
    ""router"": ""readwritesplit"",
    ""state"": ""Started"",
    ""started"": ""Mon Apr 17 21:05:58 2017"",
    ""enable_root"": false,
    ""listeners"": [
        {
            ""name"": ""RW-Split-Listener"",
            ""port"": 4006,
            ""protocol"": ""MySQLClient"",
            ""authenticator"": ""MySQLAuth""
        }
    ],
    ""servers"": [
        {
            ""name"": ""server1"",
            ""address"": ""127.0.0.1"",
            ""port"": 3000,
            ""status"": ""Master, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3000,
            ""master_id"": -1,
            ""replication_depth"": 0,
            ""slaves"": [
                3001,
                3002,
                3003
            ],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        },
        {
            ""name"": ""server2"",
            ""address"": ""127.0.0.1"",
            ""port"": 3001,
            ""status"": ""Slave, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3001,
            ""master_id"": 3000,
            ""replication_depth"": 1,
            ""slaves"": [],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        },
        {
            ""name"": ""server3"",
            ""address"": ""127.0.0.1"",
            ""port"": 3002,
            ""status"": ""Slave, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3002,
            ""master_id"": 3000,
            ""replication_depth"": 1,
            ""slaves"": [],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        },
        {
            ""name"": ""server4"",
            ""address"": ""127.0.0.1"",
            ""port"": 3003,
            ""status"": ""Slave, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3003,
            ""master_id"": 3000,
            ""replication_depth"": 1,
            ""slaves"": [],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        }
    ],
    ""total_connections"": 1,
    ""connections"": 1
}
{code}",,0,2,0,241,0.642857,"REST API $end$ h1. REST API for Managing MaxScale
h2. Overview
The REST API in MaxScale should be implement as a HTTP based administrative interface that manages resources in MaxScale in a stateless (i.e. RESTful) manner. This allows transactional communication with MaxScale.
h2. Requirements
The API should implement the API documented in the [MaxScale REST API documentation|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md]. 
h2. Implementation details
The current resources closely follow the maxadmin output of the previous versions.

Currently, all object references, e.g. servers in services, are expressed as relative pathnames. This means that nested resources in services, filters and monitors need to be queried separately. The required linkage of the nested resources could be done inside MaxScale so that only one query is needed to fully display an object.

Here is an example output of the {{/services/RW-Split-Router}} resource with relative paths:
{code}
{
    ""name"": ""RW-Split-Router"",
    ""router"": ""readwritesplit"",
    ""state"": ""Started"",
    ""started"": ""Mon Apr 17 20:52:40 2017"",
    ""enable_root"": false,
    ""servers"": [
        ""/servers/server1"",
        ""/servers/server2"",
        ""/servers/server3"",
        ""/servers/server4""
    ],
    ""total_connections"": 1,
    ""connections"": 1
}
{code}

The nested resources can also be expanded into actual objects with relative ease. This allows the client to retrieve more information with a smaller amound of requests:

{code}
{
    ""name"": ""RW-Split-Router"",
    ""router"": ""readwritesplit"",
    ""state"": ""Started"",
    ""started"": ""Mon Apr 17 21:05:58 2017"",
    ""enable_root"": false,
    ""listeners"": [
        {
            ""name"": ""RW-Split-Listener"",
            ""port"": 4006,
            ""protocol"": ""MySQLClient"",
            ""authenticator"": ""MySQLAuth""
        }
    ],
    ""servers"": [
        {
            ""name"": ""server1"",
            ""address"": ""127.0.0.1"",
            ""port"": 3000,
            ""status"": ""Master, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3000,
            ""master_id"": -1,
            ""replication_depth"": 0,
            ""slaves"": [
                3001,
                3002,
                3003
            ],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        },
        {
            ""name"": ""server2"",
            ""address"": ""127.0.0.1"",
            ""port"": 3001,
            ""status"": ""Slave, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3001,
            ""master_id"": 3000,
            ""replication_depth"": 1,
            ""slaves"": [],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        },
        {
            ""name"": ""server3"",
            ""address"": ""127.0.0.1"",
            ""port"": 3002,
            ""status"": ""Slave, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3002,
            ""master_id"": 3000,
            ""replication_depth"": 1,
            ""slaves"": [],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        },
        {
            ""name"": ""server4"",
            ""address"": ""127.0.0.1"",
            ""port"": 3003,
            ""status"": ""Slave, Running"",
            ""protocol"": ""MySQLBackend"",
            ""version"": ""10.1.22-MariaDB"",
            ""node_id"": 3003,
            ""master_id"": 3000,
            ""replication_depth"": 1,
            ""slaves"": [],
            ""statictics"": {
                ""connections"": 0,
                ""total_connections"": 0,
                ""active_operations"": 0
            }
        }
    ],
    ""total_connections"": 1,
    ""connections"": 1
}
{code} $acceptance criteria:$",2,1,1,1,1,1,1,128.417,15,3,0.2,3,0.2,3,0.2,3,0.2,3,0.2
931,MXS-1225,Task,MXS,2017-04-19 07:08:44,,0,Comprehensive build and debug instructions,"Comprehensive build and debug instructions that allows anyone to

* Build MaxScale using a specific tag/branch/etc. using a specific VM
* Execute all or a specific set of tests.
* Access the results from CDash
* Access the logs of tests have been run.

Basically one intra-page that provides direct instructions or links to relevant information.",,"Comprehensive build and debug instructions $end$ Comprehensive build and debug instructions that allows anyone to

* Build MaxScale using a specific tag/branch/etc. using a specific VM
* Execute all or a specific set of tests.
* Access the results from CDash
* Access the logs of tests have been run.

Basically one intra-page that provides direct instructions or links to relevant information. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,2,0,5,0,0,0,,0,850,2,0,0,2017-04-19 07:08:44,Comprehensive build and debug instructions,"Comprehensive build and debug instructions that allows anyone to

* Build MaxScale using a specific tag/branch/etc. using a specific VM
* Execute all or a specific set of tests.
* Access the results from CDash
* Access the logs of tests have been run.

Basically one intra-page that provides direct instructions or links to relevant information.",,0,0,0,0,0.0,"Comprehensive build and debug instructions $end$ Comprehensive build and debug instructions that allows anyone to

* Build MaxScale using a specific tag/branch/etc. using a specific VM
* Execute all or a specific set of tests.
* Access the results from CDash
* Access the logs of tests have been run.

Basically one intra-page that provides direct instructions or links to relevant information. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,154,19,0.123377,7,0.0454545,5,0.0324675,3,0.0194805,3,0.0194805
932,MXS-1226,Task,MXS,2017-04-19 07:24:15,,0,Comprehensive release instructions,"Provide comprehensive release instructions that allows anyone to build, package and make a release.
",,"Comprehensive release instructions $end$ Provide comprehensive release instructions that allows anyone to build, package and make a release.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,2,0,5,0,0,0,,0,850,2,0,0,2017-04-19 07:24:15,Comprehensive release instructions,"Provide comprehensive release instructions that allows anyone to build, package and make a release.
",,0,0,0,0,0.0,"Comprehensive release instructions $end$ Provide comprehensive release instructions that allows anyone to build, package and make a release.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,155,19,0.122581,7,0.0451613,5,0.0322581,3,0.0193548,3,0.0193548
933,MXS-1229,Sub-Task,MXS,2017-04-19 09:14:32,,0,Add POST/PUT/PATCH support,"h1. PUT/POST/PATCH Support
h2. Overview
The resources that support modifications need to implement the handling of modifying requests.
h2. Details
The list of resources that support modifications:

* Servers
* Monitors
* Services

The service modifications are only done on the core level. The modules need to expose their own entry points to change module internal parameters.

h2. Supported Methods
The modifications ideally support both PUT and PATCH but due to practical limitations, PUT is implemented first. PATCH will be implemented if it is deemed necessary and possible.",,"Add POST/PUT/PATCH support $end$ h1. PUT/POST/PATCH Support
h2. Overview
The resources that support modifications need to implement the handling of modifying requests.
h2. Details
The list of resources that support modifications:

* Servers
* Monitors
* Services

The service modifications are only done on the core level. The modules need to expose their own entry points to change module internal parameters.

h2. Supported Methods
The modifications ideally support both PUT and PATCH but due to practical limitations, PUT is implemented first. PATCH will be implemented if it is deemed necessary and possible. $acceptance criteria:$",,markus makela,markus makela,Major,8,,0,1,0,4,0,5,0,,0,850,1,0,0,2017-04-19 09:14:32,Add POST/PUT/PATCH support,"h1. PUT/POST/PATCH Support
h2. Overview
The resources that support modifications need to implement the handling of modifying requests.
h2. Details
The list of resources that support modifications:
TODO: Add this list",,0,5,0,65,1.64865,"Add POST/PUT/PATCH support $end$ h1. PUT/POST/PATCH Support
h2. Overview
The resources that support modifications need to implement the handling of modifying requests.
h2. Details
The list of resources that support modifications:
TODO: Add this list $acceptance criteria:$",5,1,1,1,1,1,1,0.0,16,4,0.25,4,0.25,4,0.25,4,0.25,4,0.25
934,MXS-1230,Task,MXS,2017-04-19 09:40:54,,0,simplified test framework,"for majority of test one single backend server can be enough

such tests should be included into Maxscale source and be executable by 'make test'",,"simplified test framework $end$ for majority of test one single backend server can be enough

such tests should be included into Maxscale source and be executable by 'make test' $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,4,,0,850,0,0,0,2017-05-18 09:34:51,simplified test framework,"for majority of test one single backend server can be enough

such tests should be included into Maxscale source and be executable by 'make test'",,0,0,0,0,0.0,"simplified test framework $end$ for majority of test one single backend server can be enough

such tests should be included into Maxscale source and be executable by 'make test' $acceptance criteria:$",0,0,0,0,0,0,0,695.883,35,1,0.0285714,0,0.0,0,0.0,0,0.0,0,0.0
935,MXS-1231,Sub-Task,MXS,2017-04-19 09:44:13,,0,define test setup,"define test configuration: backend, way to connect to backend, Maxscale configuration(s), way to configure Maxscale for test, way to describe backend and send this info to test script/application

description should be in the form of some readme inside Maxscale source",,"define test setup $end$ define test configuration: backend, way to connect to backend, Maxscale configuration(s), way to configure Maxscale for test, way to describe backend and send this info to test script/application

description should be in the form of some readme inside Maxscale source $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,4,,0,3,0,1,0,0,0,,0,850,3,0,0,2017-04-19 09:44:13,define test setup,"define test configuration: backend, way to connect to backend, Maxscale configuration(s), way to configure Maxscale for test, way to describe backend and send this info to test script/application

description should be in the form of some readme inside Maxscale source",,0,0,0,0,0.0,"define test setup $end$ define test configuration: backend, way to connect to backend, Maxscale configuration(s), way to configure Maxscale for test, way to describe backend and send this info to test script/application

description should be in the form of some readme inside Maxscale source $acceptance criteria:$",0,0,0,0,0,0,0,0.0,36,1,0.0277778,0,0.0,0,0.0,0,0.0,0,0.0
936,MXS-1232,Sub-Task,MXS,2017-04-19 09:45:23,,0,simplified test framework bring up script,this script should be a part of 'make test',,simplified test framework bring up script $end$ this script should be a part of 'make test' $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,2,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-04-19 09:45:23,simplified test framework bring up script,this script should be a part of 'make test',,0,0,0,0,0.0,simplified test framework bring up script $end$ this script should be a part of 'make test' $acceptance criteria:$,0,0,0,0,0,0,0,0.0,37,1,0.027027,0,0.0,0,0.0,0,0.0,0,0.0
937,MXS-1233,Sub-Task,MXS,2017-04-19 09:52:06,,0,simplified test framework backend check and clean up script,"backend can be broken after the test, we need to check backend before next test",,"simplified test framework backend check and clean up script $end$ backend can be broken after the test, we need to check backend before next test $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,2,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-04-19 09:52:06,simplified test framework backend check and clean up script,"backend can be broken after the test, we need to check backend before next test",,0,0,0,0,0.0,"simplified test framework backend check and clean up script $end$ backend can be broken after the test, we need to check backend before next test $acceptance criteria:$",0,0,0,0,0,0,0,0.0,38,1,0.0263158,0,0.0,0,0.0,0,0.0,0,0.0
938,MXS-1234,Task,MXS,2017-04-19 09:57:59,,0,2.1.3 Release Preparations,,,2.1.3 Release Preparations $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,3,0,0,5,,0,850,0,0,0,2017-04-19 09:57:59,2.1.3 Release Preparations,,,0,0,0,0,0.0,2.1.3 Release Preparations $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,156,19,0.121795,7,0.0448718,5,0.0320513,3,0.0192308,3,0.0192308
939,MXS-1236,Sub-Task,MXS,2017-04-19 10:01:06,,0,Add tests for REST API,"h1. Create tests for the REST API
The REST API needs a set of tests. The tests should validate all resources and check that all of the RPC commands work as expected.
h2. Test details
Test all resources described in [this document|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md].
 ",,"Add tests for REST API $end$ h1. Create tests for the REST API
The REST API needs a set of tests. The tests should validate all resources and check that all of the RPC commands work as expected.
h2. Test details
Test all resources described in [this document|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md].
  $acceptance criteria:$",,markus makela,markus makela,Major,3,,0,1,0,4,0,0,0,,0,850,1,0,0,2017-04-19 10:01:06,Add tests for REST API,"h1. Create tests for the REST API
The REST API needs a set of tests. The tests should validate all resources and check that all of the RPC commands work as expected.
h2. Test details
Test all resources described in [this document|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md].
 ",,0,0,0,0,0.0,"Add tests for REST API $end$ h1. Create tests for the REST API
The REST API needs a set of tests. The tests should validate all resources and check that all of the RPC commands work as expected.
h2. Test details
Test all resources described in [this document|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/REST-API/API.md].
  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,17,5,0.294118,5,0.294118,5,0.294118,5,0.294118,5,0.294118
940,MXS-1237,Task,MXS,2017-04-19 10:23:32,,0,Check moving of tutorials to KB,,,Check moving of tutorials to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-04-19 10:23:46,Check moving of tutorials to KB,,,0,0,0,0,0.0,Check moving of tutorials to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,157,19,0.121019,7,0.044586,5,0.0318471,3,0.0191083,3,0.0191083
941,MXS-1238,Sub-Task,MXS,2017-04-20 07:02:03,,0,2.1.3 Release Notes,,,2.1.3 Release Notes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-04-20 07:02:03,2.1.3 Release Notes,,,0,0,0,0,0.0,2.1.3 Release Notes $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,158,19,0.120253,7,0.0443038,5,0.0316456,3,0.0189873,3,0.0189873
942,MXS-1239,Sub-Task,MXS,2017-04-20 07:02:14,,0,2.1.3 ChangeLog,,,2.1.3 ChangeLog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-04-20 07:02:14,2.1.3 ChangeLog,,,0,0,0,0,0.0,2.1.3 ChangeLog $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,159,19,0.119497,7,0.0440252,5,0.0314465,3,0.0188679,3,0.0188679
943,MXS-1240,Sub-Task,MXS,2017-04-20 07:02:43,,0,2.1.3 Upgrading to 2.1,Update 2.0 to 2.1 upgrading document with new changes.,,2.1.3 Upgrading to 2.1 $end$ Update 2.0 to 2.1 upgrading document with new changes. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-04-20 07:02:43,2.1.3 Upgrading to 2.1,Update 2.0 to 2.1 upgrading document with new changes.,,0,0,0,0,0.0,2.1.3 Upgrading to 2.1 $end$ Update 2.0 to 2.1 upgrading document with new changes. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,160,19,0.11875,7,0.04375,5,0.03125,3,0.01875,3,0.01875
944,MXS-1241,Sub-Task,MXS,2017-04-20 07:03:17,,0,2.1.3 Create 2.1.3 branch and final tag.,,,2.1.3 Create 2.1.3 branch and final tag. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-04-20 07:03:17,2.1.3 Create 2.1.3 branch and final tag.,,,0,0,0,0,0.0,2.1.3 Create 2.1.3 branch and final tag. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,161,19,0.118012,7,0.0434783,5,0.0310559,3,0.0186335,3,0.0186335
945,MXS-1242,Sub-Task,MXS,2017-04-20 07:03:34,,0,2.1.3 Sync documentation to KB,,,2.1.3 Sync documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-04-20 07:03:34,2.1.3 Sync documentation to KB,,,0,0,0,0,0.0,2.1.3 Sync documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,162,19,0.117284,7,0.0432099,5,0.0308642,3,0.0185185,3,0.0185185
946,MXS-1245,New Feature,MXS,2017-04-24 05:42:31,,0,Support statement pipelining,"MXS-1203 fixed the batched statement execution bug. The fix does not allow parallel execution of the batched statements which removes all of the benefits of batched statement execution.

A more correct implementation of this would record the order of the executed statements and return the responses in the correct order.",,"Support statement pipelining $end$ MXS-1203 fixed the batched statement execution bug. The fix does not allow parallel execution of the batched statements which removes all of the benefits of batched statement execution.

A more correct implementation of this would record the order of the executed statements and return the responses in the correct order. $acceptance criteria:$",,markus makela,markus makela,Major,23,,1,1,2,1,0,0,0,,0,850,0,0,0,2020-12-08 07:44:36,Support statement pipelining,"MXS-1203 fixed the batched statement execution bug. The fix does not allow parallel execution of the batched statements which removes all of the benefits of batched statement execution.

A more correct implementation of this would record the order of the executed statements and return the responses in the correct order.",,0,0,0,0,0.0,"Support statement pipelining $end$ MXS-1203 fixed the batched statement execution bug. The fix does not allow parallel execution of the batched statements which removes all of the benefits of batched statement execution.

A more correct implementation of this would record the order of the executed statements and return the responses in the correct order. $acceptance criteria:$",0,0,0,0,0,0,0,31778.0,18,5,0.277778,5,0.277778,5,0.277778,5,0.277778,5,0.277778
947,MXS-1247,Task,MXS,2017-04-24 06:17:20,MXS-1246,0,Parse SELECTs using window functions,The language modifications introduced by window functions need to be recognized so that SELECTs using window functions will be completely parsed.,,Parse SELECTs using window functions $end$ The language modifications introduced by window functions need to be recognized so that SELECTs using window functions will be completely parsed. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-07-27 08:43:14,Parse SELECTs using window functions,The language modifications introduced by window functions need to be recognized so that SELECTs using window functions will be completely parsed.,,0,0,0,0,0.0,Parse SELECTs using window functions $end$ The language modifications introduced by window functions need to be recognized so that SELECTs using window functions will be completely parsed. $acceptance criteria:$,0,0,0,0,0,0,1,2258.42,163,19,0.116564,7,0.0429448,5,0.0306748,3,0.0184049,3,0.0184049
948,MXS-1248,Task,MXS,2017-04-24 06:20:32,MXS-1246,0,Report fields used in WITH clause,"Given a statement like
{{code}}
WITH t AS (SELECT a FROM t1 WHERE b >= 'c')
SELECT * FROM t2,t WHERE t2.c=t.a;
{{code}}
the current query classifier will not report fields used in the {{WITH}} clause, e.g. {{b}}.

Consequently, the database firewall filter will not block a statement using a forbidden field in the {{SELECT}} of the {{WITH}} clause.",,"Report fields used in WITH clause $end$ Given a statement like
{{code}}
WITH t AS (SELECT a FROM t1 WHERE b >= 'c')
SELECT * FROM t2,t WHERE t2.c=t.a;
{{code}}
the current query classifier will not report fields used in the {{WITH}} clause, e.g. {{b}}.

Consequently, the database firewall filter will not block a statement using a forbidden field in the {{SELECT}} of the {{WITH}} clause. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,0,0,0,1,0,1,0,,0,850,0,1,0,2017-06-28 08:23:20,Report fields used in WITH clause,"Given a statement like
{{code}}
WITH t AS (SELECT a FROM t1 WHERE b >= 'c')
SELECT * FROM t2,t WHERE t2.c=t.a;
{{code}}
the current query classifier will not report fields used in the {{WITH}} clause, e.g. {{b}}.

Consequently, the database firewall filter will not block a statement using a forbidden field in the {{SELECT}} of the {{WITH}} clause.",,0,0,0,0,0.0,"Report fields used in WITH clause $end$ Given a statement like
{{code}}
WITH t AS (SELECT a FROM t1 WHERE b >= 'c')
SELECT * FROM t2,t WHERE t2.c=t.a;
{{code}}
the current query classifier will not report fields used in the {{WITH}} clause, e.g. {{b}}.

Consequently, the database firewall filter will not block a statement using a forbidden field in the {{SELECT}} of the {{WITH}} clause. $acceptance criteria:$",0,0,0,0,0,0,0,1562.03,164,19,0.115854,7,0.0426829,5,0.0304878,3,0.0182927,3,0.0182927
949,MXS-1249,Task,MXS,2017-04-24 06:22:23,MXS-1246,0,Recognize read-only JSON functions,The _readonly_ JSON functions should be recognized so that statements using such functions can be sent to slaves.,,Recognize read-only JSON functions $end$ The _readonly_ JSON functions should be recognized so that statements using such functions can be sent to slaves. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-06-14 09:25:48,Recognize read-only JSON functions,The _readonly_ JSON functions should be recognized so that statements using such functions can be sent to slaves.,,0,0,0,0,0.0,Recognize read-only JSON functions $end$ The _readonly_ JSON functions should be recognized so that statements using such functions can be sent to slaves. $acceptance criteria:$,0,0,0,0,0,0,0,1227.05,165,19,0.115152,7,0.0424242,5,0.030303,3,0.0181818,3,0.0181818
950,MXS-1252,Sub-Task,MXS,2017-04-28 06:19:14,,0,Implement core diagnostic output,The MaxScale core should expose the global configuration parameters and any diagnostic information that is already exposed by MaxAdmin. This task is the implementation of the [{{/maxscale}}|https://github.com/mariadb-corporation/MaxScale/blob/MXS-1220/Documentation/REST-API/Resources-MaxScale.md] resource.,,Implement core diagnostic output $end$ The MaxScale core should expose the global configuration parameters and any diagnostic information that is already exposed by MaxAdmin. This task is the implementation of the [{{/maxscale}}|https://github.com/mariadb-corporation/MaxScale/blob/MXS-1220/Documentation/REST-API/Resources-MaxScale.md] resource. $acceptance criteria:$,,markus makela,markus makela,Major,2,,0,1,0,4,0,0,0,,0,850,1,0,0,2017-04-28 06:19:14,Implement core diagnostic output,The MaxScale core should expose the global configuration parameters and any diagnostic information that is already exposed by MaxAdmin. This task is the implementation of the [{{/maxscale}}|https://github.com/mariadb-corporation/MaxScale/blob/MXS-1220/Documentation/REST-API/Resources-MaxScale.md] resource.,,0,0,0,0,0.0,Implement core diagnostic output $end$ The MaxScale core should expose the global configuration parameters and any diagnostic information that is already exposed by MaxAdmin. This task is the implementation of the [{{/maxscale}}|https://github.com/mariadb-corporation/MaxScale/blob/MXS-1220/Documentation/REST-API/Resources-MaxScale.md] resource. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,19,5,0.263158,5,0.263158,5,0.263158,5,0.263158,5,0.263158
951,MXS-1253,Sub-Task,MXS,2017-04-28 07:28:43,,0,Update REST API resource documentation,The documented resources and their output are in conflict. The documentation needs to be updated to reflect the actual resources.,,Update REST API resource documentation $end$ The documented resources and their output are in conflict. The documentation needs to be updated to reflect the actual resources. $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2017-04-28 07:28:43,Update REST API resource documentation,The documented resources and their output are in conflict. The documentation needs to be updated to reflect the actual resources.,,0,0,0,0,0.0,Update REST API resource documentation $end$ The documented resources and their output are in conflict. The documentation needs to be updated to reflect the actual resources. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,20,5,0.25,5,0.25,5,0.25,5,0.25,5,0.25
952,MXS-1257,Task,MXS,2017-05-03 06:32:40,MXS-2246,0,Porting maxRows filter to CPP,"MaxRows filter is written in C.

This task is about porting the code to C++ as other MaxScale filters",,"Porting maxRows filter to CPP $end$ MaxRows filter is written in C.

This task is about porting the code to C++ as other MaxScale filters $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Minor,20,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-05-03 09:30:28,Porting maxRows filter to CPP,"MaxRows filter is written in C.

This task is about porting the code to C++ as other MaxScale filters",,0,0,0,0,0.0,"Porting maxRows filter to CPP $end$ MaxRows filter is written in C.

This task is about porting the code to C++ as other MaxScale filters $acceptance criteria:$",0,0,0,0,0,0,0,2.95,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
953,MXS-1258,Sub-Task,MXS,2017-05-04 07:18:30,,0,Ensure qc test cases pass when 10.3 is used.,,,Ensure qc test cases pass when 10.3 is used. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-05-04 07:18:30,Ensure qc test cases pass when 10.3 is used.,,,0,0,0,0,0.0,Ensure qc test cases pass when 10.3 is used. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,166,19,0.114458,7,0.0421687,5,0.0301205,3,0.0180723,3,0.0180723
954,MXS-1261,Task,MXS,2017-05-08 11:04:13,,0,"""Fill"" should have a default value.","The _fill_ value of the masking filter should have a default value to ensure that a column that should be masked always is, even if the configuration is not quite correct.
",,"""Fill"" should have a default value. $end$ The _fill_ value of the masking filter should have a default value to ensure that a column that should be masked always is, even if the configuration is not quite correct.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-05-31 09:44:17,"""Fill"" should have a default value.","The _fill_ value of the masking filter should have a default value to ensure that a column that should be masked always is, even if the configuration is not quite correct.
",,0,0,0,0,0.0,"""Fill"" should have a default value. $end$ The _fill_ value of the masking filter should have a default value to ensure that a column that should be masked always is, even if the configuration is not quite correct.
 $acceptance criteria:$",0,0,0,0,0,0,0,550.667,167,19,0.113772,7,0.0419162,5,0.0299401,3,0.0179641,3,0.0179641
955,MXS-1266,New Feature,MXS,2017-05-16 10:30:49,,0,Binlog cache hierarchy for MariaDB GTID replication,"Enable a binlog cache hierarchy with replication domain, server_id and filename in order to fully replicate from MariaDB 10 Master(s) using GTID.

This will eliminate the need of issuing FLUSH LOGS or RESET MASTER TO  in the new master.",,"Binlog cache hierarchy for MariaDB GTID replication $end$ Enable a binlog cache hierarchy with replication domain, server_id and filename in order to fully replicate from MariaDB 10 Master(s) using GTID.

This will eliminate the need of issuing FLUSH LOGS or RESET MASTER TO  in the new master. $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,18,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-05-18 09:34:32,Binlog cache hierarchy for MariaDB GTID replication,"Enable a binlog cache hierarchy with replication domain, server_id and filename in order to fully replicate from MariaDB 10 Master(s) using GTID.

This will eliminate the need of issuing FLUSH LOGS or RESET MASTER TO  in the new master.",,0,0,0,0,0.0,"Binlog cache hierarchy for MariaDB GTID replication $end$ Enable a binlog cache hierarchy with replication domain, server_id and filename in order to fully replicate from MariaDB 10 Master(s) using GTID.

This will eliminate the need of issuing FLUSH LOGS or RESET MASTER TO  in the new master. $acceptance criteria:$",0,0,0,0,0,0,1,47.05,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
956,MXS-1267,New Feature,MXS,2017-05-18 09:36:59,,0,Refactor tee filter,The tee filter breaks a lot of the core rules in MaxScale. The filter should be refactored so that it uses a virtual client that  asynchronously sends the queries to the backends.,,Refactor tee filter $end$ The tee filter breaks a lot of the core rules in MaxScale. The filter should be refactored so that it uses a virtual client that  asynchronously sends the queries to the backends. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-05-18 09:37:53,Refactor tee filter,The tee filter breaks a lot of the core rules in MaxScale. The filter should be refactored so that it uses a virtual client that  asynchronously sends the queries to the backends.,,0,0,0,0,0.0,Refactor tee filter $end$ The tee filter breaks a lot of the core rules in MaxScale. The filter should be refactored so that it uses a virtual client that  asynchronously sends the queries to the backends. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,21,5,0.238095,5,0.238095,5,0.238095,5,0.238095,5,0.238095
957,MXS-1268,Task,MXS,2017-05-18 09:48:47,,0,Add support for Proxy Protocol to Connector-C,,,Add support for Proxy Protocol to Connector-C $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-05-18 09:48:47,Add support for Proxy Protocol to Connector-C,,,0,0,0,0,0.0,Add support for Proxy Protocol to Connector-C $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,168,19,0.113095,7,0.0416667,5,0.0297619,3,0.0178571,3,0.0178571
958,MXS-1274,Technical task,MXS,2017-05-24 14:32:01,,0,Ensure PL/SQL specific test cases pass.,,,Ensure PL/SQL specific test cases pass. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-05-24 14:32:01,Ensure PL/SQL specific test cases pass.,,,0,0,0,0,0.0,Ensure PL/SQL specific test cases pass. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,169,19,0.112426,7,0.0414201,5,0.0295858,3,0.0177515,3,0.0177515
959,MXS-1275,Task,MXS,2017-05-24 15:41:29,,0,"Recognize ""set SQL_MODE=ORACLE"" statements","When
{code}
set sql_mode=ORACLE
{code}
is encountered, PL/SQL mode should be turned on.
",,"Recognize ""set SQL_MODE=ORACLE"" statements $end$ When
{code}
set sql_mode=ORACLE
{code}
is encountered, PL/SQL mode should be turned on.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,1,0,,0,850,0,1,0,2017-05-31 08:57:16,"Recognize ""set SQL_MODE=ORACLE"" statements","When
{code}
set sql_mode=ORACLE
{code}
is encountered, PL/SQL mode should be turned on.
",,0,0,0,0,0.0,"Recognize ""set SQL_MODE=ORACLE"" statements $end$ When
{code}
set sql_mode=ORACLE
{code}
is encountered, PL/SQL mode should be turned on.
 $acceptance criteria:$",0,0,0,0,0,0,0,161.25,170,19,0.111765,7,0.0411765,5,0.0294118,3,0.0176471,3,0.0176471
960,MXS-1278,Task,MXS,2017-05-31 08:55:52,,0,Turn on PL/SQL dynamically.,,,Turn on PL/SQL dynamically. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,2,0,1,0,0,0,,0,850,2,0,0,2017-05-31 08:57:13,Turn on PL/SQL dynamically.,,,0,0,0,0,0.0,Turn on PL/SQL dynamically. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0166667,171,19,0.111111,7,0.0409357,5,0.0292398,3,0.0175439,3,0.0175439
961,MXS-1282,Task,MXS,2017-06-08 06:09:47,,0,Add generic debug flag to MaxScale,"For debugging purposes you may want to turn on or off certain behaviour.

Add handling for a generic {{--debug=arg1,arg2,arg3}} startup flag using which that can be managed.

Initially support for {{--debug=disable-module-unloading}}, using which the unloading of modules at process shutdown can be disabled, should be added.

The arguments given to the {{--debug}} flag for enabling or disabling certain behaviour will always be of the format {{enable-X}} and {{disable-X}}.",,"Add generic debug flag to MaxScale $end$ For debugging purposes you may want to turn on or off certain behaviour.

Add handling for a generic {{--debug=arg1,arg2,arg3}} startup flag using which that can be managed.

Initially support for {{--debug=disable-module-unloading}}, using which the unloading of modules at process shutdown can be disabled, should be added.

The arguments given to the {{--debug}} flag for enabling or disabling certain behaviour will always be of the format {{enable-X}} and {{disable-X}}. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-06-08 07:23:43,Add generic debug flag to MaxScale,"For debugging purposes you may want to turn on or off certain behaviour.

Add handling for a generic {{--debug=arg1,arg2,arg3}} startup flag using which that can be managed.

Initially support for {{--debug=disable-module-unloading}}, using which the unloading of modules at process shutdown can be disabled, should be added.

The arguments given to the {{--debug}} flag for enabling or disabling certain behaviour will always be of the format {{enable-X}} and {{disable-X}}.",,0,0,0,0,0.0,"Add generic debug flag to MaxScale $end$ For debugging purposes you may want to turn on or off certain behaviour.

Add handling for a generic {{--debug=arg1,arg2,arg3}} startup flag using which that can be managed.

Initially support for {{--debug=disable-module-unloading}}, using which the unloading of modules at process shutdown can be disabled, should be added.

The arguments given to the {{--debug}} flag for enabling or disabling certain behaviour will always be of the format {{enable-X}} and {{disable-X}}. $acceptance criteria:$",0,0,0,0,0,0,1,1.21667,172,19,0.110465,7,0.0406977,5,0.0290698,3,0.0174419,3,0.0174419
962,MXS-1286,Task,MXS,2017-06-14 09:26:00,MXS-2681,0,Refactor gateway.cc,,,Refactor gateway.cc $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,27,,0,0,0,6,0,0,0,,0,850,0,0,0,2017-06-14 09:26:00,Refactor gateway.cc,,,0,0,0,0,0.0,Refactor gateway.cc $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,173,19,0.109827,7,0.0404624,5,0.0289017,3,0.017341,3,0.017341
963,MXS-1300,Task,MXS,2017-06-28 07:53:48,,0,Implement MaxCtrl,"Implement MaxCtrl - the MaxScale administration tool that uses the new REST-API.

* Everything that is available and modifiable via the current _maxadmin_ should be available and modifiable via _maxctrl_.
* _Maxctrl_ must be able to connect to more MaxScale instances than one.
* _Maxctlr_ must be able to make modifications simultaneously to several MaxScale instances at once (some measures should be taken to reduce the risk that modifications inadvertently are made when one of the MaxScale instances is down or unreachable, e.g. first ping both before actually performing the command).
* _Maxctrl_ must be able to visualize the data from several MaxScale instances in a manner that makes it straightforward for a user to spot discrepancies.
* Even if _Maxctrl_ is connected to multiple MaxScale instances it must be possible to explicitly make modifications to a specific instance (e.g. to fix noticed discrepancies).

",,"Implement MaxCtrl $end$ Implement MaxCtrl - the MaxScale administration tool that uses the new REST-API.

* Everything that is available and modifiable via the current _maxadmin_ should be available and modifiable via _maxctrl_.
* _Maxctrl_ must be able to connect to more MaxScale instances than one.
* _Maxctlr_ must be able to make modifications simultaneously to several MaxScale instances at once (some measures should be taken to reduce the risk that modifications inadvertently are made when one of the MaxScale instances is down or unreachable, e.g. first ping both before actually performing the command).
* _Maxctrl_ must be able to visualize the data from several MaxScale instances in a manner that makes it straightforward for a user to spot discrepancies.
* Even if _Maxctrl_ is connected to multiple MaxScale instances it must be possible to explicitly make modifications to a specific instance (e.g. to fix noticed discrepancies).

 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-06-28 07:53:48,Implement MaxCtrl,"Implement MaxCtrl - the MaxScale administration tool that uses the new REST-API.

* Everything that is available and modifiable via the current _maxadmin_ should be available and modifiable via _maxctrl_.
* _Maxctrl_ must be able to connect to more MaxScale instances than one.
* _Maxctlr_ must be able to make modifications simultaneously to several MaxScale instances at once (some measures should be taken to reduce the risk that modifications inadvertently are made when one of the MaxScale instances is down or unreachable, e.g. first ping both before actually performing the command).
* _Maxctrl_ must be able to visualize the data from several MaxScale instances in a manner that makes it straightforward for a user to spot discrepancies.
* Even if _Maxctrl_ is connected to multiple MaxScale instances it must be possible to explicitly make modifications to a specific instance (e.g. to fix noticed discrepancies).

",,0,0,0,0,0.0,"Implement MaxCtrl $end$ Implement MaxCtrl - the MaxScale administration tool that uses the new REST-API.

* Everything that is available and modifiable via the current _maxadmin_ should be available and modifiable via _maxctrl_.
* _Maxctrl_ must be able to connect to more MaxScale instances than one.
* _Maxctlr_ must be able to make modifications simultaneously to several MaxScale instances at once (some measures should be taken to reduce the risk that modifications inadvertently are made when one of the MaxScale instances is down or unreachable, e.g. first ping both before actually performing the command).
* _Maxctrl_ must be able to visualize the data from several MaxScale instances in a manner that makes it straightforward for a user to spot discrepancies.
* Even if _Maxctrl_ is connected to multiple MaxScale instances it must be possible to explicitly make modifications to a specific instance (e.g. to fix noticed discrepancies).

 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,174,19,0.109195,7,0.0402299,5,0.0287356,3,0.0172414,3,0.0172414
964,MXS-1301,Task,MXS,2017-06-28 08:03:54,,0,Whitelisting of functions using dbfwfilter.,"It is possible to bypass the masking performed by the masking filter by using a function.

E.g.
{code}
> select a from tbl;
+-------+
| a     |
+-------+
| XXXXX |
+-------+
> select concat(a) from tbl;
+-----------+
| concat(a) |
+-----------+
| hello     |
+-----------+
{code}
This can be prevented using the firewall filter, but that requires you to explicitly specify each and every function, which is quite unpractical.

With the firewall filter, it should be possible to easily
* block all functions, and
* whitelist specific functions.",,"Whitelisting of functions using dbfwfilter. $end$ It is possible to bypass the masking performed by the masking filter by using a function.

E.g.
{code}
> select a from tbl;
+-------+
| a     |
+-------+
| XXXXX |
+-------+
> select concat(a) from tbl;
+-----------+
| concat(a) |
+-----------+
| hello     |
+-----------+
{code}
This can be prevented using the firewall filter, but that requires you to explicitly specify each and every function, which is quite unpractical.

With the firewall filter, it should be possible to easily
* block all functions, and
* whitelist specific functions. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,4,0,1,0,0,0,,0,850,4,0,0,2017-06-28 08:03:54,Whitelisting of functions using dbfwfilter.,"It is possible to bypass the masking performed by the masking filter by using a function.

E.g.
{code}
> select a from tbl;
+-------+
| a     |
+-------+
| XXXXX |
+-------+
> select concat(a) from tbl;
+-----------+
| concat(a) |
+-----------+
| hello     |
+-----------+
{code}
This can be prevented using the firewall filter, but that requires you to explicitly specify each and every function, which is quite unpractical.

With the firewall filter, it should be possible to easily
* block all functions, and
* whitelist specific functions.",,0,0,0,0,0.0,"Whitelisting of functions using dbfwfilter. $end$ It is possible to bypass the masking performed by the masking filter by using a function.

E.g.
{code}
> select a from tbl;
+-------+
| a     |
+-------+
| XXXXX |
+-------+
> select concat(a) from tbl;
+-----------+
| concat(a) |
+-----------+
| hello     |
+-----------+
{code}
This can be prevented using the firewall filter, but that requires you to explicitly specify each and every function, which is quite unpractical.

With the firewall filter, it should be possible to easily
* block all functions, and
* whitelist specific functions. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,175,19,0.108571,7,0.04,5,0.0285714,3,0.0171429,3,0.0171429
965,MXS-1302,Task,MXS,2017-06-28 08:14:36,,0,Masking Extensions,The masking filter should be extended to support partial masking and obfuscation.,,Masking Extensions $end$ The masking filter should be extended to support partial masking and obfuscation. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,2,0,1,2,,0,850,0,0,0,2017-06-28 08:14:36,Mask by RegExp,The masking filter should be extended to support partial masking and obfuscation.,,1,0,0,5,0.166667,Mask by RegExp $end$ The masking filter should be extended to support partial masking and obfuscation. $acceptance criteria:$,1,1,1,0,0,0,1,0.0,176,19,0.107955,7,0.0397727,5,0.0284091,3,0.0170455,3,0.0170455
966,MXS-1303,Task,MXS,2017-06-28 08:19:49,,0,Test proxy protocol as soon as 10.3 alpha is available.,,,Test proxy protocol as soon as 10.3 alpha is available. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-06-28 08:19:49,Test proxy protocol as soon as 10.3 alpha is available.,,,0,0,0,0,0.0,Test proxy protocol as soon as 10.3 alpha is available. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,177,20,0.112994,8,0.0451977,5,0.0282486,3,0.0169492,3,0.0169492
967,MXS-1305,Sub-Task,MXS,2017-06-29 08:45:47,,0,Partial masking,"It should be possible to replace just one part of a value.

As an example, so that a (Finnish) social security number such as {{061173-174A}} is replaced with {{061173-XXXX}}.

This could be done, _for instance_, so that the rule syntax is extended with a {{capture}} clause
{code}
{
    ""rules"": [
        {
            ""replace"": {
                ""column"": ""ssn""
                ""match"": "".*-(.*)""
            },
            ""with"": {
                ""fill"": ""X""
            }
    ...
{code}
and then only the captured part would be replaced with the fill character.

Note that the above is only for illustrative purposes, and the actual way for specifying what is captured and how it is replaced remains to be defined during the implementation.


""capture"" keyword has been changed to ""match"" in the JSON

Partial masking ""match"" option is an optional parameter in the ""replace"" rule.

If present it takes precedence and the filling is done in the matched string only: see comments below.
",,"Partial masking $end$ It should be possible to replace just one part of a value.

As an example, so that a (Finnish) social security number such as {{061173-174A}} is replaced with {{061173-XXXX}}.

This could be done, _for instance_, so that the rule syntax is extended with a {{capture}} clause
{code}
{
    ""rules"": [
        {
            ""replace"": {
                ""column"": ""ssn""
                ""match"": "".*-(.*)""
            },
            ""with"": {
                ""fill"": ""X""
            }
    ...
{code}
and then only the captured part would be replaced with the fill character.

Note that the above is only for illustrative purposes, and the actual way for specifying what is captured and how it is replaced remains to be defined during the implementation.


""capture"" keyword has been changed to ""match"" in the JSON

Partial masking ""match"" option is an optional parameter in the ""replace"" rule.

If present it takes precedence and the filling is done in the matched string only: see comments below.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,3,0,2,0,1,0,,0,850,3,0,0,2017-06-29 08:45:47,Partial masking,"It should be possible to replace just one part of a value.

As an example, so that a (Finnish) social security number such as {{061173-174A}} is replaced with {{061173-XXXX}}.

This could be done, _for instance_, so that the rule syntax is extended with a {{capture}} clause
{code}
{
    ""rules"": [
        {
            ""replace"": {
                ""column"": ""ssn""
                ""capture"": "".*-(.*)""
            },
            ""with"": {
                ""fill"": ""X""
            }
    ...
{code}
and then only the captured part would be replaced with the fill character.

Note that the above is only for illustrative purposes, and the actual way for specifying what is captured and how it is replaced remains to be defined during the implementation.
",,0,1,0,42,0.362832,"Partial masking $end$ It should be possible to replace just one part of a value.

As an example, so that a (Finnish) social security number such as {{061173-174A}} is replaced with {{061173-XXXX}}.

This could be done, _for instance_, so that the rule syntax is extended with a {{capture}} clause
{code}
{
    ""rules"": [
        {
            ""replace"": {
                ""column"": ""ssn""
                ""capture"": "".*-(.*)""
            },
            ""with"": {
                ""fill"": ""X""
            }
    ...
{code}
and then only the captured part would be replaced with the fill character.

Note that the above is only for illustrative purposes, and the actual way for specifying what is captured and how it is replaced remains to be defined during the implementation.
 $acceptance criteria:$",1,1,1,1,1,1,1,0.0,178,20,0.11236,8,0.0449438,5,0.0280899,3,0.0168539,3,0.0168539
968,MXS-1306,Sub-Task,MXS,2017-06-29 08:57:06,,0,Obfuscation,"It should be possible to use the masking filter for obfuscating values. That is, o consistently replace a certain value with something else.

For instance:
{code}
Bob   -> Dfg
Cecil -> Ghkrd
John  -> Klot
Bob   -> Dfg
John  -> Klot
...
{code}
The two critical properties are:
* A particular string should always be obfuscated in the same way.
* The obfuscation method should be such that it should not be trivial to reach to the original value from the obfuscated value (not e.g. ROT13).

In the masking configuration file this could be represented e.g. as.
{code}
{
    ""rules"": [
        {
            ""obfuscate"": {
                ""column"": ""ssn""
            },
            ""applies_to"": [ ... ],
            ""exempted"": [ ... ]
        },
    ...
{code}
but note that the above is only for illustrative purposes. The actual way for expressing what should be obfuscated and how is to be defined during the development.",,"Obfuscation $end$ It should be possible to use the masking filter for obfuscating values. That is, o consistently replace a certain value with something else.

For instance:
{code}
Bob   -> Dfg
Cecil -> Ghkrd
John  -> Klot
Bob   -> Dfg
John  -> Klot
...
{code}
The two critical properties are:
* A particular string should always be obfuscated in the same way.
* The obfuscation method should be such that it should not be trivial to reach to the original value from the obfuscated value (not e.g. ROT13).

In the masking configuration file this could be represented e.g. as.
{code}
{
    ""rules"": [
        {
            ""obfuscate"": {
                ""column"": ""ssn""
            },
            ""applies_to"": [ ... ],
            ""exempted"": [ ... ]
        },
    ...
{code}
but note that the above is only for illustrative purposes. The actual way for expressing what should be obfuscated and how is to be defined during the development. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,5,0,2,0,0,0,,0,850,5,0,0,2017-06-29 08:57:06,Obfuscation,"It should be possible to use the masking filter for obfuscating values. That is, o consistently replace a certain value with something else.

For instance:
{code}
Bob   -> Dfg
Cecil -> Ghkrd
John  -> Klot
Bob   -> Dfg
John  -> Klot
...
{code}
The two critical properties are:
* A particular string should always be obfuscated in the same way.
* The obfuscation method should be such that it should not be trivial to reach to the original value from the obfuscated value (not e.g. ROT13).

In the masking configuration file this could be represented e.g. as.
{code}
{
    ""rules"": [
        {
            ""obfuscate"": {
                ""column"": ""ssn""
            },
            ""applies_to"": [ ... ],
            ""exempted"": [ ... ]
        },
    ...
{code}
but note that the above is only for illustrative purposes. The actual way for expressing what should be obfuscated and how is to be defined during the development.",,0,0,0,0,0.0,"Obfuscation $end$ It should be possible to use the masking filter for obfuscating values. That is, o consistently replace a certain value with something else.

For instance:
{code}
Bob   -> Dfg
Cecil -> Ghkrd
John  -> Klot
Bob   -> Dfg
John  -> Klot
...
{code}
The two critical properties are:
* A particular string should always be obfuscated in the same way.
* The obfuscation method should be such that it should not be trivial to reach to the original value from the obfuscated value (not e.g. ROT13).

In the masking configuration file this could be represented e.g. as.
{code}
{
    ""rules"": [
        {
            ""obfuscate"": {
                ""column"": ""ssn""
            },
            ""applies_to"": [ ... ],
            ""exempted"": [ ... ]
        },
    ...
{code}
but note that the above is only for illustrative purposes. The actual way for expressing what should be obfuscated and how is to be defined during the development. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,179,21,0.117318,9,0.0502793,6,0.0335196,4,0.0223464,4,0.0223464
969,MXS-1315,New Feature,MXS,2017-07-10 14:11:22,,0,expand the source paramter possibilities beyond a single IP or subnetwork ,"the source parameter in the NamedServerFilter only allows an IP like this :

{code:java}
source=192.168.10.1
{code}


 or a wildcard pointer to a whole subnetwork like this : 

{code:java}
source=192.168.10.%
{code}


 as of 2.1.4

An interesting feature would be to provide a list of either IPs or subnetwork or even hostnames as value for this parameter like this :

{code:java}
source=192.168.10.%,192.168.21.3,mydckercontainer.mariadb.com
{code}


it would allow for example docker containers to be easily configured without previous knowledge of the network, or use of hosts file.",,"expand the source paramter possibilities beyond a single IP or subnetwork  $end$ the source parameter in the NamedServerFilter only allows an IP like this :

{code:java}
source=192.168.10.1
{code}


 or a wildcard pointer to a whole subnetwork like this : 

{code:java}
source=192.168.10.%
{code}


 as of 2.1.4

An interesting feature would be to provide a list of either IPs or subnetwork or even hostnames as value for this parameter like this :

{code:java}
source=192.168.10.%,192.168.21.3,mydckercontainer.mariadb.com
{code}


it would allow for example docker containers to be easily configured without previous knowledge of the network, or use of hosts file. $acceptance criteria:$",,Sylvain ARBAUDIE,Sylvain ARBAUDIE,Major,12,,0,3,0,2,0,0,0,,0,850,1,0,0,2018-08-14 09:43:44,expand the source paramter possibilities beyond a single IP or subnetwork ,"the source parameter in the NamedServerFilter only allows an IP like this :

{code:java}
source=192.168.10.1
{code}


 or a wildcard pointer to a whole subnetwork like this : 

{code:java}
source=192.168.10.%
{code}


 as of 2.1.4

An interesting feature would be to provide a list of either IPs or subnetwork or even hostnames as value for this parameter like this :

{code:java}
source=192.168.10.%,192.168.21.3,mydckercontainer.mariadb.com
{code}


it would allow for example docker containers to be easily configured without previous knowledge of the network, or use of hosts file.",,0,0,0,0,0.0,"expand the source paramter possibilities beyond a single IP or subnetwork  $end$ the source parameter in the NamedServerFilter only allows an IP like this :

{code:java}
source=192.168.10.1
{code}


 or a wildcard pointer to a whole subnetwork like this : 

{code:java}
source=192.168.10.%
{code}


 as of 2.1.4

An interesting feature would be to provide a list of either IPs or subnetwork or even hostnames as value for this parameter like this :

{code:java}
source=192.168.10.%,192.168.21.3,mydckercontainer.mariadb.com
{code}


it would allow for example docker containers to be easily configured without previous knowledge of the network, or use of hosts file. $acceptance criteria:$",0,0,0,0,0,0,1,9595.53,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
970,MXS-1317,New Feature,MXS,2017-07-13 20:07:47,,0,Default monitor time,"Please change the default configuration so the monitor time is not 10.000 milliseconds anymore. 

I recommend to change it to 500 or 1000. ",,"Default monitor time $end$ Please change the default configuration so the monitor time is not 10.000 milliseconds anymore. 

I recommend to change it to 500 or 1000.  $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,11,,0,2,0,1,0,0,0,,0,850,1,0,0,2017-07-27 09:40:41,Default monitor time,"Please change the default configuration so the monitor time is not 10.000 milliseconds anymore. 

I recommend to change it to 500 or 1000. ",,0,0,0,0,0.0,"Default monitor time $end$ Please change the default configuration so the monitor time is not 10.000 milliseconds anymore. 

I recommend to change it to 500 or 1000.  $acceptance criteria:$",0,0,0,0,0,0,0,325.533,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
971,MXS-1331,New Feature,MXS,2017-07-27 09:41:52,,0,Add cluster management commands to MaxCtrl,"MaxCtrl needs to be able to:
* Compare two maxscales
* Synchronize two maxscales",,"Add cluster management commands to MaxCtrl $end$ MaxCtrl needs to be able to:
* Compare two maxscales
* Synchronize two maxscales $acceptance criteria:$",,markus makela,markus makela,Major,5,,0,1,2,1,0,0,0,,0,850,1,0,0,2017-07-27 09:41:52,Add cluster management commands to MaxCtrl,"MaxCtrl needs to be able to:
* Compare two maxscales
* Synchronize two maxscales",,0,0,0,0,0.0,"Add cluster management commands to MaxCtrl $end$ MaxCtrl needs to be able to:
* Compare two maxscales
* Synchronize two maxscales $acceptance criteria:$",0,0,0,0,0,0,0,0.0,22,5,0.227273,5,0.227273,5,0.227273,5,0.227273,5,0.227273
972,MXS-1332,Task,MXS,2017-07-27 10:03:14,,0,Study MRM testing,,,Study MRM testing $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,5,0,0,0,,0,850,0,0,0,2017-07-27 10:03:14,Study MRM testing,,,0,0,0,0,0.0,Study MRM testing $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,180,21,0.116667,9,0.05,6,0.0333333,4,0.0222222,4,0.0222222
973,MXS-1333,New Feature,MXS,2017-07-27 11:09:01,,0,Provide optional query execution time in qlafilter log,"MaxScale's qlafilter as of version 2.1 allows to optionally include several useful columns in the qlafilter log using log_data parameter. Please, add column for either query end timestamp or query execution time as it is seen by MaxScale. 

This will allow to use this filter to create a slow query log of a kind, and to aggregate data to find out the total time spent on queries logged per user or any other filter.",,"Provide optional query execution time in qlafilter log $end$ MaxScale's qlafilter as of version 2.1 allows to optionally include several useful columns in the qlafilter log using log_data parameter. Please, add column for either query end timestamp or query execution time as it is seen by MaxScale. 

This will allow to use this filter to create a slow query log of a kind, and to aggregate data to find out the total time spent on queries logged per user or any other filter. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,10,,0,1,0,2,0,0,0,,0,850,0,0,0,2017-10-30 12:27:43,Provide optional query execution time in qlafilter log,"MaxScale's qlafilter as of version 2.1 allows to optionally include several useful columns in the qlafilter log using log_data parameter. Please, add column for either query end timestamp or query execution time as it is seen by MaxScale. 

This will allow to use this filter to create a slow query log of a kind, and to aggregate data to find out the total time spent on queries logged per user or any other filter.",,0,0,0,0,0.0,"Provide optional query execution time in qlafilter log $end$ MaxScale's qlafilter as of version 2.1 allows to optionally include several useful columns in the qlafilter log using log_data parameter. Please, add column for either query end timestamp or query execution time as it is seen by MaxScale. 

This will allow to use this filter to create a slow query log of a kind, and to aggregate data to find out the total time spent on queries logged per user or any other filter. $acceptance criteria:$",0,0,0,0,0,0,1,2281.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
974,MXS-1337,Task,MXS,2017-08-01 06:49:25,,0,Turn qc_sqlite into C++,Changes required by https://jira.mariadb.org/browse/MXS-1307 are easier to implement if STL collections are available.,,Turn qc_sqlite into C++ $end$ Changes required by https://jira.mariadb.org/browse/MXS-1307 are easier to implement if STL collections are available. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-08-09 09:24:35,Turn qc_sqlite into C++,Changes required by https://jira.mariadb.org/browse/MXS-1307 are easier to implement if STL collections are available.,,0,0,0,0,0.0,Turn qc_sqlite into C++ $end$ Changes required by https://jira.mariadb.org/browse/MXS-1307 are easier to implement if STL collections are available. $acceptance criteria:$,0,0,0,0,0,0,0,194.583,181,21,0.116022,9,0.0497238,6,0.0331492,4,0.0220994,4,0.0220994
975,MXS-1343,New Feature,MXS,2017-08-07 07:05:03,,0,MaxScale's binlogrouter does not send hostname to its master,"MaxScale's binlogrouter module is a slave to some master, but unlike normal slaves it does not send hostname at all to its master:

Also, slaves can set a specific hostname for master with --report-host option. I think slave_hostname is needed for binlogrouter to provide the same level of flexibility.",,"MaxScale's binlogrouter does not send hostname to its master $end$ MaxScale's binlogrouter module is a slave to some master, but unlike normal slaves it does not send hostname at all to its master:

Also, slaves can set a specific hostname for master with --report-host option. I think slave_hostname is needed for binlogrouter to provide the same level of flexibility. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,17,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-08-09 09:23:15,MaxScale's binlogrouter does not send hostname to its master,"MaxScale's binlogrouter module is a slave to some master, but unlike normal slaves it does not send hostname at all to its master:

Also, slaves can set a specific hostname for master with --report-host option. I think slave_hostname is needed for binlogrouter to provide the same level of flexibility.",,0,0,0,0,0.0,"MaxScale's binlogrouter does not send hostname to its master $end$ MaxScale's binlogrouter module is a slave to some master, but unlike normal slaves it does not send hostname at all to its master:

Also, slaves can set a specific hostname for master with --report-host option. I think slave_hostname is needed for binlogrouter to provide the same level of flexibility. $acceptance criteria:$",0,0,0,0,0,0,0,50.3,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
976,MXS-1344,New Feature,MXS,2017-08-07 11:55:22,,0,"Make binlog server ""identity"" compatible with MaxScale MySQL Monitor","Due to  some internal variable values, currently part of Binlog Router Identity, it's not possible to use a setup with Master/Slaves and Binlog Server with MaxScale MySQL monitor.

The goal is to detect binlog Server as Relay Master and don't allow any routing operations with read connection router or readwrite split router.",,"Make binlog server ""identity"" compatible with MaxScale MySQL Monitor $end$ Due to  some internal variable values, currently part of Binlog Router Identity, it's not possible to use a setup with Master/Slaves and Binlog Server with MaxScale MySQL monitor.

The goal is to detect binlog Server as Relay Master and don't allow any routing operations with read connection router or readwrite split router. $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,17,,0,2,0,2,0,0,0,,0,850,2,0,0,2017-08-09 09:23:17,"Make binlog server ""identity"" compatible with MaxScale MySQL Monitor","Due to  some internal variable values, currently part of Binlog Router Identity, it's not possible to use a setup with Master/Slaves and Binlog Server with MaxScale MySQL monitor.

The goal is to detect binlog Server as Relay Master and don't allow any routing operations with read connection router or readwrite split router.",,0,0,0,0,0.0,"Make binlog server ""identity"" compatible with MaxScale MySQL Monitor $end$ Due to  some internal variable values, currently part of Binlog Router Identity, it's not possible to use a setup with Master/Slaves and Binlog Server with MaxScale MySQL monitor.

The goal is to detect binlog Server as Relay Master and don't allow any routing operations with read connection router or readwrite split router. $acceptance criteria:$",0,0,0,0,0,0,1,45.45,11,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
977,MXS-1350,New Feature,MXS,2017-08-09 06:01:38,,0,Add XA transaction support,"The query classifiers of MaxScale do not recognize {{XA}} transactions.
{code}
XA {START|BEGIN} xid [JOIN|RESUME]
XA END xid [SUSPEND [FOR MIGRATE]]
...
{code}
The effect is that MaxScale is not aware of there being a transaction on-going.",,"Add XA transaction support $end$ The query classifiers of MaxScale do not recognize {{XA}} transactions.
{code}
XA {START|BEGIN} xid [JOIN|RESUME]
XA END xid [SUSPEND [FOR MIGRATE]]
...
{code}
The effect is that MaxScale is not aware of there being a transaction on-going. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,35,,0,3,1,3,0,1,0,,0,850,3,0,0,2017-08-22 05:51:11,MaxScale does not recognize XA transactions.,"The query classifiers of MaxScale do not recognize {{XA}} transactions.
{code}
XA {START|BEGIN} xid [JOIN|RESUME]
XA END xid [SUSPEND [FOR MIGRATE]]
...
{code}
The effect is that MaxScale is not aware of there being a transaction on-going.",,1,0,0,8,0.130435,"MaxScale does not recognize XA transactions. $end$ The query classifiers of MaxScale do not recognize {{XA}} transactions.
{code}
XA {START|BEGIN} xid [JOIN|RESUME]
XA END xid [SUSPEND [FOR MIGRATE]]
...
{code}
The effect is that MaxScale is not aware of there being a transaction on-going. $acceptance criteria:$",1,1,1,0,0,0,1,311.817,182,21,0.115385,9,0.0494506,6,0.032967,4,0.021978,4,0.021978
978,MXS-1354,New Feature,MXS,2017-08-09 09:52:31,,0,Add authorization to admin interfaces,There should be some sorts of privileges for admin interfaces.,,Add authorization to admin interfaces $end$ There should be some sorts of privileges for admin interfaces. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-08-09 09:52:37,Add authorization to admin interfaces,There should be some sorts of privileges for admin interfaces.,,0,0,0,0,0.0,Add authorization to admin interfaces $end$ There should be some sorts of privileges for admin interfaces. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,23,5,0.217391,5,0.217391,5,0.217391,5,0.217391,5,0.217391
979,MXS-1355,Task,MXS,2017-08-09 09:58:12,,0,Binlog Server and MariaDB 10.1 GTID tests,"create test cases for MXS-1266 feature, description is here https://docs.google.com/document/d/1hIqdLKFDdObBul7HHSwrh6sGMingKNFkaJBKNEJidZc/edit?ts=5940e74c",,"Binlog Server and MariaDB 10.1 GTID tests $end$ create test cases for MXS-1266 feature, description is here https://docs.google.com/document/d/1hIqdLKFDdObBul7HHSwrh6sGMingKNFkaJBKNEJidZc/edit?ts=5940e74c $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2017-08-09 09:58:49,Binlog Server and MariaDB 10.1 GTID tests,"create test cases for MXS-1266 feature, description is here https://docs.google.com/document/d/1hIqdLKFDdObBul7HHSwrh6sGMingKNFkaJBKNEJidZc/edit?ts=5940e74c",,0,0,0,0,0.0,"Binlog Server and MariaDB 10.1 GTID tests $end$ create test cases for MXS-1266 feature, description is here https://docs.google.com/document/d/1hIqdLKFDdObBul7HHSwrh6sGMingKNFkaJBKNEJidZc/edit?ts=5940e74c $acceptance criteria:$",0,0,0,0,0,0,1,0.0,39,1,0.025641,0,0.0,0,0.0,0,0.0,0,0.0
980,MXS-1364,Task,MXS,2017-08-17 06:10:14,,0,Query classifier should report columns used in function calls,"Given a statement like
{code}
select a, concat(b) from t
{code}
the query classifier will tell that the columns {{a}} and {{b}}, and the function {{concat}} are accessed/used, but it will not tell what columns the function {{concat}} accesses.",,"Query classifier should report columns used in function calls $end$ Given a statement like
{code}
select a, concat(b) from t
{code}
the query classifier will tell that the columns {{a}} and {{b}}, and the function {{concat}} are accessed/used, but it will not tell what columns the function {{concat}} accesses. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,1,0,1,1,0,0,0,,0,850,0,0,0,2017-08-22 05:49:45,Query classifier should report columns used in function calls,"Given a statement like
{code}
select a, concat(b) from t
{code}
the query classifier will tell that the columns {{a}} and {{b}}, and the function {{concat}} are accessed/used, but it will not tell what columns the function {{concat}} accesses.",,0,0,0,0,0.0,"Query classifier should report columns used in function calls $end$ Given a statement like
{code}
select a, concat(b) from t
{code}
the query classifier will tell that the columns {{a}} and {{b}}, and the function {{concat}} are accessed/used, but it will not tell what columns the function {{concat}} accesses. $acceptance criteria:$",0,0,0,0,0,0,0,119.65,183,22,0.120219,10,0.0546448,6,0.0327869,4,0.0218579,4,0.0218579
981,MXS-1370,Task,MXS,2017-08-22 09:48:21,,0,Check experimental package.,,,Check experimental package. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-08-22 09:48:21,Check experimental package.,,,0,0,0,0,0.0,Check experimental package. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,184,22,0.119565,10,0.0543478,6,0.0326087,4,0.0217391,4,0.0217391
982,MXS-1383,New Feature,MXS,2017-09-01 12:55:57,,0,"Some change master to MASTER_LOG_FILE, POS settings fail",,,"Some change master to MASTER_LOG_FILE, POS settings fail $end$ $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,13,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-05 09:43:58,"Some change master to MASTER_LOG_FILE, POS settings fail",,,0,0,0,0,0.0,"Some change master to MASTER_LOG_FILE, POS settings fail $end$ $acceptance criteria:$",0,0,0,0,0,0,0,92.8,12,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
983,MXS-1386,Task,MXS,2017-09-04 12:02:29,MXS-2533,0,Clean up DCB,"Currently there is some ambiguity about the DCB and session ownership and how a session shutdown is triggered. For instance, when calling {{dcb_close}} the dcb will also be deleted, except if it is a client dcb in which case it will be closed but not deleted and then explicitly freed in {{session_free}}.

The ownership relations should be cleaned up so that there is no ambiguity. ",,"Clean up DCB $end$ Currently there is some ambiguity about the DCB and session ownership and how a session shutdown is triggered. For instance, when calling {{dcb_close}} the dcb will also be deleted, except if it is a client dcb in which case it will be closed but not deleted and then explicitly freed in {{session_free}}.

The ownership relations should be cleaned up so that there is no ambiguity.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,23,,0,1,0,5,0,1,0,,0,850,0,0,0,2019-07-29 10:34:07,Clean up DCB <-> session relationship,"Currently there is some ambiguity about the DCB and session ownership and how a session shutdown is triggered. For instance, when calling {{dcb_close}} the dcb will also be deleted, except if it is a client dcb in which case it will be closed but not deleted and then explicitly freed in {{session_free}}.

The ownership relations should be cleaned up so that there is no ambiguity. ",,1,0,0,3,0.0405405,"Clean up DCB <-> session relationship $end$ Currently there is some ambiguity about the DCB and session ownership and how a session shutdown is triggered. For instance, when calling {{dcb_close}} the dcb will also be deleted, except if it is a client dcb in which case it will be closed but not deleted and then explicitly freed in {{session_free}}.

The ownership relations should be cleaned up so that there is no ambiguity.  $acceptance criteria:$",1,1,0,0,0,0,1,16630.5,185,22,0.118919,10,0.0540541,6,0.0324324,4,0.0216216,4,0.0216216
984,MXS-1387,New Feature,MXS,2017-09-04 15:01:46,,0,Check dcb_close(slave->dcb) calls in blr_slave.c,"Check all the calls to dcb_close(slave->dcb) when the slave connection is closed based on error code sent.
Use right error code in order to stop the slave client replication thread.",,"Check dcb_close(slave->dcb) calls in blr_slave.c $end$ Check all the calls to dcb_close(slave->dcb) when the slave connection is closed based on error code sent.
Use right error code in order to stop the slave client replication thread. $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,15,,0,0,0,1,0,3,0,,0,850,0,0,0,2017-09-05 09:43:59,Check and Fix dcb_close(slave->dcb) calls in blr_slave.c,Check all the calls to dcb_close(slave->dcb) and remove unnecessary ones when the slave connection is cloed based on error code sent.,,1,2,0,21,0.645161,Check and Fix dcb_close(slave->dcb) calls in blr_slave.c $end$ Check all the calls to dcb_close(slave->dcb) and remove unnecessary ones when the slave connection is cloed based on error code sent. $acceptance criteria:$,3,1,1,1,1,1,1,18.7,13,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
985,MXS-1389,Task,MXS,2017-09-05 09:38:57,,0,Create tests for MXS-1346,Create tests for the new functionality.,,Create tests for MXS-1346 $end$ Create tests for the new functionality. $acceptance criteria:$,,markus makela,markus makela,Major,5,,1,0,1,1,0,0,0,,0,850,0,0,0,2017-09-05 09:39:01,Create tests for MXS-1346,Create tests for the new functionality.,,0,0,0,0,0.0,Create tests for MXS-1346 $end$ Create tests for the new functionality. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,24,5,0.208333,5,0.208333,5,0.208333,5,0.208333,5,0.208333
986,MXS-1390,Task,MXS,2017-09-05 09:43:56,,0,Review the MaxScale REST API and MaxCtrl documentation,Review documentation for the REST API and MaxCtrl.,,Review the MaxScale REST API and MaxCtrl documentation $end$ Review documentation for the REST API and MaxCtrl. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-05 09:44:05,Review the MaxScale REST API and MaxCtrl documentation,Review documentation for the REST API and MaxCtrl.,,0,0,0,0,0.0,Review the MaxScale REST API and MaxCtrl documentation $end$ Review documentation for the REST API and MaxCtrl. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,25,5,0.2,5,0.2,5,0.2,5,0.2,5,0.2
987,MXS-1393,Task,MXS,2017-09-06 13:26:48,,0,Update Technical Training slides for 2.2,https://docs.google.com/presentation/d/1YFQysJxDxivNTT1U8aoFWjzMRiA8Dld7EEgUU5waMns/edit#slide=id.g26a822e343_1_0,,Update Technical Training slides for 2.2 $end$ https://docs.google.com/presentation/d/1YFQysJxDxivNTT1U8aoFWjzMRiA8Dld7EEgUU5waMns/edit#slide=id.g26a822e343_1_0 $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,1,8,,0,850,0,1,0,2017-10-03 07:03:59,Update Technical Training slides for 2.2,https://docs.google.com/presentation/d/1YFQysJxDxivNTT1U8aoFWjzMRiA8Dld7EEgUU5waMns/edit#slide=id.g26a822e343_1_0,,0,0,0,0,0.0,Update Technical Training slides for 2.2 $end$ https://docs.google.com/presentation/d/1YFQysJxDxivNTT1U8aoFWjzMRiA8Dld7EEgUU5waMns/edit#slide=id.g26a822e343_1_0 $acceptance criteria:$,0,0,0,0,0,0,0,641.617,186,23,0.123656,10,0.0537634,6,0.0322581,4,0.0215054,4,0.0215054
988,MXS-1394,Task,MXS,2017-09-06 17:37:45,,0,Use new hash algorithm to sign Ubuntu/Debian repositories,"Ubuntu xenial and Debian stretch expect a stronger signing key for apt repositories:

W: GPG error: http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release: The following signatures were invalid: 13CFDE6DD9EE9784F41AF0F670E4618A8167EE24
W: The repository 'http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release' is not signed.
N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration details.

See https://juliank.wordpress.com/2016/03/14/dropping-sha-1-support-in-apt/ for more information.

This should be taken care of ASAP, for the next minor release(s) of MaxScale; please don't wait for the next major release.",,"Use new hash algorithm to sign Ubuntu/Debian repositories $end$ Ubuntu xenial and Debian stretch expect a stronger signing key for apt repositories:

W: GPG error: http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release: The following signatures were invalid: 13CFDE6DD9EE9784F41AF0F670E4618A8167EE24
W: The repository 'http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release' is not signed.
N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration details.

See https://juliank.wordpress.com/2016/03/14/dropping-sha-1-support-in-apt/ for more information.

This should be taken care of ASAP, for the next minor release(s) of MaxScale; please don't wait for the next major release. $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,7,,0,1,0,2,0,1,0,,0,850,0,1,0,2017-10-24 07:02:05,Use new hash algorithm to sign Ubuntu/Debian repositories,"Ubuntu xenial and Debian stretch expect a stronger signing key for apt repositories:

W: GPG error: http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release: The following signatures were invalid: 13CFDE6DD9EE9784F41AF0F670E4618A8167EE24
W: The repository 'http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release' is not signed.
N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration details.

See https://juliank.wordpress.com/2016/03/14/dropping-sha-1-support-in-apt/ for more information.

This should be taken care of ASAP, for the next minor release(s) of MaxScale; please don't wait for the next major release.",,0,0,0,0,0.0,"Use new hash algorithm to sign Ubuntu/Debian repositories $end$ Ubuntu xenial and Debian stretch expect a stronger signing key for apt repositories:

W: GPG error: http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release: The following signatures were invalid: 13CFDE6DD9EE9784F41AF0F670E4618A8167EE24
W: The repository 'http://downloads.mariadb.com/MaxScale/2.1/debian stretch Release' is not signed.
N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration details.

See https://juliank.wordpress.com/2016/03/14/dropping-sha-1-support-in-apt/ for more information.

This should be taken care of ASAP, for the next minor release(s) of MaxScale; please don't wait for the next major release. $acceptance criteria:$",0,0,0,0,0,0,1,1141.4,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
989,MXS-1401,New Feature,MXS,2017-09-11 07:16:04,,0,Allow multiple store/use pairs in cache filter rules,"The cache filter currently supports only one object with store/use values. An array of store/use values would allow more precise caching rules.

Here is the example JSON that would allow the definition of multiple rules in a single file.

{code:javascript}
[
    {
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t1""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'bob'@'%'""
            }
        ]
    },{
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t2""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'alice'@'%'""
            }
        ]
    }
]
{code} ",,"Allow multiple store/use pairs in cache filter rules $end$ The cache filter currently supports only one object with store/use values. An array of store/use values would allow more precise caching rules.

Here is the example JSON that would allow the definition of multiple rules in a single file.

{code:javascript}
[
    {
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t1""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'bob'@'%'""
            }
        ]
    },{
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t2""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'alice'@'%'""
            }
        ]
    }
]
{code}  $acceptance criteria:$",,markus makela,markus makela,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-04-17 10:00:02,Allow multiple store/use pairs in cache filter rules,"The cache filter currently supports only one object with store/use values. An array of store/use values would allow more precise caching rules.

Here is the example JSON that would allow the definition of multiple rules in a single file.

{code:javascript}
[
    {
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t1""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'bob'@'%'""
            }
        ]
    },{
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t2""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'alice'@'%'""
            }
        ]
    }
]
{code} ",,0,0,0,0,0.0,"Allow multiple store/use pairs in cache filter rules $end$ The cache filter currently supports only one object with store/use values. An array of store/use values would allow more precise caching rules.

Here is the example JSON that would allow the definition of multiple rules in a single file.

{code:javascript}
[
    {
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t1""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'bob'@'%'""
            }
        ]
    },{
        ""store"": [
            {
                ""attribute"": ""table"",
                ""op"": ""="",
                ""value"": ""test.t2""
            }
        ],
        ""use"": [
            {
                ""attribute"": ""user"",
                ""op"": ""="",
                ""value"": ""'alice'@'%'""
            }
        ]
    }
]
{code}  $acceptance criteria:$",0,0,0,0,0,0,0,5234.72,26,5,0.192308,5,0.192308,5,0.192308,5,0.192308,5,0.192308
990,MXS-1417,New Feature,MXS,2017-09-15 07:19:15,,0,draining a node should be possible like haproxy does,"set server xxx maintenance causes existing connections to abort. it would be much nicer to have a set server xxx drain as haproxy does

see irc conversions with markus

(09:04:57) shinguz: does MaxScale 2.1 really not allow draining a node as HAproxy does?
(09:05:14) markusjm: what do you mean by draining?
(09:05:16) shinguz: I could only find maintenance in the doku
(09:05:34) markusjm: prevent new connections from being created?
(09:06:25) shinguz: prevent new connections from being created (as maintenance does) and keep old open until they are finished
(09:06:45) markusjm: you can remove the server from the service and won't be used by new connections
(09:06:54) shinguz: maintenance in MaxScale disconnects sessions as I have experienced in some tests
(09:06:58) markusjm: give me a sec and I'll point you to the docs
(09:07:22) markusjm: but you're right, it's not something that's very obvious and I think it could be documented better
(09:07:38) markusjm: https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#runtime-configuration-changes
(09:07:39) shinguz: what? that it disconnects old connections?
(09:08:11) markusjm: I was referring to the fact that removing a server is the same as draining a server
(09:08:27) shinguz: aha
(09:08:32) shinguz: I see, thanks!
(09:08:37) markusjm: so you should be able to do that in 2.1 with `maxadmin remove server <server> <service>`
(09:08:54) markusjm: it's sort of like a soft-maintenance mode
(09:08:55) shinguz: I see
(09:09:14) shinguz: but needs an add again and this is error prone
(09:09:55) shinguz: so set server nodeC drain imho is more intuitive...
(09:10:09) markusjm: that's true
(09:10:17) shinguz: feature request?
(09:10:27) markusjm: but from a conceptual point of view, you want to remove a server from the list of available servers for a service
(09:10:36) shinguz: no
(09:10:39) markusjm: a feature request would be good
(09:10:59) markusjm: out of curiosity, why do you want to drain the server?
(09:11:08) shinguz: when I put it to maintenance I also do not want to remove it from the entity service I want to put it in maintenance
(09:11:19) shinguz: because we get errors on existing connections
(09:11:25) shinguz: and this is ugly
(09:11:50) shinguz: so we want to drain a server until everybody has finished his work
(09:11:56) shinguz: and then put it in maintenanxce
(09:12:08) shinguz: so no lost connections
(09:12:16) markusjm: yup, that makes perfect sense
(09:12:21) shinguz: for example applications which do persistent connections
(09:12:33) shinguz: ok. I will write feature request
(09:12:46) shinguz: any preference or assignments? shall I mention you?
(09:13:04) markusjm: I think the feature request is quite self-explanatory
(09:13:14) markusjm: but please do, I like being mentioned :)
(09:13:37) shinguz: .oO(why jira???)
(09:13:40) markusjm: but I don't think it'll get in the next majod release
",,"draining a node should be possible like haproxy does $end$ set server xxx maintenance causes existing connections to abort. it would be much nicer to have a set server xxx drain as haproxy does

see irc conversions with markus

(09:04:57) shinguz: does MaxScale 2.1 really not allow draining a node as HAproxy does?
(09:05:14) markusjm: what do you mean by draining?
(09:05:16) shinguz: I could only find maintenance in the doku
(09:05:34) markusjm: prevent new connections from being created?
(09:06:25) shinguz: prevent new connections from being created (as maintenance does) and keep old open until they are finished
(09:06:45) markusjm: you can remove the server from the service and won't be used by new connections
(09:06:54) shinguz: maintenance in MaxScale disconnects sessions as I have experienced in some tests
(09:06:58) markusjm: give me a sec and I'll point you to the docs
(09:07:22) markusjm: but you're right, it's not something that's very obvious and I think it could be documented better
(09:07:38) markusjm: https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#runtime-configuration-changes
(09:07:39) shinguz: what? that it disconnects old connections?
(09:08:11) markusjm: I was referring to the fact that removing a server is the same as draining a server
(09:08:27) shinguz: aha
(09:08:32) shinguz: I see, thanks!
(09:08:37) markusjm: so you should be able to do that in 2.1 with `maxadmin remove server <server> <service>`
(09:08:54) markusjm: it's sort of like a soft-maintenance mode
(09:08:55) shinguz: I see
(09:09:14) shinguz: but needs an add again and this is error prone
(09:09:55) shinguz: so set server nodeC drain imho is more intuitive...
(09:10:09) markusjm: that's true
(09:10:17) shinguz: feature request?
(09:10:27) markusjm: but from a conceptual point of view, you want to remove a server from the list of available servers for a service
(09:10:36) shinguz: no
(09:10:39) markusjm: a feature request would be good
(09:10:59) markusjm: out of curiosity, why do you want to drain the server?
(09:11:08) shinguz: when I put it to maintenance I also do not want to remove it from the entity service I want to put it in maintenance
(09:11:19) shinguz: because we get errors on existing connections
(09:11:25) shinguz: and this is ugly
(09:11:50) shinguz: so we want to drain a server until everybody has finished his work
(09:11:56) shinguz: and then put it in maintenanxce
(09:12:08) shinguz: so no lost connections
(09:12:16) markusjm: yup, that makes perfect sense
(09:12:21) shinguz: for example applications which do persistent connections
(09:12:33) shinguz: ok. I will write feature request
(09:12:46) shinguz: any preference or assignments? shall I mention you?
(09:13:04) markusjm: I think the feature request is quite self-explanatory
(09:13:14) markusjm: but please do, I like being mentioned :)
(09:13:37) shinguz: .oO(why jira???)
(09:13:40) markusjm: but I don't think it'll get in the next majod release
 $acceptance criteria:$",,Oli Sennhauser,Oli Sennhauser,Major,9,,0,5,0,1,0,0,0,,0,850,0,0,0,2018-05-16 10:22:45,draining a node should be possible like haproxy does,"set server xxx maintenance causes existing connections to abort. it would be much nicer to have a set server xxx drain as haproxy does

see irc conversions with markus

(09:04:57) shinguz: does MaxScale 2.1 really not allow draining a node as HAproxy does?
(09:05:14) markusjm: what do you mean by draining?
(09:05:16) shinguz: I could only find maintenance in the doku
(09:05:34) markusjm: prevent new connections from being created?
(09:06:25) shinguz: prevent new connections from being created (as maintenance does) and keep old open until they are finished
(09:06:45) markusjm: you can remove the server from the service and won't be used by new connections
(09:06:54) shinguz: maintenance in MaxScale disconnects sessions as I have experienced in some tests
(09:06:58) markusjm: give me a sec and I'll point you to the docs
(09:07:22) markusjm: but you're right, it's not something that's very obvious and I think it could be documented better
(09:07:38) markusjm: https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#runtime-configuration-changes
(09:07:39) shinguz: what? that it disconnects old connections?
(09:08:11) markusjm: I was referring to the fact that removing a server is the same as draining a server
(09:08:27) shinguz: aha
(09:08:32) shinguz: I see, thanks!
(09:08:37) markusjm: so you should be able to do that in 2.1 with `maxadmin remove server <server> <service>`
(09:08:54) markusjm: it's sort of like a soft-maintenance mode
(09:08:55) shinguz: I see
(09:09:14) shinguz: but needs an add again and this is error prone
(09:09:55) shinguz: so set server nodeC drain imho is more intuitive...
(09:10:09) markusjm: that's true
(09:10:17) shinguz: feature request?
(09:10:27) markusjm: but from a conceptual point of view, you want to remove a server from the list of available servers for a service
(09:10:36) shinguz: no
(09:10:39) markusjm: a feature request would be good
(09:10:59) markusjm: out of curiosity, why do you want to drain the server?
(09:11:08) shinguz: when I put it to maintenance I also do not want to remove it from the entity service I want to put it in maintenance
(09:11:19) shinguz: because we get errors on existing connections
(09:11:25) shinguz: and this is ugly
(09:11:50) shinguz: so we want to drain a server until everybody has finished his work
(09:11:56) shinguz: and then put it in maintenanxce
(09:12:08) shinguz: so no lost connections
(09:12:16) markusjm: yup, that makes perfect sense
(09:12:21) shinguz: for example applications which do persistent connections
(09:12:33) shinguz: ok. I will write feature request
(09:12:46) shinguz: any preference or assignments? shall I mention you?
(09:13:04) markusjm: I think the feature request is quite self-explanatory
(09:13:14) markusjm: but please do, I like being mentioned :)
(09:13:37) shinguz: .oO(why jira???)
(09:13:40) markusjm: but I don't think it'll get in the next majod release
",,0,0,0,0,0.0,"draining a node should be possible like haproxy does $end$ set server xxx maintenance causes existing connections to abort. it would be much nicer to have a set server xxx drain as haproxy does

see irc conversions with markus

(09:04:57) shinguz: does MaxScale 2.1 really not allow draining a node as HAproxy does?
(09:05:14) markusjm: what do you mean by draining?
(09:05:16) shinguz: I could only find maintenance in the doku
(09:05:34) markusjm: prevent new connections from being created?
(09:06:25) shinguz: prevent new connections from being created (as maintenance does) and keep old open until they are finished
(09:06:45) markusjm: you can remove the server from the service and won't be used by new connections
(09:06:54) shinguz: maintenance in MaxScale disconnects sessions as I have experienced in some tests
(09:06:58) markusjm: give me a sec and I'll point you to the docs
(09:07:22) markusjm: but you're right, it's not something that's very obvious and I think it could be documented better
(09:07:38) markusjm: https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#runtime-configuration-changes
(09:07:39) shinguz: what? that it disconnects old connections?
(09:08:11) markusjm: I was referring to the fact that removing a server is the same as draining a server
(09:08:27) shinguz: aha
(09:08:32) shinguz: I see, thanks!
(09:08:37) markusjm: so you should be able to do that in 2.1 with `maxadmin remove server <server> <service>`
(09:08:54) markusjm: it's sort of like a soft-maintenance mode
(09:08:55) shinguz: I see
(09:09:14) shinguz: but needs an add again and this is error prone
(09:09:55) shinguz: so set server nodeC drain imho is more intuitive...
(09:10:09) markusjm: that's true
(09:10:17) shinguz: feature request?
(09:10:27) markusjm: but from a conceptual point of view, you want to remove a server from the list of available servers for a service
(09:10:36) shinguz: no
(09:10:39) markusjm: a feature request would be good
(09:10:59) markusjm: out of curiosity, why do you want to drain the server?
(09:11:08) shinguz: when I put it to maintenance I also do not want to remove it from the entity service I want to put it in maintenance
(09:11:19) shinguz: because we get errors on existing connections
(09:11:25) shinguz: and this is ugly
(09:11:50) shinguz: so we want to drain a server until everybody has finished his work
(09:11:56) shinguz: and then put it in maintenanxce
(09:12:08) shinguz: so no lost connections
(09:12:16) markusjm: yup, that makes perfect sense
(09:12:21) shinguz: for example applications which do persistent connections
(09:12:33) shinguz: ok. I will write feature request
(09:12:46) shinguz: any preference or assignments? shall I mention you?
(09:13:04) markusjm: I think the feature request is quite self-explanatory
(09:13:14) markusjm: but please do, I like being mentioned :)
(09:13:37) shinguz: .oO(why jira???)
(09:13:40) markusjm: but I don't think it'll get in the next majod release
 $acceptance criteria:$",0,0,0,0,0,0,0,5835.05,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
991,MXS-1423,New Feature,MXS,2017-09-19 09:03:03,,0,Add MASTER_USE_GTID to testbinlog.c,,,Add MASTER_USE_GTID to testbinlog.c $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-19 09:03:44,Add MASTER_USE_GTID to testbinlog.c,,,0,0,0,0,0.0,Add MASTER_USE_GTID to testbinlog.c $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,14,1,0.0714286,1,0.0714286,1,0.0714286,1,0.0714286,1,0.0714286
992,MXS-1424,Task,MXS,2017-09-19 09:06:47,,0,Binlog server Blog Posts,,,Binlog server Blog Posts $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Minor,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-09-19 09:08:01,Binlog server Blog Posts,,,0,0,0,0,0.0,Binlog server Blog Posts $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,15,1,0.0666667,1,0.0666667,1,0.0666667,1,0.0666667,1,0.0666667
993,MXS-1425,Task,MXS,2017-09-19 09:07:39,,0,MaxRows Blog post,,,MaxRows Blog post $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Minor,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-09-19 09:08:14,MaxRows Blog post,,,0,0,0,0,0.0,MaxRows Blog post $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,16,1,0.0625,1,0.0625,1,0.0625,1,0.0625,1,0.0625
994,MXS-1427,Sub-Task,MXS,2017-09-20 12:11:44,,0,Parser extensions in 2.2,,,Parser extensions in 2.2 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-20 12:11:44,Parser extensions in 2.2,,,0,0,0,0,0.0,Parser extensions in 2.2 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,195,23,0.117949,10,0.0512821,6,0.0307692,4,0.0205128,4,0.0205128
995,MXS-1428,Sub-Task,MXS,2017-09-20 12:12:34,,0,REST-API,,,REST-API $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-20 12:12:34,REST-API,,,0,0,0,0,0.0,REST-API $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,196,23,0.117347,10,0.0510204,6,0.0306122,4,0.0204082,4,0.0204082
996,MXS-1429,Sub-Task,MXS,2017-09-20 12:13:11,,0,Keepalived + MaxScale,,,Keepalived + MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-20 12:13:11,Keepalived + MaxScale,,,0,0,0,0,0.0,Keepalived + MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,197,23,0.116751,10,0.0507614,6,0.0304569,4,0.0203046,4,0.0203046
997,MXS-143,Sub-Task,MXS,2015-05-15 14:21:45,,0,Persistent Connection to backend server test,see https://mariadb.atlassian.net/browse/MXS-122 for feature description,,Persistent Connection to backend server test $end$ see https://mariadb.atlassian.net/browse/MXS-122 for feature description $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,13,,0,2,0,2,0,0,0,,0,850,2,0,0,2015-05-15 14:21:45,Persistent Connection to backend server test,see https://mariadb.atlassian.net/browse/MXS-122 for feature description,,0,0,0,0,0.0,Persistent Connection to backend server test $end$ see https://mariadb.atlassian.net/browse/MXS-122 for feature description $acceptance criteria:$,0,0,0,0,0,0,1,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
998,MXS-1430,Sub-Task,MXS,2017-09-20 12:13:45,,0,Masking Filter,,,Masking Filter $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-09-20 12:13:45,Masking Filter,,,0,0,0,0,0.0,Masking Filter $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,198,23,0.116162,10,0.050505,6,0.030303,4,0.020202,4,0.020202
999,MXS-1431,Sub-Task,MXS,2017-09-20 12:14:02,,0,Firewall Filter Extensions,,,Firewall Filter Extensions $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-20 12:14:02,Firewall Filter Extensions,,,0,0,0,0,0.0,Firewall Filter Extensions $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,199,23,0.115578,10,0.0502513,6,0.0301508,4,0.0201005,4,0.0201005
1000,MXS-1432,Sub-Task,MXS,2017-09-20 12:14:18,,0,PAM.D authentication,,,PAM.D authentication $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,1,1,0,0,0,,0,850,0,0,0,2017-09-20 12:14:18,PAM.D authentication,,,0,0,0,0,0.0,PAM.D authentication $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,200,23,0.115,10,0.05,6,0.03,4,0.02,4,0.02
1001,MXS-1433,Sub-Task,MXS,2017-09-20 12:14:29,,0,Proxy Protocol,,,Proxy Protocol $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-20 12:14:29,Proxy Protocol,,,0,0,0,0,0.0,Proxy Protocol $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,201,23,0.114428,10,0.0497512,6,0.0298507,4,0.0199005,4,0.0199005
1002,MXS-1434,Sub-Task,MXS,2017-09-20 12:15:40,,0,Binlog Improvements,,,Binlog Improvements $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-09-20 12:15:40,Binlog Improvements,,,0,0,0,0,0.0,Binlog Improvements $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,202,23,0.113861,10,0.049505,6,0.029703,4,0.019802,4,0.019802
1003,MXS-1436,Task,MXS,2017-09-18 07:22:10,MXS-1437,0,Investigate Replication Manager and extract the master down logic.,"It seems that detecting that a master is down is not quite as straightforward as one could imagine. The RM apparently does not immediately conclude that a master that appears to be down actually is down, but makes various attempts before finally accepting that it indeed is down. That apparently is necessary in order to ensure that unnecessary promotions are not performed.",,"Investigate Replication Manager and extract the master down logic. $end$ It seems that detecting that a master is down is not quite as straightforward as one could imagine. The RM apparently does not immediately conclude that a master that appears to be down actually is down, but makes various attempts before finally accepting that it indeed is down. That apparently is necessary in order to ensure that unnecessary promotions are not performed. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,0,1,2,0,0,0,,0,850,0,0,0,2017-10-03 07:18:35,Investigate Replication Manager and extract the master down logic.,"It seems that detecting that a master is down is not quite as straightforward as one could imagine. The RM apparently does not immediately conclude that a master that appears to be down actually is down, but makes various attempts before finally accepting that it indeed is down. That apparently is necessary in order to ensure that unnecessary promotions are not performed.",,0,0,0,0,0.0,"Investigate Replication Manager and extract the master down logic. $end$ It seems that detecting that a master is down is not quite as straightforward as one could imagine. The RM apparently does not immediately conclude that a master that appears to be down actually is down, but makes various attempts before finally accepting that it indeed is down. That apparently is necessary in order to ensure that unnecessary promotions are not performed. $acceptance criteria:$",0,0,0,0,0,0,1,359.933,188,23,0.12234,10,0.0531915,6,0.0319149,4,0.0212766,4,0.0212766
1004,MXS-1438,Task,MXS,2017-09-18 07:24:02,MXS-1437,0,Enhance the master down detection logic of MySQL Monitor based upon the findings of MXS-1436,The robustness and reliability of the master down detection logic of MySQL Monitor should be at least on the level as that of the Replication Manager (this with the assumption that it currently indeed is better).,,Enhance the master down detection logic of MySQL Monitor based upon the findings of MXS-1436 $end$ The robustness and reliability of the master down detection logic of MySQL Monitor should be at least on the level as that of the Replication Manager (this with the assumption that it currently indeed is better). $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,1,1,2,0,1,0,,0,850,1,1,0,2017-10-03 07:18:39,Enhance the master down detection logic of MySQL Monitor based upon the findings of MXS-1436,The robustness and reliability of the master down detection logic of MySQL Monitor should be at least on the level as that of the Replication Manager (this with the assumption that it currently indeed is better).,,0,0,0,0,0.0,Enhance the master down detection logic of MySQL Monitor based upon the findings of MXS-1436 $end$ The robustness and reliability of the master down detection logic of MySQL Monitor should be at least on the level as that of the Replication Manager (this with the assumption that it currently indeed is better). $acceptance criteria:$,0,0,0,0,0,0,1,359.9,189,23,0.121693,10,0.0529101,6,0.031746,4,0.021164,4,0.021164
1005,MXS-1440,Task,MXS,2017-09-18 08:43:13,MXS-1437,0,Switchover Support,"It would be beneficial to be able to perform switchover through MaxScale, whether or not it is configured to perform failover.
* Even if not initialting failover, MaxScale can still benefit from knowing whether switchover will be performed, e.g. by internally behaving as if the old master had been put in maintenance mode, queing new connection attempts until the new master is up again, etc.
* If MaxScale is initiating failover, then it needs to turn off that behaviour while switchover is taking place, as otherwise the switchover and the failover may interfere with each other.

It would of course be possible to
* connect to to MaxScale,
* turn off the relevant monitor,
* perform the switchover, and
* turn on the relevant monitor again

but from an end-user point of view it is simpler if it is possible to simply initiate the switchover through MaxScale and it takes care of everything.",,"Switchover Support $end$ It would be beneficial to be able to perform switchover through MaxScale, whether or not it is configured to perform failover.
* Even if not initialting failover, MaxScale can still benefit from knowing whether switchover will be performed, e.g. by internally behaving as if the old master had been put in maintenance mode, queing new connection attempts until the new master is up again, etc.
* If MaxScale is initiating failover, then it needs to turn off that behaviour while switchover is taking place, as otherwise the switchover and the failover may interfere with each other.

It would of course be possible to
* connect to to MaxScale,
* turn off the relevant monitor,
* perform the switchover, and
* turn on the relevant monitor again

but from an end-user point of view it is simpler if it is possible to simply initiate the switchover through MaxScale and it takes care of everything. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,14,,0,0,0,1,0,3,3,,0,850,0,3,0,2017-10-03 07:04:35,Switchover Support,"It would be beneficial to be able to perform switchover through MaxScale, whether or not it is configured to perform failover.
* Even if not initialting failover, MaxScale can still benefit from knowing whether switchover will be performed, e.g. by internally behaving as if the old master had been put in maintenance mode, queing new connection attempts until the new master is up again, etc.
* If MaxScale is initiating failover, then it needs to turn off that behaviour while switchover is taking place, as otherwise the switchover and the failover may interfere with each other.

It would of course be possible to
* connect to to MaxScale,
* turn off the relevant monitor,
* perform the switchover, and
* turn on the relevant monitor again

but from an end-user point of view it is simpler if it is possible to simply initiate the switchover through MaxScale and it takes care of everything.",,0,0,0,0,0.0,"Switchover Support $end$ It would be beneficial to be able to perform switchover through MaxScale, whether or not it is configured to perform failover.
* Even if not initialting failover, MaxScale can still benefit from knowing whether switchover will be performed, e.g. by internally behaving as if the old master had been put in maintenance mode, queing new connection attempts until the new master is up again, etc.
* If MaxScale is initiating failover, then it needs to turn off that behaviour while switchover is taking place, as otherwise the switchover and the failover may interfere with each other.

It would of course be possible to
* connect to to MaxScale,
* turn off the relevant monitor,
* perform the switchover, and
* turn on the relevant monitor again

but from an end-user point of view it is simpler if it is possible to simply initiate the switchover through MaxScale and it takes care of everything. $acceptance criteria:$",0,0,0,0,0,0,0,358.35,191,23,0.120419,10,0.052356,6,0.0314136,4,0.0209424,4,0.0209424
1006,MXS-1441,Sub-Task,MXS,2017-09-18 12:02:28,,0,Add support for switchover script.,"It should be possible to specify in the MySQL Monitor configuration the name of a script that is capable of performing _switchover_.

The parameters and their format that the switchover script will be called with should be documented.
",,"Add support for switchover script. $end$ It should be possible to specify in the MySQL Monitor configuration the name of a script that is capable of performing _switchover_.

The parameters and their format that the switchover script will be called with should be documented.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-09-18 12:02:28,Add support for switchover script.,"It should be possible to specify in the MySQL Monitor configuration the name of a script that is capable of performing _switchover_.

The parameters and their format that the switchover script will be called with should be documented.
",,0,0,0,0,0.0,"Add support for switchover script. $end$ It should be possible to specify in the MySQL Monitor configuration the name of a script that is capable of performing _switchover_.

The parameters and their format that the switchover script will be called with should be documented.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,192,23,0.119792,10,0.0520833,6,0.03125,4,0.0208333,4,0.0208333
1007,MXS-1442,Sub-Task,MXS,2017-09-18 12:07:04,,0,Expose REST-API for switchover,"When a _MySQL Monitor_ configuration entry has been configured with a _switchover_ entry, a corresponding REST-API should automatically be exposed.

The URL of the endpoint should contain the name of the _MySQL Monitor_ entry, so that there can be multiple clusters.",,"Expose REST-API for switchover $end$ When a _MySQL Monitor_ configuration entry has been configured with a _switchover_ entry, a corresponding REST-API should automatically be exposed.

The URL of the endpoint should contain the name of the _MySQL Monitor_ entry, so that there can be multiple clusters. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-09-18 12:07:04,Expose REST-API for switchover,"When a _MySQL Monitor_ configuration entry has been configured with a _switchover_ entry, a corresponding REST-API should automatically be exposed.

The URL of the endpoint should contain the name of the _MySQL Monitor_ entry, so that there can be multiple clusters.",,0,0,0,0,0.0,"Expose REST-API for switchover $end$ When a _MySQL Monitor_ configuration entry has been configured with a _switchover_ entry, a corresponding REST-API should automatically be exposed.

The URL of the endpoint should contain the name of the _MySQL Monitor_ entry, so that there can be multiple clusters. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,193,23,0.119171,10,0.0518135,6,0.0310881,4,0.0207254,4,0.0207254
1008,MXS-1443,Sub-Task,MXS,2017-09-18 12:14:37,,0,Perform switchover,"Actions that need to be perform include, but probably are not limited to:
* Disable failover behaviour/disable monitor.
* Ensure switchover REST-API returns busy.
* Call switchover script
* Enable REST-API
* Enable failover behaviour/enable monitor
",,"Perform switchover $end$ Actions that need to be perform include, but probably are not limited to:
* Disable failover behaviour/disable monitor.
* Ensure switchover REST-API returns busy.
* Call switchover script
* Enable REST-API
* Enable failover behaviour/enable monitor
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-09-18 12:14:37,Perform switchover,"Actions that need to be perform include, but probably are not limited to:
* Disable failover behaviour/disable monitor.
* Ensure switchover REST-API returns busy.
* Call switchover script
* Enable REST-API
* Enable failover behaviour/enable monitor
",,0,0,0,0,0.0,"Perform switchover $end$ Actions that need to be perform include, but probably are not limited to:
* Disable failover behaviour/disable monitor.
* Ensure switchover REST-API returns busy.
* Call switchover script
* Enable REST-API
* Enable failover behaviour/enable monitor
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,194,23,0.118557,10,0.0515464,6,0.0309278,4,0.0206186,4,0.0206186
1009,MXS-1445,Task,MXS,2017-09-18 07:39:32,MXS-1437,0,Add script variables,"The failover and switchover scripts need to be provided with credentials using which the scripts can access and make modifications to the affected servers. The scripts will use the same credentials as the MaxScale monitor does.

In the MaxScale configuration file it is possible to provide credentials for the monitor, to be used with all servers, and also servers specific monitor credentials.

A new [script variable|https://github.com/mariadb-corporation/MaxScale/blob/2.1/Documentation/Monitors/Monitor-Common.md#script], e.g. {{CREDENTIALS}}  could be introduced that would contain the credentials for all nodes.
{code}
script=failover.sh --slaves=$SLAVELIST --credentials=$CREDENTIALS
{code}
When executed, the command line would look like:
{code}
failover.sh --credentials=maxmon:maxpwd1@[192.168.0.201]:3306,maxmon:maxpwd2@[192.168.0.121]:3306
 --slaves=[192.168.0.201]:3306,[192.168.0.121]:3306
{code}
",,"Add script variables $end$ The failover and switchover scripts need to be provided with credentials using which the scripts can access and make modifications to the affected servers. The scripts will use the same credentials as the MaxScale monitor does.

In the MaxScale configuration file it is possible to provide credentials for the monitor, to be used with all servers, and also servers specific monitor credentials.

A new [script variable|https://github.com/mariadb-corporation/MaxScale/blob/2.1/Documentation/Monitors/Monitor-Common.md#script], e.g. {{CREDENTIALS}}  could be introduced that would contain the credentials for all nodes.
{code}
script=failover.sh --slaves=$SLAVELIST --credentials=$CREDENTIALS
{code}
When executed, the command line would look like:
{code}
failover.sh --credentials=maxmon:maxpwd1@[192.168.0.201]:3306,maxmon:maxpwd2@[192.168.0.121]:3306
 --slaves=[192.168.0.201]:3306,[192.168.0.121]:3306
{code}
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,18,,2,0,2,1,0,0,0,,0,850,0,0,0,2017-09-19 15:42:08,Add script variables,"The failover and switchover scripts need to be provided with credentials using which the scripts can access and make modifications to the affected servers. The scripts will use the same credentials as the MaxScale monitor does.

In the MaxScale configuration file it is possible to provide credentials for the monitor, to be used with all servers, and also servers specific monitor credentials.

A new [script variable|https://github.com/mariadb-corporation/MaxScale/blob/2.1/Documentation/Monitors/Monitor-Common.md#script], e.g. {{CREDENTIALS}}  could be introduced that would contain the credentials for all nodes.
{code}
script=failover.sh --slaves=$SLAVELIST --credentials=$CREDENTIALS
{code}
When executed, the command line would look like:
{code}
failover.sh --credentials=maxmon:maxpwd1@[192.168.0.201]:3306,maxmon:maxpwd2@[192.168.0.121]:3306
 --slaves=[192.168.0.201]:3306,[192.168.0.121]:3306
{code}
",,0,0,0,0,0.0,"Add script variables $end$ The failover and switchover scripts need to be provided with credentials using which the scripts can access and make modifications to the affected servers. The scripts will use the same credentials as the MaxScale monitor does.

In the MaxScale configuration file it is possible to provide credentials for the monitor, to be used with all servers, and also servers specific monitor credentials.

A new [script variable|https://github.com/mariadb-corporation/MaxScale/blob/2.1/Documentation/Monitors/Monitor-Common.md#script], e.g. {{CREDENTIALS}}  could be introduced that would contain the credentials for all nodes.
{code}
script=failover.sh --slaves=$SLAVELIST --credentials=$CREDENTIALS
{code}
When executed, the command line would look like:
{code}
failover.sh --credentials=maxmon:maxpwd1@[192.168.0.201]:3306,maxmon:maxpwd2@[192.168.0.121]:3306
 --slaves=[192.168.0.201]:3306,[192.168.0.121]:3306
{code}
 $acceptance criteria:$",0,0,0,0,0,0,0,32.0333,190,23,0.121053,10,0.0526316,6,0.0315789,4,0.0210526,4,0.0210526
1010,MXS-1446,Task,MXS,2017-09-15 12:30:15,MXS-1437,0,Introduce the concept of active/passive to MaxScale,"In _active_ mode, MaxScale behaves in the normal manner.

In _passive_ mode MaxScale
* when started, will create all services, listener sockets, REST-API endpoint, etc. just like an active MaxScale will do,
* will *expect* not to receive traffic (although it will not cause errors if it does),
* will monitor the servers, and
* will *not* launch external programs on the basis of state changes in the servers.

Whether or not MaxScale should start in _active_ or _passive_ mode should be configurable in the configuration file and also possible to indicate with a command line flag.
",,"Introduce the concept of active/passive to MaxScale $end$ In _active_ mode, MaxScale behaves in the normal manner.

In _passive_ mode MaxScale
* when started, will create all services, listener sockets, REST-API endpoint, etc. just like an active MaxScale will do,
* will *expect* not to receive traffic (although it will not cause errors if it does),
* will monitor the servers, and
* will *not* launch external programs on the basis of state changes in the servers.

Whether or not MaxScale should start in _active_ or _passive_ mode should be configurable in the configuration file and also possible to indicate with a command line flag.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,17,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-09-26 14:19:34,Introduce the concept of active/passive to MaxScale,"In _active_ mode, MaxScale behaves in the normal manner.

In _passive_ mode MaxScale
* when started, will create all services, listener sockets, REST-API endpoint, etc. just like an active MaxScale will do,
* will *expect* not to receive traffic (although it will not cause errors if it does),
* will monitor the servers, and
* will *not* launch external programs on the basis of state changes in the servers.

Whether or not MaxScale should start in _active_ or _passive_ mode should be configurable in the configuration file and also possible to indicate with a command line flag.
",,0,0,0,0,0.0,"Introduce the concept of active/passive to MaxScale $end$ In _active_ mode, MaxScale behaves in the normal manner.

In _passive_ mode MaxScale
* when started, will create all services, listener sockets, REST-API endpoint, etc. just like an active MaxScale will do,
* will *expect* not to receive traffic (although it will not cause errors if it does),
* will monitor the servers, and
* will *not* launch external programs on the basis of state changes in the servers.

Whether or not MaxScale should start in _active_ or _passive_ mode should be configurable in the configuration file and also possible to indicate with a command line flag.
 $acceptance criteria:$",0,0,0,0,0,0,0,265.817,187,23,0.122995,10,0.0534759,6,0.0320856,4,0.0213904,4,0.0213904
1011,MXS-1467,New Feature,MXS,2017-10-09 16:54:00,,0,MaxScale should monitor ColumnStore servers (UMs) for the ability to work with ColumnStore tables,"When MaxScale monitors ColumnStore nodes, it's possible that the node is able to process any DML for non-ColumnStore tables, but ColumnStore data are not available there (PMs failed, for example).

Monitors should check this, as it impacts the routing decisions.

See https://jira.mariadb.org/browse/MCOL-962 for the related request to get the information needed available via SQL.",,"MaxScale should monitor ColumnStore servers (UMs) for the ability to work with ColumnStore tables $end$ When MaxScale monitors ColumnStore nodes, it's possible that the node is able to process any DML for non-ColumnStore tables, but ColumnStore data are not available there (PMs failed, for example).

Monitors should check this, as it impacts the routing decisions.

See https://jira.mariadb.org/browse/MCOL-962 for the related request to get the information needed available via SQL. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,22,,0,1,3,3,0,0,0,,0,850,0,0,0,2018-09-25 10:39:59,MaxScale should monitor ColumnStore servers (UMs) for the ability to work with ColumnStore tables,"When MaxScale monitors ColumnStore nodes, it's possible that the node is able to process any DML for non-ColumnStore tables, but ColumnStore data are not available there (PMs failed, for example).

Monitors should check this, as it impacts the routing decisions.

See https://jira.mariadb.org/browse/MCOL-962 for the related request to get the information needed available via SQL.",,0,0,0,0,0.0,"MaxScale should monitor ColumnStore servers (UMs) for the ability to work with ColumnStore tables $end$ When MaxScale monitors ColumnStore nodes, it's possible that the node is able to process any DML for non-ColumnStore tables, but ColumnStore data are not available there (PMs failed, for example).

Monitors should check this, as it impacts the routing decisions.

See https://jira.mariadb.org/browse/MCOL-962 for the related request to get the information needed available via SQL. $acceptance criteria:$",0,0,0,0,0,0,1,8417.75,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1012,MXS-1470,Task,MXS,2017-10-16 06:56:30,,0,Download page for MaxCtrl on download page.,It should be possible to ,,Download page for MaxCtrl on download page. $end$ It should be possible to  $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-11-08 10:54:23,Download page for MaxCtrl on download page.,It should be possible to ,,0,0,0,0,0.0,Download page for MaxCtrl on download page. $end$ It should be possible to  $acceptance criteria:$,0,0,0,0,0,0,1,555.95,203,23,0.1133,10,0.0492611,6,0.0295567,4,0.0197044,4,0.0197044
1013,MXS-1474,New Feature,MXS,2017-10-16 11:09:01,,0,Cache module should be ACID compliant,"Hi,

From our lunch talk I got the impression that the cache module is not ACID compliant:
SELECT statements inside a read/write transactions return cached results. This means the statement is not sent to the server which means the read locks are not set. With REPEATABLE-READ (or SERIALIZABLE) isolation level this makes it not ACID compliant.

Please change this so read/write transactional reads do not return cached results.

Thanks,
Michaël",,"Cache module should be ACID compliant $end$ Hi,

From our lunch talk I got the impression that the cache module is not ACID compliant:
SELECT statements inside a read/write transactions return cached results. This means the statement is not sent to the server which means the read locks are not set. With REPEATABLE-READ (or SERIALIZABLE) isolation level this makes it not ACID compliant.

Please change this so read/write transactional reads do not return cached results.

Thanks,
Michaël $acceptance criteria:$",,Michaël de groot,Michaël de groot,Blocker,7,,0,3,0,1,0,0,0,,0,850,1,0,0,2017-10-24 07:17:06,Cache module should be ACID compliant,"Hi,

From our lunch talk I got the impression that the cache module is not ACID compliant:
SELECT statements inside a read/write transactions return cached results. This means the statement is not sent to the server which means the read locks are not set. With REPEATABLE-READ (or SERIALIZABLE) isolation level this makes it not ACID compliant.

Please change this so read/write transactional reads do not return cached results.

Thanks,
Michaël",,0,0,0,0,0.0,"Cache module should be ACID compliant $end$ Hi,

From our lunch talk I got the impression that the cache module is not ACID compliant:
SELECT statements inside a read/write transactions return cached results. This means the statement is not sent to the server which means the read locks are not set. With REPEATABLE-READ (or SERIALIZABLE) isolation level this makes it not ACID compliant.

Please change this so read/write transactional reads do not return cached results.

Thanks,
Michaël $acceptance criteria:$",0,0,0,0,0,0,0,188.133,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1014,MXS-1475,New Feature,MXS,2017-10-16 11:19:25,,0,Create on-demand caching that can easily be enabled,"Hi,

Please create on-demand caching (versus caching everything). In most ORM's it is quite difficult to add a comment in the query, so I would prefer to use proxy-session variables:

SET PROXY cache_enabled=1;
SET PROXY cache_soft_ttl=3600;
SET PROXY cache_hard_ttl=3630;

This provides functionality for a couple of use cases:
- 90% of the queries are not cacheable and in any case cheap to execute
- Some queries are safe to cache for a minute, other queries are safe to cache for a day or a month

Please make this work out of the box by just adding 'filter=ondemandcache' (or however you want to call it) to router config, optionally setting default ttl's but with a sane default built in as well (5 minutes / 5 minuets + 30 seconds?). Please also add filter=ondemandcache in the example config as (I suppose?) the extra expense for checking a 'SET PROXY' command is very small. If you start to use SET PROXY for other settings it would need to be checked anyways.

Thank you,
Michaël",,"Create on-demand caching that can easily be enabled $end$ Hi,

Please create on-demand caching (versus caching everything). In most ORM's it is quite difficult to add a comment in the query, so I would prefer to use proxy-session variables:

SET PROXY cache_enabled=1;
SET PROXY cache_soft_ttl=3600;
SET PROXY cache_hard_ttl=3630;

This provides functionality for a couple of use cases:
- 90% of the queries are not cacheable and in any case cheap to execute
- Some queries are safe to cache for a minute, other queries are safe to cache for a day or a month

Please make this work out of the box by just adding 'filter=ondemandcache' (or however you want to call it) to router config, optionally setting default ttl's but with a sane default built in as well (5 minutes / 5 minuets + 30 seconds?). Please also add filter=ondemandcache in the example config as (I suppose?) the extra expense for checking a 'SET PROXY' command is very small. If you start to use SET PROXY for other settings it would need to be checked anyways.

Thank you,
Michaël $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,17,,0,1,1,5,0,0,0,,0,850,1,0,0,2017-11-21 09:53:39,Create on-demand caching that can easily be enabled,"Hi,

Please create on-demand caching (versus caching everything). In most ORM's it is quite difficult to add a comment in the query, so I would prefer to use proxy-session variables:

SET PROXY cache_enabled=1;
SET PROXY cache_soft_ttl=3600;
SET PROXY cache_hard_ttl=3630;

This provides functionality for a couple of use cases:
- 90% of the queries are not cacheable and in any case cheap to execute
- Some queries are safe to cache for a minute, other queries are safe to cache for a day or a month

Please make this work out of the box by just adding 'filter=ondemandcache' (or however you want to call it) to router config, optionally setting default ttl's but with a sane default built in as well (5 minutes / 5 minuets + 30 seconds?). Please also add filter=ondemandcache in the example config as (I suppose?) the extra expense for checking a 'SET PROXY' command is very small. If you start to use SET PROXY for other settings it would need to be checked anyways.

Thank you,
Michaël",,0,0,0,0,0.0,"Create on-demand caching that can easily be enabled $end$ Hi,

Please create on-demand caching (versus caching everything). In most ORM's it is quite difficult to add a comment in the query, so I would prefer to use proxy-session variables:

SET PROXY cache_enabled=1;
SET PROXY cache_soft_ttl=3600;
SET PROXY cache_hard_ttl=3630;

This provides functionality for a couple of use cases:
- 90% of the queries are not cacheable and in any case cheap to execute
- Some queries are safe to cache for a minute, other queries are safe to cache for a day or a month

Please make this work out of the box by just adding 'filter=ondemandcache' (or however you want to call it) to router config, optionally setting default ttl's but with a sane default built in as well (5 minutes / 5 minuets + 30 seconds?). Please also add filter=ondemandcache in the example config as (I suppose?) the extra expense for checking a 'SET PROXY' command is very small. If you start to use SET PROXY for other settings it would need to be checked anyways.

Thank you,
Michaël $acceptance criteria:$",0,0,0,0,0,0,1,862.567,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1015,MXS-1479,Task,MXS,2017-10-19 12:32:51,,0,MaxScale should refuse to run as root,,,MaxScale should refuse to run as root $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-11-08 08:59:25,MaxScale should refuse to run as root,,,0,0,0,0,0.0,MaxScale should refuse to run as root $end$ $acceptance criteria:$,0,0,0,0,0,0,0,476.433,204,23,0.112745,10,0.0490196,6,0.0294118,4,0.0196078,4,0.0196078
1016,MXS-1483,New Feature,MXS,2017-10-22 14:22:00,MXS-2533,0,Add job queues to REST API,"The MaxScale REST API should implement the 202 Accepted response from the JSON API specification: http://jsonapi.org/format/#crud-creating-responses-202

This would allow asynchronous execution of jobs that could take a long time.",,"Add job queues to REST API $end$ The MaxScale REST API should implement the 202 Accepted response from the JSON API specification: http://jsonapi.org/format/#crud-creating-responses-202

This would allow asynchronous execution of jobs that could take a long time. $acceptance criteria:$",,markus makela,markus makela,Major,23,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-10-07 10:33:00,Add job queues to REST API,"The MaxScale REST API should implement the 202 Accepted response from the JSON API specification: http://jsonapi.org/format/#crud-creating-responses-202

This would allow asynchronous execution of jobs that could take a long time.",,0,0,0,0,0.0,"Add job queues to REST API $end$ The MaxScale REST API should implement the 202 Accepted response from the JSON API specification: http://jsonapi.org/format/#crud-creating-responses-202

This would allow asynchronous execution of jobs that could take a long time. $acceptance criteria:$",0,0,0,0,0,0,0,17156.2,27,5,0.185185,5,0.185185,5,0.185185,5,0.185185,5,0.185185
1017,MXS-1484,New Feature,MXS,2017-10-23 07:52:52,,0,Automatically set  binlog storage to TREE mode when mariadb10_master_gtid is on,,,Automatically set  binlog storage to TREE mode when mariadb10_master_gtid is on $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Minor,12,,0,0,1,1,0,0,0,,0,850,0,0,0,2017-10-23 08:08:45,Automatically set  binlog storage to TREE mode when mariadb10_master_gtid is on,,,0,0,0,0,0.0,Automatically set  binlog storage to TREE mode when mariadb10_master_gtid is on $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.25,17,1,0.0588235,1,0.0588235,1,0.0588235,1,0.0588235,1,0.0588235
1018,MXS-1485,New Feature,MXS,2017-10-23 08:11:00,,0,MariaDB 10 GTID is always on for slave connections,,,MariaDB 10 GTID is always on for slave connections $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,13,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-23 08:11:59,MariaDB 10 GTID is always on for slave connections,,,0,0,0,0,0.0,MariaDB 10 GTID is always on for slave connections $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,18,1,0.0555556,1,0.0555556,1,0.0555556,1,0.0555556,1,0.0555556
1019,MXS-1487,New Feature,MXS,2017-10-23 14:31:46,,0,Binlog Server setup with MySQL 5.7,Check whether a MySQL 5.7 M/S setup works with Binlog Server,,Binlog Server setup with MySQL 5.7 $end$ Check whether a MySQL 5.7 M/S setup works with Binlog Server $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Minor,14,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-10-23 14:32:23,Binlog Server setup with MySQL 5.7,Check whether a MySQL 5.7 M/S setup works with Binlog Server,,0,0,0,0,0.0,Binlog Server setup with MySQL 5.7 $end$ Check whether a MySQL 5.7 M/S setup works with Binlog Server $acceptance criteria:$,0,0,0,0,0,0,0,0.0,19,1,0.0526316,1,0.0526316,1,0.0526316,1,0.0526316,1,0.0526316
1020,MXS-1488,New Feature,MXS,2017-10-24 06:27:52,,0,Add support for show status like 'slave_received_heartbeats' in binlogserver,,,Add support for show status like 'slave_received_heartbeats' in binlogserver $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Minor,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-24 06:28:13,Add support for show status like 'slave_received_heartbeats' in binlogserver,,,0,0,0,0,0.0,Add support for show status like 'slave_received_heartbeats' in binlogserver $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,20,1,0.05,1,0.05,1,0.05,1,0.05,1,0.05
1021,MXS-1489,Task,MXS,2017-10-24 07:06:10,,0,Create generic mechanism for running tasks in parallel,For the failover mechanism you need to be able to monitor multiple servers in parallel as otherwise the time required for obtaining the state of a number slave servers linearly follow the number of slaves.,,Create generic mechanism for running tasks in parallel $end$ For the failover mechanism you need to be able to monitor multiple servers in parallel as otherwise the time required for obtaining the state of a number slave servers linearly follow the number of slaves. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,1,0,,0,850,0,0,0,2017-10-24 07:06:10,Create generic mechanism for running task in parallel,For the failover mechanism you need to be able to monitor multiple servers in parallel as otherwise the time required for obtaining the state of a number slave servers linearly follow the number of slaves.,,1,0,0,2,0.0217391,Create generic mechanism for running task in parallel $end$ For the failover mechanism you need to be able to monitor multiple servers in parallel as otherwise the time required for obtaining the state of a number slave servers linearly follow the number of slaves. $acceptance criteria:$,1,1,0,0,0,0,0,0.0,205,23,0.112195,10,0.0487805,6,0.0292683,4,0.0195122,4,0.0195122
1022,MXS-1490,Task,MXS,2017-10-24 07:15:40,,0,Selection of master candidate. ,,,Selection of master candidate.  $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-10-24 07:15:40,Selection of master candidate. ,,,0,0,0,0,0.0,Selection of master candidate.  $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,206,24,0.116505,10,0.0485437,6,0.0291262,4,0.0194175,4,0.0194175
1023,MXS-1491,Task,MXS,2017-10-24 07:15:57,,0,Promote selected candidate to master.,,,Promote selected candidate to master. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-24 07:15:57,Promote selected candidate to master.,,,0,0,0,0,0.0,Promote selected candidate to master. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,207,24,0.115942,10,0.0483092,6,0.0289855,4,0.0193237,4,0.0193237
1024,MXS-1492,Task,MXS,2017-10-24 07:16:12,,0,Update slaves with new master.,,,Update slaves with new master. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-24 07:16:12,Update slaves with new master.,,,0,0,0,0,0.0,Update slaves with new master. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,208,24,0.115385,10,0.0480769,6,0.0288462,4,0.0192308,4,0.0192308
1025,MXS-1493,New Feature,MXS,2017-10-24 09:50:49,,0,Use replication heartbeat in mysqlmon ,The replication heartbeat can be used to verify whether a master has really died.,,Use replication heartbeat in mysqlmon  $end$ The replication heartbeat can be used to verify whether a master has really died. $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-24 09:51:04,Use replication heartbeat in mysqlmon ,The replication heartbeat can be used to verify whether a master has really died.,,0,0,0,0,0.0,Use replication heartbeat in mysqlmon  $end$ The replication heartbeat can be used to verify whether a master has really died. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,28,5,0.178571,5,0.178571,5,0.178571,5,0.178571,5,0.178571
1026,MXS-1494,New Feature,MXS,2017-10-24 09:57:57,,0,Add replication credentials as mysqlmon parameters,Add {{replication-user}} and {{replication-password}} to the mysqlmon for failover.,,Add replication credentials as mysqlmon parameters $end$ Add {{replication-user}} and {{replication-password}} to the mysqlmon for failover. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-24 09:57:57,Add replication credentials as mysqlmon parameters,Add {{replication-user}} and {{replication-password}} to the mysqlmon for failover.,,0,0,0,0,0.0,Add replication credentials as mysqlmon parameters $end$ Add {{replication-user}} and {{replication-password}} to the mysqlmon for failover. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,29,5,0.172414,5,0.172414,5,0.172414,5,0.172414,5,0.172414
1027,MXS-1495,New Feature,MXS,2017-10-24 11:35:44,,0,Prevent failover if multi-source replication is in use,"If mysqlmon detects that multiple replication domains are in use or the SHOW ALL SLAVES STATUS returns multiple rows, failover should be disabled.",,"Prevent failover if multi-source replication is in use $end$ If mysqlmon detects that multiple replication domains are in use or the SHOW ALL SLAVES STATUS returns multiple rows, failover should be disabled. $acceptance criteria:$",,markus makela,markus makela,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-10-24 11:35:51,Prevent failover if multi-source replication is in use,"If mysqlmon detects that multiple replication domains are in use or the SHOW ALL SLAVES STATUS returns multiple rows, failover should be disabled.",,0,0,0,0,0.0,"Prevent failover if multi-source replication is in use $end$ If mysqlmon detects that multiple replication domains are in use or the SHOW ALL SLAVES STATUS returns multiple rows, failover should be disabled. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,30,5,0.166667,5,0.166667,5,0.166667,5,0.166667,5,0.166667
1028,MXS-1498,New Feature,MXS,2017-10-26 18:50:39,,0,Package the CDC Connector,"* The CDC Connector should be a part of the MaxScale development package.
* The package should contain a header and a static library",,"Package the CDC Connector $end$ * The CDC Connector should be a part of the MaxScale development package.
* The package should contain a header and a static library $acceptance criteria:$",,markus makela,markus makela,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-11-08 10:33:15,Package the CDC Connector,"* The CDC Connector should be a part of the MaxScale development package.
* The package should contain a header and a static library",,0,0,0,0,0.0,"Package the CDC Connector $end$ * The CDC Connector should be a part of the MaxScale development package.
* The package should contain a header and a static library $acceptance criteria:$",0,0,0,0,0,0,0,303.7,31,5,0.16129,5,0.16129,5,0.16129,5,0.16129,5,0.16129
1029,MXS-1503,New Feature,MXS,2017-10-30 21:11:18,MXS-1501,0,Master reconnection,"h2. Overview

If readwritesplit loses the connection to the master, it is possible for the client session to go into read-only mode if the {{master_failure_mode}}  is set to {{fail_on_write}} or {{error_on_write}}. If a master server becomes available when the session is in read-only mode, it should be possible to reconnect to the same master.

h2. Task Details

* Detect when new masters are available
* Refactor connection creation code to treat master and slave servers the same way",,"Master reconnection $end$ h2. Overview

If readwritesplit loses the connection to the master, it is possible for the client session to go into read-only mode if the {{master_failure_mode}}  is set to {{fail_on_write}} or {{error_on_write}}. If a master server becomes available when the session is in read-only mode, it should be possible to reconnect to the same master.

h2. Task Details

* Detect when new masters are available
* Refactor connection creation code to treat master and slave servers the same way $acceptance criteria:$",,markus makela,markus makela,Major,6,,0,0,2,1,0,0,0,,0,850,0,0,0,2018-03-20 10:25:26,Master reconnection,"h2. Overview

If readwritesplit loses the connection to the master, it is possible for the client session to go into read-only mode if the {{master_failure_mode}}  is set to {{fail_on_write}} or {{error_on_write}}. If a master server becomes available when the session is in read-only mode, it should be possible to reconnect to the same master.

h2. Task Details

* Detect when new masters are available
* Refactor connection creation code to treat master and slave servers the same way",,0,0,0,0,0.0,"Master reconnection $end$ h2. Overview

If readwritesplit loses the connection to the master, it is possible for the client session to go into read-only mode if the {{master_failure_mode}}  is set to {{fail_on_write}} or {{error_on_write}}. If a master server becomes available when the session is in read-only mode, it should be possible to reconnect to the same master.

h2. Task Details

* Detect when new masters are available
* Refactor connection creation code to treat master and slave servers the same way $acceptance criteria:$",0,0,0,0,0,0,0,3373.23,32,5,0.15625,5,0.15625,5,0.15625,5,0.15625,5,0.15625
1030,MXS-1505,New Feature,MXS,2017-10-30 21:25:27,MXS-1501,0,Retry failed writes,"h2. Overview

If a write fails when autocommit is enabled and no transactions are open, it is safe to attempt a retry of the query if a replacement master server is available.

h2. Task Details

This task is mainly about refactoring the retrying code to handle multiple types of queries and targets. The rough outline of this task is:
* Store target type of the active query
* Abstract retrying by trying to find a server of the same target type
* Take the new code into use for both reads and writes",,"Retry failed writes $end$ h2. Overview

If a write fails when autocommit is enabled and no transactions are open, it is safe to attempt a retry of the query if a replacement master server is available.

h2. Task Details

This task is mainly about refactoring the retrying code to handle multiple types of queries and targets. The rough outline of this task is:
* Store target type of the active query
* Abstract retrying by trying to find a server of the same target type
* Take the new code into use for both reads and writes $acceptance criteria:$",,markus makela,markus makela,Major,2,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-04-03 09:21:53,Retry failed writes,"h2. Overview

If a write fails when autocommit is enabled and no transactions are open, it is safe to attempt a retry of the query if a replacement master server is available.

h2. Task Details

This task is mainly about refactoring the retrying code to handle multiple types of queries and targets. The rough outline of this task is:
* Store target type of the active query
* Abstract retrying by trying to find a server of the same target type
* Take the new code into use for both reads and writes",,0,0,0,0,0.0,"Retry failed writes $end$ h2. Overview

If a write fails when autocommit is enabled and no transactions are open, it is safe to attempt a retry of the query if a replacement master server is available.

h2. Task Details

This task is mainly about refactoring the retrying code to handle multiple types of queries and targets. The rough outline of this task is:
* Store target type of the active query
* Abstract retrying by trying to find a server of the same target type
* Take the new code into use for both reads and writes $acceptance criteria:$",0,0,0,0,0,0,0,3707.93,33,5,0.151515,5,0.151515,5,0.151515,5,0.151515,5,0.151515
1031,MXS-1506,New Feature,MXS,2017-10-30 21:31:25,MXS-1501,0,Delayed query retry,"h2. Overview

If the connection to the master server is lost but a replacement is not immediately available, the retrying of the failed query needs to be delayed. The delay should be minimal meaning that the retrying is done as soon as a new master server is available.

h2. Task Details

* Add a way for routers to get notified of new servers
* Create a mechanism that allows delayed execution of events
 
h2. Implementation Details

To keep the retrying mechanism simple, the routers are not notified of new events but attempt to route the query again until either the routing is successful or a timeout is hit. This removes the need for separate retrying logic as the normal routing logic covers both cases.",,"Delayed query retry $end$ h2. Overview

If the connection to the master server is lost but a replacement is not immediately available, the retrying of the failed query needs to be delayed. The delay should be minimal meaning that the retrying is done as soon as a new master server is available.

h2. Task Details

* Add a way for routers to get notified of new servers
* Create a mechanism that allows delayed execution of events
 
h2. Implementation Details

To keep the retrying mechanism simple, the routers are not notified of new events but attempt to route the query again until either the routing is successful or a timeout is hit. This removes the need for separate retrying logic as the normal routing logic covers both cases. $acceptance criteria:$",,markus makela,markus makela,Major,5,,0,1,1,2,0,1,0,,0,850,1,0,0,2018-03-20 10:26:36,Delayed query retry,"h2. Overview

If the connection to the master server is lost but a replacement is not immediately available, the retrying of the failed query needs to be delayed. The delay should be minimal meaning that the retrying is done as soon as a new master server is available.

h2. Task Details

* Add a way for routers to get notified of new servers
* Create a mechanism that allows delayed execution of events ",,0,1,0,51,0.64557,"Delayed query retry $end$ h2. Overview

If the connection to the master server is lost but a replacement is not immediately available, the retrying of the failed query needs to be delayed. The delay should be minimal meaning that the retrying is done as soon as a new master server is available.

h2. Task Details

* Add a way for routers to get notified of new servers
* Create a mechanism that allows delayed execution of events  $acceptance criteria:$",1,1,1,1,1,1,1,3372.92,34,5,0.147059,5,0.147059,5,0.147059,5,0.147059,5,0.147059
1032,MXS-1507,New Feature,MXS,2017-10-30 21:36:09,MXS-1501,0,Transaction replay,"h2. Overview

Open transactions can be replayed on a replacement server if the following criteria are met:
* Only {{SELECT ... FOR UPDATE}} reads have been done
* The contents of the {{SELECT ... FOR UPDATE}} statements are identical

This does not provide a 100% guarantee of an identical end result for the migration as the storage engine can alter the locking order of rows.

h2. Task Details

* Track transaction contents
* Calculate checksum/hash for {{SELECT ... FOR UPDATE}} results
* Replay executed queries

Some of the session command code can be reused or re-purposed to handle the execution of the commands.",,"Transaction replay $end$ h2. Overview

Open transactions can be replayed on a replacement server if the following criteria are met:
* Only {{SELECT ... FOR UPDATE}} reads have been done
* The contents of the {{SELECT ... FOR UPDATE}} statements are identical

This does not provide a 100% guarantee of an identical end result for the migration as the storage engine can alter the locking order of rows.

h2. Task Details

* Track transaction contents
* Calculate checksum/hash for {{SELECT ... FOR UPDATE}} results
* Replay executed queries

Some of the session command code can be reused or re-purposed to handle the execution of the commands. $acceptance criteria:$",,markus makela,markus makela,Major,9,,0,1,1,1,0,2,0,,0,850,1,0,0,2018-04-17 10:08:04,Transaction migration,"h2. Overview

Open transactions can be migrated to a replacement server if the following criteria are met:
* Only {{SELECT ... FOR UPDATE}} reads have been done
* The contents of the {{SELECT ... FOR UPDATE}} statements are identical

This does not provide a 100% guarantee of an identical end result for the migration as the storage engine can alter the locking order of rows.

h2. Task Details

* Track transaction contents
* Calculate checksum/hash for {{SELECT ... FOR UPDATE}} results
* Replay executed queries

Some of the session command code can be reused or re-purposed to handle the execution of the commands.",,1,1,0,6,0.0277778,"Transaction migration $end$ h2. Overview

Open transactions can be migrated to a replacement server if the following criteria are met:
* Only {{SELECT ... FOR UPDATE}} reads have been done
* The contents of the {{SELECT ... FOR UPDATE}} statements are identical

This does not provide a 100% guarantee of an identical end result for the migration as the storage engine can alter the locking order of rows.

h2. Task Details

* Track transaction contents
* Calculate checksum/hash for {{SELECT ... FOR UPDATE}} results
* Replay executed queries

Some of the session command code can be reused or re-purposed to handle the execution of the commands. $acceptance criteria:$",2,1,1,0,0,0,1,4044.52,35,6,0.171429,6,0.171429,6,0.171429,6,0.171429,6,0.171429
1033,MXS-151,Sub-Task,MXS,2015-05-15 16:28:51,,0,Transaction safety tests,,,Transaction safety tests $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,11,,0,2,0,2,0,0,0,,0,850,2,0,0,2015-05-15 16:28:51,Transaction safety tests,,,0,0,0,0,0.0,Transaction safety tests $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1034,MXS-1512,Task,MXS,2017-11-08 08:50:10,,0,Create test(s) for new cache behavior,,,Create test(s) for new cache behavior $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-11-08 08:50:10,Create test(s) for new cache behavior,,,0,0,0,0,0.0,Create test(s) for new cache behavior $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,209,24,0.114833,10,0.0478469,6,0.0287081,4,0.0191388,4,0.0191388
1035,MXS-1513,Task,MXS,2017-11-08 09:02:01,,0,Switchover,,,Switchover $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-11-08 09:02:01,Switchover,,,0,0,0,0,0.0,Switchover $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,210,24,0.114286,10,0.047619,6,0.0285714,4,0.0190476,4,0.0190476
1036,MXS-1514,Task,MXS,2017-11-08 10:41:50,,0,Create failover test cases,,,Create failover test cases $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-11-08 10:41:50,Create failover test cases,,,0,0,0,0,0.0,Create failover test cases $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,211,24,0.113744,10,0.0473934,6,0.028436,4,0.0189573,4,0.0189573
1037,MXS-1515,Task,MXS,2017-11-08 14:27:54,,0,Benchmark the delay that MaxScale adds to a request,"Benchmark the minimum delay that MaxScale adds, on average, for each request.
The setup should have a client application on one server with MariaDB running on another server. MaxScale should be installed on the same server as the client application.

The setups that should be benchmarked:

* Direct connection to database
* Connection through MaxScale with readconnroute
* Connection through MaxScale with readwritesplit",,"Benchmark the delay that MaxScale adds to a request $end$ Benchmark the minimum delay that MaxScale adds, on average, for each request.
The setup should have a client application on one server with MariaDB running on another server. MaxScale should be installed on the same server as the client application.

The setups that should be benchmarked:

* Direct connection to database
* Connection through MaxScale with readconnroute
* Connection through MaxScale with readwritesplit $acceptance criteria:$",,markus makela,markus makela,Major,7,,0,4,0,1,0,0,0,,0,850,3,0,0,2018-09-25 10:32:13,Benchmark the delay that MaxScale adds to a request,"Benchmark the minimum delay that MaxScale adds, on average, for each request.
The setup should have a client application on one server with MariaDB running on another server. MaxScale should be installed on the same server as the client application.

The setups that should be benchmarked:

* Direct connection to database
* Connection through MaxScale with readconnroute
* Connection through MaxScale with readwritesplit",,0,0,0,0,0.0,"Benchmark the delay that MaxScale adds to a request $end$ Benchmark the minimum delay that MaxScale adds, on average, for each request.
The setup should have a client application on one server with MariaDB running on another server. MaxScale should be installed on the same server as the client application.

The setups that should be benchmarked:

* Direct connection to database
* Connection through MaxScale with readconnroute
* Connection through MaxScale with readwritesplit $acceptance criteria:$",0,0,0,0,0,0,0,7700.07,36,7,0.194444,7,0.194444,6,0.166667,6,0.166667,6,0.166667
1038,MXS-152,Sub-Task,MXS,2015-05-15 16:29:38,,0,support to promote a slave to new master without touching any slaves: TESTING,,,support to promote a slave to new master without touching any slaves: TESTING $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,1,1,2,0,0,0,,0,850,1,0,0,2015-05-15 16:29:38,support to promote a slave to new master without touching any slaves: TESTING,,,0,0,0,0,0.0,support to promote a slave to new master without touching any slaves: TESTING $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1039,MXS-1520,Task,MXS,2017-11-15 12:01:21,,0,Integrate all build and test scripts and templates to Maxscale repo,"- get rid of 'build-scripts' repo
- move all build and test scripts and all MDBCI templates to Maxscale
- make scripts 'executable' without Jenkins and dirty hacks (only MDBCI is needed as pre-requrement)",,"Integrate all build and test scripts and templates to Maxscale repo $end$ - get rid of 'build-scripts' repo
- move all build and test scripts and all MDBCI templates to Maxscale
- make scripts 'executable' without Jenkins and dirty hacks (only MDBCI is needed as pre-requrement) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,2,0,0,3,,0,850,0,0,0,2017-11-21 10:56:07,Integrate all build and test scripts and templates to Maxscale repo,"- get rid of 'build-scripts' repo
- move all build and test scripts and all MDBCI templates to Maxscale
- make scripts 'executable' without Jenkins and dirty hacks (only MDBCI is needed as pre-requrement)",,0,0,0,0,0.0,"Integrate all build and test scripts and templates to Maxscale repo $end$ - get rid of 'build-scripts' repo
- move all build and test scripts and all MDBCI templates to Maxscale
- make scripts 'executable' without Jenkins and dirty hacks (only MDBCI is needed as pre-requrement) $acceptance criteria:$",0,0,0,0,0,0,1,142.9,40,1,0.025,0,0.0,0,0.0,0,0.0,0,0.0
1040,MXS-1521,Sub-Task,MXS,2017-11-15 12:02:11,,0,move build scripts and templates to Maxscale,,,move build scripts and templates to Maxscale $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-11-15 12:02:11,move build scripts and templates to Maxscale,,,0,0,0,0,0.0,move build scripts and templates to Maxscale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,41,1,0.0243902,0,0.0,0,0.0,0,0.0,0,0.0
1041,MXS-1522,Sub-Task,MXS,2017-11-15 12:02:56,,0,all needed environmental vars have to have default values in the scripts,,,all needed environmental vars have to have default values in the scripts $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-11-15 12:02:56,all needed environmental vars have to have default values in the scripts,,,0,0,0,0,0.0,all needed environmental vars have to have default values in the scripts $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,42,1,0.0238095,0,0.0,0,0.0,0,0.0,0,0.0
1042,MXS-1523,Sub-Task,MXS,2017-11-15 12:03:49,,0,all test scripts and templates to be moved to Maxscale,,,all test scripts and templates to be moved to Maxscale $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-11-15 12:03:49,all test scripts and templates to be moved to Maxscale,,,0,0,0,0,0.0,all test scripts and templates to be moved to Maxscale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,43,1,0.0232558,0,0.0,0,0.0,0,0.0,0,0.0
1043,MXS-153,Sub-Task,MXS,2015-05-15 16:32:09,,0,TEST: Incomplete binlog (when master crash),,,TEST: Incomplete binlog (when master crash) $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,2,0,2,0,0,0,,0,850,2,0,0,2015-05-15 16:32:09,TEST: Incomplete binlog (when master crash),,,0,0,0,0,0.0,TEST: Incomplete binlog (when master crash) $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1044,MXS-1530,Task,MXS,2017-11-20 08:00:47,,0,Check Binlog Server setup with MariaDB 10.2,Check whether Binlog Server is still working with MariaDB 10.2 due to new events added in 10.2,,Check Binlog Server setup with MariaDB 10.2 $end$ Check whether Binlog Server is still working with MariaDB 10.2 due to new events added in 10.2 $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Minor,15,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-11-21 09:19:18,Check Binlog Server setup with MariaDB 10.2,Check whether Binlog Server is still working with MariaDB 10.2 due to new events added in 10.2,,0,0,0,0,0.0,Check Binlog Server setup with MariaDB 10.2 $end$ Check whether Binlog Server is still working with MariaDB 10.2 due to new events added in 10.2 $acceptance criteria:$,0,0,0,0,0,0,0,25.3,21,1,0.047619,1,0.047619,1,0.047619,1,0.047619,1,0.047619
1045,MXS-1533,Task,MXS,2017-11-20 14:34:48,,0,Try to join standalone servers to a replication cluster.,"Should be enabled by a setting. If a START SLAVE has been attempted on a server, and it's sql-thread has stopped, don't try again as it probably ran into an error.",,"Try to join standalone servers to a replication cluster. $end$ Should be enabled by a setting. If a START SLAVE has been attempted on a server, and it's sql-thread has stopped, don't try again as it probably ran into an error. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Minor,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-11-20 14:34:48,Try to join standalone servers to a replication cluster.,"Should be enabled by a setting. If a START SLAVE has been attempted on a server, and it's sql-thread has stopped, don't try again as it probably ran into an error.",,0,0,0,0,0.0,"Try to join standalone servers to a replication cluster. $end$ Should be enabled by a setting. If a START SLAVE has been attempted on a server, and it's sql-thread has stopped, don't try again as it probably ran into an error. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
1046,MXS-1537,Task,MXS,2017-11-22 17:32:22,,0,Document MaxScale CDC Connector,Create documentation content(in gdoc) for maxscale-cdc-connector library similar to https://mariadb.com/kb/en/library/columnstore-bulk-write-sdk/,,Document MaxScale CDC Connector $end$ Create documentation content(in gdoc) for maxscale-cdc-connector library similar to https://mariadb.com/kb/en/library/columnstore-bulk-write-sdk/ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-02-13 10:25:47,Document MaxScale CDC Connector,Create documentation content(in gdoc) for maxscale-cdc-connector library similar to https://mariadb.com/kb/en/library/columnstore-bulk-write-sdk/,,0,0,0,0,0.0,Document MaxScale CDC Connector $end$ Create documentation content(in gdoc) for maxscale-cdc-connector library similar to https://mariadb.com/kb/en/library/columnstore-bulk-write-sdk/ $acceptance criteria:$,0,0,0,0,0,0,0,1984.88,25,5,0.2,2,0.08,0,0.0,0,0.0,0,0.0
1047,MXS-1538,Task,MXS,2017-11-22 17:41:52,,0,Integrate CDC Connector into MaxScale builds,The [MaxScale CDC Connector|https://github.com/mariadb-corporation/maxscale-cdc-connector]  should be integrated into MaxScale so that it follows the MaxScale versioning. ,,Integrate CDC Connector into MaxScale builds $end$ The [MaxScale CDC Connector|https://github.com/mariadb-corporation/maxscale-cdc-connector]  should be integrated into MaxScale so that it follows the MaxScale versioning.  $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-01-16 11:04:41,Integrate CDC Connector into MaxScale builds,The [MaxScale CDC Connector|https://github.com/mariadb-corporation/maxscale-cdc-connector]  should be integrated into MaxScale so that it follows the MaxScale versioning. ,,0,0,0,0,0.0,Integrate CDC Connector into MaxScale builds $end$ The [MaxScale CDC Connector|https://github.com/mariadb-corporation/maxscale-cdc-connector]  should be integrated into MaxScale so that it follows the MaxScale versioning.  $acceptance criteria:$,0,0,0,0,0,0,0,1313.37,37,7,0.189189,7,0.189189,6,0.162162,6,0.162162,6,0.162162
1048,MXS-1550,New Feature,MXS,2017-11-29 07:22:16,,0,Proxy side net_write_timeout and net_read_timeout,"Put a proxy(mxs) between client and db will let server  variables net_write_timeout and net_read_timeout not work， for example，if current client is hang，server's reponse cannot send to client, it will block on tcp send operation, with time gos on, it will trigger net_write_timeout to break this conneciion, but if put proxy(mxs) in the middle, if proxy is health ,server can always send data to proxy without block;

Proposal:  
  Use session track mechanism to track latest net_write_timeout/net_read_timeout value, and apply it in proxy side;
",,"Proxy side net_write_timeout and net_read_timeout $end$ Put a proxy(mxs) between client and db will let server  variables net_write_timeout and net_read_timeout not work， for example，if current client is hang，server's reponse cannot send to client, it will block on tcp send operation, with time gos on, it will trigger net_write_timeout to break this conneciion, but if put proxy(mxs) in the middle, if proxy is health ,server can always send data to proxy without block;

Proposal:  
  Use session track mechanism to track latest net_write_timeout/net_read_timeout value, and apply it in proxy side;
 $acceptance criteria:$",,dapeng huang,dapeng huang,Major,11,,0,3,0,2,0,0,0,,0,850,0,0,0,2019-05-13 10:38:50,Proxy side net_write_timeout and net_read_timeout,"Put a proxy(mxs) between client and db will let server  variables net_write_timeout and net_read_timeout not work， for example，if current client is hang，server's reponse cannot send to client, it will block on tcp send operation, with time gos on, it will trigger net_write_timeout to break this conneciion, but if put proxy(mxs) in the middle, if proxy is health ,server can always send data to proxy without block;

Proposal:  
  Use session track mechanism to track latest net_write_timeout/net_read_timeout value, and apply it in proxy side;
",,0,0,0,0,0.0,"Proxy side net_write_timeout and net_read_timeout $end$ Put a proxy(mxs) between client and db will let server  variables net_write_timeout and net_read_timeout not work， for example，if current client is hang，server's reponse cannot send to client, it will block on tcp send operation, with time gos on, it will trigger net_write_timeout to break this conneciion, but if put proxy(mxs) in the middle, if proxy is health ,server can always send data to proxy without block;

Proposal:  
  Use session track mechanism to track latest net_write_timeout/net_read_timeout value, and apply it in proxy side;
 $acceptance criteria:$",0,0,0,0,0,0,1,12723.3,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1049,MXS-1557,Task,MXS,2017-12-04 13:44:41,MXS-1556,0,Test: Automatic failover with one good candidate,"* Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism indeed chose the best one.
",,"Test: Automatic failover with one good candidate $end$ * Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism indeed chose the best one.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-12-05 13:00:43,Test: Automatic failover with one good candidate,"* Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism indeed chose the best one.
",,0,0,0,0,0.0,"Test: Automatic failover with one good candidate $end$ * Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism indeed chose the best one.
 $acceptance criteria:$",0,0,0,0,0,0,0,23.2667,212,24,0.113208,10,0.0471698,6,0.0283019,4,0.0188679,4,0.0188679
1050,MXS-1558,Task,MXS,2017-12-04 13:57:14,MXS-1556,0,Test: Automatic failover with two good candidates.,"* Arrange a number of slaves in a known good configuration with two equally good master candidates.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism chose one of those.
",,"Test: Automatic failover with two good candidates. $end$ * Arrange a number of slaves in a known good configuration with two equally good master candidates.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism chose one of those.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-12-05 13:00:57,Test: Automatic failover with two good candidates.,"* Arrange a number of slaves in a known good configuration with two equally good master candidates.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism chose one of those.
",,0,0,0,0,0.0,"Test: Automatic failover with two good candidates. $end$ * Arrange a number of slaves in a known good configuration with two equally good master candidates.
* Trigger the failover mechanism to promote a slave to master.
* Check that the master selection mechanism chose one of those.
 $acceptance criteria:$",0,0,0,0,0,0,0,23.05,213,24,0.112676,10,0.0469484,6,0.028169,4,0.0187793,4,0.0187793
1051,MXS-1559,Task,MXS,2017-12-04 13:58:53,MXS-1556,0,Test: Manual failover with one good candidate.,"* Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate failover to that one choice.
* Should succeed.
",,"Test: Manual failover with one good candidate. $end$ * Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate failover to that one choice.
* Should succeed.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2017-12-05 13:04:29,Test: Manual failover with one good candidate.,"* Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate failover to that one choice.
* Should succeed.
",,0,0,0,0,0.0,"Test: Manual failover with one good candidate. $end$ * Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate failover to that one choice.
* Should succeed.
 $acceptance criteria:$",0,0,0,0,0,0,0,23.0833,214,24,0.11215,10,0.046729,6,0.0280374,4,0.0186916,4,0.0186916
1052,MXS-1560,Task,MXS,2017-12-04 13:59:20,MXS-1556,0,Test: Manual failover with several good candidates,"* Arrange a number of slaves in a known good configuration, with two equally good master candidates.
* Manually initiate failover to one of those.
* Should succeed.
",,"Test: Manual failover with several good candidates $end$ * Arrange a number of slaves in a known good configuration, with two equally good master candidates.
* Manually initiate failover to one of those.
* Should succeed.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,1,0,,0,850,0,0,0,2017-12-05 13:04:29,Test: Manual failover with two good candidates,"* Arrange a number of slaves in a known good configuration, with two equally good master candidates.
* Manually initiate failover to one of those.
* Should succeed.
",,1,0,0,2,0.0263158,"Test: Manual failover with two good candidates $end$ * Arrange a number of slaves in a known good configuration, with two equally good master candidates.
* Manually initiate failover to one of those.
* Should succeed.
 $acceptance criteria:$",1,1,0,0,0,0,0,23.0833,215,24,0.111628,10,0.0465116,6,0.027907,4,0.0186047,4,0.0186047
1053,MXS-1561,Task,MXS,2017-12-04 13:59:50,MXS-1556,0,Test: Manual switchover with badly chosen master,"* Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate switchover to some other slave.
* Should fail.
",,"Test: Manual switchover with badly chosen master $end$ * Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate switchover to some other slave.
* Should fail.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,2,0,,0,850,0,0,0,2017-12-05 13:04:30,Test: Manual failover with badly chosen master,"* Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate failover to some other slave.
* Should fail.
",,1,1,0,4,0.0540541,"Test: Manual failover with badly chosen master $end$ * Arrange a number of slaves in a known good configuration, with a deterministic best choice.
* Manually initiate failover to some other slave.
* Should fail.
 $acceptance criteria:$",2,1,0,0,0,0,0,23.0667,216,25,0.115741,10,0.0462963,6,0.0277778,4,0.0185185,4,0.0185185
1054,MXS-1562,Task,MXS,2017-12-04 14:00:17,MXS-1556,0,Test: Switchover under ideal conditions,"* Create a master-slave replication setup.
* Generate some traffic and wait until all slaves are up to date.
* Initiate switchover to some slave.
* Verify that the specified slave is now the master and that the old master is a slave.
",,"Test: Switchover under ideal conditions $end$ * Create a master-slave replication setup.
* Generate some traffic and wait until all slaves are up to date.
* Initiate switchover to some slave.
* Verify that the specified slave is now the master and that the old master is a slave.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2017-12-05 13:03:30,Test: Switchover under ideal conditions,"* Create a master-slave replication setup.
* Generate some traffic and wait until all slaves are up to date.
* Initiate switchover to some slave.
* Verify that the specified slave is now the master and that the old master is a slave.
",,0,0,0,0,0.0,"Test: Switchover under ideal conditions $end$ * Create a master-slave replication setup.
* Generate some traffic and wait until all slaves are up to date.
* Initiate switchover to some slave.
* Verify that the specified slave is now the master and that the old master is a slave.
 $acceptance criteria:$",0,0,0,0,0,0,1,23.05,217,26,0.119816,10,0.0460829,6,0.0276498,4,0.0184332,4,0.0184332
1055,MXS-1563,Task,MXS,2017-12-04 14:01:09,MXS-1556,0,Test: Switchover under harder conditions,"* Create a master-slave replication setup.
* Generate some traffic and if possible arrange things so that slaves will lag behind the master.
* Start a read-write transaction.
* Initiate switchover to some slave.
* Check that any new update in the previously started transaction fails.
* Check that it no longer is possible to initiate new writes through MaxScale.
* Wait for the switchover operation to finish (the switchover mechanism should wait for the chosen slave to catch up, and then promote it to master).
* Verify that the specified slave is now the new master and that the old master is a slave.
",,"Test: Switchover under harder conditions $end$ * Create a master-slave replication setup.
* Generate some traffic and if possible arrange things so that slaves will lag behind the master.
* Start a read-write transaction.
* Initiate switchover to some slave.
* Check that any new update in the previously started transaction fails.
* Check that it no longer is possible to initiate new writes through MaxScale.
* Wait for the switchover operation to finish (the switchover mechanism should wait for the chosen slave to catch up, and then promote it to master).
* Verify that the specified slave is now the new master and that the old master is a slave.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,11,,0,0,0,3,0,0,0,,0,850,0,0,0,2017-12-05 13:03:43,Test: Switchover under harder conditions,"* Create a master-slave replication setup.
* Generate some traffic and if possible arrange things so that slaves will lag behind the master.
* Start a read-write transaction.
* Initiate switchover to some slave.
* Check that any new update in the previously started transaction fails.
* Check that it no longer is possible to initiate new writes through MaxScale.
* Wait for the switchover operation to finish (the switchover mechanism should wait for the chosen slave to catch up, and then promote it to master).
* Verify that the specified slave is now the new master and that the old master is a slave.
",,0,0,0,0,0.0,"Test: Switchover under harder conditions $end$ * Create a master-slave replication setup.
* Generate some traffic and if possible arrange things so that slaves will lag behind the master.
* Start a read-write transaction.
* Initiate switchover to some slave.
* Check that any new update in the previously started transaction fails.
* Check that it no longer is possible to initiate new writes through MaxScale.
* Wait for the switchover operation to finish (the switchover mechanism should wait for the chosen slave to catch up, and then promote it to master).
* Verify that the specified slave is now the new master and that the old master is a slave.
 $acceptance criteria:$",0,0,0,0,0,0,1,23.0333,218,26,0.119266,10,0.0458716,6,0.0275229,4,0.0183486,4,0.0183486
1056,MXS-1565,Task,MXS,2017-12-04 14:02:15,MXS-1556,0,Test: Rejoin rejected for old master,"* Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Take down all slaves.
* Generate some traffic on the master.
* Take down the master.
* Bring all slaves up.
* Allow the failover mechanism to do slave promotion.
* Raise the old master and verify that it is not rejoined.",,"Test: Rejoin rejected for old master $end$ * Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Take down all slaves.
* Generate some traffic on the master.
* Take down the master.
* Bring all slaves up.
* Allow the failover mechanism to do slave promotion.
* Raise the old master and verify that it is not rejoined. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,11,,0,1,0,2,0,1,0,,0,850,1,1,0,2017-12-19 11:21:49,Test: Rejoin rejected for old master,"* Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Take down all slaves.
* Generate some traffic on the master.
* Take down the master.
* Bring all slaves up.
* Allow the failover mechanism to do slave promotion.
* Raise the old master and verify that it is not rejoined.",,0,0,0,0,0.0,"Test: Rejoin rejected for old master $end$ * Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Take down all slaves.
* Generate some traffic on the master.
* Take down the master.
* Bring all slaves up.
* Allow the failover mechanism to do slave promotion.
* Raise the old master and verify that it is not rejoined. $acceptance criteria:$",0,0,0,0,0,0,1,357.317,219,26,0.118721,10,0.0456621,6,0.0273973,4,0.0182648,4,0.0182648
1057,MXS-1568,Task,MXS,2017-12-04 14:09:46,MXS-1556,0,Test: Rolling restart slaves,"* Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Stop a slave.
* Start the slave.
* Ensure that slave is rejoined.
* Perform this for every slave.
",,"Test: Rolling restart slaves $end$ * Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Stop a slave.
* Start the slave.
* Ensure that slave is rejoined.
* Perform this for every slave.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-12-19 11:20:45,Test: Rolling restart slaves,"* Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Stop a slave.
* Start the slave.
* Ensure that slave is rejoined.
* Perform this for every slave.
",,0,0,0,0,0.0,"Test: Rolling restart slaves $end$ * Create a master-slave replication setup.
* Generate some traffic and ensure that all slaves are up to date.
* Stop a slave.
* Start the slave.
* Ensure that slave is rejoined.
* Perform this for every slave.
 $acceptance criteria:$",0,0,0,0,0,0,0,357.167,220,26,0.118182,10,0.0454545,6,0.0272727,4,0.0181818,4,0.0181818
1058,MXS-1569,Task,MXS,2017-12-04 14:12:24,MXS-1556,0,"Test: Stress test, straightforward.","* Create a master-slave replication setup.
* Continuously generate traffic of varying kind.
* At regular intervals take down the current master and allow the failover mechanism to promote a new master.
* Raise the old master.
* Continue for the specified time or until there is only a master left.
* At the end, if the cluster is smaller than at the beginning, check that it is because all excluded servers could not be rejoined after having been raised.
",,"Test: Stress test, straightforward. $end$ * Create a master-slave replication setup.
* Continuously generate traffic of varying kind.
* At regular intervals take down the current master and allow the failover mechanism to promote a new master.
* Raise the old master.
* Continue for the specified time or until there is only a master left.
* At the end, if the cluster is smaller than at the beginning, check that it is because all excluded servers could not be rejoined after having been raised.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-01-02 10:17:21,"Test: Stress test, straightforward.","* Create a master-slave replication setup.
* Continuously generate traffic of varying kind.
* At regular intervals take down the current master and allow the failover mechanism to promote a new master.
* Raise the old master.
* Continue for the specified time or until there is only a master left.
* At the end, if the cluster is smaller than at the beginning, check that it is because all excluded servers could not be rejoined after having been raised.
",,0,0,0,0,0.0,"Test: Stress test, straightforward. $end$ * Create a master-slave replication setup.
* Continuously generate traffic of varying kind.
* At regular intervals take down the current master and allow the failover mechanism to promote a new master.
* Raise the old master.
* Continue for the specified time or until there is only a master left.
* At the end, if the cluster is smaller than at the beginning, check that it is because all excluded servers could not be rejoined after having been raised.
 $acceptance criteria:$",0,0,0,0,0,0,0,692.067,221,26,0.117647,10,0.0452489,6,0.0271493,4,0.0180995,4,0.0180995
1059,MXS-1570,Task,MXS,2017-12-04 14:22:12,MXS-1556,0,Test: Automatic failover with no good candidate,"* Arrange a number of slaves in a known configuration with no suitable master candidate.
* Trigger the failover mechanism to promote a slave to master.

Still open exactly which behaviour is desired. Do nothing and leave a set of read only slaves, or promote one slave and have only a single master and no slaves.
",,"Test: Automatic failover with no good candidate $end$ * Arrange a number of slaves in a known configuration with no suitable master candidate.
* Trigger the failover mechanism to promote a slave to master.

Still open exactly which behaviour is desired. Do nothing and leave a set of read only slaves, or promote one slave and have only a single master and no slaves.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-12-05 13:01:12,Test: Automatic failover with no good candidate,"* Arrange a number of slaves in a known configuration with no suitable master candidate.
* Trigger the failover mechanism to promote a slave to master.

Still open exactly which behaviour is desired. Do nothing and leave a set of read only slaves, or promote one slave and have only a single master and no slaves.
",,0,0,0,0,0.0,"Test: Automatic failover with no good candidate $end$ * Arrange a number of slaves in a known configuration with no suitable master candidate.
* Trigger the failover mechanism to promote a slave to master.

Still open exactly which behaviour is desired. Do nothing and leave a set of read only slaves, or promote one slave and have only a single master and no slaves.
 $acceptance criteria:$",0,0,0,0,0,0,0,22.65,222,26,0.117117,10,0.045045,6,0.027027,4,0.018018,4,0.018018
1060,MXS-1571,Task,MXS,2017-12-05 08:45:30,,0,Binlog Filter documentation,,,Binlog Filter documentation $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-12-05 12:55:48,Binlog Filter documentation,,,0,0,0,0,0.0,Binlog Filter documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,0,4.16667,22,1,0.0454545,1,0.0454545,1,0.0454545,1,0.0454545,1,0.0454545
1061,MXS-1572,Task,MXS,2017-12-05 08:47:11,,0,Binlog Events KB,,,Binlog Events KB $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2017-12-05 12:55:51,Binlog Events KB,,,0,0,0,0,0.0,Binlog Events KB $end$ $acceptance criteria:$,0,0,0,0,0,0,0,4.13333,23,1,0.0434783,1,0.0434783,1,0.0434783,1,0.0434783,1,0.0434783
1062,MXS-158,Task,MXS,2015-05-18 09:45:46,,0,Test Bin Log Server Phase 2,,,Test Bin Log Server Phase 2 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,20,,0,0,0,2,0,0,5,,0,850,0,0,0,2016-02-24 09:54:40,Test Bin Log Server Phase 2,,,0,0,0,0,0.0,Test Bin Log Server Phase 2 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,6768.13,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1063,MXS-1584,New Feature,MXS,2017-12-19 19:08:02,,0,Add SEQUENCE support,"The following is an excerpt from a MaxScale log.
{code}
2017-12-19 19:00:10   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select next value for seq1 
2017-12-19 19:00:10   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
2017-12-19 19:00:53   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ|QUERY_TYPE_WRITE, stmt: select nextval(seq1) 
2017-12-19 19:00:53   info   : (6) [readwritesplit] Route query to master       [192.168.121.57]:3306 <
2017-12-19 19:03:30   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select seq1.nextval 
2017-12-19 19:03:30   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
{code}

It shows that the logically equivalent statements are not handled in a consistent way. All of the statements should be classified as reads that must be sent to the master, the same way that {{@@last_insert_id}} is treated.",,"Add SEQUENCE support $end$ The following is an excerpt from a MaxScale log.
{code}
2017-12-19 19:00:10   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select next value for seq1 
2017-12-19 19:00:10   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
2017-12-19 19:00:53   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ|QUERY_TYPE_WRITE, stmt: select nextval(seq1) 
2017-12-19 19:00:53   info   : (6) [readwritesplit] Route query to master       [192.168.121.57]:3306 <
2017-12-19 19:03:30   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select seq1.nextval 
2017-12-19 19:03:30   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
{code}

It shows that the logically equivalent statements are not handled in a consistent way. All of the statements should be classified as reads that must be sent to the master, the same way that {{@@last_insert_id}} is treated. $acceptance criteria:$",,markus makela,markus makela,Major,6,,1,0,1,1,0,0,0,,0,850,0,0,0,2018-01-02 10:17:14,Add SEQUENCE support,"The following is an excerpt from a MaxScale log.
{code}
2017-12-19 19:00:10   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select next value for seq1 
2017-12-19 19:00:10   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
2017-12-19 19:00:53   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ|QUERY_TYPE_WRITE, stmt: select nextval(seq1) 
2017-12-19 19:00:53   info   : (6) [readwritesplit] Route query to master       [192.168.121.57]:3306 <
2017-12-19 19:03:30   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select seq1.nextval 
2017-12-19 19:03:30   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
{code}

It shows that the logically equivalent statements are not handled in a consistent way. All of the statements should be classified as reads that must be sent to the master, the same way that {{@@last_insert_id}} is treated.",,0,0,0,0,0.0,"Add SEQUENCE support $end$ The following is an excerpt from a MaxScale log.
{code}
2017-12-19 19:00:10   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select next value for seq1 
2017-12-19 19:00:10   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
2017-12-19 19:00:53   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ|QUERY_TYPE_WRITE, stmt: select nextval(seq1) 
2017-12-19 19:00:53   info   : (6) [readwritesplit] Route query to master       [192.168.121.57]:3306 <
2017-12-19 19:03:30   info   : (6) [readwritesplit] > Autocommit: [enabled], trx is [not open], cmd: (0x03) COM_QUERY, type: QUERY_TYPE_READ, stmt: select seq1.nextval 
2017-12-19 19:03:30   info   : (6) [readwritesplit] Route query to slave        [192.168.121.95]:3306 <
{code}

It shows that the logically equivalent statements are not handled in a consistent way. All of the statements should be classified as reads that must be sent to the master, the same way that {{@@last_insert_id}} is treated. $acceptance criteria:$",0,0,0,0,0,0,0,327.15,38,7,0.184211,7,0.184211,6,0.157895,6,0.157895,6,0.157895
1064,MXS-1598,New Feature,MXS,2018-01-09 11:32:40,,0,heartbeat replication don't support multimaster,"Maxscale configured with 


{code:java}
Service
router=readwritesplit
master_accept_reads=true
max_slave_replication_lag


Monitor
module=mysqlmon
multimaster=true

detect_replication_lag=true
{code}


Setup 
Master1 - Master 2 - Salve


{code:java}
2018-01-08 14:42:40   notice : Server changed state: db-node1[x.x.x.x1:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node2[x.x.x.2:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node3[X.X.X.3:3306]: new_slave. [Running] -> [Slave, Running]
{code}


Heartbeat tables will only be filled on Master1 (x.x.x.1) and Slave (x.x.x.3)

possible root cause:

heartbeat did not recognize master as a slave


{code:c++}
 if ((!SERVER_IN_MAINT(ptr->server)) && SERVER_IS_RUNNING(ptr->server))
                {
                    if (ptr->server->node_id != root_master->server->node_id &&
                        (SERVER_IS_SLAVE(ptr->server) ||
                         SERVER_IS_RELAY_SERVER(ptr->server)))
{code}
",,"heartbeat replication don't support multimaster $end$ Maxscale configured with 


{code:java}
Service
router=readwritesplit
master_accept_reads=true
max_slave_replication_lag


Monitor
module=mysqlmon
multimaster=true

detect_replication_lag=true
{code}


Setup 
Master1 - Master 2 - Salve


{code:java}
2018-01-08 14:42:40   notice : Server changed state: db-node1[x.x.x.x1:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node2[x.x.x.2:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node3[X.X.X.3:3306]: new_slave. [Running] -> [Slave, Running]
{code}


Heartbeat tables will only be filled on Master1 (x.x.x.1) and Slave (x.x.x.3)

possible root cause:

heartbeat did not recognize master as a slave


{code:c++}
 if ((!SERVER_IN_MAINT(ptr->server)) && SERVER_IS_RUNNING(ptr->server))
                {
                    if (ptr->server->node_id != root_master->server->node_id &&
                        (SERVER_IS_SLAVE(ptr->server) ||
                         SERVER_IS_RELAY_SERVER(ptr->server)))
{code}
 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,21,,0,1,0,4,0,0,0,,0,850,0,0,0,2018-09-25 07:22:35,heartbeat replication don't support multimaster,"Maxscale configured with 


{code:java}
Service
router=readwritesplit
master_accept_reads=true
max_slave_replication_lag


Monitor
module=mysqlmon
multimaster=true

detect_replication_lag=true
{code}


Setup 
Master1 - Master 2 - Salve


{code:java}
2018-01-08 14:42:40   notice : Server changed state: db-node1[x.x.x.x1:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node2[x.x.x.2:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node3[X.X.X.3:3306]: new_slave. [Running] -> [Slave, Running]
{code}


Heartbeat tables will only be filled on Master1 (x.x.x.1) and Slave (x.x.x.3)

possible root cause:

heartbeat did not recognize master as a slave


{code:c++}
 if ((!SERVER_IN_MAINT(ptr->server)) && SERVER_IS_RUNNING(ptr->server))
                {
                    if (ptr->server->node_id != root_master->server->node_id &&
                        (SERVER_IS_SLAVE(ptr->server) ||
                         SERVER_IS_RELAY_SERVER(ptr->server)))
{code}
",,0,0,0,0,0.0,"heartbeat replication don't support multimaster $end$ Maxscale configured with 


{code:java}
Service
router=readwritesplit
master_accept_reads=true
max_slave_replication_lag


Monitor
module=mysqlmon
multimaster=true

detect_replication_lag=true
{code}


Setup 
Master1 - Master 2 - Salve


{code:java}
2018-01-08 14:42:40   notice : Server changed state: db-node1[x.x.x.x1:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node2[x.x.x.2:3306]: new_master. [Running] -> [Master, Running]
2018-01-08 14:42:40   notice : Server changed state: db-node3[X.X.X.3:3306]: new_slave. [Running] -> [Slave, Running]
{code}


Heartbeat tables will only be filled on Master1 (x.x.x.1) and Slave (x.x.x.3)

possible root cause:

heartbeat did not recognize master as a slave


{code:c++}
 if ((!SERVER_IN_MAINT(ptr->server)) && SERVER_IS_RUNNING(ptr->server))
                {
                    if (ptr->server->node_id != root_master->server->node_id &&
                        (SERVER_IS_SLAVE(ptr->server) ||
                         SERVER_IS_RELAY_SERVER(ptr->server)))
{code}
 $acceptance criteria:$",0,0,0,0,0,0,1,6211.82,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1065,MXS-1599,Task,MXS,2018-01-09 12:48:17,,0,Add rejoin command to mariadbmon,"Just like failover can be invoked manually, it should be possible to invoke rejoin manually.",,"Add rejoin command to mariadbmon $end$ Just like failover can be invoked manually, it should be possible to invoke rejoin manually. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-01-16 08:41:54,Add rejoin command to mariadbmon,"Just like failover can be invoked manually, it should be possible to invoke rejoin manually.",,0,0,0,0,0.0,"Add rejoin command to mariadbmon $end$ Just like failover can be invoked manually, it should be possible to invoke rejoin manually. $acceptance criteria:$",0,0,0,0,0,0,0,163.883,223,26,0.116592,10,0.044843,6,0.0269058,4,0.0179372,4,0.0179372
1066,MXS-1607,Task,MXS,2018-01-16 08:40:19,,0,"Blog post: Managing MaxScale HA Cluster with MaxCtrl, and Keepalived",,,"Blog post: Managing MaxScale HA Cluster with MaxCtrl, and Keepalived $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,1,1,0,1,0,,0,850,0,0,0,2018-01-16 08:40:19,"Blog post: Managing MaxScale HA Clusterwith MaxCtrl, and Keepalived",,,1,0,0,3,0.166667,"Blog post: Managing MaxScale HA Clusterwith MaxCtrl, and Keepalived $end$ $acceptance criteria:$",1,1,0,0,0,0,0,0.0,224,26,0.116071,10,0.0446429,6,0.0267857,4,0.0178571,4,0.0178571
1067,MXS-1608,Task,MXS,2018-01-16 08:41:29,,0,Blog post: Pseudo-anonymization for GDPR compliance with MaxScale,,,Blog post: Pseudo-anonymization for GDPR compliance with MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-01-16 08:41:29,Blog post: Pseudo-anonymization for GDPR compliance with MaxScale,,,0,0,0,0,0.0,Blog post: Pseudo-anonymization for GDPR compliance with MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,225,27,0.12,10,0.0444444,6,0.0266667,4,0.0177778,4,0.0177778
1068,MXS-1611,Task,MXS,2018-01-16 10:55:31,,0,M18 MaxScale Architecture and Performance,,,M18 MaxScale Architecture and Performance $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,3,0,0,0,,0,850,0,0,0,2018-01-16 10:55:31,M18 MaxScale Architecture and Performance,,,0,0,0,0,0.0,M18 MaxScale Architecture and Performance $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,226,27,0.119469,10,0.0442478,6,0.0265487,4,0.0176991,4,0.0176991
1069,MXS-1612,Task,MXS,2018-01-16 10:56:02,,0,M18 Talk: Why Abstract Away the Underlying Database Infrastructure,,,M18 Talk: Why Abstract Away the Underlying Database Infrastructure $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-01-16 10:56:02,M18 Talk: Why Abstract Away the Underlying Database Infrastructure,,,0,0,0,0,0.0,M18 Talk: Why Abstract Away the Underlying Database Infrastructure $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,227,27,0.118943,10,0.0440529,6,0.0264317,4,0.0176211,4,0.0176211
1070,MXS-1622,Task,MXS,2018-01-22 13:40:17,,0,Add failover-ignored-servers to MariaDBMon,"The situation may be such that some servers must not be used as master, when performing a failover.

Add {{failover-ignored-servers}} configuration parameter using which such servers can be specified.",,"Add failover-ignored-servers to MariaDBMon $end$ The situation may be such that some servers must not be used as master, when performing a failover.

Add {{failover-ignored-servers}} configuration parameter using which such servers can be specified. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-01-30 10:17:05,Add failover-ignored-servers to MariaDBMon,"The situation may be such that some servers must not be used as master, when performing a failover.

Add {{failover-ignored-servers}} configuration parameter using which such servers can be specified.",,0,0,0,0,0.0,"Add failover-ignored-servers to MariaDBMon $end$ The situation may be such that some servers must not be used as master, when performing a failover.

Add {{failover-ignored-servers}} configuration parameter using which such servers can be specified. $acceptance criteria:$",0,0,0,0,0,0,0,188.6,228,27,0.118421,10,0.0438596,6,0.0263158,4,0.0175439,4,0.0175439
1071,MXS-1624,Task,MXS,2018-01-24 10:43:31,,0,Cache mapping from canonical statement -> query classification result,"# Canonicalize statement
# Is the canonicalized statement in the mapping.
# If so, use the result and route accordingly.
# If not, parse the original statement and store a mapping from canonicalize statement to query classification result.",,"Cache mapping from canonical statement -> query classification result $end$ # Canonicalize statement
# Is the canonicalized statement in the mapping.
# If so, use the result and route accordingly.
# If not, parse the original statement and store a mapping from canonicalize statement to query classification result. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,0,2,4,0,0,0,,0,850,0,0,0,2018-05-15 08:04:57,Cache mapping from canonical statement -> query classification result,"# Canonicalize statement
# Is the canonicalized statement in the mapping.
# If so, use the result and route accordingly.
# If not, parse the original statement and store a mapping from canonicalize statement to query classification result.",,0,0,0,0,0.0,"Cache mapping from canonical statement -> query classification result $end$ # Canonicalize statement
# Is the canonicalized statement in the mapping.
# If so, use the result and route accordingly.
# If not, parse the original statement and store a mapping from canonicalize statement to query classification result. $acceptance criteria:$",0,0,0,0,0,0,1,2661.35,229,27,0.117904,10,0.0436681,6,0.0262009,4,0.0174672,4,0.0174672
1072,MXS-1625,Task,MXS,2018-01-24 10:48:42,,0,Move all query classification into the query classifier.,"* Currently some query classification is performed by readwritesplit.
* All that classification should be moved inside the query classifier itself.
* Basically all that you need from the query classifier is whether a statement should be sent to the master, to some slave or to all servers.",,"Move all query classification into the query classifier. $end$ * Currently some query classification is performed by readwritesplit.
* All that classification should be moved inside the query classifier itself.
* Basically all that you need from the query classifier is whether a statement should be sent to the master, to some slave or to all servers. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,1,4,0,0,0,,0,850,0,0,0,2018-02-13 10:42:25,Move all query classification into the query classifier.,"* Currently some query classification is performed by readwritesplit.
* All that classification should be moved inside the query classifier itself.
* Basically all that you need from the query classifier is whether a statement should be sent to the master, to some slave or to all servers.",,0,0,0,0,0.0,"Move all query classification into the query classifier. $end$ * Currently some query classification is performed by readwritesplit.
* All that classification should be moved inside the query classifier itself.
* Basically all that you need from the query classifier is whether a statement should be sent to the master, to some slave or to all servers. $acceptance criteria:$",0,0,0,0,0,0,1,479.883,230,27,0.117391,10,0.0434783,6,0.026087,4,0.0173913,4,0.0173913
1073,MXS-1632,New Feature,MXS,2018-01-28 17:58:20,,0,Maxscale Workload Statistics By Server By readwrite available via REST API,"Enable statistics on workload by Server, by Readwritesplit via Maxscale Rest API.

The usecase for this would be for:

1) Be able to get workload information that could be rendered in a realtime monitoring tool/solution
2) Be leveraged to in some sort of external monitoring process that could determine whether additional nodes should be added to scale (i.e. autoscale)

Some thought will need to be put into things like:
 - Total Counts (by server by readwrite) - This would likely be easiest/fine for first release
 - Reset Total Counts (this could allow someone to manually reset the count or reset it automated every 5/10/30 minutes)
 - Some way to retrieve Counts for last 5,10,30,60 minutes/etc

",,"Maxscale Workload Statistics By Server By readwrite available via REST API $end$ Enable statistics on workload by Server, by Readwritesplit via Maxscale Rest API.

The usecase for this would be for:

1) Be able to get workload information that could be rendered in a realtime monitoring tool/solution
2) Be leveraged to in some sort of external monitoring process that could determine whether additional nodes should be added to scale (i.e. autoscale)

Some thought will need to be put into things like:
 - Total Counts (by server by readwrite) - This would likely be easiest/fine for first release
 - Reset Total Counts (this could allow someone to manually reset the count or reset it automated every 5/10/30 minutes)
 - Some way to retrieve Counts for last 5,10,30,60 minutes/etc

 $acceptance criteria:$",,Austin Rutherford,Austin Rutherford,Minor,10,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-08-29 09:42:22,Maxscale Workload Statistics By Server By readwrite available via REST API,"Enable statistics on workload by Server, by Readwritesplit via Maxscale Rest API.

The usecase for this would be for:

1) Be able to get workload information that could be rendered in a realtime monitoring tool/solution
2) Be leveraged to in some sort of external monitoring process that could determine whether additional nodes should be added to scale (i.e. autoscale)

Some thought will need to be put into things like:
 - Total Counts (by server by readwrite) - This would likely be easiest/fine for first release
 - Reset Total Counts (this could allow someone to manually reset the count or reset it automated every 5/10/30 minutes)
 - Some way to retrieve Counts for last 5,10,30,60 minutes/etc

",,0,0,0,0,0.0,"Maxscale Workload Statistics By Server By readwrite available via REST API $end$ Enable statistics on workload by Server, by Readwritesplit via Maxscale Rest API.

The usecase for this would be for:

1) Be able to get workload information that could be rendered in a realtime monitoring tool/solution
2) Be leveraged to in some sort of external monitoring process that could determine whether additional nodes should be added to scale (i.e. autoscale)

Some thought will need to be put into things like:
 - Total Counts (by server by readwrite) - This would likely be easiest/fine for first release
 - Reset Total Counts (this could allow someone to manually reset the count or reset it automated every 5/10/30 minutes)
 - Some way to retrieve Counts for last 5,10,30,60 minutes/etc

 $acceptance criteria:$",0,0,0,0,0,0,1,5103.73,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1074,MXS-1635,New Feature,MXS,2018-01-29 20:14:21,,0,Allow binding of the backend IP address,Provide a bind-address for backend connections to the MariaDB server.,,Allow binding of the backend IP address $end$ Provide a bind-address for backend connections to the MariaDB server. $acceptance criteria:$,,Kyle Joiner,Kyle Joiner,Minor,5,,0,3,0,1,0,0,0,,0,850,3,0,0,2018-01-30 10:22:42,Allow binding of the backend IP address,Provide a bind-address for backend connections to the MariaDB server.,,0,0,0,0,0.0,Allow binding of the backend IP address $end$ Provide a bind-address for backend connections to the MariaDB server. $acceptance criteria:$,0,0,0,0,0,0,0,14.1333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1075,MXS-1636,Task,MXS,2018-01-30 10:23:13,,0,M18 Failover presentation,,,M18 Failover presentation $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Trivial,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-01-30 10:25:53,M18 Failover presentation,,,0,0,0,0,0.0,M18 Failover presentation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0333333,2,1,0.5,0,0.0,0,0.0,0,0.0,0,0.0
1076,MXS-1638,Task,MXS,2018-01-30 17:07:39,,0,Add information about MaxCtrl and Cluster Synchronization to Keepalived document,Add information about MaxCtrl and cluster management to https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Tutorials/Failover-with-Keepalived.md,,Add information about MaxCtrl and Cluster Synchronization to Keepalived document $end$ Add information about MaxCtrl and cluster management to https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Tutorials/Failover-with-Keepalived.md $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,1,1,0,0,0,,0,850,1,0,0,2018-02-13 10:27:12,Add information about MaxCtrl and Cluster Synchronization to Keepalived document,Add information about MaxCtrl and cluster management to https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Tutorials/Failover-with-Keepalived.md,,0,0,0,0,0.0,Add information about MaxCtrl and Cluster Synchronization to Keepalived document $end$ Add information about MaxCtrl and cluster management to https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Tutorials/Failover-with-Keepalived.md $acceptance criteria:$,0,0,0,0,0,0,0,329.317,231,27,0.116883,10,0.04329,6,0.025974,4,0.017316,4,0.017316
1077,MXS-1639,New Feature,MXS,2018-01-30 17:32:45,,0,Perform SQL statements prior to client operations during failover,Provide a mechanism to perform SQL statements to a master prior to allowing client connections after promotion when performing a failover.  This will allow to change the value of log_slave_updates or enabling triggers and events.,,Perform SQL statements prior to client operations during failover $end$ Provide a mechanism to perform SQL statements to a master prior to allowing client connections after promotion when performing a failover.  This will allow to change the value of log_slave_updates or enabling triggers and events. $acceptance criteria:$,,Kyle Joiner,Kyle Joiner,Minor,16,,0,3,0,1,0,3,0,,0,850,3,3,0,2018-04-17 10:05:19,Perform SQL statements prior to client operations during failover,Provide a mechanism to perform SQL statements to a master prior to allowing client connections after promotion when performing a failover.  This will allow to change the value of log_slave_updates or enabling triggers and events.,,0,0,0,0,0.0,Perform SQL statements prior to client operations during failover $end$ Provide a mechanism to perform SQL statements to a master prior to allowing client connections after promotion when performing a failover.  This will allow to change the value of log_slave_updates or enabling triggers and events. $acceptance criteria:$,0,0,0,0,0,0,0,1840.53,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1078,MXS-1656,Task,MXS,2018-02-07 17:08:15,,0,Show GTID information in maxctrl or maxadmin list servers command,"Display current GTID and slave GTID in the  maxcrl or maxadmin ""list servers"" output for Master and slaves

See attached example.  !Screen Shot 2018-02-07 at 9.06.54 AM.png|! 

",,"Show GTID information in maxctrl or maxadmin list servers command $end$ Display current GTID and slave GTID in the  maxcrl or maxadmin ""list servers"" output for Master and slaves

See attached example.  !Screen Shot 2018-02-07 at 9.06.54 AM.png|! 

 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,16,,0,3,1,1,0,2,0,,0,850,1,2,0,2018-02-13 10:26:33,Show GTID information in maxctrl or maxadmin list servers command,"Display current GTID and slave GTID in the  maxcrl or maxadmin ""list servers"" output for Master and slaves

See attached example.  !Screen Shot 2018-02-07 at 9.06.54 AM.png|! 

",,0,0,0,0,0.0,"Show GTID information in maxctrl or maxadmin list servers command $end$ Display current GTID and slave GTID in the  maxcrl or maxadmin ""list servers"" output for Master and slaves

See attached example.  !Screen Shot 2018-02-07 at 9.06.54 AM.png|! 

 $acceptance criteria:$",0,0,0,0,0,0,0,137.3,26,5,0.192308,2,0.0769231,0,0.0,0,0.0,0,0.0
1079,MXS-1662,New Feature,MXS,2018-02-09 19:02:36,,0,Allow all users in a specific group to use MaxAdmin,"In MaxScale 2.1, Linux user accounts can be given access to maxadmin:

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#working-with-administration-interface-users

Some users would like this to be expanded, so that all Linux users within a specific Linux group could be given access to maxadmin. e.g. executing something like this would allow any user in the ""dba"" group to use MaxAdmin:

{noformat}
MaxScale> enable group dba
{noformat}

It would be preferable if this would also work for non-local groups pulled in from certain PAM plugins, such as those from Active Directory or LDAP.",,"Allow all users in a specific group to use MaxAdmin $end$ In MaxScale 2.1, Linux user accounts can be given access to maxadmin:

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#working-with-administration-interface-users

Some users would like this to be expanded, so that all Linux users within a specific Linux group could be given access to maxadmin. e.g. executing something like this would allow any user in the ""dba"" group to use MaxAdmin:

{noformat}
MaxScale> enable group dba
{noformat}

It would be preferable if this would also work for non-local groups pulled in from certain PAM plugins, such as those from Active Directory or LDAP. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,13,,0,8,0,4,0,0,0,,0,850,0,0,0,2019-03-18 11:43:16,Allow all users in a specific group to use MaxAdmin,"In MaxScale 2.1, Linux user accounts can be given access to maxadmin:

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#working-with-administration-interface-users

Some users would like this to be expanded, so that all Linux users within a specific Linux group could be given access to maxadmin. e.g. executing something like this would allow any user in the ""dba"" group to use MaxAdmin:

{noformat}
MaxScale> enable group dba
{noformat}

It would be preferable if this would also work for non-local groups pulled in from certain PAM plugins, such as those from Active Directory or LDAP.",,0,0,0,0,0.0,"Allow all users in a specific group to use MaxAdmin $end$ In MaxScale 2.1, Linux user accounts can be given access to maxadmin:

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-21-maxadmin-admin-interface/#working-with-administration-interface-users

Some users would like this to be expanded, so that all Linux users within a specific Linux group could be given access to maxadmin. e.g. executing something like this would allow any user in the ""dba"" group to use MaxAdmin:

{noformat}
MaxScale> enable group dba
{noformat}

It would be preferable if this would also work for non-local groups pulled in from certain PAM plugins, such as those from Active Directory or LDAP. $acceptance criteria:$",0,0,0,0,0,0,1,9640.67,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1080,MXS-1663,Task,MXS,2018-02-12 05:52:36,,0,MaxScale should detect conflicting configurations and refuse to start,"MaxScale should detect if two or more instances of MaxScale are running on the same computer and refuse to start if they are started in a way that causes conflicts between them.

Case in point; MXS-1661 describes a situation where when there was an authentication error, MaxScale would log {{database is locked}}. The reason was that two MaxScale instances were running; both using the same cache directory and thus the same sqlite user database.

When a second MaxScale instances is started, it should detect if there is such a conflict, log a descriptive error message, and refuse to start.",,"MaxScale should detect conflicting configurations and refuse to start $end$ MaxScale should detect if two or more instances of MaxScale are running on the same computer and refuse to start if they are started in a way that causes conflicts between them.

Case in point; MXS-1661 describes a situation where when there was an authentication error, MaxScale would log {{database is locked}}. The reason was that two MaxScale instances were running; both using the same cache directory and thus the same sqlite user database.

When a second MaxScale instances is started, it should detect if there is such a conflict, log a descriptive error message, and refuse to start. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,1,0,1,3,0,0,0,,0,850,0,0,0,2018-07-10 09:10:52,MaxScale should detect conflicting configurations and refuse to start,"MaxScale should detect if two or more instances of MaxScale are running on the same computer and refuse to start if they are started in a way that causes conflicts between them.

Case in point; MXS-1661 describes a situation where when there was an authentication error, MaxScale would log {{database is locked}}. The reason was that two MaxScale instances were running; both using the same cache directory and thus the same sqlite user database.

When a second MaxScale instances is started, it should detect if there is such a conflict, log a descriptive error message, and refuse to start.",,0,0,0,0,0.0,"MaxScale should detect conflicting configurations and refuse to start $end$ MaxScale should detect if two or more instances of MaxScale are running on the same computer and refuse to start if they are started in a way that causes conflicts between them.

Case in point; MXS-1661 describes a situation where when there was an authentication error, MaxScale would log {{database is locked}}. The reason was that two MaxScale instances were running; both using the same cache directory and thus the same sqlite user database.

When a second MaxScale instances is started, it should detect if there is such a conflict, log a descriptive error message, and refuse to start. $acceptance criteria:$",0,0,0,0,0,0,1,3555.3,232,27,0.116379,10,0.0431034,6,0.0258621,4,0.0172414,4,0.0172414
1081,MXS-1664,Task,MXS,2018-02-12 10:58:38,,0,Add --runtimedir config parameter for specifying relative location of all directories/files,"In case you want to run multiple MaxScale instances on one computer, it is necessary to explicitly specify numerous directories and files.

To make it easier to do that, there should be one flag using which (almost all) directorylocations can be specified. E.g.
{code}
maxscale --runtimedir=/maxscale/instance1
{code}
That the would case MaxScale to look for everything else relative to that directory
{code}
/maxscale/instance1/etc/maxscale.cnf
/maxscale/instance1/var/log/maxscale/maxscale.log
/maxscale/instance1/var/cache/maxscale/
...
{code}
This flag would be almost the same as {{-- basedir}}, the difference being that {{--basedir}} also specifies the location of the binary files. 

In practice,{{-- basedir}} is mostly useful for developers and other who may want to run multiple maxscale _versions_ concurrently or one after the other, while {{-- runtimedir}} would be useful for MaxScale end-users who want to run different MaxScale _instances_ (of the same version) concurrently.
",,"Add --runtimedir config parameter for specifying relative location of all directories/files $end$ In case you want to run multiple MaxScale instances on one computer, it is necessary to explicitly specify numerous directories and files.

To make it easier to do that, there should be one flag using which (almost all) directorylocations can be specified. E.g.
{code}
maxscale --runtimedir=/maxscale/instance1
{code}
That the would case MaxScale to look for everything else relative to that directory
{code}
/maxscale/instance1/etc/maxscale.cnf
/maxscale/instance1/var/log/maxscale/maxscale.log
/maxscale/instance1/var/cache/maxscale/
...
{code}
This flag would be almost the same as {{-- basedir}}, the difference being that {{--basedir}} also specifies the location of the binary files. 

In practice,{{-- basedir}} is mostly useful for developers and other who may want to run multiple maxscale _versions_ concurrently or one after the other, while {{-- runtimedir}} would be useful for MaxScale end-users who want to run different MaxScale _instances_ (of the same version) concurrently.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,1,0,1,1,0,0,0,,0,850,0,0,0,2018-06-26 09:28:51,Add --runtimedir config parameter for specifying relative location of all directories/files,"In case you want to run multiple MaxScale instances on one computer, it is necessary to explicitly specify numerous directories and files.

To make it easier to do that, there should be one flag using which (almost all) directorylocations can be specified. E.g.
{code}
maxscale --runtimedir=/maxscale/instance1
{code}
That the would case MaxScale to look for everything else relative to that directory
{code}
/maxscale/instance1/etc/maxscale.cnf
/maxscale/instance1/var/log/maxscale/maxscale.log
/maxscale/instance1/var/cache/maxscale/
...
{code}
This flag would be almost the same as {{-- basedir}}, the difference being that {{--basedir}} also specifies the location of the binary files. 

In practice,{{-- basedir}} is mostly useful for developers and other who may want to run multiple maxscale _versions_ concurrently or one after the other, while {{-- runtimedir}} would be useful for MaxScale end-users who want to run different MaxScale _instances_ (of the same version) concurrently.
",,0,0,0,0,0.0,"Add --runtimedir config parameter for specifying relative location of all directories/files $end$ In case you want to run multiple MaxScale instances on one computer, it is necessary to explicitly specify numerous directories and files.

To make it easier to do that, there should be one flag using which (almost all) directorylocations can be specified. E.g.
{code}
maxscale --runtimedir=/maxscale/instance1
{code}
That the would case MaxScale to look for everything else relative to that directory
{code}
/maxscale/instance1/etc/maxscale.cnf
/maxscale/instance1/var/log/maxscale/maxscale.log
/maxscale/instance1/var/cache/maxscale/
...
{code}
This flag would be almost the same as {{-- basedir}}, the difference being that {{--basedir}} also specifies the location of the binary files. 

In practice,{{-- basedir}} is mostly useful for developers and other who may want to run multiple maxscale _versions_ concurrently or one after the other, while {{-- runtimedir}} would be useful for MaxScale end-users who want to run different MaxScale _instances_ (of the same version) concurrently.
 $acceptance criteria:$",0,0,0,0,0,0,0,3214.5,233,27,0.11588,10,0.0429185,6,0.0257511,4,0.0171674,4,0.0171674
1082,MXS-168,Sub-Task,MXS,2015-05-22 10:29:00,,0,Add support for function blocking to dbfwfilter,"The dbfwfilter could deny functions like it denies columns. An example rule would be like this:

{noformat}
rule no_func deny functions user sum min max
{noformat}

This would block the user(), sum(), min() and max() functions.",,"Add support for function blocking to dbfwfilter $end$ The dbfwfilter could deny functions like it denies columns. An example rule would be like this:

{noformat}
rule no_func deny functions user sum min max
{noformat}

This would block the user(), sum(), min() and max() functions. $acceptance criteria:$",,markus makela,markus makela,Minor,14,,0,3,1,1,0,0,0,,0,850,3,0,0,2015-05-22 10:29:00,Add support for function blocking to dbfwfilter,"The dbfwfilter could deny functions like it denies columns. An example rule would be like this:

{noformat}
rule no_func deny functions user sum min max
{noformat}

This would block the user(), sum(), min() and max() functions.",,0,0,0,0,0.0,"Add support for function blocking to dbfwfilter $end$ The dbfwfilter could deny functions like it denies columns. An example rule would be like this:

{noformat}
rule no_func deny functions user sum min max
{noformat}

This would block the user(), sum(), min() and max() functions. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1083,MXS-1685,New Feature,MXS,2018-02-26 20:16:17,MXS-2530,0,Automatic Schema Generation,"It would be very handy if, when attempting to parse the mysqlbinlog to create CDC events, if a table was/is missing the .avro files, if maxscale could automatically connect to a read slave, and do a show create table to get the information to automatically generate the necessary files.

Currently, if a .avro file is missing or corrupt, maxscale will basically ignore those events until a table alter statement appears in the binlog. This is suboptimal in the sense that in the event of maxscale exiting unexpectedly, it may not resume operation in the fashion it is desired.

",,"Automatic Schema Generation $end$ It would be very handy if, when attempting to parse the mysqlbinlog to create CDC events, if a table was/is missing the .avro files, if maxscale could automatically connect to a read slave, and do a show create table to get the information to automatically generate the necessary files.

Currently, if a .avro file is missing or corrupt, maxscale will basically ignore those events until a table alter statement appears in the binlog. This is suboptimal in the sense that in the event of maxscale exiting unexpectedly, it may not resume operation in the fashion it is desired.

 $acceptance criteria:$",,Drew Schatt,Drew Schatt,Major,27,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-06-12 09:26:15,Automatic Schema Generation,"It would be very handy if, when attempting to parse the mysqlbinlog to create CDC events, if a table was/is missing the .avro files, if maxscale could automatically connect to a read slave, and do a show create table to get the information to automatically generate the necessary files.

Currently, if a .avro file is missing or corrupt, maxscale will basically ignore those events until a table alter statement appears in the binlog. This is suboptimal in the sense that in the event of maxscale exiting unexpectedly, it may not resume operation in the fashion it is desired.

",,0,0,0,0,0.0,"Automatic Schema Generation $end$ It would be very handy if, when attempting to parse the mysqlbinlog to create CDC events, if a table was/is missing the .avro files, if maxscale could automatically connect to a read slave, and do a show create table to get the information to automatically generate the necessary files.

Currently, if a .avro file is missing or corrupt, maxscale will basically ignore those events until a table alter statement appears in the binlog. This is suboptimal in the sense that in the event of maxscale exiting unexpectedly, it may not resume operation in the fashion it is desired.

 $acceptance criteria:$",0,0,0,0,0,0,1,2533.15,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1084,MXS-1686,New Feature,MXS,2018-02-26 20:19:47,MXS-2682,0,Direct Kafka Integration,"Currently, the CDC events are presented to the user via a particular channel, and a custom script has to pull those events, and then send them on to kafka (if using the kafka integration).

It would be ideal if maxscale could directly send these events directly to kafka. Necessary information would be the kafka cluster address, the zookeeper address(es), and what tables (via regex, ideally both white and black lists) that should be sent. 

In theory, this could happen on the fly, and completely skip writing these events to disk.

It would be nice if maxscale could use zookeeper to keep the current position of the last event sent, in order to not send duplicates after a crash.
",,"Direct Kafka Integration $end$ Currently, the CDC events are presented to the user via a particular channel, and a custom script has to pull those events, and then send them on to kafka (if using the kafka integration).

It would be ideal if maxscale could directly send these events directly to kafka. Necessary information would be the kafka cluster address, the zookeeper address(es), and what tables (via regex, ideally both white and black lists) that should be sent. 

In theory, this could happen on the fly, and completely skip writing these events to disk.

It would be nice if maxscale could use zookeeper to keep the current position of the last event sent, in order to not send duplicates after a crash.
 $acceptance criteria:$",,Drew Schatt,Drew Schatt,Major,15,,0,3,1,1,0,0,0,,0,850,3,0,0,2020-01-20 11:19:08,Direct Kafka Integration,"Currently, the CDC events are presented to the user via a particular channel, and a custom script has to pull those events, and then send them on to kafka (if using the kafka integration).

It would be ideal if maxscale could directly send these events directly to kafka. Necessary information would be the kafka cluster address, the zookeeper address(es), and what tables (via regex, ideally both white and black lists) that should be sent. 

In theory, this could happen on the fly, and completely skip writing these events to disk.

It would be nice if maxscale could use zookeeper to keep the current position of the last event sent, in order to not send duplicates after a crash.
",,0,0,0,0,0.0,"Direct Kafka Integration $end$ Currently, the CDC events are presented to the user via a particular channel, and a custom script has to pull those events, and then send them on to kafka (if using the kafka integration).

It would be ideal if maxscale could directly send these events directly to kafka. Necessary information would be the kafka cluster address, the zookeeper address(es), and what tables (via regex, ideally both white and black lists) that should be sent. 

In theory, this could happen on the fly, and completely skip writing these events to disk.

It would be nice if maxscale could use zookeeper to keep the current position of the last event sent, in order to not send duplicates after a crash.
 $acceptance criteria:$",0,0,0,0,0,0,0,16623.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1085,MXS-1687,New Feature,MXS,2018-02-26 20:22:57,MXS-2682,0,Kafka integration failover,"We have a need for highly reliable CDC events to be sent from maxscale to kafka (for billing reasons).

Currently, to protect against maxscale crashes (either the process or the whole machine), we would have to run multiple instances of maxscale, each of which would process the binlogs and send them to kafka, leading to duplicate events.

I believe that Marcus mentioned the possibility to have maxscale's cluster (which is aware of active/passive maxscale instances) be able to failover the processing. This would ideally tie in with the direct kafka integration.",,"Kafka integration failover $end$ We have a need for highly reliable CDC events to be sent from maxscale to kafka (for billing reasons).

Currently, to protect against maxscale crashes (either the process or the whole machine), we would have to run multiple instances of maxscale, each of which would process the binlogs and send them to kafka, leading to duplicate events.

I believe that Marcus mentioned the possibility to have maxscale's cluster (which is aware of active/passive maxscale instances) be able to failover the processing. This would ideally tie in with the direct kafka integration. $acceptance criteria:$",,Drew Schatt,Drew Schatt,Major,19,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-10-12 10:47:22,Kafka integration failover,"We have a need for highly reliable CDC events to be sent from maxscale to kafka (for billing reasons).

Currently, to protect against maxscale crashes (either the process or the whole machine), we would have to run multiple instances of maxscale, each of which would process the binlogs and send them to kafka, leading to duplicate events.

I believe that Marcus mentioned the possibility to have maxscale's cluster (which is aware of active/passive maxscale instances) be able to failover the processing. This would ideally tie in with the direct kafka integration.",,0,0,0,0,0.0,"Kafka integration failover $end$ We have a need for highly reliable CDC events to be sent from maxscale to kafka (for billing reasons).

Currently, to protect against maxscale crashes (either the process or the whole machine), we would have to run multiple instances of maxscale, each of which would process the binlogs and send them to kafka, leading to duplicate events.

I believe that Marcus mentioned the possibility to have maxscale's cluster (which is aware of active/passive maxscale instances) be able to failover the processing. This would ideally tie in with the direct kafka integration. $acceptance criteria:$",0,0,0,0,0,0,0,23006.4,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1086,MXS-1702,Task,MXS,2018-03-06 10:29:04,,0,Rewrite canonicalization code,Rewrite the code that canonicalisizes the SQL.,,Rewrite canonicalization code $end$ Rewrite the code that canonicalisizes the SQL. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,1,1,0,0,0,,0,850,1,0,0,2018-03-06 10:29:08,Rewrite canonicalization code,Rewrite the code that canonicalisizes the SQL.,,0,0,0,0,0.0,Rewrite canonicalization code $end$ Rewrite the code that canonicalisizes the SQL. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,39,7,0.179487,7,0.179487,6,0.153846,6,0.153846,6,0.153846
1087,MXS-1703,Task,MXS,2018-03-06 10:30:19,,0,Refactor Monitor Code,To develop branch.,,Refactor Monitor Code $end$ To develop branch. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,1,0,5,0,0,0,,0,850,1,0,0,2018-03-06 10:30:19,Refactor Monitor Code,To develop branch.,,0,0,0,0,0.0,Refactor Monitor Code $end$ To develop branch. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,234,27,0.115385,10,0.042735,6,0.025641,4,0.017094,4,0.017094
1088,MXS-1712,New Feature,MXS,2018-03-09 13:08:37,,0,Add mrm like bootstrap feature,"Add to Maxscale the MRM bootstrap feature , which resets all GTIDs.

The feature shall do the following.

Situation: Gtid:s are messed up but the actual data within the servers are in sync, or at least compatible.
Process:
1) RESET MASTER on all servers, this deletes binlogs and clears gtid_binlog_pos
2) RESET SLAVE on slaves so they forget their gtid positions. ""Slave"" here means any server trying to replicate from master.
3) Modify gtid_slave_pos as needed and set up replication
",,"Add mrm like bootstrap feature $end$ Add to Maxscale the MRM bootstrap feature , which resets all GTIDs.

The feature shall do the following.

Situation: Gtid:s are messed up but the actual data within the servers are in sync, or at least compatible.
Process:
1) RESET MASTER on all servers, this deletes binlogs and clears gtid_binlog_pos
2) RESET SLAVE on slaves so they forget their gtid positions. ""Slave"" here means any server trying to replicate from master.
3) Modify gtid_slave_pos as needed and set up replication
 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,14,,0,1,0,2,0,2,0,,0,850,0,0,0,2018-08-29 09:36:14,Add mrm like bootstrap feature,"Add to Maxscale the MRM bootstrap feature , which resets all GTIDs.

Here the MRM Go original code for this feature.

{code:go}

$scope.bootstrap = function() {
var r = confirm(""Bootstrap operation will destroy your existing replication setup. \n Are you really sure?"");
if (r == true) {
  var response = $http.get('/bootstrap');
  response.success(function(data, status, headers, config) {
      console.log(""Ok."");
    });

  response.error(function(data, status, headers, config) {
      console.log(""Error."");
    });
  }
};

$scope.gtidstring = function(arr) {
  var output = [];
  if (arr != null) {
    for (i = 0; i < arr.length; i++) {
      var gtid = """";
      gtid = arr[i][""DomainID""] + '-' + arr[i][""ServerID""] + '-' + arr[i][""SeqNo""];
      output.push(gtid)
    }
    return output.join("","");
  }
  return '';
};
}]);
{code}
",,0,2,0,164,0.827869,"Add mrm like bootstrap feature $end$ Add to Maxscale the MRM bootstrap feature , which resets all GTIDs.

Here the MRM Go original code for this feature.

{code:go}

$scope.bootstrap = function() {
var r = confirm(""Bootstrap operation will destroy your existing replication setup. \n Are you really sure?"");
if (r == true) {
  var response = $http.get('/bootstrap');
  response.success(function(data, status, headers, config) {
      console.log(""Ok."");
    });

  response.error(function(data, status, headers, config) {
      console.log(""Error."");
    });
  }
};

$scope.gtidstring = function(arr) {
  var output = [];
  if (arr != null) {
    for (i = 0; i < arr.length; i++) {
      var gtid = """";
      gtid = arr[i][""DomainID""] + '-' + arr[i][""ServerID""] + '-' + arr[i][""SeqNo""];
      output.push(gtid)
    }
    return output.join("","");
  }
  return '';
};
}]);
{code}
 $acceptance criteria:$",2,1,1,1,1,1,1,4148.45,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1089,MXS-1720,New Feature,MXS,2018-03-16 04:05:40,MXS-2532,0,Priori causal read,"Using master_wait_gitd(MXS-199)  to achieve causal read is not work so well when replication lag cannot ignore (such as > 100ms); We tested it  with 5.7 and turn on logical clock, and set innodb_flush_log_at_trx_commit = 1000 on replica, read write ratio is 5:1, the performance is no better than route to single node; Maybe Mysql 8.0's  Writeset-based replication will improve this, or Polardb's physical replication.

Because replication lag  has many influence factors，so we need another way to achieve causal read;

Our proposal:
1. Find a way to get latest gtid info of every replica;
     a. Regularly get it from monitor
     b. Update it when ok packet is received,  if it is newer than recorded one, update it using CAS;
2. Compare backend's gtid in proxy side;
",,"Priori causal read $end$ Using master_wait_gitd(MXS-199)  to achieve causal read is not work so well when replication lag cannot ignore (such as > 100ms); We tested it  with 5.7 and turn on logical clock, and set innodb_flush_log_at_trx_commit = 1000 on replica, read write ratio is 5:1, the performance is no better than route to single node; Maybe Mysql 8.0's  Writeset-based replication will improve this, or Polardb's physical replication.

Because replication lag  has many influence factors，so we need another way to achieve causal read;

Our proposal:
1. Find a way to get latest gtid info of every replica;
     a. Regularly get it from monitor
     b. Update it when ok packet is received,  if it is newer than recorded one, update it using CAS;
2. Compare backend's gtid in proxy side;
 $acceptance criteria:$",,dapeng huang,dapeng huang,Major,30,,1,3,3,2,0,0,0,,0,850,2,0,0,2019-10-28 11:25:42,Priori causal read,"Using master_wait_gitd(MXS-199)  to achieve causal read is not work so well when replication lag cannot ignore (such as > 100ms); We tested it  with 5.7 and turn on logical clock, and set innodb_flush_log_at_trx_commit = 1000 on replica, read write ratio is 5:1, the performance is no better than route to single node; Maybe Mysql 8.0's  Writeset-based replication will improve this, or Polardb's physical replication.

Because replication lag  has many influence factors，so we need another way to achieve causal read;

Our proposal:
1. Find a way to get latest gtid info of every replica;
     a. Regularly get it from monitor
     b. Update it when ok packet is received,  if it is newer than recorded one, update it using CAS;
2. Compare backend's gtid in proxy side;
",,0,0,0,0,0.0,"Priori causal read $end$ Using master_wait_gitd(MXS-199)  to achieve causal read is not work so well when replication lag cannot ignore (such as > 100ms); We tested it  with 5.7 and turn on logical clock, and set innodb_flush_log_at_trx_commit = 1000 on replica, read write ratio is 5:1, the performance is no better than route to single node; Maybe Mysql 8.0's  Writeset-based replication will improve this, or Polardb's physical replication.

Because replication lag  has many influence factors，so we need another way to achieve causal read;

Our proposal:
1. Find a way to get latest gtid info of every replica;
     a. Regularly get it from monitor
     b. Update it when ok packet is received,  if it is newer than recorded one, update it using CAS;
2. Compare backend's gtid in proxy side;
 $acceptance criteria:$",0,0,0,0,0,0,1,14191.3,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1090,MXS-1724,Task,MXS,2018-03-20 10:27:13,,0,Merge 2.1 test changes into 2.2,,,Merge 2.1 test changes into 2.2 $end$ $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-03-20 10:27:16,Merge 2.1 test changes into 2.2,,,0,0,0,0,0.0,Merge 2.1 test changes into 2.2 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,40,7,0.175,7,0.175,6,0.15,6,0.15,6,0.15
1091,MXS-1725,Task,MXS,2018-03-20 10:29:11,,0,"Allow ""terminating"" filters to reply via normal response processing chain.",,,"Allow ""terminating"" filters to reply via normal response processing chain. $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-03-20 10:29:11,"Allow ""terminating"" filters to reply via normal response processing chain.",,,0,0,0,0,0.0,"Allow ""terminating"" filters to reply via normal response processing chain. $end$ $acceptance criteria:$",0,0,0,0,0,0,0,0.0,235,27,0.114894,10,0.0425532,6,0.0255319,4,0.0170213,4,0.0170213
1092,MXS-1726,Task,MXS,2018-03-20 10:30:49,,0,integration of performance testing into build system,,,integration of performance testing into build system $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,1,0,3,0,0,5,,0,850,1,0,0,2018-03-20 10:30:53,integration of performance testing into build system,,,0,0,0,0,0.0,integration of performance testing into build system $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,44,1,0.0227273,0,0.0,0,0.0,0,0.0,0,0.0
1093,MXS-173,New Feature,MXS,2015-05-27 21:41:50,,0,Implement a query throttling filter,"A filter which would implement and improve upon the dbfwfilter's _limit_queries_ rule would be useful for situations where sudden traffic spikes cause a performance loss. This would also improve the quality of service by guaranteeing a consistent rate of queries to the backend instead of large spikes.

The dbfwfilter's _limit_queries_ rule is documented here: https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Filters/Database-Firewall-Filter.md#limit_queries",,"Implement a query throttling filter $end$ A filter which would implement and improve upon the dbfwfilter's _limit_queries_ rule would be useful for situations where sudden traffic spikes cause a performance loss. This would also improve the quality of service by guaranteeing a consistent rate of queries to the backend instead of large spikes.

The dbfwfilter's _limit_queries_ rule is documented here: https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Filters/Database-Firewall-Filter.md#limit_queries $acceptance criteria:$",,markus makela,markus makela,Major,15,,0,1,0,3,0,1,0,,0,850,0,1,0,2018-04-17 10:07:05,Implement a query throttling filter,"A filter which would implement and improve upon the dbfwfilter's _limit_queries_ rule would be useful for situations where sudden traffic spikes cause a performance loss. This would also improve the quality of service by guaranteeing a consistent rate of queries to the backend instead of large spikes.

The dbfwfilter's _limit_queries_ rule is documented here: https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Filters/Database-Firewall-Filter.md#limit_queries",,0,0,0,0,0.0,"Implement a query throttling filter $end$ A filter which would implement and improve upon the dbfwfilter's _limit_queries_ rule would be useful for situations where sudden traffic spikes cause a performance loss. This would also improve the quality of service by guaranteeing a consistent rate of queries to the backend instead of large spikes.

The dbfwfilter's _limit_queries_ rule is documented here: https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Filters/Database-Firewall-Filter.md#limit_queries $acceptance criteria:$",0,0,0,0,0,0,1,25332.4,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1094,MXS-1744,Task,MXS,2018-03-27 11:15:49,,0,The Gtid-class should save all triplets,"Currently it saves just one gtid-triplet, should save them all (1-2-3,2-3-4, etc) for future features.",,"The Gtid-class should save all triplets $end$ Currently it saves just one gtid-triplet, should save them all (1-2-3,2-3-4, etc) for future features. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Minor,11,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-03-27 11:19:11,The Gtid-class should save all triplets,"Currently it saves just one gtid-triplet, should save them all (1-2-3,2-3-4, etc) for future features.",,0,0,0,0,0.0,"The Gtid-class should save all triplets $end$ Currently it saves just one gtid-triplet, should save them all (1-2-3,2-3-4, etc) for future features. $acceptance criteria:$",0,0,0,0,0,0,1,0.05,3,1,0.333333,0,0.0,0,0.0,0,0.0,0,0.0
1095,MXS-1745,Task,MXS,2018-03-27 11:18:15,,0,Save all SlaveStatus rows when querying SHOW ALL SLAVES STATUS,All slave connection information should be saved to handle multimaster correctly.,,Save all SlaveStatus rows when querying SHOW ALL SLAVES STATUS $end$ All slave connection information should be saved to handle multimaster correctly. $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-04-17 10:07:11,Save all SlaveStatus rows when querying SHOW ALL SLAVES STATUS,All slave connection information should be saved to handle multimaster correctly.,,0,0,0,0,0.0,Save all SlaveStatus rows when querying SHOW ALL SLAVES STATUS $end$ All slave connection information should be saved to handle multimaster correctly. $acceptance criteria:$,0,0,0,0,0,0,1,502.8,4,1,0.25,0,0.0,0,0.0,0,0.0,0,0.0
1096,MXS-1753,Task,MXS,2018-04-03 09:12:55,MXS-1501,0,Create more master re-connection tests,"Implement tests for master reconnection.

* Failover with mariadbmon
* Switchover with mariadbmon",,"Create more master re-connection tests $end$ Implement tests for master reconnection.

* Failover with mariadbmon
* Switchover with mariadbmon $acceptance criteria:$",,markus makela,markus makela,Major,6,,0,0,1,1,0,1,0,,0,850,0,0,0,2018-04-03 09:12:55,Create more master re-connection tests,Implement tests for master reconnection.,,0,1,0,8,0.615385,Create more master re-connection tests $end$ Implement tests for master reconnection. $acceptance criteria:$,1,1,1,0,0,0,1,0.0,41,7,0.170732,7,0.170732,6,0.146341,6,0.146341,6,0.146341
1097,MXS-1754,Task,MXS,2018-04-03 09:18:26,,0,Generalize worker mechanism and MessageQueue for other threads than routing threads.,,,Generalize worker mechanism and MessageQueue for other threads than routing threads. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-04-03 09:18:26,Generalize worker mechanism and MessageQueue for other threads than routing threads.,,,0,0,0,0,0.0,Generalize worker mechanism and MessageQueue for other threads than routing threads. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,236,27,0.114407,10,0.0423729,6,0.0254237,4,0.0169492,4,0.0169492
1098,MXS-1763,Task,MXS,2018-04-06 12:18:11,,0,Move default location of maxadmin socket to /run/maxscale,"Some monitoring services managed via systemd may have privateTmp set to true (for good reasons, see https://access.redhat.com/blogs/766093/posts/1976243). It means they can not access maxadmin via default socket, /tmp/maxadmin.sock and may require additional configuration.

It would be useful to move socket to /run/maxadmin and change default value accordingly.",,"Move default location of maxadmin socket to /run/maxscale $end$ Some monitoring services managed via systemd may have privateTmp set to true (for good reasons, see https://access.redhat.com/blogs/766093/posts/1976243). It means they can not access maxadmin via default socket, /tmp/maxadmin.sock and may require additional configuration.

It would be useful to move socket to /run/maxadmin and change default value accordingly. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-07-31 10:52:39,Move default location of maxadmin socket to /run/maxscale,"Some monitoring services managed via systemd may have privateTmp set to true (for good reasons, see https://access.redhat.com/blogs/766093/posts/1976243). It means they can not access maxadmin via default socket, /tmp/maxadmin.sock and may require additional configuration.

It would be useful to move socket to /run/maxadmin and change default value accordingly.",,0,0,0,0,0.0,"Move default location of maxadmin socket to /run/maxscale $end$ Some monitoring services managed via systemd may have privateTmp set to true (for good reasons, see https://access.redhat.com/blogs/766093/posts/1976243). It means they can not access maxadmin via default socket, /tmp/maxadmin.sock and may require additional configuration.

It would be useful to move socket to /run/maxadmin and change default value accordingly. $acceptance criteria:$",0,0,0,0,0,0,0,2782.57,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1099,MXS-177,Sub-Task,MXS,2015-06-02 18:30:20,,0,Fixed Connection Throttling,"Hi,

it would be great to have a throttling feature in Maxscale, to avoid using too many connections in the backend, and to avoid sending connection errors to applications (up to some level)

Scenario:
MariadB> set global max_connections = 100;
maxadmin> set max_active_connections = 100;
maxadmin> set max_waiting_connections = 200;

When 100 connections are opened, the following will be waiting for a free slot, not returning max_connection error to client.

Thanks in advance!
Joffrey",,"Fixed Connection Throttling $end$ Hi,

it would be great to have a throttling feature in Maxscale, to avoid using too many connections in the backend, and to avoid sending connection errors to applications (up to some level)

Scenario:
MariadB> set global max_connections = 100;
maxadmin> set max_active_connections = 100;
maxadmin> set max_waiting_connections = 200;

When 100 connections are opened, the following will be waiting for a free slot, not returning max_connection error to client.

Thanks in advance!
Joffrey $acceptance criteria:$",,Joffrey MICHAIE,Joffrey MICHAIE,Major,21,,0,10,1,3,0,1,0,,0,850,3,0,0,2016-04-19 09:23:08,Connection Throttling,"Hi,

it would be great to have a throttling feature in Maxscale, to avoid using too many connections in the backend, and to avoid sending connection errors to applications (up to some level)

Scenario:
MariadB> set global max_connections = 100;
maxadmin> set max_active_connections = 100;
maxadmin> set max_waiting_connections = 200;

When 100 connections are opened, the following will be waiting for a free slot, not returning max_connection error to client.

Thanks in advance!
Joffrey",,1,0,0,1,0.0126582,"Connection Throttling $end$ Hi,

it would be great to have a throttling feature in Maxscale, to avoid using too many connections in the backend, and to avoid sending connection errors to applications (up to some level)

Scenario:
MariadB> set global max_connections = 100;
maxadmin> set max_active_connections = 100;
maxadmin> set max_waiting_connections = 200;

When 100 connections are opened, the following will be waiting for a free slot, not returning max_connection error to client.

Thanks in advance!
Joffrey $acceptance criteria:$",1,1,0,0,0,0,1,7718.87,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1100,MXS-1774,Task,MXS,2018-04-11 08:35:52,,0,Function usage with masked columns should automatically be masked.,,,Function usage with masked columns should automatically be masked. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-26 06:44:26,Function usage with masked columns should automatically be masked.,,,0,0,0,0,0.0,Function usage with masked columns should automatically be masked. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1822.13,237,27,0.113924,10,0.0421941,6,0.0253165,4,0.0168776,4,0.0168776
1101,MXS-1775,New Feature,MXS,2018-04-11 08:41:14,,0,Use disk-space information when making failover and routing decisions,,,Use disk-space information when making failover and routing decisions $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,24,,1,1,2,8,0,0,3,,0,850,1,0,0,2018-04-17 10:06:22,Use disk-space information when making failover and routing decisions,,,0,0,0,0,0.0,Use disk-space information when making failover and routing decisions $end$ $acceptance criteria:$,0,0,0,0,0,0,1,145.417,238,27,0.113445,10,0.0420168,6,0.0252101,4,0.0168067,4,0.0168067
1102,MXS-1777,New Feature,MXS,2018-04-11 09:51:08,,0,Adaptive routing based upon server load conditions,"Route queries based upon response time from servers. Prioritize based upon the amount of time it takes for a server to response to a query. The prioritization can be based upon

* The median or 95 percentile response time for all queries during a specified time period (e.g. 15 m).
* The median or 95 percentile response time for the canonical for of queries during a specified time period (e.g. 15 m).

The above relies upon the assumption that the queries are roughly alike over a certain perdio of time.
",,"Adaptive routing based upon server load conditions $end$ Route queries based upon response time from servers. Prioritize based upon the amount of time it takes for a server to response to a query. The prioritization can be based upon

* The median or 95 percentile response time for all queries during a specified time period (e.g. 15 m).
* The median or 95 percentile response time for the canonical for of queries during a specified time period (e.g. 15 m).

The above relies upon the assumption that the queries are roughly alike over a certain perdio of time.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,17,,0,0,0,10,0,0,0,,0,850,0,0,0,2018-05-16 10:31:47,Adaptive routing based upon server load conditions,"Route queries based upon response time from servers. Prioritize based upon the amount of time it takes for a server to response to a query. The prioritization can be based upon

* The median or 95 percentile response time for all queries during a specified time period (e.g. 15 m).
* The median or 95 percentile response time for the canonical for of queries during a specified time period (e.g. 15 m).

The above relies upon the assumption that the queries are roughly alike over a certain perdio of time.
",,0,0,0,0,0.0,"Adaptive routing based upon server load conditions $end$ Route queries based upon response time from servers. Prioritize based upon the amount of time it takes for a server to response to a query. The prioritization can be based upon

* The median or 95 percentile response time for all queries during a specified time period (e.g. 15 m).
* The median or 95 percentile response time for the canonical for of queries during a specified time period (e.g. 15 m).

The above relies upon the assumption that the queries are roughly alike over a certain perdio of time.
 $acceptance criteria:$",0,0,0,0,0,0,1,840.667,239,27,0.112971,10,0.041841,6,0.0251046,4,0.0167364,4,0.0167364
1103,MXS-1778,New Feature,MXS,2018-04-11 10:01:49,,0,Consistent reads by the utlization of GTID in OK packet.,"https://jira.mariadb.org/browse/MDEV-11956 will provide the last GTID in the OK packet. That can be utilized for providing consistent reads in the context of MaxScale and _readwritesplit_.

When e.g. an UPDATE is performed on the master, MaxScale will store the GTID. When a SELECT comes along, MaxScale could inject _SELECT MASTER_GTID_WAIT(xyz)_ before the actual _SELECT_, thus ensuring that whatever modifications the UPDATE caused, will be available when the actual _SELECT_ is executed on the chosen slave.",,"Consistent reads by the utlization of GTID in OK packet. $end$ https://jira.mariadb.org/browse/MDEV-11956 will provide the last GTID in the OK packet. That can be utilized for providing consistent reads in the context of MaxScale and _readwritesplit_.

When e.g. an UPDATE is performed on the master, MaxScale will store the GTID. When a SELECT comes along, MaxScale could inject _SELECT MASTER_GTID_WAIT(xyz)_ before the actual _SELECT_, thus ensuring that whatever modifications the UPDATE caused, will be available when the actual _SELECT_ is executed on the chosen slave. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,1,0,2,1,0,0,0,,0,850,0,0,0,2018-05-16 10:22:34,Consistent reads by the utlization of GTID in OK packet.,"https://jira.mariadb.org/browse/MDEV-11956 will provide the last GTID in the OK packet. That can be utilized for providing consistent reads in the context of MaxScale and _readwritesplit_.

When e.g. an UPDATE is performed on the master, MaxScale will store the GTID. When a SELECT comes along, MaxScale could inject _SELECT MASTER_GTID_WAIT(xyz)_ before the actual _SELECT_, thus ensuring that whatever modifications the UPDATE caused, will be available when the actual _SELECT_ is executed on the chosen slave.",,0,0,0,0,0.0,"Consistent reads by the utlization of GTID in OK packet. $end$ https://jira.mariadb.org/browse/MDEV-11956 will provide the last GTID in the OK packet. That can be utilized for providing consistent reads in the context of MaxScale and _readwritesplit_.

When e.g. an UPDATE is performed on the master, MaxScale will store the GTID. When a SELECT comes along, MaxScale could inject _SELECT MASTER_GTID_WAIT(xyz)_ before the actual _SELECT_, thus ensuring that whatever modifications the UPDATE caused, will be available when the actual _SELECT_ is executed on the chosen slave. $acceptance criteria:$",0,0,0,0,0,0,0,840.333,240,27,0.1125,10,0.0416667,6,0.025,4,0.0166667,4,0.0166667
1104,MXS-1780,New Feature,MXS,2018-04-11 10:10:30,,0,Provide access to current queries via REST-API,"‘SHOW MAXSCALE QUERIES’ - shows the list of queries currently running through MaxScale, the service it is associated with, filters on the service, the user@client sending the query, backend server where the query is running, start time when query was received. The ability to run this command shall be configurable.
",,"Provide access to current queries via REST-API $end$ ‘SHOW MAXSCALE QUERIES’ - shows the list of queries currently running through MaxScale, the service it is associated with, filters on the service, the user@client sending the query, backend server where the query is running, start time when query was received. The ability to run this command shall be configurable.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-09-12 10:31:27,Provide access to current queries via REST-API,"‘SHOW MAXSCALE QUERIES’ - shows the list of queries currently running through MaxScale, the service it is associated with, filters on the service, the user@client sending the query, backend server where the query is running, start time when query was received. The ability to run this command shall be configurable.
",,0,0,0,0,0.0,"Provide access to current queries via REST-API $end$ ‘SHOW MAXSCALE QUERIES’ - shows the list of queries currently running through MaxScale, the service it is associated with, filters on the service, the user@client sending the query, backend server where the query is running, start time when query was received. The ability to run this command shall be configurable.
 $acceptance criteria:$",0,0,0,0,0,0,0,3696.33,241,27,0.112033,10,0.0414938,6,0.0248963,4,0.0165975,4,0.0165975
1105,MXS-1782,New Feature,MXS,2018-04-11 10:23:27,,0,Expand MaxCtrl and REST-API,"MaxCtrl should be a complete replacement for maxadmin, but for maxadmin's capability to create users.
",,"Expand MaxCtrl and REST-API $end$ MaxCtrl should be a complete replacement for maxadmin, but for maxadmin's capability to create users.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,2,0,2,0,0,0,,0,850,2,0,0,2018-05-02 09:26:49,Expand MaxCtrl and REST-API,"MaxCtrl should be a complete replacement for maxadmin, but for maxadmin's capability to create users.
",,0,0,0,0,0.0,"Expand MaxCtrl and REST-API $end$ MaxCtrl should be a complete replacement for maxadmin, but for maxadmin's capability to create users.
 $acceptance criteria:$",0,0,0,0,0,0,1,503.05,242,27,0.11157,10,0.0413223,6,0.0247934,4,0.0165289,4,0.0165289
1106,MXS-1783,Task,MXS,2018-04-11 10:26:16,,0,Build qc_mysqlembedded using 10.4,Ensure that MaxScale supports the new features of 10.4. Likely to require changes in the parser.,,Build qc_mysqlembedded using 10.4 $end$ Ensure that MaxScale supports the new features of 10.4. Likely to require changes in the parser. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,0,0,1,0,1,0,,0,850,0,0,0,2019-05-27 10:30:51,Align with 10.4,Ensure that MaxScale supports the new features of 10.4. Likely to require changes in the parser.,,1,0,0,5,0.136364,Align with 10.4 $end$ Ensure that MaxScale supports the new features of 10.4. Likely to require changes in the parser. $acceptance criteria:$,1,1,1,0,0,0,1,9864.07,243,27,0.111111,10,0.0411523,6,0.0246914,4,0.0164609,4,0.0164609
1107,MXS-1789,Sub-Task,MXS,2018-04-13 09:09:58,,0,setup servers for performance testing,"install basic system and setup access keys, create config file on the Jenkins host",,"setup servers for performance testing $end$ install basic system and setup access keys, create config file on the Jenkins host $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,12,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-04-13 09:09:58,setup servers for performance testing,"install basic system and setup access keys, create config file on the Jenkins host",,0,0,0,0,0.0,"setup servers for performance testing $end$ install basic system and setup access keys, create config file on the Jenkins host $acceptance criteria:$",0,0,0,0,0,0,1,0.0,45,1,0.0222222,0,0.0,0,0.0,0,0.0,0,0.0
1108,MXS-1790,Sub-Task,MXS,2018-04-13 09:10:45,,0,test runs on new servers ,execute first performance tests on new servers using exiting Jenkins job,,test runs on new servers  $end$ execute first performance tests on new servers using exiting Jenkins job $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-04-13 09:10:45,test runs on new servers ,execute first performance tests on new servers using exiting Jenkins job,,0,0,0,0,0.0,test runs on new servers  $end$ execute first performance tests on new servers using exiting Jenkins job $acceptance criteria:$,0,0,0,0,0,0,1,0.0,46,1,0.0217391,0,0.0,0,0.0,0,0.0,0,0.0
1109,MXS-1791,Sub-Task,MXS,2018-04-13 09:11:54,,0,create performance test Jenkins job triggered by timer,,,create performance test Jenkins job triggered by timer $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-04-13 09:11:54,create performance test Jenkins job triggered by timer,,,0,0,0,0,0.0,create performance test Jenkins job triggered by timer $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,47,1,0.0212766,0,0.0,0,0.0,0,0.0,0,0.0
1110,MXS-1792,Sub-Task,MXS,2018-04-13 09:12:14,,0,create performance test Jenkins job triggered by push,,,create performance test Jenkins job triggered by push $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-04-13 09:12:14,create performance test Jenkins job triggered by push,,,0,0,0,0,0.0,create performance test Jenkins job triggered by push $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,48,1,0.0208333,0,0.0,0,0.0,0,0.0,0,0.0
1111,MXS-1793,Sub-Task,MXS,2018-04-13 09:14:12,,0,create temporal performance testin setup using -02 -03,"move all functional testing to -01
configure all performance test jobs to use -02 and -03",,"create temporal performance testin setup using -02 -03 $end$ move all functional testing to -01
configure all performance test jobs to use -02 and -03 $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2018-04-13 09:14:12,create temporal performance testin setup using -02 -03,"move all functional testing to -01
configure all performance test jobs to use -02 and -03",,0,0,0,0,0.0,"create temporal performance testin setup using -02 -03 $end$ move all functional testing to -01
configure all performance test jobs to use -02 and -03 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,49,1,0.0204082,0,0.0,0,0.0,0,0.0,0,0.0
1112,MXS-1794,Sub-Task,MXS,2018-04-13 09:15:12,,0,Test performance test result visualizer ,,,Test performance test result visualizer  $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-04-13 09:15:12,Test performance test result visualizer ,,,0,0,0,0,0.0,Test performance test result visualizer  $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,50,1,0.02,0,0.0,0,0.0,0,0.0,0,0.0
1113,MXS-1795,Sub-Task,MXS,2018-04-13 09:17:43,,0,final correction to performance test script,"- use proper my.cnf
- revise maxscale.cnf
- check Maxscale build job integration into performance test job",,"final correction to performance test script $end$ - use proper my.cnf
- revise maxscale.cnf
- check Maxscale build job integration into performance test job $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-04-13 09:17:43,final correction to performance test script,"- use proper my.cnf
- revise maxscale.cnf
- check Maxscale build job integration into performance test job",,0,0,0,0,0.0,"final correction to performance test script $end$ - use proper my.cnf
- revise maxscale.cnf
- check Maxscale build job integration into performance test job $acceptance criteria:$",0,0,0,0,0,0,1,0.0,51,1,0.0196078,0,0.0,0,0.0,0,0.0,0,0.0
1114,MXS-1796,Task,MXS,2018-04-13 09:19:47,,0,Integrate keepalive-based tests into regular runs,,,Integrate keepalive-based tests into regular runs $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-08-14 09:55:10,Integrate keepalive-based tests into regular runs,,,0,0,0,0,0.0,Integrate keepalive-based tests into regular runs $end$ $acceptance criteria:$,0,0,0,0,0,0,0,2952.58,52,1,0.0192308,0,0.0,0,0.0,0,0.0,0,0.0
1115,MXS-1797,Task,MXS,2018-04-13 09:21:07,,0,revise Jenkins jobs,"- remove non-actual jobs
- disable duplicated regular test runs",,"revise Jenkins jobs $end$ - remove non-actual jobs
- disable duplicated regular test runs $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-04-17 10:14:45,revise Jenkins jobs,"- remove non-actual jobs
- disable duplicated regular test runs",,0,0,0,0,0.0,"revise Jenkins jobs $end$ - remove non-actual jobs
- disable duplicated regular test runs $acceptance criteria:$",0,0,0,0,0,0,0,96.8833,53,1,0.0188679,0,0.0,0,0.0,0,0.0,0,0.0
1116,MXS-1799,Task,MXS,2018-04-13 14:05:03,,0,Add timestamps to retain_last_statements messages,Having timestamps with the SQL statements helps see if the session was idle when it was closed.,,Add timestamps to retain_last_statements messages $end$ Having timestamps with the SQL statements helps see if the session was idle when it was closed. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-06 11:50:05,Add timestamps to retain_last_statements messages,Having timestamps with the SQL statements helps see if the session was idle when it was closed.,,0,0,0,0,0.0,Add timestamps to retain_last_statements messages $end$ Having timestamps with the SQL statements helps see if the session was idle when it was closed. $acceptance criteria:$,0,0,0,0,0,0,0,9309.75,42,8,0.190476,8,0.190476,6,0.142857,6,0.142857,6,0.142857
1117,MXS-1809,Task,MXS,2018-04-17 09:01:28,,0,Provide timer mechanism in Worker,,,Provide timer mechanism in Worker $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-04-17 09:01:28,Provide timer mechanism in Worker,,,0,0,0,0,0.0,Provide timer mechanism in Worker $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,244,28,0.114754,11,0.045082,6,0.0245902,4,0.0163934,4,0.0163934
1118,MXS-1810,New Feature,MXS,2018-04-17 10:10:10,MXS-1501,0,Add resultset checksums into readwritesplit,,,Add resultset checksums into readwritesplit $end$ $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,1,1,1,0,0,0,,0,850,1,0,0,2018-04-17 10:10:10,Add resultset checksums into readwritesplit,,,0,0,0,0,0.0,Add resultset checksums into readwritesplit $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,43,8,0.186047,8,0.186047,6,0.139535,6,0.139535,6,0.139535
1119,MXS-1811,Task,MXS,2018-04-17 10:10:32,,0,Setup VM testing environment,,,Setup VM testing environment $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-04-17 10:10:32,Setup VM testing environment,,,0,0,0,0,0.0,Setup VM testing environment $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,245,28,0.114286,11,0.044898,6,0.0244898,4,0.0163265,4,0.0163265
1120,MXS-1812,Task,MXS,2018-04-17 10:11:06,,0,Create test for query throttling filter,,,Create test for query throttling filter $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,3,0,0,0,,0,850,0,0,0,2018-04-17 10:11:06,Create test for query throttling filter,,,0,0,0,0,0.0,Create test for query throttling filter $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,246,28,0.113821,11,0.0447154,6,0.0243902,4,0.0162602,4,0.0162602
1121,MXS-1840,Task,MXS,2018-05-02 08:25:29,,0,Compile all routers as C++.,,,Compile all routers as C++. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-05-02 08:25:29,Compile all routers as C++.,,,0,0,0,0,0.0,Compile all routers as C++. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,247,28,0.11336,11,0.0445344,6,0.0242915,4,0.0161943,4,0.0161943
1122,MXS-1841,Task,MXS,2018-05-02 08:25:38,,0,Compile all filters as C++.,,,Compile all filters as C++. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-05-02 08:25:38,Compile all filters as C++.,,,0,0,0,0,0.0,Compile all filters as C++. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,248,28,0.112903,11,0.0443548,6,0.0241935,4,0.016129,4,0.016129
1123,MXS-1842,Task,MXS,2018-05-02 08:26:06,,0,Compile all authenticators as C++.,,,Compile all authenticators as C++. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-05-02 08:26:06,Compile all authenticators as C++.,,,0,0,0,0,0.0,Compile all authenticators as C++. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,249,28,0.11245,11,0.0441767,6,0.0240964,4,0.0160643,4,0.0160643
1124,MXS-1844,Task,MXS,2018-05-02 09:10:13,,0,"Create test for ""show eventTimes""",,,"Create test for ""show eventTimes"" $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,1,0,1,1,0,0,0,,0,850,0,0,0,2018-05-02 09:10:13,"Create test for ""show eventTimes""",,,0,0,0,0,0.0,"Create test for ""show eventTimes"" $end$ $acceptance criteria:$",0,0,0,0,0,0,0,0.0,250,28,0.112,11,0.044,6,0.024,4,0.016,4,0.016
1125,MXS-1845,New Feature,MXS,2018-05-02 09:34:00,,0,Handle multiple slave connections for failover/switchover/rejoin,,,Handle multiple slave connections for failover/switchover/rejoin $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,21,,0,2,1,7,0,0,0,,0,850,2,0,0,2018-05-02 09:35:35,Handle multiple slave connections for failover/switchover/rejoin,,,0,0,0,0,0.0,Handle multiple slave connections for failover/switchover/rejoin $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,5,1,0.2,0,0.0,0,0.0,0,0.0,0,0.0
1126,MXS-1849,New Feature,MXS,2018-05-03 12:46:05,,0,Table Family Sharding Router,"Feature request:
MaxScale should be able to route a query coming from an application to appropriate server based on table name of in the query. 
",,"Table Family Sharding Router $end$ Feature request:
MaxScale should be able to route a query coming from an application to appropriate server based on table name of in the query. 
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,10,,0,1,0,2,0,0,0,,0,850,0,0,0,2018-06-12 08:11:13,Table Family Sharding Router,"Feature request:
MaxScale should be able to route a query coming from an application to appropriate server based on table name of in the query. 
",,0,0,0,0,0.0,"Table Family Sharding Router $end$ Feature request:
MaxScale should be able to route a query coming from an application to appropriate server based on table name of in the query. 
 $acceptance criteria:$",0,0,0,0,0,0,1,955.417,27,5,0.185185,2,0.0740741,0,0.0,0,0.0,0,0.0
1127,MXS-185,New Feature,MXS,2015-06-09 14:19:00,,0,"""read-only"" transactions to be directed to the slaves by the readwritesplit router","As per the recent status of the Current Release: 1.1.0 , MaxScale readwritesplit router routes all queries wrapped in a transaction to the master.

It will be good to detect the ""READ ONLY"" transactions and to be able to direct them to the available slaves. 

",,"""read-only"" transactions to be directed to the slaves by the readwritesplit router $end$ As per the recent status of the Current Release: 1.1.0 , MaxScale readwritesplit router routes all queries wrapped in a transaction to the master.

It will be good to detect the ""READ ONLY"" transactions and to be able to direct them to the available slaves. 

 $acceptance criteria:$",,Stoykov,Stoykov,Major,10,,1,5,1,1,0,0,0,,0,850,1,0,0,2017-01-18 10:20:56,"""read-only"" transactions to be directed to the slaves by the readwritesplit router","As per the recent status of the Current Release: 1.1.0 , MaxScale readwritesplit router routes all queries wrapped in a transaction to the master.

It will be good to detect the ""READ ONLY"" transactions and to be able to direct them to the available slaves. 

",,0,0,0,0,0.0,"""read-only"" transactions to be directed to the slaves by the readwritesplit router $end$ As per the recent status of the Current Release: 1.1.0 , MaxScale readwritesplit router routes all queries wrapped in a transaction to the master.

It will be good to detect the ""READ ONLY"" transactions and to be able to direct them to the available slaves. 

 $acceptance criteria:$",0,0,0,0,0,0,0,14132.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1128,MXS-1855,Task,MXS,2018-05-04 08:22:51,,0,BuildBot evaluation,"attempt to install BuildBot, add first build job, try to compare with Jenkins",,"BuildBot evaluation $end$ attempt to install BuildBot, add first build job, try to compare with Jenkins $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-05-16 10:37:01,BuildBot evaluation,"attempt to install BuildBot, add first build job, try to compare with Jenkins",,0,0,0,0,0.0,"BuildBot evaluation $end$ attempt to install BuildBot, add first build job, try to compare with Jenkins $acceptance criteria:$",0,0,0,0,0,0,1,290.233,54,1,0.0185185,0,0.0,0,0.0,0,0.0,0,0.0
1129,MXS-1859,New Feature,MXS,2018-05-07 12:17:27,,0,MariaDB Monitor should optionally enforce read-only on rejoining slave nodes,"On a server that freshly joins a cluster, but is not an active slave, the MariaBD
Monitor will perform ""SET GLOBAL read_only=1"" and ""START SLAVE"".

If that server reastarts later the MariaDB Monitor will take no action on it
though, as it detects that slave thread are already running, so that START SLAVE
is not needed.

So read_only will not be set either though, as SET GLOBAL settings do not persist
across server restarts, and such a slave will be running with read_only=0 then, 
unless read_only is also enforced via the servers option file.

In an ideal world MaxScale would maybe be able to perform persistent configuration
chagens to monitored servers, but as we don't have that yet (with the exception of
SET PERSISTENT in MySQL 8.0) MaxScale should at least optionally enforce read_only=1
on restarted slaves, e.g. with a ""force-slave-read-only=1"" monitor option.",,"MariaDB Monitor should optionally enforce read-only on rejoining slave nodes $end$ On a server that freshly joins a cluster, but is not an active slave, the MariaBD
Monitor will perform ""SET GLOBAL read_only=1"" and ""START SLAVE"".

If that server reastarts later the MariaDB Monitor will take no action on it
though, as it detects that slave thread are already running, so that START SLAVE
is not needed.

So read_only will not be set either though, as SET GLOBAL settings do not persist
across server restarts, and such a slave will be running with read_only=0 then, 
unless read_only is also enforced via the servers option file.

In an ideal world MaxScale would maybe be able to perform persistent configuration
chagens to monitored servers, but as we don't have that yet (with the exception of
SET PERSISTENT in MySQL 8.0) MaxScale should at least optionally enforce read_only=1
on restarted slaves, e.g. with a ""force-slave-read-only=1"" monitor option. $acceptance criteria:$",,Hartmut Holzgraefe,Hartmut Holzgraefe,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-05-16 07:22:36,MariaDB Monitor should optionally enforce read-only on rejoining slave nodes,"On a server that freshly joins a cluster, but is not an active slave, the MariaBD
Monitor will perform ""SET GLOBAL read_only=1"" and ""START SLAVE"".

If that server reastarts later the MariaDB Monitor will take no action on it
though, as it detects that slave thread are already running, so that START SLAVE
is not needed.

So read_only will not be set either though, as SET GLOBAL settings do not persist
across server restarts, and such a slave will be running with read_only=0 then, 
unless read_only is also enforced via the servers option file.

In an ideal world MaxScale would maybe be able to perform persistent configuration
chagens to monitored servers, but as we don't have that yet (with the exception of
SET PERSISTENT in MySQL 8.0) MaxScale should at least optionally enforce read_only=1
on restarted slaves, e.g. with a ""force-slave-read-only=1"" monitor option.",,0,0,0,0,0.0,"MariaDB Monitor should optionally enforce read-only on rejoining slave nodes $end$ On a server that freshly joins a cluster, but is not an active slave, the MariaBD
Monitor will perform ""SET GLOBAL read_only=1"" and ""START SLAVE"".

If that server reastarts later the MariaDB Monitor will take no action on it
though, as it detects that slave thread are already running, so that START SLAVE
is not needed.

So read_only will not be set either though, as SET GLOBAL settings do not persist
across server restarts, and such a slave will be running with read_only=0 then, 
unless read_only is also enforced via the servers option file.

In an ideal world MaxScale would maybe be able to perform persistent configuration
chagens to monitored servers, but as we don't have that yet (with the exception of
SET PERSISTENT in MySQL 8.0) MaxScale should at least optionally enforce read_only=1
on restarted slaves, e.g. with a ""force-slave-read-only=1"" monitor option. $acceptance criteria:$",0,0,0,0,0,0,0,211.083,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1130,MXS-1863,Task,MXS,2018-05-09 13:34:01,,0,local test config,"investigate the possibility to run all system tests without VMs, just running several MariaDB servers on different ports ",,"local test config $end$ investigate the possibility to run all system tests without VMs, just running several MariaDB servers on different ports  $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,16,,0,0,0,5,0,0,3,,0,850,0,0,0,2018-08-29 09:52:02,local test config,"investigate the possibility to run all system tests without VMs, just running several MariaDB servers on different ports ",,0,0,0,0,0.0,"local test config $end$ investigate the possibility to run all system tests without VMs, just running several MariaDB servers on different ports  $acceptance criteria:$",0,0,0,0,0,0,1,2684.3,55,1,0.0181818,0,0.0,0,0.0,0,0.0,0,0.0
1131,MXS-1868,Sub-Task,MXS,2018-05-16 10:18:45,,0,Deploy MDBCI from package,"- test new MDBCI package
- change test scripts in Maxscale
- deploy everything on max-tst-xx.mariadb.com",,"Deploy MDBCI from package $end$ - test new MDBCI package
- change test scripts in Maxscale
- deploy everything on max-tst-xx.mariadb.com $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,12,,0,1,0,5,0,0,0,,0,850,1,0,0,2018-05-16 10:37:12,Deploy MDBCI from package,"- test new MDBCI package
- change test scripts in Maxscale
- deploy everything on max-tst-xx.mariadb.com",,0,0,0,0,0.0,"Deploy MDBCI from package $end$ - test new MDBCI package
- change test scripts in Maxscale
- deploy everything on max-tst-xx.mariadb.com $acceptance criteria:$",0,0,0,0,0,0,1,0.3,56,1,0.0178571,0,0.0,0,0.0,0,0.0,0,0.0
1132,MXS-1869,Task,MXS,2018-05-16 10:28:34,,0,Deprecate throttling in FireWall filter.,,,Deprecate throttling in FireWall filter. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-05-16 10:28:34,Deprecate throttling in FireWall filter.,,,0,0,0,0,0.0,Deprecate throttling in FireWall filter. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,251,28,0.111554,11,0.0438247,6,0.0239044,4,0.0159363,4,0.0159363
1133,MXS-1870,Task,MXS,2018-05-16 10:37:58,,0,Include Ubuntu Bionic in the build system.,,,Include Ubuntu Bionic in the build system. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-05-16 10:37:58,Include Ubuntu Bionic in the build system.,,,0,0,0,0,0.0,Include Ubuntu Bionic in the build system. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,252,28,0.111111,11,0.0436508,6,0.0238095,4,0.015873,4,0.015873
1134,MXS-1881,Task,MXS,2018-05-22 17:17:02,,0,Refactor avrorouter,Refactor and clean up the avrorouter.,,Refactor avrorouter $end$ Refactor and clean up the avrorouter. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 09:21:18,Refactor avrorouter,Refactor and clean up the avrorouter.,,0,0,0,0,0.0,Refactor avrorouter $end$ Refactor and clean up the avrorouter. $acceptance criteria:$,0,0,0,0,0,0,0,496.067,44,8,0.181818,8,0.181818,6,0.136364,6,0.136364,6,0.136364
1135,MXS-1884,Task,MXS,2018-05-25 18:23:59,,0,"Document what ReadConnRoute's router_options ""list"" should look like","The MaxScale documentation for ReadConnRoute says the following:

{quote}
router_options can contain a list of valid server roles. These roles are used as the valid types of servers the router will form connections to when new sessions are created.

    router_options=slave

Here is a list of all possible values for the router_options.
{quote}

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-readconnroute/#router-options

As you can see, it says that router_options can refer to a list, but it does not say what that list should look like. I assume that it means that it should be a CSV, such as:

{noformat}
router_options=master,slave
{noformat}

If so, then the documentation should probably say so.",,"Document what ReadConnRoute's router_options ""list"" should look like $end$ The MaxScale documentation for ReadConnRoute says the following:

{quote}
router_options can contain a list of valid server roles. These roles are used as the valid types of servers the router will form connections to when new sessions are created.

    router_options=slave

Here is a list of all possible values for the router_options.
{quote}

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-readconnroute/#router-options

As you can see, it says that router_options can refer to a list, but it does not say what that list should look like. I assume that it means that it should be a CSV, such as:

{noformat}
router_options=master,slave
{noformat}

If so, then the documentation should probably say so. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 08:05:57,"Document what ReadConnRoute's router_options ""list"" should look like","The MaxScale documentation for ReadConnRoute says the following:

{quote}
router_options can contain a list of valid server roles. These roles are used as the valid types of servers the router will form connections to when new sessions are created.

    router_options=slave

Here is a list of all possible values for the router_options.
{quote}

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-readconnroute/#router-options

As you can see, it says that router_options can refer to a list, but it does not say what that list should look like. I assume that it means that it should be a CSV, such as:

{noformat}
router_options=master,slave
{noformat}

If so, then the documentation should probably say so.",,0,0,0,0,0.0,"Document what ReadConnRoute's router_options ""list"" should look like $end$ The MaxScale documentation for ReadConnRoute says the following:

{quote}
router_options can contain a list of valid server roles. These roles are used as the valid types of servers the router will form connections to when new sessions are created.

    router_options=slave

Here is a list of all possible values for the router_options.
{quote}

https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-readconnroute/#router-options

As you can see, it says that router_options can refer to a list, but it does not say what that list should look like. I assume that it means that it should be a CSV, such as:

{noformat}
router_options=master,slave
{noformat}

If so, then the documentation should probably say so. $acceptance criteria:$",0,0,0,0,0,0,0,421.683,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1136,MXS-1886,Task,MXS,2018-05-29 10:15:45,,0,Provide detailed information why a slave is not promoted to master,"In case of failover/switchover, if a slave cannot be promoted to master, MaxScale should log detailed information as to why that is the case, so that administrators become aware of the reason and thus also have something to work on.
",,"Provide detailed information why a slave is not promoted to master $end$ In case of failover/switchover, if a slave cannot be promoted to master, MaxScale should log detailed information as to why that is the case, so that administrators become aware of the reason and thus also have something to work on.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-06-12 08:06:12,Provide detailed information why a slave is not promoted to master,"In case of failover/switchover, if a slave cannot be promoted to master, MaxScale should log detailed information as to why that is the case, so that administrators become aware of the reason and thus also have something to work on.
",,0,0,0,0,0.0,"Provide detailed information why a slave is not promoted to master $end$ In case of failover/switchover, if a slave cannot be promoted to master, MaxScale should log detailed information as to why that is the case, so that administrators become aware of the reason and thus also have something to work on.
 $acceptance criteria:$",0,0,0,0,0,0,1,333.833,253,28,0.110672,11,0.0434783,6,0.0237154,4,0.0158103,4,0.0158103
1137,MXS-1892,New Feature,MXS,2018-06-01 08:01:09,MXS-2532,0,Support deprecate eof,"Since mxs can get many useful  info from server side var session track, 
but  without the feature deprecate eof, we can only get some info from ok packet;
if we can handle deprecate eof, we can get more info from server side;
For example, get lasted gtid from response of COM_QUERY, in the last eof packet;

https://dev.mysql.com/worklog/task/?id=7766",,"Support deprecate eof $end$ Since mxs can get many useful  info from server side var session track, 
but  without the feature deprecate eof, we can only get some info from ok packet;
if we can handle deprecate eof, we can get more info from server side;
For example, get lasted gtid from response of COM_QUERY, in the last eof packet;

https://dev.mysql.com/worklog/task/?id=7766 $acceptance criteria:$",,dapeng huang,dapeng huang,Major,37,,0,3,2,1,0,0,0,,0,850,2,0,0,2019-05-28 10:23:57,Support deprecate eof,"Since mxs can get many useful  info from server side var session track, 
but  without the feature deprecate eof, we can only get some info from ok packet;
if we can handle deprecate eof, we can get more info from server side;
For example, get lasted gtid from response of COM_QUERY, in the last eof packet;

https://dev.mysql.com/worklog/task/?id=7766",,0,0,0,0,0.0,"Support deprecate eof $end$ Since mxs can get many useful  info from server side var session track, 
but  without the feature deprecate eof, we can only get some info from ok packet;
if we can handle deprecate eof, we can get more info from server side;
For example, get lasted gtid from response of COM_QUERY, in the last eof packet;

https://dev.mysql.com/worklog/task/?id=7766 $acceptance criteria:$",0,0,0,0,0,0,0,8666.37,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1138,MXS-1895,New Feature,MXS,2018-06-02 12:24:35,MXS-2532,0,Readconnroute must show the number of routed queries per service per backend,"Folks,

Working with a reading pool of slaves, having three async slaves replicating from an individual MariaDB Cluster node, I am missing to have out the number of routed queries by backend out of the command show services, as you can see below:

{code:java}
[root@mxs01 ~]# maxadmin show service Read-Pool-Service
    Service:                             Read-Pool-Service
    Router:                              readconnroute
    State:                               Started
    Number of router sessions:       260
    Current no. of router sessions:    261
    Number of queries forwarded:       24368
    Started:                             Fri Jun  1 20:38:51 2018
    Root user access:                    Disabled
    Backend databases:
        [165.227.225.138]:3306 Protocol: MariaDBBackend    Name: async01
        [138.68.158.166]:3306  Protocol: MariaDBBackend    Name: async02
        [165.227.225.121]:3306 Protocol: MariaDBBackend    Name: async03
    Total connections:                   261
    Currently connected:                 261
{code}

I would like to see something like (the number of routed queries per service per backend):

{code:java}
Transactions routed:
 - Name: async01: 127
 - Name: async02: 243
 - Name: async03: 342
{code}
This is going to be of a great benefit for the sake of having good statistics for capacity planning and to understand better the routing decisions made by the readconnroute router for load balancing incoming traffic and when round-robing it. 

Thanks folks!",,"Readconnroute must show the number of routed queries per service per backend $end$ Folks,

Working with a reading pool of slaves, having three async slaves replicating from an individual MariaDB Cluster node, I am missing to have out the number of routed queries by backend out of the command show services, as you can see below:

{code:java}
[root@mxs01 ~]# maxadmin show service Read-Pool-Service
    Service:                             Read-Pool-Service
    Router:                              readconnroute
    State:                               Started
    Number of router sessions:       260
    Current no. of router sessions:    261
    Number of queries forwarded:       24368
    Started:                             Fri Jun  1 20:38:51 2018
    Root user access:                    Disabled
    Backend databases:
        [165.227.225.138]:3306 Protocol: MariaDBBackend    Name: async01
        [138.68.158.166]:3306  Protocol: MariaDBBackend    Name: async02
        [165.227.225.121]:3306 Protocol: MariaDBBackend    Name: async03
    Total connections:                   261
    Currently connected:                 261
{code}

I would like to see something like (the number of routed queries per service per backend):

{code:java}
Transactions routed:
 - Name: async01: 127
 - Name: async02: 243
 - Name: async03: 342
{code}
This is going to be of a great benefit for the sake of having good statistics for capacity planning and to understand better the routing decisions made by the readconnroute router for load balancing incoming traffic and when round-robing it. 

Thanks folks! $acceptance criteria:$",,Wagner Bianchi,Wagner Bianchi,Major,19,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-12-09 11:35:28,Readconnroute must show the number of routed queries per service per backend,"Folks,

Working with a reading pool of slaves, having three async slaves replicating from an individual MariaDB Cluster node, I am missing to have out the number of routed queries by backend out of the command show services, as you can see below:

{code:java}
[root@mxs01 ~]# maxadmin show service Read-Pool-Service
    Service:                             Read-Pool-Service
    Router:                              readconnroute
    State:                               Started
    Number of router sessions:       260
    Current no. of router sessions:    261
    Number of queries forwarded:       24368
    Started:                             Fri Jun  1 20:38:51 2018
    Root user access:                    Disabled
    Backend databases:
        [165.227.225.138]:3306 Protocol: MariaDBBackend    Name: async01
        [138.68.158.166]:3306  Protocol: MariaDBBackend    Name: async02
        [165.227.225.121]:3306 Protocol: MariaDBBackend    Name: async03
    Total connections:                   261
    Currently connected:                 261
{code}

I would like to see something like (the number of routed queries per service per backend):

{code:java}
Transactions routed:
 - Name: async01: 127
 - Name: async02: 243
 - Name: async03: 342
{code}
This is going to be of a great benefit for the sake of having good statistics for capacity planning and to understand better the routing decisions made by the readconnroute router for load balancing incoming traffic and when round-robing it. 

Thanks folks!",,0,0,0,0,0.0,"Readconnroute must show the number of routed queries per service per backend $end$ Folks,

Working with a reading pool of slaves, having three async slaves replicating from an individual MariaDB Cluster node, I am missing to have out the number of routed queries by backend out of the command show services, as you can see below:

{code:java}
[root@mxs01 ~]# maxadmin show service Read-Pool-Service
    Service:                             Read-Pool-Service
    Router:                              readconnroute
    State:                               Started
    Number of router sessions:       260
    Current no. of router sessions:    261
    Number of queries forwarded:       24368
    Started:                             Fri Jun  1 20:38:51 2018
    Root user access:                    Disabled
    Backend databases:
        [165.227.225.138]:3306 Protocol: MariaDBBackend    Name: async01
        [138.68.158.166]:3306  Protocol: MariaDBBackend    Name: async02
        [165.227.225.121]:3306 Protocol: MariaDBBackend    Name: async03
    Total connections:                   261
    Currently connected:                 261
{code}

I would like to see something like (the number of routed queries per service per backend):

{code:java}
Transactions routed:
 - Name: async01: 127
 - Name: async02: 243
 - Name: async03: 342
{code}
This is going to be of a great benefit for the sake of having good statistics for capacity planning and to understand better the routing decisions made by the readconnroute router for load balancing incoming traffic and when round-robing it. 

Thanks folks! $acceptance criteria:$",0,0,0,0,0,0,0,13319.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1139,MXS-1903,Sub-Task,MXS,2018-06-06 07:42:07,,0,Query disk space information and make it available to monitors,,,Query disk space information and make it available to monitors $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,8,0,0,0,,0,850,0,0,0,2018-06-06 07:42:07,Query disk space information and make it available to monitors,,,0,0,0,0,0.0,Query disk space information and make it available to monitors $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,254,28,0.110236,11,0.0433071,6,0.023622,4,0.015748,4,0.015748
1140,MXS-1904,Sub-Task,MXS,2018-06-06 07:45:30,,0,Add configuration parameter for low disk space action,"Add configuration parameter
{code}
switchover_on_low_disk_space=[true|false]
{code}
using which it can be specified what the monitor should do when the disk space gets low.",,"Add configuration parameter for low disk space action $end$ Add configuration parameter
{code}
switchover_on_low_disk_space=[true|false]
{code}
using which it can be specified what the monitor should do when the disk space gets low. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,0,0,8,0,0,0,,0,850,0,0,0,2018-06-06 07:45:30,Add configuration parameter for low disk space action,"Add configuration parameter
{code}
switchover_on_low_disk_space=[true|false]
{code}
using which it can be specified what the monitor should do when the disk space gets low.",,0,0,0,0,0.0,"Add configuration parameter for low disk space action $end$ Add configuration parameter
{code}
switchover_on_low_disk_space=[true|false]
{code}
using which it can be specified what the monitor should do when the disk space gets low. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,255,28,0.109804,11,0.0431373,6,0.0235294,4,0.0156863,4,0.0156863
1141,MXS-1905,Sub-Task,MXS,2018-06-06 08:32:08,,0,Take switchover_on_low_disk_space into account when making switchover decision,,,Take switchover_on_low_disk_space into account when making switchover decision $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,8,0,0,0,,0,850,0,0,0,2018-06-06 08:32:08,Take switchover_on_low_disk_space into account when making switchover decision,,,0,0,0,0,0.0,Take switchover_on_low_disk_space into account when making switchover decision $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,256,28,0.109375,11,0.0429688,6,0.0234375,4,0.015625,4,0.015625
1142,MXS-1908,Task,MXS,2018-06-08 11:47:10,,0,Replace HASHTABLE in log_manager.cc with unordered_map.,,,Replace HASHTABLE in log_manager.cc with unordered_map. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 07:09:04,Replace HASHTABLE in log_manager.cc with unordered_map.,,,0,0,0,0,0.0,Replace HASHTABLE in log_manager.cc with unordered_map. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,91.35,257,28,0.108949,11,0.0428016,6,0.0233463,4,0.0155642,4,0.0155642
1143,MXS-1914,Task,MXS,2018-06-12 07:05:08,,0,Move maxscale_shutdown into the core,"The maxscale_shutdown function should be defined in maxscale-common to be able to resolve all references at link time. Currently the administrative modules (cli, maxinfo) have undefined references at link time.",,"Move maxscale_shutdown into the core $end$ The maxscale_shutdown function should be defined in maxscale-common to be able to resolve all references at link time. Currently the administrative modules (cli, maxinfo) have undefined references at link time. $acceptance criteria:$",,markus makela,markus makela,Minor,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 09:21:34,Move maxscale_shutdown into the core,"The maxscale_shutdown function should be defined in maxscale-common to be able to resolve all references at link time. Currently the administrative modules (cli, maxinfo) have undefined references at link time.",,0,0,0,0,0.0,"Move maxscale_shutdown into the core $end$ The maxscale_shutdown function should be defined in maxscale-common to be able to resolve all references at link time. Currently the administrative modules (cli, maxinfo) have undefined references at link time. $acceptance criteria:$",0,0,0,0,0,0,0,2.26667,45,8,0.177778,8,0.177778,6,0.133333,6,0.133333,6,0.133333
1144,MXS-1915,Task,MXS,2018-06-12 07:47:40,,0,Move all monitors on top of maxscale::Worker,,,Move all monitors on top of maxscale::Worker $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 07:47:40,Move all monitors on top of maxscale::Worker,,,0,0,0,0,0.0,Move all monitors on top of maxscale::Worker $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,258,28,0.108527,11,0.0426357,6,0.0232558,4,0.0155039,4,0.0155039
1145,MXS-1917,Task,MXS,2018-06-12 08:15:43,,0,Remove HASHTABLE usage from config.cc,,,Remove HASHTABLE usage from config.cc $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 08:15:43,Remove HASHTABLE usage from config.cc,,,0,0,0,0,0.0,Remove HASHTABLE usage from config.cc $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,259,28,0.108108,11,0.042471,6,0.023166,4,0.015444,4,0.015444
1146,MXS-1918,Task,MXS,2018-06-12 08:17:06,,0,Remove HASHTABLE usage from dcb.cc,,,Remove HASHTABLE usage from dcb.cc $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-06-12 08:17:06,Remove HASHTABLE usage from dcb.cc,,,0,0,0,0,0.0,Remove HASHTABLE usage from dcb.cc $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,260,28,0.107692,11,0.0423077,6,0.0230769,4,0.0153846,4,0.0153846
1147,MXS-1919,Task,MXS,2018-06-12 09:26:21,,0,BuildBot deployment,,,BuildBot deployment $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-06-13 05:01:19,BuildBot deployment,,,0,0,0,0,0.0,BuildBot deployment $end$ $acceptance criteria:$,0,0,0,0,0,0,1,19.5667,57,1,0.0175439,0,0.0,0,0.0,0,0.0,0,0.0
1148,MXS-192,Task,MXS,2015-06-12 17:17:34,,0,Test Configuration synch across nodes,Test MXS-179 that uses lsyncd to keep configuration changes across MaxScale nodes in sync.,,Test Configuration synch across nodes $end$ Test MXS-179 that uses lsyncd to keep configuration changes across MaxScale nodes in sync. $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Minor,19,,0,7,1,1,0,2,0,,0,850,7,0,0,2016-02-24 09:54:48,Test Configuration synch across nodes,Test MXS-179 that uses lsynchd to keep configuration changes across MaxScale nodes in synch,,0,2,0,4,0.0909091,Test Configuration synch across nodes $end$ Test MXS-179 that uses lsynchd to keep configuration changes across MaxScale nodes in synch $acceptance criteria:$,2,1,0,0,0,0,0,6160.62,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1149,MXS-1923,New Feature,MXS,2018-06-13 15:24:04,MXS-2531,0,The Desynced state of Galera Node is not given in maxctrl server list,"The Desynced state of Galera Node is not given in maxctrl server list

When Galera Node was desynced  from the group with setting up wsrep_desync,
the maxctrl  is displaying just  Running.
It would be more clear if give in addition the Desynced state in the  maxctrl server list.





on server2  set wsrep_desync and FTWRL
check that Node gets in Desynced state 
check  the State of server2  from MaxScale Node  via maxctrl list  servers

{noformat}
MariaDB [(none)]> set global wsrep_desync=1 ;
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]> flush tables with read lock ;
Query OK, 0 rows affected (0.000 sec)
{noformat}


check locally on server2 the wsrep state

{noformat}

MariaDB [(none)]> show status like ""%state%"" ;
+-------------------------------------------+--------------------------------------+
| Variable_name                             | Value                                |
+-------------------------------------------+--------------------------------------+
| Max_statement_time_exceeded               | 0                                    |
| Performance_schema_statement_classes_lost | 0                                    |
| wsrep_cluster_state_uuid                  | 3c15149f-5766-11e8-9a99-22bc53d40581 |
| wsrep_evs_state                           | OPERATIONAL                          |
| wsrep_local_state                         | 2                                    |
| wsrep_local_state_comment                 | Donor/Desynced                       |
| wsrep_local_state_uuid                    | 3c15149f-5766-11e8-9a99-22bc53d40581 |
+-------------------------------------------+--------------------------------------+
7 rows in set (0.001 sec)

{noformat}



 The wsrep Desynced  state of server is not displayed from the MaxScale Node
{noformat}
[root@t4w1 /]#  maxctrl list  servers
┌─────────┬─────────────────┬──────┬─────────────┬─────────────────────────┬──────┐
│ Server  │ Address         │ Port │ Connections │ State                   │ GTID │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server1 │ 192.168.104.193 │ 3306 │ 0           │ Master, Synced, Running │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server2 │ 192.168.104.195 │ 3306 │ 0           │ Running                 │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server3 │ 192.168.104.196 │ 3306 │ 0           │ Slave, Synced, Running  │      │
└─────────┴─────────────────┴──────┴─────────────┴─────────────────────────┴──────┘

{noformat}


",,"The Desynced state of Galera Node is not given in maxctrl server list $end$ The Desynced state of Galera Node is not given in maxctrl server list

When Galera Node was desynced  from the group with setting up wsrep_desync,
the maxctrl  is displaying just  Running.
It would be more clear if give in addition the Desynced state in the  maxctrl server list.





on server2  set wsrep_desync and FTWRL
check that Node gets in Desynced state 
check  the State of server2  from MaxScale Node  via maxctrl list  servers

{noformat}
MariaDB [(none)]> set global wsrep_desync=1 ;
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]> flush tables with read lock ;
Query OK, 0 rows affected (0.000 sec)
{noformat}


check locally on server2 the wsrep state

{noformat}

MariaDB [(none)]> show status like ""%state%"" ;
+-------------------------------------------+--------------------------------------+
| Variable_name                             | Value                                |
+-------------------------------------------+--------------------------------------+
| Max_statement_time_exceeded               | 0                                    |
| Performance_schema_statement_classes_lost | 0                                    |
| wsrep_cluster_state_uuid                  | 3c15149f-5766-11e8-9a99-22bc53d40581 |
| wsrep_evs_state                           | OPERATIONAL                          |
| wsrep_local_state                         | 2                                    |
| wsrep_local_state_comment                 | Donor/Desynced                       |
| wsrep_local_state_uuid                    | 3c15149f-5766-11e8-9a99-22bc53d40581 |
+-------------------------------------------+--------------------------------------+
7 rows in set (0.001 sec)

{noformat}



 The wsrep Desynced  state of server is not displayed from the MaxScale Node
{noformat}
[root@t4w1 /]#  maxctrl list  servers
┌─────────┬─────────────────┬──────┬─────────────┬─────────────────────────┬──────┐
│ Server  │ Address         │ Port │ Connections │ State                   │ GTID │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server1 │ 192.168.104.193 │ 3306 │ 0           │ Master, Synced, Running │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server2 │ 192.168.104.195 │ 3306 │ 0           │ Running                 │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server3 │ 192.168.104.196 │ 3306 │ 0           │ Slave, Synced, Running  │      │
└─────────┴─────────────────┴──────┴─────────────┴─────────────────────────┴──────┘

{noformat}


 $acceptance criteria:$",,Zdravelina Sokolovska,Zdravelina Sokolovska,Major,42,,0,3,0,3,0,0,0,,0,850,1,0,0,2020-02-03 10:53:23,The Desynced state of Galera Node is not given in maxctrl server list,"The Desynced state of Galera Node is not given in maxctrl server list

When Galera Node was desynced  from the group with setting up wsrep_desync,
the maxctrl  is displaying just  Running.
It would be more clear if give in addition the Desynced state in the  maxctrl server list.





on server2  set wsrep_desync and FTWRL
check that Node gets in Desynced state 
check  the State of server2  from MaxScale Node  via maxctrl list  servers

{noformat}
MariaDB [(none)]> set global wsrep_desync=1 ;
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]> flush tables with read lock ;
Query OK, 0 rows affected (0.000 sec)
{noformat}


check locally on server2 the wsrep state

{noformat}

MariaDB [(none)]> show status like ""%state%"" ;
+-------------------------------------------+--------------------------------------+
| Variable_name                             | Value                                |
+-------------------------------------------+--------------------------------------+
| Max_statement_time_exceeded               | 0                                    |
| Performance_schema_statement_classes_lost | 0                                    |
| wsrep_cluster_state_uuid                  | 3c15149f-5766-11e8-9a99-22bc53d40581 |
| wsrep_evs_state                           | OPERATIONAL                          |
| wsrep_local_state                         | 2                                    |
| wsrep_local_state_comment                 | Donor/Desynced                       |
| wsrep_local_state_uuid                    | 3c15149f-5766-11e8-9a99-22bc53d40581 |
+-------------------------------------------+--------------------------------------+
7 rows in set (0.001 sec)

{noformat}



 The wsrep Desynced  state of server is not displayed from the MaxScale Node
{noformat}
[root@t4w1 /]#  maxctrl list  servers
┌─────────┬─────────────────┬──────┬─────────────┬─────────────────────────┬──────┐
│ Server  │ Address         │ Port │ Connections │ State                   │ GTID │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server1 │ 192.168.104.193 │ 3306 │ 0           │ Master, Synced, Running │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server2 │ 192.168.104.195 │ 3306 │ 0           │ Running                 │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server3 │ 192.168.104.196 │ 3306 │ 0           │ Slave, Synced, Running  │      │
└─────────┴─────────────────┴──────┴─────────────┴─────────────────────────┴──────┘

{noformat}


",,0,0,0,0,0.0,"The Desynced state of Galera Node is not given in maxctrl server list $end$ The Desynced state of Galera Node is not given in maxctrl server list

When Galera Node was desynced  from the group with setting up wsrep_desync,
the maxctrl  is displaying just  Running.
It would be more clear if give in addition the Desynced state in the  maxctrl server list.





on server2  set wsrep_desync and FTWRL
check that Node gets in Desynced state 
check  the State of server2  from MaxScale Node  via maxctrl list  servers

{noformat}
MariaDB [(none)]> set global wsrep_desync=1 ;
Query OK, 0 rows affected (0.001 sec)

MariaDB [(none)]> flush tables with read lock ;
Query OK, 0 rows affected (0.000 sec)
{noformat}


check locally on server2 the wsrep state

{noformat}

MariaDB [(none)]> show status like ""%state%"" ;
+-------------------------------------------+--------------------------------------+
| Variable_name                             | Value                                |
+-------------------------------------------+--------------------------------------+
| Max_statement_time_exceeded               | 0                                    |
| Performance_schema_statement_classes_lost | 0                                    |
| wsrep_cluster_state_uuid                  | 3c15149f-5766-11e8-9a99-22bc53d40581 |
| wsrep_evs_state                           | OPERATIONAL                          |
| wsrep_local_state                         | 2                                    |
| wsrep_local_state_comment                 | Donor/Desynced                       |
| wsrep_local_state_uuid                    | 3c15149f-5766-11e8-9a99-22bc53d40581 |
+-------------------------------------------+--------------------------------------+
7 rows in set (0.001 sec)

{noformat}



 The wsrep Desynced  state of server is not displayed from the MaxScale Node
{noformat}
[root@t4w1 /]#  maxctrl list  servers
┌─────────┬─────────────────┬──────┬─────────────┬─────────────────────────┬──────┐
│ Server  │ Address         │ Port │ Connections │ State                   │ GTID │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server1 │ 192.168.104.193 │ 3306 │ 0           │ Master, Synced, Running │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server2 │ 192.168.104.195 │ 3306 │ 0           │ Running                 │      │
├─────────┼─────────────────┼──────┼─────────────┼─────────────────────────┼──────┤
│ server3 │ 192.168.104.196 │ 3306 │ 0           │ Slave, Synced, Running  │      │
└─────────┴─────────────────┴──────┴─────────────┴─────────────────────────┴──────┘

{noformat}


 $acceptance criteria:$",0,0,0,0,0,0,1,14395.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1150,MXS-1929,New Feature,MXS,2018-06-18 06:44:18,,0,Please add maxadmin/maxctrl runtime configuration command to add new service,Please add maxadmin/maxctrl runtime configuration command to add new service without restart maxscale instance. ,,Please add maxadmin/maxctrl runtime configuration command to add new service $end$ Please add maxadmin/maxctrl runtime configuration command to add new service without restart maxscale instance.  $acceptance criteria:$,,Nilnandan Joshi,Nilnandan Joshi,Major,8,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-07-10 09:13:46,Please add maxadmin/maxctrl runtime configuration command to add new service,Please add maxadmin/maxctrl runtime configuration command to add new service without restart maxscale instance. ,,0,0,0,0,0.0,Please add maxadmin/maxctrl runtime configuration command to add new service $end$ Please add maxadmin/maxctrl runtime configuration command to add new service without restart maxscale instance.  $acceptance criteria:$,0,0,0,0,0,0,1,530.483,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1151,MXS-194,Sub-Task,MXS,2015-06-15 08:33:13,,0,"Add support for DROP, CREATE, GRANT, USE","Add ability to blacklist or white list DROP, CREATE, GRANT, USE queries",,"Add support for DROP, CREATE, GRANT, USE $end$ Add ability to blacklist or white list DROP, CREATE, GRANT, USE queries $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,14,,0,3,0,1,0,3,0,,0,850,3,0,0,2015-06-15 08:33:13,"Add support for DROP, CREATE, GRANT","Add ability to blacklist or white list DROP, CREATE or GRANT queries",,1,2,0,9,0.238095,"Add support for DROP, CREATE, GRANT $end$ Add ability to blacklist or white list DROP, CREATE or GRANT queries $acceptance criteria:$",3,1,1,0,0,0,1,0.0,4,1,0.25,0,0.0,0,0.0,0,0.0,0,0.0
1152,MXS-1940,Task,MXS,2018-06-26 05:31:50,,0,Remove the experimental label of the Cache,"* Remove the _experimental_ blurb from the cache documentation.
* Make {{thread_specific}} the default mode of the cache.
* Mark the module as _not_ being in development.

",,"Remove the experimental label of the Cache $end$ * Remove the _experimental_ blurb from the cache documentation.
* Make {{thread_specific}} the default mode of the cache.
* Mark the module as _not_ being in development.

 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-06-26 06:31:42,Remove the experimental label of the Cache,"* Remove the _experimental_ blurb from the cache documentation.
* Make {{thread_specific}} the default mode of the cache.
* Mark the module as _not_ being in development.

",,0,0,0,0,0.0,"Remove the experimental label of the Cache $end$ * Remove the _experimental_ blurb from the cache documentation.
* Make {{thread_specific}} the default mode of the cache.
* Mark the module as _not_ being in development.

 $acceptance criteria:$",0,0,0,0,0,0,0,0.983333,261,28,0.10728,11,0.0421456,6,0.0229885,4,0.0153257,4,0.0153257
1153,MXS-1951,New Feature,MXS,2018-07-02 06:14:56,,0,Try out SO_REUSEPORT,"[SO_REUSEPORT|http://man7.org/linux/man-pages/man7/socket.7.html] allows multiple sockets to bind to the same port. This improves the performance by removing the need for cross-thread communication without sacrificing the balancing of load between the threads. Compared to multiple threads listening on a single socket, multiple threads listening on multiple sockets should result in fairer load balancing as well as improved overall performance.",,"Try out SO_REUSEPORT $end$ [SO_REUSEPORT|http://man7.org/linux/man-pages/man7/socket.7.html] allows multiple sockets to bind to the same port. This improves the performance by removing the need for cross-thread communication without sacrificing the balancing of load between the threads. Compared to multiple threads listening on a single socket, multiple threads listening on multiple sockets should result in fairer load balancing as well as improved overall performance. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,16,,0,0,0,4,0,0,0,,0,850,0,0,0,2018-07-31 10:57:24,Try out SO_REUSEPORT,"[SO_REUSEPORT|http://man7.org/linux/man-pages/man7/socket.7.html] allows multiple sockets to bind to the same port. This improves the performance by removing the need for cross-thread communication without sacrificing the balancing of load between the threads. Compared to multiple threads listening on a single socket, multiple threads listening on multiple sockets should result in fairer load balancing as well as improved overall performance.",,0,0,0,0,0.0,"Try out SO_REUSEPORT $end$ [SO_REUSEPORT|http://man7.org/linux/man-pages/man7/socket.7.html] allows multiple sockets to bind to the same port. This improves the performance by removing the need for cross-thread communication without sacrificing the balancing of load between the threads. Compared to multiple threads listening on a single socket, multiple threads listening on multiple sockets should result in fairer load balancing as well as improved overall performance. $acceptance criteria:$",0,0,0,0,0,0,1,700.7,262,28,0.10687,11,0.0419847,6,0.0229008,4,0.0152672,4,0.0152672
1154,MXS-1954,New Feature,MXS,2018-07-03 13:59:23,MXS-2531,0,Explain why servers change state,"When a server changes state, it should always be accompanied with a message explaining why the state changed.

For example, if a server loses the slave status a log message should be logged before the state change message is logged. Similarly for masters being set into read-only mode, a message stating that a master transitioned into read-only mode should be logged.",,"Explain why servers change state $end$ When a server changes state, it should always be accompanied with a message explaining why the state changed.

For example, if a server loses the slave status a log message should be logged before the state change message is logged. Similarly for masters being set into read-only mode, a message stating that a master transitioned into read-only mode should be logged. $acceptance criteria:$",,markus makela,markus makela,Minor,17,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-02-03 10:53:19,Explain why servers change state,"When a server changes state, it should always be accompanied with a message explaining why the state changed.

For example, if a server loses the slave status a log message should be logged before the state change message is logged. Similarly for masters being set into read-only mode, a message stating that a master transitioned into read-only mode should be logged.",,0,0,0,0,0.0,"Explain why servers change state $end$ When a server changes state, it should always be accompanied with a message explaining why the state changed.

For example, if a server loses the slave status a log message should be logged before the state change message is logged. Similarly for masters being set into read-only mode, a message stating that a master transitioned into read-only mode should be logged. $acceptance criteria:$",0,0,0,0,0,0,0,13916.9,46,8,0.173913,8,0.173913,6,0.130435,6,0.130435,6,0.130435
1155,MXS-1957,Task,MXS,2018-07-04 13:10:16,,0,Add MariaDBAuth and MariaDBBackendAuth aliases,"MySQLAuth → MariaDBAuth
MySQLBackendAuth → MariaDBBackendAuth",,"Add MariaDBAuth and MariaDBBackendAuth aliases $end$ MySQLAuth → MariaDBAuth
MySQLBackendAuth → MariaDBBackendAuth $acceptance criteria:$",,markus makela,markus makela,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-05-06 11:50:21,Add MariaDBAuth and MariaDBBackendAuth aliases,"MySQLAuth → MariaDBAuth
MySQLBackendAuth → MariaDBBackendAuth",,0,0,0,0,0.0,"Add MariaDBAuth and MariaDBBackendAuth aliases $end$ MySQLAuth → MariaDBAuth
MySQLBackendAuth → MariaDBBackendAuth $acceptance criteria:$",0,0,0,0,0,0,1,7342.67,47,8,0.170213,8,0.170213,6,0.12766,6,0.12766,6,0.12766
1156,MXS-1962,Task,MXS,2018-07-06 08:25:13,,0,Create test that verifies MXS-1774,,,Create test that verifies MXS-1774 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-07-31 10:50:37,Create test that verifies MXS-1774,,,0,0,0,0,0.0,Create test that verifies MXS-1774 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,602.417,263,28,0.106464,11,0.0418251,6,0.0228137,4,0.0152091,4,0.0152091
1157,MXS-1968,Task,MXS,2018-07-10 08:59:46,,0,BuildBot builders,,,BuildBot builders $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,2,0,0,3,,0,850,0,0,0,2018-07-10 09:14:27,BuildBot builders,,,0,0,0,0,0.0,BuildBot builders $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.233333,58,1,0.0172414,0,0.0,0,0.0,0,0.0,0,0.0
1158,MXS-1969,Sub-Task,MXS,2018-07-10 09:00:48,,0,build_and_test,,,build_and_test $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-07-10 09:00:48,build_and_test,,,0,0,0,0,0.0,build_and_test $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,59,1,0.0169492,0,0.0,0,0.0,0,0.0,0,0.0
1159,MXS-1970,Sub-Task,MXS,2018-07-10 09:01:12,,0,run_test_snapshot,,,run_test_snapshot $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-07-10 09:01:12,run_test_snapshot,,,0,0,0,0,0.0,run_test_snapshot $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,60,1,0.0166667,0,0.0,0,0.0,0,0.0,0,0.0
1160,MXS-1971,Sub-Task,MXS,2018-07-10 09:06:36,,0,performance test builders,,,performance test builders $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-07-10 09:06:36,performance test builders,,,0,0,0,0,0.0,performance test builders $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,61,1,0.0163934,0,0.0,0,0.0,0,0.0,0,0.0
1161,MXS-1972,Sub-Task,MXS,2018-07-10 09:06:55,,0,cleanup builders,,,cleanup builders $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,2,,0,1,0,2,0,0,0,,0,850,1,0,0,2018-07-10 09:06:55,cleanup builders,,,0,0,0,0,0.0,cleanup builders $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,62,1,0.016129,0,0.0,0,0.0,0,0.0,0,0.0
1162,MXS-1973,New Feature,MXS,2018-07-11 17:21:26,,0,Add option to do reverse DNS lookups on IP addresses for MaxAdmin output,"MaxAdmin currently only prints IP addresses for hosts in its output. i.e.:

{noformat}
$ maxadmin list sessions
-----------------+-----------------+----------------+--------------------------
Session | Client | Service | State
-----------------+-----------------+----------------+--------------------------
215811 | ::ffff:10.1.1.6 | Galera-Service | Session ready for routing
216121 | ::ffff:10.1.1.5 | Galera-Service | Session ready for routing
216783 | ::ffff:10.1.1.1 | Galera-Service | Session ready for routing
{noformat}

Some users would like to have an option that would allow them to tell maxadmin to do reverse DNS lookups on IP addresses, and then to have the hostnames printed in addition to or instead of the IP addresses.",,"Add option to do reverse DNS lookups on IP addresses for MaxAdmin output $end$ MaxAdmin currently only prints IP addresses for hosts in its output. i.e.:

{noformat}
$ maxadmin list sessions
-----------------+-----------------+----------------+--------------------------
Session | Client | Service | State
-----------------+-----------------+----------------+--------------------------
215811 | ::ffff:10.1.1.6 | Galera-Service | Session ready for routing
216121 | ::ffff:10.1.1.5 | Galera-Service | Session ready for routing
216783 | ::ffff:10.1.1.1 | Galera-Service | Session ready for routing
{noformat}

Some users would like to have an option that would allow them to tell maxadmin to do reverse DNS lookups on IP addresses, and then to have the hostnames printed in addition to or instead of the IP addresses. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,15,,0,1,0,6,0,0,0,,0,850,0,0,0,2019-03-18 11:40:01,Add option to do reverse DNS lookups on IP addresses for MaxAdmin output,"MaxAdmin currently only prints IP addresses for hosts in its output. i.e.:

{noformat}
$ maxadmin list sessions
-----------------+-----------------+----------------+--------------------------
Session | Client | Service | State
-----------------+-----------------+----------------+--------------------------
215811 | ::ffff:10.1.1.6 | Galera-Service | Session ready for routing
216121 | ::ffff:10.1.1.5 | Galera-Service | Session ready for routing
216783 | ::ffff:10.1.1.1 | Galera-Service | Session ready for routing
{noformat}

Some users would like to have an option that would allow them to tell maxadmin to do reverse DNS lookups on IP addresses, and then to have the hostnames printed in addition to or instead of the IP addresses.",,0,0,0,0,0.0,"Add option to do reverse DNS lookups on IP addresses for MaxAdmin output $end$ MaxAdmin currently only prints IP addresses for hosts in its output. i.e.:

{noformat}
$ maxadmin list sessions
-----------------+-----------------+----------------+--------------------------
Session | Client | Service | State
-----------------+-----------------+----------------+--------------------------
215811 | ::ffff:10.1.1.6 | Galera-Service | Session ready for routing
216121 | ::ffff:10.1.1.5 | Galera-Service | Session ready for routing
216783 | ::ffff:10.1.1.1 | Galera-Service | Session ready for routing
{noformat}

Some users would like to have an option that would allow them to tell maxadmin to do reverse DNS lookups on IP addresses, and then to have the hostnames printed in addition to or instead of the IP addresses. $acceptance criteria:$",0,0,0,0,0,0,1,5994.3,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1163,MXS-1976,New Feature,MXS,2018-07-12 17:32:23,,0,MaxAdmin Shutting Down A Service should specify / warn that new session requests are neither accepted nor denied.,"When 
{code:java}
maxadmin shutdown service
{code}
 is in progress, MaxScale continues to service current connections but does not accept new ones. This is documented in the knowledge-base [here|https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-maxadmin-admin-interface/#shutting-down-a-monitor].

The problem is that new connection requests are not denied , nor are they accepted. They simply hang. It would not be a problem if the default ConnectTimeout in Connector/J were not infinite/undefined, but the combination of default behaviors means that clients rather than being actively denied are left in limbo when trying to connect to a MaxScale service in the process of shutting down.

One solution might be to edit the documentation to warn users that MaxScale does not actively refuse new connections when it's in the process of shutting down, instead simply not answering, letting them know that some connectors do not time out unless configured to do so.

Another alternative might be to have the service actively reject new connections when it is in the process of shutting down.",,"MaxAdmin Shutting Down A Service should specify / warn that new session requests are neither accepted nor denied. $end$ When 
{code:java}
maxadmin shutdown service
{code}
 is in progress, MaxScale continues to service current connections but does not accept new ones. This is documented in the knowledge-base [here|https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-maxadmin-admin-interface/#shutting-down-a-monitor].

The problem is that new connection requests are not denied , nor are they accepted. They simply hang. It would not be a problem if the default ConnectTimeout in Connector/J were not infinite/undefined, but the combination of default behaviors means that clients rather than being actively denied are left in limbo when trying to connect to a MaxScale service in the process of shutting down.

One solution might be to edit the documentation to warn users that MaxScale does not actively refuse new connections when it's in the process of shutting down, instead simply not answering, letting them know that some connectors do not time out unless configured to do so.

Another alternative might be to have the service actively reject new connections when it is in the process of shutting down. $acceptance criteria:$",,Juan,Juan,Major,10,,0,2,0,1,0,1,0,,0,850,2,1,0,2018-11-13 11:56:19,MaxAdmin Shutting Down A Service should specify / warn that new session requests are neither accepted nor denied.,"When 
{code:java}
maxadmin shutdown service
{code}
 is in progress, MaxScale continues to service current connections but does not accept new ones. This is documented in the knowledge-base [here|https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-maxadmin-admin-interface/#shutting-down-a-monitor].

The problem is that new connection requests are not denied , nor are they accepted. They simply hang. It would not be a problem if the default ConnectTimeout in Connector/J were not infinite/undefined, but the combination of default behaviors means that clients rather than being actively denied are left in limbo when trying to connect to a MaxScale service in the process of shutting down.

One solution might be to edit the documentation to warn users that MaxScale does not actively refuse new connections when it's in the process of shutting down, instead simply not answering, letting them know that some connectors do not time out unless configured to do so.

Another alternative might be to have the service actively reject new connections when it is in the process of shutting down.",,0,0,0,0,0.0,"MaxAdmin Shutting Down A Service should specify / warn that new session requests are neither accepted nor denied. $end$ When 
{code:java}
maxadmin shutdown service
{code}
 is in progress, MaxScale continues to service current connections but does not accept new ones. This is documented in the knowledge-base [here|https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale-22-maxadmin-admin-interface/#shutting-down-a-monitor].

The problem is that new connection requests are not denied , nor are they accepted. They simply hang. It would not be a problem if the default ConnectTimeout in Connector/J were not infinite/undefined, but the combination of default behaviors means that clients rather than being actively denied are left in limbo when trying to connect to a MaxScale service in the process of shutting down.

One solution might be to edit the documentation to warn users that MaxScale does not actively refuse new connections when it's in the process of shutting down, instead simply not answering, letting them know that some connectors do not time out unless configured to do so.

Another alternative might be to have the service actively reject new connections when it is in the process of shutting down. $acceptance criteria:$",0,0,0,0,0,0,0,2970.38,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1164,MXS-1980,New Feature,MXS,2018-07-16 16:42:42,,0,Support Galera cluster nodes as masters for Binlog Router,"Binlog Router and Binlog-Avro Router to replicate from Galera cluster in a failsafe manner
- If Binlog Router or Binlog-Avro Router is replicating from one Galera node, it should be possible to configure list of additional Galera nodes as possible masters in case of failure of the current node that MaxScale is replicating from 
- When Binlog Router and Binlog-Avro Router detects that the one Galera node that it is replicating from has failed, MaxScale need to change master to one of the other Galera node that is in running state.
",,"Support Galera cluster nodes as masters for Binlog Router $end$ Binlog Router and Binlog-Avro Router to replicate from Galera cluster in a failsafe manner
- If Binlog Router or Binlog-Avro Router is replicating from one Galera node, it should be possible to configure list of additional Galera nodes as possible masters in case of failure of the current node that MaxScale is replicating from 
- When Binlog Router and Binlog-Avro Router detects that the one Galera node that it is replicating from has failed, MaxScale need to change master to one of the other Galera node that is in running state.
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,7,,2,0,2,1,0,0,0,,0,850,0,0,0,2018-09-26 05:43:34,Support Galera cluster nodes as masters for Binlog Router,"Binlog Router and Binlog-Avro Router to replicate from Galera cluster in a failsafe manner
- If Binlog Router or Binlog-Avro Router is replicating from one Galera node, it should be possible to configure list of additional Galera nodes as possible masters in case of failure of the current node that MaxScale is replicating from 
- When Binlog Router and Binlog-Avro Router detects that the one Galera node that it is replicating from has failed, MaxScale need to change master to one of the other Galera node that is in running state.
",,0,0,0,0,0.0,"Support Galera cluster nodes as masters for Binlog Router $end$ Binlog Router and Binlog-Avro Router to replicate from Galera cluster in a failsafe manner
- If Binlog Router or Binlog-Avro Router is replicating from one Galera node, it should be possible to configure list of additional Galera nodes as possible masters in case of failure of the current node that MaxScale is replicating from 
- When Binlog Router and Binlog-Avro Router detects that the one Galera node that it is replicating from has failed, MaxScale need to change master to one of the other Galera node that is in running state.
 $acceptance criteria:$",0,0,0,0,0,0,0,1717.0,28,5,0.178571,2,0.0714286,0,0.0,0,0.0,0,0.0
1165,MXS-1984,New Feature,MXS,2018-07-19 15:38:05,,0,Allow the source command in the new promotion_sql_file option ,"Not being able to perform a SOURCE command makes this very limited.  Please add the source command so that a sql file can be created with a list of events to be enabled and then read back in to enable them from slave_disabled.

Related to:
https://jira.mariadb.org/browse/MXS-1639",,"Allow the source command in the new promotion_sql_file option  $end$ Not being able to perform a SOURCE command makes this very limited.  Please add the source command so that a sql file can be created with a list of events to be enabled and then read back in to enable them from slave_disabled.

Related to:
https://jira.mariadb.org/browse/MXS-1639 $acceptance criteria:$",,Kyle Joiner,Kyle Joiner,Major,19,,0,0,0,3,0,0,0,,0,850,0,0,0,2018-10-30 11:40:05,Allow the source command in the new promotion_sql_file option ,"Not being able to perform a SOURCE command makes this very limited.  Please add the source command so that a sql file can be created with a list of events to be enabled and then read back in to enable them from slave_disabled.

Related to:
https://jira.mariadb.org/browse/MXS-1639",,0,0,0,0,0.0,"Allow the source command in the new promotion_sql_file option  $end$ Not being able to perform a SOURCE command makes this very limited.  Please add the source command so that a sql file can be created with a list of events to be enabled and then read back in to enable them from slave_disabled.

Related to:
https://jira.mariadb.org/browse/MXS-1639 $acceptance criteria:$",0,0,0,0,0,0,1,2468.03,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1166,MXS-1989,Task,MXS,2018-07-27 16:00:06,,0,Automate release publishing,"The task of moving code from the CI repo to the official repo can be automated. Same goes for the generation of the emails that are sent when a release is done.
",,"Automate release publishing $end$ The task of moving code from the CI repo to the official repo can be automated. Same goes for the generation of the emails that are sent when a release is done.
 $acceptance criteria:$",,markus makela,markus makela,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-13 10:34:53,Automate release publishing,"The task of moving code from the CI repo to the official repo can be automated. Same goes for the generation of the emails that are sent when a release is done.
",,0,0,0,0,0.0,"Automate release publishing $end$ The task of moving code from the CI repo to the official repo can be automated. Same goes for the generation of the emails that are sent when a release is done.
 $acceptance criteria:$",0,0,0,0,0,0,0,6954.57,48,8,0.166667,8,0.166667,6,0.125,6,0.125,6,0.125
1167,MXS-1992,Task,MXS,2018-07-30 09:56:23,,0,Make query classification cache configurable,,,Make query classification cache configurable $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-07-31 10:06:51,Make query classification cache configurable,,,0,0,0,0,0.0,Make query classification cache configurable $end$ $acceptance criteria:$,0,0,0,0,0,0,0,24.1667,264,28,0.106061,11,0.0416667,6,0.0227273,4,0.0151515,4,0.0151515
1168,MXS-1993,Task,MXS,2018-07-31 10:58:20,,0,Investigate clang-format as a replacement for Astyle,,,Investigate clang-format as a replacement for Astyle $end$ $acceptance criteria:$,,markus makela,markus makela,Major,8,,0,0,0,4,0,0,0,,0,850,0,0,0,2018-07-31 10:59:45,Investigate clang-format as a replacement for Astyle,,,0,0,0,0,0.0,Investigate clang-format as a replacement for Astyle $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,49,8,0.163265,8,0.163265,6,0.122449,6,0.122449,6,0.122449
1169,MXS-1995,Task,MXS,2018-08-01 18:54:38,,0,Add SLES 15 build of maxscale,,,Add SLES 15 build of maxscale $end$ $acceptance criteria:$,,David Thompson,David Thompson,Major,15,,0,3,0,4,0,0,0,,0,850,2,0,0,2018-08-14 09:54:45,Add SLES 15 build of maxscale,,,0,0,0,0,0.0,Add SLES 15 build of maxscale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,303.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1170,MXS-2004,Task,MXS,2018-08-10 08:10:07,,0,Replace THREAD with std::thread.,,,Replace THREAD with std::thread. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-08-14 08:58:23,Replace THREAD with std::thread.,,,0,0,0,0,0.0,Replace THREAD with std::thread. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,96.8,265,28,0.10566,11,0.0415094,6,0.0226415,4,0.0150943,4,0.0150943
1171,MXS-2005,Task,MXS,2018-08-10 10:34:11,,0,Rewrite the MaxScale log manager.,,,Rewrite the MaxScale log manager. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-08-14 09:05:47,Rewrite the MaxScale log manager.,,,0,0,0,0,0.0,Rewrite the MaxScale log manager. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,94.5167,266,28,0.105263,11,0.0413534,6,0.0225564,4,0.0150376,4,0.0150376
1172,MXS-2008,Task,MXS,2018-08-13 07:41:27,,0,Move Worker to maxbase.,,,Move Worker to maxbase. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-08-14 08:58:34,Move Worker to maxbase.,,,0,0,0,0,0.0,Move Worker to maxbase. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,25.2833,267,28,0.104869,11,0.0411985,6,0.0224719,4,0.0149813,4,0.0149813
1173,MXS-2012,Task,MXS,2018-08-14 09:50:30,,0,Look into replication lag mechanism,,,Look into replication lag mechanism $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-08-14 09:50:30,Look into replication lag mechanism,,,0,0,0,0,0.0,Look into replication lag mechanism $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,268,28,0.104478,11,0.0410448,6,0.0223881,4,0.0149254,4,0.0149254
1174,MXS-2022,Task,MXS,2018-08-23 15:04:59,,0,Benchmark 2.3,"Benchmark MaxScale 2.3 using the same setup as in MXS-1597, so that the results are comparable.

Use the tag `maxscale-2.3.0`.

Please run the benchmarks both with {{query_classifier_cache_size=0}} defined in the {{[MaxScale]}} section, and with that configuration setting not defined at all.

In the former case, the query classification will be performed exactly like in 2.2, so the benchmark results will thus show how 2.3 compares with 2.2, with the general mechanisms being roughly the same. In the latter case, the query classification results will be cached and reused, which should lead to better performance.",,"Benchmark 2.3 $end$ Benchmark MaxScale 2.3 using the same setup as in MXS-1597, so that the results are comparable.

Use the tag `maxscale-2.3.0`.

Please run the benchmarks both with {{query_classifier_cache_size=0}} defined in the {{[MaxScale]}} section, and with that configuration setting not defined at all.

In the former case, the query classification will be performed exactly like in 2.2, so the benchmark results will thus show how 2.3 compares with 2.2, with the general mechanisms being roughly the same. In the latter case, the query classification results will be cached and reused, which should lead to better performance. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,0,1,0,1,0,2,0,,0,850,1,0,0,2018-10-09 09:23:32,Benchmark 2.3,"Benchmark MaxScale 2.3 using the same setup as in MXS-1597 so that the results are comparable.

Use the tag `maxscale-2.3.0`.",,0,2,0,76,3.0,"Benchmark 2.3 $end$ Benchmark MaxScale 2.3 using the same setup as in MXS-1597 so that the results are comparable.

Use the tag `maxscale-2.3.0`. $acceptance criteria:$",2,1,1,1,1,1,1,1122.3,269,28,0.104089,11,0.0408922,6,0.0223048,4,0.0148699,4,0.0148699
1175,MXS-2025,Task,MXS,2018-08-27 07:40:28,,0,Refactor R/W-Split handling of backends and backend selection,,,Refactor R/W-Split handling of backends and backend selection $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Major,18,,0,0,0,5,0,0,0,,0,850,0,0,0,2018-08-29 09:47:34,Refactor R/W-Split handling of backends and backend selection,,,0,0,0,0,0.0,Refactor R/W-Split handling of backends and backend selection $end$ $acceptance criteria:$,0,0,0,0,0,0,1,50.1167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1176,MXS-2029,Sub-Task,MXS,2018-08-29 09:08:58,,0,local test setup documentation,"document whole process, check all docs in MDBCI, BuildBot",,"local test setup documentation $end$ document whole process, check all docs in MDBCI, BuildBot $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,2,,0,1,0,5,0,0,0,,0,850,1,0,0,2018-08-29 09:08:58,local test setup documentation,"document whole process, check all docs in MDBCI, BuildBot",,0,0,0,0,0.0,"local test setup documentation $end$ document whole process, check all docs in MDBCI, BuildBot $acceptance criteria:$",0,0,0,0,0,0,1,0.0,63,1,0.015873,0,0.0,0,0.0,0,0.0,0,0.0
1177,MXS-2030,Sub-Task,MXS,2018-08-29 09:10:43,,0,"test MDBCI setup with latest Ubuntu, CentOS, Mint, Debian, SUSE","MDBCI and libvirt setup to be tested against major distributions, all findings to be added to documentation",,"test MDBCI setup with latest Ubuntu, CentOS, Mint, Debian, SUSE $end$ MDBCI and libvirt setup to be tested against major distributions, all findings to be added to documentation $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,3,,0,7,0,5,0,0,0,,0,850,7,0,0,2018-08-29 09:10:43,"test MDBCI setup with latest Ubuntu, CentOS, Mint, Debian, SUSE","MDBCI and libvirt setup to be tested against major distributions, all findings to be added to documentation",,0,0,0,0,0.0,"test MDBCI setup with latest Ubuntu, CentOS, Mint, Debian, SUSE $end$ MDBCI and libvirt setup to be tested against major distributions, all findings to be added to documentation $acceptance criteria:$",0,0,0,0,0,0,1,0.0,64,1,0.015625,0,0.0,0,0.0,0,0.0,0,0.0
1178,MXS-2031,Task,MXS,2018-08-29 09:17:12,,0,Performance test setup,,,Performance test setup $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,2,0,0,3,,0,850,0,0,0,2018-09-25 10:53:52,Performance test setup,,,0,0,0,0,0.0,Performance test setup $end$ $acceptance criteria:$,0,0,0,0,0,0,1,649.6,65,1,0.0153846,0,0.0,0,0.0,0,0.0,0,0.0
1179,MXS-2044,New Feature,MXS,2018-09-11 21:46:56,MXS-2529,0,Ability to set filebase to stdout,"I have maxscale deployed in kubernetes and I would like to send the filter logs to ELK stack, via the stdout logger.

It will be great if the filter support sending logs to stdout so that we can ship the query log logs to any log aggregation framework.

Example proposal 

[SQLLogger]
type=filter
module=qlafilter
log_data=user, query
filebase=stdout",,"Ability to set filebase to stdout $end$ I have maxscale deployed in kubernetes and I would like to send the filter logs to ELK stack, via the stdout logger.

It will be great if the filter support sending logs to stdout so that we can ship the query log logs to any log aggregation framework.

Example proposal 

[SQLLogger]
type=filter
module=qlafilter
log_data=user, query
filebase=stdout $acceptance criteria:$",,Kidanekal Hailu,Kidanekal Hailu,Minor,14,,0,1,0,1,0,0,0,,0,850,0,0,0,2019-12-09 11:50:14,Ability to set filebase to stdout,"I have maxscale deployed in kubernetes and I would like to send the filter logs to ELK stack, via the stdout logger.

It will be great if the filter support sending logs to stdout so that we can ship the query log logs to any log aggregation framework.

Example proposal 

[SQLLogger]
type=filter
module=qlafilter
log_data=user, query
filebase=stdout",,0,0,0,0,0.0,"Ability to set filebase to stdout $end$ I have maxscale deployed in kubernetes and I would like to send the filter logs to ELK stack, via the stdout logger.

It will be great if the filter support sending logs to stdout so that we can ship the query log logs to any log aggregation framework.

Example proposal 

[SQLLogger]
type=filter
module=qlafilter
log_data=user, query
filebase=stdout $acceptance criteria:$",0,0,0,0,0,0,0,10886.0,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1180,MXS-2050,New Feature,MXS,2018-09-14 12:23:48,,0,make -query all log filter- logrotate compatible,"query-log-all-filter can produce under some circumstances huge files, so rotating would be nice feature.

Like the already compatible maxscale.log , the defined 

filebase=/var/logs/qla/queries

should be compatible to the liniux logrotate machanism,


[logrotate|https://linux.die.net/man/8/logrotate]


",,"make -query all log filter- logrotate compatible $end$ query-log-all-filter can produce under some circumstances huge files, so rotating would be nice feature.

Like the already compatible maxscale.log , the defined 

filebase=/var/logs/qla/queries

should be compatible to the liniux logrotate machanism,


[logrotate|https://linux.die.net/man/8/logrotate]


 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,24,,1,0,1,5,0,0,0,,0,850,0,0,0,2018-09-25 10:34:26,make -query all log filter- logrotate compatible,"query-log-all-filter can produce under some circumstances huge files, so rotating would be nice feature.

Like the already compatible maxscale.log , the defined 

filebase=/var/logs/qla/queries

should be compatible to the liniux logrotate machanism,


[logrotate|https://linux.die.net/man/8/logrotate]


",,0,0,0,0,0.0,"make -query all log filter- logrotate compatible $end$ query-log-all-filter can produce under some circumstances huge files, so rotating would be nice feature.

Like the already compatible maxscale.log , the defined 

filebase=/var/logs/qla/queries

should be compatible to the liniux logrotate machanism,


[logrotate|https://linux.die.net/man/8/logrotate]


 $acceptance criteria:$",0,0,0,0,0,0,1,262.167,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
1181,MXS-2051,Task,MXS,2018-09-14 15:32:04,,0,Remove REST API section from 2.1 version of the manual,It is confusing as the REST API actually only started to be available with MaxScale 2.2.,,Remove REST API section from 2.1 version of the manual $end$ It is confusing as the REST API actually only started to be available with MaxScale 2.2. $acceptance criteria:$,,Hartmut Holzgraefe,Hartmut Holzgraefe,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-09-25 08:22:18,Remove REST API section from 2.1 version of the manual,It is confusing as the REST API actually only started to be available with MaxScale 2.2.,,0,0,0,0,0.0,Remove REST API section from 2.1 version of the manual $end$ It is confusing as the REST API actually only started to be available with MaxScale 2.2. $acceptance criteria:$,0,0,0,0,0,0,0,256.833,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1182,MXS-2054,Task,MXS,2018-09-17 09:20:57,,0,Routing to MariaDB Server and MariaDB ColumnStore from MaxScale,"Test following

(1) Cluster 1 - MariaDB Master/Slave or Galera Cluster - Server1, Server2, Server3
(2) Cluster 2 - MAriaDB ColumnStore - UM1, UM2 (and PM1, PM2, PM3)

(3) MaxScale configured with 
*  A service using read-write split router and named server filter. 
*  In this service servers=Server1,Server2,Server3,UM1,UM2
*  In named server filer certain regex pattern go to Server1,Server2,Server3, another regex pattern go to UM1, UM2
* A mariadbmon or galeramon monitor with servers=Server1,Server2,Server3 (note no UM1, UM2 here)
* Manually set UM1 and UM2's server status in MaxScale via MaxCtrl or maxadmin to ""slave""

Now send queries from client - verify that regex pattern configured to go to Server1,Server2,Server3 do route to Server1,Server2,Server3 in read-write split manner, and regex pattern configured to go to UM1, and UM2 do route to UM1,UM2

An example config file to be attached later on to this Jira",,"Routing to MariaDB Server and MariaDB ColumnStore from MaxScale $end$ Test following

(1) Cluster 1 - MariaDB Master/Slave or Galera Cluster - Server1, Server2, Server3
(2) Cluster 2 - MAriaDB ColumnStore - UM1, UM2 (and PM1, PM2, PM3)

(3) MaxScale configured with 
*  A service using read-write split router and named server filter. 
*  In this service servers=Server1,Server2,Server3,UM1,UM2
*  In named server filer certain regex pattern go to Server1,Server2,Server3, another regex pattern go to UM1, UM2
* A mariadbmon or galeramon monitor with servers=Server1,Server2,Server3 (note no UM1, UM2 here)
* Manually set UM1 and UM2's server status in MaxScale via MaxCtrl or maxadmin to ""slave""

Now send queries from client - verify that regex pattern configured to go to Server1,Server2,Server3 do route to Server1,Server2,Server3 in read-write split manner, and regex pattern configured to go to UM1, and UM2 do route to UM1,UM2

An example config file to be attached later on to this Jira $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,13,,0,2,0,1,0,0,0,,0,850,2,0,0,2018-09-25 07:23:34,Routing to MariaDB Server and MariaDB ColumnStore from MaxScale,"Test following

(1) Cluster 1 - MariaDB Master/Slave or Galera Cluster - Server1, Server2, Server3
(2) Cluster 2 - MAriaDB ColumnStore - UM1, UM2 (and PM1, PM2, PM3)

(3) MaxScale configured with 
*  A service using read-write split router and named server filter. 
*  In this service servers=Server1,Server2,Server3,UM1,UM2
*  In named server filer certain regex pattern go to Server1,Server2,Server3, another regex pattern go to UM1, UM2
* A mariadbmon or galeramon monitor with servers=Server1,Server2,Server3 (note no UM1, UM2 here)
* Manually set UM1 and UM2's server status in MaxScale via MaxCtrl or maxadmin to ""slave""

Now send queries from client - verify that regex pattern configured to go to Server1,Server2,Server3 do route to Server1,Server2,Server3 in read-write split manner, and regex pattern configured to go to UM1, and UM2 do route to UM1,UM2

An example config file to be attached later on to this Jira",,0,0,0,0,0.0,"Routing to MariaDB Server and MariaDB ColumnStore from MaxScale $end$ Test following

(1) Cluster 1 - MariaDB Master/Slave or Galera Cluster - Server1, Server2, Server3
(2) Cluster 2 - MAriaDB ColumnStore - UM1, UM2 (and PM1, PM2, PM3)

(3) MaxScale configured with 
*  A service using read-write split router and named server filter. 
*  In this service servers=Server1,Server2,Server3,UM1,UM2
*  In named server filer certain regex pattern go to Server1,Server2,Server3, another regex pattern go to UM1, UM2
* A mariadbmon or galeramon monitor with servers=Server1,Server2,Server3 (note no UM1, UM2 here)
* Manually set UM1 and UM2's server status in MaxScale via MaxCtrl or maxadmin to ""slave""

Now send queries from client - verify that regex pattern configured to go to Server1,Server2,Server3 do route to Server1,Server2,Server3 in read-write split manner, and regex pattern configured to go to UM1, and UM2 do route to UM1,UM2

An example config file to be attached later on to this Jira $acceptance criteria:$",0,0,0,0,0,0,0,190.033,29,5,0.172414,2,0.0689655,0,0.0,0,0.0,0,0.0
1183,MXS-2057,New Feature,MXS,2018-09-17 16:03:43,,0,Watchdog for MaxScale,"Provide a watchdog utility for MaxScale that runs as a process on the MaxScale server and continuously monitors MaxScale process
1. It should detect that MaxScale is hung and crash with signal 6 (using watchdog?)
2. It should have the option to generate a core dump upon the crash
3. When there are multiple MaxScale nodes in HA configuration, other Maxscale nodes watchdog should detect if the Maxscale node with the setting of active (i.e. passive=0) went down and elect one of the other MaxScale nodes to be active and assign the elected MaxScale node with passive=0",,"Watchdog for MaxScale $end$ Provide a watchdog utility for MaxScale that runs as a process on the MaxScale server and continuously monitors MaxScale process
1. It should detect that MaxScale is hung and crash with signal 6 (using watchdog?)
2. It should have the option to generate a core dump upon the crash
3. When there are multiple MaxScale nodes in HA configuration, other Maxscale nodes watchdog should detect if the Maxscale node with the setting of active (i.e. passive=0) went down and elect one of the other MaxScale nodes to be active and assign the elected MaxScale node with passive=0 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,8,,0,3,0,2,0,0,1,,0,850,0,0,0,2018-10-30 12:47:25,Watchdog for MaxScale,"Provide a watchdog utility for MaxScale that runs as a process on the MaxScale server and continuously monitors MaxScale process
1. It should detect that MaxScale is hung and crash with signal 6 (using watchdog?)
2. It should have the option to generate a core dump upon the crash
3. When there are multiple MaxScale nodes in HA configuration, other Maxscale nodes watchdog should detect if the Maxscale node with the setting of active (i.e. passive=0) went down and elect one of the other MaxScale nodes to be active and assign the elected MaxScale node with passive=0",,0,0,0,0,0.0,"Watchdog for MaxScale $end$ Provide a watchdog utility for MaxScale that runs as a process on the MaxScale server and continuously monitors MaxScale process
1. It should detect that MaxScale is hung and crash with signal 6 (using watchdog?)
2. It should have the option to generate a core dump upon the crash
3. When there are multiple MaxScale nodes in HA configuration, other Maxscale nodes watchdog should detect if the Maxscale node with the setting of active (i.e. passive=0) went down and elect one of the other MaxScale nodes to be active and assign the elected MaxScale node with passive=0 $acceptance criteria:$",0,0,0,0,0,0,1,1028.72,30,5,0.166667,2,0.0666667,0,0.0,0,0.0,0,0.0
1184,MXS-2059,New Feature,MXS,2018-09-18 18:12:03,MXS-2529,0,Tee directly to backend server,"Since the tee filter was rewritten to use a proper network connection, the queries could be duplicated directly to a backend server. This would simplify the configurations by removing the need for the second service.",,"Tee directly to backend server $end$ Since the tee filter was rewritten to use a proper network connection, the queries could be duplicated directly to a backend server. This would simplify the configurations by removing the need for the second service. $acceptance criteria:$",,markus makela,markus makela,Major,12,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-10-28 11:29:55,Tee directly to backend server,"Since the tee filter was rewritten to use a proper network connection, the queries could be duplicated directly to a backend server. This would simplify the configurations by removing the need for the second service.",,0,0,0,0,0.0,"Tee directly to backend server $end$ Since the tee filter was rewritten to use a proper network connection, the queries could be duplicated directly to a backend server. This would simplify the configurations by removing the need for the second service. $acceptance criteria:$",0,0,0,0,0,0,0,9713.28,50,8,0.16,8,0.16,6,0.12,6,0.12,6,0.12
1185,MXS-2061,Task,MXS,2018-09-19 15:19:01,,0,Beta check of documentation for 2.3 features,Assure that all the new/modified features of 2.3 are documented,,Beta check of documentation for 2.3 features $end$ Assure that all the new/modified features of 2.3 are documented $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-09-25 08:23:09,Beta check of documentation for 2.3 features,Assure that all the new/modified features of 2.3 are documented,,0,0,0,0,0.0,Beta check of documentation for 2.3 features $end$ Assure that all the new/modified features of 2.3 are documented $acceptance criteria:$,0,0,0,0,0,0,0,137.067,31,5,0.16129,2,0.0645161,0,0.0,0,0.0,0,0.0
1186,MXS-2069,Task,MXS,2018-09-25 10:25:20,,0,Run tests with ASAN,Build MaxScale for tests with ASAN.,,Run tests with ASAN $end$ Build MaxScale for tests with ASAN. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-09-25 10:25:25,Run tests with ASAN,Build MaxScale for tests with ASAN.,,0,0,0,0,0.0,Run tests with ASAN $end$ Build MaxScale for tests with ASAN. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,51,8,0.156863,8,0.156863,6,0.117647,6,0.117647,6,0.117647
1187,MXS-2071,Task,MXS,2018-09-25 10:46:18,,0,Deprecate weights,,,Deprecate weights $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-09-25 10:46:18,Deprecate weights,,,0,0,0,0,0.0,Deprecate weights $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,270,29,0.107407,12,0.0444444,7,0.0259259,5,0.0185185,5,0.0185185
1188,MXS-2074,New Feature,MXS,2018-09-27 21:23:08,MXS-2529,0,Handle prepared statements in NamedServerFilter,"This is a request to allow NamedServerFilter to be able to route prepared statements to a specific server based on user and/or host.  This currently works fine for SELECT, INSERT, UPDATE, DELETE, but does not work for prepared statements.

Per Markus, ""the namedserverfilter will add a hint for the preparation but the execution of the prepared statement will not get a hint"".

We see this when running sysbench against the NamedServerFilter, as the queries are not routed, unless you specify the --db-ps-mode=disable option.",,"Handle prepared statements in NamedServerFilter $end$ This is a request to allow NamedServerFilter to be able to route prepared statements to a specific server based on user and/or host.  This currently works fine for SELECT, INSERT, UPDATE, DELETE, but does not work for prepared statements.

Per Markus, ""the namedserverfilter will add a hint for the preparation but the execution of the prepared statement will not get a hint"".

We see this when running sysbench against the NamedServerFilter, as the queries are not routed, unless you specify the --db-ps-mode=disable option. $acceptance criteria:$",,Chris Calender,Chris Calender,Major,30,,0,1,0,2,0,1,0,,0,850,0,0,0,2020-03-02 11:29:36,Request to allow NamedServerFilter to be able to route prepared statements' queries to specific server based on user and/or host,"This is a request to allow NamedServerFilter to be able to route prepared statements to a specific server based on user and/or host.  This currently works fine for SELECT, INSERT, UPDATE, DELETE, but does not work for prepared statements.

Per Markus, ""the namedserverfilter will add a hint for the preparation but the execution of the prepared statement will not get a hint"".

We see this when running sysbench against the NamedServerFilter, as the queries are not routed, unless you specify the --db-ps-mode=disable option.",,1,0,0,23,0.179245,"Request to allow NamedServerFilter to be able to route prepared statements' queries to specific server based on user and/or host $end$ This is a request to allow NamedServerFilter to be able to route prepared statements to a specific server based on user and/or host.  This currently works fine for SELECT, INSERT, UPDATE, DELETE, but does not work for prepared statements.

Per Markus, ""the namedserverfilter will add a hint for the preparation but the execution of the prepared statement will not get a hint"".

We see this when running sysbench against the NamedServerFilter, as the queries are not routed, unless you specify the --db-ps-mode=disable option. $acceptance criteria:$",1,1,1,1,1,1,1,12518.1,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1189,MXS-2077,New Feature,MXS,2018-10-03 00:30:22,,0,Provide more information in list clients output.,"Add the username, host and any other client/host information to the list clients output of Maxadmin to give more information similar to SHOW FULL PROCESSLIST.",,"Provide more information in list clients output. $end$ Add the username, host and any other client/host information to the list clients output of Maxadmin to give more information similar to SHOW FULL PROCESSLIST. $acceptance criteria:$",,Kyle Joiner,Kyle Joiner,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-11-12 07:59:24,Provide more information in list clients output.,"Add the username, host and any other client/host information to the list clients output of Maxadmin to give more information similar to SHOW FULL PROCESSLIST.",,0,0,0,0,0.0,"Provide more information in list clients output. $end$ Add the username, host and any other client/host information to the list clients output of Maxadmin to give more information similar to SHOW FULL PROCESSLIST. $acceptance criteria:$",0,0,0,0,0,0,0,967.483,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1190,MXS-2078,New Feature,MXS,2018-10-04 10:03:45,,0,Add statistics to RWSplit,"Server average query response time for lifetime
Server average query response time 1s, 10s, 1m  (might be too expensive)
Server number of queries 1s, 10s, 1m
Session average length (time)
Session active/idle ratio
Session average number of queries
and more",,"Add statistics to RWSplit $end$ Server average query response time for lifetime
Server average query response time 1s, 10s, 1m  (might be too expensive)
Server number of queries 1s, 10s, 1m
Session average length (time)
Session active/idle ratio
Session average number of queries
and more $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-10-09 10:20:58,Add statistics to RWSplit,"Server average query response time for lifetime
Server average query response time 1s, 10s, 1m  (might be too expensive)
Server number of queries 1s, 10s, 1m
Session average length (time)
Session active/idle ratio
Session average number of queries
and more",,0,0,0,0,0.0,"Add statistics to RWSplit $end$ Server average query response time for lifetime
Server average query response time 1s, 10s, 1m  (might be too expensive)
Server number of queries 1s, 10s, 1m
Session average length (time)
Session active/idle ratio
Session average number of queries
and more $acceptance criteria:$",0,0,0,0,0,0,1,120.283,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1191,MXS-208,Sub-Task,MXS,2015-06-17 04:06:05,,0,Load Test MaxScale 1.3,"Run Heavy Traffic against MaxScale 
(1) Overnight(12 hours) - collect log files, any crash files/backtrace  - provide it to development for diagnosis
(2)  24 hours -  collect error log files, any crash files/backtrace  - provide it to development for diagnosis
(3) 7 days - collect error log files, any crash files/backtrace  - provide it to development for diagnosis

The above set should be tested for (a) read connect routing (b) readwrite splitter (c) schema routing (d) with filters (e) without filters

[~tturenko] Please estimate resources needed for this task  - how many virtual machines/servers in parallel. These tests need to run continuously in parallel to our functional and regression testing.",,"Load Test MaxScale 1.3 $end$ Run Heavy Traffic against MaxScale 
(1) Overnight(12 hours) - collect log files, any crash files/backtrace  - provide it to development for diagnosis
(2)  24 hours -  collect error log files, any crash files/backtrace  - provide it to development for diagnosis
(3) 7 days - collect error log files, any crash files/backtrace  - provide it to development for diagnosis

The above set should be tested for (a) read connect routing (b) readwrite splitter (c) schema routing (d) with filters (e) without filters

[~tturenko] Please estimate resources needed for this task  - how many virtual machines/servers in parallel. These tests need to run continuously in parallel to our functional and regression testing. $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,12,,0,3,0,2,0,1,0,,0,850,3,0,0,2015-06-17 04:06:05,Load Test MaxScale 1.2,"Run Heavy Traffic against MaxScale 
(1) Overnight(12 hours) - collect log files, any crash files/backtrace  - provide it to development for diagnosis
(2)  24 hours -  collect error log files, any crash files/backtrace  - provide it to development for diagnosis
(3) 7 days - collect error log files, any crash files/backtrace  - provide it to development for diagnosis

The above set should be tested for (a) read connect routing (b) readwrite splitter (c) schema routing (d) with filters (e) without filters

[~tturenko] Please estimate resources needed for this task  - how many virtual machines/servers in parallel. These tests need to run continuously in parallel to our functional and regression testing.",,1,0,0,2,0.00854701,"Load Test MaxScale 1.2 $end$ Run Heavy Traffic against MaxScale 
(1) Overnight(12 hours) - collect log files, any crash files/backtrace  - provide it to development for diagnosis
(2)  24 hours -  collect error log files, any crash files/backtrace  - provide it to development for diagnosis
(3) 7 days - collect error log files, any crash files/backtrace  - provide it to development for diagnosis

The above set should be tested for (a) read connect routing (b) readwrite splitter (c) schema routing (d) with filters (e) without filters

[~tturenko] Please estimate resources needed for this task  - how many virtual machines/servers in parallel. These tests need to run continuously in parallel to our functional and regression testing. $acceptance criteria:$",1,1,0,0,0,0,1,0.0,5,2,0.4,1,0.2,0,0.0,0,0.0,0,0.0
1192,MXS-2086,Task,MXS,2018-10-09 09:09:48,,0,Prepare 2.3 training slides.,,,Prepare 2.3 training slides. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-10-09 09:09:48,Prepare 2.3 training slides.,,,0,0,0,0,0.0,Prepare 2.3 training slides. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,271,29,0.107011,12,0.0442804,7,0.0258303,5,0.0184502,5,0.0184502
1193,MXS-2087,Task,MXS,2018-10-09 09:09:58,,0,Prepare 2.3 training slides.,,,Prepare 2.3 training slides. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-10-09 09:09:58,Prepare 2.3 training slides.,,,0,0,0,0,0.0,Prepare 2.3 training slides. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,272,29,0.106618,12,0.0441176,7,0.0257353,5,0.0183824,5,0.0183824
1194,MXS-2088,Task,MXS,2018-10-09 09:10:06,,0,Prepare 2.3 training slides.,,,Prepare 2.3 training slides. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-10-09 09:10:06,Prepare 2.3 training slides.,,,0,0,0,0,0.0,Prepare 2.3 training slides. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,273,29,0.106227,12,0.043956,7,0.025641,5,0.018315,5,0.018315
1195,MXS-2089,Task,MXS,2018-10-09 09:10:12,,0,Prepare 2.3 training slides.,,,Prepare 2.3 training slides. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-10-09 09:10:12,Prepare 2.3 training slides.,,,0,0,0,0,0.0,Prepare 2.3 training slides. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,274,29,0.105839,12,0.0437956,7,0.0255474,5,0.0182482,5,0.0182482
1196,MXS-2090,Task,MXS,2018-10-09 09:44:32,,0,Remove requirement of GTID based replication for MXS-1980,,,Remove requirement of GTID based replication for MXS-1980 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-10-09 09:44:32,Remove requirement of GTID based replication for MXS-1980,,,0,0,0,0,0.0,Remove requirement of GTID based replication for MXS-1980 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,275,29,0.105455,12,0.0436364,7,0.0254545,5,0.0181818,5,0.0181818
1197,MXS-2091,Task,MXS,2018-10-09 10:26:09,,0,2.3 Blog post,,,2.3 Blog post $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-10-09 10:26:09,2.3 Blog post,,,0,0,0,0,0.0,2.3 Blog post $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,276,29,0.105072,12,0.0434783,7,0.0253623,5,0.0181159,5,0.0181159
1198,MXS-2092,Task,MXS,2018-10-09 10:26:40,,0,2.3 Blog post,,,2.3 Blog post $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,4,0,0,0,,0,850,0,0,0,2018-10-09 10:26:40,2.3 Blog post,,,0,0,0,0,0.0,2.3 Blog post $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,277,29,0.104693,12,0.0433213,7,0.0252708,5,0.0180505,5,0.0180505
1199,MXS-2093,Task,MXS,2018-10-09 10:27:03,,0,2.3 Blog post,,,2.3 Blog post $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,4,,0,850,0,0,0,2018-10-09 10:27:03,2.3 Blog post,,,0,0,0,0,0.0,2.3 Blog post $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,278,29,0.104317,12,0.0431655,7,0.0251799,5,0.0179856,5,0.0179856
1200,MXS-2094,Task,MXS,2018-10-09 10:27:55,,0,2.3 Blog post,,,2.3 Blog post $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,11,,0,0,0,6,0,0,0,,0,850,0,0,0,2018-10-09 10:27:55,2.3 Blog post,,,0,0,0,0,0.0,2.3 Blog post $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,279,29,0.103943,12,0.0430108,7,0.0250896,5,0.0179211,5,0.0179211
1201,MXS-2105,New Feature,MXS,2018-10-18 19:25:59,MXS-2528,0,Support SSL revocation lists in MaxScale,Currently MaxScale does not support [ssl_crl|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crl] or [ssl_crlpath|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crlpath]. This is considered an element of full support for TLS encryption as expressed by the customer in the associated support issue.,,Support SSL revocation lists in MaxScale $end$ Currently MaxScale does not support [ssl_crl|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crl] or [ssl_crlpath|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crlpath]. This is considered an element of full support for TLS encryption as expressed by the customer in the associated support issue. $acceptance criteria:$,,Juan,Juan,Minor,16,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-10-07 10:29:53,Support SSL revocation lists in MaxScale,Currently MaxScale does not support [ssl_crl|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crl] or [ssl_crlpath|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crlpath]. This is considered an element of full support for TLS encryption as expressed by the customer in the associated support issue.,,0,0,0,0,0.0,Support SSL revocation lists in MaxScale $end$ Currently MaxScale does not support [ssl_crl|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crl] or [ssl_crlpath|https://mariadb.com/kb/en/library/ssltls-system-variables/#ssl_crlpath]. This is considered an element of full support for TLS encryption as expressed by the customer in the associated support issue. $acceptance criteria:$,0,0,0,0,0,0,0,8487.05,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1202,MXS-2112,New Feature,MXS,2018-10-25 15:31:34,,0,Create tool to gather a total system report,"The tool can be used to get all log files, configurations and relevant statuses from any running maxscale instances. This simplifies the process of gathering information when there are problems with maxscale.",,"Create tool to gather a total system report $end$ The tool can be used to get all log files, configurations and relevant statuses from any running maxscale instances. This simplifies the process of gathering information when there are problems with maxscale. $acceptance criteria:$",,markus makela,markus makela,Major,11,,0,0,0,4,0,0,0,,0,850,0,0,0,2018-11-27 12:04:41,Create tool to gather a total system report,"The tool can be used to get all log files, configurations and relevant statuses from any running maxscale instances. This simplifies the process of gathering information when there are problems with maxscale.",,0,0,0,0,0.0,"Create tool to gather a total system report $end$ The tool can be used to get all log files, configurations and relevant statuses from any running maxscale instances. This simplifies the process of gathering information when there are problems with maxscale. $acceptance criteria:$",0,0,0,0,0,0,1,788.55,52,8,0.153846,8,0.153846,6,0.115385,6,0.115385,6,0.115385
1203,MXS-2120,New Feature,MXS,2018-10-29 08:36:29,,0,Maxscale doesn't detect master and do failover if master is down before maxscale restart. ,"It seems Maxscale doesn't detect master and do failover if master is down before maxscale restart. 

{code}
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Master, Running
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale stop
Stopping MaxScale:                                         [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ master/stop
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale start
Starting MaxScale: maxscale (pid 26334) is running...      [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Down
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$
{code}

So if master is down and we start maxscale, it should do the failover, select any of the slave and make it to master. ",,"Maxscale doesn't detect master and do failover if master is down before maxscale restart.  $end$ It seems Maxscale doesn't detect master and do failover if master is down before maxscale restart. 

{code}
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Master, Running
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale stop
Stopping MaxScale:                                         [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ master/stop
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale start
Starting MaxScale: maxscale (pid 26334) is running...      [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Down
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$
{code}

So if master is down and we start maxscale, it should do the failover, select any of the slave and make it to master.  $acceptance criteria:$",,Nilnandan Joshi,Nilnandan Joshi,Major,15,,0,2,1,1,0,0,0,,0,850,1,0,0,2018-11-13 11:46:10,Maxscale doesn't detect master and do failover if master is down before maxscale restart. ,"It seems Maxscale doesn't detect master and do failover if master is down before maxscale restart. 

{code}
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Master, Running
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale stop
Stopping MaxScale:                                         [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ master/stop
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale start
Starting MaxScale: maxscale (pid 26334) is running...      [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Down
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$
{code}

So if master is down and we start maxscale, it should do the failover, select any of the slave and make it to master. ",,0,0,0,0,0.0,"Maxscale doesn't detect master and do failover if master is down before maxscale restart.  $end$ It seems Maxscale doesn't detect master and do failover if master is down before maxscale restart. 

{code}
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Master, Running
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale stop
Stopping MaxScale:                                         [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ master/stop
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ sudo /etc/init.d/maxscale start
Starting MaxScale: maxscale (pid 26334) is running...      [  OK  ]
[nil@centos68 rsandbox_mariadb-10_2_14]$ 
[nil@centos68 rsandbox_mariadb-10_2_14]$ maxadmin list servers
Servers.
-------------------+-----------------+-------+-------------+--------------------
Server             | Address         | Port  | Connections | Status              
-------------------+-----------------+-------+-------------+--------------------
master             | 127.0.0.1       | 24044 |           0 | Down
slave1             | 127.0.0.1       | 24045 |           0 | Running
slave2             | 127.0.0.1       | 24046 |           0 | Running
-------------------+-----------------+-------+-------------+--------------------
[nil@centos68 rsandbox_mariadb-10_2_14]$
{code}

So if master is down and we start maxscale, it should do the failover, select any of the slave and make it to master.  $acceptance criteria:$",0,0,0,0,0,0,0,363.15,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1204,MXS-2124,Task,MXS,2018-10-30 11:29:16,,0,separate functional and performance tests by push in BuildBot,,,separate functional and performance tests by push in BuildBot $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-10-30 12:02:25,separate functional and performance tests by push in BuildBot,,,0,0,0,0,0.0,separate functional and performance tests by push in BuildBot $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.55,66,1,0.0151515,0,0.0,0,0.0,0,0.0,0,0.0
1205,MXS-2125,Task,MXS,2018-10-30 11:29:49,,0,performance test BuildBot task tuning,,,performance test BuildBot task tuning $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,0,0,3,0,0,3,,0,850,0,0,0,2018-10-30 12:02:22,performance test BuildBot task tuning,,,0,0,0,0,0.0,performance test BuildBot task tuning $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.533333,67,1,0.0149254,0,0.0,0,0.0,0,0.0,0,0.0
1206,MXS-2126,Sub-Task,MXS,2018-10-30 11:30:54,,0,Bind Maxscale and sysbench threads to certain CPU cores,,,Bind Maxscale and sysbench threads to certain CPU cores $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2018-10-30 11:30:54,Bind Maxscale and sysbench threads to certain CPU cores,,,0,0,0,0,0.0,Bind Maxscale and sysbench threads to certain CPU cores $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,68,1,0.0147059,0,0.0,0,0.0,0,0.0,0,0.0
1207,MXS-2127,Sub-Task,MXS,2018-10-30 11:31:40,,0,Bind every backend MariaDB server to certain CPU core,,,Bind every backend MariaDB server to certain CPU core $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2018-10-30 11:31:40,Bind every backend MariaDB server to certain CPU core,,,0,0,0,0,0.0,Bind every backend MariaDB server to certain CPU core $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,69,1,0.0144928,0,0.0,0,0.0,0,0.0,0,0.0
1208,MXS-2128,Sub-Task,MXS,2018-10-30 11:33:09,,0,patches for sysbench lua script,"lua scripts are not compatible with latest sysbench version 
also lua scripts should be integrated into performance test application repo or Maxscale repo",,"patches for sysbench lua script $end$ lua scripts are not compatible with latest sysbench version 
also lua scripts should be integrated into performance test application repo or Maxscale repo $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,3,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-10-30 11:33:09,patches for sysbench lua script,"lua scripts are not compatible with latest sysbench version 
also lua scripts should be integrated into performance test application repo or Maxscale repo",,0,0,0,0,0.0,"patches for sysbench lua script $end$ lua scripts are not compatible with latest sysbench version 
also lua scripts should be integrated into performance test application repo or Maxscale repo $acceptance criteria:$",0,0,0,0,0,0,1,0.0,70,1,0.0142857,0,0.0,0,0.0,0,0.0,0,0.0
1209,MXS-213,New Feature,MXS,2015-06-22 12:51:17,,0,Binlog Server Phase 3: Support Semi-Sync Replication in Binlog Server  ,"The mysqlbinlog backup implementation is sharing the code with the server semisync slave plugin. 
https://github.com/facebook/mysql-5.6/commit/60930e21c2c1d52c059cf89b37caaf0d40338a8f

Semi-sync implementation would enable lossless failover by sending backlog events to a candidate slave ",,"Binlog Server Phase 3: Support Semi-Sync Replication in Binlog Server   $end$ The mysqlbinlog backup implementation is sharing the code with the server semisync slave plugin. 
https://github.com/facebook/mysql-5.6/commit/60930e21c2c1d52c059cf89b37caaf0d40338a8f

Semi-sync implementation would enable lossless failover by sending backlog events to a candidate slave  $acceptance criteria:$",,VAROQUI Stephane,VAROQUI Stephane,Major,33,,0,5,0,3,0,1,0,,0,850,1,1,0,2016-01-12 12:26:57,Binlog Server Phase 3: Support Semi-Sync Replication in Binlog Server  ,"The mysqlbinlog backup implementation is sharing the code with the server semisync slave plugin. 
https://github.com/facebook/mysql-5.6/commit/60930e21c2c1d52c059cf89b37caaf0d40338a8f

Semi-sync implementation would enable lossless failover by sending backlog events to a candidate slave ",,0,0,0,0,0.0,"Binlog Server Phase 3: Support Semi-Sync Replication in Binlog Server   $end$ The mysqlbinlog backup implementation is sharing the code with the server semisync slave plugin. 
https://github.com/facebook/mysql-5.6/commit/60930e21c2c1d52c059cf89b37caaf0d40338a8f

Semi-sync implementation would enable lossless failover by sending backlog events to a candidate slave  $acceptance criteria:$",0,0,0,0,0,0,1,4895.58,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1210,MXS-2134,Sub-Task,MXS,2018-11-02 11:27:55,,0,"BLR + Galera + ""failover"" Blog",,,"BLR + Galera + ""failover"" Blog $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-02 11:27:55,"BLR + Galera + ""failover"" Blog",,,0,0,0,0,0.0,"BLR + Galera + ""failover"" Blog $end$ $acceptance criteria:$",0,0,0,0,0,0,1,0.0,280,29,0.103571,12,0.0428571,7,0.025,5,0.0178571,5,0.0178571
1211,MXS-2135,Sub-Task,MXS,2018-11-02 11:40:39,,0,Masking blog,,,Masking blog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-02 11:40:39,Masking blog,,,0,0,0,0,0.0,Masking blog $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,281,29,0.103203,12,0.0427046,7,0.024911,5,0.0177936,5,0.0177936
1212,MXS-2145,New Feature,MXS,2018-11-06 14:36:13,MXS-2529,0,new options prequery_sql_file  and connection_sql_file,"In some cases, it can be useful to execute some SQL statements at begin of the session or the query to set some session variables like autocommit, timeout, isolation_level etc.

New common router options.

prequery_sql_file=/home/root/scripts/prequery.sql
connection_sql_file=/home/root/scripts/connection.sql
",,"new options prequery_sql_file  and connection_sql_file $end$ In some cases, it can be useful to execute some SQL statements at begin of the session or the query to set some session variables like autocommit, timeout, isolation_level etc.

New common router options.

prequery_sql_file=/home/root/scripts/prequery.sql
connection_sql_file=/home/root/scripts/connection.sql
 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,20,,0,3,0,5,0,0,0,,0,850,0,0,0,2020-01-20 11:39:46,new options prequery_sql_file  and connection_sql_file,"In some cases, it can be useful to execute some SQL statements at begin of the session or the query to set some session variables like autocommit, timeout, isolation_level etc.

New common router options.

prequery_sql_file=/home/root/scripts/prequery.sql
connection_sql_file=/home/root/scripts/connection.sql
",,0,0,0,0,0.0,"new options prequery_sql_file  and connection_sql_file $end$ In some cases, it can be useful to execute some SQL statements at begin of the session or the query to set some session variables like autocommit, timeout, isolation_level etc.

New common router options.

prequery_sql_file=/home/root/scripts/prequery.sql
connection_sql_file=/home/root/scripts/connection.sql
 $acceptance criteria:$",0,0,0,0,0,0,1,10557.0,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
1213,MXS-2146,Task,MXS,2018-11-06 17:39:49,,0,Add test case for csmon,"Now with MXS-1467 done, a test case is needed but this depends on the test environment being able to provide a multi-node setup.",,"Add test case for csmon $end$ Now with MXS-1467 done, a test case is needed but this depends on the test environment being able to provide a multi-node setup. $acceptance criteria:$",,markus makela,markus makela,Major,8,,0,1,1,1,0,0,0,,0,850,0,0,0,2018-11-13 11:03:49,Add test case for csmon,"Now with MXS-1467 done, a test case is needed but this depends on the test environment being able to provide a multi-node setup.",,0,0,0,0,0.0,"Add test case for csmon $end$ Now with MXS-1467 done, a test case is needed but this depends on the test environment being able to provide a multi-node setup. $acceptance criteria:$",0,0,0,0,0,0,0,161.4,53,8,0.150943,8,0.150943,6,0.113208,6,0.113208,6,0.113208
1214,MXS-2149,Sub-Task,MXS,2018-11-07 14:33:03,,0,Create REST-API watchdog,,,Create REST-API watchdog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-07 14:33:03,Create REST-API watchdog,,,0,0,0,0,0.0,Create REST-API watchdog $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,282,29,0.102837,12,0.0425532,7,0.0248227,5,0.0177305,5,0.0177305
1215,MXS-2152,Task,MXS,2018-11-08 20:14:14,,0,Add test coverage measurement,Seeing how well each part of the code is covered by the test suite makes writing of new test cases easier. Untested areas of code should have higher priority and a high level of coverage is assumed to produce more stable software. ,,Add test coverage measurement $end$ Seeing how well each part of the code is covered by the test suite makes writing of new test cases easier. Untested areas of code should have higher priority and a high level of coverage is assumed to produce more stable software.  $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,1,1,1,0,0,0,,0,850,1,0,0,2019-04-29 14:48:44,Add test coverage measurement,Seeing how well each part of the code is covered by the test suite makes writing of new test cases easier. Untested areas of code should have higher priority and a high level of coverage is assumed to produce more stable software. ,,0,0,0,0,0.0,Add test coverage measurement $end$ Seeing how well each part of the code is covered by the test suite makes writing of new test cases easier. Untested areas of code should have higher priority and a high level of coverage is assumed to produce more stable software.  $acceptance criteria:$,0,0,0,0,0,0,0,4122.57,54,8,0.148148,8,0.148148,6,0.111111,6,0.111111,6,0.111111
1216,MXS-2154,Sub-Task,MXS,2018-11-09 10:56:47,,0,Comment Filter blog,,,Comment Filter blog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-09 10:56:47,Comment Filter blog,,,0,0,0,0,0.0,Comment Filter blog $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,283,29,0.102473,12,0.0424028,7,0.024735,5,0.0176678,5,0.0176678
1217,MXS-2155,Sub-Task,MXS,2018-11-09 12:19:18,,0,Table Family Sharding blog,,,Table Family Sharding blog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-09 12:19:18,Table Family Sharding blog,,,0,0,0,0,0.0,Table Family Sharding blog $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,284,29,0.102113,12,0.0422535,7,0.0246479,5,0.0176056,5,0.0176056
1218,MXS-2160,Task,MXS,2018-11-12 12:31:26,,0,Profile MaxScale with valgrind,,,Profile MaxScale with valgrind $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2018-11-13 11:54:22,Profile MaxScale with valgrind,,,0,0,0,0,0.0,Profile MaxScale with valgrind $end$ $acceptance criteria:$,0,0,0,0,0,0,0,23.3667,285,29,0.101754,12,0.0421053,7,0.0245614,5,0.0175439,5,0.0175439
1219,MXS-2161,Task,MXS,2018-11-13 08:30:53,,0,Smart Query Routing - high level design,,,Smart Query Routing - high level design $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,1,0,1,0,2,0,,0,850,1,0,0,2019-01-08 12:00:42,Smart Query Routing - Design,,,2,0,0,4,0.375,Smart Query Routing - Design $end$ $acceptance criteria:$,2,1,0,0,0,0,0,1347.48,286,29,0.101399,12,0.041958,7,0.0244755,5,0.0174825,5,0.0174825
1220,MXS-2163,Task,MXS,2018-11-13 08:33:39,MXS-2162,0,Clustrix Authenticator - Design,,,Clustrix Authenticator - Design $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-13 12:01:47,Clustrix Authenticator - Design,,,0,0,0,0,0.0,Clustrix Authenticator - Design $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.46667,287,30,0.10453,12,0.0418118,7,0.0243902,5,0.0174216,5,0.0174216
1221,MXS-2164,Task,MXS,2018-11-13 08:33:53,MXS-2162,0,Clustrix Monitor - Design,,,Clustrix Monitor - Design $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-13 12:02:10,Clustrix Monitor - Design,,,0,0,0,0,0.0,Clustrix Monitor - Design $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.46667,288,30,0.104167,12,0.0416667,7,0.0243056,5,0.0173611,5,0.0173611
1222,MXS-2165,Task,MXS,2018-11-13 08:34:26,MXS-2282,0,CDC Data Adapter Integration - Design,"h2. High-level Design
The system has three distinct types of processes:

* Replicating events from the master
* Processing events generated in ROW format and sending them to ColumnStore
* Processing events generated in STATEMENT format and sending them to ColumnStore

The process that replicates the events must drive the other two processes by delegating work and performing the necessary synchronizations when DDL statements are being processed. The driving process, henceforth referred to as the IO process, has one or more ""outputs"": The process that handles STATEMENT based events, henceforth referred to as the SQL process, and the process that handles the ROW based event handlers, henceforth referred to as the Table processes.

The SQL process receives all events that are replicated as plain-text SQL statements. With MIXED format replication most of the data will be in this format. All DDL statements as well as DML statements that weren't deemed necessary to replicate in ROW format events are sent as plain-text SQL. With ROW format replication, only DDL statements are sent as plain-text SQL and all other events are replicated as binary ROW format events.

A Table process exists for each opened table in the replication stream. This process will handle the conversion of raw binary format data into a suitable native format that can be exported to ColumnStore via the ColumnStore Bulk Load API. As the ColumnStore API only supports INSERT operations, UPDATE and DELETE operations replicated as ROW events must be translated back into SQL statements and executed on the database directly.

The SQL and Table processes are mutually exclusive, if the SQL process is running the Table process must be synchronized and stopped and vice versa. This guarantees that all ROW events are processed before any schema changes can occur and that all SQL statements are processed before ROW events with a new table layout are processed.   

 !CDC-design-overview.png|thumbnail! 

h2. Limitations
* Performance with moderate rates of UPDATE and DELETE queries is expected to be bad due to the lack of support for direct PM updates and general slowness for SQL layer DML queries.
* Upon failure the last transaction that was applied to ColumnStore is to be stored inside the data adapter. This implies a possibility of applying the same transaction twice.",,"CDC Data Adapter Integration - Design $end$ h2. High-level Design
The system has three distinct types of processes:

* Replicating events from the master
* Processing events generated in ROW format and sending them to ColumnStore
* Processing events generated in STATEMENT format and sending them to ColumnStore

The process that replicates the events must drive the other two processes by delegating work and performing the necessary synchronizations when DDL statements are being processed. The driving process, henceforth referred to as the IO process, has one or more ""outputs"": The process that handles STATEMENT based events, henceforth referred to as the SQL process, and the process that handles the ROW based event handlers, henceforth referred to as the Table processes.

The SQL process receives all events that are replicated as plain-text SQL statements. With MIXED format replication most of the data will be in this format. All DDL statements as well as DML statements that weren't deemed necessary to replicate in ROW format events are sent as plain-text SQL. With ROW format replication, only DDL statements are sent as plain-text SQL and all other events are replicated as binary ROW format events.

A Table process exists for each opened table in the replication stream. This process will handle the conversion of raw binary format data into a suitable native format that can be exported to ColumnStore via the ColumnStore Bulk Load API. As the ColumnStore API only supports INSERT operations, UPDATE and DELETE operations replicated as ROW events must be translated back into SQL statements and executed on the database directly.

The SQL and Table processes are mutually exclusive, if the SQL process is running the Table process must be synchronized and stopped and vice versa. This guarantees that all ROW events are processed before any schema changes can occur and that all SQL statements are processed before ROW events with a new table layout are processed.   

 !CDC-design-overview.png|thumbnail! 

h2. Limitations
* Performance with moderate rates of UPDATE and DELETE queries is expected to be bad due to the lack of support for direct PM updates and general slowness for SQL layer DML queries.
* Upon failure the last transaction that was applied to ColumnStore is to be stored inside the data adapter. This implies a possibility of applying the same transaction twice. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,1,0,1,0,4,0,,0,850,1,0,0,2019-01-08 11:14:03,CDC Data Adapter Integration - Design,"h2. Limitations
* Performance with moderate rates of UPDATE and DELETE queries is expected to be bad due to the lack of support for direct PM updates and general slowness for SQL layer DML queries.
* Upon failure the last transaction that was applied to ColumnStore is to be stored inside the data adapter. This implies a possibility of applying the same transaction twice.",,0,4,0,311,4.26027,"CDC Data Adapter Integration - Design $end$ h2. Limitations
* Performance with moderate rates of UPDATE and DELETE queries is expected to be bad due to the lack of support for direct PM updates and general slowness for SQL layer DML queries.
* Upon failure the last transaction that was applied to ColumnStore is to be stored inside the data adapter. This implies a possibility of applying the same transaction twice. $acceptance criteria:$",4,1,1,1,1,1,1,1346.65,289,30,0.103806,12,0.0415225,7,0.0242215,5,0.017301,5,0.017301
1223,MXS-2166,New Feature,MXS,2018-11-13 14:43:11,MXS-2529,0,Default SQL mode should be service specific.,"Using the global parameter {{sql_mode}} you can specify whether MaxScale should by default expect regular MariaDB SQL or PL/SQL.

That is inconvenient if some of the applications expect the former and some the latter and requires that either group set the right mode using {{set sql_mode=[oracle|default]}} at runtime.

If {{sql_mode}}  was made into a _service_ parameter, you could have one service expecting MariaDB SQL and another service expecting PL/SQL, and have the client connect to the service that has the right default. That way applications would not have to set the mode explicitly.",,"Default SQL mode should be service specific. $end$ Using the global parameter {{sql_mode}} you can specify whether MaxScale should by default expect regular MariaDB SQL or PL/SQL.

That is inconvenient if some of the applications expect the former and some the latter and requires that either group set the right mode using {{set sql_mode=[oracle|default]}} at runtime.

If {{sql_mode}}  was made into a _service_ parameter, you could have one service expecting MariaDB SQL and another service expecting PL/SQL, and have the client connect to the service that has the right default. That way applications would not have to set the mode explicitly. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,20,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-08-13 10:17:27,Default SQL mode should be service specific.,"Using the global parameter {{sql_mode}} you can specify whether MaxScale should by default expect regular MariaDB SQL or PL/SQL.

That is inconvenient if some of the applications expect the former and some the latter and requires that either group set the right mode using {{set sql_mode=[oracle|default]}} at runtime.

If {{sql_mode}}  was made into a _service_ parameter, you could have one service expecting MariaDB SQL and another service expecting PL/SQL, and have the client connect to the service that has the right default. That way applications would not have to set the mode explicitly.",,0,0,0,0,0.0,"Default SQL mode should be service specific. $end$ Using the global parameter {{sql_mode}} you can specify whether MaxScale should by default expect regular MariaDB SQL or PL/SQL.

That is inconvenient if some of the applications expect the former and some the latter and requires that either group set the right mode using {{set sql_mode=[oracle|default]}} at runtime.

If {{sql_mode}}  was made into a _service_ parameter, you could have one service expecting MariaDB SQL and another service expecting PL/SQL, and have the client connect to the service that has the right default. That way applications would not have to set the mode explicitly. $acceptance criteria:$",0,0,0,0,0,0,0,6547.57,290,31,0.106897,13,0.0448276,8,0.0275862,6,0.0206897,6,0.0206897
1224,MXS-2168,Task,MXS,2018-11-15 13:16:55,,0,"Add ""assume_unique_hostnames"" option to MariaDBMonitor","Currently, the monitor attempts to only use server id:s when building the replication topology. This has the weakness that the id:s cannot be used or trusted in any situation where MaxScale was started when a server was already down.

Example: If master (e.g.  server1) is down when MaxScale starts, and a slave claims that it is replicating from server1's host:port, MaxScale cannot assume that the slave is actually replicating from server1. This is because hostnames may not be unique, the hostnames seen by the backends may be different from the ones seen by MaxScale. This complicates cluster detection quite a bit.

In practice however, such setups are rare. Failover/switchover use the hostnames in MaxScale configuration when issuing CHANGE MASTER TO-commands, so any exotic network configuration is already incompatible with cluster manipulation features.

For these reasons, a monitor option which allows MaxScale to assume that hostnames/IPs are unique and consistent is required. This setting would be on by default and a requirement for any cluster operations.",,"Add ""assume_unique_hostnames"" option to MariaDBMonitor $end$ Currently, the monitor attempts to only use server id:s when building the replication topology. This has the weakness that the id:s cannot be used or trusted in any situation where MaxScale was started when a server was already down.

Example: If master (e.g.  server1) is down when MaxScale starts, and a slave claims that it is replicating from server1's host:port, MaxScale cannot assume that the slave is actually replicating from server1. This is because hostnames may not be unique, the hostnames seen by the backends may be different from the ones seen by MaxScale. This complicates cluster detection quite a bit.

In practice however, such setups are rare. Failover/switchover use the hostnames in MaxScale configuration when issuing CHANGE MASTER TO-commands, so any exotic network configuration is already incompatible with cluster manipulation features.

For these reasons, a monitor option which allows MaxScale to assume that hostnames/IPs are unique and consistent is required. This setting would be on by default and a requirement for any cluster operations. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2018-11-15 13:42:28,"Add ""assume_unique_hostnames"" option to MariaDBMonitor","Currently, the monitor attempts to only use server id:s when building the replication topology. This has the weakness that the id:s cannot be used or trusted in any situation where MaxScale was started when a server was already down.

Example: If master (e.g.  server1) is down when MaxScale starts, and a slave claims that it is replicating from server1's host:port, MaxScale cannot assume that the slave is actually replicating from server1. This is because hostnames may not be unique, the hostnames seen by the backends may be different from the ones seen by MaxScale. This complicates cluster detection quite a bit.

In practice however, such setups are rare. Failover/switchover use the hostnames in MaxScale configuration when issuing CHANGE MASTER TO-commands, so any exotic network configuration is already incompatible with cluster manipulation features.

For these reasons, a monitor option which allows MaxScale to assume that hostnames/IPs are unique and consistent is required. This setting would be on by default and a requirement for any cluster operations.",,0,0,0,0,0.0,"Add ""assume_unique_hostnames"" option to MariaDBMonitor $end$ Currently, the monitor attempts to only use server id:s when building the replication topology. This has the weakness that the id:s cannot be used or trusted in any situation where MaxScale was started when a server was already down.

Example: If master (e.g.  server1) is down when MaxScale starts, and a slave claims that it is replicating from server1's host:port, MaxScale cannot assume that the slave is actually replicating from server1. This is because hostnames may not be unique, the hostnames seen by the backends may be different from the ones seen by MaxScale. This complicates cluster detection quite a bit.

In practice however, such setups are rare. Failover/switchover use the hostnames in MaxScale configuration when issuing CHANGE MASTER TO-commands, so any exotic network configuration is already incompatible with cluster manipulation features.

For these reasons, a monitor option which allows MaxScale to assume that hostnames/IPs are unique and consistent is required. This setting would be on by default and a requirement for any cluster operations. $acceptance criteria:$",0,0,0,0,0,0,0,0.416667,6,1,0.166667,0,0.0,0,0.0,0,0.0,0,0.0
1225,MXS-2169,Task,MXS,2018-11-15 13:36:00,,0,"Split rejoin to two operations, add ""enforce_simple_topology""-option","The current ""auto_rejoin"" is too general as it does two things: rejoins standalone masters and also forces a 1-Master-N-slaves topology. These should be separated. Rejoin should only rejoin master servers which MaxScale saw going down and were failed over. This feature can work for arbitrary topologies.

Another option, ""enforce_simple_topology"", lets MaxScale assume that the topology should be one master with N slaves replicating from it (no relays, multimaster etc). This would allow for a similar aggressive rejoin which is currently implemented. Even the failover of a master which went down while MaxScale was off may be best moved to this feature, as it would be a bit like autopiloting. By activating this feature, a DBA gives MaxScale the liberty to manage the cluster as it seems fit. When this feature is not on, the monitor is more careful, preserves the topology if possible and only performs operations when it can be reasonably sure it's the right thing to do.",,"Split rejoin to two operations, add ""enforce_simple_topology""-option $end$ The current ""auto_rejoin"" is too general as it does two things: rejoins standalone masters and also forces a 1-Master-N-slaves topology. These should be separated. Rejoin should only rejoin master servers which MaxScale saw going down and were failed over. This feature can work for arbitrary topologies.

Another option, ""enforce_simple_topology"", lets MaxScale assume that the topology should be one master with N slaves replicating from it (no relays, multimaster etc). This would allow for a similar aggressive rejoin which is currently implemented. Even the failover of a master which went down while MaxScale was off may be best moved to this feature, as it would be a bit like autopiloting. By activating this feature, a DBA gives MaxScale the liberty to manage the cluster as it seems fit. When this feature is not on, the monitor is more careful, preserves the topology if possible and only performs operations when it can be reasonably sure it's the right thing to do. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,27,,0,0,2,6,0,0,0,,0,850,0,0,0,2018-11-27 12:20:15,"Split rejoin to two operations, add ""enforce_simple_topology""-option","The current ""auto_rejoin"" is too general as it does two things: rejoins standalone masters and also forces a 1-Master-N-slaves topology. These should be separated. Rejoin should only rejoin master servers which MaxScale saw going down and were failed over. This feature can work for arbitrary topologies.

Another option, ""enforce_simple_topology"", lets MaxScale assume that the topology should be one master with N slaves replicating from it (no relays, multimaster etc). This would allow for a similar aggressive rejoin which is currently implemented. Even the failover of a master which went down while MaxScale was off may be best moved to this feature, as it would be a bit like autopiloting. By activating this feature, a DBA gives MaxScale the liberty to manage the cluster as it seems fit. When this feature is not on, the monitor is more careful, preserves the topology if possible and only performs operations when it can be reasonably sure it's the right thing to do.",,0,0,0,0,0.0,"Split rejoin to two operations, add ""enforce_simple_topology""-option $end$ The current ""auto_rejoin"" is too general as it does two things: rejoins standalone masters and also forces a 1-Master-N-slaves topology. These should be separated. Rejoin should only rejoin master servers which MaxScale saw going down and were failed over. This feature can work for arbitrary topologies.

Another option, ""enforce_simple_topology"", lets MaxScale assume that the topology should be one master with N slaves replicating from it (no relays, multimaster etc). This would allow for a similar aggressive rejoin which is currently implemented. Even the failover of a master which went down while MaxScale was off may be best moved to this feature, as it would be a bit like autopiloting. By activating this feature, a DBA gives MaxScale the liberty to manage the cluster as it seems fit. When this feature is not on, the monitor is more careful, preserves the topology if possible and only performs operations when it can be reasonably sure it's the right thing to do. $acceptance criteria:$",0,0,0,0,0,0,1,286.733,7,1,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
1226,MXS-2192,Task,MXS,2018-11-27 10:59:17,,0,Run performance test on Sofia DC  and AWS,check results stabiltiy,,Run performance test on Sofia DC  and AWS $end$ check results stabiltiy $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-11-27 12:09:55,Run performance test on Sofia DC  and AWS,check results stabiltiy,,0,0,0,0,0.0,Run performance test on Sofia DC  and AWS $end$ check results stabiltiy $acceptance criteria:$,0,0,0,0,0,0,0,1.16667,71,1,0.0140845,0,0.0,0,0.0,0,0.0,0,0.0
1227,MXS-2195,Task,MXS,2018-11-27 11:25:23,,0,Investigate server ad-hoc test environment,,,Investigate server ad-hoc test environment $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,1,0,3,0,0,0,,0,850,1,0,0,2018-11-27 11:25:23,Investigate server ad-hoc test environment,,,0,0,0,0,0.0,Investigate server ad-hoc test environment $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,291,31,0.106529,13,0.0446735,8,0.0274914,6,0.0206186,6,0.0206186
1228,MXS-2196,Task,MXS,2018-11-27 11:33:17,,0,Move listeners out of services,The listeners should not be a part of the services they point to. This should make MXS-1951 easier to implement. ,,Move listeners out of services $end$ The listeners should not be a part of the services they point to. This should make MXS-1951 easier to implement.  $acceptance criteria:$,,markus makela,markus makela,Major,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-11-27 11:33:24,Move listeners out of services,The listeners should not be a part of the services they point to. This should make MXS-1951 easier to implement. ,,0,0,0,0,0.0,Move listeners out of services $end$ The listeners should not be a part of the services they point to. This should make MXS-1951 easier to implement.  $acceptance criteria:$,0,0,0,0,0,0,1,0.0,55,8,0.145455,8,0.145455,6,0.109091,6,0.109091,6,0.109091
1229,MXS-2197,Task,MXS,2018-11-27 11:40:45,MXS-2246,0,Make all routers C++,Use router and filter templates for all router and filter modules. Make APIs C++ to make it better. Add destructors to all routers and filters.,,Make all routers C++ $end$ Use router and filter templates for all router and filter modules. Make APIs C++ to make it better. Add destructors to all routers and filters. $acceptance criteria:$,,markus makela,markus makela,Major,14,,0,2,0,3,0,2,0,,0,850,2,0,0,2018-11-27 11:40:53,Make everything C++,Use router and filter templates for all router and filter modules. Make APIs C++ to make it better. Add destructors to all routers and filters.,,2,0,0,3,0.0645161,Make everything C++ $end$ Use router and filter templates for all router and filter modules. Make APIs C++ to make it better. Add destructors to all routers and filters. $acceptance criteria:$,2,1,0,0,0,0,1,0.0,56,8,0.142857,8,0.142857,6,0.107143,6,0.107143,6,0.107143
1230,MXS-2205,Task,MXS,2018-12-03 10:10:08,,0,Convert header files to c++,"Before converting the basic datatypes (SERVER, DCB etc) to classes the header files should be in C++. This is just a superficial conversion, the contents of the files are kept mostly intact.",,"Convert header files to c++ $end$ Before converting the basic datatypes (SERVER, DCB etc) to classes the header files should be in C++. This is just a superficial conversion, the contents of the files are kept mostly intact. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-12-03 10:10:25,Convert header files to c++,"Before converting the basic datatypes (SERVER, DCB etc) to classes the header files should be in C++. This is just a superficial conversion, the contents of the files are kept mostly intact.",,0,0,0,0,0.0,"Convert header files to c++ $end$ Before converting the basic datatypes (SERVER, DCB etc) to classes the header files should be in C++. This is just a superficial conversion, the contents of the files are kept mostly intact. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,8,1,0.125,0,0.0,0,0.0,0,0.0,0,0.0
1231,MXS-2208,Task,MXS,2018-12-04 13:37:11,MXS-2162,0,Add HTTP library.,The Clustrix monitor need to ping all servers regularly using HTTP.,,Add HTTP library. $end$ The Clustrix monitor need to ping all servers regularly using HTTP. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-12-11 11:39:21,Add HTTP library.,The Clustrix monitor need to ping all servers regularly using HTTP.,,0,0,0,0,0.0,Add HTTP library. $end$ The Clustrix monitor need to ping all servers regularly using HTTP. $acceptance criteria:$,0,0,0,0,0,0,0,166.033,292,31,0.106164,13,0.0445205,8,0.0273973,6,0.0205479,6,0.0205479
1232,MXS-2218,Task,MXS,2018-12-11 10:40:44,,0,Turn main thread into main worker.,Turn main thread into main worker and move the routing worker currently run in the main thread into a separate thread.,,Turn main thread into main worker. $end$ Turn main thread into main worker and move the routing worker currently run in the main thread into a separate thread. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2018-12-11 10:40:44,Turn main thread into main worker.,Turn main thread into main worker and move the routing worker currently run in the main thread into a separate thread.,,0,0,0,0,0.0,Turn main thread into main worker. $end$ Turn main thread into main worker and move the routing worker currently run in the main thread into a separate thread. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,293,31,0.105802,13,0.0443686,8,0.0273038,6,0.0204778,6,0.0204778
1233,MXS-2219,Task,MXS,2018-12-11 10:40:58,,0,Implement basic Clustrix Monitor,,,Implement basic Clustrix Monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,0,2,0,1,0,,0,850,1,0,0,2018-12-11 10:40:58,Implement Clustrix Monitor,,,1,0,0,1,0.166667,Implement Clustrix Monitor $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,294,31,0.105442,13,0.0442177,8,0.0272109,6,0.0204082,6,0.0204082
1234,MXS-2220,Task,MXS,2018-12-11 11:21:06,MXS-2246,0,Convert Server-struct to C++-class,"Move vulnerable fields to private class, use private class in Core methods and runtime modifications etc.",,"Convert Server-struct to C++-class $end$ Move vulnerable fields to private class, use private class in Core methods and runtime modifications etc. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2018-12-11 11:39:15,Convert Server-struct to C++-class,"Move vulnerable fields to private class, use private class in Core methods and runtime modifications etc.",,0,0,0,0,0.0,"Convert Server-struct to C++-class $end$ Move vulnerable fields to private class, use private class in Core methods and runtime modifications etc. $acceptance criteria:$",0,0,0,0,0,0,1,0.3,9,1,0.111111,0,0.0,0,0.0,0,0.0,0,0.0
1235,MXS-2223,Task,MXS,2018-12-13 07:24:58,,0,Additional logging when dealing with delayed slaves,"When detect_replication_lag setting is used, it would be great to have a way to get the following information recorded in Maxscale log:

1) Seconds behind master (or exact delay) as detected by Maxscale when there is a lag
2) Time when read connections are routed to Master because of delay
3) Time when read connections are routed back to slave (they are within allowed delay)
",,"Additional logging when dealing with delayed slaves $end$ When detect_replication_lag setting is used, it would be great to have a way to get the following information recorded in Maxscale log:

1) Seconds behind master (or exact delay) as detected by Maxscale when there is a lag
2) Time when read connections are routed to Master because of delay
3) Time when read connections are routed back to slave (they are within allowed delay)
 $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-01-08 11:51:03,Additional logging when dealing with delayed slaves,"When detect_replication_lag setting is used, it would be great to have a way to get the following information recorded in Maxscale log:

1) Seconds behind master (or exact delay) as detected by Maxscale when there is a lag
2) Time when read connections are routed to Master because of delay
3) Time when read connections are routed back to slave (they are within allowed delay)
",,0,0,0,0,0.0,"Additional logging when dealing with delayed slaves $end$ When detect_replication_lag setting is used, it would be great to have a way to get the following information recorded in Maxscale log:

1) Seconds behind master (or exact delay) as detected by Maxscale when there is a lag
2) Time when read connections are routed to Master because of delay
3) Time when read connections are routed back to slave (they are within allowed delay)
 $acceptance criteria:$",0,0,0,0,0,0,1,628.433,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1236,MXS-2225,Task,MXS,2018-12-19 10:59:27,,0,Clarify documentation of maxadmin/maxctrl,"It's rather easy to get confused how the users and enabled accounts of maxadmin relate to the users of maxctrl.

Ensure that the documentation clearly explains that and also what impact the presence of the default user and password have.
",,"Clarify documentation of maxadmin/maxctrl $end$ It's rather easy to get confused how the users and enabled accounts of maxadmin relate to the users of maxctrl.

Ensure that the documentation clearly explains that and also what impact the presence of the default user and password have.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-01-21 12:54:37,Clarify documentation of maxadmin/maxctrl,"It's rather easy to get confused how the users and enabled accounts of maxadmin relate to the users of maxctrl.

Ensure that the documentation clearly explains that and also what impact the presence of the default user and password have.
",,0,0,0,0,0.0,"Clarify documentation of maxadmin/maxctrl $end$ It's rather easy to get confused how the users and enabled accounts of maxadmin relate to the users of maxctrl.

Ensure that the documentation clearly explains that and also what impact the presence of the default user and password have.
 $acceptance criteria:$",0,0,0,0,0,0,0,793.917,295,32,0.108475,13,0.0440678,8,0.0271186,6,0.020339,6,0.020339
1237,MXS-2226,Task,MXS,2018-12-20 09:09:48,,0,Create long-running test for catching leaks.,"Create a long running (over night) test for catching memory leaks.

* MaxScale configured with a typical M/S backend and RWS.
* Clients of different character - varying session length, workload, amount of prepared statement, etc. - that continuously execute queries.

The amount of memory used should be tracked so that it at the end is possible to say whether the memory usage stays flat or is continuously increasing.
",,"Create long-running test for catching leaks. $end$ Create a long running (over night) test for catching memory leaks.

* MaxScale configured with a typical M/S backend and RWS.
* Clients of different character - varying session length, workload, amount of prepared statement, etc. - that continuously execute queries.

The amount of memory used should be tracked so that it at the end is possible to say whether the memory usage stays flat or is continuously increasing.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,3,0,0,1,,0,850,0,0,0,2019-01-08 11:58:22,Create long-running test for catching leaks.,"Create a long running (over night) test for catching memory leaks.

* MaxScale configured with a typical M/S backend and RWS.
* Clients of different character - varying session length, workload, amount of prepared statement, etc. - that continuously execute queries.

The amount of memory used should be tracked so that it at the end is possible to say whether the memory usage stays flat or is continuously increasing.
",,0,0,0,0,0.0,"Create long-running test for catching leaks. $end$ Create a long running (over night) test for catching memory leaks.

* MaxScale configured with a typical M/S backend and RWS.
* Clients of different character - varying session length, workload, amount of prepared statement, etc. - that continuously execute queries.

The amount of memory used should be tracked so that it at the end is possible to say whether the memory usage stays flat or is continuously increasing.
 $acceptance criteria:$",0,0,0,0,0,0,1,458.8,296,32,0.108108,13,0.0439189,8,0.027027,6,0.0202703,6,0.0202703
1238,MXS-2229,Task,MXS,2018-12-27 10:02:56,,0,"At startup, log as much as possible about the environment.","At startup, MaxScale should log:
* OS version
* Amount of memory
* Number of cores.
* etc.

That would be quite useful information to have when sorting out problems.",,"At startup, log as much as possible about the environment. $end$ At startup, MaxScale should log:
* OS version
* Amount of memory
* Number of cores.
* etc.

That would be quite useful information to have when sorting out problems. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-08-27 09:57:39,"At startup, log as much as possible about the environment.","At startup, MaxScale should log:
* OS version
* Amount of memory
* Number of cores.
* etc.

That would be quite useful information to have when sorting out problems.",,0,0,0,0,0.0,"At startup, log as much as possible about the environment. $end$ At startup, MaxScale should log:
* OS version
* Amount of memory
* Number of cores.
* etc.

That would be quite useful information to have when sorting out problems. $acceptance criteria:$",0,0,0,0,0,0,0,5831.9,297,32,0.107744,13,0.043771,8,0.026936,6,0.020202,6,0.020202
1239,MXS-2236,Sub-Task,MXS,2019-01-02 07:00:07,,0,long running test with different load ,own implemenation of long lasting test (without sysbench),,long running test with different load  $end$ own implemenation of long lasting test (without sysbench) $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-01-02 07:00:07,long running test with different load ,own implemenation of long lasting test (without sysbench),,0,0,0,0,0.0,long running test with different load  $end$ own implemenation of long lasting test (without sysbench) $acceptance criteria:$,0,0,0,0,0,0,1,0.0,72,1,0.0138889,0,0.0,0,0.0,0,0.0,0,0.0
1240,MXS-2243,Task,MXS,2019-01-08 11:28:14,,0,Create test VM setup from tests using MDBCI labels,"MDBCI now can create partial VM setup (start only needed VMs). It allows to run tests with minumim number of VMs and simplify local test runs (no need to bring 10 or more VMs). Every test should run MDBCI by itself to ask from MDBCI to create VM set according test labels (labels like REPL_BACKEND, GALERA_BACKEND, etc)",,"Create test VM setup from tests using MDBCI labels $end$ MDBCI now can create partial VM setup (start only needed VMs). It allows to run tests with minumim number of VMs and simplify local test runs (no need to bring 10 or more VMs). Every test should run MDBCI by itself to ask from MDBCI to create VM set according test labels (labels like REPL_BACKEND, GALERA_BACKEND, etc) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,13,,0,0,0,6,0,0,0,,0,850,0,0,0,2019-01-22 11:38:49,Create test VM setup from tests using MDBCI labels,"MDBCI now can create partial VM setup (start only needed VMs). It allows to run tests with minumim number of VMs and simplify local test runs (no need to bring 10 or more VMs). Every test should run MDBCI by itself to ask from MDBCI to create VM set according test labels (labels like REPL_BACKEND, GALERA_BACKEND, etc)",,0,0,0,0,0.0,"Create test VM setup from tests using MDBCI labels $end$ MDBCI now can create partial VM setup (start only needed VMs). It allows to run tests with minumim number of VMs and simplify local test runs (no need to bring 10 or more VMs). Every test should run MDBCI by itself to ask from MDBCI to create VM set according test labels (labels like REPL_BACKEND, GALERA_BACKEND, etc) $acceptance criteria:$",0,0,0,0,0,0,1,336.167,73,1,0.0136986,0,0.0,0,0.0,0,0.0,0,0.0
1241,MXS-2244,Task,MXS,2019-01-08 11:38:17,,0,SmartQuery Implementation phase 1,"Setup a master-slave cluster with both InnoDB and ColumnStore servers.
Explore viability of load-balancing the cluster using the query-canonical form mapped to server speed.
",,"SmartQuery Implementation phase 1 $end$ Setup a master-slave cluster with both InnoDB and ColumnStore servers.
Explore viability of load-balancing the cluster using the query-canonical form mapped to server speed.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-01-08 11:38:17,SmartQuery Implementation phase 1,"Setup a master-slave cluster with both InnoDB and ColumnStore servers.
Explore viability of load-balancing the cluster using the query-canonical form mapped to server speed.
",,0,0,0,0,0.0,"SmartQuery Implementation phase 1 $end$ Setup a master-slave cluster with both InnoDB and ColumnStore servers.
Explore viability of load-balancing the cluster using the query-canonical form mapped to server speed.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,298,32,0.107383,13,0.0436242,8,0.0268456,6,0.0201342,6,0.0201342
1242,MXS-2253,Sub-Task,MXS,2019-01-09 23:16:49,,0,Different grains of time in maxscale.cnf,"The maxscale.cnf file has some fields that use seconds and some that use milliseconds.  Customers can get these different grains to be confusing.  All timeouts/fields with time should either be milliseconds or seconds.

Tactically, we could put comments after the field in the default .cnf file.

monitor_interval=2500  # this field is in milliseconds",,"Different grains of time in maxscale.cnf $end$ The maxscale.cnf file has some fields that use seconds and some that use milliseconds.  Customers can get these different grains to be confusing.  All timeouts/fields with time should either be milliseconds or seconds.

Tactically, we could put comments after the field in the default .cnf file.

monitor_interval=2500  # this field is in milliseconds $acceptance criteria:$",,Austin Rutherford,Austin Rutherford,Minor,6,,0,4,0,7,0,0,0,,0,850,2,0,0,2019-02-05 10:33:02,Different grains of time in maxscale.cnf,"The maxscale.cnf file has some fields that use seconds and some that use milliseconds.  Customers can get these different grains to be confusing.  All timeouts/fields with time should either be milliseconds or seconds.

Tactically, we could put comments after the field in the default .cnf file.

monitor_interval=2500  # this field is in milliseconds",,0,0,0,0,0.0,"Different grains of time in maxscale.cnf $end$ The maxscale.cnf file has some fields that use seconds and some that use milliseconds.  Customers can get these different grains to be confusing.  All timeouts/fields with time should either be milliseconds or seconds.

Tactically, we could put comments after the field in the default .cnf file.

monitor_interval=2500  # this field is in milliseconds $acceptance criteria:$",0,0,0,0,0,0,1,635.267,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1243,MXS-2260,Task,MXS,2019-01-12 00:02:30,,0,Document examples on how to use REST API with popular REST clients,"I think it would be helpful to document some examples on how to use the REST API with some popular REST clients, such as curl.

I did some testing myself.

First, I configured the following:

{noformat}
[maxscale]
...
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
{noformat}

And then I did some tests with the passive flag.

To read the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#get-global-information

And did the following:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1876
Last-Modified: Fri, 11 Jan 2019 23:50:34 GMT
ETag: ""1""
Date: Fri, 11 Jan 19 23:52:01 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": true,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""uptime"": 1835
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat}

And then to change the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#update-maxscale-parameters

And did the following:

{noformat}
$ curl --include --request PATCH --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale --data @- <<EOF
> {
>     ""data"": {
>         ""attributes"": {
>             ""parameters"": {
>                 ""passive"": false
>             }
>         }
>     }
> }
> EOF
HTTP/1.1 204 No Content
Connection: Keep-Alive
Date: Fri, 11 Jan 19 23:52:45 GMT
{noformat}

And then I confirmed that the value changed by reading it again:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1877
Last-Modified: Fri, 11 Jan 2019 23:52:45 GMT
ETag: ""2""
Date: Fri, 11 Jan 19 23:53:17 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": false,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:52:42 GMT"",
            ""uptime"": 1911
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat}",,"Document examples on how to use REST API with popular REST clients $end$ I think it would be helpful to document some examples on how to use the REST API with some popular REST clients, such as curl.

I did some testing myself.

First, I configured the following:

{noformat}
[maxscale]
...
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
{noformat}

And then I did some tests with the passive flag.

To read the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#get-global-information

And did the following:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1876
Last-Modified: Fri, 11 Jan 2019 23:50:34 GMT
ETag: ""1""
Date: Fri, 11 Jan 19 23:52:01 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": true,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""uptime"": 1835
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat}

And then to change the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#update-maxscale-parameters

And did the following:

{noformat}
$ curl --include --request PATCH --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale --data @- <<EOF
> {
>     ""data"": {
>         ""attributes"": {
>             ""parameters"": {
>                 ""passive"": false
>             }
>         }
>     }
> }
> EOF
HTTP/1.1 204 No Content
Connection: Keep-Alive
Date: Fri, 11 Jan 19 23:52:45 GMT
{noformat}

And then I confirmed that the value changed by reading it again:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1877
Last-Modified: Fri, 11 Jan 2019 23:52:45 GMT
ETag: ""2""
Date: Fri, 11 Jan 19 23:53:17 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": false,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:52:42 GMT"",
            ""uptime"": 1911
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat} $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,11,,0,2,0,3,0,0,0,,0,850,2,0,0,2019-02-05 07:53:45,Document examples on how to use REST API with popular REST clients,"I think it would be helpful to document some examples on how to use the REST API with some popular REST clients, such as curl.

I did some testing myself.

First, I configured the following:

{noformat}
[maxscale]
...
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
{noformat}

And then I did some tests with the passive flag.

To read the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#get-global-information

And did the following:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1876
Last-Modified: Fri, 11 Jan 2019 23:50:34 GMT
ETag: ""1""
Date: Fri, 11 Jan 19 23:52:01 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": true,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""uptime"": 1835
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat}

And then to change the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#update-maxscale-parameters

And did the following:

{noformat}
$ curl --include --request PATCH --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale --data @- <<EOF
> {
>     ""data"": {
>         ""attributes"": {
>             ""parameters"": {
>                 ""passive"": false
>             }
>         }
>     }
> }
> EOF
HTTP/1.1 204 No Content
Connection: Keep-Alive
Date: Fri, 11 Jan 19 23:52:45 GMT
{noformat}

And then I confirmed that the value changed by reading it again:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1877
Last-Modified: Fri, 11 Jan 2019 23:52:45 GMT
ETag: ""2""
Date: Fri, 11 Jan 19 23:53:17 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": false,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:52:42 GMT"",
            ""uptime"": 1911
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat}",,0,0,0,0,0.0,"Document examples on how to use REST API with popular REST clients $end$ I think it would be helpful to document some examples on how to use the REST API with some popular REST clients, such as curl.

I did some testing myself.

First, I configured the following:

{noformat}
[maxscale]
...
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
{noformat}

And then I did some tests with the passive flag.

To read the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#get-global-information

And did the following:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1876
Last-Modified: Fri, 11 Jan 2019 23:50:34 GMT
ETag: ""1""
Date: Fri, 11 Jan 19 23:52:01 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": true,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""uptime"": 1835
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat}

And then to change the value, I followed these directions:

https://mariadb.com/kb/en/mariadb-maxscale-23-maxscale-resource/#update-maxscale-parameters

And did the following:

{noformat}
$ curl --include --request PATCH --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale --data @- <<EOF
> {
>     ""data"": {
>         ""attributes"": {
>             ""parameters"": {
>                 ""passive"": false
>             }
>         }
>     }
> }
> EOF
HTTP/1.1 204 No Content
Connection: Keep-Alive
Date: Fri, 11 Jan 19 23:52:45 GMT
{noformat}

And then I confirmed that the value changed by reading it again:

{noformat}
$ curl --include --basic --user ""admin:mariadb"" http://127.0.0.1:8989/v1/maxscale
HTTP/1.1 200 OK
Connection: Keep-Alive
Content-Length: 1877
Last-Modified: Fri, 11 Jan 2019 23:52:45 GMT
ETag: ""2""
Date: Fri, 11 Jan 19 23:53:17 GMT
Content-Type: application/json

{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/""
    },
    ""data"": {
        ""attributes"": {
            ""parameters"": {
                ""libdir"": ""/usr/lib64/maxscale"",
                ""datadir"": ""/var/lib/maxscale"",
                ""process_datadir"": ""/var/lib/maxscale/data2997"",
                ""cachedir"": ""/var/cache/maxscale"",
                ""configdir"": ""/etc"",
                ""config_persistdir"": ""/var/lib/maxscale/maxscale.cnf.d"",
                ""module_configdir"": ""/etc/maxscale.modules.d"",
                ""piddir"": ""/var/run/maxscale"",
                ""logdir"": ""/var/log/maxscale"",
                ""langdir"": ""/var/lib/maxscale"",
                ""execdir"": ""/usr/bin"",
                ""connector_plugindir"": ""/var/lib/plugin"",
                ""threads"": 4,
                ""thread_stack_size"": 8388608,
                ""writeq_high_water"": 0,
                ""writeq_low_water"": 0,
                ""auth_connect_timeout"": 3,
                ""auth_read_timeout"": 1,
                ""auth_write_timeout"": 2,
                ""skip_permission_checks"": false,
                ""admin_auth"": true,
                ""admin_enabled"": true,
                ""admin_log_auth_failures"": true,
                ""admin_host"": ""127.0.0.1"",
                ""admin_port"": 8989,
                ""admin_ssl_key"": """",
                ""admin_ssl_cert"": """",
                ""admin_ssl_ca_cert"": """",
                ""passive"": false,
                ""query_classifier"": """",
                ""query_classifier_cache_size"": 415775129
            },
            ""version"": ""2.3.2"",
            ""commit"": ""1126c687a4570f60ee26a163520198a3263ccbbd"",
            ""started_at"": ""Fri, 11 Jan 2019 23:21:26 GMT"",
            ""activated_at"": ""Fri, 11 Jan 2019 23:52:42 GMT"",
            ""uptime"": 1911
        },
        ""id"": ""maxscale"",
        ""type"": ""maxscale""
    }
}
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,583.85,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1244,MXS-2267,Task,MXS,2019-01-16 18:21:50,,0,Document which accounts PAM authenticators will actually use,"Based on the query in the commit for MXS-1716, PAM authenticators will only actually use PAM accounts that meet certain conditions.

https://github.com/mariadb-corporation/MaxScale/commit/aa260cf6cf5a91682fa6176f70d3c55263cec57e

PAM authenticators will use an account if:

* It uses the PAM plugin for authentication (plugin=pam in mysql.user).

And if:

* It has global SELECT privileges;
* Or it has some database-level privilege;
* Or it some table-level privilege.

This should probably be documented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/",,"Document which accounts PAM authenticators will actually use $end$ Based on the query in the commit for MXS-1716, PAM authenticators will only actually use PAM accounts that meet certain conditions.

https://github.com/mariadb-corporation/MaxScale/commit/aa260cf6cf5a91682fa6176f70d3c55263cec57e

PAM authenticators will use an account if:

* It uses the PAM plugin for authentication (plugin=pam in mysql.user).

And if:

* It has global SELECT privileges;
* Or it has some database-level privilege;
* Or it some table-level privilege.

This should probably be documented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/ $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,4,,1,0,4,1,0,0,0,,0,850,0,0,0,2019-01-21 10:21:29,Document which accounts PAM authenticators will actually use,"Based on the query in the commit for MXS-1716, PAM authenticators will only actually use PAM accounts that meet certain conditions.

https://github.com/mariadb-corporation/MaxScale/commit/aa260cf6cf5a91682fa6176f70d3c55263cec57e

PAM authenticators will use an account if:

* It uses the PAM plugin for authentication (plugin=pam in mysql.user).

And if:

* It has global SELECT privileges;
* Or it has some database-level privilege;
* Or it some table-level privilege.

This should probably be documented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/",,0,0,0,0,0.0,"Document which accounts PAM authenticators will actually use $end$ Based on the query in the commit for MXS-1716, PAM authenticators will only actually use PAM accounts that meet certain conditions.

https://github.com/mariadb-corporation/MaxScale/commit/aa260cf6cf5a91682fa6176f70d3c55263cec57e

PAM authenticators will use an account if:

* It uses the PAM plugin for authentication (plugin=pam in mysql.user).

And if:

* It has global SELECT privileges;
* Or it has some database-level privilege;
* Or it some table-level privilege.

This should probably be documented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/ $acceptance criteria:$",0,0,0,0,0,0,0,111.983,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1245,MXS-2269,Task,MXS,2019-01-16 21:20:05,,0,Document user and group mapping support for PAM authenticators,"MaxScale's PAM authenticators support user and group mapping, but this is completely undocumented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The relevant information and limitations should probably be documented. Feel free to refer to the MariaDB Server documentation that describes the pam_user_map PAM module, where it is relevant:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/

One very important thing to document is that based on the commit associated with MXS-1758, it looks like group mapping only works with MaxScale if the proxy user is the ''@'%' anonymous user.",,"Document user and group mapping support for PAM authenticators $end$ MaxScale's PAM authenticators support user and group mapping, but this is completely undocumented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The relevant information and limitations should probably be documented. Feel free to refer to the MariaDB Server documentation that describes the pam_user_map PAM module, where it is relevant:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/

One very important thing to document is that based on the commit associated with MXS-1758, it looks like group mapping only works with MaxScale if the proxy user is the ''@'%' anonymous user. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,4,,2,0,5,1,0,0,0,,0,850,0,0,0,2019-01-21 10:22:04,Document user and group mapping support for PAM authenticators,"MaxScale's PAM authenticators support user and group mapping, but this is completely undocumented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The relevant information and limitations should probably be documented. Feel free to refer to the MariaDB Server documentation that describes the pam_user_map PAM module, where it is relevant:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/

One very important thing to document is that based on the commit associated with MXS-1758, it looks like group mapping only works with MaxScale if the proxy user is the ''@'%' anonymous user.",,0,0,0,0,0.0,"Document user and group mapping support for PAM authenticators $end$ MaxScale's PAM authenticators support user and group mapping, but this is completely undocumented:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The relevant information and limitations should probably be documented. Feel free to refer to the MariaDB Server documentation that describes the pam_user_map PAM module, where it is relevant:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/

One very important thing to document is that based on the commit associated with MXS-1758, it looks like group mapping only works with MaxScale if the proxy user is the ''@'%' anonymous user. $acceptance criteria:$",0,0,0,0,0,0,0,109.017,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1246,MXS-2271,Task,MXS,2019-01-17 10:30:11,MXS-2246,0,Change MXS_MONITOR and related structs to classes,,,Change MXS_MONITOR and related structs to classes $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,12,,0,0,0,6,0,0,0,,0,850,0,0,0,2019-01-22 11:37:54,Change MXS_MONITOR and related structs to classes,,,0,0,0,0,0.0,Change MXS_MONITOR and related structs to classes $end$ $acceptance criteria:$,0,0,0,0,0,0,1,121.117,10,1,0.1,0,0.0,0,0.0,0,0.0,0,0.0
1247,MXS-2273,Task,MXS,2019-01-21 11:10:28,MXS-2162,0,Introduce server state BEING_DRAINED,,,Introduce server state BEING_DRAINED $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-01-21 11:10:28,Introduce server state BEING_DRAINED,,,0,0,0,0,0.0,Introduce server state BEING_DRAINED $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,299,32,0.107023,13,0.0434783,8,0.0267559,6,0.0200669,6,0.0200669
1248,MXS-2274,Task,MXS,2019-01-21 12:09:15,MXS-2162,0,Reserve names starting with @@.,"All objects (servers, services, filters, etc.) of MaxScale have names.

To ensure that there never can be any name-clashes between objects created by the database administrator - either statically via the configuration file or dynamically using maxadmin or maxctrl - and objects created by MaxScale itself (or some module), names starting with {{@@}} will be reserved.",,"Reserve names starting with @@. $end$ All objects (servers, services, filters, etc.) of MaxScale have names.

To ensure that there never can be any name-clashes between objects created by the database administrator - either statically via the configuration file or dynamically using maxadmin or maxctrl - and objects created by MaxScale itself (or some module), names starting with {{@@}} will be reserved. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,2,0,,0,850,0,0,0,2019-01-21 12:09:15,Reserve names starting with @.,"All objects (servers, services, filters, etc.) of MaxScale have names.

To ensure that there never can be any name-clashes between objects created by the database administrator - either statically via the configuration file or dynamically using maxadmin or maxctrl - and objects created by MaxScale itself (or some module), names starting with {{@}} will be reserved.",,1,1,0,4,0.03125,"Reserve names starting with @. $end$ All objects (servers, services, filters, etc.) of MaxScale have names.

To ensure that there never can be any name-clashes between objects created by the database administrator - either statically via the configuration file or dynamically using maxadmin or maxctrl - and objects created by MaxScale itself (or some module), names starting with {{@}} will be reserved. $acceptance criteria:$",2,1,0,0,0,0,0,0.0,300,32,0.106667,13,0.0433333,8,0.0266667,6,0.02,6,0.02
1249,MXS-2275,Task,MXS,2019-01-21 13:36:36,MXS-2162,0,Add module commands for initiating/canceling soft failure.,,,Add module commands for initiating/canceling soft failure. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-01-21 13:36:36,Add module commands for initiating/canceling soft failure.,,,0,0,0,0,0.0,Add module commands for initiating/canceling soft failure. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,301,33,0.109635,13,0.0431894,8,0.0265781,6,0.0199336,6,0.0199336
1250,MXS-2276,Task,MXS,2019-01-21 13:55:07,MXS-2162,0,Use all servers and not just bootstrap servers for checking cluster state.,,,Use all servers and not just bootstrap servers for checking cluster state. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-01-21 13:55:07,Use all servers and not just bootstrap servers for checking cluster state.,,,0,0,0,0,0.0,Use all servers and not just bootstrap servers for checking cluster state. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,302,33,0.109272,13,0.0430464,8,0.0264901,6,0.0198675,6,0.0198675
1251,MXS-2279,Task,MXS,2019-01-22 09:16:47,MXS-2278,0,Smart Query test setup,"Setup docker images etc. for streamlined testing.
",,"Smart Query test setup $end$ Setup docker images etc. for streamlined testing.
 $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-01-22 11:38:17,Smart Query test setup,"Setup docker images etc. for streamlined testing.
",,0,0,0,0,0.0,"Smart Query test setup $end$ Setup docker images etc. for streamlined testing.
 $acceptance criteria:$",0,0,0,0,0,0,0,2.35,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1252,MXS-2280,Task,MXS,2019-01-22 09:53:38,MXS-2278,0,Smart Query design prototype architecture,"Design reasonable architecture for prototype. This is not a final design for MaxScale SmartQuery components, but should be relatively clean to allow some reuse.",,"Smart Query design prototype architecture $end$ Design reasonable architecture for prototype. This is not a final design for MaxScale SmartQuery components, but should be relatively clean to allow some reuse. $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,8,,0,0,0,4,0,0,0,,0,850,0,0,0,2019-01-22 11:38:22,Smart Query design prototype architecture,"Design reasonable architecture for prototype. This is not a final design for MaxScale SmartQuery components, but should be relatively clean to allow some reuse.",,0,0,0,0,0.0,"Smart Query design prototype architecture $end$ Design reasonable architecture for prototype. This is not a final design for MaxScale SmartQuery components, but should be relatively clean to allow some reuse. $acceptance criteria:$",0,0,0,0,0,0,1,1.73333,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1253,MXS-2281,Task,MXS,2019-01-22 10:03:47,MXS-2278,0,Smart Query build prototype,Implement prototype code.,,Smart Query build prototype $end$ Implement prototype code. $acceptance criteria:$,,Niclas Antti,Niclas Antti,Major,7,,0,0,0,4,0,0,0,,0,850,0,0,0,2019-01-22 11:38:30,Smart Query build prototype,Implement prototype code.,,0,0,0,0,0.0,Smart Query build prototype $end$ Implement prototype code. $acceptance criteria:$,0,0,0,0,0,0,1,1.56667,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1254,MXS-2285,New Feature,MXS,2019-01-22 11:24:49,MXS-2282,0,Add CREATE TABLE parsing into the query classifier,"The following information from a CREATE TABLE statement is needed:
* Presence of indexes
* [Table options|https://mariadb.com/kb/en/library/create-table/#table-options] (specifically the {{ENGINE=...}} option)


The most important part is the capability to find out whether a CREATE TABLE statement contains index definitions or explicit ENGINE options. Optionally the following would speed up the processing:
 
* Table name
* Field names
* Field types (INT, VARCHAR etc.)
* Field attributes (UNSIGNED, NOT NULL)

For the field information, indexes must be identified and removed from the SQL statement as they aren't supported by ColumnStore. Unsupported types must also be converted to supported ones (in practice only SERIAL needs to be replaced with {{BIGINT UNSIGNED}}). For the table options, the main goal is to filter out any options that aren't supported by the ColumnStore engine and to remove explicit {{ENGINE=...}} options.
",,"Add CREATE TABLE parsing into the query classifier $end$ The following information from a CREATE TABLE statement is needed:
* Presence of indexes
* [Table options|https://mariadb.com/kb/en/library/create-table/#table-options] (specifically the {{ENGINE=...}} option)


The most important part is the capability to find out whether a CREATE TABLE statement contains index definitions or explicit ENGINE options. Optionally the following would speed up the processing:
 
* Table name
* Field names
* Field types (INT, VARCHAR etc.)
* Field attributes (UNSIGNED, NOT NULL)

For the field information, indexes must be identified and removed from the SQL statement as they aren't supported by ColumnStore. Unsupported types must also be converted to supported ones (in practice only SERIAL needs to be replaced with {{BIGINT UNSIGNED}}). For the table options, the main goal is to filter out any options that aren't supported by the ColumnStore engine and to remove explicit {{ENGINE=...}} options.
 $acceptance criteria:$",,markus makela,markus makela,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-01-22 11:32:19,Add CREATE TABLE parsing into the query classifier,"The following information from a CREATE TABLE statement is needed:
* Presence of indexes
* [Table options|https://mariadb.com/kb/en/library/create-table/#table-options] (specifically the {{ENGINE=...}} option)


The most important part is the capability to find out whether a CREATE TABLE statement contains index definitions or explicit ENGINE options. Optionally the following would speed up the processing:
 
* Table name
* Field names
* Field types (INT, VARCHAR etc.)
* Field attributes (UNSIGNED, NOT NULL)

For the field information, indexes must be identified and removed from the SQL statement as they aren't supported by ColumnStore. Unsupported types must also be converted to supported ones (in practice only SERIAL needs to be replaced with {{BIGINT UNSIGNED}}). For the table options, the main goal is to filter out any options that aren't supported by the ColumnStore engine and to remove explicit {{ENGINE=...}} options.
",,0,0,0,0,0.0,"Add CREATE TABLE parsing into the query classifier $end$ The following information from a CREATE TABLE statement is needed:
* Presence of indexes
* [Table options|https://mariadb.com/kb/en/library/create-table/#table-options] (specifically the {{ENGINE=...}} option)


The most important part is the capability to find out whether a CREATE TABLE statement contains index definitions or explicit ENGINE options. Optionally the following would speed up the processing:
 
* Table name
* Field names
* Field types (INT, VARCHAR etc.)
* Field attributes (UNSIGNED, NOT NULL)

For the field information, indexes must be identified and removed from the SQL statement as they aren't supported by ColumnStore. Unsupported types must also be converted to supported ones (in practice only SERIAL needs to be replaced with {{BIGINT UNSIGNED}}). For the table options, the main goal is to filter out any options that aren't supported by the ColumnStore engine and to remove explicit {{ENGINE=...}} options.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.116667,57,9,0.157895,8,0.140351,6,0.105263,6,0.105263,6,0.105263
1255,MXS-2294,Task,MXS,2019-01-26 09:53:41,,0,Document how to configure user and group mapping for PAM authenticators,"MaxScale's PAM authenticators support user and group mapping, but the documentation doesn't show how to configure it:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The process should probably be documented, similar to the MariaDB Server documentation that describes how to configure it from start to finish:

https://mariadb.com/kb/en/library/configuring-pam-authentication-and-user-mapping-with-unix-authentication/

Here's how I was able to configure it:

I started with a 2-node Galera Cluster and a MaxScale 2.3.3 instance with the following configuration:

{noformat}
[maxscale]
threads=4
syslog=1
maxlog=1
#log_to_shm=1
log_warning=1
log_notice=1
log_info=1
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
connector_plugindir=/usr/lib64/mysql/plugin/

[C1N1]
type=server
address=172.30.0.249
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[C1N2]
type=server
address=172.30.0.32
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[Galera-Monitor]
type=monitor
module=galeramon
servers=C1N1,
        C1N2
user=maxscale
password=password
monitor_interval=10000

[Read-Listener]
type=listener
service=Splitter-Service
port=3306
protocol=MariaDBClient
authenticator=PAMAuth

[Splitter-Service]
type=service
router=readwritesplit
servers=C1N1,
        C1N2
user=maxscale
password=password
max_slave_connections=100%
{noformat}

1.) First, on both backend nodes, I created the monitor user in PAM and set the user's password:

{noformat}
sudo adduser maxscale
sudo passwd maxscale
{noformat}

2.) Then, on both backend nodes and the maxscale node, I created the PAM user and group that I want to test:

{noformat}
sudo useradd alice
sudo passwd alice
sudo groupadd dba
sudo usermod -a -G dba alice 
{noformat}

3.) Then, on both backend nodes and the maxscale node, I also had to create a PAM user with the same name as the MariaDB user that my group is going to be mapped to:

{noformat}
sudo useradd dba -g dba
{noformat}

Because of this:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/#pam-user-with-same-name-as-mapped-mariadb-user-must-exist

4.) Then, on both backend nodes and the maxscale node, I compiled and installed the pam_user_map PAM module:

{noformat}
sudo yum install gcc pam-devel
wget https://raw.githubusercontent.com/MariaDB/server/10.4/plugin/auth_pam/mapper/pam_user_map.c 
gcc pam_user_map.c -shared -lpam -fPIC -o pam_user_map.so 
sudo install --mode=0755 pam_user_map.so /lib64/security/ 
{noformat}

5.) Then, on both backend nodes and the maxscale node, I configured my user and group mapping in /etc/security/user_map.conf:

{noformat}
@dba:dba
{noformat}


6.) Then, on both backend nodes, I installed the PAM authentication plugin:

{noformat}
INSTALL SONAME 'auth_pam';
{noformat}

7.) Then, on both backend nodes and the maxscale node, I configured the PAM service in /etc/pam.d/mariadb:

{noformat}
auth required pam_unix.so audit
auth optional pam_user_map.so
account required pam_unix.so audit
{noformat}

8.) Then, on both backend nodes, I gave the mysql user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow mysql
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

9.) Then, on the maxscale instance, I gave the maxscale user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow maxscale
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

10.) Then, on both backend nodes, I created my monitor user:

{noformat}
CREATE USER 'maxscale'@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT ALL PRIVILEGES ON *.* TO 'maxscale'@'%';
{noformat}

11.) Then, on both backend nodes, I created my dba user:

{noformat}
CREATE USER 'dba'@'%' IDENTIFIED BY 'strongpassword';
GRANT ALL PRIVILEGES ON *.* TO 'dba'@'%' ;
{noformat}

12.) Then, on the backend nodes, I created my anonymous proxy user.

First, I had to do some cleanup:

{noformat}
DELETE FROM mysql.db WHERE User='' AND Host='%';
FLUSH PRIVILEGES;
{noformat}

Because of this:

https://mariadb.com/kb/en/library/create-user/#fixing-a-legacy-default-anonymous-account

And then I created my anonymous proxy user:

{noformat}
CREATE USER ''@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT PROXY ON 'dba'@'%' TO ''@'%';
{noformat}

13.) Then, I restarted both backend nodes and maxscale.

14.) Then, I tested it out:

{noformat}
[ec2-user@ip-172-30-0-106 ~]$ mysql -u alice -h 172.30.0.106 -p
Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 5
Server version: 10.1.37-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> SELECT USER(), CURRENT_USER();
+--------------------------------------------------+----------------+
| USER()                                           | CURRENT_USER() |
+--------------------------------------------------+----------------+
| alice@ip-172-30-0-106.us-west-2.compute.internal | dba@%          |
+--------------------------------------------------+----------------+
1 row in set (0.001 sec)
{noformat}",,"Document how to configure user and group mapping for PAM authenticators $end$ MaxScale's PAM authenticators support user and group mapping, but the documentation doesn't show how to configure it:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The process should probably be documented, similar to the MariaDB Server documentation that describes how to configure it from start to finish:

https://mariadb.com/kb/en/library/configuring-pam-authentication-and-user-mapping-with-unix-authentication/

Here's how I was able to configure it:

I started with a 2-node Galera Cluster and a MaxScale 2.3.3 instance with the following configuration:

{noformat}
[maxscale]
threads=4
syslog=1
maxlog=1
#log_to_shm=1
log_warning=1
log_notice=1
log_info=1
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
connector_plugindir=/usr/lib64/mysql/plugin/

[C1N1]
type=server
address=172.30.0.249
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[C1N2]
type=server
address=172.30.0.32
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[Galera-Monitor]
type=monitor
module=galeramon
servers=C1N1,
        C1N2
user=maxscale
password=password
monitor_interval=10000

[Read-Listener]
type=listener
service=Splitter-Service
port=3306
protocol=MariaDBClient
authenticator=PAMAuth

[Splitter-Service]
type=service
router=readwritesplit
servers=C1N1,
        C1N2
user=maxscale
password=password
max_slave_connections=100%
{noformat}

1.) First, on both backend nodes, I created the monitor user in PAM and set the user's password:

{noformat}
sudo adduser maxscale
sudo passwd maxscale
{noformat}

2.) Then, on both backend nodes and the maxscale node, I created the PAM user and group that I want to test:

{noformat}
sudo useradd alice
sudo passwd alice
sudo groupadd dba
sudo usermod -a -G dba alice 
{noformat}

3.) Then, on both backend nodes and the maxscale node, I also had to create a PAM user with the same name as the MariaDB user that my group is going to be mapped to:

{noformat}
sudo useradd dba -g dba
{noformat}

Because of this:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/#pam-user-with-same-name-as-mapped-mariadb-user-must-exist

4.) Then, on both backend nodes and the maxscale node, I compiled and installed the pam_user_map PAM module:

{noformat}
sudo yum install gcc pam-devel
wget https://raw.githubusercontent.com/MariaDB/server/10.4/plugin/auth_pam/mapper/pam_user_map.c 
gcc pam_user_map.c -shared -lpam -fPIC -o pam_user_map.so 
sudo install --mode=0755 pam_user_map.so /lib64/security/ 
{noformat}

5.) Then, on both backend nodes and the maxscale node, I configured my user and group mapping in /etc/security/user_map.conf:

{noformat}
@dba:dba
{noformat}


6.) Then, on both backend nodes, I installed the PAM authentication plugin:

{noformat}
INSTALL SONAME 'auth_pam';
{noformat}

7.) Then, on both backend nodes and the maxscale node, I configured the PAM service in /etc/pam.d/mariadb:

{noformat}
auth required pam_unix.so audit
auth optional pam_user_map.so
account required pam_unix.so audit
{noformat}

8.) Then, on both backend nodes, I gave the mysql user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow mysql
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

9.) Then, on the maxscale instance, I gave the maxscale user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow maxscale
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

10.) Then, on both backend nodes, I created my monitor user:

{noformat}
CREATE USER 'maxscale'@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT ALL PRIVILEGES ON *.* TO 'maxscale'@'%';
{noformat}

11.) Then, on both backend nodes, I created my dba user:

{noformat}
CREATE USER 'dba'@'%' IDENTIFIED BY 'strongpassword';
GRANT ALL PRIVILEGES ON *.* TO 'dba'@'%' ;
{noformat}

12.) Then, on the backend nodes, I created my anonymous proxy user.

First, I had to do some cleanup:

{noformat}
DELETE FROM mysql.db WHERE User='' AND Host='%';
FLUSH PRIVILEGES;
{noformat}

Because of this:

https://mariadb.com/kb/en/library/create-user/#fixing-a-legacy-default-anonymous-account

And then I created my anonymous proxy user:

{noformat}
CREATE USER ''@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT PROXY ON 'dba'@'%' TO ''@'%';
{noformat}

13.) Then, I restarted both backend nodes and maxscale.

14.) Then, I tested it out:

{noformat}
[ec2-user@ip-172-30-0-106 ~]$ mysql -u alice -h 172.30.0.106 -p
Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 5
Server version: 10.1.37-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> SELECT USER(), CURRENT_USER();
+--------------------------------------------------+----------------+
| USER()                                           | CURRENT_USER() |
+--------------------------------------------------+----------------+
| alice@ip-172-30-0-106.us-west-2.compute.internal | dba@%          |
+--------------------------------------------------+----------------+
1 row in set (0.001 sec)
{noformat} $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,10,,3,0,7,2,0,2,0,,0,850,0,2,0,2019-02-05 10:17:33,Document how to configure user and group mapping for PAM authenticators,"MaxScale's PAM authenticators support user and group mapping, but the documentation doesn't show how to configure it:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The process should probably be documented, similar to the MariaDB Server documentation that describes how to configure it from start to finish:

https://mariadb.com/kb/en/library/configuring-pam-authentication-and-user-mapping-with-unix-authentication/

Here's how I was able to configure it:

I started with a 2-node Galera Cluster and a MaxScale 2.3.3 instance with the following configuration:

{noformat}
[maxscale]
threads=4
syslog=1
maxlog=1
#log_to_shm=1
log_warning=1
log_notice=1
log_info=1
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
connector_plugindir=/usr/lib64/mysql/plugin/

[C1N1]
type=server
address=172.30.0.249
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[C1N2]
type=server
address=172.30.0.32
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[Galera-Monitor]
type=monitor
module=galeramon
servers=C1N1,
        C1N2
user=maxscale
password=password
monitor_interval=10000

[Read-Listener]
type=listener
service=Splitter-Service
port=3306
protocol=MariaDBClient
authenticator=PAMAuth

[Splitter-Service]
type=service
router=readwritesplit
servers=C1N1,
        C1N2
user=maxscale
password=password
max_slave_connections=100%
{noformat}

1.) First, on both backend nodes, I created the monitor user in PAM and set the user's password:

{noformat}
sudo adduser maxscale
sudo passwd maxscale
{noformat}

2.) Then, on both backend nodes and the maxscale node, I created the PAM user and group that I want to test:

{noformat}
sudo useradd alice
sudo passwd alice
sudo groupadd dba
sudo usermod -a -G dba alice 
{noformat}

3.) Then, on both backend nodes and the maxscale node, I also had to create a PAM user with the same name as the MariaDB user that my group is going to be mapped to:

{noformat}
sudo useradd dba -g dba
{noformat}

Because of this:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/#pam-user-with-same-name-as-mapped-mariadb-user-must-exist

4.) Then, on both backend nodes and the maxscale node, I compiled and installed the pam_user_map PAM module:

{noformat}
sudo yum install gcc pam-devel
wget https://raw.githubusercontent.com/MariaDB/server/10.4/plugin/auth_pam/mapper/pam_user_map.c 
gcc pam_user_map.c -shared -lpam -fPIC -o pam_user_map.so 
sudo install --mode=0755 pam_user_map.so /lib64/security/ 
{noformat}

5.) Then, on both backend nodes and the maxscale node, I configured my user and group mapping in /etc/security/user_map.conf:

{noformat}
@dba:dba
{noformat}


6.) Then, on both backend nodes, I installed the PAM authentication plugin:

{noformat}
INSTALL SONAME 'auth_pam';
{noformat}

7.) Then, on both backend nodes and the maxscale node, I configured the PAM service in /etc/pam.d/mariadb:

{noformat}
auth required pam_unix.so audit
auth optional pam_user_map.so
account required pam_unix.so audit
{noformat}

8.) Then, on both backend nodes, I gave the mysql user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow mysql
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

9.) Then, on the maxscale instance, I gave the maxscale user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow maxscale
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

10.) Then, on both backend nodes, I created my monitor user:

{noformat}
CREATE USER 'maxscale'@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT ALL PRIVILEGES ON *.* TO 'maxscale'@'%';
{noformat}

11.) Then, on both backend nodes, I created my dba user:

{noformat}
CREATE USER 'dba'@'%' IDENTIFIED BY 'strongpassword';
GRANT ALL PRIVILEGES ON *.* TO 'dba'@'%' ;
{noformat}

12.) Then, on the backend nodes, I created my anonymous proxy user.

First, I had to do some cleanup:

{noformat}
DELETE FROM mysql.db WHERE User='' AND Host='%';
FLUSH PRIVILEGES;
{noformat}

Because of this:

https://mariadb.com/kb/en/library/create-user/#fixing-a-legacy-default-anonymous-account

And then I created my anonymous proxy user:

{noformat}
CREATE USER ''@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT PROXY ON 'dba'@'%' TO ''@'%';
{noformat}

13.) Then, I restarted both backend nodes and maxscale.

14.) Then, I tested it out:

{noformat}
[ec2-user@ip-172-30-0-106 ~]$ mysql -u alice -h 172.30.0.106 -p
Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 5
Server version: 10.1.37-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> SELECT USER(), CURRENT_USER();
+--------------------------------------------------+----------------+
| USER()                                           | CURRENT_USER() |
+--------------------------------------------------+----------------+
| alice@ip-172-30-0-106.us-west-2.compute.internal | dba@%          |
+--------------------------------------------------+----------------+
1 row in set (0.001 sec)
{noformat}",,0,0,0,0,0.0,"Document how to configure user and group mapping for PAM authenticators $end$ MaxScale's PAM authenticators support user and group mapping, but the documentation doesn't show how to configure it:

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/

The process should probably be documented, similar to the MariaDB Server documentation that describes how to configure it from start to finish:

https://mariadb.com/kb/en/library/configuring-pam-authentication-and-user-mapping-with-unix-authentication/

Here's how I was able to configure it:

I started with a 2-node Galera Cluster and a MaxScale 2.3.3 instance with the following configuration:

{noformat}
[maxscale]
threads=4
syslog=1
maxlog=1
#log_to_shm=1
log_warning=1
log_notice=1
log_info=1
admin_host=127.0.0.1
admin_port=8989
admin_auth=1
admin_enabled=1
connector_plugindir=/usr/lib64/mysql/plugin/

[C1N1]
type=server
address=172.30.0.249
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[C1N2]
type=server
address=172.30.0.32
port=3306
protocol=MariaDBBackend
authenticator=PAMBackendAuth

[Galera-Monitor]
type=monitor
module=galeramon
servers=C1N1,
        C1N2
user=maxscale
password=password
monitor_interval=10000

[Read-Listener]
type=listener
service=Splitter-Service
port=3306
protocol=MariaDBClient
authenticator=PAMAuth

[Splitter-Service]
type=service
router=readwritesplit
servers=C1N1,
        C1N2
user=maxscale
password=password
max_slave_connections=100%
{noformat}

1.) First, on both backend nodes, I created the monitor user in PAM and set the user's password:

{noformat}
sudo adduser maxscale
sudo passwd maxscale
{noformat}

2.) Then, on both backend nodes and the maxscale node, I created the PAM user and group that I want to test:

{noformat}
sudo useradd alice
sudo passwd alice
sudo groupadd dba
sudo usermod -a -G dba alice 
{noformat}

3.) Then, on both backend nodes and the maxscale node, I also had to create a PAM user with the same name as the MariaDB user that my group is going to be mapped to:

{noformat}
sudo useradd dba -g dba
{noformat}

Because of this:

https://mariadb.com/kb/en/library/user-and-group-mapping-with-pam/#pam-user-with-same-name-as-mapped-mariadb-user-must-exist

4.) Then, on both backend nodes and the maxscale node, I compiled and installed the pam_user_map PAM module:

{noformat}
sudo yum install gcc pam-devel
wget https://raw.githubusercontent.com/MariaDB/server/10.4/plugin/auth_pam/mapper/pam_user_map.c 
gcc pam_user_map.c -shared -lpam -fPIC -o pam_user_map.so 
sudo install --mode=0755 pam_user_map.so /lib64/security/ 
{noformat}

5.) Then, on both backend nodes and the maxscale node, I configured my user and group mapping in /etc/security/user_map.conf:

{noformat}
@dba:dba
{noformat}


6.) Then, on both backend nodes, I installed the PAM authentication plugin:

{noformat}
INSTALL SONAME 'auth_pam';
{noformat}

7.) Then, on both backend nodes and the maxscale node, I configured the PAM service in /etc/pam.d/mariadb:

{noformat}
auth required pam_unix.so audit
auth optional pam_user_map.so
account required pam_unix.so audit
{noformat}

8.) Then, on both backend nodes, I gave the mysql user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow mysql
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

9.) Then, on the maxscale instance, I gave the maxscale user access to /etc/shadow:

{noformat}
sudo groupadd shadow
sudo usermod -a -G shadow maxscale
sudo chown root:shadow /etc/shadow
sudo chmod g+r /etc/shadow
{noformat}

10.) Then, on both backend nodes, I created my monitor user:

{noformat}
CREATE USER 'maxscale'@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT ALL PRIVILEGES ON *.* TO 'maxscale'@'%';
{noformat}

11.) Then, on both backend nodes, I created my dba user:

{noformat}
CREATE USER 'dba'@'%' IDENTIFIED BY 'strongpassword';
GRANT ALL PRIVILEGES ON *.* TO 'dba'@'%' ;
{noformat}

12.) Then, on the backend nodes, I created my anonymous proxy user.

First, I had to do some cleanup:

{noformat}
DELETE FROM mysql.db WHERE User='' AND Host='%';
FLUSH PRIVILEGES;
{noformat}

Because of this:

https://mariadb.com/kb/en/library/create-user/#fixing-a-legacy-default-anonymous-account

And then I created my anonymous proxy user:

{noformat}
CREATE USER ''@'%' IDENTIFIED VIA pam USING 'mariadb';
GRANT PROXY ON 'dba'@'%' TO ''@'%';
{noformat}

13.) Then, I restarted both backend nodes and maxscale.

14.) Then, I tested it out:

{noformat}
[ec2-user@ip-172-30-0-106 ~]$ mysql -u alice -h 172.30.0.106 -p
Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 5
Server version: 10.1.37-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> SELECT USER(), CURRENT_USER();
+--------------------------------------------------+----------------+
| USER()                                           | CURRENT_USER() |
+--------------------------------------------------+----------------+
| alice@ip-172-30-0-106.us-west-2.compute.internal | dba@%          |
+--------------------------------------------------+----------------+
1 row in set (0.001 sec)
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,240.383,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1256,MXS-23,New Feature,MXS,2014-01-22 18:15:00,,0,bugzillaId-553: maxadmin cmdline arg command vs. filename ambiguity - a potential security issue?,"This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=553

The fix version in bugzilla shows ""commit 5cfbfe39ac942e406de719612257ef797dca9c7f""

+Hartmut Holzgraefe 2014-09-22 18:15:46 UTC+
After parsing options maxadmin takes the remaining command line argument(s), first checks if these match a readable file name and executes commands from this file, or takes the extra argument(s) as literal maxadmin commands ...

Problem is that a file name can be the same as a maxadmin command, e.g.:

  echo list clients > list\ servers 
  maxadmin ... list servers

will list clients, not servers, as ""list servers"" is now a valid, readable file ....

While it's unlikely that someone names files like this by accident it may be an attack vector for maliciously changing the behaviour of maxscale invocations with comdline commands ...

proposed fix:

1) either have an explicit ""-f|--command-file FILENAME"" option

2) or do not support giving a file name at all, just rely on input redirection, e.g.:

    maxadmin ... < cmdfile.txt

  instead of 

    maxadmin ... cmdfile.txt

The ""mysql"" command line client takes the 2nd approach, so that's probably what 
maxadmin should be doing, too?
",,"bugzillaId-553: maxadmin cmdline arg command vs. filename ambiguity - a potential security issue? $end$ This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=553

The fix version in bugzilla shows ""commit 5cfbfe39ac942e406de719612257ef797dca9c7f""

+Hartmut Holzgraefe 2014-09-22 18:15:46 UTC+
After parsing options maxadmin takes the remaining command line argument(s), first checks if these match a readable file name and executes commands from this file, or takes the extra argument(s) as literal maxadmin commands ...

Problem is that a file name can be the same as a maxadmin command, e.g.:

  echo list clients > list\ servers 
  maxadmin ... list servers

will list clients, not servers, as ""list servers"" is now a valid, readable file ....

While it's unlikely that someone names files like this by accident it may be an attack vector for maliciously changing the behaviour of maxscale invocations with comdline commands ...

proposed fix:

1) either have an explicit ""-f|--command-file FILENAME"" option

2) or do not support giving a file name at all, just rely on input redirection, e.g.:

    maxadmin ... < cmdfile.txt

  instead of 

    maxadmin ... cmdfile.txt

The ""mysql"" command line client takes the 2nd approach, so that's probably what 
maxadmin should be doing, too?
 $acceptance criteria:$",,Dipti Joshi,hartmut,Minor,22,,0,2,0,1,0,3,0,,0,850,0,3,0,2017-03-29 09:22:25,bugzillaId-553: maxadmin cmdline arg command vs. filename ambiguity - a potential security issue?,"This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=553

The fix version in bugzilla shows ""commit 5cfbfe39ac942e406de719612257ef797dca9c7f""

+Hartmut Holzgraefe 2014-09-22 18:15:46 UTC+
After parsing options maxadmin takes the remaining command line argument(s), first checks if these match a readable file name and executes commands from this file, or takes the extra argument(s) as literal maxadmin commands ...

Problem is that a file name can be the same as a maxadmin command, e.g.:

  echo list clients > list\ servers 
  maxadmin ... list servers

will list clients, not servers, as ""list servers"" is now a valid, readable file ....

While it's unlikely that someone names files like this by accident it may be an attack vector for maliciously changing the behaviour of maxscale invocations with comdline commands ...

proposed fix:

1) either have an explicit ""-f|--command-file FILENAME"" option

2) or do not support giving a file name at all, just rely on input redirection, e.g.:

    maxadmin ... < cmdfile.txt

  instead of 

    maxadmin ... cmdfile.txt

The ""mysql"" command line client takes the 2nd approach, so that's probably what 
maxadmin should be doing, too?
",,0,0,0,0,0.0,"bugzillaId-553: maxadmin cmdline arg command vs. filename ambiguity - a potential security issue? $end$ This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=553

The fix version in bugzilla shows ""commit 5cfbfe39ac942e406de719612257ef797dca9c7f""

+Hartmut Holzgraefe 2014-09-22 18:15:46 UTC+
After parsing options maxadmin takes the remaining command line argument(s), first checks if these match a readable file name and executes commands from this file, or takes the extra argument(s) as literal maxadmin commands ...

Problem is that a file name can be the same as a maxadmin command, e.g.:

  echo list clients > list\ servers 
  maxadmin ... list servers

will list clients, not servers, as ""list servers"" is now a valid, readable file ....

While it's unlikely that someone names files like this by accident it may be an attack vector for maliciously changing the behaviour of maxscale invocations with comdline commands ...

proposed fix:

1) either have an explicit ""-f|--command-file FILENAME"" option

2) or do not support giving a file name at all, just rely on input redirection, e.g.:

    maxadmin ... < cmdfile.txt

  instead of 

    maxadmin ... cmdfile.txt

The ""mysql"" command line client takes the 2nd approach, so that's probably what 
maxadmin should be doing, too?
 $acceptance criteria:$",0,0,0,0,0,0,0,27879.1,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1257,MXS-2313,New Feature,MXS,2019-02-04 14:54:43,,0,Replace weightby,"The {{weightby}} mechanism is rarely used to do what it is intended for, which is manual adjustments to the load balancing alogorithms, and most of the time it is abused to forcibly discard a server from routing.

The use of conditional server use, or in other words priority based usage, is a valid use-case as in some scenarios a local server is always better compared a remote one but the remote server should be usable in case the local one fails.",,"Replace weightby $end$ The {{weightby}} mechanism is rarely used to do what it is intended for, which is manual adjustments to the load balancing alogorithms, and most of the time it is abused to forcibly discard a server from routing.

The use of conditional server use, or in other words priority based usage, is a valid use-case as in some scenarios a local server is always better compared a remote one but the remote server should be usable in case the local one fails. $acceptance criteria:$",,markus makela,markus makela,Major,6,,0,4,0,1,0,0,0,,0,850,1,0,0,2019-03-04 11:47:32,Replace weightby,"The {{weightby}} mechanism is rarely used to do what it is intended for, which is manual adjustments to the load balancing alogorithms, and most of the time it is abused to forcibly discard a server from routing.

The use of conditional server use, or in other words priority based usage, is a valid use-case as in some scenarios a local server is always better compared a remote one but the remote server should be usable in case the local one fails.",,0,0,0,0,0.0,"Replace weightby $end$ The {{weightby}} mechanism is rarely used to do what it is intended for, which is manual adjustments to the load balancing alogorithms, and most of the time it is abused to forcibly discard a server from routing.

The use of conditional server use, or in other words priority based usage, is a valid use-case as in some scenarios a local server is always better compared a remote one but the remote server should be usable in case the local one fails. $acceptance criteria:$",0,0,0,0,0,0,0,668.867,58,9,0.155172,8,0.137931,6,0.103448,6,0.103448,6,0.103448
1258,MXS-2314,New Feature,MXS,2019-02-04 16:42:27,MXS-2162,0,Allow Services to be Populated by Monitor,"Currently the servers have to be separately defined for both the service and the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
servers=Server1,Server2,Server3
{code}
Since the set of servers is typically the same, this is both tedious and error-prone. By allowing a service to refer to a monitor, the servers of the service do not explicitly be listed but are defined by the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
monitor=Monitor1
{code}
This also makes it possible to allow the monitor to dynamically define the servers at runtime.",,"Allow Services to be Populated by Monitor $end$ Currently the servers have to be separately defined for both the service and the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
servers=Server1,Server2,Server3
{code}
Since the set of servers is typically the same, this is both tedious and error-prone. By allowing a service to refer to a monitor, the servers of the service do not explicitly be listed but are defined by the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
monitor=Monitor1
{code}
This also makes it possible to allow the monitor to dynamically define the servers at runtime. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,1,1,0,0,0,,0,850,0,0,0,2019-02-04 16:44:24,Allow Services to be Populated by Monitor,"Currently the servers have to be separately defined for both the service and the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
servers=Server1,Server2,Server3
{code}
Since the set of servers is typically the same, this is both tedious and error-prone. By allowing a service to refer to a monitor, the servers of the service do not explicitly be listed but are defined by the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
monitor=Monitor1
{code}
This also makes it possible to allow the monitor to dynamically define the servers at runtime.",,0,0,0,0,0.0,"Allow Services to be Populated by Monitor $end$ Currently the servers have to be separately defined for both the service and the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
servers=Server1,Server2,Server3
{code}
Since the set of servers is typically the same, this is both tedious and error-prone. By allowing a service to refer to a monitor, the servers of the service do not explicitly be listed but are defined by the monitor.
{code}
[Monitor1]
type=monitor
...
servers=Server1,Server2,Server3

[Service1]
type=service
...
monitor=Monitor1
{code}
This also makes it possible to allow the monitor to dynamically define the servers at runtime. $acceptance criteria:$",0,0,0,0,0,0,0,0.0166667,303,33,0.108911,13,0.0429043,8,0.0264026,6,0.019802,6,0.019802
1259,MXS-2316,Task,MXS,2019-02-05 10:34:27,,0,Prepare for M19,,,Prepare for M19 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-02-05 10:34:27,Prepare for M19,,,0,0,0,0,0.0,Prepare for M19 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,304,33,0.108553,13,0.0427632,8,0.0263158,6,0.0197368,6,0.0197368
1260,MXS-2317,Task,MXS,2019-02-05 10:34:52,,0,Prepare for M19,,,Prepare for M19 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-02-05 10:34:52,Prepare for M19,,,0,0,0,0,0.0,Prepare for M19 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,305,33,0.108197,13,0.0426229,8,0.0262295,6,0.0196721,6,0.0196721
1261,MXS-2319,Task,MXS,2019-02-07 09:44:52,MXS-2278,0,Smart Query standalone test applications,"Wide test coverage is crucial for smart query development. Building a couple of standalone test applications, rather than prototyping inside MaxScale. 
- Test app to exercise smart query heuristics and benchmarking hand written smart routing rules.
- Sql generator that generates a mix of queries that fall into both column, row and in-between categories.
Logging hours to this Jira for building the apps. The heuristics, routing, rules etc. that correspond to prototyping are logged to MXS-2281, Smart Query build prototype.",,"Smart Query standalone test applications $end$ Wide test coverage is crucial for smart query development. Building a couple of standalone test applications, rather than prototyping inside MaxScale. 
- Test app to exercise smart query heuristics and benchmarking hand written smart routing rules.
- Sql generator that generates a mix of queries that fall into both column, row and in-between categories.
Logging hours to this Jira for building the apps. The heuristics, routing, rules etc. that correspond to prototyping are logged to MXS-2281, Smart Query build prototype. $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-03-04 11:25:48,Smart Query standalone test applications,"Wide test coverage is crucial for smart query development. Building a couple of standalone test applications, rather than prototyping inside MaxScale. 
- Test app to exercise smart query heuristics and benchmarking hand written smart routing rules.
- Sql generator that generates a mix of queries that fall into both column, row and in-between categories.
Logging hours to this Jira for building the apps. The heuristics, routing, rules etc. that correspond to prototyping are logged to MXS-2281, Smart Query build prototype.",,0,0,0,0,0.0,"Smart Query standalone test applications $end$ Wide test coverage is crucial for smart query development. Building a couple of standalone test applications, rather than prototyping inside MaxScale. 
- Test app to exercise smart query heuristics and benchmarking hand written smart routing rules.
- Sql generator that generates a mix of queries that fall into both column, row and in-between categories.
Logging hours to this Jira for building the apps. The heuristics, routing, rules etc. that correspond to prototyping are logged to MXS-2281, Smart Query build prototype. $acceptance criteria:$",0,0,0,0,0,0,1,601.667,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1262,MXS-2327,New Feature,MXS,2019-02-13 08:51:21,,0,Introduce duration suffixes,"Currently durations in the MaxScale configuration file are represented both in seconds and milliseconds, but it is not possible to know the unit from the value alone, but you must consult the documentation.

Durations should be expressed together with a suffix that clearly tells the unit. For instance:
{code}
param=1h
param=60m
param=3600s
param=3600000ms
{code}",,"Introduce duration suffixes $end$ Currently durations in the MaxScale configuration file are represented both in seconds and milliseconds, but it is not possible to know the unit from the value alone, but you must consult the documentation.

Durations should be expressed together with a suffix that clearly tells the unit. For instance:
{code}
param=1h
param=60m
param=3600s
param=3600000ms
{code} $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,7,0,0,3,,0,850,0,0,0,2019-02-13 09:53:37,Introduce duration suffixes,"Currently durations in the MaxScale configuration file are represented both in seconds and milliseconds, but it is not possible to know the unit from the value alone, but you must consult the documentation.

Durations should be expressed together with a suffix that clearly tells the unit. For instance:
{code}
param=1h
param=60m
param=3600s
param=3600000ms
{code}",,0,0,0,0,0.0,"Introduce duration suffixes $end$ Currently durations in the MaxScale configuration file are represented both in seconds and milliseconds, but it is not possible to know the unit from the value alone, but you must consult the documentation.

Durations should be expressed together with a suffix that clearly tells the unit. For instance:
{code}
param=1h
param=60m
param=3600s
param=3600000ms
{code} $acceptance criteria:$",0,0,0,0,0,0,1,1.03333,306,33,0.107843,13,0.0424837,8,0.0261438,6,0.0196078,6,0.0196078
1263,MXS-2329,Sub-Task,MXS,2019-02-13 08:59:44,,0,Use duration type for all duration parameter values,"In all cases where duration configuration parameters are used, the type of the corresponding parameter declaration should be {{MXS_MODULE_PARAM_DURATION}}.

$ grep -r 'second' *|cut -f 1 -d ':'|sort|uniq
Changelog.md
Filters/Cache.md
Filters/CCRFilter.md
Filters/Database-Firewall-Filter.md
Filters/Luafilter.md
Filters/Throttle.md
Filters/Top-N-Filter.md
Filters/Transaction-Performance-Monitoring-Filter.md
Getting-Started/Configuration-Guide.md
Monitors/Aurora-Monitor.md
Monitors/MariaDB-Monitor.md
Monitors/Monitor-Common.md
Reference/MaxAdmin.md
Reference/MaxCtrl.md
Reference/Module-Commands.md
Release-Notes/MaxScale-2.3.0-Release-Notes.md
Release-Notes/MaxScale-2.3.1-Release-Notes.md
Release-Notes/MaxScale-2.3.2-Release-Notes.md
REST-API/Resources-MaxScale.md
Routers/Avrorouter.md
Routers/Binlogrouter.md
Routers/ReadWriteSplit.md
Routers/SchemaRouter.md
Tutorials/Administration-Tutorial.md
Tutorials/Avrorouter-Tutorial.md
Tutorials/Configuring-Galera-Monitor.md
Tutorials/Configuring-MariaDB-Monitor.md
Tutorials/Encrypting-Passwords.md
Tutorials/Filter-Tutorial.md
Tutorials/MariaDB-Monitor-Failover.md
Tutorials/MaxScale-Failover-with-Keepalived-and-MaxCtrl.md
Tutorials/MySQL-Cluster-Setup.md
Tutorials/RabbitMQ-And-Tee-Archiving.md
Tutorials/Replication-Proxy-Binlog-Router-Tutorial.md
",,"Use duration type for all duration parameter values $end$ In all cases where duration configuration parameters are used, the type of the corresponding parameter declaration should be {{MXS_MODULE_PARAM_DURATION}}.

$ grep -r 'second' *|cut -f 1 -d ':'|sort|uniq
Changelog.md
Filters/Cache.md
Filters/CCRFilter.md
Filters/Database-Firewall-Filter.md
Filters/Luafilter.md
Filters/Throttle.md
Filters/Top-N-Filter.md
Filters/Transaction-Performance-Monitoring-Filter.md
Getting-Started/Configuration-Guide.md
Monitors/Aurora-Monitor.md
Monitors/MariaDB-Monitor.md
Monitors/Monitor-Common.md
Reference/MaxAdmin.md
Reference/MaxCtrl.md
Reference/Module-Commands.md
Release-Notes/MaxScale-2.3.0-Release-Notes.md
Release-Notes/MaxScale-2.3.1-Release-Notes.md
Release-Notes/MaxScale-2.3.2-Release-Notes.md
REST-API/Resources-MaxScale.md
Routers/Avrorouter.md
Routers/Binlogrouter.md
Routers/ReadWriteSplit.md
Routers/SchemaRouter.md
Tutorials/Administration-Tutorial.md
Tutorials/Avrorouter-Tutorial.md
Tutorials/Configuring-Galera-Monitor.md
Tutorials/Configuring-MariaDB-Monitor.md
Tutorials/Encrypting-Passwords.md
Tutorials/Filter-Tutorial.md
Tutorials/MariaDB-Monitor-Failover.md
Tutorials/MaxScale-Failover-with-Keepalived-and-MaxCtrl.md
Tutorials/MySQL-Cluster-Setup.md
Tutorials/RabbitMQ-And-Tee-Archiving.md
Tutorials/Replication-Proxy-Binlog-Router-Tutorial.md
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,7,0,1,0,,0,850,0,0,0,2019-02-13 08:59:44,Use duration type for all duration parameter values,"In all cases where duration configuration parameters are used, the type of the corresponding parameter declaration should be {{MXS_MODULE_PARAM_DURATION}}.
",,0,1,0,43,1.43333,"Use duration type for all duration parameter values $end$ In all cases where duration configuration parameters are used, the type of the corresponding parameter declaration should be {{MXS_MODULE_PARAM_DURATION}}.
 $acceptance criteria:$",1,1,1,1,1,1,1,0.0,307,33,0.107492,13,0.0423453,8,0.0260586,6,0.019544,6,0.019544
1264,MXS-2332,Task,MXS,2019-02-14 18:10:01,,0,Add Drained state in addition to Being Drained,"A server can explicitly or implicitly be set in the {{Being Drained}} state, which means that no new connections will be created to the server. That state is also shown when listing the servers using _maxctrl_ or _maxadmin_.

When the number of connections has dropped to 0, the state shown should be {{Drained}}.",,"Add Drained state in addition to Being Drained $end$ A server can explicitly or implicitly be set in the {{Being Drained}} state, which means that no new connections will be created to the server. That state is also shown when listing the servers using _maxctrl_ or _maxadmin_.

When the number of connections has dropped to 0, the state shown should be {{Drained}}. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-02-15 08:46:30,Add Drained state in addition to Being Drained,"A server can explicitly or implicitly be set in the {{Being Drained}} state, which means that no new connections will be created to the server. That state is also shown when listing the servers using _maxctrl_ or _maxadmin_.

When the number of connections has dropped to 0, the state shown should be {{Drained}}.",,0,0,0,0,0.0,"Add Drained state in addition to Being Drained $end$ A server can explicitly or implicitly be set in the {{Being Drained}} state, which means that no new connections will be created to the server. That state is also shown when listing the servers using _maxctrl_ or _maxadmin_.

When the number of connections has dropped to 0, the state shown should be {{Drained}}. $acceptance criteria:$",0,0,0,0,0,0,0,14.6,308,34,0.11039,14,0.0454545,9,0.0292208,7,0.0227273,7,0.0227273
1265,MXS-2333,Task,MXS,2019-02-15 06:36:55,,0,Create tutorial for the Clustrix Monitor,,,Create tutorial for the Clustrix Monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-02-15 06:36:55,Create tutorial for the Clustrix Monitor,,,0,0,0,0,0.0,Create tutorial for the Clustrix Monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,309,34,0.110032,14,0.0453074,9,0.0291262,7,0.0226537,7,0.0226537
1266,MXS-2334,Task,MXS,2019-02-15 07:41:29,,0,Create symlink from rhel to centos,Create symlink from rhel to centos so that people who have their repo setup for rhel will get the centos binaries.,,Create symlink from rhel to centos $end$ Create symlink from rhel to centos so that people who have their repo setup for rhel will get the centos binaries. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-02-15 07:41:29,Create symlink from rhel to centos,Create symlink from rhel to centos so that people who have their repo setup for rhel will get the centos binaries.,,0,0,0,0,0.0,Create symlink from rhel to centos $end$ Create symlink from rhel to centos so that people who have their repo setup for rhel will get the centos binaries. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,310,34,0.109677,14,0.0451613,9,0.0290323,7,0.0225806,7,0.0225806
1267,MXS-2338,Task,MXS,2019-02-19 11:03:46,,0,run long test and collect valgrind logs,,,run long test and collect valgrind logs $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,2,0,2,0,0,0,,0,850,2,0,0,2019-02-19 11:23:40,run long test and collect valgrind logs,,,0,0,0,0,0.0,run long test and collect valgrind logs $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.316667,74,1,0.0135135,0,0.0,0,0.0,0,0.0,0,0.0
1268,MXS-2339,Task,MXS,2019-02-19 11:20:06,,0,Clustrix router,,,Clustrix router $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,4,0,0,0,,0,850,0,0,0,2019-02-19 11:20:06,Clustrix router,,,0,0,0,0,0.0,Clustrix router $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,311,34,0.109325,14,0.0450161,9,0.0289389,7,0.022508,7,0.022508
1269,MXS-2344,New Feature,MXS,2019-02-20 22:36:32,,0,Support MASTER_SSL in mariadbmon for encrypting replication traffic,"Need MaxScale to support encryption of replication traffic between database nodes of the local cluster being monitored as well as between the local master and the master of a remote data center.

We have been able to provide most of the SSL configuration (certificates, keys, etc) by configuring a /etc/my.cnf.d/client.cnf file on each local node.  This way I believe MaxScale needs only support the MASTER_SSL=1 option on the CHANGE MASTER TO command during failover.

Nokia has a mandatory security requirement that states that all replication traffic must be encrypted.",,"Support MASTER_SSL in mariadbmon for encrypting replication traffic $end$ Need MaxScale to support encryption of replication traffic between database nodes of the local cluster being monitored as well as between the local master and the master of a remote data center.

We have been able to provide most of the SSL configuration (certificates, keys, etc) by configuring a /etc/my.cnf.d/client.cnf file on each local node.  This way I believe MaxScale needs only support the MASTER_SSL=1 option on the CHANGE MASTER TO command during failover.

Nokia has a mandatory security requirement that states that all replication traffic must be encrypted. $acceptance criteria:$",,Richard Lane,Richard Lane,Major,5,,0,1,0,2,0,0,0,,0,850,1,0,0,2019-04-01 08:10:57,Support MASTER_SSL in mariadbmon for encrypting replication traffic,"Need MaxScale to support encryption of replication traffic between database nodes of the local cluster being monitored as well as between the local master and the master of a remote data center.

We have been able to provide most of the SSL configuration (certificates, keys, etc) by configuring a /etc/my.cnf.d/client.cnf file on each local node.  This way I believe MaxScale needs only support the MASTER_SSL=1 option on the CHANGE MASTER TO command during failover.

Nokia has a mandatory security requirement that states that all replication traffic must be encrypted.",,0,0,0,0,0.0,"Support MASTER_SSL in mariadbmon for encrypting replication traffic $end$ Need MaxScale to support encryption of replication traffic between database nodes of the local cluster being monitored as well as between the local master and the master of a remote data center.

We have been able to provide most of the SSL configuration (certificates, keys, etc) by configuring a /etc/my.cnf.d/client.cnf file on each local node.  This way I believe MaxScale needs only support the MASTER_SSL=1 option on the CHANGE MASTER TO command during failover.

Nokia has a mandatory security requirement that states that all replication traffic must be encrypted. $acceptance criteria:$",0,0,0,0,0,0,1,945.567,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1270,MXS-2346,Task,MXS,2019-02-22 10:54:14,,0,Refactor and Simplify Config/Settings Processing,,,Refactor and Simplify Config/Settings Processing $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-03-05 11:16:17,Refactor and Simplify Config/Settings Processing,,,0,0,0,0,0.0,Refactor and Simplify Config/Settings Processing $end$ $acceptance criteria:$,0,0,0,0,0,0,1,264.367,312,34,0.108974,14,0.0448718,9,0.0288462,7,0.0224359,7,0.0224359
1271,MXS-2348,New Feature,MXS,2019-02-25 02:21:44,MXS-2529,0,Resultset checksum and query duration module,"If a connection is mirrored to another server, it can be used to verify the integrity of and the behavior of upgraded systems. This helps figure out whether an upgraded system provides the same result as a reference system. By calculating a checksum for the resultset of the main connection and the duplicate connection, we know whether the results from the two servers are identical. Furthermore, by measuring the time it took for the query to complete changes in performance can be observed.

In 2.5 this can be easily implemented as a new router that behaves like the tee filter but instead of defining the service where the query is duplicated, the server where the result is returned is defined. Implementing it as a router also makes it much easier to calculate checksums and measure query durations.",,"Resultset checksum and query duration module $end$ If a connection is mirrored to another server, it can be used to verify the integrity of and the behavior of upgraded systems. This helps figure out whether an upgraded system provides the same result as a reference system. By calculating a checksum for the resultset of the main connection and the duplicate connection, we know whether the results from the two servers are identical. Furthermore, by measuring the time it took for the query to complete changes in performance can be observed.

In 2.5 this can be easily implemented as a new router that behaves like the tee filter but instead of defining the service where the query is duplicated, the server where the result is returned is defined. Implementing it as a router also makes it much easier to calculate checksums and measure query durations. $acceptance criteria:$",,markus makela,markus makela,Major,21,,0,0,0,1,0,3,0,,0,850,0,0,0,2019-10-28 11:29:54,Resultset checksums and query durations for tee filter,"If a connection is mirrored to another server, it can be used to verify the integrity of and the behavior of upgraded systems. This helps figure out whether an upgraded system provides the same result as a reference system. By calculating a checksum for the resultset of the main connection and the duplicate connection, we know whether the results from the two servers are identical. Furthermore, by measuring the time it took for the query to complete changes in performance can be observed.",,1,2,0,62,0.62766,"Resultset checksums and query durations for tee filter $end$ If a connection is mirrored to another server, it can be used to verify the integrity of and the behavior of upgraded systems. This helps figure out whether an upgraded system provides the same result as a reference system. By calculating a checksum for the resultset of the main connection and the duplicate connection, we know whether the results from the two servers are identical. Furthermore, by measuring the time it took for the query to complete changes in performance can be observed. $acceptance criteria:$",3,1,1,1,1,1,1,5889.13,59,9,0.152542,8,0.135593,6,0.101695,6,0.101695,6,0.101695
1272,MXS-2351,New Feature,MXS,2019-02-25 02:37:57,MXS-2532,0,Cache prepared statement in readwritesplit,"Some connectors do a prepare-execute-close cycle with the same prepared statement inside the connection. By caching the prepare and ignoring the close the performance is improved and the load on the backend server is reduced.

This can be a niche use-case and the benefits of this feature are not universal.",,"Cache prepared statement in readwritesplit $end$ Some connectors do a prepare-execute-close cycle with the same prepared statement inside the connection. By caching the prepare and ignoring the close the performance is improved and the load on the backend server is reduced.

This can be a niche use-case and the benefits of this feature are not universal. $acceptance criteria:$",,markus makela,markus makela,Major,21,,0,0,0,1,0,2,0,,0,850,0,0,0,2020-11-09 11:04:49,Cache prepared statement in readwritesplit,Some connectors do a prepare-execute-close cycle with the same prepared statement inside the connection. By caching the prepare and ignoring the close the performance is improved and the load on the backend server is reduced.,,0,2,0,15,0.348837,Cache prepared statement in readwritesplit $end$ Some connectors do a prepare-execute-close cycle with the same prepared statement inside the connection. By caching the prepare and ignoring the close the performance is improved and the load on the backend server is reduced. $acceptance criteria:$,2,1,1,1,1,0,1,14960.4,60,10,0.166667,9,0.15,7,0.116667,7,0.116667,7,0.116667
1273,MXS-2353,New Feature,MXS,2019-02-26 20:54:08,,0,Per service log_info,With multiple services having log_info work per service helps figure out why it isn't working. Having 10 services with only one of them with problems the info log level is too verbose.,,Per service log_info $end$ With multiple services having log_info work per service helps figure out why it isn't working. Having 10 services with only one of them with problems the info log level is too verbose. $acceptance criteria:$,,markus makela,markus makela,Major,15,,0,2,0,1,0,0,0,,0,850,1,0,0,2021-11-22 09:13:04,Per service log_info,With multiple services having log_info work per service helps figure out why it isn't working. Having 10 services with only one of them with problems the info log level is too verbose.,,0,0,0,0,0.0,Per service log_info $end$ With multiple services having log_info work per service helps figure out why it isn't working. Having 10 services with only one of them with problems the info log level is too verbose. $acceptance criteria:$,0,0,0,0,0,0,0,23988.3,61,11,0.180328,10,0.163934,8,0.131148,8,0.131148,7,0.114754
1274,MXS-2360,Task,MXS,2019-03-02 05:43:49,,0,Add CentOS 8 to MaxScale builds,Add CentOS 8 (currently still in beta) as a build target. There appear to be no packages available at the time of writing so this probably has to be postponed.,,Add CentOS 8 to MaxScale builds $end$ Add CentOS 8 (currently still in beta) as a build target. There appear to be no packages available at the time of writing so this probably has to be postponed. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-03-18 11:49:57,Add CentOS 8 to MaxScale builds,Add CentOS 8 (currently still in beta) as a build target. There appear to be no packages available at the time of writing so this probably has to be postponed.,,0,0,0,0,0.0,Add CentOS 8 to MaxScale builds $end$ Add CentOS 8 (currently still in beta) as a build target. There appear to be no packages available at the time of writing so this probably has to be postponed. $acceptance criteria:$,0,0,0,0,0,0,1,390.1,62,11,0.177419,10,0.16129,8,0.129032,8,0.129032,7,0.112903
1275,MXS-2361,Task,MXS,2019-03-04 08:48:10,,0,Introduce new way for specifying the parameters of a module,"Overlaps with MXS-2346
",,"Introduce new way for specifying the parameters of a module $end$ Overlaps with MXS-2346
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-03-04 08:48:10,Introduce new way for specifying the parameters of a module,"Overlaps with MXS-2346
",,0,0,0,0,0.0,"Introduce new way for specifying the parameters of a module $end$ Overlaps with MXS-2346
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,313,34,0.108626,14,0.0447284,9,0.028754,7,0.0223642,7,0.0223642
1276,MXS-2362,Task,MXS,2019-03-04 11:19:41,,0,Augment schemarouter documentation,,,Augment schemarouter documentation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-03-04 11:19:41,Augment schemarouter documentation,,,0,0,0,0,0.0,Augment schemarouter documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,314,34,0.10828,14,0.044586,9,0.0286624,7,0.022293,7,0.022293
1277,MXS-2363,Task,MXS,2019-03-04 11:29:35,,0,Make it possible to dump query classification cache,,,Make it possible to dump query classification cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-03-04 11:29:35,Make it possible to dump query classification cache,,,0,0,0,0,0.0,Make it possible to dump query classification cache $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,315,34,0.107937,14,0.0444444,9,0.0285714,7,0.0222222,7,0.0222222
1278,MXS-2376,Task,MXS,2019-03-12 12:22:00,,0,Add callgrind profiling to performance test runs,Adding the {{--tool=callgrind}} option to performance test runs would allow easy profiling of the performance test results.,,Add callgrind profiling to performance test runs $end$ Adding the {{--tool=callgrind}} option to performance test runs would allow easy profiling of the performance test results. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-15 14:48:12,Add callgrind profiling to performance test runs,Adding the {{--tool=callgrind}} option to performance test runs would allow easy profiling of the performance test results.,,0,0,0,0,0.0,Add callgrind profiling to performance test runs $end$ Adding the {{--tool=callgrind}} option to performance test runs would allow easy profiling of the performance test results. $acceptance criteria:$,0,0,0,0,0,0,0,818.433,63,11,0.174603,10,0.15873,8,0.126984,8,0.126984,7,0.111111
1279,MXS-2383,New Feature,MXS,2019-03-13 19:02:00,MXS-2528,0,Support PAM authentications involving more than simple password exchanges,"The documentation says the following:

{quote}
The current version of the MaxScale PAM authentication module only supports a simple password exchange. On the client side, the authentication begins with MaxScale sending an AuthSwitchRequest packet. In addition to the command, the packet contains the client plugin name dialog, a message type byte 4 and the message Password:. In the next packet, the client should send the password, which MaxScale will forward to the PAM API running on the local machine. If the password is correct, an OK packet is sent to the client. No additional PAM-related messaging is allowed, as this would indicate a more complicated authentication scheme.
{quote}

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/#implementation-details-and-limitations

Some users would like MaxScale to support PAM authentications that involve more than a single simple password exchange. For example, some PAM configurations require two inputs to login--a regular user-set password, and a 2FA token from a service like Google Authenticator or RSA SecurID.",,"Support PAM authentications involving more than simple password exchanges $end$ The documentation says the following:

{quote}
The current version of the MaxScale PAM authentication module only supports a simple password exchange. On the client side, the authentication begins with MaxScale sending an AuthSwitchRequest packet. In addition to the command, the packet contains the client plugin name dialog, a message type byte 4 and the message Password:. In the next packet, the client should send the password, which MaxScale will forward to the PAM API running on the local machine. If the password is correct, an OK packet is sent to the client. No additional PAM-related messaging is allowed, as this would indicate a more complicated authentication scheme.
{quote}

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/#implementation-details-and-limitations

Some users would like MaxScale to support PAM authentications that involve more than a single simple password exchange. For example, some PAM configurations require two inputs to login--a regular user-set password, and a 2FA token from a service like Google Authenticator or RSA SecurID. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,27,,0,2,1,2,0,0,0,,0,850,0,0,0,2020-07-10 07:45:41,Support PAM authentications involving more than simple password exchanges,"The documentation says the following:

{quote}
The current version of the MaxScale PAM authentication module only supports a simple password exchange. On the client side, the authentication begins with MaxScale sending an AuthSwitchRequest packet. In addition to the command, the packet contains the client plugin name dialog, a message type byte 4 and the message Password:. In the next packet, the client should send the password, which MaxScale will forward to the PAM API running on the local machine. If the password is correct, an OK packet is sent to the client. No additional PAM-related messaging is allowed, as this would indicate a more complicated authentication scheme.
{quote}

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/#implementation-details-and-limitations

Some users would like MaxScale to support PAM authentications that involve more than a single simple password exchange. For example, some PAM configurations require two inputs to login--a regular user-set password, and a 2FA token from a service like Google Authenticator or RSA SecurID.",,0,0,0,0,0.0,"Support PAM authentications involving more than simple password exchanges $end$ The documentation says the following:

{quote}
The current version of the MaxScale PAM authentication module only supports a simple password exchange. On the client side, the authentication begins with MaxScale sending an AuthSwitchRequest packet. In addition to the command, the packet contains the client plugin name dialog, a message type byte 4 and the message Password:. In the next packet, the client should send the password, which MaxScale will forward to the PAM API running on the local machine. If the password is correct, an OK packet is sent to the client. No additional PAM-related messaging is allowed, as this would indicate a more complicated authentication scheme.
{quote}

https://mariadb.com/kb/en/mariadb-maxscale-23-pam-authenticator/#implementation-details-and-limitations

Some users would like MaxScale to support PAM authentications that involve more than a single simple password exchange. For example, some PAM configurations require two inputs to login--a regular user-set password, and a 2FA token from a service like Google Authenticator or RSA SecurID. $acceptance criteria:$",0,0,0,0,0,0,1,11628.7,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1280,MXS-2387,Task,MXS,2019-03-18 11:20:15,,0,schedule system tests with valgrind in BuildBot,,,schedule system tests with valgrind in BuildBot $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-01 10:40:51,schedule system tests with valgrind in BuildBot,,,0,0,0,0,0.0,schedule system tests with valgrind in BuildBot $end$ $acceptance criteria:$,0,0,0,0,0,0,0,335.333,75,1,0.0133333,0,0.0,0,0.0,0,0.0,0,0.0
1281,MXS-2388,Task,MXS,2019-03-18 11:21:09,,0,schedule system tests for different distros,,,schedule system tests for different distros $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-01 10:41:01,schedule system tests for different distros,,,0,0,0,0,0.0,schedule system tests for different distros $end$ $acceptance criteria:$,0,0,0,0,0,0,0,335.317,76,1,0.0131579,0,0.0,0,0.0,0,0.0,0,0.0
1282,MXS-2397,Task,MXS,2019-03-20 16:26:39,,0,Report context of variables.,,,Report context of variables. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-03-20 16:26:42,Report context of variables.,,,0,0,0,0,0.0,Report context of variables. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,316,34,0.107595,14,0.0443038,9,0.028481,7,0.0221519,7,0.0221519
1283,MXS-240,Sub-Task,MXS,2015-06-30 00:57:28,,0,Test setup for MaxScale with MariaDB 10.1,"As 10.1.7 has been RC, we need to start testing MaxScale with 10.1 and find any issues with it. ",,"Test setup for MaxScale with MariaDB 10.1 $end$ As 10.1.7 has been RC, we need to start testing MaxScale with 10.1 and find any issues with it.  $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,6,,0,0,0,2,0,1,0,,0,850,0,0,0,2015-06-30 00:57:28,MaxScale with MariaDB 10.1,"As 10.1.7 has been RC, we need to start testing MaxScale with 10.1 and find any issues with it. ",,1,0,0,3,0.115385,"MaxScale with MariaDB 10.1 $end$ As 10.1.7 has been RC, we need to start testing MaxScale with 10.1 and find any issues with it.  $acceptance criteria:$",1,1,0,0,0,0,1,0.0,6,3,0.5,1,0.166667,0,0.0,0,0.0,0,0.0
1284,MXS-2401,Task,MXS,2019-03-21 12:02:12,MXS-2278,0,Design Smart Query Router,"Design the router Module, persistence of routing data, configuration items, connection/interaction with query classifier.",,"Design Smart Query Router $end$ Design the router Module, persistence of routing data, configuration items, connection/interaction with query classifier. $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,7,,0,0,0,4,0,0,0,,0,850,0,0,0,2019-03-25 11:31:53,Design Smart Query Router,"Design the router Module, persistence of routing data, configuration items, connection/interaction with query classifier.",,0,0,0,0,0.0,"Design Smart Query Router $end$ Design the router Module, persistence of routing data, configuration items, connection/interaction with query classifier. $acceptance criteria:$",0,0,0,0,0,0,1,95.4833,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1285,MXS-2424,Task,MXS,2019-04-05 06:29:05,,0,Store information of dynamically detected Clustrix nodes,,,Store information of dynamically detected Clustrix nodes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-15 11:31:25,Store information of dynamically detected Clustrix nodes,,,0,0,0,0,0.0,Store information of dynamically detected Clustrix nodes $end$ $acceptance criteria:$,0,0,0,0,0,0,0,245.033,317,34,0.107256,14,0.044164,9,0.0283912,7,0.022082,7,0.022082
1286,MXS-2428,Task,MXS,2019-04-08 05:58:39,,0,It should be possible to use the Clustrix monitor with a fixed set of servers.,"The Clustrix monitor autonomously detects the nodes in a cluster and populates services referring to it with the corresponding server objects. Sometimes you want more direct control over what nodes (and ports) should be used. The following should be possible:
{code}
[Node-1]
type=server
address=...
port=...
protocol=mariadbbackend

[Node-2]
...

[Node-3]
...

[TheMonitor]
type=monitor
module=clustrixmon
servers=Node-1, Node-2, Node-3
...
dynamic_node_detection=false

[TheService]
type=service
router=readconnroute
...
cluster=TheMonitor
{code}

The {{dynamic_node_detection=false}} in the monitor section would have the effect that the monitor does *not* dynamically figure out the structure of the Cluster, but simply uses the servers it has been configured with as such.

That is, the servers the Clustrix monitor has been configured with are not used as _bootstrap_ server from which the Cluster configuration is queried, but as the very nodes the monitor should make available to services.",,"It should be possible to use the Clustrix monitor with a fixed set of servers. $end$ The Clustrix monitor autonomously detects the nodes in a cluster and populates services referring to it with the corresponding server objects. Sometimes you want more direct control over what nodes (and ports) should be used. The following should be possible:
{code}
[Node-1]
type=server
address=...
port=...
protocol=mariadbbackend

[Node-2]
...

[Node-3]
...

[TheMonitor]
type=monitor
module=clustrixmon
servers=Node-1, Node-2, Node-3
...
dynamic_node_detection=false

[TheService]
type=service
router=readconnroute
...
cluster=TheMonitor
{code}

The {{dynamic_node_detection=false}} in the monitor section would have the effect that the monitor does *not* dynamically figure out the structure of the Cluster, but simply uses the servers it has been configured with as such.

That is, the servers the Clustrix monitor has been configured with are not used as _bootstrap_ server from which the Cluster configuration is queried, but as the very nodes the monitor should make available to services. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-15 08:27:48,It should be possible to use the Clustrix monitor with a fixed set of servers.,"The Clustrix monitor autonomously detects the nodes in a cluster and populates services referring to it with the corresponding server objects. Sometimes you want more direct control over what nodes (and ports) should be used. The following should be possible:
{code}
[Node-1]
type=server
address=...
port=...
protocol=mariadbbackend

[Node-2]
...

[Node-3]
...

[TheMonitor]
type=monitor
module=clustrixmon
servers=Node-1, Node-2, Node-3
...
dynamic_node_detection=false

[TheService]
type=service
router=readconnroute
...
cluster=TheMonitor
{code}

The {{dynamic_node_detection=false}} in the monitor section would have the effect that the monitor does *not* dynamically figure out the structure of the Cluster, but simply uses the servers it has been configured with as such.

That is, the servers the Clustrix monitor has been configured with are not used as _bootstrap_ server from which the Cluster configuration is queried, but as the very nodes the monitor should make available to services.",,0,0,0,0,0.0,"It should be possible to use the Clustrix monitor with a fixed set of servers. $end$ The Clustrix monitor autonomously detects the nodes in a cluster and populates services referring to it with the corresponding server objects. Sometimes you want more direct control over what nodes (and ports) should be used. The following should be possible:
{code}
[Node-1]
type=server
address=...
port=...
protocol=mariadbbackend

[Node-2]
...

[Node-3]
...

[TheMonitor]
type=monitor
module=clustrixmon
servers=Node-1, Node-2, Node-3
...
dynamic_node_detection=false

[TheService]
type=service
router=readconnroute
...
cluster=TheMonitor
{code}

The {{dynamic_node_detection=false}} in the monitor section would have the effect that the monitor does *not* dynamically figure out the structure of the Cluster, but simply uses the servers it has been configured with as such.

That is, the servers the Clustrix monitor has been configured with are not used as _bootstrap_ server from which the Cluster configuration is queried, but as the very nodes the monitor should make available to services. $acceptance criteria:$",0,0,0,0,0,0,0,170.483,318,34,0.106918,14,0.0440252,9,0.0283019,7,0.0220126,7,0.0220126
1287,MXS-2434,Task,MXS,2019-04-15 09:26:48,,0,create install_test_deps.sh,maxscale-system-tests require build and run-time dependencies that now are managed manually. Automatic installation script should be created ,,create install_test_deps.sh $end$ maxscale-system-tests require build and run-time dependencies that now are managed manually. Automatic installation script should be created  $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-15 14:48:03,create install_test_deps.sh,maxscale-system-tests require build and run-time dependencies that now are managed manually. Automatic installation script should be created ,,0,0,0,0,0.0,create install_test_deps.sh $end$ maxscale-system-tests require build and run-time dependencies that now are managed manually. Automatic installation script should be created  $acceptance criteria:$,0,0,0,0,0,0,0,5.35,77,1,0.012987,0,0.0,0,0.0,0,0.0,0,0.0
1288,MXS-2435,Task,MXS,2019-04-15 11:58:20,,0,Handle Clustrix group change in RWS,,,Handle Clustrix group change in RWS $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-15 11:58:20,Handle Clustrix group change in RWS,,,0,0,0,0,0.0,Handle Clustrix group change in RWS $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,319,34,0.106583,14,0.0438871,9,0.0282132,7,0.0219436,7,0.0219436
1289,MXS-2437,Task,MXS,2019-04-15 13:51:51,MXS-2278,0,Implement new Router that can handle router-to-router connections.,"There is no ""basic"" router in MaxScale that can handle router to router properly. This needs to be built first, before smart query can be integrated.",,"Implement new Router that can handle router-to-router connections. $end$ There is no ""basic"" router in MaxScale that can handle router to router properly. This needs to be built first, before smart query can be integrated. $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,7,,0,0,0,3,0,1,0,,0,850,0,0,0,2019-05-13 09:39:46,Implement new Router to that can handle router-to-router connections.,"There is no ""basic"" router in MaxScale that can handle router to router properly. This needs to be built first, before smart query can be integrated.",,1,0,0,1,0.0263158,"Implement new Router to that can handle router-to-router connections. $end$ There is no ""basic"" router in MaxScale that can handle router to router properly. This needs to be built first, before smart query can be integrated. $acceptance criteria:$",1,1,0,0,0,0,1,667.783,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1290,MXS-2443,New Feature,MXS,2019-04-20 19:34:32,MXS-2532,0,ORM Connection Pooling Not Working With Causal Reads,"The causal read feature of the ReadWriteSplit router does not work if the application that connects to MaxScale is using connection pooling.  For example:

[Sequelize|http://docs.sequelizejs.com/] is a promise-based Node.js ORM that includes connection pooling.

Typical database connection construction looks like this:

{code:java}
  database: {
    host: 'scdb.selectquotesenior.com',
    name: 'selectcare_sqs',
    username: 'selectcare',
    password: 'password',
    pool: {
      max: 10,
      min: 0,
      idle: 10000
    }
  },
{code}

In this scenario, the customer is using MaxScale 2.3.4 with MariaDB 10.3.12.

*_session_track_system_variables_* is set to include *_last_gtid_*.

Causal reads work as expected when directly connecting to MaxScale with a single client. 

However, when using pooling in the application layer or abstraction layer, one thread in the pool might have done the write while others may be doing the reads. Currently MaxScale cannot track this type of behavior and sees them as two separate connections which is not caught by the causal read feature of MaxScale.",,"ORM Connection Pooling Not Working With Causal Reads $end$ The causal read feature of the ReadWriteSplit router does not work if the application that connects to MaxScale is using connection pooling.  For example:

[Sequelize|http://docs.sequelizejs.com/] is a promise-based Node.js ORM that includes connection pooling.

Typical database connection construction looks like this:

{code:java}
  database: {
    host: 'scdb.selectquotesenior.com',
    name: 'selectcare_sqs',
    username: 'selectcare',
    password: 'password',
    pool: {
      max: 10,
      min: 0,
      idle: 10000
    }
  },
{code}

In this scenario, the customer is using MaxScale 2.3.4 with MariaDB 10.3.12.

*_session_track_system_variables_* is set to include *_last_gtid_*.

Causal reads work as expected when directly connecting to MaxScale with a single client. 

However, when using pooling in the application layer or abstraction layer, one thread in the pool might have done the write while others may be doing the reads. Currently MaxScale cannot track this type of behavior and sees them as two separate connections which is not caught by the causal read feature of MaxScale. $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,26,,0,3,1,1,0,0,0,,0,850,1,0,0,2019-10-28 11:25:32,ORM Connection Pooling Not Working With Causal Reads,"The causal read feature of the ReadWriteSplit router does not work if the application that connects to MaxScale is using connection pooling.  For example:

[Sequelize|http://docs.sequelizejs.com/] is a promise-based Node.js ORM that includes connection pooling.

Typical database connection construction looks like this:

{code:java}
  database: {
    host: 'scdb.selectquotesenior.com',
    name: 'selectcare_sqs',
    username: 'selectcare',
    password: 'password',
    pool: {
      max: 10,
      min: 0,
      idle: 10000
    }
  },
{code}

In this scenario, the customer is using MaxScale 2.3.4 with MariaDB 10.3.12.

*_session_track_system_variables_* is set to include *_last_gtid_*.

Causal reads work as expected when directly connecting to MaxScale with a single client. 

However, when using pooling in the application layer or abstraction layer, one thread in the pool might have done the write while others may be doing the reads. Currently MaxScale cannot track this type of behavior and sees them as two separate connections which is not caught by the causal read feature of MaxScale.",,0,0,0,0,0.0,"ORM Connection Pooling Not Working With Causal Reads $end$ The causal read feature of the ReadWriteSplit router does not work if the application that connects to MaxScale is using connection pooling.  For example:

[Sequelize|http://docs.sequelizejs.com/] is a promise-based Node.js ORM that includes connection pooling.

Typical database connection construction looks like this:

{code:java}
  database: {
    host: 'scdb.selectquotesenior.com',
    name: 'selectcare_sqs',
    username: 'selectcare',
    password: 'password',
    pool: {
      max: 10,
      min: 0,
      idle: 10000
    }
  },
{code}

In this scenario, the customer is using MaxScale 2.3.4 with MariaDB 10.3.12.

*_session_track_system_variables_* is set to include *_last_gtid_*.

Causal reads work as expected when directly connecting to MaxScale with a single client. 

However, when using pooling in the application layer or abstraction layer, one thread in the pool might have done the write while others may be doing the reads. Currently MaxScale cannot track this type of behavior and sees them as two separate connections which is not caught by the causal read feature of MaxScale. $acceptance criteria:$",0,0,0,0,0,0,0,4575.85,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1291,MXS-2455,Task,MXS,2019-04-26 08:04:17,,0,Handle all retriable Clustrix errors.,,,Handle all retriable Clustrix errors. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-26 08:04:17,Handle all retriable Clustrix errors.,,,0,0,0,0,0.0,Handle all retriable Clustrix errors. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,320,34,0.10625,14,0.04375,9,0.028125,7,0.021875,7,0.021875
1292,MXS-2456,Task,MXS,2019-04-26 08:06:08,,0,Add cap for number of attempts to replay a transaction.,"With Clustrix, a transaction can be replayed not only because of a connection breakage but also because of errors returned by Clustrix. So as not to end up in an endless replay loop in case a replayed transaction - for whatever reason - ends up with a retriable error, some form of cap should be introduced. ",,"Add cap for number of attempts to replay a transaction. $end$ With Clustrix, a transaction can be replayed not only because of a connection breakage but also because of errors returned by Clustrix. So as not to end up in an endless replay loop in case a replayed transaction - for whatever reason - ends up with a retriable error, some form of cap should be introduced.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,2,0,1,0,0,0,,0,850,2,0,0,2019-04-26 08:06:08,Add cap for number of attempts to replay a transaction.,"With Clustrix, a transaction can be replayed not only because of a connection breakage but also because of errors returned by Clustrix. So as not to end up in an endless replay loop in case a replayed transaction - for whatever reason - ends up with a retriable error, some form of cap should be introduced. ",,0,0,0,0,0.0,"Add cap for number of attempts to replay a transaction. $end$ With Clustrix, a transaction can be replayed not only because of a connection breakage but also because of errors returned by Clustrix. So as not to end up in an endless replay loop in case a replayed transaction - for whatever reason - ends up with a retriable error, some form of cap should be introduced.  $acceptance criteria:$",0,0,0,0,0,0,0,0.0,321,34,0.105919,14,0.0436137,9,0.0280374,7,0.0218069,7,0.0218069
1293,MXS-2458,Task,MXS,2019-04-29 11:57:33,,0,Integrate Clustrix into the MaxScale system test environment,,,Integrate Clustrix into the MaxScale system test environment $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-29 11:57:33,Integrate Clustrix into the MaxScale system test environment,,,0,0,0,0,0.0,Integrate Clustrix into the MaxScale system test environment $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,322,34,0.10559,14,0.0434783,9,0.0279503,7,0.0217391,7,0.0217391
1294,MXS-2460,Task,MXS,2019-04-29 14:51:40,,0,Order test servers,,,Order test servers $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-04-29 14:51:40,Order test servers,,,0,0,0,0,0.0,Order test servers $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,323,34,0.105263,14,0.0433437,9,0.0278638,7,0.0216718,7,0.0216718
1295,MXS-2475,Task,MXS,2019-05-10 12:06:29,,0,Fix mxs1980_blr_galera_server_ids,,,Fix mxs1980_blr_galera_server_ids $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-13 07:12:34,Fix mxs1980_blr_galera_server_ids,,,0,0,0,0,0.0,Fix mxs1980_blr_galera_server_ids $end$ $acceptance criteria:$,0,0,0,0,0,0,0,67.1,324,34,0.104938,14,0.0432099,9,0.0277778,7,0.0216049,7,0.0216049
1296,MXS-2478,New Feature,MXS,2019-05-10 18:44:43,MXS-2528,0,Support mysql_clear_password for PAMAuth and PAMBackendAuth,"It seems to be a known limitation that MaxScale does not yet support mysql_clear_password for PAMAuth and PAMBackendAuth. The code contains this TODO to add support for it:

{noformat}
/* PAM client helper plugin name, TODO: add support for ""mysql_clear_password"" */
const std::string DIALOG = ""dialog"";
{noformat}

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/pam_auth_common.cc#L19

The following PAMBackendAuth code seems to explicitly check that the backend server asked MaxScale to use the ""dialog"" plugin:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L67

But I don't think this particular code can even be reached, because if the backend server does not ask MaxScale to use the ""dialog"" plugin, then it will fail before that here:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L58

And throw this error message:

{noformat}
2019-05-10 14:41:24   error  : (12901) [PAMBackendAuth] Length of server AuthSwitchRequest packet was '37', expected '23'. Only simple password-based PAM authentication with one call to the conversation function is supported.
{noformat}

Regardless, I think MaxScale should also support the mysql_clear_password plugin for PAMAuth and PAMBackendAuth. If it does not, then it cannot be used with servers that have pam_use_cleartext_plugin set, which is sometimes needed for compatibility.

https://mariadb.com/kb/en/library/authentication-plugin-pam/#pam_use_cleartext_plugin

https://mariadb.com/kb/en/library/authentication-plugin-pam/#mysql_clear_password",,"Support mysql_clear_password for PAMAuth and PAMBackendAuth $end$ It seems to be a known limitation that MaxScale does not yet support mysql_clear_password for PAMAuth and PAMBackendAuth. The code contains this TODO to add support for it:

{noformat}
/* PAM client helper plugin name, TODO: add support for ""mysql_clear_password"" */
const std::string DIALOG = ""dialog"";
{noformat}

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/pam_auth_common.cc#L19

The following PAMBackendAuth code seems to explicitly check that the backend server asked MaxScale to use the ""dialog"" plugin:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L67

But I don't think this particular code can even be reached, because if the backend server does not ask MaxScale to use the ""dialog"" plugin, then it will fail before that here:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L58

And throw this error message:

{noformat}
2019-05-10 14:41:24   error  : (12901) [PAMBackendAuth] Length of server AuthSwitchRequest packet was '37', expected '23'. Only simple password-based PAM authentication with one call to the conversation function is supported.
{noformat}

Regardless, I think MaxScale should also support the mysql_clear_password plugin for PAMAuth and PAMBackendAuth. If it does not, then it cannot be used with servers that have pam_use_cleartext_plugin set, which is sometimes needed for compatibility.

https://mariadb.com/kb/en/library/authentication-plugin-pam/#pam_use_cleartext_plugin

https://mariadb.com/kb/en/library/authentication-plugin-pam/#mysql_clear_password $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,17,,0,0,1,2,0,0,0,,0,850,0,0,0,2020-04-14 10:38:09,Support mysql_clear_password for PAMAuth and PAMBackendAuth,"It seems to be a known limitation that MaxScale does not yet support mysql_clear_password for PAMAuth and PAMBackendAuth. The code contains this TODO to add support for it:

{noformat}
/* PAM client helper plugin name, TODO: add support for ""mysql_clear_password"" */
const std::string DIALOG = ""dialog"";
{noformat}

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/pam_auth_common.cc#L19

The following PAMBackendAuth code seems to explicitly check that the backend server asked MaxScale to use the ""dialog"" plugin:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L67

But I don't think this particular code can even be reached, because if the backend server does not ask MaxScale to use the ""dialog"" plugin, then it will fail before that here:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L58

And throw this error message:

{noformat}
2019-05-10 14:41:24   error  : (12901) [PAMBackendAuth] Length of server AuthSwitchRequest packet was '37', expected '23'. Only simple password-based PAM authentication with one call to the conversation function is supported.
{noformat}

Regardless, I think MaxScale should also support the mysql_clear_password plugin for PAMAuth and PAMBackendAuth. If it does not, then it cannot be used with servers that have pam_use_cleartext_plugin set, which is sometimes needed for compatibility.

https://mariadb.com/kb/en/library/authentication-plugin-pam/#pam_use_cleartext_plugin

https://mariadb.com/kb/en/library/authentication-plugin-pam/#mysql_clear_password",,0,0,0,0,0.0,"Support mysql_clear_password for PAMAuth and PAMBackendAuth $end$ It seems to be a known limitation that MaxScale does not yet support mysql_clear_password for PAMAuth and PAMBackendAuth. The code contains this TODO to add support for it:

{noformat}
/* PAM client helper plugin name, TODO: add support for ""mysql_clear_password"" */
const std::string DIALOG = ""dialog"";
{noformat}

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/pam_auth_common.cc#L19

The following PAMBackendAuth code seems to explicitly check that the backend server asked MaxScale to use the ""dialog"" plugin:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L67

But I don't think this particular code can even be reached, because if the backend server does not ask MaxScale to use the ""dialog"" plugin, then it will fail before that here:

https://github.com/mariadb-corporation/MaxScale/blob/maxscale-2.3.6/server/modules/authenticator/PAM/PAMBackendAuth/pam_backend_session.cc#L58

And throw this error message:

{noformat}
2019-05-10 14:41:24   error  : (12901) [PAMBackendAuth] Length of server AuthSwitchRequest packet was '37', expected '23'. Only simple password-based PAM authentication with one call to the conversation function is supported.
{noformat}

Regardless, I think MaxScale should also support the mysql_clear_password plugin for PAMAuth and PAMBackendAuth. If it does not, then it cannot be used with servers that have pam_use_cleartext_plugin set, which is sometimes needed for compatibility.

https://mariadb.com/kb/en/library/authentication-plugin-pam/#pam_use_cleartext_plugin

https://mariadb.com/kb/en/library/authentication-plugin-pam/#mysql_clear_password $acceptance criteria:$",0,0,0,0,0,0,1,8151.88,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1297,MXS-2480,New Feature,MXS,2019-05-11 02:56:49,,0,Write paths to opened sqlite3 databases in log when log_info is set,"MaxScale caches a lot of information in SQLite databases. These databases can cause issues sometimes. For example, if the schema of the database is not in sync with the current MaxScale version due to a downgrade, then you will see issues. When that happens, MaxScale will *not* fix itself and the locations of some of these databases can be difficult to track down.

If log_info is set, and if MaxScale opens an SQLite database, it would be extremely helpful if it could write the path of that database to the log file.

As an example, I downgraded from MaxScale 2.3 to 2.2 and have been trying to fix this error for what seems like ages:

{noformat}
2019-05-10 22:48:37   notice : [PAMAuth] Loaded 1 users for service Splitter-Service.
2019-05-10 22:48:37   error  : [PAMAuth] Failed to insert user: table pam_users has 6 columns but 5 values were supplied
{noformat}

And the error was still present after deleting almost every single file in /var/lib/maxscale and /var/cache/maxscale. Apparently this database was in /var/log/maxscale, which is a bit counterintuitive.",,"Write paths to opened sqlite3 databases in log when log_info is set $end$ MaxScale caches a lot of information in SQLite databases. These databases can cause issues sometimes. For example, if the schema of the database is not in sync with the current MaxScale version due to a downgrade, then you will see issues. When that happens, MaxScale will *not* fix itself and the locations of some of these databases can be difficult to track down.

If log_info is set, and if MaxScale opens an SQLite database, it would be extremely helpful if it could write the path of that database to the log file.

As an example, I downgraded from MaxScale 2.3 to 2.2 and have been trying to fix this error for what seems like ages:

{noformat}
2019-05-10 22:48:37   notice : [PAMAuth] Loaded 1 users for service Splitter-Service.
2019-05-10 22:48:37   error  : [PAMAuth] Failed to insert user: table pam_users has 6 columns but 5 values were supplied
{noformat}

And the error was still present after deleting almost every single file in /var/lib/maxscale and /var/cache/maxscale. Apparently this database was in /var/log/maxscale, which is a bit counterintuitive. $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,9,,0,1,0,1,0,0,0,,0,850,0,0,0,2019-05-13 09:25:36,Write paths to opened sqlite3 databases in log when log_info is set,"MaxScale caches a lot of information in SQLite databases. These databases can cause issues sometimes. For example, if the schema of the database is not in sync with the current MaxScale version due to a downgrade, then you will see issues. When that happens, MaxScale will *not* fix itself and the locations of some of these databases can be difficult to track down.

If log_info is set, and if MaxScale opens an SQLite database, it would be extremely helpful if it could write the path of that database to the log file.

As an example, I downgraded from MaxScale 2.3 to 2.2 and have been trying to fix this error for what seems like ages:

{noformat}
2019-05-10 22:48:37   notice : [PAMAuth] Loaded 1 users for service Splitter-Service.
2019-05-10 22:48:37   error  : [PAMAuth] Failed to insert user: table pam_users has 6 columns but 5 values were supplied
{noformat}

And the error was still present after deleting almost every single file in /var/lib/maxscale and /var/cache/maxscale. Apparently this database was in /var/log/maxscale, which is a bit counterintuitive.",,0,0,0,0,0.0,"Write paths to opened sqlite3 databases in log when log_info is set $end$ MaxScale caches a lot of information in SQLite databases. These databases can cause issues sometimes. For example, if the schema of the database is not in sync with the current MaxScale version due to a downgrade, then you will see issues. When that happens, MaxScale will *not* fix itself and the locations of some of these databases can be difficult to track down.

If log_info is set, and if MaxScale opens an SQLite database, it would be extremely helpful if it could write the path of that database to the log file.

As an example, I downgraded from MaxScale 2.3 to 2.2 and have been trying to fix this error for what seems like ages:

{noformat}
2019-05-10 22:48:37   notice : [PAMAuth] Loaded 1 users for service Splitter-Service.
2019-05-10 22:48:37   error  : [PAMAuth] Failed to insert user: table pam_users has 6 columns but 5 values were supplied
{noformat}

And the error was still present after deleting almost every single file in /var/lib/maxscale and /var/cache/maxscale. Apparently this database was in /var/log/maxscale, which is a bit counterintuitive. $acceptance criteria:$",0,0,0,0,0,0,0,54.4667,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1298,MXS-2481,Task,MXS,2019-05-13 08:02:11,,0,Create Clustrix specific system tests,,,Create Clustrix specific system tests $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-13 08:02:11,Create Clustrix specific system tests,,,0,0,0,0,0.0,Create Clustrix specific system tests $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,325,34,0.104615,14,0.0430769,9,0.0276923,7,0.0215385,7,0.0215385
1299,MXS-2483,Task,MXS,2019-05-13 09:22:24,,0,Rearrange SSL related code,,,Rearrange SSL related code $end$ $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-13 09:22:24,Rearrange SSL related code,,,0,0,0,0,0.0,Rearrange SSL related code $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,64,11,0.171875,10,0.15625,8,0.125,8,0.125,7,0.109375
1300,MXS-2485,Task,MXS,2019-05-13 09:57:42,,0,PacketTracker. Light class to track the state of a query.,Mostly done as part of MXS-2437,,PacketTracker. Light class to track the state of a query. $end$ Mostly done as part of MXS-2437 $acceptance criteria:$,,Niclas Antti,Niclas Antti,Major,7,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-05-13 10:36:19,PacketTracker. Light class to track the state of a query.,Mostly done as part of MXS-2437,,0,0,0,0,0.0,PacketTracker. Light class to track the state of a query. $end$ Mostly done as part of MXS-2437 $acceptance criteria:$,0,0,0,0,0,0,1,0.633333,8,1,0.125,0,0.0,0,0.0,0,0.0,0,0.0
1301,MXS-2497,New Feature,MXS,2019-05-16 20:05:21,MXS-2532,0,Support all MariaDBClient-compatible authenticators on the same listener,"A lot of users are using the PAMAuth and MySQLAuth authenticators at the same time with the same services. With the current design, a {{listener}} and a {{server}} can only each have one authenticator. Therefore, this kind of configuration requires a lot of duplication:

* Every listener needs to be duplicated for every client-side authenticator.
* Every server needs to be duplicated for every backend authenticator.
* Every dependent service also needs to be duplicated.

It seems like it should be possible to change the design to support all MariaDBClient-compatible authenticators on the same listener at the same time. I think this will be even more important as users start to use even more authentication plugins on a more regular basis, such as ed25519 and gssapi.

MariaDB Server supports all authentication plugins on the same port, so I think MaxScale should also be able to do it. When a user tries to log in to MariaDB Server, it checks the {{plugin}} column of the {{mysql.user}} table to decide which plugin to use to authenticate the user:

https://mariadb.com/kb/en/library/mysqluser-table/

MaxScale could do something similar. For example, it could have an ""authentication dispatcher"" class of some kind. This class could query the {{mysql.user}} table to determine which authentication plugin each user account uses, and write it to an SQLite table. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   plugin char(64),
   PRIMARY KEY (user, host)
);
{noformat}

When a user tries to log in to MaxScale, the ""authentication dispatcher"" can determine which authenticator to use for that user by checking the plugin mapping for that user account.

In MariaDB 10.4, a user account can be configured to use several different authentication plugins in a pre-configured order. This information is stored in the {{mysql.global_priv}} table.

https://mariadb.com/kb/en/library/mysqlglobal_priv-table/

If we wanted to support multiple authentication plugins in MaxScale too, then we could probable extend the mapping table schema to include an additional order column. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   order int,
   plugin char(64),
   PRIMARY KEY (user, host, order)
);
{noformat}",,"Support all MariaDBClient-compatible authenticators on the same listener $end$ A lot of users are using the PAMAuth and MySQLAuth authenticators at the same time with the same services. With the current design, a {{listener}} and a {{server}} can only each have one authenticator. Therefore, this kind of configuration requires a lot of duplication:

* Every listener needs to be duplicated for every client-side authenticator.
* Every server needs to be duplicated for every backend authenticator.
* Every dependent service also needs to be duplicated.

It seems like it should be possible to change the design to support all MariaDBClient-compatible authenticators on the same listener at the same time. I think this will be even more important as users start to use even more authentication plugins on a more regular basis, such as ed25519 and gssapi.

MariaDB Server supports all authentication plugins on the same port, so I think MaxScale should also be able to do it. When a user tries to log in to MariaDB Server, it checks the {{plugin}} column of the {{mysql.user}} table to decide which plugin to use to authenticate the user:

https://mariadb.com/kb/en/library/mysqluser-table/

MaxScale could do something similar. For example, it could have an ""authentication dispatcher"" class of some kind. This class could query the {{mysql.user}} table to determine which authentication plugin each user account uses, and write it to an SQLite table. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   plugin char(64),
   PRIMARY KEY (user, host)
);
{noformat}

When a user tries to log in to MaxScale, the ""authentication dispatcher"" can determine which authenticator to use for that user by checking the plugin mapping for that user account.

In MariaDB 10.4, a user account can be configured to use several different authentication plugins in a pre-configured order. This information is stored in the {{mysql.global_priv}} table.

https://mariadb.com/kb/en/library/mysqlglobal_priv-table/

If we wanted to support multiple authentication plugins in MaxScale too, then we could probable extend the mapping table schema to include an additional order column. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   order int,
   plugin char(64),
   PRIMARY KEY (user, host, order)
);
{noformat} $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,17,,0,1,4,3,0,1,0,,0,850,0,1,0,2019-12-09 11:21:43,Support all MariaDBClient-compatible authenticators on the same listener,"A lot of users are using the PAMAuth and MySQLAuth authenticators at the same time with the same services. With the current design, a {{listener}} and a {{server}} can only each have one authenticator. Therefore, this kind of configuration requires a lot of duplication:

* Every listener needs to be duplicated for every client-side authenticator.
* Every server needs to be duplicated for every backend authenticator.
* Every dependent service also needs to be duplicated.

It seems like it should be possible to change the design to support all MariaDBClient-compatible authenticators on the same listener at the same time. I think this will be even more important as users start to use even more authentication plugins on a more regular basis, such as ed25519 and gssapi.

MariaDB Server supports all authentication plugins on the same port, so I think MaxScale should also be able to do it. When a user tries to log in to MariaDB Server, it checks the {{plugin}} column of the {{mysql.user}} table to decide which plugin to use to authenticate the user:

https://mariadb.com/kb/en/library/mysqluser-table/

MaxScale could do something similar. For example, it could have an ""authentication dispatcher"" class of some kind. This class could query the {{mysql.user}} table to determine which authentication plugin each user account uses, and write it to an SQLite table. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   plugin char(64),
   PRIMARY KEY (user, host)
);
{noformat}

When a user tries to log in to MaxScale, the ""authentication dispatcher"" can determine which authenticator to use for that user by checking the plugin mapping for that user account.

In MariaDB 10.4, a user account can be configured to use several different authentication plugins in a pre-configured order. This information is stored in the {{mysql.global_priv}} table.

https://mariadb.com/kb/en/library/mysqlglobal_priv-table/

If we wanted to support multiple authentication plugins in MaxScale too, then we could probable extend the mapping table schema to include an additional order column. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   order int,
   plugin char(64),
   PRIMARY KEY (user, host, order)
);
{noformat}",,0,0,0,0,0.0,"Support all MariaDBClient-compatible authenticators on the same listener $end$ A lot of users are using the PAMAuth and MySQLAuth authenticators at the same time with the same services. With the current design, a {{listener}} and a {{server}} can only each have one authenticator. Therefore, this kind of configuration requires a lot of duplication:

* Every listener needs to be duplicated for every client-side authenticator.
* Every server needs to be duplicated for every backend authenticator.
* Every dependent service also needs to be duplicated.

It seems like it should be possible to change the design to support all MariaDBClient-compatible authenticators on the same listener at the same time. I think this will be even more important as users start to use even more authentication plugins on a more regular basis, such as ed25519 and gssapi.

MariaDB Server supports all authentication plugins on the same port, so I think MaxScale should also be able to do it. When a user tries to log in to MariaDB Server, it checks the {{plugin}} column of the {{mysql.user}} table to decide which plugin to use to authenticate the user:

https://mariadb.com/kb/en/library/mysqluser-table/

MaxScale could do something similar. For example, it could have an ""authentication dispatcher"" class of some kind. This class could query the {{mysql.user}} table to determine which authentication plugin each user account uses, and write it to an SQLite table. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   plugin char(64),
   PRIMARY KEY (user, host)
);
{noformat}

When a user tries to log in to MaxScale, the ""authentication dispatcher"" can determine which authenticator to use for that user by checking the plugin mapping for that user account.

In MariaDB 10.4, a user account can be configured to use several different authentication plugins in a pre-configured order. This information is stored in the {{mysql.global_priv}} table.

https://mariadb.com/kb/en/library/mysqlglobal_priv-table/

If we wanted to support multiple authentication plugins in MaxScale too, then we could probable extend the mapping table schema to include an additional order column. e.g.:

{noformat}
CREATE TABLE user_account_plugin_mappings (
   user char(80),
   host char(60),
   order int,
   plugin char(64),
   PRIMARY KEY (user, host, order)
);
{noformat} $acceptance criteria:$",0,0,0,0,0,0,1,4959.27,11,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1302,MXS-2505,New Feature,MXS,2019-05-22 15:54:10,MXS-2532,0,connection_keepalive independent from traffic,"Manual says:

||connection_keepalive ||||
|""The parameter value is the interval in seconds between each keepalive ping. A keepalive ping will be sent to a backend server if the connection is idle and it has not been used within n seconds where n is greater than or equal to the value of connection_keepalive. The keepalive pings are only sent when the client executes a query.""||

https://mariadb.com/kb/en/mariadb-maxscale-23-readwritesplit/#connection_keepalive

I think that the *The keepalive pings are only sent when the client executes a query* is counter intuitive.
In my opinion connection_keepalive should work especially when no queries are executed.

",,"connection_keepalive independent from traffic $end$ Manual says:

||connection_keepalive ||||
|""The parameter value is the interval in seconds between each keepalive ping. A keepalive ping will be sent to a backend server if the connection is idle and it has not been used within n seconds where n is greater than or equal to the value of connection_keepalive. The keepalive pings are only sent when the client executes a query.""||

https://mariadb.com/kb/en/mariadb-maxscale-23-readwritesplit/#connection_keepalive

I think that the *The keepalive pings are only sent when the client executes a query* is counter intuitive.
In my opinion connection_keepalive should work especially when no queries are executed.

 $acceptance criteria:$",,Claudio Nanni,Claudio Nanni,Major,16,,1,1,1,1,0,0,0,,0,850,1,0,0,2019-12-09 11:34:09,connection_keepalive independent from traffic,"Manual says:

||connection_keepalive ||||
|""The parameter value is the interval in seconds between each keepalive ping. A keepalive ping will be sent to a backend server if the connection is idle and it has not been used within n seconds where n is greater than or equal to the value of connection_keepalive. The keepalive pings are only sent when the client executes a query.""||

https://mariadb.com/kb/en/mariadb-maxscale-23-readwritesplit/#connection_keepalive

I think that the *The keepalive pings are only sent when the client executes a query* is counter intuitive.
In my opinion connection_keepalive should work especially when no queries are executed.

",,0,0,0,0,0.0,"connection_keepalive independent from traffic $end$ Manual says:

||connection_keepalive ||||
|""The parameter value is the interval in seconds between each keepalive ping. A keepalive ping will be sent to a backend server if the connection is idle and it has not been used within n seconds where n is greater than or equal to the value of connection_keepalive. The keepalive pings are only sent when the client executes a query.""||

https://mariadb.com/kb/en/mariadb-maxscale-23-readwritesplit/#connection_keepalive

I think that the *The keepalive pings are only sent when the client executes a query* is counter intuitive.
In my opinion connection_keepalive should work especially when no queries are executed.

 $acceptance criteria:$",0,0,0,0,0,0,0,4819.65,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1303,MXS-2510,Task,MXS,2019-05-27 11:24:41,,0,Ensure that MaxScale authentication still works with 10.4.,"The privileges are stored into a new table {{mysql.global_priv}} and then there will be a view {{mysql.user}} referring to the table.

Check that MaxScale works with this change.",,"Ensure that MaxScale authentication still works with 10.4. $end$ The privileges are stored into a new table {{mysql.global_priv}} and then there will be a view {{mysql.user}} referring to the table.

Check that MaxScale works with this change. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,2,0,1,0,0,0,,0,850,2,0,0,2019-05-27 11:24:41,Ensure that MaxScale authentication still works with 10.4.,"The privileges are stored into a new table {{mysql.global_priv}} and then there will be a view {{mysql.user}} referring to the table.

Check that MaxScale works with this change.",,0,0,0,0,0.0,"Ensure that MaxScale authentication still works with 10.4. $end$ The privileges are stored into a new table {{mysql.global_priv}} and then there will be a view {{mysql.user}} referring to the table.

Check that MaxScale works with this change. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,326,34,0.104294,14,0.0429448,9,0.0276074,7,0.0214724,7,0.0214724
1304,MXS-2512,Task,MXS,2019-05-28 08:13:41,,0,Enable transaction replay for additional rollback errors.,,,Enable transaction replay for additional rollback errors. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-05-28 08:13:41,Enable transaction replay for additional rollback errors.,,,0,0,0,0,0.0,Enable transaction replay for additional rollback errors. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,327,34,0.103976,14,0.0428135,9,0.0275229,7,0.0214067,7,0.0214067
1305,MXS-2514,Task,MXS,2019-05-28 10:18:04,,0,Run system test using 10.4,,,Run system test using 10.4 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,0,4,0,0,0,,0,850,1,0,0,2019-05-28 10:18:04,Run system test using 10.4,,,0,0,0,0,0.0,Run system test using 10.4 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,328,34,0.103659,14,0.0426829,9,0.027439,7,0.0213415,7,0.0213415
1306,MXS-2515,Task,MXS,2019-05-28 10:20:50,MXS-2278,0,Review KILL functionality,Review and extend test cases.,,Review KILL functionality $end$ Review and extend test cases. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,2,1,2,2,0,0,0,,0,850,1,0,0,2019-05-28 10:20:50,Review KILL functionality,Review and extend test cases.,,0,0,0,0,0.0,Review KILL functionality $end$ Review and extend test cases. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,329,34,0.103343,14,0.0425532,9,0.0273556,7,0.0212766,7,0.0212766
1307,MXS-2516,Task,MXS,2019-05-28 10:23:52,,0,Benchmark MaxScale,,,Benchmark MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-05-28 10:23:52,Benchmark MaxScale,,,0,0,0,0,0.0,Benchmark MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,330,34,0.10303,14,0.0424242,9,0.0272727,7,0.0212121,7,0.0212121
1308,MXS-2517,Task,MXS,2019-05-28 10:38:06,,0,Investigate cooperative monitoring,,,Investigate cooperative monitoring $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,14,,0,0,0,6,0,0,0,,0,850,0,0,0,2019-05-28 10:38:06,Investigate cooperative monitoring,,,0,0,0,0,0.0,Investigate cooperative monitoring $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,331,34,0.102719,14,0.0422961,9,0.0271903,7,0.021148,7,0.021148
1309,MXS-2518,Task,MXS,2019-05-28 10:39:26,,0,Run selected system test cases against Clustrix backend.,,,Run selected system test cases against Clustrix backend. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,4,0,0,0,,0,850,0,0,0,2019-05-28 10:39:26,Run selected system test cases against Clustrix backend.,,,0,0,0,0,0.0,Run selected system test cases against Clustrix backend. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,332,34,0.10241,14,0.0421687,9,0.0271084,7,0.0210843,7,0.0210843
1310,MXS-2524,Task,MXS,2019-05-29 07:36:23,,0,Write Clustrix tutorial,,,Write Clustrix tutorial $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-29 07:36:32,Write Clustrix tutorial,,,0,0,0,0,0.0,Write Clustrix tutorial $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,333,34,0.102102,14,0.042042,9,0.027027,7,0.021021,7,0.021021
1311,MXS-2537,Task,MXS,2019-05-31 11:01:22,,0,Make Clustrix monitor intervals into durations,,,Make Clustrix monitor intervals into durations $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-05-31 11:01:46,Make Clustrix monitor intervals into durations,,,0,0,0,0,0.0,Make Clustrix monitor intervals into durations $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,334,34,0.101796,14,0.0419162,9,0.0269461,7,0.0209581,7,0.0209581
1312,MXS-2540,Task,MXS,2019-06-03 10:36:05,,0,Use new config system in Clustrix monitor,,,Use new config system in Clustrix monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-06-03 10:36:09,Use new config system in Clustrix monitor,,,0,0,0,0,0.0,Use new config system in Clustrix monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,335,34,0.101493,14,0.041791,9,0.0268657,7,0.0208955,7,0.0208955
1313,MXS-2542,New Feature,MXS,2019-06-04 17:02:52,MXS-2557,0,Add rebuild server to MariaDB Monitor,"It would be nice to add a feature to MaxScale that could rebuild a corrupted machine or automatically build a new replica. That replica should then automatically join an existing cluster.  This could be accomplished in a similar method that is used with Galera SST.

On the current master:

1. Launch MariaBackup
2. Create a streaming snapshot of the datadir using xbstream/mbstream
3. Compress with a utility like pigz for minimal transfer time
4. Stream to the joiner/replica server with socat.

On the joining node:

1. Receive the socat stream
2. Decompress
3. Prepare MariaBackup
4. Find the GTID position
5. Register with the existing master / MaxScale
6. Start slave replication",,"Add rebuild server to MariaDB Monitor $end$ It would be nice to add a feature to MaxScale that could rebuild a corrupted machine or automatically build a new replica. That replica should then automatically join an existing cluster.  This could be accomplished in a similar method that is used with Galera SST.

On the current master:

1. Launch MariaBackup
2. Create a streaming snapshot of the datadir using xbstream/mbstream
3. Compress with a utility like pigz for minimal transfer time
4. Stream to the joiner/replica server with socat.

On the joining node:

1. Receive the socat stream
2. Decompress
3. Prepare MariaBackup
4. Find the GTID position
5. Register with the existing master / MaxScale
6. Start slave replication $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,55,,0,3,4,6,0,2,0,,0,850,1,0,0,2021-11-09 11:49:45,Automated State Transfer,"It would be nice to add a feature to MaxScale that could rebuild a corrupted machine or automatically build a new replica. That replica should then automatically join an existing cluster.  This could be accomplished in a similar method that is used with Galera SST.

On the current master:

1. Launch MariaBackup
2. Create a streaming snapshot of the datadir using xbstream/mbstream
3. Compress with a utility like pigz for minimal transfer time
4. Stream to the joiner/replica server with socat.

On the joining node:

1. Receive the socat stream
2. Decompress
3. Prepare MariaBackup
4. Find the GTID position
5. Register with the existing master / MaxScale
6. Start slave replication",,2,0,0,9,0.0508475,"Automated State Transfer $end$ It would be nice to add a feature to MaxScale that could rebuild a corrupted machine or automatically build a new replica. That replica should then automatically join an existing cluster.  This could be accomplished in a similar method that is used with Galera SST.

On the current master:

1. Launch MariaBackup
2. Create a streaming snapshot of the datadir using xbstream/mbstream
3. Compress with a utility like pigz for minimal transfer time
4. Stream to the joiner/replica server with socat.

On the joining node:

1. Receive the socat stream
2. Decompress
3. Prepare MariaBackup
4. Find the GTID position
5. Register with the existing master / MaxScale
6. Start slave replication $acceptance criteria:$",2,1,1,0,0,0,1,21330.8,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1314,MXS-2546,Task,MXS,2019-06-06 07:43:49,,0,Add DNS resolving to MariaDBMonitor,"When using ""assume_unique_hostnames"", the hostnames/ip-addresses of the ""Master_Host""-field of a slave connection must exactly match the address of a server in the MaxScale configuration. Make this more lenient by allowing DNS resolution on hostnames.",,"Add DNS resolving to MariaDBMonitor $end$ When using ""assume_unique_hostnames"", the hostnames/ip-addresses of the ""Master_Host""-field of a slave connection must exactly match the address of a server in the MaxScale configuration. Make this more lenient by allowing DNS resolution on hostnames. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-06-24 08:41:22,Add DNS resolving to MariaDBMonitor,"When using ""assume_unique_hostnames"", the hostnames/ip-addresses of the ""Master_Host""-field of a slave connection must exactly match the address of a server in the MaxScale configuration. Make this more lenient by allowing DNS resolution on hostnames.",,0,0,0,0,0.0,"Add DNS resolving to MariaDBMonitor $end$ When using ""assume_unique_hostnames"", the hostnames/ip-addresses of the ""Master_Host""-field of a slave connection must exactly match the address of a server in the MaxScale configuration. Make this more lenient by allowing DNS resolution on hostnames. $acceptance criteria:$",0,0,0,0,0,0,1,432.95,11,1,0.0909091,0,0.0,0,0.0,0,0.0,0,0.0
1315,MXS-2552,Task,MXS,2019-06-10 06:48:20,,0,Update documentation regarding disk plugin ,"Due to https://jira.mariadb.org/browse/MDEV-18328, FILE privileges will be needed for the monitor user so as to be able to read the information in the disks table.

Without the privilege, the table will exist, but it will be empty.

The documentation should be updated accordingly. ",,"Update documentation regarding disk plugin  $end$ Due to https://jira.mariadb.org/browse/MDEV-18328, FILE privileges will be needed for the monitor user so as to be able to read the information in the disks table.

Without the privilege, the table will exist, but it will be empty.

The documentation should be updated accordingly.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-06-10 06:48:20,Update documentation regarding disk plugin ,"Due to https://jira.mariadb.org/browse/MDEV-18328, FILE privileges will be needed for the monitor user so as to be able to read the information in the disks table.

Without the privilege, the table will exist, but it will be empty.

The documentation should be updated accordingly. ",,0,0,0,0,0.0,"Update documentation regarding disk plugin  $end$ Due to https://jira.mariadb.org/browse/MDEV-18328, FILE privileges will be needed for the monitor user so as to be able to read the information in the disks table.

Without the privilege, the table will exist, but it will be empty.

The documentation should be updated accordingly.  $acceptance criteria:$",0,0,0,0,0,0,0,0.0,336,34,0.10119,14,0.0416667,9,0.0267857,7,0.0208333,7,0.0208333
1316,MXS-2553,Task,MXS,2019-06-10 10:06:08,,0,Check 10.4 syntax modifications,,,Check 10.4 syntax modifications $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-06-10 10:06:08,Check 10.4 syntax modifications,,,0,0,0,0,0.0,Check 10.4 syntax modifications $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,337,34,0.10089,14,0.041543,9,0.0267062,7,0.0207715,7,0.0207715
1317,MXS-2555,Task,MXS,2019-06-10 10:40:49,MXS-2278,0,Smart Router,,,Smart Router $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-06-10 10:40:49,Smart Router,,,0,0,0,0,0.0,Smart Router $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,338,34,0.100592,14,0.0414201,9,0.0266272,7,0.0207101,7,0.0207101
1318,MXS-2556,Task,MXS,2019-06-10 10:42:29,MXS-2278,0,Smart router configuration,,,Smart router configuration $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-06-10 10:42:29,Smart router configuration,,,0,0,0,0,0.0,Smart router configuration $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,339,34,0.100295,14,0.0412979,9,0.0265487,7,0.020649,7,0.020649
1319,MXS-2572,Task,MXS,2019-06-24 07:54:17,,0,Create SmartRouter tests.,,,Create SmartRouter tests. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-06-24 07:54:17,Create SmartRouter tests.,,,0,0,0,0,0.0,Create SmartRouter tests. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,340,34,0.1,14,0.0411765,9,0.0264706,7,0.0205882,7,0.0205882
1320,MXS-2573,Task,MXS,2019-06-24 08:32:09,,0,Investigate performance characteristics of 2.2 and 2.3,,,Investigate performance characteristics of 2.2 and 2.3 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,12,,0,0,0,7,0,0,0,,0,850,0,0,0,2019-06-24 08:32:09,Investigate performance characteristics of 2.2 and 2.3,,,0,0,0,0,0.0,Investigate performance characteristics of 2.2 and 2.3 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,341,34,0.0997067,14,0.0410557,9,0.026393,7,0.0205279,7,0.0205279
1321,MXS-2579,New Feature,MXS,2019-06-26 14:12:16,MXS-2529,0,adding schema to qla filter,"The query metadata captured by qlafilter does not contain the schema on which the query is done.  Having this information is useful when multiple applications share a DB but are logically separated by the schema. 

A solution might be to take the schema information from the connection.  The only drawback here is queries where we explicitly define another schema. Example:
{code}
mysql -h host -u user -p salary
> [...] # any query gets automatically the `salary` schema

# edge case:
> select user from mysql.user;
{code}

I would be ok with the edge case not being covered. ",,"adding schema to qla filter $end$ The query metadata captured by qlafilter does not contain the schema on which the query is done.  Having this information is useful when multiple applications share a DB but are logically separated by the schema. 

A solution might be to take the schema information from the connection.  The only drawback here is queries where we explicitly define another schema. Example:
{code}
mysql -h host -u user -p salary
> [...] # any query gets automatically the `salary` schema

# edge case:
> select user from mysql.user;
{code}

I would be ok with the edge case not being covered.  $acceptance criteria:$",,Kadir,Kadir,Major,11,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-12-09 11:52:39,adding schema to qla filter,"The query metadata captured by qlafilter does not contain the schema on which the query is done.  Having this information is useful when multiple applications share a DB but are logically separated by the schema. 

A solution might be to take the schema information from the connection.  The only drawback here is queries where we explicitly define another schema. Example:
{code}
mysql -h host -u user -p salary
> [...] # any query gets automatically the `salary` schema

# edge case:
> select user from mysql.user;
{code}

I would be ok with the edge case not being covered. ",,0,0,0,0,0.0,"adding schema to qla filter $end$ The query metadata captured by qlafilter does not contain the schema on which the query is done.  Having this information is useful when multiple applications share a DB but are logically separated by the schema. 

A solution might be to take the schema information from the connection.  The only drawback here is queries where we explicitly define another schema. Example:
{code}
mysql -h host -u user -p salary
> [...] # any query gets automatically the `salary` schema

# edge case:
> select user from mysql.user;
{code}

I would be ok with the edge case not being covered.  $acceptance criteria:$",0,0,0,0,0,0,0,3981.67,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1322,MXS-2588,New Feature,MXS,2019-07-04 13:54:46,MXS-2682,0,Kafka to MariaDB router,"Clients are requesting a way to fetch datas from kafka topics into the mariadb server but not into column store, so in a storage engine agnostic way.

It feels like it could be a great idea to convert AVRO events (avro format would be requested on the kafka side) into binlogs, then have the instances be slaves of the avro/binlog router. 

Most of the functionalities seem to be already existing, even tho syncing them together might be tricky.",,"Kafka to MariaDB router $end$ Clients are requesting a way to fetch datas from kafka topics into the mariadb server but not into column store, so in a storage engine agnostic way.

It feels like it could be a great idea to convert AVRO events (avro format would be requested on the kafka side) into binlogs, then have the instances be slaves of the avro/binlog router. 

Most of the functionalities seem to be already existing, even tho syncing them together might be tricky. $acceptance criteria:$",,Sylvain ARBAUDIE,Sylvain ARBAUDIE,Major,22,,0,1,3,1,0,0,0,,0,850,0,0,0,2021-04-12 09:54:54,Kafka to MariaDB router,"Clients are requesting a way to fetch datas from kafka topics into the mariadb server but not into column store, so in a storage engine agnostic way.

It feels like it could be a great idea to convert AVRO events (avro format would be requested on the kafka side) into binlogs, then have the instances be slaves of the avro/binlog router. 

Most of the functionalities seem to be already existing, even tho syncing them together might be tricky.",,0,0,0,0,0.0,"Kafka to MariaDB router $end$ Clients are requesting a way to fetch datas from kafka topics into the mariadb server but not into column store, so in a storage engine agnostic way.

It feels like it could be a great idea to convert AVRO events (avro format would be requested on the kafka side) into binlogs, then have the instances be slaves of the avro/binlog router. 

Most of the functionalities seem to be already existing, even tho syncing them together might be tricky. $acceptance criteria:$",0,0,0,0,0,0,0,15548.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1323,MXS-2590,Task,MXS,2019-07-05 08:25:51,,0,Add SmartQuery system tests.,,,Add SmartQuery system tests. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,27,,0,0,0,11,0,0,0,,0,850,0,0,0,2019-07-05 08:25:51,Add SmartQuery system tests.,,,0,0,0,0,0.0,Add SmartQuery system tests. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,342,34,0.0994152,14,0.0409357,9,0.0263158,7,0.0204678,7,0.0204678
1324,MXS-2591,Task,MXS,2019-07-05 08:27:59,MXS-2635,0,Investigate/design direct router-to-router routing.,,,Investigate/design direct router-to-router routing. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,12,,0,0,1,3,0,1,0,,0,850,0,0,0,2019-07-05 08:27:59,Investigate direct router-to-router routing.,,,1,0,0,2,0.142857,Investigate direct router-to-router routing. $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,343,34,0.0991254,14,0.0408163,9,0.0262391,7,0.0204082,7,0.0204082
1325,MXS-2592,Task,MXS,2019-07-05 08:36:54,,0,Rotating session specific in-memory info log.,,,Rotating session specific in-memory info log. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-07-05 08:36:54,Rotating session specific in-memory info log.,,,0,0,0,0,0.0,Rotating session specific in-memory info log. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,344,35,0.101744,14,0.0406977,9,0.0261628,7,0.0203488,7,0.0203488
1326,MXS-2615,New Feature,MXS,2019-07-25 14:55:54,MXS-2658,0,Add expire_log_days to BinlogRouter,It would be useful to have the binary logs purged automatically like with 'expire_log_days' of the server. See also: https://jira.mariadb.org/browse/MXS-770,,Add expire_log_days to BinlogRouter $end$ It would be useful to have the binary logs purged automatically like with 'expire_log_days' of the server. See also: https://jira.mariadb.org/browse/MXS-770 $acceptance criteria:$,,Claudio Nanni,Claudio Nanni,Major,32,,0,1,0,4,0,0,0,,0,850,1,0,0,2020-05-25 11:37:33,Add expire_log_days to BinlogRouter,It would be useful to have the binary logs purged automatically like with 'expire_log_days' of the server. See also: https://jira.mariadb.org/browse/MXS-770,,0,0,0,0,0.0,Add expire_log_days to BinlogRouter $end$ It would be useful to have the binary logs purged automatically like with 'expire_log_days' of the server. See also: https://jira.mariadb.org/browse/MXS-770 $acceptance criteria:$,0,0,0,0,0,0,1,7316.68,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1327,MXS-2617,Task,MXS,2019-07-29 10:23:07,,0,Scalable cache mechanism for maxscale.,,,Scalable cache mechanism for maxscale. $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Minor,16,,0,1,0,4,0,0,0,,0,850,1,0,0,2019-07-29 10:33:00,Scalable cache mechanism for maxscale.,,,0,0,0,0,0.0,Scalable cache mechanism for maxscale. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.15,9,1,0.111111,0,0.0,0,0.0,0,0.0,0,0.0
1328,MXS-2618,Task,MXS,2019-07-29 10:52:33,,0,Design/prepare authenticator refactoring.,,,Design/prepare authenticator refactoring. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-07-29 10:52:33,Design/prepare authenticator refactoring.,,,0,0,0,0,0.0,Design/prepare authenticator refactoring. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,345,35,0.101449,14,0.0405797,9,0.026087,7,0.0202899,7,0.0202899
1329,MXS-2619,Task,MXS,2019-07-29 12:55:11,,0,repository with all versions of Maxscale,"currently Maxscale binary repo contains only one version of Maxscale. That prevents downgrade or installing certain version without re-configuring yum/apt/zypper. 

A separate repository with all versions of Maxscale to be created by separate BuildBot task in the separate directory (not touching current system)",,"repository with all versions of Maxscale $end$ currently Maxscale binary repo contains only one version of Maxscale. That prevents downgrade or installing certain version without re-configuring yum/apt/zypper. 

A separate repository with all versions of Maxscale to be created by separate BuildBot task in the separate directory (not touching current system) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-08-13 10:47:36,repository with all versions of Maxscale,"currently Maxscale binary repo contains only one version of Maxscale. That prevents downgrade or installing certain version without re-configuring yum/apt/zypper. 

A separate repository with all versions of Maxscale to be created by separate BuildBot task in the separate directory (not touching current system)",,0,0,0,0,0.0,"repository with all versions of Maxscale $end$ currently Maxscale binary repo contains only one version of Maxscale. That prevents downgrade or installing certain version without re-configuring yum/apt/zypper. 

A separate repository with all versions of Maxscale to be created by separate BuildBot task in the separate directory (not touching current system) $acceptance criteria:$",0,0,0,0,0,0,0,357.867,78,1,0.0128205,0,0.0,0,0.0,0,0.0,0,0.0
1330,MXS-2622,Task,MXS,2019-08-02 10:04:28,MXS-2668,0,Run MHD tasks in an independent thread,"MHD tasks are now run in worker#0, which is already used by many other modules like logging and maxadmin. That make API server unstable.

My scenario is, somehow the disk crashed, and the logging task just hung when writing file to disk, that made the worker#0 stopped working. I wanted to restart service by calling API interface, but worker#0 did not respond to any command. If MHD tasks run in an independent thread, at least I got a chance to rescue.

As API server is a vital part of DevOps, will you guys consider to run MHD tasks in an independent thread？ 
",,"Run MHD tasks in an independent thread $end$ MHD tasks are now run in worker#0, which is already used by many other modules like logging and maxadmin. That make API server unstable.

My scenario is, somehow the disk crashed, and the logging task just hung when writing file to disk, that made the worker#0 stopped working. I wanted to restart service by calling API interface, but worker#0 did not respond to any command. If MHD tasks run in an independent thread, at least I got a chance to rescue.

As API server is a vital part of DevOps, will you guys consider to run MHD tasks in an independent thread？ 
 $acceptance criteria:$",,lishubing,lishubing,Major,17,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-09-09 09:22:00,Run MHD tasks in an independent thread,"MHD tasks are now run in worker#0, which is already used by many other modules like logging and maxadmin. That make API server unstable.

My scenario is, somehow the disk crashed, and the logging task just hung when writing file to disk, that made the worker#0 stopped working. I wanted to restart service by calling API interface, but worker#0 did not respond to any command. If MHD tasks run in an independent thread, at least I got a chance to rescue.

As API server is a vital part of DevOps, will you guys consider to run MHD tasks in an independent thread？ 
",,0,0,0,0,0.0,"Run MHD tasks in an independent thread $end$ MHD tasks are now run in worker#0, which is already used by many other modules like logging and maxadmin. That make API server unstable.

My scenario is, somehow the disk crashed, and the logging task just hung when writing file to disk, that made the worker#0 stopped working. I wanted to restart service by calling API interface, but worker#0 did not respond to any command. If MHD tasks run in an independent thread, at least I got a chance to rescue.

As API server is a vital part of DevOps, will you guys consider to run MHD tasks in an independent thread？ 
 $acceptance criteria:$",0,0,0,0,0,0,1,911.283,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1331,MXS-2623,New Feature,MXS,2019-08-06 19:16:51,MXS-2671,0,MaxScale should write a log message if a user with SUPER privileges connects,"I think that it would be helpful if MaxScale could write a log message if a user with SUPER privileges connected to the instance. A pre-requisite for this would probably have to be that skip_authentication is not set.

https://mariadb.com/kb/en/mariadb-maxscale-23-mysql-authenticator/#skip_authentication

The main reason is that users with SUPER privileges can cause some serious issues, particularly when MaxScale is using MariaDB Monitor. In that case, users with SUPER privileges can still write to the servers while failover is occurring, which can cause failover to break. This problem seems to occur quite frequently.

If we add this log message, then maybe it should be optional. Should it be an option that is configurable in authenticator_options? Or should it only be written when log_info is enabled?",,"MaxScale should write a log message if a user with SUPER privileges connects $end$ I think that it would be helpful if MaxScale could write a log message if a user with SUPER privileges connected to the instance. A pre-requisite for this would probably have to be that skip_authentication is not set.

https://mariadb.com/kb/en/mariadb-maxscale-23-mysql-authenticator/#skip_authentication

The main reason is that users with SUPER privileges can cause some serious issues, particularly when MaxScale is using MariaDB Monitor. In that case, users with SUPER privileges can still write to the servers while failover is occurring, which can cause failover to break. This problem seems to occur quite frequently.

If we add this log message, then maybe it should be optional. Should it be an option that is configurable in authenticator_options? Or should it only be written when log_info is enabled? $acceptance criteria:$",,Geoff Montee,Geoff Montee,Major,33,,0,0,0,5,0,0,0,,0,850,0,0,0,2019-08-13 10:36:52,MaxScale should write a log message if a user with SUPER privileges connects,"I think that it would be helpful if MaxScale could write a log message if a user with SUPER privileges connected to the instance. A pre-requisite for this would probably have to be that skip_authentication is not set.

https://mariadb.com/kb/en/mariadb-maxscale-23-mysql-authenticator/#skip_authentication

The main reason is that users with SUPER privileges can cause some serious issues, particularly when MaxScale is using MariaDB Monitor. In that case, users with SUPER privileges can still write to the servers while failover is occurring, which can cause failover to break. This problem seems to occur quite frequently.

If we add this log message, then maybe it should be optional. Should it be an option that is configurable in authenticator_options? Or should it only be written when log_info is enabled?",,0,0,0,0,0.0,"MaxScale should write a log message if a user with SUPER privileges connects $end$ I think that it would be helpful if MaxScale could write a log message if a user with SUPER privileges connected to the instance. A pre-requisite for this would probably have to be that skip_authentication is not set.

https://mariadb.com/kb/en/mariadb-maxscale-23-mysql-authenticator/#skip_authentication

The main reason is that users with SUPER privileges can cause some serious issues, particularly when MaxScale is using MariaDB Monitor. In that case, users with SUPER privileges can still write to the servers while failover is occurring, which can cause failover to break. This problem seems to occur quite frequently.

If we add this log message, then maybe it should be optional. Should it be an option that is configurable in authenticator_options? Or should it only be written when log_info is enabled? $acceptance criteria:$",0,0,0,0,0,0,1,159.333,12,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1332,MXS-2629,Task,MXS,2019-08-09 12:35:17,,0,Cleanup mariadbclient,,,Cleanup mariadbclient $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-08-12 05:41:40,Cleanup mariadbclient,,,0,0,0,0,0.0,Cleanup mariadbclient $end$ $acceptance criteria:$,0,0,0,0,0,0,0,65.1,346,35,0.101156,14,0.0404624,9,0.0260116,7,0.0202312,7,0.0202312
1333,MXS-2632,Task,MXS,2019-08-13 09:58:52,,0,Refactor protocol interface,,,Refactor protocol interface $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,14,,0,1,1,6,0,0,0,,0,850,1,0,0,2019-08-13 09:58:52,Refactor protocol interface,,,0,0,0,0,0.0,Refactor protocol interface $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,347,35,0.100865,14,0.0403458,9,0.0259366,7,0.0201729,7,0.0201729
1334,MXS-2636,Task,MXS,2019-08-13 10:28:30,MXS-2635,0,Implement service-to-service routing,Extend the configuration to allow services to route queries directly to other services.,,Implement service-to-service routing $end$ Extend the configuration to allow services to route queries directly to other services. $acceptance criteria:$,,markus makela,markus makela,Major,14,,0,0,0,3,0,1,0,,0,850,0,0,0,2019-08-13 10:28:48,Implement service-to-service configuration,Extend the configuration to allow services to route queries directly to other services.,,1,0,0,2,0.0526316,Implement service-to-service configuration $end$ Extend the configuration to allow services to route queries directly to other services. $acceptance criteria:$,1,1,0,0,0,0,1,0.0,65,11,0.169231,10,0.153846,8,0.123077,8,0.123077,7,0.107692
1335,MXS-2646,New Feature,MXS,2019-08-20 17:04:50,MXS-3387,0,Add JSON listener (MongoDB API),,,Add JSON listener (MongoDB API) $end$ $acceptance criteria:$,,Manjot Singh,Manjot Singh,Major,44,,0,0,2,2,0,1,0,,0,850,0,1,0,2020-09-14 10:51:38,Add JSON listener (MongoDB API),,,0,0,0,0,0.0,Add JSON listener (MongoDB API) $end$ $acceptance criteria:$,0,0,0,0,0,0,1,9377.77,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1336,MXS-2653,Task,MXS,2019-09-02 09:31:29,,0,Add backend SSL tests into daily test run,select tests and create clones of them to run using monitor user that requires ssl to connect to the backend,,Add backend SSL tests into daily test run $end$ select tests and create clones of them to run using monitor user that requires ssl to connect to the backend $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-09-09 09:16:40,Add backend SSL tests into daily test run,select tests and create clones of them to run using monitor user that requires ssl to connect to the backend,,0,0,0,0,0.0,Add backend SSL tests into daily test run $end$ select tests and create clones of them to run using monitor user that requires ssl to connect to the backend $acceptance criteria:$,0,0,0,0,0,0,0,167.75,79,1,0.0126582,0,0.0,0,0.0,0,0.0,0,0.0
1337,MXS-2654,Task,MXS,2019-09-03 05:22:10,,0,Document QC cache metrics,,,Document QC cache metrics $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,1,0,2,0,0,0,,0,850,1,0,0,2019-09-09 09:16:32,Document QC cache metrics,,,0,0,0,0,0.0,Document QC cache metrics $end$ $acceptance criteria:$,0,0,0,0,0,0,1,147.9,348,35,0.100575,14,0.0402299,9,0.0258621,7,0.0201149,7,0.0201149
1338,MXS-2656,Task,MXS,2019-09-03 12:40:15,MXS-2681,0,Cleanup persistent connection handling,,,Cleanup persistent connection handling $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-09-09 09:21:50,Cleanup persistent connection handling,,,0,0,0,0,0.0,Cleanup persistent connection handling $end$ $acceptance criteria:$,0,0,0,0,0,0,1,140.683,349,35,0.100287,14,0.0401146,9,0.025788,7,0.0200573,7,0.0200573
1339,MXS-2662,New Feature,MXS,2019-09-05 11:03:20,MXS-2658,0,KMIP Binlog Data At Rest Encryption,"* Add data-at-rest encryption to the binlogrouter
* Make it possible to use KMIP to retrieve encryption keys
* Add support (or at least make it easy to do in the future) for other key management systems (e.g. Hashicorp Vault)",,"KMIP Binlog Data At Rest Encryption $end$ * Add data-at-rest encryption to the binlogrouter
* Make it possible to use KMIP to retrieve encryption keys
* Add support (or at least make it easy to do in the future) for other key management systems (e.g. Hashicorp Vault) $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,23,,0,1,0,6,0,0,5,,0,850,0,0,0,2022-05-10 08:53:58,KMIP Binlog Data At Rest Encryption,"* Add data-at-rest encryption to the binlogrouter
* Make it possible to use KMIP to retrieve encryption keys
* Add support (or at least make it easy to do in the future) for other key management systems (e.g. Hashicorp Vault)",,0,0,0,0,0.0,"KMIP Binlog Data At Rest Encryption $end$ * Add data-at-rest encryption to the binlogrouter
* Make it possible to use KMIP to retrieve encryption keys
* Add support (or at least make it easy to do in the future) for other key management systems (e.g. Hashicorp Vault) $acceptance criteria:$",0,0,0,0,0,0,1,23469.8,2,1,0.5,1,0.5,0,0.0,0,0.0,0,0.0
1340,MXS-2665,Task,MXS,2019-09-06 07:26:26,MXS-2635,0,Document service-to-service routing,Document the {{target}} parameter and how it interacts with the {{cluster}} and {{servers}} parameters.,,Document service-to-service routing $end$ Document the {{target}} parameter and how it interacts with the {{cluster}} and {{servers}} parameters. $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-09-09 09:18:28,Document service-to-service routing,Document the {{target}} parameter and how it interacts with the {{cluster}} and {{servers}} parameters.,,0,0,0,0,0.0,Document service-to-service routing $end$ Document the {{target}} parameter and how it interacts with the {{cluster}} and {{servers}} parameters. $acceptance criteria:$,0,0,0,0,0,0,0,73.8667,66,12,0.181818,10,0.151515,8,0.121212,8,0.121212,7,0.106061
1341,MXS-2669,Task,MXS,2019-09-09 10:32:13,,0,Fix using local maxscale with system tests,,,Fix using local maxscale with system tests $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-09-09 10:32:13,Fix using local maxscale with system tests,,,0,0,0,0,0.0,Fix using local maxscale with system tests $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,350,35,0.1,14,0.04,9,0.0257143,7,0.02,7,0.02
1342,MXS-2670,Task,MXS,2019-09-09 12:11:54,,0,Cleanup protocol and authenticator-modules,This is a separate task for cleaning up the protocol and authenticator-modules themselves instead of the generic interface.,,Cleanup protocol and authenticator-modules $end$ This is a separate task for cleaning up the protocol and authenticator-modules themselves instead of the generic interface. $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,15,,0,1,1,3,0,2,0,,0,850,1,0,0,2019-09-09 12:13:12,Cleanup protocol-modules,This is a separate task for cleaning up the protocol-modules themselves instead of the generic interface.,,1,1,0,8,0.285714,Cleanup protocol-modules $end$ This is a separate task for cleaning up the protocol-modules themselves instead of the generic interface. $acceptance criteria:$,2,1,1,0,0,0,1,0.0166667,12,1,0.0833333,0,0.0,0,0.0,0,0.0,0,0.0
1343,MXS-2685,Task,MXS,2019-09-17 13:24:42,,0,Change SmartRouter query cache to use SharedData,,,Change SmartRouter query cache to use SharedData $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Minor,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-09-17 13:27:28,Change SmartRouter query cache to use SharedData,,,0,0,0,0,0.0,Change SmartRouter query cache to use SharedData $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0333333,10,1,0.1,0,0.0,0,0.0,0,0.0,0,0.0
1344,MXS-2691,New Feature,MXS,2019-09-20 06:23:29,MXS-1133,0,Shared Cache,"Add the ability to share a distributed cache among MaxScale servers.

Offer a choice of cache backends: 

Local (not distributed)
Memcache (distributed)
Hazelcast (distributed)
Redis (distributed)
Ehcache (distributed)",,"Shared Cache $end$ Add the ability to share a distributed cache among MaxScale servers.

Offer a choice of cache backends: 

Local (not distributed)
Memcache (distributed)
Hazelcast (distributed)
Redis (distributed)
Ehcache (distributed) $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,23,,0,1,3,4,0,4,0,,0,850,1,2,0,2019-11-12 11:24:08,Distributed Cache,Add the ability to share a distributed cache among MaxScale servers.,,1,1,0,19,1.125,Distributed Cache $end$ Add the ability to share a distributed cache among MaxScale servers. $acceptance criteria:$,2,1,1,1,1,0,1,1277.0,3,1,0.333333,1,0.333333,0,0.0,0,0.0,0,0.0
1345,MXS-2692,New Feature,MXS,2019-09-20 20:02:00,MXS-2658,0,Monitor of binlog router,"Hi, 

In order to be able to monitor MaxScale binlogrouter, is it possible to manage the monitoring in order to view the activity of binlogrouter directly in maxctrl command ?

Thanks 
",,"Monitor of binlog router $end$ Hi, 

In order to be able to monitor MaxScale binlogrouter, is it possible to manage the monitoring in order to view the activity of binlogrouter directly in maxctrl command ?

Thanks 
 $acceptance criteria:$",,Sebastien GIRAUD,Sebastien GIRAUD,Major,20,,0,2,0,2,0,0,0,,0,850,0,0,0,2020-08-31 12:14:11,Monitor of binlog router,"Hi, 

In order to be able to monitor MaxScale binlogrouter, is it possible to manage the monitoring in order to view the activity of binlogrouter directly in maxctrl command ?

Thanks 
",,0,0,0,0,0.0,"Monitor of binlog router $end$ Hi, 

In order to be able to monitor MaxScale binlogrouter, is it possible to manage the monitoring in order to view the activity of binlogrouter directly in maxctrl command ?

Thanks 
 $acceptance criteria:$",0,0,0,0,0,0,1,8296.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1346,MXS-2695,Task,MXS,2019-09-23 11:26:37,,0,Benchmark MaxScale,,,Benchmark MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-09-23 11:26:37,Benchmark MaxScale,,,0,0,0,0,0.0,Benchmark MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,351,35,0.0997151,14,0.039886,9,0.025641,7,0.019943,7,0.019943
1347,MXS-2696,Task,MXS,2019-09-23 11:38:55,,0,Combine result tracking codes,Combine the resultset tracking code in the cache and masking filters into the backend protocol resultset tracking.,,Combine result tracking codes $end$ Combine the resultset tracking code in the cache and masking filters into the backend protocol resultset tracking. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2019-09-23 11:38:55,Combine result tracking codes,Combine the resultset tracking code in the cache and masking filters into the backend protocol resultset tracking.,,0,0,0,0,0.0,Combine result tracking codes $end$ Combine the resultset tracking code in the cache and masking filters into the backend protocol resultset tracking. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,67,12,0.179104,10,0.149254,8,0.119403,8,0.119403,7,0.104478
1348,MXS-2708,New Feature,MXS,2019-10-02 09:57:32,MXS-1133,0,memcached storage for cache,,,memcached storage for cache $end$ $acceptance criteria:$,,Todd Stoffel,Todd Stoffel,Major,16,,0,0,1,2,0,2,0,,0,850,0,0,0,2019-12-12 14:08:58,Plugable Cache Backends,"Offer a choice of cache backends: 

Local (not distributed)
Memcache (distributed)
Hazelcast (distributed)
Redis (distributed)
Ehcache (distributed)",,1,1,0,24,0.869565,"Plugable Cache Backends $end$ Offer a choice of cache backends: 

Local (not distributed)
Memcache (distributed)
Hazelcast (distributed)
Redis (distributed)
Ehcache (distributed) $acceptance criteria:$",2,1,1,1,1,1,1,1708.18,4,2,0.5,2,0.5,1,0.25,1,0.25,0,0.0
1349,MXS-2709,New Feature,MXS,2019-10-02 10:06:27,,0,ETL / Data Migration Service,"Plan a new feature to provide AWS DMS type service.
Details later.",,"ETL / Data Migration Service $end$ Plan a new feature to provide AWS DMS type service.
Details later. $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,40,,5,3,10,12,0,4,10,,0,850,2,4,0,2022-09-12 10:19:02,ETL / Data Migration Service,"Plan a new feature to provide AWS DMS type service.
Details later.",,0,0,0,0,0.0,"ETL / Data Migration Service $end$ Plan a new feature to provide AWS DMS type service.
Details later. $acceptance criteria:$",0,0,0,0,0,0,1,25824.2,5,3,0.6,3,0.6,2,0.4,2,0.4,1,0.2
1350,MXS-2715,Task,MXS,2019-10-07 07:41:54,,0,"Automate Otaniemi test system deployment, docker-remote commands.",,,"Automate Otaniemi test system deployment, docker-remote commands. $end$ $acceptance criteria:$",,Niclas Antti,Niclas Antti,Minor,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-10-07 10:34:08,"Automate Otaniemi test system deployment, docker-remote commands.",,,0,0,0,0,0.0,"Automate Otaniemi test system deployment, docker-remote commands. $end$ $acceptance criteria:$",0,0,0,0,0,0,1,2.86667,11,1,0.0909091,0,0.0,0,0.0,0,0.0,0,0.0
1351,MXS-2716,Task,MXS,2019-10-07 09:41:10,,0,Perform systemd watchdog notifications in dedicated non-worker thread.,,,Perform systemd watchdog notifications in dedicated non-worker thread. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-10-07 09:41:10,Perform systemd watchdog notifications in dedicated non-worker thread.,,,0,0,0,0,0.0,Perform systemd watchdog notifications in dedicated non-worker thread. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,352,35,0.0994318,14,0.0397727,9,0.0255682,7,0.0198864,7,0.0198864
1352,MXS-2717,Task,MXS,2019-10-07 10:27:42,MXS-2528,0,Generic user account management for MariaDB,,,Generic user account management for MariaDB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,14,,0,0,0,5,0,0,0,,0,850,0,0,0,2019-10-07 10:27:42,Generic user account management for MariaDB,,,0,0,0,0,0.0,Generic user account management for MariaDB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,353,35,0.0991501,14,0.0396601,9,0.0254958,7,0.01983,7,0.01983
1353,MXS-2718,Task,MXS,2019-10-07 10:44:11,,0,test maxscale-system-test with own build of Chef,"now Chef is build by MDBCI build script (ready-to-use binaries are commercial now) 
the task is test all MDBCI functions with this new Chef build",,"test maxscale-system-test with own build of Chef $end$ now Chef is build by MDBCI build script (ready-to-use binaries are commercial now) 
the task is test all MDBCI functions with this new Chef build $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-10-07 10:44:26,test maxscale-system-test with own build of Chef,"now Chef is build by MDBCI build script (ready-to-use binaries are commercial now) 
the task is test all MDBCI functions with this new Chef build",,0,0,0,0,0.0,"test maxscale-system-test with own build of Chef $end$ now Chef is build by MDBCI build script (ready-to-use binaries are commercial now) 
the task is test all MDBCI functions with this new Chef build $acceptance criteria:$",0,0,0,0,0,0,1,0.0,80,1,0.0125,0,0.0,0,0.0,0,0.0,0,0.0
1354,MXS-2719,New Feature,MXS,2019-10-07 13:15:31,MXS-2531,0,Allow queuing of switchover/failover,"As the two operations can take a long time, making it possible to start the failover process without having to wait for it to complete would make it more comfortable to use. In addition to this, the queued operations could be cancelled if they are at a point where it is possible (e.g. waiting for slaves to catch up).",,"Allow queuing of switchover/failover $end$ As the two operations can take a long time, making it possible to start the failover process without having to wait for it to complete would make it more comfortable to use. In addition to this, the queued operations could be cancelled if they are at a point where it is possible (e.g. waiting for slaves to catch up). $acceptance criteria:$",,markus makela,markus makela,Major,15,,0,1,0,2,0,0,0,,0,850,1,0,0,2020-08-31 12:13:57,Allow queuing of switchover/failover,"As the two operations can take a long time, making it possible to start the failover process without having to wait for it to complete would make it more comfortable to use. In addition to this, the queued operations could be cancelled if they are at a point where it is possible (e.g. waiting for slaves to catch up).",,0,0,0,0,0.0,"Allow queuing of switchover/failover $end$ As the two operations can take a long time, making it possible to start the failover process without having to wait for it to complete would make it more comfortable to use. In addition to this, the queued operations could be cancelled if they are at a point where it is possible (e.g. waiting for slaves to catch up). $acceptance criteria:$",0,0,0,0,0,0,1,7894.97,68,12,0.176471,10,0.147059,8,0.117647,8,0.117647,7,0.102941
1355,MXS-272,New Feature,MXS,2015-07-15 18:23:59,,0,Hintrouter,"The _hintrouter_ is a router that routes solely based on hints, so it need to be used in conjunction with a filter that provides those hints. An example of such a filter is the _namedserverfilter_ that more aptly should be called _regexhintfilter_.

In addition it should be possible to specify some default behaviour that is followed if a hint is missing. For instance, you could specify that if a statement lacks a hint, then the statement is routed to some slave, which would mean that the _namedserverfilter_ would then only need to be configured to catch and hint statements that need to be sent to the master. The default behaviour could also be that an error should be returned or that the statement should be routed to all backends. Depending on the context different approaches may the appropritate and thus the behaviour should be configurable.

Together _namedserverfilter_ and _hintrouter_ would provide a very low-overhead solution appropriate in certain situations.",,"Hintrouter $end$ The _hintrouter_ is a router that routes solely based on hints, so it need to be used in conjunction with a filter that provides those hints. An example of such a filter is the _namedserverfilter_ that more aptly should be called _regexhintfilter_.

In addition it should be possible to specify some default behaviour that is followed if a hint is missing. For instance, you could specify that if a statement lacks a hint, then the statement is routed to some slave, which would mean that the _namedserverfilter_ would then only need to be configured to catch and hint statements that need to be sent to the master. The default behaviour could also be that an error should be returned or that the statement should be routed to all backends. Depending on the context different approaches may the appropritate and thus the behaviour should be configurable.

Together _namedserverfilter_ and _hintrouter_ would provide a very low-overhead solution appropriate in certain situations. $acceptance criteria:$",,markus makela,markus makela,Major,11,,0,1,0,1,0,2,0,,0,850,1,0,0,2017-03-15 10:45:10,Regex router,"A router which would route queries to servers based on regular expression matches. An ordered list of regular expressions would be matched against the query and if one matches the query would be routed to the associated server.

This would be an improvement over the namedserverfilter and readwritesplit combination which currently serves this need. The aforementioned combination does not override the readwritesplit router's priorities and only works with read queries. The new router module would allow the user to control the destination servers and could be used to provide a simpler and more lightweight solution for routing.",,1,1,0,229,1.42157,"Regex router $end$ A router which would route queries to servers based on regular expression matches. An ordered list of regular expressions would be matched against the query and if one matches the query would be routed to the associated server.

This would be an improvement over the namedserverfilter and readwritesplit combination which currently serves this need. The aforementioned combination does not override the readwritesplit router's priorities and only works with read queries. The new router module would allow the user to control the destination servers and could be used to provide a simpler and more lightweight solution for routing. $acceptance criteria:$",2,1,1,1,1,1,1,14608.3,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1356,MXS-2723,New Feature,MXS,2019-10-14 10:59:53,,0,MariaDBMonitor add script event when max_slave_replication_lag is reached,add a new event to the mariadbmonitor to launch an external script when a slave server reaches a configured limit of slave replication lag.,,MariaDBMonitor add script event when max_slave_replication_lag is reached $end$ add a new event to the mariadbmonitor to launch an external script when a slave server reaches a configured limit of slave replication lag. $acceptance criteria:$,,Maikel Punie,Maikel Punie,Minor,19,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-11-09 12:09:54,MariaDBMonitor add script event when max_slave_replication_lag is reached,add a new event to the mariadbmonitor to launch an external script when a slave server reaches a configured limit of slave replication lag.,,0,0,0,0,0.0,MariaDBMonitor add script event when max_slave_replication_lag is reached $end$ add a new event to the mariadbmonitor to launch an external script when a slave server reaches a configured limit of slave replication lag. $acceptance criteria:$,0,0,0,0,0,0,1,9409.17,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1357,MXS-2724,New Feature,MXS,2019-10-15 12:04:07,MXS-2533,0,Expose memory usage information,At times it's very hard to know why MaxScale is consuming memory and which part in particular is causing it. Exposing e.g. the DCB buffer lengths would give an indication as to where the memory is going.  ,,Expose memory usage information $end$ At times it's very hard to know why MaxScale is consuming memory and which part in particular is causing it. Exposing e.g. the DCB buffer lengths would give an indication as to where the memory is going.   $acceptance criteria:$,,markus makela,markus makela,Minor,31,,0,6,2,2,0,0,0,,0,850,5,0,0,2022-06-06 05:35:00,Expose memory usage information,At times it's very hard to know why MaxScale is consuming memory and which part in particular is causing it. Exposing e.g. the DCB buffer lengths would give an indication as to where the memory is going.  ,,0,0,0,0,0.0,Expose memory usage information $end$ At times it's very hard to know why MaxScale is consuming memory and which part in particular is causing it. Exposing e.g. the DCB buffer lengths would give an indication as to where the memory is going.   $acceptance criteria:$,0,0,0,0,0,0,1,23153.5,69,12,0.173913,10,0.144928,8,0.115942,8,0.115942,7,0.101449
1358,MXS-2729,Task,MXS,2019-10-22 11:27:40,,0,Implement worker load balancing,"If the work-load and life-time of different sessions differ significantly, it may lead to a situation where a particular worker continuously has a higher load than all other workers.

MaxScale should be capable of moving a session from one worker to another, so that the load of all workers can be kept roughly the same.
",,"Implement worker load balancing $end$ If the work-load and life-time of different sessions differ significantly, it may lead to a situation where a particular worker continuously has a higher load than all other workers.

MaxScale should be capable of moving a session from one worker to another, so that the load of all workers can be kept roughly the same.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,11,,1,0,1,3,0,0,0,,0,850,0,0,0,2019-10-28 08:45:05,Implement worker load balancing,"If the work-load and life-time of different sessions differ significantly, it may lead to a situation where a particular worker continuously has a higher load than all other workers.

MaxScale should be capable of moving a session from one worker to another, so that the load of all workers can be kept roughly the same.
",,0,0,0,0,0.0,"Implement worker load balancing $end$ If the work-load and life-time of different sessions differ significantly, it may lead to a situation where a particular worker continuously has a higher load than all other workers.

MaxScale should be capable of moving a session from one worker to another, so that the load of all workers can be kept roughly the same.
 $acceptance criteria:$",0,0,0,0,0,0,1,141.283,354,35,0.0988701,14,0.039548,9,0.0254237,7,0.019774,7,0.019774
1359,MXS-2736,Task,MXS,2019-10-28 08:47:37,MXS-2658,0,Pinloki Requirements,,,Pinloki Requirements $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,33,,0,0,0,14,0,0,0,,0,850,0,0,0,2019-10-28 08:47:37,Pinloki Requirements,,,0,0,0,0,0.0,Pinloki Requirements $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,355,35,0.0985916,14,0.0394366,9,0.0253521,7,0.0197183,7,0.0197183
1360,MXS-2737,Task,MXS,2019-10-28 11:23:41,,0,setup max-tst-04 and test tests with host in Google Cloud,,,setup max-tst-04 and test tests with host in Google Cloud $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,16,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-10-28 11:36:30,setup max-tst-04 and test tests with host in Google Cloud,,,0,0,0,0,0.0,setup max-tst-04 and test tests with host in Google Cloud $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.2,81,1,0.0123457,0,0.0,0,0.0,0,0.0,0,0.0
1361,MXS-2739,Task,MXS,2019-10-28 11:26:58,,0,"run performance test for latest releases of 2.3, 2.2, 2.4",(Nokia request),,"run performance test for latest releases of 2.3, 2.2, 2.4 $end$ (Nokia request) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-10-28 11:36:12,"run performance test for latest releases of 2.3, 2.2, 2.4",(Nokia request),,0,0,0,0,0.0,"run performance test for latest releases of 2.3, 2.2, 2.4 $end$ (Nokia request) $acceptance criteria:$",0,0,0,0,0,0,0,0.15,82,1,0.0121951,0,0.0,0,0.0,0,0.0,0,0.0
1362,MXS-2740,Task,MXS,2019-10-28 11:28:59,,0,New adaptive routing algorithm,,,New adaptive routing algorithm $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Minor,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-10-28 11:30:12,New adaptive routing algorithm,,,0,0,0,0,0.0,New adaptive routing algorithm $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,12,1,0.0833333,0,0.0,0,0.0,0,0.0,0,0.0
1363,MXS-2741,New Feature,MXS,2019-10-28 11:33:43,,0,New query statistics,remove statistics related to old adaptive algorithm,,New query statistics $end$ remove statistics related to old adaptive algorithm $acceptance criteria:$,,Niclas Antti,Niclas Antti,Minor,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-10-28 11:34:06,New query statistics,remove statistics related to old adaptive algorithm,,0,0,0,0,0.0,New query statistics $end$ remove statistics related to old adaptive algorithm $acceptance criteria:$,0,0,0,0,0,0,1,0.0,13,1,0.0769231,0,0.0,0,0.0,0,0.0,0,0.0
1364,MXS-2748,New Feature,MXS,2019-10-29 21:29:06,,0,Show how many commands were executed by a session when using Persistent Connections,"Folks,

For the sake of better tuning the {{max_sescmd_history}} and avoid it to be too large (wasting of memory) or even avoiding it to be too low, leading to inconsistencies, can we show up out of the {{maxctrl show sessions}} command how many session commands a session is running? The outcome here is to make sure we can have an average number + 10 to have a good measure for tunning the {{max_sescmd_history}} (also using the {{prune_sescmd_history}} to cap on the session's memory consumption).

Currently, we have this as a return for the {{maxctrl show sessions}}:


{code:java}
┌────────────────┬───────────────────────────┐
│ Id             │ 5900882                   │
├────────────────┼───────────────────────────┤
│ Service        │ Galera-Service            │
├────────────────┼───────────────────────────┤
│ State          │ Session ready for routing │
├────────────────┼───────────────────────────┤
│ User           │ demo02                    │
├────────────────┼───────────────────────────┤
│ Host           │ ::ffff:10.4.0.32          │
├────────────────┼───────────────────────────┤
│ Connected      │ Mon Oct 28 02:33:05 2019  │
├────────────────┼───────────────────────────┤
│ Idle           │ 13.9                      │
├────────────────┼───────────────────────────┤
│ Connections    │ ms-db01                   │
│                │ ms-db03                   │
│                │ ms-db02                   │
├────────────────┼───────────────────────────┤
│ Connection IDs │ 11253918                  │
│                │ 10478447                  │
│                │ 10501213                  │
├────────────────┼───────────────────────────┤
│ Queries        │                           │
├────────────────┼───────────────────────────┤
│ Log            │                           │
└────────────────┴───────────────────────────┘
{code}

Maybe we can include the number of commands traffic'ed somewhere as we already have the Connection IDs being listed.

What do you think? Thanks!",,"Show how many commands were executed by a session when using Persistent Connections $end$ Folks,

For the sake of better tuning the {{max_sescmd_history}} and avoid it to be too large (wasting of memory) or even avoiding it to be too low, leading to inconsistencies, can we show up out of the {{maxctrl show sessions}} command how many session commands a session is running? The outcome here is to make sure we can have an average number + 10 to have a good measure for tunning the {{max_sescmd_history}} (also using the {{prune_sescmd_history}} to cap on the session's memory consumption).

Currently, we have this as a return for the {{maxctrl show sessions}}:


{code:java}
┌────────────────┬───────────────────────────┐
│ Id             │ 5900882                   │
├────────────────┼───────────────────────────┤
│ Service        │ Galera-Service            │
├────────────────┼───────────────────────────┤
│ State          │ Session ready for routing │
├────────────────┼───────────────────────────┤
│ User           │ demo02                    │
├────────────────┼───────────────────────────┤
│ Host           │ ::ffff:10.4.0.32          │
├────────────────┼───────────────────────────┤
│ Connected      │ Mon Oct 28 02:33:05 2019  │
├────────────────┼───────────────────────────┤
│ Idle           │ 13.9                      │
├────────────────┼───────────────────────────┤
│ Connections    │ ms-db01                   │
│                │ ms-db03                   │
│                │ ms-db02                   │
├────────────────┼───────────────────────────┤
│ Connection IDs │ 11253918                  │
│                │ 10478447                  │
│                │ 10501213                  │
├────────────────┼───────────────────────────┤
│ Queries        │                           │
├────────────────┼───────────────────────────┤
│ Log            │                           │
└────────────────┴───────────────────────────┘
{code}

Maybe we can include the number of commands traffic'ed somewhere as we already have the Connection IDs being listed.

What do you think? Thanks! $acceptance criteria:$",,Wagner Bianchi,Wagner Bianchi,Major,19,,0,2,0,5,0,0,0,,0,850,0,0,0,2020-11-09 12:16:59,Show how many commands were executed by a session when using Persistent Connections,"Folks,

For the sake of better tuning the {{max_sescmd_history}} and avoid it to be too large (wasting of memory) or even avoiding it to be too low, leading to inconsistencies, can we show up out of the {{maxctrl show sessions}} command how many session commands a session is running? The outcome here is to make sure we can have an average number + 10 to have a good measure for tunning the {{max_sescmd_history}} (also using the {{prune_sescmd_history}} to cap on the session's memory consumption).

Currently, we have this as a return for the {{maxctrl show sessions}}:


{code:java}
┌────────────────┬───────────────────────────┐
│ Id             │ 5900882                   │
├────────────────┼───────────────────────────┤
│ Service        │ Galera-Service            │
├────────────────┼───────────────────────────┤
│ State          │ Session ready for routing │
├────────────────┼───────────────────────────┤
│ User           │ demo02                    │
├────────────────┼───────────────────────────┤
│ Host           │ ::ffff:10.4.0.32          │
├────────────────┼───────────────────────────┤
│ Connected      │ Mon Oct 28 02:33:05 2019  │
├────────────────┼───────────────────────────┤
│ Idle           │ 13.9                      │
├────────────────┼───────────────────────────┤
│ Connections    │ ms-db01                   │
│                │ ms-db03                   │
│                │ ms-db02                   │
├────────────────┼───────────────────────────┤
│ Connection IDs │ 11253918                  │
│                │ 10478447                  │
│                │ 10501213                  │
├────────────────┼───────────────────────────┤
│ Queries        │                           │
├────────────────┼───────────────────────────┤
│ Log            │                           │
└────────────────┴───────────────────────────┘
{code}

Maybe we can include the number of commands traffic'ed somewhere as we already have the Connection IDs being listed.

What do you think? Thanks!",,0,0,0,0,0.0,"Show how many commands were executed by a session when using Persistent Connections $end$ Folks,

For the sake of better tuning the {{max_sescmd_history}} and avoid it to be too large (wasting of memory) or even avoiding it to be too low, leading to inconsistencies, can we show up out of the {{maxctrl show sessions}} command how many session commands a session is running? The outcome here is to make sure we can have an average number + 10 to have a good measure for tunning the {{max_sescmd_history}} (also using the {{prune_sescmd_history}} to cap on the session's memory consumption).

Currently, we have this as a return for the {{maxctrl show sessions}}:


{code:java}
┌────────────────┬───────────────────────────┐
│ Id             │ 5900882                   │
├────────────────┼───────────────────────────┤
│ Service        │ Galera-Service            │
├────────────────┼───────────────────────────┤
│ State          │ Session ready for routing │
├────────────────┼───────────────────────────┤
│ User           │ demo02                    │
├────────────────┼───────────────────────────┤
│ Host           │ ::ffff:10.4.0.32          │
├────────────────┼───────────────────────────┤
│ Connected      │ Mon Oct 28 02:33:05 2019  │
├────────────────┼───────────────────────────┤
│ Idle           │ 13.9                      │
├────────────────┼───────────────────────────┤
│ Connections    │ ms-db01                   │
│                │ ms-db03                   │
│                │ ms-db02                   │
├────────────────┼───────────────────────────┤
│ Connection IDs │ 11253918                  │
│                │ 10478447                  │
│                │ 10501213                  │
├────────────────┼───────────────────────────┤
│ Queries        │                           │
├────────────────┼───────────────────────────┤
│ Log            │                           │
└────────────────┴───────────────────────────┘
{code}

Maybe we can include the number of commands traffic'ed somewhere as we already have the Connection IDs being listed.

What do you think? Thanks! $acceptance criteria:$",0,0,0,0,0,0,1,9038.78,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1365,MXS-2749,New Feature,MXS,2019-10-31 15:00:05,,0,Allow filter to be applied by database role,"This is a feature request on behalf of a customer. The customer uses MaxScale filter to mask data, but woudl like to be able to specify not just applicable (or exempt) users, but also database roles. 

To my untrained eye this seems possible, since roles are stored in MariaDB server and are applied using a SQL statement which goes through MaxScale - so MaxScale could be (or already is?) aware of the current role that the connected user has assumed (even if it is its default role) and adjust its behaviour accordingly. ",,"Allow filter to be applied by database role $end$ This is a feature request on behalf of a customer. The customer uses MaxScale filter to mask data, but woudl like to be able to specify not just applicable (or exempt) users, but also database roles. 

To my untrained eye this seems possible, since roles are stored in MariaDB server and are applied using a SQL statement which goes through MaxScale - so MaxScale could be (or already is?) aware of the current role that the connected user has assumed (even if it is its default role) and adjust its behaviour accordingly.  $acceptance criteria:$",,Assen Totin,Assen Totin,Major,11,,0,1,0,4,0,0,0,,0,850,1,0,0,2020-09-28 12:16:19,Allow filter to be applied by database role,"This is a feature request on behalf of a customer. The customer uses MaxScale filter to mask data, but woudl like to be able to specify not just applicable (or exempt) users, but also database roles. 

To my untrained eye this seems possible, since roles are stored in MariaDB server and are applied using a SQL statement which goes through MaxScale - so MaxScale could be (or already is?) aware of the current role that the connected user has assumed (even if it is its default role) and adjust its behaviour accordingly. ",,0,0,0,0,0.0,"Allow filter to be applied by database role $end$ This is a feature request on behalf of a customer. The customer uses MaxScale filter to mask data, but woudl like to be able to specify not just applicable (or exempt) users, but also database roles. 

To my untrained eye this seems possible, since roles are stored in MariaDB server and are applied using a SQL statement which goes through MaxScale - so MaxScale could be (or already is?) aware of the current role that the connected user has assumed (even if it is its default role) and adjust its behaviour accordingly.  $acceptance criteria:$",0,0,0,0,0,0,1,7989.27,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1366,MXS-2754,New Feature,MXS,2019-11-07 05:28:20,,0,Add filter alteration,"Filters cannot be altered at run-time. The only way to alter one is to create a copy of it and replace the filter the service uses.

By allowing alterations to filters, the overall process of changing filter configurations is greatly simplified. ",,"Add filter alteration $end$ Filters cannot be altered at run-time. The only way to alter one is to create a copy of it and replace the filter the service uses.

By allowing alterations to filters, the overall process of changing filter configurations is greatly simplified.  $acceptance criteria:$",,markus makela,markus makela,Major,15,,1,2,1,2,0,0,0,,0,850,2,0,0,2020-08-31 12:14:45,Add filter alteration,"Filters cannot be altered at run-time. The only way to alter one is to create a copy of it and replace the filter the service uses.

By allowing alterations to filters, the overall process of changing filter configurations is greatly simplified. ",,0,0,0,0,0.0,"Add filter alteration $end$ Filters cannot be altered at run-time. The only way to alter one is to create a copy of it and replace the filter the service uses.

By allowing alterations to filters, the overall process of changing filter configurations is greatly simplified.  $acceptance criteria:$",0,0,0,0,0,0,1,7158.77,70,12,0.171429,10,0.142857,8,0.114286,8,0.114286,7,0.1
1367,MXS-2758,New Feature,MXS,2019-11-07 16:49:41,MXS-2681,0,Enable the systemd unit after setting up the maxscale package,"Folks,

Is it possible to enable the systemd unit after setting up the maxscale package?

{code:java}
[root@mxs01 ~]# systemctl is-enabled maxscale.service
disabled
[root@mxs01 ~]# systemctl --now enable maxscale.service
Created symlink from /etc/systemd/system/multi-user.target.wants/maxscale.service to /usr/lib/systemd/system/maxscale.service.
{code}

The MariaDB Server has it enabled.

Thanks!",,"Enable the systemd unit after setting up the maxscale package $end$ Folks,

Is it possible to enable the systemd unit after setting up the maxscale package?

{code:java}
[root@mxs01 ~]# systemctl is-enabled maxscale.service
disabled
[root@mxs01 ~]# systemctl --now enable maxscale.service
Created symlink from /etc/systemd/system/multi-user.target.wants/maxscale.service to /usr/lib/systemd/system/maxscale.service.
{code}

The MariaDB Server has it enabled.

Thanks! $acceptance criteria:$",,Wagner Bianchi,Wagner Bianchi,Minor,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-01-07 10:51:37,Enable the systemd unit after setting up the maxscale package,"Folks,

Is it possible to enable the systemd unit after setting up the maxscale package?

{code:java}
[root@mxs01 ~]# systemctl is-enabled maxscale.service
disabled
[root@mxs01 ~]# systemctl --now enable maxscale.service
Created symlink from /etc/systemd/system/multi-user.target.wants/maxscale.service to /usr/lib/systemd/system/maxscale.service.
{code}

The MariaDB Server has it enabled.

Thanks!",,0,0,0,0,0.0,"Enable the systemd unit after setting up the maxscale package $end$ Folks,

Is it possible to enable the systemd unit after setting up the maxscale package?

{code:java}
[root@mxs01 ~]# systemctl is-enabled maxscale.service
disabled
[root@mxs01 ~]# systemctl --now enable maxscale.service
Created symlink from /etc/systemd/system/multi-user.target.wants/maxscale.service to /usr/lib/systemd/system/maxscale.service.
{code}

The MariaDB Server has it enabled.

Thanks! $acceptance criteria:$",0,0,0,0,0,0,0,1458.02,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1368,MXS-2766,Task,MXS,2019-11-12 11:19:38,,0,run system tests using AWS (via Teraform),,,run system tests using AWS (via Teraform) $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,12,,0,0,0,3,0,0,0,,0,850,0,0,0,2019-11-12 11:32:52,run system tests using AWS (via Teraform),,,0,0,0,0,0.0,run system tests using AWS (via Teraform) $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.216667,83,1,0.0120482,0,0.0,0,0.0,0,0.0,0,0.0
1369,MXS-2767,Task,MXS,2019-11-12 11:20:35,,0,Configure BuildBot to report build_all failures to Slack,,,Configure BuildBot to report build_all failures to Slack $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,12,,0,0,0,4,0,0,0,,0,850,0,0,0,2019-11-12 11:32:24,Configure BuildBot to report build_all failures to Slack,,,0,0,0,0,0.0,Configure BuildBot to report build_all failures to Slack $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.183333,84,1,0.0119048,0,0.0,0,0.0,0,0.0,0,0.0
1370,MXS-2768,Task,MXS,2019-11-12 11:22:04,,0,Investigate Teraform and Google cloud interaction,,,Investigate Teraform and Google cloud interaction $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,13,,0,1,0,4,0,0,0,,0,850,1,0,0,2019-11-12 11:32:19,Investigate Teraform and Google cloud interaction,,,0,0,0,0,0.0,Investigate Teraform and Google cloud interaction $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.166667,85,1,0.0117647,0,0.0,0,0.0,0,0.0,0,0.0
1371,MXS-2769,Task,MXS,2019-11-12 11:22:08,,0,Improve speed by not using kernel clock where possible,,,Improve speed by not using kernel clock where possible $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Minor,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-11-12 11:27:22,Improve speed by not using kernel clock where possible,,,0,0,0,0,0.0,Improve speed by not using kernel clock where possible $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0833333,14,1,0.0714286,0,0.0,0,0.0,0,0.0,0,0.0
1372,MXS-2774,Task,MXS,2019-11-19 11:45:25,,0,Take new config mechanism into use for global MaxScale config,,,Take new config mechanism into use for global MaxScale config $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-01-20 10:25:10,Take new config mechanism into use for global MaxScale config,,,0,0,0,0,0.0,Take new config mechanism into use for global MaxScale config $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1486.65,356,35,0.0983146,14,0.0393258,9,0.0252809,7,0.0196629,7,0.0196629
1373,MXS-2781,Task,MXS,2019-11-27 11:23:37,,0,Smart Routing Blog,,,Smart Routing Blog $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,11,,0,0,0,1,0,0,0,,0,850,0,0,0,2019-11-27 11:23:37,Smart Routing Blog,,,0,0,0,0,0.0,Smart Routing Blog $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,357,35,0.0980392,14,0.0392157,9,0.0252101,7,0.0196078,7,0.0196078
1374,MXS-2785,New Feature,MXS,2019-12-02 19:26:12,,0,Allow database renaming in binlogfilter,"Since database renaming can only be done with binlog_format=ROW, being able to do it with MIXED and STATEMENT would make it very helpful.",,"Allow database renaming in binlogfilter $end$ Since database renaming can only be done with binlog_format=ROW, being able to do it with MIXED and STATEMENT would make it very helpful. $acceptance criteria:$",,markus makela,markus makela,Major,7,,0,1,0,1,0,0,0,,0,850,0,0,0,2019-12-09 11:27:51,Allow database renaming in binlogfilter,"Since database renaming can only be done with binlog_format=ROW, being able to do it with MIXED and STATEMENT would make it very helpful.",,0,0,0,0,0.0,"Allow database renaming in binlogfilter $end$ Since database renaming can only be done with binlog_format=ROW, being able to do it with MIXED and STATEMENT would make it very helpful. $acceptance criteria:$",0,0,0,0,0,0,0,160.017,71,12,0.169014,10,0.140845,8,0.112676,8,0.112676,7,0.0985916
1375,MXS-2790,Task,MXS,2019-12-09 11:26:24,,0,Fix maxscale system tests to work with AWS,,,Fix maxscale system tests to work with AWS $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2019-12-09 11:26:24,Fix maxscale system tests to work with AWS,,,0,0,0,0,0.0,Fix maxscale system tests to work with AWS $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,358,35,0.0977654,14,0.0391061,9,0.0251397,7,0.0195531,7,0.0195531
1376,MXS-2804,Task,MXS,2019-12-17 10:57:10,,0,MaxScale RPMs should be signed with key that belongs to @mariadb.com,"MaxScale RPMs are signed by a key belonging to maxscale@googlegroups.com. This may be a problem for the environments with strict rules for key management. Auditors may ask why a supported product is signed by a key that does not belong to the vendor.

It makes sense to renew the key with an @mariadb.com one or release ""Enterprise"" MaxScale RPMs signed by one.",,"MaxScale RPMs should be signed with key that belongs to @mariadb.com $end$ MaxScale RPMs are signed by a key belonging to maxscale@googlegroups.com. This may be a problem for the environments with strict rules for key management. Auditors may ask why a supported product is signed by a key that does not belong to the vendor.

It makes sense to renew the key with an @mariadb.com one or release ""Enterprise"" MaxScale RPMs signed by one. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,23,,0,3,0,5,0,0,2,,0,850,3,0,0,2020-01-07 10:08:14,MaxScale RPMs should be signed with key that belongs to @mariadb.com,"MaxScale RPMs are signed by a key belonging to maxscale@googlegroups.com. This may be a problem for the environments with strict rules for key management. Auditors may ask why a supported product is signed by a key that does not belong to the vendor.

It makes sense to renew the key with an @mariadb.com one or release ""Enterprise"" MaxScale RPMs signed by one.",,0,0,0,0,0.0,"MaxScale RPMs should be signed with key that belongs to @mariadb.com $end$ MaxScale RPMs are signed by a key belonging to maxscale@googlegroups.com. This may be a problem for the environments with strict rules for key management. Auditors may ask why a supported product is signed by a key that does not belong to the vendor.

It makes sense to renew the key with an @mariadb.com one or release ""Enterprise"" MaxScale RPMs signed by one. $acceptance criteria:$",0,0,0,0,0,0,1,503.183,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1377,MXS-2806,New Feature,MXS,2019-12-17 15:42:04,,0,Add ability to stop/start Listener,"In MaxScale we can (remotely) start/stop certain components like services; however, MaxScale does lacks the ability to start/stop a listener. 

The main expected feature here is that all existing client connections (and related server connections) should be terminated - either immediately or after completing the current query; it would be nice to have a config option to chose termination mode.

This feature is useful in all cases when we want to ensure clients connect to the proper servers; a typical one is a service reconfiguration when all existing connections will still point to the old servers, which we want to avoid; currently the only way of doing this is a complete restart of the daemon, which may be undesired when done frequently or when MaxScale is engaged with multiple separate clusters.

It is understood that if the admin interface or REST API listener gets stopped, the corresponding management channel becomes unavailable. 
",,"Add ability to stop/start Listener $end$ In MaxScale we can (remotely) start/stop certain components like services; however, MaxScale does lacks the ability to start/stop a listener. 

The main expected feature here is that all existing client connections (and related server connections) should be terminated - either immediately or after completing the current query; it would be nice to have a config option to chose termination mode.

This feature is useful in all cases when we want to ensure clients connect to the proper servers; a typical one is a service reconfiguration when all existing connections will still point to the old servers, which we want to avoid; currently the only way of doing this is a complete restart of the daemon, which may be undesired when done frequently or when MaxScale is engaged with multiple separate clusters.

It is understood that if the admin interface or REST API listener gets stopped, the corresponding management channel becomes unavailable. 
 $acceptance criteria:$",,Assen Totin,Assen Totin,Major,9,,0,1,0,1,0,0,0,,0,850,0,0,0,2020-09-28 12:08:42,Add ability to stop/start Listener,"In MaxScale we can (remotely) start/stop certain components like services; however, MaxScale does lacks the ability to start/stop a listener. 

The main expected feature here is that all existing client connections (and related server connections) should be terminated - either immediately or after completing the current query; it would be nice to have a config option to chose termination mode.

This feature is useful in all cases when we want to ensure clients connect to the proper servers; a typical one is a service reconfiguration when all existing connections will still point to the old servers, which we want to avoid; currently the only way of doing this is a complete restart of the daemon, which may be undesired when done frequently or when MaxScale is engaged with multiple separate clusters.

It is understood that if the admin interface or REST API listener gets stopped, the corresponding management channel becomes unavailable. 
",,0,0,0,0,0.0,"Add ability to stop/start Listener $end$ In MaxScale we can (remotely) start/stop certain components like services; however, MaxScale does lacks the ability to start/stop a listener. 

The main expected feature here is that all existing client connections (and related server connections) should be terminated - either immediately or after completing the current query; it would be nice to have a config option to chose termination mode.

This feature is useful in all cases when we want to ensure clients connect to the proper servers; a typical one is a service reconfiguration when all existing connections will still point to the old servers, which we want to avoid; currently the only way of doing this is a complete restart of the daemon, which may be undesired when done frequently or when MaxScale is engaged with multiple separate clusters.

It is understood that if the admin interface or REST API listener gets stopped, the corresponding management channel becomes unavailable. 
 $acceptance criteria:$",0,0,0,0,0,0,0,6860.43,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1378,MXS-2807,New Feature,MXS,2019-12-17 15:56:30,,0,Add option for connection termination on `stop service`,"Stopping the maxscale service (e.g., from maxctrl) does not terminate existing connections. This is rather counter-intuitive and confusing (stopping a module should prevent data flow through it completely). While it may be undesirable to change this default behaviour, it will be quite useful to get a config option that will achieve this - terminate existing connection on maxscale stop. 

The same applies when a service is stopped - existing connections still work, which is confusing for a stopped service. Again, having an option to terminate connections when a service is stopped would be quite helpful.",,"Add option for connection termination on `stop service` $end$ Stopping the maxscale service (e.g., from maxctrl) does not terminate existing connections. This is rather counter-intuitive and confusing (stopping a module should prevent data flow through it completely). While it may be undesirable to change this default behaviour, it will be quite useful to get a config option that will achieve this - terminate existing connection on maxscale stop. 

The same applies when a service is stopped - existing connections still work, which is confusing for a stopped service. Again, having an option to terminate connections when a service is stopped would be quite helpful. $acceptance criteria:$",,Assen Totin,Assen Totin,Major,9,,0,1,0,1,0,1,0,,0,850,0,1,0,2020-09-28 12:08:32,Add option for connection termination on `stop service`,"Stopping the maxscale service (e.g., from maxctrl) does not terminate existing connections. This is rather counter-intuitive and confusing (stopping a module should prevent data flow through it completely). While it may be undesirable to change this default behaviour, it will be quite useful to get a config option that will achieve this - terminate existing connection on maxscale stop. 

The same applies when a service is stopped - existing connections still work, which is confusing for a stopped service. Again, having an option to terminate connections when a service is stopped would be quite helpful.",,0,0,0,0,0.0,"Add option for connection termination on `stop service` $end$ Stopping the maxscale service (e.g., from maxctrl) does not terminate existing connections. This is rather counter-intuitive and confusing (stopping a module should prevent data flow through it completely). While it may be undesirable to change this default behaviour, it will be quite useful to get a config option that will achieve this - terminate existing connection on maxscale stop. 

The same applies when a service is stopped - existing connections still work, which is confusing for a stopped service. Again, having an option to terminate connections when a service is stopped would be quite helpful. $acceptance criteria:$",0,0,0,0,0,0,0,6860.2,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1379,MXS-2808,New Feature,MXS,2019-12-17 16:32:09,,0,Configuration sync between MaxScale instances,"h2. Overview
Synchronize configuration changes done on one MaxScale to other MaxScale instances in the group. If a new MaxScale joins the group, will synchronize with the rest of the cluster.

h5. Group Membership
The group of MaxScale instances is implicitly defined by the database cluster they use. This retains the feature of MaxScale instances not being aware of each other which makes provisioning of new nodes much easier.

h5. Data Storage
The configuration information is stored in a table in the database which contains the cluster name (the cluster name from MaxScale configuration), the configuration version number (incremented for each change) and the actual configuration data as JSON. The cluster name is derived from the name of the monitor that monitors the cluster.  The version number starts from 0 and the first dynamic change causes it to be set to 1. This means the first update stored in the database will have the version number 1 and all subsequent configuration updates increment the version.

h5. Updating the Configuration
Before a configuration change is done in MaxScale, the current version is selected from the database inside of a transaction using {{SELECT ... FOR UPDATE}}. If the version is the same as the version stored in MaxScale, the update can proceed. If the version is not the same, an error is returned to the client. Once the modifications to the internal state of MaxScale have been done, an attempt to update the row is made. As we used {{SELECT ... FOR UPDATE}} to read the version, either we succeed in updating the row or we fail due to a deadlock conflict in the transaction. This guarantees that only one MaxScale succeeds in updating the configuration. The MaxScale that fails to perform the update will return an error to the client and read the new configuration from the database.

h5. Locally Cached Configuration
Whenever the configuration changes, it is stored as JSON on disk. If this cached configuration is available, it will be used instead of the static configuration files. This allows the configuration changes to persist through a crash as well as through temporary outages in the cluster. If the transition from the current configuration to the configuration stored in the cluster fails, the cached configuration is discarded. This guarantees that only one attempt is done to start with a cached configuration and all subsequent startups use the static configuration files.

h5. Configuration Synchronization
The propagation of configuration updates is done by periodically polling the database for the latest configuration version. If the version in the database is newer than the one in MaxScale, the diff between the new configuration and the current configuration is applied. If successful, the MaxScale is now in sync with the cluster. If the application of the new configuration from the database fails, an attempt to roll back any partial changes is made. If the rollback is successful, this version of the configuration is ignored and MaxScale will attempt to synchronize again with the next update. If both the application of the configuration and the rollback attempt fail, then the MaxScale configuration is in an indeterminate state. In this case, the MaxScale will discard any cached configurations and restart the process using the static configuration files. This means there are three possible outcomes for an update from configration version 3 to version 4:

* Update is successful -> version 4, configuration from version 4
* Update fails but rollback is successful -> version 4, configuration from version 3
* Both update and rollback fail -> version 0, process is restarted, configuration from static files

The benefit of gracefully rolling back failed changes and ignoring the update is that it allows local failures to be tolerated without causing a severe outage. For example, this can happen if the TLS certificates are updated but the files do not exist on all nodes of the cluster.

If the application of the latest configuration found in the cluster fails when starting with static configuration files, the process is not restarted to prevent an infinite restart loop. At this point it is assumed that it is better to leave MaxScale running with the static configuration than to keep restarting it in the hopes that the configuration is eventually applied successfully. In this case, new configuration updates are attempted similarly to how they are attempted in the case where the rollback is successful.

h5. Propagation of Errors
Whenever a node in the cluster fails to apply the configuration, it stores the error message in a field in the corresponding row. This way any failures to apply the configuration are made visible to other MaxScale instances which can then be displayed in the GUI. This can also be used to signal that a MaxScale instances has read the configuration from the database.

----
Original description:

In certain cases (like telco infra) having 1+1 MaxScale instances (active and standby via keepalived and friends) is deemed insufficient and both instances are required to be running _and_ used by clients. While it is possible to use two MaxScale instances independently, the question of keeping their configuration in sync remains tricky and needs some external state machine like Puppet + a cleverly crafted config files to provide unique values where needed.

This request is to provide a way for sync'ing the config of one MaxScale to another in a more automated way than just export->scp->import. We see several intriguing options:

- Configure active config replication - especially useful when changes are made via the API. Still leaves open the question of how to get config updates in a MaxScale instance that was down for some time and comes up. (The benefit from this is that it may open the door for implementing connection hand-over between MaxScale instances when using traditional failover with ""jumping"" IP address).

- Use external config storage like etcd which is used for similar purpose in complex and proven projects like Redhat OpenShift. Personally, this is my favourite, at least when it comes to configuring replicated instances of a service. 
",,"Configuration sync between MaxScale instances $end$ h2. Overview
Synchronize configuration changes done on one MaxScale to other MaxScale instances in the group. If a new MaxScale joins the group, will synchronize with the rest of the cluster.

h5. Group Membership
The group of MaxScale instances is implicitly defined by the database cluster they use. This retains the feature of MaxScale instances not being aware of each other which makes provisioning of new nodes much easier.

h5. Data Storage
The configuration information is stored in a table in the database which contains the cluster name (the cluster name from MaxScale configuration), the configuration version number (incremented for each change) and the actual configuration data as JSON. The cluster name is derived from the name of the monitor that monitors the cluster.  The version number starts from 0 and the first dynamic change causes it to be set to 1. This means the first update stored in the database will have the version number 1 and all subsequent configuration updates increment the version.

h5. Updating the Configuration
Before a configuration change is done in MaxScale, the current version is selected from the database inside of a transaction using {{SELECT ... FOR UPDATE}}. If the version is the same as the version stored in MaxScale, the update can proceed. If the version is not the same, an error is returned to the client. Once the modifications to the internal state of MaxScale have been done, an attempt to update the row is made. As we used {{SELECT ... FOR UPDATE}} to read the version, either we succeed in updating the row or we fail due to a deadlock conflict in the transaction. This guarantees that only one MaxScale succeeds in updating the configuration. The MaxScale that fails to perform the update will return an error to the client and read the new configuration from the database.

h5. Locally Cached Configuration
Whenever the configuration changes, it is stored as JSON on disk. If this cached configuration is available, it will be used instead of the static configuration files. This allows the configuration changes to persist through a crash as well as through temporary outages in the cluster. If the transition from the current configuration to the configuration stored in the cluster fails, the cached configuration is discarded. This guarantees that only one attempt is done to start with a cached configuration and all subsequent startups use the static configuration files.

h5. Configuration Synchronization
The propagation of configuration updates is done by periodically polling the database for the latest configuration version. If the version in the database is newer than the one in MaxScale, the diff between the new configuration and the current configuration is applied. If successful, the MaxScale is now in sync with the cluster. If the application of the new configuration from the database fails, an attempt to roll back any partial changes is made. If the rollback is successful, this version of the configuration is ignored and MaxScale will attempt to synchronize again with the next update. If both the application of the configuration and the rollback attempt fail, then the MaxScale configuration is in an indeterminate state. In this case, the MaxScale will discard any cached configurations and restart the process using the static configuration files. This means there are three possible outcomes for an update from configration version 3 to version 4:

* Update is successful -> version 4, configuration from version 4
* Update fails but rollback is successful -> version 4, configuration from version 3
* Both update and rollback fail -> version 0, process is restarted, configuration from static files

The benefit of gracefully rolling back failed changes and ignoring the update is that it allows local failures to be tolerated without causing a severe outage. For example, this can happen if the TLS certificates are updated but the files do not exist on all nodes of the cluster.

If the application of the latest configuration found in the cluster fails when starting with static configuration files, the process is not restarted to prevent an infinite restart loop. At this point it is assumed that it is better to leave MaxScale running with the static configuration than to keep restarting it in the hopes that the configuration is eventually applied successfully. In this case, new configuration updates are attempted similarly to how they are attempted in the case where the rollback is successful.

h5. Propagation of Errors
Whenever a node in the cluster fails to apply the configuration, it stores the error message in a field in the corresponding row. This way any failures to apply the configuration are made visible to other MaxScale instances which can then be displayed in the GUI. This can also be used to signal that a MaxScale instances has read the configuration from the database.

----
Original description:

In certain cases (like telco infra) having 1+1 MaxScale instances (active and standby via keepalived and friends) is deemed insufficient and both instances are required to be running _and_ used by clients. While it is possible to use two MaxScale instances independently, the question of keeping their configuration in sync remains tricky and needs some external state machine like Puppet + a cleverly crafted config files to provide unique values where needed.

This request is to provide a way for sync'ing the config of one MaxScale to another in a more automated way than just export->scp->import. We see several intriguing options:

- Configure active config replication - especially useful when changes are made via the API. Still leaves open the question of how to get config updates in a MaxScale instance that was down for some time and comes up. (The benefit from this is that it may open the door for implementing connection hand-over between MaxScale instances when using traditional failover with ""jumping"" IP address).

- Use external config storage like etcd which is used for similar purpose in complex and proven projects like Redhat OpenShift. Personally, this is my favourite, at least when it comes to configuring replicated instances of a service. 
 $acceptance criteria:$",,Assen Totin,Assen Totin,Major,15,,0,4,3,1,0,5,0,,0,850,3,0,0,2021-05-24 10:50:26,Configuration sync between MaxScale instances (or common config storage),"In certain cases (like telco infra) having 1+1 MaxScale instances (active and standby via keepalived and friends) is deemed insufficient and both instances are required to be running _and_ used by clients. While it is possible to use two MaxScale instances independently, the question of keeping their configuration in sync remains tricky and needs some external state machine like Puppet + a cleverly crafted config files to provide unique values where needed.

This request is to provide a way for sync'ing the config of one MaxScale to another in a more automated way than just export->scp->import. We see several intriguing options:

- Configure active config replication - especially useful when changes are made via the API. Still leaves open the question of how to get config updates in a MaxScale instance that was down for some time and comes up. (The benefit from this is that it may open the door for implementing connection hand-over between MaxScale instances when using traditional failover with ""jumping"" IP address).

- Use external config storage like etcd which is used for similar purpose in complex and proven projects like Redhat OpenShift. Personally, this is my favourite, at least when it comes to configuring replicated instances of a service. 
",,1,4,0,806,3.71759,"Configuration sync between MaxScale instances (or common config storage) $end$ In certain cases (like telco infra) having 1+1 MaxScale instances (active and standby via keepalived and friends) is deemed insufficient and both instances are required to be running _and_ used by clients. While it is possible to use two MaxScale instances independently, the question of keeping their configuration in sync remains tricky and needs some external state machine like Puppet + a cleverly crafted config files to provide unique values where needed.

This request is to provide a way for sync'ing the config of one MaxScale to another in a more automated way than just export->scp->import. We see several intriguing options:

- Configure active config replication - especially useful when changes are made via the API. Still leaves open the question of how to get config updates in a MaxScale instance that was down for some time and comes up. (The benefit from this is that it may open the door for implementing connection hand-over between MaxScale instances when using traditional failover with ""jumping"" IP address).

- Use external config storage like etcd which is used for similar purpose in complex and proven projects like Redhat OpenShift. Personally, this is my favourite, at least when it comes to configuring replicated instances of a service. 
 $acceptance criteria:$",5,1,1,1,1,1,1,12570.3,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1380,MXS-2817,Task,MXS,2019-12-30 09:48:40,,0,Add thread pool to MaxScale,,,Add thread pool to MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-01-07 10:08:26,Add thread pool to MaxScale,,,0,0,0,0,0.0,Add thread pool to MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,192.317,359,35,0.097493,14,0.0389972,9,0.0250696,7,0.0194986,7,0.0194986
1381,MXS-2819,New Feature,MXS,2019-12-31 09:47:11,MXS-1133,0,redis storage for cache,,,redis storage for cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-01-07 06:47:57,redis storage for cache,,,0,0,0,0,0.0,redis storage for cache $end$ $acceptance criteria:$,0,0,0,0,0,0,0,165.0,360,35,0.0972222,14,0.0388889,9,0.025,7,0.0194444,7,0.0194444
1382,MXS-2826,Task,MXS,2020-01-07 10:41:51,,0,Rewrite avrorouter SQL parser,,,Rewrite avrorouter SQL parser $end$ $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-01-07 10:41:51,Rewrite avrorouter SQL parser,,,0,0,0,0,0.0,Rewrite avrorouter SQL parser $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,72,12,0.166667,10,0.138889,8,0.111111,8,0.111111,7,0.0972222
1383,MXS-2827,Task,MXS,2020-01-07 10:43:18,,0,Add user account refreshing to MariaDB-protocol,Should also do some general authenticator cleanup.,,Add user account refreshing to MariaDB-protocol $end$ Should also do some general authenticator cleanup. $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-01-07 11:01:02,Add user account refreshing to MariaDB-protocol,Should also do some general authenticator cleanup.,,0,0,0,0,0.0,Add user account refreshing to MariaDB-protocol $end$ Should also do some general authenticator cleanup. $acceptance criteria:$,0,0,0,0,0,0,0,0.283333,13,2,0.153846,1,0.0769231,0,0.0,0,0.0,0,0.0
1384,MXS-2830,Task,MXS,2020-01-08 13:25:26,,0,Fix com_change_user-handling,"Due to the authentication changes, user changing doesn't work. The handling of the com_change_user should be separated from normal query handling, as it may involve multiple authentication messages with client and backends. For a first version, just having it work with mysql_native_auth is enough.",,"Fix com_change_user-handling $end$ Due to the authentication changes, user changing doesn't work. The handling of the com_change_user should be separated from normal query handling, as it may involve multiple authentication messages with client and backends. For a first version, just having it work with mysql_native_auth is enough. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,11,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-01-08 13:27:20,Fix com_change_user-handling,"Due to the authentication changes, user changing doesn't work. The handling of the com_change_user should be separated from normal query handling, as it may involve multiple authentication messages with client and backends. For a first version, just having it work with mysql_native_auth is enough.",,0,0,0,0,0.0,"Fix com_change_user-handling $end$ Due to the authentication changes, user changing doesn't work. The handling of the com_change_user should be separated from normal query handling, as it may involve multiple authentication messages with client and backends. For a first version, just having it work with mysql_native_auth is enough. $acceptance criteria:$",0,0,0,0,0,0,1,0.0166667,14,2,0.142857,1,0.0714286,0,0.0,0,0.0,0,0.0
1385,MXS-2835,Task,MXS,2020-01-14 10:43:29,,0,Add support for soft TTL to memcached cache storage,,,Add support for soft TTL to memcached cache storage $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-02 08:27:00,Add support for soft TTL to memcached cache storage,,,0,0,0,0,0.0,Add support for soft TTL to memcached cache storage $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1149.72,361,35,0.0969529,14,0.0387812,9,0.0249307,7,0.0193906,7,0.0193906
1386,MXS-2838,New Feature,MXS,2020-01-16 10:05:17,,0,Add prepared statement support for hintfilter,"Currently prepared statements are not supported in the hintfilter. To support them, the filter will have to track the prepared statement ID and add a routing hint to the COM_STMT_EXECUTE.",,"Add prepared statement support for hintfilter $end$ Currently prepared statements are not supported in the hintfilter. To support them, the filter will have to track the prepared statement ID and add a routing hint to the COM_STMT_EXECUTE. $acceptance criteria:$",,markus makela,markus makela,Minor,17,,1,1,1,2,0,0,0,,0,850,0,0,0,2021-02-15 11:30:57,Add prepared statement support for hintfilter,"Currently prepared statements are not supported in the hintfilter. To support them, the filter will have to track the prepared statement ID and add a routing hint to the COM_STMT_EXECUTE.",,0,0,0,0,0.0,"Add prepared statement support for hintfilter $end$ Currently prepared statements are not supported in the hintfilter. To support them, the filter will have to track the prepared statement ID and add a routing hint to the COM_STMT_EXECUTE. $acceptance criteria:$",0,0,0,0,0,0,1,9505.42,73,12,0.164384,10,0.136986,8,0.109589,8,0.109589,7,0.0958904
1387,MXS-2841,Task,MXS,2020-01-20 10:27:15,,0,Create tests for shared cache,,,Create tests for shared cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-01-20 10:27:15,Create tests for shared cache,,,0,0,0,0,0.0,Create tests for shared cache $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,362,35,0.0966851,14,0.038674,9,0.0248619,7,0.019337,7,0.019337
1388,MXS-2842,Task,MXS,2020-01-20 11:23:02,,0,Benchmark shared cache,,,Benchmark shared cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,1,0,5,0,0,0,,0,850,1,0,0,2020-01-20 11:23:02,Benchmark shared cache,,,0,0,0,0,0.0,Benchmark shared cache $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,363,35,0.0964187,14,0.0385675,9,0.0247934,7,0.0192837,7,0.0192837
1389,MXS-2856,Task,MXS,2020-01-28 19:13:19,,0,Create regular expression for database renaming ,Create a more robust regular expression for database renaming with binlogfilter.,,Create regular expression for database renaming  $end$ Create a more robust regular expression for database renaming with binlogfilter. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-02-03 11:11:01,Create regular expression for database renaming ,Create a more robust regular expression for database renaming with binlogfilter.,,0,0,0,0,0.0,Create regular expression for database renaming  $end$ Create a more robust regular expression for database renaming with binlogfilter. $acceptance criteria:$,0,0,0,0,0,0,0,135.95,74,12,0.162162,10,0.135135,8,0.108108,8,0.108108,7,0.0945946
1390,MXS-2866,Task,MXS,2020-02-03 08:57:41,,0,Build MaxScale from source.,"* Follow the instructions at https://github.com/mariadb-corporation/MaxScale/blob/2.4/Documentation/Getting-Started/Building-MaxScale-from-Source-Code.md and build MaxScale from source.
* Create a simple configuration and check that you can access a MariaDB database through MaxScale.",,"Build MaxScale from source. $end$ * Follow the instructions at https://github.com/mariadb-corporation/MaxScale/blob/2.4/Documentation/Getting-Started/Building-MaxScale-from-Source-Code.md and build MaxScale from source.
* Create a simple configuration and check that you can access a MariaDB database through MaxScale. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-02-03 08:57:41,Build MaxScale from source.,"* Follow the instructions at https://github.com/mariadb-corporation/MaxScale/blob/2.4/Documentation/Getting-Started/Building-MaxScale-from-Source-Code.md and build MaxScale from source.
* Create a simple configuration and check that you can access a MariaDB database through MaxScale.",,0,0,0,0,0.0,"Build MaxScale from source. $end$ * Follow the instructions at https://github.com/mariadb-corporation/MaxScale/blob/2.4/Documentation/Getting-Started/Building-MaxScale-from-Source-Code.md and build MaxScale from source.
* Create a simple configuration and check that you can access a MariaDB database through MaxScale. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,364,35,0.0961538,14,0.0384615,9,0.0247253,7,0.0192308,7,0.0192308
1391,MXS-2867,Task,MXS,2020-02-03 09:01:34,,0,Familiarize yourself with Vue.js,"The MaxScale GUI will be created using Vue.js.
Install, explore and e.g. recreate the assignment using that framework.",,"Familiarize yourself with Vue.js $end$ The MaxScale GUI will be created using Vue.js.
Install, explore and e.g. recreate the assignment using that framework. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-02-03 09:01:34,Familiarize yourself with Vue.js,"The MaxScale GUI will be created using Vue.js.
Install, explore and e.g. recreate the assignment using that framework.",,0,0,0,0,0.0,"Familiarize yourself with Vue.js $end$ The MaxScale GUI will be created using Vue.js.
Install, explore and e.g. recreate the assignment using that framework. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,365,35,0.0958904,14,0.0383562,9,0.0246575,7,0.0191781,7,0.0191781
1392,MXS-2869,Task,MXS,2020-02-03 10:56:09,,0,Install and try out the Monitoring Application ,https://github.com/mariadb-corporation/stardust/tree/develop/client,,Install and try out the Monitoring Application  $end$ https://github.com/mariadb-corporation/stardust/tree/develop/client $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-02-03 10:56:09,Install and try out the Monitoring Application ,https://github.com/mariadb-corporation/stardust/tree/develop/client,,0,0,0,0,0.0,Install and try out the Monitoring Application  $end$ https://github.com/mariadb-corporation/stardust/tree/develop/client $acceptance criteria:$,0,0,0,0,0,0,1,0.0,366,35,0.0956284,14,0.0382514,9,0.0245902,7,0.0191257,7,0.0191257
1393,MXS-287,New Feature,MXS,2015-07-27 16:28:49,,0,Access databases from command line failed when grants set with wildcarded hosts,"We have used the wildcard capability with grants in our current MySQL setup, using grants such as 


{code:java}
grant all on `%user%`.* to 'user'@'%' identified by 'somepassword';
{code}


If we try this with the command line client pointed at a maxscale host, specifying a database gives us an 'access denied' failure.


{code:java}
mysql -h maxscale_host -u user -p user_db
{code}


whereas connecting to one of the Galera nodes directly works

{code:java}
mysql -h galera_host -u user -p user_db
{code}

If we explicitly set the grant for a specific database, then the client works
{code:java}
grant all on `user_db`.* to 'user'@'%' identified by 'somepassword';
{code}

In all cases, connecting without specifying the database and then choosing the database with a use statement works.",,"Access databases from command line failed when grants set with wildcarded hosts $end$ We have used the wildcard capability with grants in our current MySQL setup, using grants such as 


{code:java}
grant all on `%user%`.* to 'user'@'%' identified by 'somepassword';
{code}


If we try this with the command line client pointed at a maxscale host, specifying a database gives us an 'access denied' failure.


{code:java}
mysql -h maxscale_host -u user -p user_db
{code}


whereas connecting to one of the Galera nodes directly works

{code:java}
mysql -h galera_host -u user -p user_db
{code}

If we explicitly set the grant for a specific database, then the client works
{code:java}
grant all on `user_db`.* to 'user'@'%' identified by 'somepassword';
{code}

In all cases, connecting without specifying the database and then choosing the database with a use statement works. $acceptance criteria:$",,Simon Hanmer,Simon Hanmer,Major,13,,0,12,2,1,0,0,0,,0,850,0,0,0,2017-01-04 10:52:38,Access databases from command line failed when grants set with wildcarded hosts,"We have used the wildcard capability with grants in our current MySQL setup, using grants such as 


{code:java}
grant all on `%user%`.* to 'user'@'%' identified by 'somepassword';
{code}


If we try this with the command line client pointed at a maxscale host, specifying a database gives us an 'access denied' failure.


{code:java}
mysql -h maxscale_host -u user -p user_db
{code}


whereas connecting to one of the Galera nodes directly works

{code:java}
mysql -h galera_host -u user -p user_db
{code}

If we explicitly set the grant for a specific database, then the client works
{code:java}
grant all on `user_db`.* to 'user'@'%' identified by 'somepassword';
{code}

In all cases, connecting without specifying the database and then choosing the database with a use statement works.",,0,0,0,0,0.0,"Access databases from command line failed when grants set with wildcarded hosts $end$ We have used the wildcard capability with grants in our current MySQL setup, using grants such as 


{code:java}
grant all on `%user%`.* to 'user'@'%' identified by 'somepassword';
{code}


If we try this with the command line client pointed at a maxscale host, specifying a database gives us an 'access denied' failure.


{code:java}
mysql -h maxscale_host -u user -p user_db
{code}


whereas connecting to one of the Galera nodes directly works

{code:java}
mysql -h galera_host -u user -p user_db
{code}

If we explicitly set the grant for a specific database, then the client works
{code:java}
grant all on `user_db`.* to 'user'@'%' identified by 'somepassword';
{code}

In all cases, connecting without specifying the database and then choosing the database with a use statement works. $acceptance criteria:$",0,0,0,0,0,0,0,12642.4,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1394,MXS-2870,Task,MXS,2020-02-03 11:08:17,,0,Create tests for kafkacdc,,,Create tests for kafkacdc $end$ $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-02-03 11:08:17,Create tests for kafkacdc,,,0,0,0,0,0.0,Create tests for kafkacdc $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,75,12,0.16,10,0.133333,8,0.106667,8,0.106667,7,0.0933333
1395,MXS-2875,Task,MXS,2020-02-05 10:10:12,,0,execute maxscale-system-tests also in the VM,to make testing environment scaleable we need to use GCloud and executed tests themselves on the cloud VM,,execute maxscale-system-tests also in the VM $end$ to make testing environment scaleable we need to use GCloud and executed tests themselves on the cloud VM $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,10,,0,1,0,2,0,0,0,,0,850,1,0,0,2020-02-05 10:10:31,execute maxscale-system-tests also in the VM,to make testing environment scaleable we need to use GCloud and executed tests themselves on the cloud VM,,0,0,0,0,0.0,execute maxscale-system-tests also in the VM $end$ to make testing environment scaleable we need to use GCloud and executed tests themselves on the cloud VM $acceptance criteria:$,0,0,0,0,0,0,1,0.0,86,1,0.0116279,0,0.0,0,0.0,0,0.0,0,0.0
1396,MXS-2882,New Feature,MXS,2020-02-07 07:04:17,,0,Add server details in qlafilter where query is routed  ,"Kindly add server details in qlafilter to know where each query is routed 
(i.e on which server? master/slave/galera node etc)   . 
https://mariadb.com/kb/en/mariadb-maxscale-24-query-log-all-filter/

",,"Add server details in qlafilter where query is routed   $end$ Kindly add server details in qlafilter to know where each query is routed 
(i.e on which server? master/slave/galera node etc)   . 
https://mariadb.com/kb/en/mariadb-maxscale-24-query-log-all-filter/

 $acceptance criteria:$",,Nilnandan Joshi,Nilnandan Joshi,Major,13,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-02-14 10:40:17,Add server details in qlafilter where query is routed  ,"Kindly add server details in qlafilter to know where each query is routed 
(i.e on which server? master/slave/galera node etc)   . 
https://mariadb.com/kb/en/mariadb-maxscale-24-query-log-all-filter/

",,0,0,0,0,0.0,"Add server details in qlafilter where query is routed   $end$ Kindly add server details in qlafilter to know where each query is routed 
(i.e on which server? master/slave/galera node etc)   . 
https://mariadb.com/kb/en/mariadb-maxscale-24-query-log-all-filter/

 $acceptance criteria:$",0,0,0,0,0,0,0,17715.6,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1397,MXS-2897,New Feature,MXS,2020-02-24 10:28:25,,0,Add Json Web Token support to REST API,,,Add Json Web Token support to REST API $end$ $acceptance criteria:$,,markus makela,markus makela,Major,10,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-02-24 10:30:22,Add Json Web Token support to REST API,,,0,0,0,0,0.0,Add Json Web Token support to REST API $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0166667,76,12,0.157895,10,0.131579,8,0.105263,8,0.105263,7,0.0921053
1398,MXS-2902,Task,MXS,2020-02-28 11:43:37,,0,auth_all_servers not yet implemented ,"Currently this causes the following tests in 2.5 to fail:
* sharding
* mxs431
",,"auth_all_servers not yet implemented  $end$ Currently this causes the following tests in 2.5 to fail:
* sharding
* mxs431
 $acceptance criteria:$",,markus makela,markus makela,Major,10,,0,0,0,2,0,1,0,,0,850,0,0,0,2020-02-28 11:43:37,auth_all_users not yet implemented ,"Currently this causes the following tests in 2.5 to fail:
* sharding
* mxs431
",,1,0,0,2,0.047619,"auth_all_users not yet implemented  $end$ Currently this causes the following tests in 2.5 to fail:
* sharding
* mxs431
 $acceptance criteria:$",1,1,0,0,0,0,1,0.0,77,12,0.155844,10,0.12987,8,0.103896,8,0.103896,7,0.0909091
1399,MXS-2904,New Feature,MXS,2020-03-02 08:26:22,,0,Document MaxScale performance tuning,Document best practices on how to tune MaxScale and the OS. ,,Document MaxScale performance tuning $end$ Document best practices on how to tune MaxScale and the OS.  $acceptance criteria:$,,markus makela,markus makela,Minor,14,,1,1,1,1,0,0,0,,0,850,1,0,0,2022-04-25 10:33:47,Document MaxScale performance tuning,Document best practices on how to tune MaxScale and the OS. ,,0,0,0,0,0.0,Document MaxScale performance tuning $end$ Document best practices on how to tune MaxScale and the OS.  $acceptance criteria:$,0,0,0,0,0,0,0,18818.1,78,13,0.166667,10,0.128205,8,0.102564,8,0.102564,7,0.0897436
1400,MXS-2905,Task,MXS,2020-03-02 11:05:05,,0,Pinloki prototype,,,Pinloki prototype $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Major,7,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-03-02 11:24:11,Pinloki prototype,,,0,0,0,0,0.0,Pinloki prototype $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.316667,15,1,0.0666667,0,0.0,0,0.0,0,0.0,0,0.0
1401,MXS-2906,Task,MXS,2020-03-02 11:25:40,,0,Prototype MaxGui using Vue.,,,Prototype MaxGui using Vue. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-03-02 11:25:40,Prototype MaxGui using Vue.,,,0,0,0,0,0.0,Prototype MaxGui using Vue. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,367,35,0.0953678,14,0.0381471,9,0.0245232,7,0.0190736,7,0.0190736
1402,MXS-2909,Task,MXS,2020-03-03 08:25:00,MXS-2533,0,Link monitors to services via REST API,It is currently not possible to link monitors to services via the REST API. This should be doable only when the service has no servers or other monitors it uses.,,Link monitors to services via REST API $end$ It is currently not possible to link monitors to services via the REST API. This should be doable only when the service has no servers or other monitors it uses. $acceptance criteria:$,,markus makela,markus makela,Major,11,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-16 11:04:42,Link monitors to services via REST API,It is currently not possible to link monitors to services via the REST API. This should be doable only when the service has no servers or other monitors it uses.,,0,0,0,0,0.0,Link monitors to services via REST API $end$ It is currently not possible to link monitors to services via the REST API. This should be doable only when the service has no servers or other monitors it uses. $acceptance criteria:$,0,0,0,0,0,0,0,314.65,79,13,0.164557,10,0.126582,8,0.101266,8,0.101266,7,0.0886076
1403,MXS-2911,Task,MXS,2020-03-03 14:36:43,,0,Document how REST API relationships should be modified,Currently the documentation doesn't explicitly explain how relationships are to be modified.,,Document how REST API relationships should be modified $end$ Currently the documentation doesn't explicitly explain how relationships are to be modified. $acceptance criteria:$,,markus makela,markus makela,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-04-14 10:11:03,Document how REST API relationships should be modified,Currently the documentation doesn't explicitly explain how relationships are to be modified.,,0,0,0,0,0.0,Document how REST API relationships should be modified $end$ Currently the documentation doesn't explicitly explain how relationships are to be modified. $acceptance criteria:$,0,0,0,0,0,0,0,1003.57,80,13,0.1625,10,0.125,8,0.1,8,0.1,7,0.0875
1404,MXS-2912,Task,MXS,2020-03-03 18:57:16,MXS-2953,0,Take ownership of all MaxScale Docker images,"The MaxScale team will take ownership of:

https://github.com/mariadb-corporation/maxscale-docker

and

https://hub.docker.com/repository/docker/mariadb/maxscale

It has been decided by company executives that the product teams will own their respective Docker images. There should be no updates to these images without MaxScale product management approval.",,"Take ownership of all MaxScale Docker images $end$ The MaxScale team will take ownership of:

https://github.com/mariadb-corporation/maxscale-docker

and

https://hub.docker.com/repository/docker/mariadb/maxscale

It has been decided by company executives that the product teams will own their respective Docker images. There should be no updates to these images without MaxScale product management approval. $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,30,,0,4,0,11,0,0,0,,0,850,4,0,0,2020-03-16 11:31:33,Take ownership of all MaxScale Docker images,"The MaxScale team will take ownership of:

https://github.com/mariadb-corporation/maxscale-docker

and

https://hub.docker.com/repository/docker/mariadb/maxscale

It has been decided by company executives that the product teams will own their respective Docker images. There should be no updates to these images without MaxScale product management approval.",,0,0,0,0,0.0,"Take ownership of all MaxScale Docker images $end$ The MaxScale team will take ownership of:

https://github.com/mariadb-corporation/maxscale-docker

and

https://hub.docker.com/repository/docker/mariadb/maxscale

It has been decided by company executives that the product teams will own their respective Docker images. There should be no updates to these images without MaxScale product management approval. $acceptance criteria:$",0,0,0,0,0,0,1,304.567,6,3,0.5,3,0.5,2,0.333333,2,0.333333,1,0.166667
1405,MXS-2913,Task,MXS,2020-03-04 11:41:40,,0,Take new configuration system into use in all modules.,,,Take new configuration system into use in all modules. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,18,,0,1,0,8,0,0,0,,0,850,1,0,0,2020-03-04 11:41:42,Take new configuration system into use in all modules.,,,0,0,0,0,0.0,Take new configuration system into use in all modules. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,368,35,0.0951087,14,0.0380435,9,0.0244565,7,0.0190217,7,0.0190217
1406,MXS-2916,Task,MXS,2020-03-05 19:38:40,,0,Implement log_password_mismatch for 2.5,The authenticator option added for MXS-2891 is not currently implemented in 2.5.,,Implement log_password_mismatch for 2.5 $end$ The authenticator option added for MXS-2891 is not currently implemented in 2.5. $acceptance criteria:$,,markus makela,markus makela,Major,11,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-04-14 17:03:00,Implement log_password_mismatch for 2.5,The authenticator option added for MXS-2891 is not currently implemented in 2.5.,,0,0,0,0,0.0,Implement log_password_mismatch for 2.5 $end$ The authenticator option added for MXS-2891 is not currently implemented in 2.5. $acceptance criteria:$,0,0,0,0,0,0,1,957.4,81,13,0.160494,10,0.123457,8,0.0987654,8,0.0987654,7,0.0864198
1407,MXS-2922,Task,MXS,2020-03-13 10:18:10,MXS-2557,0,Expose Columnstore management operations from Columnstore monitor,,,Expose Columnstore management operations from Columnstore monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-03-13 10:22:26,Expose Columnstore management operations from Columnstore monitor,,,0,0,0,0,0.0,Expose Columnstore management operations from Columnstore monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0666667,369,35,0.0948509,14,0.0379404,9,0.0243902,7,0.0189702,7,0.0189702
1408,MXS-2923,New Feature,MXS,2020-03-16 07:34:11,MXS-2533,0,Implement Sparse Fieldsets,Implementing [sparse fieldsets|https://jsonapi.org/format/#fetching-sparse-fieldsets] would reduce the overhead of doing REST API requests.,,Implement Sparse Fieldsets $end$ Implementing [sparse fieldsets|https://jsonapi.org/format/#fetching-sparse-fieldsets] would reduce the overhead of doing REST API requests. $acceptance criteria:$,,markus makela,markus makela,Major,14,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-16 09:44:52,Implement Sparse Fieldsets,Implementing [sparse fieldsets|https://jsonapi.org/format/#fetching-sparse-fieldsets] would reduce the overhead of doing REST API requests.,,0,0,0,0,0.0,Implement Sparse Fieldsets $end$ Implementing [sparse fieldsets|https://jsonapi.org/format/#fetching-sparse-fieldsets] would reduce the overhead of doing REST API requests. $acceptance criteria:$,0,0,0,0,0,0,0,2.16667,82,13,0.158537,10,0.121951,8,0.097561,8,0.097561,7,0.0853659
1409,MXS-2924,New Feature,MXS,2020-03-16 07:39:52,MXS-2533,0,Make listeners a separate REST API resource,This is more in line with the way they are defined in the configuration file. It also simplifies the use of listeners via the REST API by allowing listing of listeners without finding the associated service.,,Make listeners a separate REST API resource $end$ This is more in line with the way they are defined in the configuration file. It also simplifies the use of listeners via the REST API by allowing listing of listeners without finding the associated service. $acceptance criteria:$,,markus makela,markus makela,Major,14,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-03-16 09:44:50,Make listeners a separate REST API resource,This is more in line with the way they are defined in the configuration file. It also simplifies the use of listeners via the REST API by allowing listing of listeners without finding the associated service.,,0,0,0,0,0.0,Make listeners a separate REST API resource $end$ This is more in line with the way they are defined in the configuration file. It also simplifies the use of listeners via the REST API by allowing listing of listeners without finding the associated service. $acceptance criteria:$,0,0,0,0,0,0,1,2.06667,83,13,0.156627,10,0.120482,8,0.0963855,8,0.0963855,7,0.0843373
1410,MXS-2925,Task,MXS,2020-03-16 09:44:12,,0,Upgrade yargs,Update yargs from 8.0 to 15.0 and fix anything that breaks.,,Upgrade yargs $end$ Update yargs from 8.0 to 15.0 and fix anything that breaks. $acceptance criteria:$,,markus makela,markus makela,Major,14,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-03-16 09:44:48,Upgrade yargs,Update yargs from 8.0 to 15.0 and fix anything that breaks.,,0,0,0,0,0.0,Upgrade yargs $end$ Update yargs from 8.0 to 15.0 and fix anything that breaks. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,84,13,0.154762,10,0.119048,8,0.0952381,8,0.0952381,7,0.0833333
1411,MXS-2926,Task,MXS,2020-03-16 09:51:41,,0,Adapt Columnstore monitor to updaming changes in Columnstore,,,Adapt Columnstore monitor to updaming changes in Columnstore $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-16 09:51:41,Adapt Columnstore monitor to updaming changes in Columnstore,,,0,0,0,0,0.0,Adapt Columnstore monitor to updaming changes in Columnstore $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,370,35,0.0945946,14,0.0378378,9,0.0243243,7,0.0189189,7,0.0189189
1412,MXS-2927,Task,MXS,2020-03-16 11:26:55,,0,Implementation of MaxGUI,,,Implementation of MaxGUI $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,0,0,8,0,0,0,,0,850,0,0,0,2020-03-16 11:26:55,Implementation of MaxGUI,,,0,0,0,0,0.0,Implementation of MaxGUI $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,371,35,0.0943396,14,0.0377358,9,0.0242588,7,0.0188679,7,0.0188679
1413,MXS-2928,New Feature,MXS,2020-03-16 11:27:11,MXS-2533,0,Serve GUI files via the REST API,The GUI files need to be served by MaxScale.,,Serve GUI files via the REST API $end$ The GUI files need to be served by MaxScale. $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-16 11:27:11,Serve GUI files via the REST API,The GUI files need to be served by MaxScale.,,0,0,0,0,0.0,Serve GUI files via the REST API $end$ The GUI files need to be served by MaxScale. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,85,13,0.152941,10,0.117647,8,0.0941176,8,0.0941176,7,0.0823529
1414,MXS-2929,Task,MXS,2020-03-16 11:29:34,,0,BuildBot in Gcloud,move Maxscale BuildBot to GCloud,,BuildBot in Gcloud $end$ move Maxscale BuildBot to GCloud $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-03-16 11:29:54,BuildBot in Gcloud,move Maxscale BuildBot to GCloud,,0,0,0,0,0.0,BuildBot in Gcloud $end$ move Maxscale BuildBot to GCloud $acceptance criteria:$,0,0,0,0,0,0,1,0.0,87,1,0.0114943,0,0.0,0,0.0,0,0.0,0,0.0
1415,MXS-2934,Task,MXS,2020-03-17 13:57:58,,0,Add support for PUT to mxb::http,,,Add support for PUT to mxb::http $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-03-17 13:58:10,Add support for PUT to mxb::http,,,0,0,0,0,0.0,Add support for PUT to mxb::http $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,372,35,0.094086,14,0.0376344,9,0.0241935,7,0.0188172,7,0.0188172
1416,MXS-2937,Task,MXS,2020-03-18 10:59:17,,0,Create test case for MXS-2919,Preliminary test case in commit 05cf6e0d1efd869b999e58fd1e51858259ada26c.,,Create test case for MXS-2919 $end$ Preliminary test case in commit 05cf6e0d1efd869b999e58fd1e51858259ada26c. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-30 10:14:50,Create test case for MXS-2919,Preliminary test case in commit 05cf6e0d1efd869b999e58fd1e51858259ada26c.,,0,0,0,0,0.0,Create test case for MXS-2919 $end$ Preliminary test case in commit 05cf6e0d1efd869b999e58fd1e51858259ada26c. $acceptance criteria:$,0,0,0,0,0,0,0,287.25,86,13,0.151163,10,0.116279,8,0.0930233,8,0.0930233,7,0.0813954
1417,MXS-2941,Task,MXS,2020-03-20 13:41:57,,0,"Authenticator setting ""skip_authentication"" should skip authentication",The setting currently only changes if the host pattern is matched to the client ip. The setting could also skip the authentication step to match the wording. Perhaps these two should be different settings?,,"Authenticator setting ""skip_authentication"" should skip authentication $end$ The setting currently only changes if the host pattern is matched to the client ip. The setting could also skip the authentication step to match the wording. Perhaps these two should be different settings? $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-04-13 14:34:13,"Authenticator setting ""skip_authentication"" should skip authentication",The setting currently only changes if the host pattern is matched to the client ip. The setting could also skip the authentication step to match the wording. Perhaps these two should be different settings?,,0,0,0,0,0.0,"Authenticator setting ""skip_authentication"" should skip authentication $end$ The setting currently only changes if the host pattern is matched to the client ip. The setting could also skip the authentication step to match the wording. Perhaps these two should be different settings? $acceptance criteria:$",0,0,0,0,0,0,1,576.867,15,2,0.133333,1,0.0666667,0,0.0,0,0.0,0,0.0
1418,MXS-2944,Task,MXS,2020-03-26 23:12:59,,0,Remove Obsolete Documentation From Source Code,"We have a couple of articles in https://github.com/mariadb-corporation/MaxScale/tree/2.4/Documentation/Tutorials that need to be removed.

",,"Remove Obsolete Documentation From Source Code $end$ We have a couple of articles in https://github.com/mariadb-corporation/MaxScale/tree/2.4/Documentation/Tutorials that need to be removed.

 $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Minor,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-03-30 10:14:53,Remove Obsolete Documentation From Source Code,"We have a couple of articles in https://github.com/mariadb-corporation/MaxScale/tree/2.4/Documentation/Tutorials that need to be removed.

",,0,0,0,0,0.0,"Remove Obsolete Documentation From Source Code $end$ We have a couple of articles in https://github.com/mariadb-corporation/MaxScale/tree/2.4/Documentation/Tutorials that need to be removed.

 $acceptance criteria:$",0,0,0,0,0,0,0,83.0167,7,3,0.428571,3,0.428571,2,0.285714,2,0.285714,1,0.142857
1419,MXS-2946,Task,MXS,2020-03-30 09:17:13,,0,Adapt Columnstore monitor to changes in Columnstore administrative REST-API,,,Adapt Columnstore monitor to changes in Columnstore administrative REST-API $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,1,5,0,0,0,,0,850,0,0,0,2020-03-30 09:17:13,Adapt Columnstore monitor to changes in Columnstore administrative REST-API,,,0,0,0,0,0.0,Adapt Columnstore monitor to changes in Columnstore administrative REST-API $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,373,35,0.0938338,14,0.0375335,9,0.0241287,7,0.0187668,7,0.0187668
1420,MXS-2947,Task,MXS,2020-03-30 10:18:30,,0,Pinloki Implementation,,,Pinloki Implementation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,0,2,4,0,0,0,,0,850,0,0,0,2020-03-30 10:18:30,Pinloki Implementation,,,0,0,0,0,0.0,Pinloki Implementation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,374,35,0.0935829,14,0.0374332,9,0.0240642,7,0.0187166,7,0.0187166
1421,MXS-2949,Task,MXS,2020-03-30 10:26:41,,0,Test that servers detect a lost monitor connection,Play with the tcp keepalive-settings on the server.,,Test that servers detect a lost monitor connection $end$ Play with the tcp keepalive-settings on the server. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-03-30 10:26:41,Test that servers detect a lost monitor connection,Play with the tcp keepalive-settings on the server.,,0,0,0,0,0.0,Test that servers detect a lost monitor connection $end$ Play with the tcp keepalive-settings on the server. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,375,35,0.0933333,14,0.0373333,9,0.024,7,0.0186667,7,0.0186667
1422,MXS-295,Sub-Task,MXS,2015-07-29 22:45:31,,0,MaxScale on RHEL7 ppc64,"We do have a package for RHEL7 on ppc64le, but it seems that we do not have a package for RHEL7 on ppc64. I think we are supposed to support that platform.
",,"MaxScale on RHEL7 ppc64 $end$ We do have a package for RHEL7 on ppc64le, but it seems that we do not have a package for RHEL7 on ppc64. I think we are supposed to support that platform.
 $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2015-07-29 22:45:31,MaxScale on RHEL7 ppc64,"We do have a package for RHEL7 on ppc64le, but it seems that we do not have a package for RHEL7 on ppc64. I think we are supposed to support that platform.
",,0,0,0,0,0.0,"MaxScale on RHEL7 ppc64 $end$ We do have a package for RHEL7 on ppc64le, but it seems that we do not have a package for RHEL7 on ppc64. I think we are supposed to support that platform.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1423,MXS-2955,Task,MXS,2020-04-06 16:37:21,,0,Remove support for multi-line parameters,"The support of multi-line parameters has the side effect of causing duplicate parameters to be combined into one parameter. For example, the following parameters:
{code}
param=true
param=false
{code}
are interpreted as:
{code}
param=true,false
{code}
Most of the time this causes unintelligible errors about parameter types. Dropping support for multi-line parameters would prevent this.",,"Remove support for multi-line parameters $end$ The support of multi-line parameters has the side effect of causing duplicate parameters to be combined into one parameter. For example, the following parameters:
{code}
param=true
param=false
{code}
are interpreted as:
{code}
param=true,false
{code}
Most of the time this causes unintelligible errors about parameter types. Dropping support for multi-line parameters would prevent this. $acceptance criteria:$",,markus makela,markus makela,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-10-19 10:17:23,Remove support for multi-line parameters,"The support of multi-line parameters has the side effect of causing duplicate parameters to be combined into one parameter. For example, the following parameters:
{code}
param=true
param=false
{code}
are interpreted as:
{code}
param=true,false
{code}
Most of the time this causes unintelligible errors about parameter types. Dropping support for multi-line parameters would prevent this.",,0,0,0,0,0.0,"Remove support for multi-line parameters $end$ The support of multi-line parameters has the side effect of causing duplicate parameters to be combined into one parameter. For example, the following parameters:
{code}
param=true
param=false
{code}
are interpreted as:
{code}
param=true,false
{code}
Most of the time this causes unintelligible errors about parameter types. Dropping support for multi-line parameters would prevent this. $acceptance criteria:$",0,0,0,0,0,0,0,4697.67,87,13,0.149425,10,0.114943,8,0.091954,8,0.091954,7,0.0804598
1424,MXS-2958,Task,MXS,2020-04-08 08:20:25,,0,Finalize authenticator cleanup,"The mysqlauth-sources still have much unused code. Go through it, implement it in user account manager if applicable, and remove.",,"Finalize authenticator cleanup $end$ The mysqlauth-sources still have much unused code. Go through it, implement it in user account manager if applicable, and remove. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,11,,0,0,0,5,0,0,0,,0,850,0,0,0,2020-04-08 08:26:15,Finalize authenticator cleanup,"The mysqlauth-sources still have much unused code. Go through it, implement it in user account manager if applicable, and remove.",,0,0,0,0,0.0,"Finalize authenticator cleanup $end$ The mysqlauth-sources still have much unused code. Go through it, implement it in user account manager if applicable, and remove. $acceptance criteria:$",0,0,0,0,0,0,1,0.0833333,16,2,0.125,1,0.0625,0,0.0,0,0.0,0,0.0
1425,MXS-2965,Task,MXS,2020-04-14 08:10:21,,0,Remove MaxAdmin.,,,Remove MaxAdmin. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-04-14 08:10:21,Remove MaxAdmin.,,,0,0,0,0,0.0,Remove MaxAdmin. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,376,35,0.0930851,14,0.037234,9,0.0239362,7,0.018617,7,0.018617
1426,MXS-2974,Task,MXS,2020-04-27 10:02:49,,0,Implement slave replication,Implement the code that handles slave registration and sending of events.,,Implement slave replication $end$ Implement the code that handles slave registration and sending of events. $acceptance criteria:$,,markus makela,markus makela,Major,2,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-04-27 10:03:00,Implement slave replication,Implement the code that handles slave registration and sending of events.,,0,0,0,0,0.0,Implement slave replication $end$ Implement the code that handles slave registration and sending of events. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,88,13,0.147727,10,0.113636,8,0.0909091,8,0.0909091,7,0.0795455
1427,MXS-2975,Task,MXS,2020-04-27 10:04:18,,0,Finalize master replication,Finish integrating the master to maxscale side of replication and add any missing parts.,,Finalize master replication $end$ Finish integrating the master to maxscale side of replication and add any missing parts. $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,1,2,0,0,0,,0,850,0,0,0,2020-04-27 10:06:07,Finalize master replication,Finish integrating the master to maxscale side of replication and add any missing parts.,,0,0,0,0,0.0,Finalize master replication $end$ Finish integrating the master to maxscale side of replication and add any missing parts. $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,89,13,0.146067,10,0.11236,8,0.0898876,8,0.0898876,7,0.0786517
1428,MXS-2976,Task,MXS,2020-04-27 10:37:02,,0,"Check 2.3, 2.4 and develop test-cases",,,"Check 2.3, 2.4 and develop test-cases $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,0,1,0,4,0,0,0,,0,850,1,0,0,2020-04-27 10:37:02,"Check 2.3, 2.4 and develop test-cases",,,0,0,0,0,0.0,"Check 2.3, 2.4 and develop test-cases $end$ $acceptance criteria:$",0,0,0,0,0,0,1,0.0,377,35,0.0928382,14,0.0371353,9,0.0238727,7,0.0185676,7,0.0185676
1429,MXS-2977,Task,MXS,2020-04-27 10:43:00,,0,Investigate MaxScale docker setup,,,Investigate MaxScale docker setup $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-04-27 10:43:00,Investigate MaxScale docker setup,,,0,0,0,0,0.0,Investigate MaxScale docker setup $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,378,35,0.0925926,14,0.037037,9,0.0238095,7,0.0185185,7,0.0185185
1430,MXS-2986,Task,MXS,2020-05-11 10:15:43,MXS-2658,0,Add pinloki test cases,,,Add pinloki test cases $end$ $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-05-11 10:15:43,Add pinloki test cases,,,0,0,0,0,0.0,Add pinloki test cases $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,90,13,0.144444,10,0.111111,8,0.0888889,8,0.0888889,7,0.0777778
1431,MXS-2987,Task,MXS,2020-05-11 10:18:08,,0,Release 2.4.9 Docker Imager,,,Release 2.4.9 Docker Imager $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-05-11 10:18:08,Release 2.4.9 Docker Imager,,,0,0,0,0,0.0,Release 2.4.9 Docker Imager $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,379,35,0.0923483,14,0.0369393,9,0.0237467,7,0.0184697,7,0.0184697
1432,MXS-2988,Task,MXS,2020-05-11 10:26:30,,0,Review pinloki,,,Review pinloki $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-05-11 10:26:30,Review pinloki,,,0,0,0,0,0.0,Review pinloki $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,380,35,0.0921053,14,0.0368421,9,0.0236842,7,0.0184211,7,0.0184211
1433,MXS-2999,Task,MXS,2020-05-18 11:28:29,,0,Cleanup password encryption/decryption,,,Cleanup password encryption/decryption $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-05-18 11:29:32,Cleanup password encryption/decryption,,,0,0,0,0,0.0,Cleanup password encryption/decryption $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,17,2,0.117647,1,0.0588235,0,0.0,0,0.0,0,0.0
1434,MXS-300,New Feature,MXS,2015-08-03 20:39:13,,0,LONGBLOB are currently not supported.,"Why LONGBLOB are currently not supported? 
For small data in LONGBLOB, it seems fine.
In which situation, it will cause problem?
For 1.0.5, is it support LONGBLOB? Because I did not see ""LONGBLOB are currently not supported."" in the document.
",,"LONGBLOB are currently not supported. $end$ Why LONGBLOB are currently not supported? 
For small data in LONGBLOB, it seems fine.
In which situation, it will cause problem?
For 1.0.5, is it support LONGBLOB? Because I did not see ""LONGBLOB are currently not supported."" in the document.
 $acceptance criteria:$",,cai sunny,cai sunny,Major,14,,1,6,1,2,0,0,0,,0,850,1,0,0,2016-05-04 09:31:42,LONGBLOB are currently not supported.,"Why LONGBLOB are currently not supported? 
For small data in LONGBLOB, it seems fine.
In which situation, it will cause problem?
For 1.0.5, is it support LONGBLOB? Because I did not see ""LONGBLOB are currently not supported."" in the document.
",,0,0,0,0,0.0,"LONGBLOB are currently not supported. $end$ Why LONGBLOB are currently not supported? 
For small data in LONGBLOB, it seems fine.
In which situation, it will cause problem?
For 1.0.5, is it support LONGBLOB? Because I did not see ""LONGBLOB are currently not supported."" in the document.
 $acceptance criteria:$",0,0,0,0,0,0,1,6588.87,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1435,MXS-3003,New Feature,MXS,2020-05-22 06:49:37,,0,Support inbound proxy protocol,"It would be nice if MaxScale could support inbound proxy_protocol connections.

Even nicer if it can forward the received headers to the backend also using proxy_protocol.

This will enable the usage of a load-balancer in front of MaxScale and still gain the simplification of MariaDB account management.

The end goal is to have a high availability setup of MaxScale without using any virtual IP address or KeepAlived services.
",,"Support inbound proxy protocol $end$ It would be nice if MaxScale could support inbound proxy_protocol connections.

Even nicer if it can forward the received headers to the backend also using proxy_protocol.

This will enable the usage of a load-balancer in front of MaxScale and still gain the simplification of MariaDB account management.

The end goal is to have a high availability setup of MaxScale without using any virtual IP address or KeepAlived services.
 $acceptance criteria:$",,Laurent Indermühle,Laurent Indermühle,Minor,28,,0,1,0,4,0,1,0,,0,850,0,0,0,2022-11-21 11:20:58,Support inbound proxy_protocol,"It would be nice if MaxScale could support inbound proxy_protocol connections.

Even nicer if it can forward the received headers to the backend also using proxy_protocol.

This will enable the usage of a load-balancer in front of MaxScale and still gain the simplification of MariaDB account management.

The end goal is to have a high availability setup of MaxScale without using any virtual IP address or KeepAlived services.
",,1,0,0,3,0.027027,"Support inbound proxy_protocol $end$ It would be nice if MaxScale could support inbound proxy_protocol connections.

Even nicer if it can forward the received headers to the backend also using proxy_protocol.

This will enable the usage of a load-balancer in front of MaxScale and still gain the simplification of MariaDB account management.

The end goal is to have a high availability setup of MaxScale without using any virtual IP address or KeepAlived services.
 $acceptance criteria:$",1,1,0,0,0,0,1,21916.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1436,MXS-3007,Task,MXS,2020-05-25 11:32:42,,0,Fix SLES12 build failure,,,Fix SLES12 build failure $end$ $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-05-25 11:32:42,Fix SLES12 build failure,,,0,0,0,0,0.0,Fix SLES12 build failure $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,91,13,0.142857,10,0.10989,8,0.0879121,8,0.0879121,7,0.0769231
1437,MXS-3008,Task,MXS,2020-05-25 11:35:30,,0,Add TLS support for MaxScale <-> Master communication,,,Add TLS support for MaxScale <-> Master communication $end$ $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-05-25 11:35:30,Add TLS support for MaxScale <-> Master communication,,,0,0,0,0,0.0,Add TLS support for MaxScale <-> Master communication $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,92,13,0.141304,10,0.108696,8,0.0869565,8,0.0869565,7,0.076087
1438,MXS-3009,Task,MXS,2020-05-25 11:41:31,,0,Check that no slave are replicating from binlogs being purged,"When doing a PURGE BINARY LOGS, make sure that no slave is replicating from any of the binlgs in the range.",,"Check that no slave are replicating from binlogs being purged $end$ When doing a PURGE BINARY LOGS, make sure that no slave is replicating from any of the binlgs in the range. $acceptance criteria:$",,markus makela,markus makela,Major,5,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-05-25 11:41:31,Check that no slave are replicating from binlogs being purged,"When doing a PURGE BINARY LOGS, make sure that no slave is replicating from any of the binlgs in the range.",,0,0,0,0,0.0,"Check that no slave are replicating from binlogs being purged $end$ When doing a PURGE BINARY LOGS, make sure that no slave is replicating from any of the binlgs in the range. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,93,13,0.139785,10,0.107527,8,0.0860215,8,0.0860215,7,0.0752688
1439,MXS-3018,Task,MXS,2020-06-01 15:43:38,,0,Add better customization settings to MariaDBMonitor server roles,,,Add better customization settings to MariaDBMonitor server roles $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Major,11,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-06-08 11:27:13,Add better customization settings to MariaDBMonitor server roles,,,0,0,0,0,0.0,Add better customization settings to MariaDBMonitor server roles $end$ $acceptance criteria:$,0,0,0,0,0,0,1,163.717,18,2,0.111111,1,0.0555556,0,0.0,0,0.0,0,0.0
1440,MXS-3029,Task,MXS,2020-06-08 09:21:33,,0,Run performance tests for shared cache,,,Run performance tests for shared cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-06-08 09:21:33,Run performance tests for shared cache,,,0,0,0,0,0.0,Run performance tests for shared cache $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,381,35,0.0918635,14,0.0367454,9,0.023622,7,0.0183727,7,0.0183727
1441,MXS-3030,Task,MXS,2020-06-08 09:22:33,,0,Test Columnstore monitor,,,Test Columnstore monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-06-08 09:22:33,Test Columnstore monitor,,,0,0,0,0,0.0,Test Columnstore monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,382,35,0.091623,14,0.0366492,9,0.0235602,7,0.0183246,7,0.0183246
1442,MXS-3031,Task,MXS,2020-06-08 09:58:19,,0,Harden 2.5,,,Harden 2.5 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-06-08 09:58:19,Harden 2.5,,,0,0,0,0,0.0,Harden 2.5 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,383,35,0.0913838,14,0.0365535,9,0.0234987,7,0.0182768,7,0.0182768
1443,MXS-3033,Task,MXS,2020-06-08 10:49:14,,0,Remove old binlogrouter,"* Remove source code
* Add link to new implementation in old documentation
* Document how to move from the old implementation to new implementation",,"Remove old binlogrouter $end$ * Remove source code
* Add link to new implementation in old documentation
* Document how to move from the old implementation to new implementation $acceptance criteria:$",,markus makela,markus makela,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2020-06-08 10:49:14,Remove binlogrouter,"* Remove source code
* Add link to new implementation in old documentation
* Document how to move from the old implementation to new implementation",,1,0,0,1,0.0333333,"Remove binlogrouter $end$ * Remove source code
* Add link to new implementation in old documentation
* Document how to move from the old implementation to new implementation $acceptance criteria:$",1,1,0,0,0,0,0,0.0,94,13,0.138298,10,0.106383,8,0.0851064,8,0.0851064,7,0.0744681
1444,MXS-3034,Task,MXS,2020-06-08 10:49:22,,0,Finish pinloki documentation,,,Finish pinloki documentation $end$ $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,2,0,,0,850,0,0,0,2020-06-08 10:49:22,finish pinloki documentation,,,2,0,0,2,0.166667,finish pinloki documentation $end$ $acceptance criteria:$,2,1,0,0,0,0,0,0.0,95,14,0.147368,10,0.105263,8,0.0842105,8,0.0842105,7,0.0736842
1445,MXS-3037,New Feature,MXS,2020-06-12 08:12:02,,0,show detail processlist like at mariadb,"on mariadb when i need check what are query is running on mariadb instance.
i can see with SHOW PROCESSLIST or select to information_schema.processlist.

But after mariadb connect to maxscale i can't see process  list like that.

any advice ?",,"show detail processlist like at mariadb $end$ on mariadb when i need check what are query is running on mariadb instance.
i can see with SHOW PROCESSLIST or select to information_schema.processlist.

But after mariadb connect to maxscale i can't see process  list like that.

any advice ? $acceptance criteria:$",,febriyant,febriyant,Major,16,,0,7,2,1,0,0,0,,0,850,1,0,0,2021-10-11 08:45:34,show detail processlist like at mariadb,"on mariadb when i need check what are query is running on mariadb instance.
i can see with SHOW PROCESSLIST or select to information_schema.processlist.

But after mariadb connect to maxscale i can't see process  list like that.

any advice ?",,0,0,0,0,0.0,"show detail processlist like at mariadb $end$ on mariadb when i need check what are query is running on mariadb instance.
i can see with SHOW PROCESSLIST or select to information_schema.processlist.

But after mariadb connect to maxscale i can't see process  list like that.

any advice ? $acceptance criteria:$",0,0,0,0,0,0,0,11664.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1446,MXS-304,New Feature,MXS,2015-08-10 15:04:03,,0,Enabled nested configuration files,"Allowing hierarchical configuration files will make the management of MaxScale's configuration a lot cleaner. This is in line with the MariaDB server which has the /etc/my.cnf.d/ folder. MaxScale should have a /etc/maxscale.cnf.d/ folder which has all the relevant configuration files.

When provided with a configuration file xyz.cnf, MaxScale should automatically and recursively read all cnf-files from the directory xyz.cnf.d and all its subdirectories.
",,"Enabled nested configuration files $end$ Allowing hierarchical configuration files will make the management of MaxScale's configuration a lot cleaner. This is in line with the MariaDB server which has the /etc/my.cnf.d/ folder. MaxScale should have a /etc/maxscale.cnf.d/ folder which has all the relevant configuration files.

When provided with a configuration file xyz.cnf, MaxScale should automatically and recursively read all cnf-files from the directory xyz.cnf.d and all its subdirectories.
 $acceptance criteria:$",,markus makela,markus makela,Major,11,,0,0,0,2,0,2,0,,0,850,0,0,0,2016-10-05 08:52:02,Enable include directives in config file,Allowing include directives in the configuration file will make the management of MaxScale's configuration a lot cleaner. This is in line with the MariaDB server which has the /etc/my.cnf.d/ folder. MaxScale should have a /etc/maxscale.cnf.d/ folder which has all the relevant configuration files. The main maxscale.cnf file could have a single include directive which includes the files in the /etc/maxscale.cnf.d/ folder.,,1,1,0,54,0.457143,Enable include directives in config file $end$ Allowing include directives in the configuration file will make the management of MaxScale's configuration a lot cleaner. This is in line with the MariaDB server which has the /etc/my.cnf.d/ folder. MaxScale should have a /etc/maxscale.cnf.d/ folder which has all the relevant configuration files. The main maxscale.cnf file could have a single include directive which includes the files in the /etc/maxscale.cnf.d/ folder. $acceptance criteria:$,2,1,1,1,1,1,1,10121.8,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
1447,MXS-3047,Task,MXS,2020-06-22 08:51:19,,0,Update Columnstore monitor to CS API 0.4.0,,,Update Columnstore monitor to CS API 0.4.0 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-06-22 08:51:19,Update Columnstore monitor to CS API 0.4.0,,,0,0,0,0,0.0,Update Columnstore monitor to CS API 0.4.0 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,384,35,0.0911458,14,0.0364583,9,0.0234375,7,0.0182292,7,0.0182292
1448,MXS-3048,Task,MXS,2020-06-22 10:11:26,,0,Unit-tests for MaxGUI,,,Unit-tests for MaxGUI $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,14,,0,0,0,6,0,0,0,,0,850,0,0,0,2020-06-22 10:11:26,Unit-tests for MaxGUI,,,0,0,0,0,0.0,Unit-tests for MaxGUI $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,385,35,0.0909091,14,0.0363636,9,0.0233766,7,0.0181818,7,0.0181818
1449,MXS-3066,Task,MXS,2020-07-06 07:46:18,,0,Move MaxGui to the MaxScale repository.,,,Move MaxGui to the MaxScale repository. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-07-06 07:46:18,Move MaxGui to the MaxScale repository.,,,0,0,0,0,0.0,Move MaxGui to the MaxScale repository. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,386,35,0.0906736,14,0.0362694,9,0.0233161,7,0.0181347,7,0.0181347
1450,MXS-3067,Task,MXS,2020-07-06 08:44:07,,0,Load all modules on startup,All modules should be loaded on startup in order to be able to provide information about the parameters they accept.,,Load all modules on startup $end$ All modules should be loaded on startup in order to be able to provide information about the parameters they accept. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-07-06 08:44:07,Load all modules on startup,All modules should be loaded on startup in order to be able to provide information about the parameters they accept.,,0,0,0,0,0.0,Load all modules on startup $end$ All modules should be loaded on startup in order to be able to provide information about the parameters they accept. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,96,15,0.15625,10,0.104167,8,0.0833333,8,0.0833333,7,0.0729167
1451,MXS-307,New Feature,MXS,2015-08-12 18:46:38,,0,Add multi-source replication support in MaxScale,"Add support for multi-source replication in Binlog Server

(1) Binlog Server should be able to replicate from multiple masters
Support CHANGE MASTER syntax for registering with multiple masters
CHANGE MASTER 'master-name' MASTER_HOST = 'master_host_ip', MASTER_USER = 'master_replication_user', MASTER_PASSWORD = 'password', MASTER_PORT = 3306;

(2) Binlog Server should be able to re-write the schema name
Support the same behavior as --replicate-rewrite-db option in MariaDB/MySQL slave

See example how it is done in a MariaDB/MySQL slave https://mariadb.com/blog/multisource-replication-how-resolve-schema-name-conflicts",,"Add multi-source replication support in MaxScale $end$ Add support for multi-source replication in Binlog Server

(1) Binlog Server should be able to replicate from multiple masters
Support CHANGE MASTER syntax for registering with multiple masters
CHANGE MASTER 'master-name' MASTER_HOST = 'master_host_ip', MASTER_USER = 'master_replication_user', MASTER_PASSWORD = 'password', MASTER_PORT = 3306;

(2) Binlog Server should be able to re-write the schema name
Support the same behavior as --replicate-rewrite-db option in MariaDB/MySQL slave

See example how it is done in a MariaDB/MySQL slave https://mariadb.com/blog/multisource-replication-how-resolve-schema-name-conflicts $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-08-02 10:21:34,Add multi-source replication support in MaxScale,"Add support for multi-source replication in Binlog Server

(1) Binlog Server should be able to replicate from multiple masters
Support CHANGE MASTER syntax for registering with multiple masters
CHANGE MASTER 'master-name' MASTER_HOST = 'master_host_ip', MASTER_USER = 'master_replication_user', MASTER_PASSWORD = 'password', MASTER_PORT = 3306;

(2) Binlog Server should be able to re-write the schema name
Support the same behavior as --replicate-rewrite-db option in MariaDB/MySQL slave

See example how it is done in a MariaDB/MySQL slave https://mariadb.com/blog/multisource-replication-how-resolve-schema-name-conflicts",,0,0,0,0,0.0,"Add multi-source replication support in MaxScale $end$ Add support for multi-source replication in Binlog Server

(1) Binlog Server should be able to replicate from multiple masters
Support CHANGE MASTER syntax for registering with multiple masters
CHANGE MASTER 'master-name' MASTER_HOST = 'master_host_ip', MASTER_USER = 'master_replication_user', MASTER_PASSWORD = 'password', MASTER_PORT = 3306;

(2) Binlog Server should be able to re-write the schema name
Support the same behavior as --replicate-rewrite-db option in MariaDB/MySQL slave

See example how it is done in a MariaDB/MySQL slave https://mariadb.com/blog/multisource-replication-how-resolve-schema-name-conflicts $acceptance criteria:$",0,0,0,0,0,0,0,8535.57,7,4,0.571429,1,0.142857,0,0.0,0,0.0,0,0.0
1452,MXS-3071,New Feature,MXS,2020-07-13 14:36:26,,0,Display filters and listeners on the dashboard page of MaxGui,"Currently, Servers,  Current Sessions and Services tables can be seen on the dashboard page. 
filters and listeners should be there as well, so that we can have an overview of all available resources.
There should be also details page for filter and listener, from that page, parameters table and relationship table will be displayed in read mode only as filter and listener don't allow to be updated once created.  Clicking the gear icon should allow to delete that filter or listener.
On current service details page,  a listener relationship table should be shown with anchor link to listener details page.",,"Display filters and listeners on the dashboard page of MaxGui $end$ Currently, Servers,  Current Sessions and Services tables can be seen on the dashboard page. 
filters and listeners should be there as well, so that we can have an overview of all available resources.
There should be also details page for filter and listener, from that page, parameters table and relationship table will be displayed in read mode only as filter and listener don't allow to be updated once created.  Clicking the gear icon should allow to delete that filter or listener.
On current service details page,  a listener relationship table should be shown with anchor link to listener details page. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,20,,0,1,1,1,0,3,0,,0,850,1,1,0,2020-08-03 10:12:05,Display filters and listeners on the dashboard page of MaxGui,"Currently, Servers,  Current Sessions and Services tables can be seen on the dashboard page. 
filters, listeners and monitors should be there as well, so that we can have an overview of all available resources, then we can have a delete button on each table row. ",,0,2,0,81,1.18966,"Display filters and listeners on the dashboard page of MaxGui $end$ Currently, Servers,  Current Sessions and Services tables can be seen on the dashboard page. 
filters, listeners and monitors should be there as well, so that we can have an overview of all available resources, then we can have a delete button on each table row.  $acceptance criteria:$",2,1,1,1,1,1,1,499.583,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1453,MXS-3086,Task,MXS,2020-07-22 10:08:58,,0,System test classes refactoring,,,System test classes refactoring $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,26,,0,0,0,6,0,0,0,,0,850,0,0,0,2020-07-22 10:26:38,System test classes refactoring,,,0,0,0,0,0.0,System test classes refactoring $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.283333,19,2,0.105263,1,0.0526316,0,0.0,0,0.0,0,0.0
1454,MXS-309,Sub-Task,MXS,2015-08-13 00:08:57,,0,create MDBCI JSONs for all needed test configurations,"JSON descriptions of all needed combinations of MariaDB/MySQL versions and distros have to be created.

backend distros can be the same for all (probably one for AWS-bases setups and one - for VBOX and one for QEMU)",,"create MDBCI JSONs for all needed test configurations $end$ JSON descriptions of all needed combinations of MariaDB/MySQL versions and distros have to be created.

backend distros can be the same for all (probably one for AWS-bases setups and one - for VBOX and one for QEMU) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Critical,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2015-08-13 00:08:57,create MDBCI JSONs for all needed test configurations,"JSON descriptions of all needed combinations of MariaDB/MySQL versions and distros have to be created.

backend distros can be the same for all (probably one for AWS-bases setups and one - for VBOX and one for QEMU)",,0,0,0,0,0.0,"create MDBCI JSONs for all needed test configurations $end$ JSON descriptions of all needed combinations of MariaDB/MySQL versions and distros have to be created.

backend distros can be the same for all (probably one for AWS-bases setups and one - for VBOX and one for QEMU) $acceptance criteria:$",0,0,0,0,0,0,1,0.0,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1455,MXS-3091,New Feature,MXS,2020-07-23 16:33:32,,0,readconnroute router restrict reads to slave(s) only,"Maxscale with router=readconnroute to allow read traffic to be served even when the replication is broken (i.e slave IO or SQL process stopped). As per documentation we can do this by setting the router_options=running. However, with this setting this also sends traffic to the Master DB which we do not want. We want the read traffic to only go to the slaves even if the replication is broken. Master DB should only be chosen as a read candidate if none of the slave servers are available.
It seems like it's not possible to configure readconnroute to use all other servers except the master.",,"readconnroute router restrict reads to slave(s) only $end$ Maxscale with router=readconnroute to allow read traffic to be served even when the replication is broken (i.e slave IO or SQL process stopped). As per documentation we can do this by setting the router_options=running. However, with this setting this also sends traffic to the Master DB which we do not want. We want the read traffic to only go to the slaves even if the replication is broken. Master DB should only be chosen as a read candidate if none of the slave servers are available.
It seems like it's not possible to configure readconnroute to use all other servers except the master. $acceptance criteria:$",,Muhammad Irfan,Muhammad Irfan,Major,15,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-10-12 10:42:14,readconnroute router restrict reads to slave(s) only,"Maxscale with router=readconnroute to allow read traffic to be served even when the replication is broken (i.e slave IO or SQL process stopped). As per documentation we can do this by setting the router_options=running. However, with this setting this also sends traffic to the Master DB which we do not want. We want the read traffic to only go to the slaves even if the replication is broken. Master DB should only be chosen as a read candidate if none of the slave servers are available.
It seems like it's not possible to configure readconnroute to use all other servers except the master.",,0,0,0,0,0.0,"readconnroute router restrict reads to slave(s) only $end$ Maxscale with router=readconnroute to allow read traffic to be served even when the replication is broken (i.e slave IO or SQL process stopped). As per documentation we can do this by setting the router_options=running. However, with this setting this also sends traffic to the Master DB which we do not want. We want the read traffic to only go to the slaves even if the replication is broken. Master DB should only be chosen as a read candidate if none of the slave servers are available.
It seems like it's not possible to configure readconnroute to use all other servers except the master. $acceptance criteria:$",0,0,0,0,0,0,1,1938.13,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1456,MXS-3095,New Feature,MXS,2020-07-27 10:59:40,,0,"kafkaCDC DML events not include table_schema,table_name information","when consume kafkacdc , DML events not include schame,table information, it hard to program biz logic",,"kafkaCDC DML events not include table_schema,table_name information $end$ when consume kafkacdc , DML events not include schame,table information, it hard to program biz logic $acceptance criteria:$",,wilsonlau,wilsonlau,Major,9,,0,7,0,1,0,0,0,,0,850,6,0,0,2020-08-18 10:39:13,"kafkaCDC DML events not include table_schema,table_name information","when consume kafkacdc , DML events not include schame,table information, it hard to program biz logic",,0,0,0,0,0.0,"kafkaCDC DML events not include table_schema,table_name information $end$ when consume kafkacdc , DML events not include schame,table information, it hard to program biz logic $acceptance criteria:$",0,0,0,0,0,0,0,527.65,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1457,MXS-3099,New Feature,MXS,2020-08-03 09:43:09,,0,Add AddNode/RemoveNode to the Columnstore Monitor,,,Add AddNode/RemoveNode to the Columnstore Monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-08-03 09:43:09,Add AddNode/RemoveNode to the Columnstore Monitor,,,0,0,0,0,0.0,Add AddNode/RemoveNode to the Columnstore Monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,387,35,0.0904393,14,0.0361757,9,0.0232558,7,0.0180879,7,0.0180879
1458,MXS-3107,New Feature,MXS,2020-08-10 07:34:34,,0,Self adjusting Columnstore monitor,"Currently double book-keeping is required when using the MaxScale Columnstore monitor for monitoring a Columnstore cluster.

Although the Columnstore cluster (obviously) is aware of what nodes are part of the cluster, MaxScale needs to be explicitly told which those nodes/servers are.

This arrangement is
* fragile
* error prone and
* userhostile.

It should be so that you only need to tell MaxScale how to get in contact with the Columnstore cluster, after which everything is automatic. As new nodes are added to the cluster or existing nodes removed, irrespective of whether it is done indirectly via MaxScale or directly using Columnstore, MaxScale (the monitor  + all routers using the information provided by the monitor) should adapt and things should just work.

Essentially, the Columnstore monitor should work just the way the Clustrix monitor currently works. In practice this means that
* the servers the Columnstore monitor is configured with are _only_ used by the monitor in order to get in contact the Columnstore cluster (one is sufficient, but more can be provided),
* the Columnstore monitor fetches the Columnstore configuration and creates transient server objects (inside MaxScale) according to the configuration, and
* services do not explicitly refer to servers, but to the Columnstore monitor.

The end-result is that the addition or removal of a node to/from the Columnstore cluster requires no changes whatsoever to the MaxScale configuration.

A sample configuration could look like:
{code}
[CS-bootstrap-node-1]
type=server
address=10.10.10.10
port=3306
protocol=mariadbbackend

[CS-bootstrap-node-2]
type=server
address=10.10.10.11
port=3306
protocol=mariadbbackend
{code}
This server entries are _exactly_ like all other server entries, but they are only used for bootstrapping the monitor.

{code}
[CsMonitor]
type=monitor
module=csmon
version=1.5
servers=CS-bootstrap-node-1, CS-bootstrap-node-2
{code}
The servers the monitor is explicitly provided with are _only_ used by the monitor for getting in contact with the Columnstore cluster. One entry is sufficient, but in case that happens to be down at startup, it's advisable to specify additional.

Using that information, the Columnstore monitor connects to the Columnstore cluster and asks for its configuration. Using the configuration the monitor
* stores persistently information about the nodes and their IPs, and
* creates a dynamic server instance for node.

The former, so that the monitor at subsequent starts has connection information for _all_ nodes. That is, the bootstrap servers are essentially only needed for the _very first_ startup when the monitor has no knowledge of the cluster. At subsequent startups the bootstrap information is essentially not needed.

The dynamic server instances will be named like:
{code}
@@CsMonitor:node1
@@CsMonitor:node1
{code}
The {{@@}} prefix is to ensure that there _cannot_ be a name-clash with a server object manually created by a user. As these server instances are transient to their nature, that is, will disappear if the corresponding node is removed, they should never be explicitly referred to.

Services should neither list the bootstrap servers, nor dynamic ones, but should simply refer to the monitor.
{code}
[My-Service]
type=service
cluster=CsMonitor
```
{code}
That is, _no_ {{servers}} entry, but instead a {{cluster}} entry that refers to the monitor.

With this setup, MaxScale + Columnstore will just work without a need for any kind of manual reconfiguration, regardless of the changes made to the Columnstore cluster.",,"Self adjusting Columnstore monitor $end$ Currently double book-keeping is required when using the MaxScale Columnstore monitor for monitoring a Columnstore cluster.

Although the Columnstore cluster (obviously) is aware of what nodes are part of the cluster, MaxScale needs to be explicitly told which those nodes/servers are.

This arrangement is
* fragile
* error prone and
* userhostile.

It should be so that you only need to tell MaxScale how to get in contact with the Columnstore cluster, after which everything is automatic. As new nodes are added to the cluster or existing nodes removed, irrespective of whether it is done indirectly via MaxScale or directly using Columnstore, MaxScale (the monitor  + all routers using the information provided by the monitor) should adapt and things should just work.

Essentially, the Columnstore monitor should work just the way the Clustrix monitor currently works. In practice this means that
* the servers the Columnstore monitor is configured with are _only_ used by the monitor in order to get in contact the Columnstore cluster (one is sufficient, but more can be provided),
* the Columnstore monitor fetches the Columnstore configuration and creates transient server objects (inside MaxScale) according to the configuration, and
* services do not explicitly refer to servers, but to the Columnstore monitor.

The end-result is that the addition or removal of a node to/from the Columnstore cluster requires no changes whatsoever to the MaxScale configuration.

A sample configuration could look like:
{code}
[CS-bootstrap-node-1]
type=server
address=10.10.10.10
port=3306
protocol=mariadbbackend

[CS-bootstrap-node-2]
type=server
address=10.10.10.11
port=3306
protocol=mariadbbackend
{code}
This server entries are _exactly_ like all other server entries, but they are only used for bootstrapping the monitor.

{code}
[CsMonitor]
type=monitor
module=csmon
version=1.5
servers=CS-bootstrap-node-1, CS-bootstrap-node-2
{code}
The servers the monitor is explicitly provided with are _only_ used by the monitor for getting in contact with the Columnstore cluster. One entry is sufficient, but in case that happens to be down at startup, it's advisable to specify additional.

Using that information, the Columnstore monitor connects to the Columnstore cluster and asks for its configuration. Using the configuration the monitor
* stores persistently information about the nodes and their IPs, and
* creates a dynamic server instance for node.

The former, so that the monitor at subsequent starts has connection information for _all_ nodes. That is, the bootstrap servers are essentially only needed for the _very first_ startup when the monitor has no knowledge of the cluster. At subsequent startups the bootstrap information is essentially not needed.

The dynamic server instances will be named like:
{code}
@@CsMonitor:node1
@@CsMonitor:node1
{code}
The {{@@}} prefix is to ensure that there _cannot_ be a name-clash with a server object manually created by a user. As these server instances are transient to their nature, that is, will disappear if the corresponding node is removed, they should never be explicitly referred to.

Services should neither list the bootstrap servers, nor dynamic ones, but should simply refer to the monitor.
{code}
[My-Service]
type=service
cluster=CsMonitor
```
{code}
That is, _no_ {{servers}} entry, but instead a {{cluster}} entry that refers to the monitor.

With this setup, MaxScale + Columnstore will just work without a need for any kind of manual reconfiguration, regardless of the changes made to the Columnstore cluster. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-08-31 08:01:14,Self adjusting Columnstore monitor,"Currently double book-keeping is required when using the MaxScale Columnstore monitor for monitoring a Columnstore cluster.

Although the Columnstore cluster (obviously) is aware of what nodes are part of the cluster, MaxScale needs to be explicitly told which those nodes/servers are.

This arrangement is
* fragile
* error prone and
* userhostile.

It should be so that you only need to tell MaxScale how to get in contact with the Columnstore cluster, after which everything is automatic. As new nodes are added to the cluster or existing nodes removed, irrespective of whether it is done indirectly via MaxScale or directly using Columnstore, MaxScale (the monitor  + all routers using the information provided by the monitor) should adapt and things should just work.

Essentially, the Columnstore monitor should work just the way the Clustrix monitor currently works. In practice this means that
* the servers the Columnstore monitor is configured with are _only_ used by the monitor in order to get in contact the Columnstore cluster (one is sufficient, but more can be provided),
* the Columnstore monitor fetches the Columnstore configuration and creates transient server objects (inside MaxScale) according to the configuration, and
* services do not explicitly refer to servers, but to the Columnstore monitor.

The end-result is that the addition or removal of a node to/from the Columnstore cluster requires no changes whatsoever to the MaxScale configuration.

A sample configuration could look like:
{code}
[CS-bootstrap-node-1]
type=server
address=10.10.10.10
port=3306
protocol=mariadbbackend

[CS-bootstrap-node-2]
type=server
address=10.10.10.11
port=3306
protocol=mariadbbackend
{code}
This server entries are _exactly_ like all other server entries, but they are only used for bootstrapping the monitor.

{code}
[CsMonitor]
type=monitor
module=csmon
version=1.5
servers=CS-bootstrap-node-1, CS-bootstrap-node-2
{code}
The servers the monitor is explicitly provided with are _only_ used by the monitor for getting in contact with the Columnstore cluster. One entry is sufficient, but in case that happens to be down at startup, it's advisable to specify additional.

Using that information, the Columnstore monitor connects to the Columnstore cluster and asks for its configuration. Using the configuration the monitor
* stores persistently information about the nodes and their IPs, and
* creates a dynamic server instance for node.

The former, so that the monitor at subsequent starts has connection information for _all_ nodes. That is, the bootstrap servers are essentially only needed for the _very first_ startup when the monitor has no knowledge of the cluster. At subsequent startups the bootstrap information is essentially not needed.

The dynamic server instances will be named like:
{code}
@@CsMonitor:node1
@@CsMonitor:node1
{code}
The {{@@}} prefix is to ensure that there _cannot_ be a name-clash with a server object manually created by a user. As these server instances are transient to their nature, that is, will disappear if the corresponding node is removed, they should never be explicitly referred to.

Services should neither list the bootstrap servers, nor dynamic ones, but should simply refer to the monitor.
{code}
[My-Service]
type=service
cluster=CsMonitor
```
{code}
That is, _no_ {{servers}} entry, but instead a {{cluster}} entry that refers to the monitor.

With this setup, MaxScale + Columnstore will just work without a need for any kind of manual reconfiguration, regardless of the changes made to the Columnstore cluster.",,0,0,0,0,0.0,"Self adjusting Columnstore monitor $end$ Currently double book-keeping is required when using the MaxScale Columnstore monitor for monitoring a Columnstore cluster.

Although the Columnstore cluster (obviously) is aware of what nodes are part of the cluster, MaxScale needs to be explicitly told which those nodes/servers are.

This arrangement is
* fragile
* error prone and
* userhostile.

It should be so that you only need to tell MaxScale how to get in contact with the Columnstore cluster, after which everything is automatic. As new nodes are added to the cluster or existing nodes removed, irrespective of whether it is done indirectly via MaxScale or directly using Columnstore, MaxScale (the monitor  + all routers using the information provided by the monitor) should adapt and things should just work.

Essentially, the Columnstore monitor should work just the way the Clustrix monitor currently works. In practice this means that
* the servers the Columnstore monitor is configured with are _only_ used by the monitor in order to get in contact the Columnstore cluster (one is sufficient, but more can be provided),
* the Columnstore monitor fetches the Columnstore configuration and creates transient server objects (inside MaxScale) according to the configuration, and
* services do not explicitly refer to servers, but to the Columnstore monitor.

The end-result is that the addition or removal of a node to/from the Columnstore cluster requires no changes whatsoever to the MaxScale configuration.

A sample configuration could look like:
{code}
[CS-bootstrap-node-1]
type=server
address=10.10.10.10
port=3306
protocol=mariadbbackend

[CS-bootstrap-node-2]
type=server
address=10.10.10.11
port=3306
protocol=mariadbbackend
{code}
This server entries are _exactly_ like all other server entries, but they are only used for bootstrapping the monitor.

{code}
[CsMonitor]
type=monitor
module=csmon
version=1.5
servers=CS-bootstrap-node-1, CS-bootstrap-node-2
{code}
The servers the monitor is explicitly provided with are _only_ used by the monitor for getting in contact with the Columnstore cluster. One entry is sufficient, but in case that happens to be down at startup, it's advisable to specify additional.

Using that information, the Columnstore monitor connects to the Columnstore cluster and asks for its configuration. Using the configuration the monitor
* stores persistently information about the nodes and their IPs, and
* creates a dynamic server instance for node.

The former, so that the monitor at subsequent starts has connection information for _all_ nodes. That is, the bootstrap servers are essentially only needed for the _very first_ startup when the monitor has no knowledge of the cluster. At subsequent startups the bootstrap information is essentially not needed.

The dynamic server instances will be named like:
{code}
@@CsMonitor:node1
@@CsMonitor:node1
{code}
The {{@@}} prefix is to ensure that there _cannot_ be a name-clash with a server object manually created by a user. As these server instances are transient to their nature, that is, will disappear if the corresponding node is removed, they should never be explicitly referred to.

Services should neither list the bootstrap servers, nor dynamic ones, but should simply refer to the monitor.
{code}
[My-Service]
type=service
cluster=CsMonitor
```
{code}
That is, _no_ {{servers}} entry, but instead a {{cluster}} entry that refers to the monitor.

With this setup, MaxScale + Columnstore will just work without a need for any kind of manual reconfiguration, regardless of the changes made to the Columnstore cluster. $acceptance criteria:$",0,0,0,0,0,0,1,504.433,388,35,0.0902062,14,0.0360825,9,0.0231959,7,0.0180412,7,0.0180412
1459,MXS-3108,New Feature,MXS,2020-08-10 08:00:42,,0,Possibility to inject filters into running sessions,"Currently it is at runtime possible change (add or remove) the filters of a service, but it is not possible to change the filters of a running session.

Being able to add/remove a filter to/from a session would not only add architectural clarity, but be a powerful feature that could be used for many purposes.

Currently, when a problem occurs in a production system, the actions available for sorting out the problem are quite limited; basically you can turn on _info_ level logging. Typically it is also quite hard to repeat problems occurring in a production environment. And typically it is also rather straightforward to fix problems once their cause is known. Not finding out the cause is often caused by now having enough information.

With a possibility to inject filters at runtime, there could be particular _debug_ module that would make extensive logging, which could be injected into a running session for collecting information and then be removed. The potential for extensive logging would impose no cost whatsoever when it is not being used.

It would also make it possible to easily test what impact a particular filter has by adding and then removing it from a running session.

A more speculative use-case would the possibility to change the filters of a long-running session during certain time-periods of the day.

Being able to add/remove filters of a running session would implicitly make it trivial to measure the cost of a particular filter in the routing chain.",,"Possibility to inject filters into running sessions $end$ Currently it is at runtime possible change (add or remove) the filters of a service, but it is not possible to change the filters of a running session.

Being able to add/remove a filter to/from a session would not only add architectural clarity, but be a powerful feature that could be used for many purposes.

Currently, when a problem occurs in a production system, the actions available for sorting out the problem are quite limited; basically you can turn on _info_ level logging. Typically it is also quite hard to repeat problems occurring in a production environment. And typically it is also rather straightforward to fix problems once their cause is known. Not finding out the cause is often caused by now having enough information.

With a possibility to inject filters at runtime, there could be particular _debug_ module that would make extensive logging, which could be injected into a running session for collecting information and then be removed. The potential for extensive logging would impose no cost whatsoever when it is not being used.

It would also make it possible to easily test what impact a particular filter has by adding and then removing it from a running session.

A more speculative use-case would the possibility to change the filters of a long-running session during certain time-periods of the day.

Being able to add/remove filters of a running session would implicitly make it trivial to measure the cost of a particular filter in the routing chain. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,2,1,2,0,0,0,,0,850,2,0,0,2020-11-09 12:02:27,Possibility to inject filters into running sessions,"Currently it is at runtime possible change (add or remove) the filters of a service, but it is not possible to change the filters of a running session.

Being able to add/remove a filter to/from a session would not only add architectural clarity, but be a powerful feature that could be used for many purposes.

Currently, when a problem occurs in a production system, the actions available for sorting out the problem are quite limited; basically you can turn on _info_ level logging. Typically it is also quite hard to repeat problems occurring in a production environment. And typically it is also rather straightforward to fix problems once their cause is known. Not finding out the cause is often caused by now having enough information.

With a possibility to inject filters at runtime, there could be particular _debug_ module that would make extensive logging, which could be injected into a running session for collecting information and then be removed. The potential for extensive logging would impose no cost whatsoever when it is not being used.

It would also make it possible to easily test what impact a particular filter has by adding and then removing it from a running session.

A more speculative use-case would the possibility to change the filters of a long-running session during certain time-periods of the day.

Being able to add/remove filters of a running session would implicitly make it trivial to measure the cost of a particular filter in the routing chain.",,0,0,0,0,0.0,"Possibility to inject filters into running sessions $end$ Currently it is at runtime possible change (add or remove) the filters of a service, but it is not possible to change the filters of a running session.

Being able to add/remove a filter to/from a session would not only add architectural clarity, but be a powerful feature that could be used for many purposes.

Currently, when a problem occurs in a production system, the actions available for sorting out the problem are quite limited; basically you can turn on _info_ level logging. Typically it is also quite hard to repeat problems occurring in a production environment. And typically it is also rather straightforward to fix problems once their cause is known. Not finding out the cause is often caused by now having enough information.

With a possibility to inject filters at runtime, there could be particular _debug_ module that would make extensive logging, which could be injected into a running session for collecting information and then be removed. The potential for extensive logging would impose no cost whatsoever when it is not being used.

It would also make it possible to easily test what impact a particular filter has by adding and then removing it from a running session.

A more speculative use-case would the possibility to change the filters of a long-running session during certain time-periods of the day.

Being able to add/remove filters of a running session would implicitly make it trivial to measure the cost of a particular filter in the routing chain. $acceptance criteria:$",0,0,0,0,0,0,1,2188.02,389,35,0.0899743,14,0.0359897,9,0.0231362,7,0.0179949,7,0.0179949
1460,MXS-3111,New Feature,MXS,2020-08-17 06:09:49,,0,MaxGui should show what module a monitor is using,"We have a details page for a monitor but we haven't shown what module a monitor is using.
Monitor module can be shown on the top left corner of monitor details page as illustrated in the attachment.",,"MaxGui should show what module a monitor is using $end$ We have a details page for a monitor but we haven't shown what module a monitor is using.
Monitor module can be shown on the top left corner of monitor details page as illustrated in the attachment. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-08-18 10:02:02,MaxGui should show what module a monitor is using,"We have a details page for a monitor but we haven't shown what module a monitor is using.
Monitor module can be shown on the top left corner of monitor details page as illustrated in the attachment.",,0,0,0,0,0.0,"MaxGui should show what module a monitor is using $end$ We have a details page for a monitor but we haven't shown what module a monitor is using.
Monitor module can be shown on the top left corner of monitor details page as illustrated in the attachment. $acceptance criteria:$",0,0,0,0,0,0,0,27.8667,1,1,1.0,1,1.0,1,1.0,1,1.0,1,1.0
1461,MXS-3116,Task,MXS,2020-08-18 07:21:03,,0,Update MaxScale Docker Image,"* To be based upon centos
* There should be an init process, e.g. tini.
* Useful tools such as less, ping, etc. should be included.",,"Update MaxScale Docker Image $end$ * To be based upon centos
* There should be an init process, e.g. tini.
* Useful tools such as less, ping, etc. should be included. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-08-18 07:21:44,Update MaxScale Docker Image,"* To be based upon centos
* There should be an init process, e.g. tini.
* Useful tools such as less, ping, etc. should be included.",,0,0,0,0,0.0,"Update MaxScale Docker Image $end$ * To be based upon centos
* There should be an init process, e.g. tini.
* Useful tools such as less, ping, etc. should be included. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,390,35,0.0897436,14,0.0358974,9,0.0230769,7,0.0179487,7,0.0179487
1462,MXS-3117,Sub-Task,MXS,2020-08-18 10:26:13,,0,Refactor Maxscale repository creation,move repo creation from internal scripts to reprepro tool on the repo server,,Refactor Maxscale repository creation $end$ move repo creation from internal scripts to reprepro tool on the repo server $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,3,,0,1,0,5,0,0,0,,0,850,1,0,0,2020-08-18 10:26:13,Refactor Maxscale repository creation,move repo creation from internal scripts to reprepro tool on the repo server,,0,0,0,0,0.0,Refactor Maxscale repository creation $end$ move repo creation from internal scripts to reprepro tool on the repo server $acceptance criteria:$,0,0,0,0,0,0,1,0.0,88,1,0.0113636,0,0.0,0,0.0,0,0.0,0,0.0
1463,MXS-3118,Sub-Task,MXS,2020-08-18 10:28:12,,0,Create several users on the repo signature server,mdbe-ci-repo server should have a possibility to use different keys to sign packages and repos. Keys can be the same and can be different for different products or even different versions,,Create several users on the repo signature server $end$ mdbe-ci-repo server should have a possibility to use different keys to sign packages and repos. Keys can be the same and can be different for different products or even different versions $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,5,0,0,0,,0,850,0,0,0,2020-08-18 10:28:12,Create several users on the repo signature server,mdbe-ci-repo server should have a possibility to use different keys to sign packages and repos. Keys can be the same and can be different for different products or even different versions,,0,0,0,0,0.0,Create several users on the repo signature server $end$ mdbe-ci-repo server should have a possibility to use different keys to sign packages and repos. Keys can be the same and can be different for different products or even different versions $acceptance criteria:$,0,0,0,0,0,0,1,0.0,89,1,0.011236,0,0.0,0,0.0,0,0.0,0,0.0
1464,MXS-3129,New Feature,MXS,2020-08-26 03:37:58,,0,Add Switchover to WebGUI,Expand the WebGUI to include a button for switchover,,Add Switchover to WebGUI $end$ Expand the WebGUI to include a button for switchover $acceptance criteria:$,,Todd Stoffel,Todd Stoffel,Major,10,,0,0,0,1,0,0,1,,0,850,0,0,0,2020-09-14 08:08:08,Add Switchover to WebGUI,Expand the WebGUI to include a button for switchover,,0,0,0,0,0.0,Add Switchover to WebGUI $end$ Expand the WebGUI to include a button for switchover $acceptance criteria:$,0,0,0,0,0,0,0,460.5,8,3,0.375,3,0.375,2,0.25,2,0.25,1,0.125
1465,MXS-3138,New Feature,MXS,2020-08-28 06:02:45,,0,Show monitors which doesn't monitor any servers on MaxGUI dashboard,"Currently, the Servers tab on MaxGUI dashboard shows only monitored servers. It should show monitors that have empty relationship as well.",,"Show monitors which doesn't monitor any servers on MaxGUI dashboard $end$ Currently, the Servers tab on MaxGUI dashboard shows only monitored servers. It should show monitors that have empty relationship as well. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,10,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-08-31 08:41:17,Show monitors which doesn't monitor any servers on MaxGUI dashboard,"Currently, the Servers tab on MaxGUI dashboard shows only monitored servers. It should show monitors that have empty relationship as well.",,0,0,0,0,0.0,"Show monitors which doesn't monitor any servers on MaxGUI dashboard $end$ Currently, the Servers tab on MaxGUI dashboard shows only monitored servers. It should show monitors that have empty relationship as well. $acceptance criteria:$",0,0,0,0,0,0,0,74.6333,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
1466,MXS-3139,Task,MXS,2020-08-28 06:20:03,,0,Reorder content of tab navigation on Service details page,"Currently, the first tab is 'Servers & Sessions', second tab is 'Parameters & Diagnostics'.  The current sessions table, current connections graph as well as Router Diagnostics table, their values changes over time. However, only connections graph and sessions table are set to poll data every 10 seconds. 
I think we should group those tables into a tab called 'Session & Diagnostics', so now we can observe changes in one place.  Then parameters table is moved to a tab called 'Parameters & Relationships', this makes sense since we show parameters, relationships of the service and those tables are editable.",,"Reorder content of tab navigation on Service details page $end$ Currently, the first tab is 'Servers & Sessions', second tab is 'Parameters & Diagnostics'.  The current sessions table, current connections graph as well as Router Diagnostics table, their values changes over time. However, only connections graph and sessions table are set to poll data every 10 seconds. 
I think we should group those tables into a tab called 'Session & Diagnostics', so now we can observe changes in one place.  Then parameters table is moved to a tab called 'Parameters & Relationships', this makes sense since we show parameters, relationships of the service and those tables are editable. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-06-17 08:27:57,Reorder content of tab navigation on Service details page,"Currently, the first tab is 'Servers & Sessions', second tab is 'Parameters & Diagnostics'.  The current sessions table, current connections graph as well as Router Diagnostics table, their values changes over time. However, only connections graph and sessions table are set to poll data every 10 seconds. 
I think we should group those tables into a tab called 'Session & Diagnostics', so now we can observe changes in one place.  Then parameters table is moved to a tab called 'Parameters & Relationships', this makes sense since we show parameters, relationships of the service and those tables are editable.",,0,0,0,0,0.0,"Reorder content of tab navigation on Service details page $end$ Currently, the first tab is 'Servers & Sessions', second tab is 'Parameters & Diagnostics'.  The current sessions table, current connections graph as well as Router Diagnostics table, their values changes over time. However, only connections graph and sessions table are set to poll data every 10 seconds. 
I think we should group those tables into a tab called 'Session & Diagnostics', so now we can observe changes in one place.  Then parameters table is moved to a tab called 'Parameters & Relationships', this makes sense since we show parameters, relationships of the service and those tables are editable. $acceptance criteria:$",0,0,0,0,0,0,0,7034.12,3,1,0.333333,1,0.333333,1,0.333333,1,0.333333,1,0.333333
1467,MXS-3145,New Feature,MXS,2020-08-28 11:18:15,,0,Tables having nested rows should be expanded by default,"router diagnostics, monitor diagnostics and maxscale parameters tables are tables having nested rows. Having them expanded by default would be informative for user.
",,"Tables having nested rows should be expanded by default $end$ router diagnostics, monitor diagnostics and maxscale parameters tables are tables having nested rows. Having them expanded by default would be informative for user.
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-08-31 08:39:58,Tables having nested rows should be expanded by default,"router diagnostics, monitor diagnostics and maxscale parameters tables are tables having nested rows. Having them expanded by default would be informative for user.
",,0,0,0,0,0.0,"Tables having nested rows should be expanded by default $end$ router diagnostics, monitor diagnostics and maxscale parameters tables are tables having nested rows. Having them expanded by default would be informative for user.
 $acceptance criteria:$",0,0,0,0,0,0,0,69.35,4,1,0.25,1,0.25,1,0.25,1,0.25,1,0.25
1468,MXS-3146,Task,MXS,2020-08-31 09:50:43,,0,Move avro library installation from build scripts to MaxScale,The avro client library is installed in the build scripts when MaxScale could install it by itself via CMake.,,Move avro library installation from build scripts to MaxScale $end$ The avro client library is installed in the build scripts when MaxScale could install it by itself via CMake. $acceptance criteria:$,,markus makela,markus makela,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-10-26 07:00:12,Move avro library installation from build scripts to MaxScale,The avro client library is installed in the build scripts when MaxScale could install it by itself via CMake.,,0,0,0,0,0.0,Move avro library installation from build scripts to MaxScale $end$ The avro client library is installed in the build scripts when MaxScale could install it by itself via CMake. $acceptance criteria:$,0,0,0,0,0,0,0,1341.15,97,15,0.154639,10,0.103093,8,0.0824742,8,0.0824742,7,0.0721649
1469,MXS-3151,New Feature,MXS,2020-09-02 13:44:50,,0,"Allow to make extra_port the default, not the fallback, for monitors","Right now the extra_port defined for a server will only be used when a connect attempt on the regular port fails

""This allows MaxScale to connect even when max_connections has been reached on the backend server. If this parameter is defined and a connection to the normal port fails, the alternative port is used.""

As even when successfully connected to the regular port monitoring queries on that connection can stall while waiting in the pool-of-threads execution queue it would IMHO make more sense to have the monitor(s) use the extra port by default, to ensure that monitoring queries and switchover/failover actions are handled with priority and don't have to wait in the thread pool execution queue if the thread pool is fully loaded with query execution threads already.

Monitoring queries will still suffer from CPU overload if pool-of-threads was not properly configured to prevent this, but at least they won't additionally suffer from having to wait in the thread pool queue in such cases, too.",,"Allow to make extra_port the default, not the fallback, for monitors $end$ Right now the extra_port defined for a server will only be used when a connect attempt on the regular port fails

""This allows MaxScale to connect even when max_connections has been reached on the backend server. If this parameter is defined and a connection to the normal port fails, the alternative port is used.""

As even when successfully connected to the regular port monitoring queries on that connection can stall while waiting in the pool-of-threads execution queue it would IMHO make more sense to have the monitor(s) use the extra port by default, to ensure that monitoring queries and switchover/failover actions are handled with priority and don't have to wait in the thread pool execution queue if the thread pool is fully loaded with query execution threads already.

Monitoring queries will still suffer from CPU overload if pool-of-threads was not properly configured to prevent this, but at least they won't additionally suffer from having to wait in the thread pool queue in such cases, too. $acceptance criteria:$",,Hartmut Holzgraefe,Hartmut Holzgraefe,Major,9,,0,3,0,1,0,0,0,,0,850,2,0,0,2020-09-28 12:23:29,"Allow to make extra_port the default, not the fallback, for monitors","Right now the extra_port defined for a server will only be used when a connect attempt on the regular port fails

""This allows MaxScale to connect even when max_connections has been reached on the backend server. If this parameter is defined and a connection to the normal port fails, the alternative port is used.""

As even when successfully connected to the regular port monitoring queries on that connection can stall while waiting in the pool-of-threads execution queue it would IMHO make more sense to have the monitor(s) use the extra port by default, to ensure that monitoring queries and switchover/failover actions are handled with priority and don't have to wait in the thread pool execution queue if the thread pool is fully loaded with query execution threads already.

Monitoring queries will still suffer from CPU overload if pool-of-threads was not properly configured to prevent this, but at least they won't additionally suffer from having to wait in the thread pool queue in such cases, too.",,0,0,0,0,0.0,"Allow to make extra_port the default, not the fallback, for monitors $end$ Right now the extra_port defined for a server will only be used when a connect attempt on the regular port fails

""This allows MaxScale to connect even when max_connections has been reached on the backend server. If this parameter is defined and a connection to the normal port fails, the alternative port is used.""

As even when successfully connected to the regular port monitoring queries on that connection can stall while waiting in the pool-of-threads execution queue it would IMHO make more sense to have the monitor(s) use the extra port by default, to ensure that monitoring queries and switchover/failover actions are handled with priority and don't have to wait in the thread pool execution queue if the thread pool is fully loaded with query execution threads already.

Monitoring queries will still suffer from CPU overload if pool-of-threads was not properly configured to prevent this, but at least they won't additionally suffer from having to wait in the thread pool queue in such cases, too. $acceptance criteria:$",0,0,0,0,0,0,0,622.633,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1470,MXS-3154,New Feature,MXS,2020-09-03 10:03:38,,0,Showing MaxScale log on MaxGUI,,,Showing MaxScale log on MaxGUI $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,19,,1,3,2,4,0,0,2,,0,850,3,0,0,2020-09-14 10:49:40,Showing MaxScale log on MaxGUI,,,0,0,0,0,0.0,Showing MaxScale log on MaxGUI $end$ $acceptance criteria:$,0,0,0,0,0,0,1,264.767,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
1471,MXS-3159,New Feature,MXS,2020-09-07 09:23:29,,0,Allowing changing the refresh interval of MaxGUI,"At the moment, MaxGUI uses a hard-coded refresh interval. i.e. 10 seconds for fetching data. This applies to the dashboard pages and pages having graphs. 
There is a refresh-rate UI component made in branch 7. We can re-use that for this. ",,"Allowing changing the refresh interval of MaxGUI $end$ At the moment, MaxGUI uses a hard-coded refresh interval. i.e. 10 seconds for fetching data. This applies to the dashboard pages and pages having graphs. 
There is a refresh-rate UI component made in branch 7. We can re-use that for this.  $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,38,,0,1,0,1,0,4,0,,0,850,0,3,0,2022-04-11 09:48:21,Make refresh interval configurable,"At the moment, MaxGUI uses a hard-coded refresh interval. i.e. 10 seconds for fetching data. This applies to the dashboard pages and pages having graphs. 
There is a refresh-rate UI component made in branch 7. We can re-use that for this. ",,1,0,0,7,0.104167,"Make refresh interval configurable $end$ At the moment, MaxGUI uses a hard-coded refresh interval. i.e. 10 seconds for fetching data. This applies to the dashboard pages and pages having graphs. 
There is a refresh-rate UI component made in branch 7. We can re-use that for this.  $acceptance criteria:$",1,1,1,0,0,0,1,13944.4,6,1,0.166667,1,0.166667,1,0.166667,1,0.166667,1,0.166667
1472,MXS-3178,New Feature,MXS,2020-09-14 09:53:51,,0,Extend REST-API so that it is possible to fetch the log.,,,Extend REST-API so that it is possible to fetch the log. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-09-14 09:53:51,Extend REST-API so that it is possible to fetch the log.,,,0,0,0,0,0.0,Extend REST-API so that it is possible to fetch the log. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,391,35,0.0895141,14,0.0358056,9,0.0230179,7,0.0179028,7,0.0179028
1473,MXS-3180,Sub-Task,MXS,2020-09-14 10:42:46,,0,Switchover unit tests in MaxGUI,,,Switchover unit tests in MaxGUI $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-09-14 10:42:46,Switchover unit tests in MaxGUI,,,0,0,0,0,0.0,Switchover unit tests in MaxGUI $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,7,2,0.285714,2,0.285714,1,0.142857,1,0.142857,1,0.142857
1474,MXS-3182,Task,MXS,2020-09-14 10:58:21,,0,Convert NamedServerFilter to new configuration system,,,Convert NamedServerFilter to new configuration system $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-09-14 10:58:21,Convert NamedServerFilter to new configuration system,,,0,0,0,0,0.0,Convert NamedServerFilter to new configuration system $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,392,35,0.0892857,14,0.0357143,9,0.0229592,7,0.0178571,7,0.0178571
1475,MXS-3189,Task,MXS,2020-09-16 05:43:32,,0,Remove Database Firewall filter,"Was deprecated in 2.6 and should be removed in 2.7.
",,"Remove Database Firewall filter $end$ Was deprecated in 2.6 and should be removed in 2.7.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,14,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-02-14 10:35:54,Remove Database Firewall filter,"Was deprecated in 2.6 and should be removed in 2.7.
",,0,0,0,0,0.0,"Remove Database Firewall filter $end$ Was deprecated in 2.6 and should be removed in 2.7.
 $acceptance criteria:$",0,0,0,0,0,0,0,12388.9,393,35,0.0890585,14,0.0356234,9,0.0229008,7,0.0178117,7,0.0178117
1476,MXS-319,Task,MXS,2015-08-18 23:55:50,MXS-315,0,Testing Tasks for 1.3 release,,,Testing Tasks for 1.3 release $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,12,,0,3,0,2,0,0,12,,0,850,3,0,0,2016-01-12 17:55:04,Testing Tasks for 1.3 release,,,0,0,0,0,0.0,Testing Tasks for 1.3 release $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3521.98,8,4,0.5,1,0.125,0,0.0,0,0.0,0,0.0
1477,MXS-3193,New Feature,MXS,2020-09-16 14:56:19,,0,Allow the binlog router to replicate from Galera cluster with failover.,In MariaDB server 10.5 Galera Cluster can sync the GTID among the nodes.  Please allow the binlog router to use the primary as the master with failover if the primary node changes.,,Allow the binlog router to replicate from Galera cluster with failover. $end$ In MariaDB server 10.5 Galera Cluster can sync the GTID among the nodes.  Please allow the binlog router to use the primary as the master with failover if the primary node changes. $acceptance criteria:$,,Kyle Joiner,Kyle Joiner,Major,17,,0,0,0,8,0,0,0,,0,850,0,0,0,2020-09-28 12:04:53,Allow the binlog router to replicate from Galera cluster with failover.,In MariaDB server 10.5 Galera Cluster can sync the GTID among the nodes.  Please allow the binlog router to use the primary as the master with failover if the primary node changes.,,0,0,0,0,0.0,Allow the binlog router to replicate from Galera cluster with failover. $end$ In MariaDB server 10.5 Galera Cluster can sync the GTID among the nodes.  Please allow the binlog router to use the primary as the master with failover if the primary node changes. $acceptance criteria:$,0,0,0,0,0,0,1,285.133,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1478,MXS-3204,Task,MXS,2020-09-23 11:22:03,,0,Cleanup module loading code,,,Cleanup module loading code $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,8,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-09-28 12:43:08,Cleanup module loading code,,,0,0,0,0,0.0,Cleanup module loading code $end$ $acceptance criteria:$,0,0,0,0,0,0,1,121.35,20,2,0.1,1,0.05,0,0.0,0,0.0,0,0.0
1479,MXS-3208,Task,MXS,2020-09-28 07:30:55,,0,Investigate and implement Mongo API connection sequence.,,,Investigate and implement Mongo API connection sequence. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,1,2,0,0,0,,0,850,0,0,0,2020-09-28 07:30:55,Investigate and implement Mongo API connection sequence.,,,0,0,0,0,0.0,Investigate and implement Mongo API connection sequence. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,394,35,0.0888325,14,0.035533,9,0.0228426,7,0.0177665,7,0.0177665
1480,MXS-3209,Task,MXS,2020-09-28 10:30:59,,0,Write tests for new pinloki featurs,,,Write tests for new pinloki featurs $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Major,9,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-09-28 12:05:06,Write tests for new pinloki featurs,,,0,0,0,0,0.0,Write tests for new pinloki featurs $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1.56667,16,1,0.0625,0,0.0,0,0.0,0,0.0,0,0.0
1481,MXS-3210,Task,MXS,2020-09-28 11:42:05,,0,Add test cases for REST API log data,Add tests for MXS-3178.,,Add test cases for REST API log data $end$ Add tests for MXS-3178. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-09-28 11:42:05,Add test cases for REST API log data,Add tests for MXS-3178.,,0,0,0,0,0.0,Add test cases for REST API log data $end$ Add tests for MXS-3178. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,98,15,0.153061,10,0.102041,8,0.0816327,8,0.0816327,7,0.0714286
1482,MXS-3211,New Feature,MXS,2020-09-28 11:54:06,,0,Allow pinloki failover to be performed by the monitor,"Currently pinloki will switch to a new primary if the current one goes down. This is not correct in the case where the failover is initiated via maxscale (i.e. the old primary does not go down, but becomes a replica).",,"Allow pinloki failover to be performed by the monitor $end$ Currently pinloki will switch to a new primary if the current one goes down. This is not correct in the case where the failover is initiated via maxscale (i.e. the old primary does not go down, but becomes a replica). $acceptance criteria:$",,Niclas Antti,Niclas Antti,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-09-28 12:05:18,Allow pinloki failover to be performed by the monitor,"Currently pinloki will switch to a new primary if the current one goes down. This is not correct in the case where the failover is initiated via maxscale (i.e. the old primary does not go down, but becomes a replica).",,0,0,0,0,0.0,"Allow pinloki failover to be performed by the monitor $end$ Currently pinloki will switch to a new primary if the current one goes down. This is not correct in the case where the failover is initiated via maxscale (i.e. the old primary does not go down, but becomes a replica). $acceptance criteria:$",0,0,0,0,0,0,1,0.183333,17,1,0.0588235,0,0.0,0,0.0,0,0.0,0,0.0
1483,MXS-3216,New Feature,MXS,2020-09-30 07:59:54,,0,Add Columnstore operations to MaxGUI,"From MaxGUI, it should be possible to

* add a new node to a Columnstore cluster
* remove a node from a cluster
* stop the cluster
* start the cluster
* view the status of the cluster

The functionality for performing these is already exposed as call commands on the Columnstore monitor. However, somehow MaxGUI must be made aware that servers monitored by a particular monitor together constitute a Columnstore cluster and that they can be manipulated via that monitor.
",,"Add Columnstore operations to MaxGUI $end$ From MaxGUI, it should be possible to

* add a new node to a Columnstore cluster
* remove a node from a cluster
* stop the cluster
* start the cluster
* view the status of the cluster

The functionality for performing these is already exposed as call commands on the Columnstore monitor. However, somehow MaxGUI must be made aware that servers monitored by a particular monitor together constitute a Columnstore cluster and that they can be manipulated via that monitor.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,19,,0,0,1,2,0,0,0,,0,850,0,0,0,2022-07-04 09:14:55,Add Columnstore operations to MaxGUI,"From MaxGUI, it should be possible to

* add a new node to a Columnstore cluster
* remove a node from a cluster
* stop the cluster
* start the cluster
* view the status of the cluster

The functionality for performing these is already exposed as call commands on the Columnstore monitor. However, somehow MaxGUI must be made aware that servers monitored by a particular monitor together constitute a Columnstore cluster and that they can be manipulated via that monitor.
",,0,0,0,0,0.0,"Add Columnstore operations to MaxGUI $end$ From MaxGUI, it should be possible to

* add a new node to a Columnstore cluster
* remove a node from a cluster
* stop the cluster
* start the cluster
* view the status of the cluster

The functionality for performing these is already exposed as call commands on the Columnstore monitor. However, somehow MaxGUI must be made aware that servers monitored by a particular monitor together constitute a Columnstore cluster and that they can be manipulated via that monitor.
 $acceptance criteria:$",0,0,0,0,0,0,1,15409.2,395,35,0.0886076,14,0.035443,9,0.0227848,7,0.0177215,7,0.0177215
1484,MXS-3217,New Feature,MXS,2020-09-30 08:06:02,,0,Add Columnstore operations to MariaDB Monitor,"Currently the call commands of the monitor using which the Columnstore cluster can be manipulated are synchronous. This makes them ill suited for use from MaxGUI, as without extra effort on MaxGUI's side it would made the UI unresponsive for the duration of the call.

As csmon is obsolete, the functionality should be moved from csmon to mariadbmon, while at the same time making them asynchronous.",,"Add Columnstore operations to MariaDB Monitor $end$ Currently the call commands of the monitor using which the Columnstore cluster can be manipulated are synchronous. This makes them ill suited for use from MaxGUI, as without extra effort on MaxGUI's side it would made the UI unresponsive for the duration of the call.

As csmon is obsolete, the functionality should be moved from csmon to mariadbmon, while at the same time making them asynchronous. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,17,,0,1,1,2,0,2,0,,0,850,0,0,0,2022-04-11 07:47:58,Make Columnstore operations asynchronous,"Currently the call commands of the monitor using which the Columnstore cluster can be manipulated are synchronous. This makes them ill suited for use from MaxGUI, as without extra effort on MaxGUI's side it would made the UI unresponsive for the duration of the call.",,1,1,0,27,0.480769,"Make Columnstore operations asynchronous $end$ Currently the call commands of the monitor using which the Columnstore cluster can be manipulated are synchronous. This makes them ill suited for use from MaxGUI, as without extra effort on MaxGUI's side it would made the UI unresponsive for the duration of the call. $acceptance criteria:$",2,1,1,1,1,1,1,13391.7,396,35,0.0883838,14,0.0353535,9,0.0227273,7,0.0176768,7,0.0176768
1485,MXS-3225,Task,MXS,2020-10-06 16:45:05,,0,System test for pam 2FA,,,System test for pam 2FA $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Major,10,,0,0,0,4,0,0,0,,0,850,0,0,0,2020-10-06 16:45:34,System test for pam 2FA,,,0,0,0,0,0.0,System test for pam 2FA $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,21,2,0.0952381,1,0.047619,0,0.0,0,0.0,0,0.0
1486,MXS-3228,Sub-Task,MXS,2020-10-08 07:00:05,,0,Writing unit test for showing logs feature in maxgui,,,Writing unit test for showing logs feature in maxgui $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,1,0,4,0,0,0,,0,850,1,0,0,2020-10-08 07:00:05,Writing unit test for showing logs feature in maxgui,,,0,0,0,0,0.0,Writing unit test for showing logs feature in maxgui $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,8,2,0.25,2,0.25,1,0.125,1,0.125,1,0.125
1487,MXS-3233,Task,MXS,2020-10-12 10:06:10,,0,Investigate and implement Mongo authentication,,,Investigate and implement Mongo authentication $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,15,,0,0,1,4,0,0,0,,0,850,0,0,0,2020-10-12 10:06:10,Investigate and implement Mongo authentication,,,0,0,0,0,0.0,Investigate and implement Mongo authentication $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,397,36,0.0906801,15,0.0377834,10,0.0251889,8,0.0201511,8,0.0201511
1488,MXS-3236,Task,MXS,2020-10-13 14:19:20,,0,Make system test programs are ASAN clean.,"The system tests programs are not ASAN clean, meaning that if they are built with ASAN enabled they will cause a failure when run under ctest.",,"Make system test programs are ASAN clean. $end$ The system tests programs are not ASAN clean, meaning that if they are built with ASAN enabled they will cause a failure when run under ctest. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,11,,0,0,0,1,0,2,0,,0,850,0,0,0,2020-10-13 14:19:20,System test programs are not ASAN clean.,"The system tests programs are not ASAN clean, meaning that if they are built with ASAN enabled they will cause a failure when run under ctest.",,2,0,0,4,0.0833333,"System test programs are not ASAN clean. $end$ The system tests programs are not ASAN clean, meaning that if they are built with ASAN enabled they will cause a failure when run under ctest. $acceptance criteria:$",2,1,0,0,0,0,0,0.0,398,36,0.0904523,15,0.0376884,10,0.0251256,8,0.0201005,8,0.0201005
1489,MXS-3243,Task,MXS,2020-10-19 11:04:36,,0,Optimize query canonicalization,The current canonicalization code processes the code one byte at a time. The processing speed could be improved by using SIMD instructions. The methods described in [this paper|https://arxiv.org/pdf/1902.08318.pdf] could be adapted for canonicalization of SQL statements.,,Optimize query canonicalization $end$ The current canonicalization code processes the code one byte at a time. The processing speed could be improved by using SIMD instructions. The methods described in [this paper|https://arxiv.org/pdf/1902.08318.pdf] could be adapted for canonicalization of SQL statements. $acceptance criteria:$,,markus makela,markus makela,Major,17,,0,0,0,8,0,0,0,,0,850,0,0,0,2020-11-09 12:23:31,Optimize query canonicalization,The current canonicalization code processes the code one byte at a time. The processing speed could be improved by using SIMD instructions. The methods described in [this paper|https://arxiv.org/pdf/1902.08318.pdf] could be adapted for canonicalization of SQL statements.,,0,0,0,0,0.0,Optimize query canonicalization $end$ The current canonicalization code processes the code one byte at a time. The processing speed could be improved by using SIMD instructions. The methods described in [this paper|https://arxiv.org/pdf/1902.08318.pdf] could be adapted for canonicalization of SQL statements. $acceptance criteria:$,0,0,0,0,0,0,1,505.3,99,15,0.151515,10,0.10101,8,0.0808081,8,0.0808081,7,0.0707071
1490,MXS-3249,Task,MXS,2020-10-21 07:26:16,,0,Fix system test false positives when run under BB.,"A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression.",,"Fix system test false positives when run under BB. $end$ A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,11,,0,0,0,1,0,4,0,,0,850,0,0,0,2020-10-21 07:26:16,Remove instability of masking_mysqltest and masking_user.,The tests almost always fail when run by BB but almost never when run locally.,,2,2,0,45,1.41667,Remove instability of masking_mysqltest and masking_user. $end$ The tests almost always fail when run by BB but almost never when run locally. $acceptance criteria:$,4,1,1,1,1,1,1,0.0,399,37,0.0927318,15,0.037594,10,0.0250627,8,0.0200501,8,0.0200501
1491,MXS-3258,Task,MXS,2020-10-26 06:52:11,,0,Use mxs::Reply for resultset collection,The current code uses {{modutil_count_signal_packets}} which is error prone and unreliable.,,Use mxs::Reply for resultset collection $end$ The current code uses {{modutil_count_signal_packets}} which is error prone and unreliable. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-10-26 06:52:11,Use mxs::Reply for resultset collection,The current code uses {{modutil_count_signal_packets}} which is error prone and unreliable.,,0,0,0,0,0.0,Use mxs::Reply for resultset collection $end$ The current code uses {{modutil_count_signal_packets}} which is error prone and unreliable. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,100,15,0.15,10,0.1,8,0.08,8,0.08,7,0.07
1492,MXS-3259,Task,MXS,2020-10-26 07:43:07,,0,Fix system test false positives when run under BB.,"A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression.",,"Fix system test false positives when run under BB. $end$ A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,11,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-10-26 07:44:02,Fix system test false positives when run under BB.,"A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression.",,0,0,0,0,0.0,"Fix system test false positives when run under BB. $end$ A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,400,38,0.095,16,0.04,11,0.0275,9,0.0225,9,0.0225
1493,MXS-3260,Task,MXS,2020-10-26 07:48:27,,0,Fix system test false positives when run under BB.,"A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression.",,"Fix system test false positives when run under BB. $end$ A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,3,0,0,0,,0,850,0,0,0,2020-10-26 07:49:09,Fix system test false positives when run under BB.,"A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression.",,0,0,0,0,0.0,"Fix system test false positives when run under BB. $end$ A significant number of system tests fail when run under BB, but not when run locally. Figure out why, and fix the test so that a failure signals a true regression. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,401,38,0.0947631,16,0.0399003,11,0.0274314,9,0.0224439,9,0.0224439
1494,MXS-3262,New Feature,MXS,2020-10-27 17:51:20,,0,Add create-backup and restore-from-backup commands to MariaDB-Monitor,"Add manual commands to create a backup from a server and restoring a server from a backup. The backups are stored on a remote server. The ip of the remote server and the storage directory must be given in MaxScale configuration.

Original description:
{quote}It would be extremely helpful to be able to manage and configure backups through MaxScale. User could determine the storage location and frequency of backup. Even include an interface that listed successful backups and alerted on failed backups.

Could roll this into a backup/restore component to automatically rebuild a slave down the line as well. {quote}",,"Add create-backup and restore-from-backup commands to MariaDB-Monitor $end$ Add manual commands to create a backup from a server and restoring a server from a backup. The backups are stored on a remote server. The ip of the remote server and the storage directory must be given in MaxScale configuration.

Original description:
{quote}It would be extremely helpful to be able to manage and configure backups through MaxScale. User could determine the storage location and frequency of backup. Even include an interface that listed successful backups and alerted on failed backups.

Could roll this into a backup/restore component to automatically rebuild a slave down the line as well. {quote} $acceptance criteria:$",,Kathryn Sizemore,Kathryn Sizemore,Minor,32,,0,0,0,5,0,4,0,,0,850,0,1,0,2022-09-12 10:17:10,Backup Scheduler,"It would be extremely helpful to be able to manage and configure backups through MaxScale. User could determine the storage location and frequency of backup. Even include an interface that listed successful backups and alerted on failed backups.


Could roll this into a backup/restore component to automatically rebuild a slave down the line as well. ",,1,2,0,55,0.866667,"Backup Scheduler $end$ It would be extremely helpful to be able to manage and configure backups through MaxScale. User could determine the storage location and frequency of backup. Even include an interface that listed successful backups and alerted on failed backups.


Could roll this into a backup/restore component to automatically rebuild a slave down the line as well.  $acceptance criteria:$",3,1,1,1,1,1,1,16432.4,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1495,MXS-3265,Task,MXS,2020-10-28 16:42:31,,0,Better Connection Pooling and multiplexing - Phase 1,"Larger customers may have thousands or tens of thousands of connections to the database. To mitigate risk to the database from many active connections and still allow larger in-memory buffers, we should have maxscale pool connections to the backend databases.

Connection pooling could be implemented by multiplexing N client connections over M connections to a backend MariaDB server. After a client connection request completes successful authentication with a backend MariaDB server, the proxy severs the link between the backend connection and client connection and parks it in a connection pool for the backend server. 

The server connection pool size should be configurable or dynamic, and typically a small number. Forks of MaxScale have leveraged the persistent connections feature to implement the server connection pool. When receiving a query on a client connection, MaxScale can pick a backend connection in the pool, link it with the client connection, and forward the query to the backend MariaDB server. MaxScale understands the transaction context of a client session and therefore it knows to keep the linked backend connection until transaction commits. The link must be kept and used for forwarding the query result back to the client.

One challenge in this connection pooling implementation is knowing when to unlink and return the backend connection to the pool. A query response consists of one or more MariaDB packets. Because MaxScale keeps one-to-one link between a client connection and a backend connection, it just forwards response packets as they come. In connection pooling mode, unlinking a backend connection prematurely would cause the client to wait indefinitely for the complete set of MariaDB packets. For correct query response forwarding, consider implementing packets by following the MariaDB client server protocol for COM_QUERY_RESPONSE. That way, MaxScale will not unlink a backend connection until it has seen the complete MariaDB packets of a query response. Aside from forwarding responses, it would allow us to measure query response size for monitoring.",,"Better Connection Pooling and multiplexing - Phase 1 $end$ Larger customers may have thousands or tens of thousands of connections to the database. To mitigate risk to the database from many active connections and still allow larger in-memory buffers, we should have maxscale pool connections to the backend databases.

Connection pooling could be implemented by multiplexing N client connections over M connections to a backend MariaDB server. After a client connection request completes successful authentication with a backend MariaDB server, the proxy severs the link between the backend connection and client connection and parks it in a connection pool for the backend server. 

The server connection pool size should be configurable or dynamic, and typically a small number. Forks of MaxScale have leveraged the persistent connections feature to implement the server connection pool. When receiving a query on a client connection, MaxScale can pick a backend connection in the pool, link it with the client connection, and forward the query to the backend MariaDB server. MaxScale understands the transaction context of a client session and therefore it knows to keep the linked backend connection until transaction commits. The link must be kept and used for forwarding the query result back to the client.

One challenge in this connection pooling implementation is knowing when to unlink and return the backend connection to the pool. A query response consists of one or more MariaDB packets. Because MaxScale keeps one-to-one link between a client connection and a backend connection, it just forwards response packets as they come. In connection pooling mode, unlinking a backend connection prematurely would cause the client to wait indefinitely for the complete set of MariaDB packets. For correct query response forwarding, consider implementing packets by following the MariaDB client server protocol for COM_QUERY_RESPONSE. That way, MaxScale will not unlink a backend connection until it has seen the complete MariaDB packets of a query response. Aside from forwarding responses, it would allow us to measure query response size for monitoring. $acceptance criteria:$",,Manjot Singh,Manjot Singh,Major,22,,0,4,2,3,0,1,0,,0,850,0,0,0,2021-01-18 11:51:33,Better Connection Pooling and multiplexing,"Larger customers may have thousands or tens of thousands of connections to the database. To mitigate risk to the database from many active connections and still allow larger in-memory buffers, we should have maxscale pool connections to the backend databases.

Connection pooling could be implemented by multiplexing N client connections over M connections to a backend MariaDB server. After a client connection request completes successful authentication with a backend MariaDB server, the proxy severs the link between the backend connection and client connection and parks it in a connection pool for the backend server. 

The server connection pool size should be configurable or dynamic, and typically a small number. Forks of MaxScale have leveraged the persistent connections feature to implement the server connection pool. When receiving a query on a client connection, MaxScale can pick a backend connection in the pool, link it with the client connection, and forward the query to the backend MariaDB server. MaxScale understands the transaction context of a client session and therefore it knows to keep the linked backend connection until transaction commits. The link must be kept and used for forwarding the query result back to the client.

One challenge in this connection pooling implementation is knowing when to unlink and return the backend connection to the pool. A query response consists of one or more MariaDB packets. Because MaxScale keeps one-to-one link between a client connection and a backend connection, it just forwards response packets as they come. In connection pooling mode, unlinking a backend connection prematurely would cause the client to wait indefinitely for the complete set of MariaDB packets. For correct query response forwarding, consider implementing packets by following the MariaDB client server protocol for COM_QUERY_RESPONSE. That way, MaxScale will not unlink a backend connection until it has seen the complete MariaDB packets of a query response. Aside from forwarding responses, it would allow us to measure query response size for monitoring.",,1,0,0,3,0.00911854,"Better Connection Pooling and multiplexing $end$ Larger customers may have thousands or tens of thousands of connections to the database. To mitigate risk to the database from many active connections and still allow larger in-memory buffers, we should have maxscale pool connections to the backend databases.

Connection pooling could be implemented by multiplexing N client connections over M connections to a backend MariaDB server. After a client connection request completes successful authentication with a backend MariaDB server, the proxy severs the link between the backend connection and client connection and parks it in a connection pool for the backend server. 

The server connection pool size should be configurable or dynamic, and typically a small number. Forks of MaxScale have leveraged the persistent connections feature to implement the server connection pool. When receiving a query on a client connection, MaxScale can pick a backend connection in the pool, link it with the client connection, and forward the query to the backend MariaDB server. MaxScale understands the transaction context of a client session and therefore it knows to keep the linked backend connection until transaction commits. The link must be kept and used for forwarding the query result back to the client.

One challenge in this connection pooling implementation is knowing when to unlink and return the backend connection to the pool. A query response consists of one or more MariaDB packets. Because MaxScale keeps one-to-one link between a client connection and a backend connection, it just forwards response packets as they come. In connection pooling mode, unlinking a backend connection prematurely would cause the client to wait indefinitely for the complete set of MariaDB packets. For correct query response forwarding, consider implementing packets by following the MariaDB client server protocol for COM_QUERY_RESPONSE. That way, MaxScale will not unlink a backend connection until it has seen the complete MariaDB packets of a query response. Aside from forwarding responses, it would allow us to measure query response size for monitoring. $acceptance criteria:$",1,1,0,0,0,0,1,1963.15,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1496,MXS-3268,New Feature,MXS,2020-10-29 08:47:32,,0,Maxscale should auto detect master if there is none in cluster,"Hi Team, 

Recently, customer have observed one scenario where master was down due to some reason and maxscale said that ""Master has failed. If master status does not change in 4 monitor passes, failover begins."" but before failover happened, master started and maxscale set it to ""Slave, Running"" .
{code}
2020-10-17 01:14:19 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:14:19 notice : Server changed state: node2[10.232.86.133:6603]: master_down. [Master, Running] -> [Down]
2020-10-17 01:14:19 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
2020-10-17 01:14:44 warning: [mariadbmon] The current master server 'node2' is no longer valid because it is in read-only mode, but there is no valid alternative to swap to.
2020-10-17 01:14:44 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:14:44 notice : Server changed state: node2[10.232.86.133:6603]: slave_up. [Down] -> [Slave, Running]
{code}

So now they had three node with ""Slave, Running"" and Maxscale didn't make any of the server to master. Finally, they had to restart node2 server and then failover happened. 

{code}
2020-10-17 01:54:01 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:54:01 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:54:01 notice : Server changed state: node2[10.232.86.133:6603]: slave_down. [Slave, Running] -> [Down]
2020-10-17 01:54:01 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
...
2020-10-17 01:54:21 notice : [mariadbmon] Selecting a server to promote and replace 'node2'. Candidates are: 'node1', 'node3'.
2020-10-17 01:54:21 notice : [mariadbmon] Selected 'node1'.
2020-10-17 01:54:21 notice : [mariadbmon] Performing automatic failover to replace failed master 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] Redirecting 'node3' to replicate from 'node1' instead of 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] All redirects successful.
2020-10-17 01:54:22 notice : [mariadbmon] All redirected slaves successfully started replication from 'node1'.
2020-10-17 01:54:22 notice : [mariadbmon] Failover 'node2' -> 'node1' performed.
{code}

Can we add some functionality in maxscale which can check about master server frequently ?
and if there is no master then based on GTID, it can decide which has latest ID and make it master and other nodes to slaves?  ",,"Maxscale should auto detect master if there is none in cluster $end$ Hi Team, 

Recently, customer have observed one scenario where master was down due to some reason and maxscale said that ""Master has failed. If master status does not change in 4 monitor passes, failover begins."" but before failover happened, master started and maxscale set it to ""Slave, Running"" .
{code}
2020-10-17 01:14:19 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:14:19 notice : Server changed state: node2[10.232.86.133:6603]: master_down. [Master, Running] -> [Down]
2020-10-17 01:14:19 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
2020-10-17 01:14:44 warning: [mariadbmon] The current master server 'node2' is no longer valid because it is in read-only mode, but there is no valid alternative to swap to.
2020-10-17 01:14:44 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:14:44 notice : Server changed state: node2[10.232.86.133:6603]: slave_up. [Down] -> [Slave, Running]
{code}

So now they had three node with ""Slave, Running"" and Maxscale didn't make any of the server to master. Finally, they had to restart node2 server and then failover happened. 

{code}
2020-10-17 01:54:01 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:54:01 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:54:01 notice : Server changed state: node2[10.232.86.133:6603]: slave_down. [Slave, Running] -> [Down]
2020-10-17 01:54:01 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
...
2020-10-17 01:54:21 notice : [mariadbmon] Selecting a server to promote and replace 'node2'. Candidates are: 'node1', 'node3'.
2020-10-17 01:54:21 notice : [mariadbmon] Selected 'node1'.
2020-10-17 01:54:21 notice : [mariadbmon] Performing automatic failover to replace failed master 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] Redirecting 'node3' to replicate from 'node1' instead of 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] All redirects successful.
2020-10-17 01:54:22 notice : [mariadbmon] All redirected slaves successfully started replication from 'node1'.
2020-10-17 01:54:22 notice : [mariadbmon] Failover 'node2' -> 'node1' performed.
{code}

Can we add some functionality in maxscale which can check about master server frequently ?
and if there is no master then based on GTID, it can decide which has latest ID and make it master and other nodes to slaves?   $acceptance criteria:$",,Nilnandan Joshi,Nilnandan Joshi,Major,17,,0,3,0,2,0,0,0,,0,850,0,0,0,2021-03-15 11:26:04,Maxscale should auto detect master if there is none in cluster,"Hi Team, 

Recently, customer have observed one scenario where master was down due to some reason and maxscale said that ""Master has failed. If master status does not change in 4 monitor passes, failover begins."" but before failover happened, master started and maxscale set it to ""Slave, Running"" .
{code}
2020-10-17 01:14:19 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:14:19 notice : Server changed state: node2[10.232.86.133:6603]: master_down. [Master, Running] -> [Down]
2020-10-17 01:14:19 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
2020-10-17 01:14:44 warning: [mariadbmon] The current master server 'node2' is no longer valid because it is in read-only mode, but there is no valid alternative to swap to.
2020-10-17 01:14:44 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:14:44 notice : Server changed state: node2[10.232.86.133:6603]: slave_up. [Down] -> [Slave, Running]
{code}

So now they had three node with ""Slave, Running"" and Maxscale didn't make any of the server to master. Finally, they had to restart node2 server and then failover happened. 

{code}
2020-10-17 01:54:01 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:54:01 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:54:01 notice : Server changed state: node2[10.232.86.133:6603]: slave_down. [Slave, Running] -> [Down]
2020-10-17 01:54:01 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
...
2020-10-17 01:54:21 notice : [mariadbmon] Selecting a server to promote and replace 'node2'. Candidates are: 'node1', 'node3'.
2020-10-17 01:54:21 notice : [mariadbmon] Selected 'node1'.
2020-10-17 01:54:21 notice : [mariadbmon] Performing automatic failover to replace failed master 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] Redirecting 'node3' to replicate from 'node1' instead of 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] All redirects successful.
2020-10-17 01:54:22 notice : [mariadbmon] All redirected slaves successfully started replication from 'node1'.
2020-10-17 01:54:22 notice : [mariadbmon] Failover 'node2' -> 'node1' performed.
{code}

Can we add some functionality in maxscale which can check about master server frequently ?
and if there is no master then based on GTID, it can decide which has latest ID and make it master and other nodes to slaves?  ",,0,0,0,0,0.0,"Maxscale should auto detect master if there is none in cluster $end$ Hi Team, 

Recently, customer have observed one scenario where master was down due to some reason and maxscale said that ""Master has failed. If master status does not change in 4 monitor passes, failover begins."" but before failover happened, master started and maxscale set it to ""Slave, Running"" .
{code}
2020-10-17 01:14:19 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:14:19 notice : Server changed state: node2[10.232.86.133:6603]: master_down. [Master, Running] -> [Down]
2020-10-17 01:14:19 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
2020-10-17 01:14:44 warning: [mariadbmon] The current master server 'node2' is no longer valid because it is in read-only mode, but there is no valid alternative to swap to.
2020-10-17 01:14:44 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:14:44 notice : Server changed state: node2[10.232.86.133:6603]: slave_up. [Down] -> [Slave, Running]
{code}

So now they had three node with ""Slave, Running"" and Maxscale didn't make any of the server to master. Finally, they had to restart node2 server and then failover happened. 

{code}
2020-10-17 01:54:01 error : Monitor was unable to connect to server node2[10.232.86.133:6603] : ''
2020-10-17 01:54:01 error : [mariadbmon] No Master can be determined. Last known was 10.232.86.133:6603
2020-10-17 01:54:01 notice : Server changed state: node2[10.232.86.133:6603]: slave_down. [Slave, Running] -> [Down]
2020-10-17 01:54:01 warning: [mariadbmon] Master has failed. If master status does not change in 4 monitor passes, failover begins.
...
2020-10-17 01:54:21 notice : [mariadbmon] Selecting a server to promote and replace 'node2'. Candidates are: 'node1', 'node3'.
2020-10-17 01:54:21 notice : [mariadbmon] Selected 'node1'.
2020-10-17 01:54:21 notice : [mariadbmon] Performing automatic failover to replace failed master 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] Redirecting 'node3' to replicate from 'node1' instead of 'node2'.
2020-10-17 01:54:21 notice : [mariadbmon] All redirects successful.
2020-10-17 01:54:22 notice : [mariadbmon] All redirected slaves successfully started replication from 'node1'.
2020-10-17 01:54:22 notice : [mariadbmon] Failover 'node2' -> 'node1' performed.
{code}

Can we add some functionality in maxscale which can check about master server frequently ?
and if there is no master then based on GTID, it can decide which has latest ID and make it master and other nodes to slaves?   $acceptance criteria:$",0,0,0,0,0,0,1,3290.63,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1497,MXS-3269,New Feature,MXS,2020-10-29 15:01:14,,0,Make it possible to change at runtime the number of threads used by MaxScale,"MaxScale can use an arbitrary number of worker or routing threads. The number of threads that MaxScale should use can be specified in the configuration file, either as a fixed number or as {{auto}} in which case MaxScale will use as many threads as there are CPU cores in the system. It never makes sense to use _more_ threads than what there are CPU cores in the machine MaxScale is running on. Currently the number of threads cannot be changed at runtime.

Being able to change the number of threads used by MaxScale has not been particularly important as the number of CPU cores will not change when you are running on real hardware. However, since MaxScale increasingly is run in a docker container that assumption is no longer true.

Using [docker update|https://docs.docker.com/engine/reference/commandline/update/] it is possible to change at runtime the number of CPUs available to a particular container. Thus, the number of CPUs available to MaxScale can no longer be assumed to be fixed after startup.

It would be a powerful feature if MaxScale would automatically adjust to the number of CPUs available to the docker container it is running in.",,"Make it possible to change at runtime the number of threads used by MaxScale $end$ MaxScale can use an arbitrary number of worker or routing threads. The number of threads that MaxScale should use can be specified in the configuration file, either as a fixed number or as {{auto}} in which case MaxScale will use as many threads as there are CPU cores in the system. It never makes sense to use _more_ threads than what there are CPU cores in the machine MaxScale is running on. Currently the number of threads cannot be changed at runtime.

Being able to change the number of threads used by MaxScale has not been particularly important as the number of CPU cores will not change when you are running on real hardware. However, since MaxScale increasingly is run in a docker container that assumption is no longer true.

Using [docker update|https://docs.docker.com/engine/reference/commandline/update/] it is possible to change at runtime the number of CPUs available to a particular container. Thus, the number of CPUs available to MaxScale can no longer be assumed to be fixed after startup.

It would be a powerful feature if MaxScale would automatically adjust to the number of CPUs available to the docker container it is running in. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,18,,1,0,2,6,0,0,0,,0,850,0,0,0,2022-09-12 10:19:59,Make it possible to change at runtime the number of threads used by MaxScale,"MaxScale can use an arbitrary number of worker or routing threads. The number of threads that MaxScale should use can be specified in the configuration file, either as a fixed number or as {{auto}} in which case MaxScale will use as many threads as there are CPU cores in the system. It never makes sense to use _more_ threads than what there are CPU cores in the machine MaxScale is running on. Currently the number of threads cannot be changed at runtime.

Being able to change the number of threads used by MaxScale has not been particularly important as the number of CPU cores will not change when you are running on real hardware. However, since MaxScale increasingly is run in a docker container that assumption is no longer true.

Using [docker update|https://docs.docker.com/engine/reference/commandline/update/] it is possible to change at runtime the number of CPUs available to a particular container. Thus, the number of CPUs available to MaxScale can no longer be assumed to be fixed after startup.

It would be a powerful feature if MaxScale would automatically adjust to the number of CPUs available to the docker container it is running in.",,0,0,0,0,0.0,"Make it possible to change at runtime the number of threads used by MaxScale $end$ MaxScale can use an arbitrary number of worker or routing threads. The number of threads that MaxScale should use can be specified in the configuration file, either as a fixed number or as {{auto}} in which case MaxScale will use as many threads as there are CPU cores in the system. It never makes sense to use _more_ threads than what there are CPU cores in the machine MaxScale is running on. Currently the number of threads cannot be changed at runtime.

Being able to change the number of threads used by MaxScale has not been particularly important as the number of CPU cores will not change when you are running on real hardware. However, since MaxScale increasingly is run in a docker container that assumption is no longer true.

Using [docker update|https://docs.docker.com/engine/reference/commandline/update/] it is possible to change at runtime the number of CPUs available to a particular container. Thus, the number of CPUs available to MaxScale can no longer be assumed to be fixed after startup.

It would be a powerful feature if MaxScale would automatically adjust to the number of CPUs available to the docker container it is running in. $acceptance criteria:$",0,0,0,0,0,0,1,16387.3,402,38,0.0945274,16,0.039801,11,0.0273632,9,0.0223881,9,0.0223881
1498,MXS-3277,Task,MXS,2020-11-02 10:46:43,,0,install Kerberos for system-tests using MDBCI ,"currenty kerberos-related tests call yum via ssh to install Kerberos related packages to the Maxscale and Backend nodes.
now MDBCI use support for Kerberos packages installation, so, test should simply call it instead of direct interaction with yum (allows to use any distro)",,"install Kerberos for system-tests using MDBCI  $end$ currenty kerberos-related tests call yum via ssh to install Kerberos related packages to the Maxscale and Backend nodes.
now MDBCI use support for Kerberos packages installation, so, test should simply call it instead of direct interaction with yum (allows to use any distro) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2020-11-09 12:24:22,install Kerberos for system-tests using MDBCI ,"currenty kerberos-related tests call yum via ssh to install Kerberos related packages to the Maxscale and Backend nodes.
now MDBCI use support for Kerberos packages installation, so, test should simply call it instead of direct interaction with yum (allows to use any distro)",,0,0,0,0,0.0,"install Kerberos for system-tests using MDBCI  $end$ currenty kerberos-related tests call yum via ssh to install Kerberos related packages to the Maxscale and Backend nodes.
now MDBCI use support for Kerberos packages installation, so, test should simply call it instead of direct interaction with yum (allows to use any distro) $acceptance criteria:$",0,0,0,0,0,0,0,169.617,90,1,0.0111111,0,0.0,0,0.0,0,0.0,0,0.0
1499,MXS-3290,Task,MXS,2020-11-06 07:28:44,MXS-3387,0,Implement find command.,,,Implement find command. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,14,,0,1,0,5,0,0,0,,0,850,1,0,0,2020-11-09 10:58:32,Implement find command.,,,0,0,0,0,0.0,Implement find command. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,75.4833,403,38,0.0942928,16,0.0397022,11,0.0272953,9,0.0223325,9,0.0223325
1500,MXS-3300,Task,MXS,2020-11-16 13:14:20,,0,Make it possible to provide protocol specific configuration parameters,,,Make it possible to provide protocol specific configuration parameters $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-11-24 09:22:32,Make it possible to provide protocol specific configuration parameters,,,0,0,0,0,0.0,Make it possible to provide protocol specific configuration parameters $end$ $acceptance criteria:$,0,0,0,0,0,0,0,188.133,404,38,0.0940594,16,0.039604,11,0.0272277,9,0.0222772,9,0.0222772
1501,MXS-3305,Task,MXS,2020-11-24 06:51:48,,0,Session level log configuration,Make {{log_info}} and other logging levels a configurable part of of the session. This allows verbose logging to be enabled for only certain sessions.,,Session level log configuration $end$ Make {{log_info}} and other logging levels a configurable part of of the session. This allows verbose logging to be enabled for only certain sessions. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2020-11-24 09:20:28,Session level log configuration,Make {{log_info}} and other logging levels a configurable part of of the session. This allows verbose logging to be enabled for only certain sessions.,,0,0,0,0,0.0,Session level log configuration $end$ Make {{log_info}} and other logging levels a configurable part of of the session. This allows verbose logging to be enabled for only certain sessions. $acceptance criteria:$,0,0,0,0,0,0,0,2.46667,101,15,0.148515,10,0.0990099,8,0.0792079,8,0.0792079,7,0.0693069
1502,MXS-3306,Task,MXS,2020-11-24 10:30:43,,0,Check that test.repl->N is not misused.,"In the test  environment {{test.repl->N}} means the number of server VMs available, not the number of servers used by the test.  A misassumption of its meaning may mean that a test fails if it is run after a heavy test that uses 15 VMs instead of the usual 4.",,"Check that test.repl->N is not misused. $end$ In the test  environment {{test.repl->N}} means the number of server VMs available, not the number of servers used by the test.  A misassumption of its meaning may mean that a test fails if it is run after a heavy test that uses 15 VMs instead of the usual 4. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-11-24 10:30:43,Check that test.repl->N is not misused.,"In the test  environment {{test.repl->N}} means the number of server VMs available, not the number of servers used by the test.  A misassumption of its meaning may mean that a test fails if it is run after a heavy test that uses 15 VMs instead of the usual 4.",,0,0,0,0,0.0,"Check that test.repl->N is not misused. $end$ In the test  environment {{test.repl->N}} means the number of server VMs available, not the number of servers used by the test.  A misassumption of its meaning may mean that a test fails if it is run after a heavy test that uses 15 VMs instead of the usual 4. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,405,38,0.0938272,16,0.0395062,11,0.0271605,9,0.0222222,9,0.0222222
1503,MXS-334,New Feature,MXS,2015-08-26 01:43:01,,0,Enable Pam.d Support,"We currently use pam.d to authenticate users against are database, so have one central user management solution.  MariaDB plugin works great for this, but would be nice if could use the same users while utilizing Maxscale.",,"Enable Pam.d Support $end$ We currently use pam.d to authenticate users against are database, so have one central user management solution.  MariaDB plugin works great for this, but would be nice if could use the same users while utilizing Maxscale. $acceptance criteria:$",,kevin oswald,kevin oswald,Major,8,,3,3,10,3,0,0,0,,0,850,2,0,0,2017-06-14 09:25:30,Enable Pam.d Support,"We currently use pam.d to authenticate users against are database, so have one central user management solution.  MariaDB plugin works great for this, but would be nice if could use the same users while utilizing Maxscale.",,0,0,0,0,0.0,"Enable Pam.d Support $end$ We currently use pam.d to authenticate users against are database, so have one central user management solution.  MariaDB plugin works great for this, but would be nice if could use the same users while utilizing Maxscale. $acceptance criteria:$",0,0,0,0,0,0,1,15799.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1504,MXS-3340,Task,MXS,2020-12-16 06:55:37,,0,Specific grants should be used when running system tests,"Currently the users used when running the system tests have
{code}
GRANT ALL PRIVILEGES ON *.* TO ...
{code}

The users used by the services and monitors should have exactly the privileges listed in the documentation.",,"Specific grants should be used when running system tests $end$ Currently the users used when running the system tests have
{code}
GRANT ALL PRIVILEGES ON *.* TO ...
{code}

The users used by the services and monitors should have exactly the privileges listed in the documentation. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-01-04 09:12:07,Specific grants should be used when running system tests,"Currently the users used when running the system tests have
{code}
GRANT ALL PRIVILEGES ON *.* TO ...
{code}

The users used by the services and monitors should have exactly the privileges listed in the documentation.",,0,0,0,0,0.0,"Specific grants should be used when running system tests $end$ Currently the users used when running the system tests have
{code}
GRANT ALL PRIVILEGES ON *.* TO ...
{code}

The users used by the services and monitors should have exactly the privileges listed in the documentation. $acceptance criteria:$",0,0,0,0,0,0,0,458.267,406,38,0.0935961,16,0.0394089,11,0.0270936,9,0.0221675,9,0.0221675
1505,MXS-3355,Task,MXS,2020-12-30 07:59:16,,0,Take MariaDB monitor specific user into use in MariaDB Monitor tests.,,,Take MariaDB monitor specific user into use in MariaDB Monitor tests. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2020-12-30 07:59:16,Take MariaDB monitor specific user into use in MariaDB Monitor tests.,,,0,0,0,0,0.0,Take MariaDB monitor specific user into use in MariaDB Monitor tests. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,407,38,0.0933661,16,0.039312,11,0.027027,9,0.022113,9,0.022113
1506,MXS-3356,Task,MXS,2020-12-30 08:01:57,,0,Take Galera monitor specific user into use in tests using Galera.,,,Take Galera monitor specific user into use in tests using Galera. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2020-12-30 08:01:57,Take Galera monitor specific user into use in tests using Galera.,,,0,0,0,0,0.0,Take Galera monitor specific user into use in tests using Galera. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,408,38,0.0931373,16,0.0392157,11,0.0269608,9,0.0220588,9,0.0220588
1507,MXS-3357,New Feature,MXS,2020-12-30 21:06:35,,0,Priority of Master Promotion,"In several topologies , there are servers that are more suited to be master if possible and others that might be a DR or we would only like to be a master as a last resort. 

I would like the ability to prioritize the servers that are reviewed for master eligibility in a failover. 
For example, If I am currently running master on MariaDB-1 and I also have MariaDB-2, MariaDB-3, and MariaDB-DR, In the event of failover, I want MariaDB-2 and then MariaDB-3 to be checked for Master viability before ever checking MariaDB-DR. I don't want to exclude DR entirely with servers_no_promotion, but I want it to be the very last resort in the event 1, 2, & 3 are all down.

Similarly, I might have more resources allocated to MariaDB1 & MariaDB2 but have a smaller server running as a backup slave on MariaDB3 (not an ideal scenario, but not uncommon with limited budgets). In a failover scenario, I might want to always prioritize failing over to 1 & 2 before resorting to 3. ",,"Priority of Master Promotion $end$ In several topologies , there are servers that are more suited to be master if possible and others that might be a DR or we would only like to be a master as a last resort. 

I would like the ability to prioritize the servers that are reviewed for master eligibility in a failover. 
For example, If I am currently running master on MariaDB-1 and I also have MariaDB-2, MariaDB-3, and MariaDB-DR, In the event of failover, I want MariaDB-2 and then MariaDB-3 to be checked for Master viability before ever checking MariaDB-DR. I don't want to exclude DR entirely with servers_no_promotion, but I want it to be the very last resort in the event 1, 2, & 3 are all down.

Similarly, I might have more resources allocated to MariaDB1 & MariaDB2 but have a smaller server running as a backup slave on MariaDB3 (not an ideal scenario, but not uncommon with limited budgets). In a failover scenario, I might want to always prioritize failing over to 1 & 2 before resorting to 3.  $acceptance criteria:$",,Kathryn Sizemore,Kathryn Sizemore,Minor,13,,0,2,0,1,0,0,0,,0,850,2,0,0,2021-03-29 10:33:22,Priority of Master Promotion,"In several topologies , there are servers that are more suited to be master if possible and others that might be a DR or we would only like to be a master as a last resort. 

I would like the ability to prioritize the servers that are reviewed for master eligibility in a failover. 
For example, If I am currently running master on MariaDB-1 and I also have MariaDB-2, MariaDB-3, and MariaDB-DR, In the event of failover, I want MariaDB-2 and then MariaDB-3 to be checked for Master viability before ever checking MariaDB-DR. I don't want to exclude DR entirely with servers_no_promotion, but I want it to be the very last resort in the event 1, 2, & 3 are all down.

Similarly, I might have more resources allocated to MariaDB1 & MariaDB2 but have a smaller server running as a backup slave on MariaDB3 (not an ideal scenario, but not uncommon with limited budgets). In a failover scenario, I might want to always prioritize failing over to 1 & 2 before resorting to 3. ",,0,0,0,0,0.0,"Priority of Master Promotion $end$ In several topologies , there are servers that are more suited to be master if possible and others that might be a DR or we would only like to be a master as a last resort. 

I would like the ability to prioritize the servers that are reviewed for master eligibility in a failover. 
For example, If I am currently running master on MariaDB-1 and I also have MariaDB-2, MariaDB-3, and MariaDB-DR, In the event of failover, I want MariaDB-2 and then MariaDB-3 to be checked for Master viability before ever checking MariaDB-DR. I don't want to exclude DR entirely with servers_no_promotion, but I want it to be the very last resort in the event 1, 2, & 3 are all down.

Similarly, I might have more resources allocated to MariaDB1 & MariaDB2 but have a smaller server running as a backup slave on MariaDB3 (not an ideal scenario, but not uncommon with limited budgets). In a failover scenario, I might want to always prioritize failing over to 1 & 2 before resorting to 3.  $acceptance criteria:$",0,0,0,0,0,0,0,2125.43,1,1,1.0,1,1.0,1,1.0,1,1.0,1,1.0
1508,MXS-3361,Task,MXS,2021-01-04 05:49:22,,0,Rewrite ignorable query processing,The current mechanism is used for both COM_CHANGE_USER response handling as well as handling of any queries whose result should not be sent up the routing chain. Mixing these two isn't optimal as it results in relatively confusing code.,,Rewrite ignorable query processing $end$ The current mechanism is used for both COM_CHANGE_USER response handling as well as handling of any queries whose result should not be sent up the routing chain. Mixing these two isn't optimal as it results in relatively confusing code. $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,0,2,1,0,0,0,,0,850,0,0,0,2021-01-04 06:03:01,Rewrite ignorable query processing,The current mechanism is used for both COM_CHANGE_USER response handling as well as handling of any queries whose result should not be sent up the routing chain. Mixing these two isn't optimal as it results in relatively confusing code.,,0,0,0,0,0.0,Rewrite ignorable query processing $end$ The current mechanism is used for both COM_CHANGE_USER response handling as well as handling of any queries whose result should not be sent up the routing chain. Mixing these two isn't optimal as it results in relatively confusing code. $acceptance criteria:$,0,0,0,0,0,0,0,0.216667,102,15,0.147059,10,0.0980392,8,0.0784314,8,0.0784314,7,0.0686274
1509,MXS-3362,Task,MXS,2021-01-04 07:31:32,,0,Make session commands a generic mechanism,"The max_sescmd_history, disable_sescmd_history and prune_sescmd_history parameters can be made into common service parameters. The act of replaying the history can be done completely on the protocol level without the need for routers to be involved.",,"Make session commands a generic mechanism $end$ The max_sescmd_history, disable_sescmd_history and prune_sescmd_history parameters can be made into common service parameters. The act of replaying the history can be done completely on the protocol level without the need for routers to be involved. $acceptance criteria:$",,markus makela,markus makela,Major,11,,0,0,1,2,0,0,1,,0,850,0,0,0,2021-01-04 07:32:07,Make session commands a generic mechanism,"The max_sescmd_history, disable_sescmd_history and prune_sescmd_history parameters can be made into common service parameters. The act of replaying the history can be done completely on the protocol level without the need for routers to be involved.",,0,0,0,0,0.0,"Make session commands a generic mechanism $end$ The max_sescmd_history, disable_sescmd_history and prune_sescmd_history parameters can be made into common service parameters. The act of replaying the history can be done completely on the protocol level without the need for routers to be involved. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,103,15,0.145631,10,0.0970874,8,0.0776699,8,0.0776699,7,0.0679612
1510,MXS-3367,Task,MXS,2021-01-07 10:46:58,MXS-3387,0,Create system tests for mongodbclient,,,Create system tests for mongodbclient $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,5,0,0,0,,0,850,0,0,0,2021-01-07 10:46:58,Create system tests for mongodbclient,,,0,0,0,0,0.0,Create system tests for mongodbclient $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,409,38,0.0929095,16,0.0391198,11,0.0268949,9,0.0220049,9,0.0220049
1511,MXS-3368,Task,MXS,2021-01-07 11:41:11,,0,Check all Maxscale test runs with enterprise server,"- check all versions of ES are installed correctly, MDBCI templates are correct, actual tests are executed
- check schedule of test runs with ES backend is correct, all runs are executed during last weekend
- check ""on push to ES"" Maxscale test runs are triggered
- set task ""create separate dashboard"" to FRUCT (separate daily Maxscale tests and weekly ""Maxscale + ES"" tests)",,"Check all Maxscale test runs with enterprise server $end$ - check all versions of ES are installed correctly, MDBCI templates are correct, actual tests are executed
- check schedule of test runs with ES backend is correct, all runs are executed during last weekend
- check ""on push to ES"" Maxscale test runs are triggered
- set task ""create separate dashboard"" to FRUCT (separate daily Maxscale tests and weekly ""Maxscale + ES"" tests) $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,17,,0,2,0,6,0,0,0,,0,850,2,0,0,2021-01-07 11:59:49,Check all Maxscale test runs with enterprise server,"- check all versions of ES are installed correctly, MDBCI templates are correct, actual tests are executed
- check schedule of test runs with ES backend is correct, all runs are executed during last weekend
- check ""on push to ES"" Maxscale test runs are triggered
- set task ""create separate dashboard"" to FRUCT (separate daily Maxscale tests and weekly ""Maxscale + ES"" tests)",,0,0,0,0,0.0,"Check all Maxscale test runs with enterprise server $end$ - check all versions of ES are installed correctly, MDBCI templates are correct, actual tests are executed
- check schedule of test runs with ES backend is correct, all runs are executed during last weekend
- check ""on push to ES"" Maxscale test runs are triggered
- set task ""create separate dashboard"" to FRUCT (separate daily Maxscale tests and weekly ""Maxscale + ES"" tests) $acceptance criteria:$",0,0,0,0,0,0,1,0.3,91,1,0.010989,0,0.0,0,0.0,0,0.0,0,0.0
1512,MXS-3371,Task,MXS,2021-01-11 12:54:50,MXS-3387,0,Implement Insert,,,Implement Insert $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-01-11 12:54:50,Implement Insert,,,0,0,0,0,0.0,Implement Insert $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,410,38,0.0926829,16,0.0390244,11,0.0268293,9,0.0219512,9,0.0219512
1513,MXS-3373,Task,MXS,2021-01-14 11:09:37,MXS-3387,0,Implement Delete command in the Mongo protocol.,,,Implement Delete command in the Mongo protocol. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-01-18 09:00:29,Implement Delete command in the Mongo protocol.,,,0,0,0,0,0.0,Implement Delete command in the Mongo protocol. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,93.8333,411,38,0.0924574,16,0.0389294,11,0.026764,9,0.0218978,9,0.0218978
1514,MXS-3375,Sub-Task,MXS,2021-01-15 15:06:43,,0,Make PS management a generic feature,Move the prepared statement handling into the protocol module. This allows the code in the routers to be removed and is required to be in place before session commands are moved there. ,,Make PS management a generic feature $end$ Move the prepared statement handling into the protocol module. This allows the code in the routers to be removed and is required to be in place before session commands are moved there.  $acceptance criteria:$,,markus makela,markus makela,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-01-15 15:06:43,Make PS management a generic feature,Move the prepared statement handling into the protocol module. This allows the code in the routers to be removed and is required to be in place before session commands are moved there. ,,0,0,0,0,0.0,Make PS management a generic feature $end$ Move the prepared statement handling into the protocol module. This allows the code in the routers to be removed and is required to be in place before session commands are moved there.  $acceptance criteria:$,0,0,0,0,0,0,1,0.0,104,15,0.144231,10,0.0961538,8,0.0769231,8,0.0769231,7,0.0673077
1515,MXS-3377,Task,MXS,2021-01-18 11:17:18,,0,Add KILL command parsing,Add KILL command parsing to the query classifier. This allows it to be used for first-pass detection of special queries.,,Add KILL command parsing $end$ Add KILL command parsing to the query classifier. This allows it to be used for first-pass detection of special queries. $acceptance criteria:$,,markus makela,markus makela,Major,16,,0,0,0,3,0,0,0,,0,850,0,0,0,2021-01-18 11:17:18,Add KILL command parsing,Add KILL command parsing to the query classifier. This allows it to be used for first-pass detection of special queries.,,0,0,0,0,0.0,Add KILL command parsing $end$ Add KILL command parsing to the query classifier. This allows it to be used for first-pass detection of special queries. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,105,15,0.142857,10,0.0952381,8,0.0761905,8,0.0761905,7,0.0666667
1516,MXS-3378,Task,MXS,2021-01-18 11:53:38,,0,Update smartrouter documentation.,,,Update smartrouter documentation. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-01-18 11:53:38,Update smartrouter documentation.,,,0,0,0,0,0.0,Update smartrouter documentation. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,412,38,0.092233,16,0.038835,11,0.026699,9,0.0218447,9,0.0218447
1517,MXS-3384,New Feature,MXS,2021-01-26 10:21:35,,0,Able to list/group servers by Monitor name,"Have the ability to list the servers by a specific Monitor

{code:java}
shell> maxctrl list servers --monitor <monitor-name>
{code}

or maybe just add the Monitor name in the output as a column, but depending on the column name, the output might not be able to fit on the screen

Another thought to have the output automatically grouped by the respective monitors of no specific monitor name is specified.",,"Able to list/group servers by Monitor name $end$ Have the ability to list the servers by a specific Monitor

{code:java}
shell> maxctrl list servers --monitor <monitor-name>
{code}

or maybe just add the Monitor name in the output as a column, but depending on the column name, the output might not be able to fit on the screen

Another thought to have the output automatically grouped by the respective monitors of no specific monitor name is specified. $acceptance criteria:$",,Faisal Saeed,Faisal Saeed,Minor,12,,0,1,0,1,0,0,0,,0,850,0,0,0,2022-09-12 10:07:49,Able to list/group servers by Monitor name,"Have the ability to list the servers by a specific Monitor

{code:java}
shell> maxctrl list servers --monitor <monitor-name>
{code}

or maybe just add the Monitor name in the output as a column, but depending on the column name, the output might not be able to fit on the screen

Another thought to have the output automatically grouped by the respective monitors of no specific monitor name is specified.",,0,0,0,0,0.0,"Able to list/group servers by Monitor name $end$ Have the ability to list the servers by a specific Monitor

{code:java}
shell> maxctrl list servers --monitor <monitor-name>
{code}

or maybe just add the Monitor name in the output as a column, but depending on the column name, the output might not be able to fit on the screen

Another thought to have the output automatically grouped by the respective monitors of no specific monitor name is specified. $acceptance criteria:$",0,0,0,0,0,0,0,14255.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1518,MXS-3385,Task,MXS,2021-01-26 13:23:07,MXS-3387,0,Implement Update,,,Implement Update $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-01-26 13:23:07,Implement Update,,,0,0,0,0,0.0,Implement Update $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,413,38,0.0920097,16,0.0387409,11,0.0266344,9,0.0217918,9,0.0217918
1519,MXS-3388,Task,MXS,2021-01-29 13:29:22,MXS-3387,0,Return proper error document on fatal error.,"If the client uses unsupported functionality, a proper error document should be returned.",,"Return proper error document on fatal error. $end$ If the client uses unsupported functionality, a proper error document should be returned. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-02-01 10:25:04,Return proper error document on fatal error.,"If the client uses unsupported functionality, a proper error document should be returned.",,0,0,0,0,0.0,"Return proper error document on fatal error. $end$ If the client uses unsupported functionality, a proper error document should be returned. $acceptance criteria:$",0,0,0,0,0,0,0,68.9167,414,38,0.0917874,16,0.0386473,11,0.02657,9,0.0217391,9,0.0217391
1520,MXS-3389,Task,MXS,2021-01-29 13:35:38,MXS-3387,0,Stream data from backend.,"Instead of waiting for the full result-set from the backend, before sending a reply to the client, a documents should be sent as data becomes available, in a pace guided by the client asking for more data.
",,"Stream data from backend. $end$ Instead of waiting for the full result-set from the backend, before sending a reply to the client, a documents should be sent as data becomes available, in a pace guided by the client asking for more data.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,13,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-03-29 09:59:27,Stream data from backend.,"Instead of waiting for the full result-set from the backend, before sending a reply to the client, a documents should be sent as data becomes available, in a pace guided by the client asking for more data.
",,0,0,0,0,0.0,"Stream data from backend. $end$ Instead of waiting for the full result-set from the backend, before sending a reply to the client, a documents should be sent as data becomes available, in a pace guided by the client asking for more data.
 $acceptance criteria:$",0,0,0,0,0,0,1,1412.38,415,38,0.0915663,16,0.0385542,11,0.026506,9,0.0216867,9,0.0216867
1521,MXS-3390,Task,MXS,2021-01-29 13:51:27,MXS-3387,0,Allow multiple Update/Delete commands in one request,,,Allow multiple Update/Delete commands in one request $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-03-08 08:36:57,Allow multiple Update/Delete commands in one request,,,0,0,0,0,0.0,Allow multiple Update/Delete commands in one request $end$ $acceptance criteria:$,0,0,0,0,0,0,1,906.75,416,38,0.0913462,16,0.0384615,11,0.0264423,9,0.0216346,9,0.0216346
1522,MXS-3391,Task,MXS,2021-01-29 13:53:59,MXS-3387,0,Auto-create collections.,,,Auto-create collections. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,0,3,0,0,0,,0,850,0,0,0,2021-02-01 10:25:24,Auto-create collections.,,,0,0,0,0,0.0,Auto-create collections. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,68.5167,417,38,0.0911271,16,0.0383693,11,0.0263789,9,0.0215827,9,0.0215827
1523,MXS-3394,New Feature,MXS,2021-02-02 07:35:14,,0,"""Rewrite Filter""","There are statements that are expensive to perform but that can be optimized, yet are not optimized by the server. In some cases such statements can not only be identified by a regular expression, but also rewritten by one.

The primary driver of the rewriting filter is to provide a mechanism using which DBAs can provide regular expressions that rewrite statements, so that their execution by the server is faster.

It should be noted that rewriting anything with regular expression is very fragile, as it is easy to create something that matches when it shouldn't and that does not match when it should, so as a tool it is quite blunt.

It shall be possible
* to provide an arbitrary number of ordered regular expressions that are applied on all statements passing through MaxScale, and
* to change at runtime the list of expressions.

However, note that the list of expressions are fixed at session creation time. Changes take effect only on sessions created after the changes have been made.

It should be possible:
* to specify whether the expressions are applied until first match or whether subsequent expressions should be applied to the altered statement.

This sounds a bit like _Named Server Filter_ and _Regex Filter_; why not extend either of those? There are a number of reasons why that is not feasible.
* Creating the boiler plate of a filter is trivial, so not having to do that saves nothing.
* Both of those filters have their complete configuration in the configuration file, which is not only clumsy and fragile, but not applicable if the number of regular expressions can be large and if they must be modifiable at runtime.
* As what is described here is different from what either of those filters are directly intended for, there would have to be different modes and the filter would have to behave differently depending on the mode it is running in. That would add complexity and the risk for the old behaviour breaking would be quite high.",,"""Rewrite Filter"" $end$ There are statements that are expensive to perform but that can be optimized, yet are not optimized by the server. In some cases such statements can not only be identified by a regular expression, but also rewritten by one.

The primary driver of the rewriting filter is to provide a mechanism using which DBAs can provide regular expressions that rewrite statements, so that their execution by the server is faster.

It should be noted that rewriting anything with regular expression is very fragile, as it is easy to create something that matches when it shouldn't and that does not match when it should, so as a tool it is quite blunt.

It shall be possible
* to provide an arbitrary number of ordered regular expressions that are applied on all statements passing through MaxScale, and
* to change at runtime the list of expressions.

However, note that the list of expressions are fixed at session creation time. Changes take effect only on sessions created after the changes have been made.

It should be possible:
* to specify whether the expressions are applied until first match or whether subsequent expressions should be applied to the altered statement.

This sounds a bit like _Named Server Filter_ and _Regex Filter_; why not extend either of those? There are a number of reasons why that is not feasible.
* Creating the boiler plate of a filter is trivial, so not having to do that saves nothing.
* Both of those filters have their complete configuration in the configuration file, which is not only clumsy and fragile, but not applicable if the number of regular expressions can be large and if they must be modifiable at runtime.
* As what is described here is different from what either of those filters are directly intended for, there would have to be different modes and the filter would have to behave differently depending on the mode it is running in. That would add complexity and the risk for the old behaviour breaking would be quite high. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,28,,0,0,0,8,0,1,0,,0,850,0,0,0,2022-04-25 09:59:48,"""Rewriting Filter""","There are statements that are expensive to perform but that can be optimized, yet are not optimized by the server. In some cases such statements can not only be identified by a regular expression, but also rewritten by one.

The primary driver of the rewriting filter is to provide a mechanism using which DBAs can provide regular expressions that rewrite statements, so that their execution by the server is faster.

It should be noted that rewriting anything with regular expression is very fragile, as it is easy to create something that matches when it shouldn't and that does not match when it should, so as a tool it is quite blunt.

It shall be possible
* to provide an arbitrary number of ordered regular expressions that are applied on all statements passing through MaxScale, and
* to change at runtime the list of expressions.

However, note that the list of expressions are fixed at session creation time. Changes take effect only on sessions created after the changes have been made.

It should be possible:
* to specify whether the expressions are applied until first match or whether subsequent expressions should be applied to the altered statement.

This sounds a bit like _Named Server Filter_ and _Regex Filter_; why not extend either of those? There are a number of reasons why that is not feasible.
* Creating the boiler plate of a filter is trivial, so not having to do that saves nothing.
* Both of those filters have their complete configuration in the configuration file, which is not only clumsy and fragile, but not applicable if the number of regular expressions can be large and if they must be modifiable at runtime.
* As what is described here is different from what either of those filters are directly intended for, there would have to be different modes and the filter would have to behave differently depending on the mode it is running in. That would add complexity and the risk for the old behaviour breaking would be quite high.",,1,0,0,2,0.00290698,"""Rewriting Filter"" $end$ There are statements that are expensive to perform but that can be optimized, yet are not optimized by the server. In some cases such statements can not only be identified by a regular expression, but also rewritten by one.

The primary driver of the rewriting filter is to provide a mechanism using which DBAs can provide regular expressions that rewrite statements, so that their execution by the server is faster.

It should be noted that rewriting anything with regular expression is very fragile, as it is easy to create something that matches when it shouldn't and that does not match when it should, so as a tool it is quite blunt.

It shall be possible
* to provide an arbitrary number of ordered regular expressions that are applied on all statements passing through MaxScale, and
* to change at runtime the list of expressions.

However, note that the list of expressions are fixed at session creation time. Changes take effect only on sessions created after the changes have been made.

It should be possible:
* to specify whether the expressions are applied until first match or whether subsequent expressions should be applied to the altered statement.

This sounds a bit like _Named Server Filter_ and _Regex Filter_; why not extend either of those? There are a number of reasons why that is not feasible.
* Creating the boiler plate of a filter is trivial, so not having to do that saves nothing.
* Both of those filters have their complete configuration in the configuration file, which is not only clumsy and fragile, but not applicable if the number of regular expressions can be large and if they must be modifiable at runtime.
* As what is described here is different from what either of those filters are directly intended for, there would have to be different modes and the filter would have to behave differently depending on the mode it is running in. That would add complexity and the risk for the old behaviour breaking would be quite high. $acceptance criteria:$",1,1,0,0,0,0,1,10730.4,418,38,0.0909091,16,0.0382775,11,0.0263158,9,0.0215311,9,0.0215311
1524,MXS-3398,New Feature,MXS,2021-02-04 12:23:40,,0,Auto-adjust MaxScale parameters according to server settings,"Some MaxScale parameters depend upon server parameters in such a way that if a MaxScale parameter is configured without any regard to the server parameters(s) on which it depends, then MaxScale may not function properly.

For instance, the {{persistmaxtime}} specifies how long a connection can stay in the pool before it is discarded. However, it makes no sense with the value of {{persistmaxtime}} being larger than {{wait_timeout}} of the server. The former should always be smaller than the latter.

MaxScale should sniff the server parameters and, if necessary, adjust any MaxScale parameters that are in conflict with the server ones. For some cases it should be possible to specify that the MaxScale parameter is directly derived from a particular server parameter.

The sniffing should be performed at regular intervals, so that any changes made at runtime to the server parameters are automatically taken into account.",,"Auto-adjust MaxScale parameters according to server settings $end$ Some MaxScale parameters depend upon server parameters in such a way that if a MaxScale parameter is configured without any regard to the server parameters(s) on which it depends, then MaxScale may not function properly.

For instance, the {{persistmaxtime}} specifies how long a connection can stay in the pool before it is discarded. However, it makes no sense with the value of {{persistmaxtime}} being larger than {{wait_timeout}} of the server. The former should always be smaller than the latter.

MaxScale should sniff the server parameters and, if necessary, adjust any MaxScale parameters that are in conflict with the server ones. For some cases it should be possible to specify that the MaxScale parameter is directly derived from a particular server parameter.

The sniffing should be performed at regular intervals, so that any changes made at runtime to the server parameters are automatically taken into account. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,17,,0,0,0,3,0,0,0,,0,850,0,0,0,2022-04-11 07:50:35,Auto-adjust MaxScale parameters according to server settings,"Some MaxScale parameters depend upon server parameters in such a way that if a MaxScale parameter is configured without any regard to the server parameters(s) on which it depends, then MaxScale may not function properly.

For instance, the {{persistmaxtime}} specifies how long a connection can stay in the pool before it is discarded. However, it makes no sense with the value of {{persistmaxtime}} being larger than {{wait_timeout}} of the server. The former should always be smaller than the latter.

MaxScale should sniff the server parameters and, if necessary, adjust any MaxScale parameters that are in conflict with the server ones. For some cases it should be possible to specify that the MaxScale parameter is directly derived from a particular server parameter.

The sniffing should be performed at regular intervals, so that any changes made at runtime to the server parameters are automatically taken into account.",,0,0,0,0,0.0,"Auto-adjust MaxScale parameters according to server settings $end$ Some MaxScale parameters depend upon server parameters in such a way that if a MaxScale parameter is configured without any regard to the server parameters(s) on which it depends, then MaxScale may not function properly.

For instance, the {{persistmaxtime}} specifies how long a connection can stay in the pool before it is discarded. However, it makes no sense with the value of {{persistmaxtime}} being larger than {{wait_timeout}} of the server. The former should always be smaller than the latter.

MaxScale should sniff the server parameters and, if necessary, adjust any MaxScale parameters that are in conflict with the server ones. For some cases it should be possible to specify that the MaxScale parameter is directly derived from a particular server parameter.

The sniffing should be performed at regular intervals, so that any changes made at runtime to the server parameters are automatically taken into account. $acceptance criteria:$",0,0,0,0,0,0,1,10339.4,419,39,0.0930788,16,0.0381862,11,0.026253,9,0.0214797,9,0.0214797
1525,MXS-3401,Task,MXS,2021-02-09 08:15:19,,0,Use new configuration mechanism for services,The services still use the old configuration mechanism which should be replaced with the new one.,,Use new configuration mechanism for services $end$ The services still use the old configuration mechanism which should be replaced with the new one. $acceptance criteria:$,,markus makela,markus makela,Major,10,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-02-15 11:02:03,Use new configuration mechanism for services,The services still use the old configuration mechanism which should be replaced with the new one.,,0,0,0,0,0.0,Use new configuration mechanism for services $end$ The services still use the old configuration mechanism which should be replaced with the new one. $acceptance criteria:$,0,0,0,0,0,0,1,146.767,106,15,0.141509,10,0.0943396,8,0.0754717,8,0.0754717,7,0.0660377
1526,MXS-3407,Task,MXS,2021-02-15 11:45:13,,0,Investigate if SIMD provides an advantage,,,Investigate if SIMD provides an advantage $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,11,,0,0,0,5,0,0,0,,0,850,0,0,0,2021-02-15 11:45:13,Investigate if SIMD provides an advantage,,,0,0,0,0,0.0,Investigate if SIMD provides an advantage $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,420,39,0.0928571,16,0.0380952,11,0.0261905,9,0.0214286,9,0.0214286
1527,MXS-3411,New Feature,MXS,2021-02-19 07:40:21,,0,support kafka authentication,"For  security reason, maxscale should support basic authencation possibilites for kafkacdc.

[Authentication Kafka|http://kafka.apache.org/090/documentation.html#security]

",,"support kafka authentication $end$ For  security reason, maxscale should support basic authencation possibilites for kafkacdc.

[Authentication Kafka|http://kafka.apache.org/090/documentation.html#security]

 $acceptance criteria:$",,Richard Stracke,Richard Stracke,Major,10,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-03-15 11:23:00,support kafka authentication,"For  security reason, maxscale should support basic authencation possibilites for kafkacdc.

[Authentication Kafka|http://kafka.apache.org/090/documentation.html#security]

",,0,0,0,0,0.0,"support kafka authentication $end$ For  security reason, maxscale should support basic authencation possibilites for kafkacdc.

[Authentication Kafka|http://kafka.apache.org/090/documentation.html#security]

 $acceptance criteria:$",0,0,0,0,0,0,0,579.7,5,1,0.2,1,0.2,1,0.2,1,0.2,1,0.2
1528,MXS-3413,New Feature,MXS,2021-02-22 21:19:57,,0,"The persistence of on-the-fly parameter changes needs to be somehow exposed, and more manageable.","MaxScale reads parameters from configuration files on startup, and then reads any parameters that may have been modified interactively with ""maxctrl alter"" commands.

This is because any parameters modified with ""maxctrl alter"" get cached in /var/lib/maxscale/maxscale.cnf.d, and this behavior is not documented, for example, in [the alter section of the KB|https://mariadb.com/kb/en/mariadb-maxscale-22-maxctrl/#alter].

This behavior is handy for the experienced user in some special circumstances, but quite counter-intuitive, since it makes what should be volatile changes (and is volatile changes in all other MariaDB products) into the most persistent and overriding parameter changes, and hidden from the view of more customary configuration locations, without providing any sort of feedback to the user or system in parameter displays such as the output of ""maxctrl show maxscale"".

* To make administrators aware of these extra parameters being applied on start-up, object names including servers, routers, filters and monitors, on the output of any maxctrl command should have a visual indicator (asterisk or color-change) whenever they are running with parameters modified interactively, either through a maxctrl alter command or from getting picked up from the cache file in /var/lib/maxscale/maxscale.cnf.d
* On startup MaxScale should log warnings when parameters have been read from someplace other than user-defined configuration files.
* The option to persist interactive changes should have a parametric switch, by default turned off: ""maxctrl_persistent_alter=false"", for instance.",,"The persistence of on-the-fly parameter changes needs to be somehow exposed, and more manageable. $end$ MaxScale reads parameters from configuration files on startup, and then reads any parameters that may have been modified interactively with ""maxctrl alter"" commands.

This is because any parameters modified with ""maxctrl alter"" get cached in /var/lib/maxscale/maxscale.cnf.d, and this behavior is not documented, for example, in [the alter section of the KB|https://mariadb.com/kb/en/mariadb-maxscale-22-maxctrl/#alter].

This behavior is handy for the experienced user in some special circumstances, but quite counter-intuitive, since it makes what should be volatile changes (and is volatile changes in all other MariaDB products) into the most persistent and overriding parameter changes, and hidden from the view of more customary configuration locations, without providing any sort of feedback to the user or system in parameter displays such as the output of ""maxctrl show maxscale"".

* To make administrators aware of these extra parameters being applied on start-up, object names including servers, routers, filters and monitors, on the output of any maxctrl command should have a visual indicator (asterisk or color-change) whenever they are running with parameters modified interactively, either through a maxctrl alter command or from getting picked up from the cache file in /var/lib/maxscale/maxscale.cnf.d
* On startup MaxScale should log warnings when parameters have been read from someplace other than user-defined configuration files.
* The option to persist interactive changes should have a parametric switch, by default turned off: ""maxctrl_persistent_alter=false"", for instance. $acceptance criteria:$",,Juan,Juan,Major,25,,0,13,0,1,0,0,0,,0,850,5,0,0,2021-10-11 08:23:50,"The persistence of on-the-fly parameter changes needs to be somehow exposed, and more manageable.","MaxScale reads parameters from configuration files on startup, and then reads any parameters that may have been modified interactively with ""maxctrl alter"" commands.

This is because any parameters modified with ""maxctrl alter"" get cached in /var/lib/maxscale/maxscale.cnf.d, and this behavior is not documented, for example, in [the alter section of the KB|https://mariadb.com/kb/en/mariadb-maxscale-22-maxctrl/#alter].

This behavior is handy for the experienced user in some special circumstances, but quite counter-intuitive, since it makes what should be volatile changes (and is volatile changes in all other MariaDB products) into the most persistent and overriding parameter changes, and hidden from the view of more customary configuration locations, without providing any sort of feedback to the user or system in parameter displays such as the output of ""maxctrl show maxscale"".

* To make administrators aware of these extra parameters being applied on start-up, object names including servers, routers, filters and monitors, on the output of any maxctrl command should have a visual indicator (asterisk or color-change) whenever they are running with parameters modified interactively, either through a maxctrl alter command or from getting picked up from the cache file in /var/lib/maxscale/maxscale.cnf.d
* On startup MaxScale should log warnings when parameters have been read from someplace other than user-defined configuration files.
* The option to persist interactive changes should have a parametric switch, by default turned off: ""maxctrl_persistent_alter=false"", for instance.",,0,0,0,0,0.0,"The persistence of on-the-fly parameter changes needs to be somehow exposed, and more manageable. $end$ MaxScale reads parameters from configuration files on startup, and then reads any parameters that may have been modified interactively with ""maxctrl alter"" commands.

This is because any parameters modified with ""maxctrl alter"" get cached in /var/lib/maxscale/maxscale.cnf.d, and this behavior is not documented, for example, in [the alter section of the KB|https://mariadb.com/kb/en/mariadb-maxscale-22-maxctrl/#alter].

This behavior is handy for the experienced user in some special circumstances, but quite counter-intuitive, since it makes what should be volatile changes (and is volatile changes in all other MariaDB products) into the most persistent and overriding parameter changes, and hidden from the view of more customary configuration locations, without providing any sort of feedback to the user or system in parameter displays such as the output of ""maxctrl show maxscale"".

* To make administrators aware of these extra parameters being applied on start-up, object names including servers, routers, filters and monitors, on the output of any maxctrl command should have a visual indicator (asterisk or color-change) whenever they are running with parameters modified interactively, either through a maxctrl alter command or from getting picked up from the cache file in /var/lib/maxscale/maxscale.cnf.d
* On startup MaxScale should log warnings when parameters have been read from someplace other than user-defined configuration files.
* The option to persist interactive changes should have a parametric switch, by default turned off: ""maxctrl_persistent_alter=false"", for instance. $acceptance criteria:$",0,0,0,0,0,0,0,5531.05,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1529,MXS-3417,Task,MXS,2021-02-25 13:26:03,,0,System test cleanup,Miscellaneous system test cleanup,,System test cleanup $end$ Miscellaneous system test cleanup $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,20,,0,0,0,10,0,0,0,,0,850,0,0,0,2021-03-01 11:48:47,System test cleanup,Miscellaneous system test cleanup,,0,0,0,0,0.0,System test cleanup $end$ Miscellaneous system test cleanup $acceptance criteria:$,0,0,0,0,0,0,1,94.3667,22,2,0.0909091,1,0.0454545,0,0.0,0,0.0,0,0.0
1530,MXS-3418,Task,MXS,2021-03-01 10:32:54,,0,Ensure system tests can be run against a local MaxScale.,"Sometimes when developing MaxScale and/or system tests, it would be convenient to run the test against a locally running MaxScale instead of against a MaxScale running in a VM. The support should basically be there, but currently seems not to be working.
",,"Ensure system tests can be run against a local MaxScale. $end$ Sometimes when developing MaxScale and/or system tests, it would be convenient to run the test against a locally running MaxScale instead of against a MaxScale running in a VM. The support should basically be there, but currently seems not to be working.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-01 10:32:54,Ensure system tests can be run against a local MaxScale.,"Sometimes when developing MaxScale and/or system tests, it would be convenient to run the test against a locally running MaxScale instead of against a MaxScale running in a VM. The support should basically be there, but currently seems not to be working.
",,0,0,0,0,0.0,"Ensure system tests can be run against a local MaxScale. $end$ Sometimes when developing MaxScale and/or system tests, it would be convenient to run the test against a locally running MaxScale instead of against a MaxScale running in a VM. The support should basically be there, but currently seems not to be working.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,421,39,0.0926366,16,0.0380047,11,0.0261283,9,0.0213777,9,0.0213777
1531,MXS-3420,Task,MXS,2021-03-01 11:38:07,,0,"Document new pooling mechanism,",,,"Document new pooling mechanism, $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-01 11:38:07,"Document new pooling mechanism,",,,0,0,0,0,0.0,"Document new pooling mechanism, $end$ $acceptance criteria:$",0,0,0,0,0,0,0,0.0,422,39,0.0924171,16,0.0379147,11,0.0260663,9,0.021327,9,0.021327
1532,MXS-3421,Task,MXS,2021-03-01 11:40:42,,0,Investigate need for MXS-3268,,,Investigate need for MXS-3268 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-03-01 11:40:42,Investigate need for MXS-3268,,,0,0,0,0,0.0,Investigate need for MXS-3268 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,423,39,0.0921986,16,0.0378251,11,0.0260047,9,0.0212766,9,0.0212766
1533,MXS-3422,Task,MXS,2021-03-01 11:50:28,,0,"Benchmark and compare performance of MaxScale 2.4, 2.5, develop ",,,"Benchmark and compare performance of MaxScale 2.4, 2.5, develop  $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,17,,0,2,0,4,0,0,0,,0,850,2,0,0,2021-03-01 11:50:28,"Benchmark and compare performance of MaxScale 2.4, 2.5, develop ",,,0,0,0,0,0.0,"Benchmark and compare performance of MaxScale 2.4, 2.5, develop  $end$ $acceptance criteria:$",0,0,0,0,0,0,1,0.0,424,39,0.0919811,16,0.0377358,11,0.0259434,9,0.0212264,9,0.0212264
1534,MXS-3429,Task,MXS,2021-03-08 08:33:45,MXS-3387,0,Return error or empty document in case of unrecognized command.,,,Return error or empty document in case of unrecognized command. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-08 08:33:45,Return error or empty document in case of unrecognized command.,,,0,0,0,0,0.0,Return error or empty document in case of unrecognized command. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,425,39,0.0917647,16,0.0376471,11,0.0258824,9,0.0211765,9,0.0211765
1535,MXS-3430,Task,MXS,2021-03-08 08:39:14,MXS-3387,0,Create system tests for all commands.,,,Create system tests for all commands. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,24,,0,0,0,5,0,1,0,,0,850,0,0,0,2021-03-08 08:39:14,Create comprehensive test for Mongo query object parsing and handling.,,,1,0,0,12,0.615385,Create comprehensive test for Mongo query object parsing and handling. $end$ $acceptance criteria:$,1,1,1,1,0,0,1,0.0,426,39,0.0915493,16,0.0375587,11,0.0258216,9,0.0211268,9,0.0211268
1536,MXS-3431,Task,MXS,2021-03-08 09:10:24,MXS-3387,0,Accept client defined document id.,,,Accept client defined document id. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-08 09:10:24,Accept client defined document id.,,,0,0,0,0,0.0,Accept client defined document id. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,427,40,0.0936768,17,0.0398126,12,0.028103,9,0.0210773,9,0.0210773
1537,MXS-3435,Task,MXS,2021-03-11 13:56:00,MXS-3387,0,Ensure the Mongo shell works with mongodbclient,,,Ensure the Mongo shell works with mongodbclient $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-11 13:56:00,Ensure the Mongo shell works with mongodbclient,,,0,0,0,0,0.0,Ensure the Mongo shell works with mongodbclient $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,428,40,0.0934579,17,0.0397196,12,0.0280374,9,0.021028,9,0.021028
1538,MXS-3442,Task,MXS,2021-03-15 06:33:50,MXS-3387,0,Use the 4.4 protocol in both directions.,,,Use the 4.4 protocol in both directions. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-15 06:33:50,Use the 4.4 protocol in both directions.,,,0,0,0,0,0.0,Use the 4.4 protocol in both directions. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,429,40,0.0932401,17,0.039627,12,0.027972,9,0.020979,9,0.020979
1539,MXS-3444,Task,MXS,2021-03-15 14:25:56,MXS-3387,0,Start checking and producing package checksum,,,Start checking and producing package checksum $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-29 08:26:13,Start checking and producing package checksum,,,0,0,0,0,0.0,Start checking and producing package checksum $end$ $acceptance criteria:$,0,0,0,0,0,0,0,330.0,430,40,0.0930233,17,0.0395349,12,0.027907,9,0.0209302,9,0.0209302
1540,MXS-3451,Task,MXS,2021-03-18 08:47:27,MXS-3387,0,Remove dependency on internal files.,,,Remove dependency on internal files. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-03-18 08:47:27,Remove dependency on internal files.,,,0,0,0,0,0.0,Remove dependency on internal files. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,431,40,0.0928074,17,0.0394432,12,0.0278422,9,0.0208817,9,0.0208817
1541,MXS-3461,Task,MXS,2021-03-26 09:43:27,,0,Move mirror into the main package,Move the module into the main package and add any missing features to it that are required.,,Move mirror into the main package $end$ Move the module into the main package and add any missing features to it that are required. $acceptance criteria:$,,markus makela,markus makela,Minor,8,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-03-29 10:07:49,Move mirror into the main package,Move the module into the main package and add any missing features to it that are required.,,0,0,0,0,0.0,Move mirror into the main package $end$ Move the module into the main package and add any missing features to it that are required. $acceptance criteria:$,0,0,0,0,0,0,0,72.4,107,15,0.140187,10,0.0934579,8,0.0747664,8,0.0747664,7,0.0654206
1542,MXS-3465,Task,MXS,2021-03-29 07:23:59,MXS-3464,0,Create query editor component,"Basic features of the query editor must have:
* Syntax highlighting and auto completion
* Auto suggest table names, table columns
* Option to format sql code
",,"Create query editor component $end$ Basic features of the query editor must have:
* Syntax highlighting and auto completion
* Auto suggest table names, table columns
* Option to format sql code
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,17,,0,0,0,2,0,3,2,,0,850,0,0,0,2021-03-29 08:10:43,Create SQL editor component,"Either creating a component from scratch or reusing existing library where user can 
* Enter SQL queries with syntax highlighting.
* Colorize reserve SQL keywords
",,1,2,0,50,0.8125,"Create SQL editor component $end$ Either creating a component from scratch or reusing existing library where user can 
* Enter SQL queries with syntax highlighting.
* Colorize reserve SQL keywords
 $acceptance criteria:$",3,1,1,1,1,1,1,0.766667,9,2,0.222222,2,0.222222,1,0.111111,1,0.111111,1,0.111111
1543,MXS-3466,Task,MXS,2021-03-29 07:31:29,MXS-3464,0,Create route to query editor page,"TBD
Needs the UI design to decide and depends on the way of set up connection. e.g. Clicking on server name, it goes to sever details page, in sever detail page,
there could be a button to access SQL editor page.",,"Create route to query editor page $end$ TBD
Needs the UI design to decide and depends on the way of set up connection. e.g. Clicking on server name, it goes to sever details page, in sever detail page,
there could be a button to access SQL editor page. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,11,,0,0,0,1,0,1,0,,0,850,0,1,0,2021-04-13 15:03:44,Create route to query editor page,"TBD
Needs the UI design to decide and depends on the way of set up connection. e.g. Clicking on server name, it goes to sever details page, in sever detail page,
there could be a button to access SQL editor page.",,0,0,0,0,0.0,"Create route to query editor page $end$ TBD
Needs the UI design to decide and depends on the way of set up connection. e.g. Clicking on server name, it goes to sever details page, in sever detail page,
there could be a button to access SQL editor page. $acceptance criteria:$",0,0,0,0,0,0,0,367.533,10,3,0.3,3,0.3,2,0.2,2,0.2,2,0.2
1544,MXS-3467,Task,MXS,2021-03-29 07:58:50,MXS-3464,0,Create query result component,"A table component to show the result of user's sql query.

There should be two tabs to switch 
# Results: show results of the sql query get from the editor.
# Data preview: Show data preview when user select `Preview Data` or `View Details` options in the table list sidebar.

In Data Preview tab, user should be able to see table data  with two buttons to choose to show either `Preview Data` or `View Details`
* Data for `Preview Data` is getting by:
{code:java}
SELECT * FROM schema.table_name;
{code}

* Data for `View Details` option is getting by: 
{code:java}
DESCRIBE schema.table_name;
{code}

Table functionalities:
* Table column should be re-sizable
* User can export and download query result to different file formats such as csv, xlsx, json
* Table can be filtered
* Able to sort and filter result
* Able to toggle table columns",,"Create query result component $end$ A table component to show the result of user's sql query.

There should be two tabs to switch 
# Results: show results of the sql query get from the editor.
# Data preview: Show data preview when user select `Preview Data` or `View Details` options in the table list sidebar.

In Data Preview tab, user should be able to see table data  with two buttons to choose to show either `Preview Data` or `View Details`
* Data for `Preview Data` is getting by:
{code:java}
SELECT * FROM schema.table_name;
{code}

* Data for `View Details` option is getting by: 
{code:java}
DESCRIBE schema.table_name;
{code}

Table functionalities:
* Table column should be re-sizable
* User can export and download query result to different file formats such as csv, xlsx, json
* Table can be filtered
* Able to sort and filter result
* Able to toggle table columns $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,23,,0,0,0,4,0,5,14,,0,850,0,1,0,2021-04-21 07:07:24,Create query result component,"A table component to show the result of user's sql query.
* Table column should be re-sizable
* User can export and download query result to different file formats such as csv, xlsx, json
* Table can be filtered
* Table can be maximize/minimize",,0,4,0,109,2.05882,"Create query result component $end$ A table component to show the result of user's sql query.
* Table column should be re-sizable
* User can export and download query result to different file formats such as csv, xlsx, json
* Table can be filtered
* Table can be maximize/minimize $acceptance criteria:$",4,1,1,1,1,1,1,551.133,11,3,0.272727,3,0.272727,2,0.181818,2,0.181818,2,0.181818
1545,MXS-3470,Task,MXS,2021-03-29 10:13:12,,0,Create design for kafka to mariadb router,,,Create design for kafka to mariadb router $end$ $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-03-29 10:14:35,Create design for kafka to mariadb router,,,0,0,0,0,0.0,Create design for kafka to mariadb router $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0166667,108,15,0.138889,10,0.0925926,8,0.0740741,8,0.0740741,7,0.0648148
1546,MXS-3473,Task,MXS,2021-03-30 13:06:23,MXS-3387,0,Add commands,,,Add commands $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,15,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-01 10:05:08,Add commands,,,0,0,0,0,0.0,Add commands $end$ $acceptance criteria:$,0,0,0,0,0,0,0,44.9667,432,40,0.0925926,17,0.0393519,12,0.0277778,9,0.0208333,9,0.0208333
1547,MXS-3475,New Feature,MXS,2021-03-31 14:34:21,,0,Extend PAM support to include Group Mapping,"*strong text*At present, MaxScale can do a lot of what MariaDB server can do with PAM - except for group mapping. This is a request to upgrade that support to the fuller parity with the server. 

Ideally, the installation and deployment would mimic the ways and means used when the server is so configured. Yet certain higher features of the server syntax such as explicitly defined ROLES and syntactically explicit GRANT PROXY do not need to be supported in the same exact way.

*Justification.*
MaxScale is a required part of XPAND deployments. Current sales priority for XPAND is so called ""performance option"", whereby MDB server is bypassed altogether, and the workload is distributed by MaxScale directly to the nodes of XPAND cluster. XPAND does not support LDAP at present.

A customer wants to use Active Directory in this configuration. Currently we can satisfy the authentication part (including two phase) using MaxScale. The theory is that it a good deal easier and architecturally better to add group mapping to MaxScale than to build a whole PAM apparatus in XPAND.

*Easier* - very likely
*Architecturally Better*: XPAND is presented to the market as a ""smart engine"". When it operates under MDB Server, all security is handled by PAM of the server. ""Performance option"" is just that, performance option. It relies on MaxScale for many serious things, from workload distribution to transaction replay. Given that MaxScale also provides security checks and even access controls, it is certainly stands to reason that not duplicating the functionality is better than duplicating it.

*Customer Requirements*
1.	General corporate policies - 
    a.	are users treated as individuals, or are the assigned to groups? *""Yes, assigned to groups in AD. We need to be able to map these groups into small number of XPAND users, each representing a  business role""*
    b.	If so, can one user be in multiple groups? *""yes. initially, the first group retrieved by PAM should be used as default. Additional refinements may be considered for later""*
2.	Required operation with database:
    a.	Authentication by Active Directory on every connect request is assumed, but:
    b.	Is two-factor authentication for database access required? *""No""*
    c.	Mapping of AD groups into database users (associating users with business roles and so minimizing grant maintenance requirements)? *""Yes, as above""*
3.	Business rules. 
    a.	We hear canned applications are using session pooling. Is it true? If so, what is the current access control scheme? *""We have one schema per application. Applications are all microservices. All users are validated by the app server, all database connections go under the same userid""*
    i.	Authentication by web server or app server? *""Yes, as above""*
              1.        If so, do all sessions use the same userid, or *""Yes now""*
              2.	Are sessions grouped somehow into ""business roles""? If so, are these roles obtained from Active Directory groupings or are they maintained elsewhere? *""maybe in a future""*
    b.	Will there be, in addition to canned application(s), and ""ad hoc"" access from general tools (like SQL tools, BI tools, or even Client tools)? *""Correct. this is the primary reason why LDAP support is required (something which would not be really critical if canned applications was all there was to it)""*
    i.	If affirmative - what is the security model? Individual grants for each user, group grants for business roles? *""The latter, mainly. In specific cases where a single individual has specific database privileges, we are fine creating a dedicated group in AD""* 
4.	Deployment models
   a.	Will MaxScale be the required component, or do you plan load balancers on app server or Clients to deal with XPAND cluster (and its scale up and down events)? *""Yes""*
",,"Extend PAM support to include Group Mapping $end$ *strong text*At present, MaxScale can do a lot of what MariaDB server can do with PAM - except for group mapping. This is a request to upgrade that support to the fuller parity with the server. 

Ideally, the installation and deployment would mimic the ways and means used when the server is so configured. Yet certain higher features of the server syntax such as explicitly defined ROLES and syntactically explicit GRANT PROXY do not need to be supported in the same exact way.

*Justification.*
MaxScale is a required part of XPAND deployments. Current sales priority for XPAND is so called ""performance option"", whereby MDB server is bypassed altogether, and the workload is distributed by MaxScale directly to the nodes of XPAND cluster. XPAND does not support LDAP at present.

A customer wants to use Active Directory in this configuration. Currently we can satisfy the authentication part (including two phase) using MaxScale. The theory is that it a good deal easier and architecturally better to add group mapping to MaxScale than to build a whole PAM apparatus in XPAND.

*Easier* - very likely
*Architecturally Better*: XPAND is presented to the market as a ""smart engine"". When it operates under MDB Server, all security is handled by PAM of the server. ""Performance option"" is just that, performance option. It relies on MaxScale for many serious things, from workload distribution to transaction replay. Given that MaxScale also provides security checks and even access controls, it is certainly stands to reason that not duplicating the functionality is better than duplicating it.

*Customer Requirements*
1.	General corporate policies - 
    a.	are users treated as individuals, or are the assigned to groups? *""Yes, assigned to groups in AD. We need to be able to map these groups into small number of XPAND users, each representing a  business role""*
    b.	If so, can one user be in multiple groups? *""yes. initially, the first group retrieved by PAM should be used as default. Additional refinements may be considered for later""*
2.	Required operation with database:
    a.	Authentication by Active Directory on every connect request is assumed, but:
    b.	Is two-factor authentication for database access required? *""No""*
    c.	Mapping of AD groups into database users (associating users with business roles and so minimizing grant maintenance requirements)? *""Yes, as above""*
3.	Business rules. 
    a.	We hear canned applications are using session pooling. Is it true? If so, what is the current access control scheme? *""We have one schema per application. Applications are all microservices. All users are validated by the app server, all database connections go under the same userid""*
    i.	Authentication by web server or app server? *""Yes, as above""*
              1.        If so, do all sessions use the same userid, or *""Yes now""*
              2.	Are sessions grouped somehow into ""business roles""? If so, are these roles obtained from Active Directory groupings or are they maintained elsewhere? *""maybe in a future""*
    b.	Will there be, in addition to canned application(s), and ""ad hoc"" access from general tools (like SQL tools, BI tools, or even Client tools)? *""Correct. this is the primary reason why LDAP support is required (something which would not be really critical if canned applications was all there was to it)""*
    i.	If affirmative - what is the security model? Individual grants for each user, group grants for business roles? *""The latter, mainly. In specific cases where a single individual has specific database privileges, we are fine creating a dedicated group in AD""* 
4.	Deployment models
   a.	Will MaxScale be the required component, or do you plan load balancers on app server or Clients to deal with XPAND cluster (and its scale up and down events)? *""Yes""*
 $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Blocker,30,,0,1,0,4,0,5,0,,0,850,1,4,0,2021-04-12 09:16:48,Extend PAM support to include Group Mapping,"At present, MaxScale can do a lot of what MariaDB server can do with PAM - except for group mapping. This is a request to upgrade that support to the fuller parity with the server. 

Ideally, the installation and deployment would mimic the ways and means used when the server is so configured. Yet certain higher features of the server syntax such as explicitly defined ROLES and syntactically explicit GRANT PROXY do not need to be supported in the same exact way.

*Justification.*
MaxScale is a required part of XPAND deployments. Current sales priority for XPAND is so called ""performance option"", whereby MDB server is bypassed altogether, and the workload is distributed by MaxScale directly to the nodes of XPAND cluster. XPAND does not support LDAP at present.

A customer wants to use Active Directory in this configuration. Currently we can satisfy the authentication part (including two phase) using MaxScale. The theory is that it a good deal easier and architecturally better to add group mapping to MaxScale than to build a whole PAM apparatus in XPAND.

*Easier* - very likely
*Architecturally Better*: XPAND is presented to the market as a ""smart engine"". When it operates under MDB Server, all security is handled by PAM of the server. ""Performance option"" is just that, performance option. It relies on MaxScale for many serious things, from workload distribution to transaction replay. Given that MaxScale also provides security checks and even access controls, it is certainly stands to reason that not duplicating the functionality is better than duplicating it.

*Customer Requirements*
1.	General corporate policies - 
    a.	are users treated as individuals, or are the assigned to groups? *""Yes, assigned to groups in AD. We need to be able to map these groups into small number of XPAND users, each representing a  business role""*
    b.	If so, can one user be in multiple groups? *""yes. initially, the first group retrieved by PAM should be used as default. Additional refinements may be considered for later""*
2.	Required operation with database:
    a.	Authentication by Active Directory on every connect request is assumed, but:
    b.	Is two-factor authentication for database access required? *""No""*
    c.	Mapping of AD groups into database users (associating users with business roles and so minimizing grant maintenance requirements)? *""Yes, as above""*
3.	Business rules. 
    a.	We hear canned applications are using session pooling. Is it true? If so, what is the current access control scheme? *""We have one schema per application. Applications are all microservices. All users are validated by the app server, all database connections go under the same userid""*
    i.	Authentication by web server or app server? *""Yes, as above""*
              1.        If so, do all sessions use the same userid, or *""Yes now""*
              2.	Are sessions grouped somehow into ""business roles""? If so, are these roles obtained from Active Directory groupings or are they maintained elsewhere? *""maybe in a future""*
    b.	Will there be, in addition to canned application(s), and ""ad hoc"" access from general tools (like SQL tools, BI tools, or even Client tools)? *""Correct. this is the primary reason why LDAP support is required (something which would not be really critical if canned applications was all there was to it)""*
    i.	If affirmative - what is the security model? Individual grants for each user, group grants for business roles? *""The latter, mainly. In specific cases where a single individual has specific database privileges, we are fine creating a dedicated group in AD""* 
4.	Deployment models
   a.	Will MaxScale be the required component, or do you plan load balancers on app server or Clients to deal with XPAND cluster (and its scale up and down events)? *""Yes""*
",,0,1,0,3,0.00323625,"Extend PAM support to include Group Mapping $end$ At present, MaxScale can do a lot of what MariaDB server can do with PAM - except for group mapping. This is a request to upgrade that support to the fuller parity with the server. 

Ideally, the installation and deployment would mimic the ways and means used when the server is so configured. Yet certain higher features of the server syntax such as explicitly defined ROLES and syntactically explicit GRANT PROXY do not need to be supported in the same exact way.

*Justification.*
MaxScale is a required part of XPAND deployments. Current sales priority for XPAND is so called ""performance option"", whereby MDB server is bypassed altogether, and the workload is distributed by MaxScale directly to the nodes of XPAND cluster. XPAND does not support LDAP at present.

A customer wants to use Active Directory in this configuration. Currently we can satisfy the authentication part (including two phase) using MaxScale. The theory is that it a good deal easier and architecturally better to add group mapping to MaxScale than to build a whole PAM apparatus in XPAND.

*Easier* - very likely
*Architecturally Better*: XPAND is presented to the market as a ""smart engine"". When it operates under MDB Server, all security is handled by PAM of the server. ""Performance option"" is just that, performance option. It relies on MaxScale for many serious things, from workload distribution to transaction replay. Given that MaxScale also provides security checks and even access controls, it is certainly stands to reason that not duplicating the functionality is better than duplicating it.

*Customer Requirements*
1.	General corporate policies - 
    a.	are users treated as individuals, or are the assigned to groups? *""Yes, assigned to groups in AD. We need to be able to map these groups into small number of XPAND users, each representing a  business role""*
    b.	If so, can one user be in multiple groups? *""yes. initially, the first group retrieved by PAM should be used as default. Additional refinements may be considered for later""*
2.	Required operation with database:
    a.	Authentication by Active Directory on every connect request is assumed, but:
    b.	Is two-factor authentication for database access required? *""No""*
    c.	Mapping of AD groups into database users (associating users with business roles and so minimizing grant maintenance requirements)? *""Yes, as above""*
3.	Business rules. 
    a.	We hear canned applications are using session pooling. Is it true? If so, what is the current access control scheme? *""We have one schema per application. Applications are all microservices. All users are validated by the app server, all database connections go under the same userid""*
    i.	Authentication by web server or app server? *""Yes, as above""*
              1.        If so, do all sessions use the same userid, or *""Yes now""*
              2.	Are sessions grouped somehow into ""business roles""? If so, are these roles obtained from Active Directory groupings or are they maintained elsewhere? *""maybe in a future""*
    b.	Will there be, in addition to canned application(s), and ""ad hoc"" access from general tools (like SQL tools, BI tools, or even Client tools)? *""Correct. this is the primary reason why LDAP support is required (something which would not be really critical if canned applications was all there was to it)""*
    i.	If affirmative - what is the security model? Individual grants for each user, group grants for business roles? *""The latter, mainly. In specific cases where a single individual has specific database privileges, we are fine creating a dedicated group in AD""* 
4.	Deployment models
   a.	Will MaxScale be the required component, or do you plan load balancers on app server or Clients to deal with XPAND cluster (and its scale up and down events)? *""Yes""*
 $acceptance criteria:$",1,1,0,0,0,0,1,282.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1548,MXS-3477,Task,MXS,2021-03-31 17:17:58,MXS-3387,0,Use VARCHAR as id type and make length configurable with default of 24.,,,Use VARCHAR as id type and make length configurable with default of 24. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-12 05:24:07,Use VARCHAR as id type and make length configurable with default of 24.,,,0,0,0,0,0.0,Use VARCHAR as id type and make length configurable with default of 24. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,276.1,433,40,0.0923788,17,0.039261,12,0.0277136,9,0.0207852,9,0.0207852
1549,MXS-3480,Sub-Task,MXS,2021-04-01 09:00:25,,0,Researching for javascript editor library that can be integrated to Vue.js,"The library needs to meet at least the following requirements:
* MIT license
* Regularly maintained and supported
* Easy to use in Vue.js
* Easy to customize and extend in terms of css styling and functionalities
* Syntax highlighting and auto completion
* Auto suggest table names.
* Option to format sql code
 
 ",,"Researching for javascript editor library that can be integrated to Vue.js $end$ The library needs to meet at least the following requirements:
* MIT license
* Regularly maintained and supported
* Easy to use in Vue.js
* Easy to customize and extend in terms of css styling and functionalities
* Syntax highlighting and auto completion
* Auto suggest table names.
* Option to format sql code
 
  $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,1,0,2,0,1,0,,0,850,1,0,0,2021-04-01 09:00:25,Researching for javascript editor library that can be integrated to Vue.js,"The library needs to meet at least the following requirements:
* MIT license
* Regularly maintained and supported
* Easy to use in Vue.js
* Easy to customize and extend in terms of css styling and functionalities
* Syntax highlighting and auto completion
* Auto suggest table names.
 
 ",,0,1,0,6,0.0967742,"Researching for javascript editor library that can be integrated to Vue.js $end$ The library needs to meet at least the following requirements:
* MIT license
* Regularly maintained and supported
* Easy to use in Vue.js
* Easy to customize and extend in terms of css styling and functionalities
* Syntax highlighting and auto completion
* Auto suggest table names.
 
  $acceptance criteria:$",1,1,1,0,0,0,1,0.0,12,4,0.333333,4,0.333333,3,0.25,3,0.25,3,0.25
1550,MXS-3482,New Feature,MXS,2021-04-06 09:22:48,,0,Add option file for maxctrl,"Similar to the {{mysql}} command line client, MaxCtrl should have a file for options. This helps when the default administrative user is changed.

",,"Add option file for maxctrl $end$ Similar to the {{mysql}} command line client, MaxCtrl should have a file for options. This helps when the default administrative user is changed.

 $acceptance criteria:$",,Nilnandan Joshi,Nilnandan Joshi,Major,8,,0,4,0,1,0,0,0,,0,850,1,0,0,2021-05-24 09:35:44,Add option file for maxctrl,"Similar to the {{mysql}} command line client, MaxCtrl should have a file for options. This helps when the default administrative user is changed.

",,0,0,0,0,0.0,"Add option file for maxctrl $end$ Similar to the {{mysql}} command line client, MaxCtrl should have a file for options. This helps when the default administrative user is changed.

 $acceptance criteria:$",0,0,0,0,0,0,0,1152.2,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1551,MXS-3484,Task,MXS,2021-04-06 13:07:40,MXS-3387,0,Add command for Mongo -> SQL conversion.,Add MaxScale specific command using which it is possible to query from the Mongo shell what SQL a particular Mongo command will be converted into. Valuable both during development and as a debugging tool for end-users.,,Add command for Mongo -> SQL conversion. $end$ Add MaxScale specific command using which it is possible to query from the Mongo shell what SQL a particular Mongo command will be converted into. Valuable both during development and as a debugging tool for end-users. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-12 05:24:02,Add command for Mongo -> SQL conversion.,Add MaxScale specific command using which it is possible to query from the Mongo shell what SQL a particular Mongo command will be converted into. Valuable both during development and as a debugging tool for end-users.,,0,0,0,0,0.0,Add command for Mongo -> SQL conversion. $end$ Add MaxScale specific command using which it is possible to query from the Mongo shell what SQL a particular Mongo command will be converted into. Valuable both during development and as a debugging tool for end-users. $acceptance criteria:$,0,0,0,0,0,0,0,136.267,434,40,0.0921659,17,0.0391705,12,0.0276498,9,0.0207373,9,0.0207373
1552,MXS-3488,Task,MXS,2021-04-09 08:30:55,MXS-3387,0,Add option for MaxScale specific handling of 'ordered',,,Add option for MaxScale specific handling of 'ordered' $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-12 06:27:44,Add option for MaxScale specific handling of 'ordered',,,0,0,0,0,0.0,Add option for MaxScale specific handling of 'ordered' $end$ $acceptance criteria:$,0,0,0,0,0,0,0,69.9333,435,40,0.091954,17,0.0390805,12,0.0275862,9,0.0206897,9,0.0206897
1553,MXS-3489,Task,MXS,2021-04-09 08:32:20,MXS-3387,0,Make it possible to change the config at runtime.,,,Make it possible to change the config at runtime. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-12 06:27:41,Make it possible to change the config at runtime.,,,0,0,0,0,0.0,Make it possible to change the config at runtime. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,69.9167,436,40,0.0917431,17,0.0389908,12,0.0275229,9,0.0206422,9,0.0206422
1554,MXS-3490,New Feature,MXS,2021-04-12 07:00:47,,0,Xpand monitor should detect and handle group change explicitly,"Currently the Xpand monitor treats group change errors as any other error. That is, it'll cause the monitor to abandon the current ""hub"" (the Xpand node it uses for fetching cluster topology information) and connect to another node, which will fail with a group change error. After that the monitor will at regular intervals connect to each node, which will fail, until the group change is over.

At the same time, the monitor will ping the health check port of each node and but for a node that is removed, it will continue to return OK. That is, as far as any routers are concerned those nodes/servers appear to be ready to use. However, that's just an appearance as any attempt to use them will end with a group change error.

This means that there will be an awful amount of activity and error handling that simply cannot be resolved before the group change is over. Thus, the Xpand monitor:

* should detect whenever a monitor operation fails due to a group change, and in that case
* stop the normal health check ping,
* mark all servers (internally) as being down,
* regularly connect in order to find out whether the group change has finished, and in that case
* check the cluster configuration and remove/add servers, and
* turn on the regular health check ping, which will cause the servers to be marked as being up.

That way a great deal of activity will basically stop for the duration of the group change. Until the group change is over, there is no point in doing anything else than checking whether the group change is over. ",,"Xpand monitor should detect and handle group change explicitly $end$ Currently the Xpand monitor treats group change errors as any other error. That is, it'll cause the monitor to abandon the current ""hub"" (the Xpand node it uses for fetching cluster topology information) and connect to another node, which will fail with a group change error. After that the monitor will at regular intervals connect to each node, which will fail, until the group change is over.

At the same time, the monitor will ping the health check port of each node and but for a node that is removed, it will continue to return OK. That is, as far as any routers are concerned those nodes/servers appear to be ready to use. However, that's just an appearance as any attempt to use them will end with a group change error.

This means that there will be an awful amount of activity and error handling that simply cannot be resolved before the group change is over. Thus, the Xpand monitor:

* should detect whenever a monitor operation fails due to a group change, and in that case
* stop the normal health check ping,
* mark all servers (internally) as being down,
* regularly connect in order to find out whether the group change has finished, and in that case
* check the cluster configuration and remove/add servers, and
* turn on the regular health check ping, which will cause the servers to be marked as being up.

That way a great deal of activity will basically stop for the duration of the group change. Until the group change is over, there is no point in doing anything else than checking whether the group change is over.  $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,14,,1,2,1,2,0,0,0,,0,850,1,0,0,2022-03-14 10:04:52,Xpand monitor should detect and handle group change explicitly,"Currently the Xpand monitor treats group change errors as any other error. That is, it'll cause the monitor to abandon the current ""hub"" (the Xpand node it uses for fetching cluster topology information) and connect to another node, which will fail with a group change error. After that the monitor will at regular intervals connect to each node, which will fail, until the group change is over.

At the same time, the monitor will ping the health check port of each node and but for a node that is removed, it will continue to return OK. That is, as far as any routers are concerned those nodes/servers appear to be ready to use. However, that's just an appearance as any attempt to use them will end with a group change error.

This means that there will be an awful amount of activity and error handling that simply cannot be resolved before the group change is over. Thus, the Xpand monitor:

* should detect whenever a monitor operation fails due to a group change, and in that case
* stop the normal health check ping,
* mark all servers (internally) as being down,
* regularly connect in order to find out whether the group change has finished, and in that case
* check the cluster configuration and remove/add servers, and
* turn on the regular health check ping, which will cause the servers to be marked as being up.

That way a great deal of activity will basically stop for the duration of the group change. Until the group change is over, there is no point in doing anything else than checking whether the group change is over. ",,0,0,0,0,0.0,"Xpand monitor should detect and handle group change explicitly $end$ Currently the Xpand monitor treats group change errors as any other error. That is, it'll cause the monitor to abandon the current ""hub"" (the Xpand node it uses for fetching cluster topology information) and connect to another node, which will fail with a group change error. After that the monitor will at regular intervals connect to each node, which will fail, until the group change is over.

At the same time, the monitor will ping the health check port of each node and but for a node that is removed, it will continue to return OK. That is, as far as any routers are concerned those nodes/servers appear to be ready to use. However, that's just an appearance as any attempt to use them will end with a group change error.

This means that there will be an awful amount of activity and error handling that simply cannot be resolved before the group change is over. Thus, the Xpand monitor:

* should detect whenever a monitor operation fails due to a group change, and in that case
* stop the normal health check ping,
* mark all servers (internally) as being down,
* regularly connect in order to find out whether the group change has finished, and in that case
* check the cluster configuration and remove/add servers, and
* turn on the regular health check ping, which will cause the servers to be marked as being up.

That way a great deal of activity will basically stop for the duration of the group change. Until the group change is over, there is no point in doing anything else than checking whether the group change is over.  $acceptance criteria:$",0,0,0,0,0,0,1,8067.07,437,40,0.0915332,17,0.0389016,12,0.02746,9,0.020595,9,0.020595
1555,MXS-3491,Sub-Task,MXS,2021-04-12 07:49:49,,0,Cleanup and bump package version,,,Cleanup and bump package version $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-04-12 07:49:49,Cleanup and bump package version,,,0,0,0,0,0.0,Cleanup and bump package version $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,13,5,0.384615,5,0.384615,3,0.230769,3,0.230769,3,0.230769
1556,MXS-3492,Task,MXS,2021-04-12 09:15:39,,0,Setup demo MaxScale/MaxGUI environment.,,,Setup demo MaxScale/MaxGUI environment. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,1,0,2,0,0,0,,0,850,1,0,0,2021-04-12 09:15:39,Setup demo MaxScale/MaxGUI environment.,,,0,0,0,0,0.0,Setup demo MaxScale/MaxGUI environment. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,438,40,0.0913242,17,0.0388128,12,0.0273973,9,0.0205479,9,0.0205479
1557,MXS-3493,Task,MXS,2021-04-12 09:20:48,MXS-3464,0,Create split panel component,This component allows to resize panels vertically or horizontally ,,Create split panel component $end$ This component allows to resize panels vertically or horizontally  $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,9,,0,0,0,1,0,3,0,,0,850,0,0,0,2021-04-12 09:22:19,Create resizable wrapper component,This component allows its child to be resizable,,1,2,0,13,0.533333,Create resizable wrapper component $end$ This component allows its child to be resizable $acceptance criteria:$,3,1,1,1,0,0,1,0.0166667,14,5,0.357143,5,0.357143,3,0.214286,3,0.214286,3,0.214286
1558,MXS-3494,Task,MXS,2021-04-12 09:31:49,MXS-3464,0,Create query page layout view,"The layout is drafted as follows until the UI design is provided:
Left column:  Table list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable.",,"Create query page layout view $end$ The layout is drafted as follows until the UI design is provided:
Left column:  Table list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,1,0,1,3,,0,850,0,0,0,2021-04-12 09:32:20,Create query editor layout view,"The layout is drafted as follows until the UI design is provided:
Left column:  Table list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable.",,1,0,0,2,0.0208333,"Create query editor layout view $end$ The layout is drafted as follows until the UI design is provided:
Left column:  Table list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable. $acceptance criteria:$",1,1,0,0,0,0,0,0.0,15,6,0.4,6,0.4,4,0.266667,3,0.2,3,0.2
1559,MXS-3495,Task,MXS,2021-04-12 09:56:12,,0,Create tests for Kafka to MariaDB router,,,Create tests for Kafka to MariaDB router $end$ $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-04-12 09:56:12,Create tests for Kafka to MariaDB router,,,0,0,0,0,0.0,Create tests for Kafka to MariaDB router $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,109,15,0.137615,10,0.0917431,8,0.0733945,8,0.0733945,7,0.0642202
1560,MXS-3496,Task,MXS,2021-04-12 09:56:54,,0,Document Kafka to MariaDB router,,,Document Kafka to MariaDB router $end$ $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-04-12 09:56:54,Document Kafka to MariaDB router,,,0,0,0,0,0.0,Document Kafka to MariaDB router $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,110,15,0.136364,10,0.0909091,8,0.0727273,8,0.0727273,7,0.0636364
1561,MXS-3497,Task,MXS,2021-04-12 09:57:44,,0,Extend mirror router tests,Add tests for the Kafka exporter and any new features that were added for 2.6.,,Extend mirror router tests $end$ Add tests for the Kafka exporter and any new features that were added for 2.6. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-04-12 09:57:44,Extend mirror router tests,Add tests for the Kafka exporter and any new features that were added for 2.6.,,0,0,0,0,0.0,Extend mirror router tests $end$ Add tests for the Kafka exporter and any new features that were added for 2.6. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,111,15,0.135135,10,0.0900901,8,0.0720721,8,0.0720721,7,0.0630631
1562,MXS-3499,New Feature,MXS,2021-04-13 08:31:26,,0,Add prepared statement support to causal_reads,"The current causal_reads mechanism relies on the SQL in question being modifiable. As this is not possible for COM_STMT_EXECUTE queries, a different synchronization mechanism must be created for causal_reads=local and causal_reads=global.

One option is to route a normal COM_QUERY packet with the MASTER_GTID_WAIT to the server and then sending the COM_STMT_EXECUTE without waiting for the response. The result of the MASTER_GTID_WAIT would be ignored and the query would never be retried if the MASTER_GTID_WAIT failed. This mode would have a ""best-effort"" guarantee of the causality of the reads.

Another option is to just wait for the COM_QUERY to finish before sending the COM_STMT_EXECUTE to the server. This would still improve read scaling but it would roughly double the latency for each query.

The third option is to use the BEGIN NOT ATOMIC syntax combined with immediate execution of prepared statements. This retains the same limitations and benefits that the COM_QUERY ONE does with the exception of causing an extra prepare-execute-close cycle to occur in the server.


----
*Original description:*

Hi

Maxscale is not able to deal with causal read and read/write split. This force to use only one node of the topology in place. 


",,"Add prepared statement support to causal_reads $end$ The current causal_reads mechanism relies on the SQL in question being modifiable. As this is not possible for COM_STMT_EXECUTE queries, a different synchronization mechanism must be created for causal_reads=local and causal_reads=global.

One option is to route a normal COM_QUERY packet with the MASTER_GTID_WAIT to the server and then sending the COM_STMT_EXECUTE without waiting for the response. The result of the MASTER_GTID_WAIT would be ignored and the query would never be retried if the MASTER_GTID_WAIT failed. This mode would have a ""best-effort"" guarantee of the causality of the reads.

Another option is to just wait for the COM_QUERY to finish before sending the COM_STMT_EXECUTE to the server. This would still improve read scaling but it would roughly double the latency for each query.

The third option is to use the BEGIN NOT ATOMIC syntax combined with immediate execution of prepared statements. This retains the same limitations and benefits that the COM_QUERY ONE does with the exception of causing an extra prepare-execute-close cycle to occur in the server.


----
*Original description:*

Hi

Maxscale is not able to deal with causal read and read/write split. This force to use only one node of the topology in place. 


 $acceptance criteria:$",,Massimo,Massimo,Major,13,,0,6,1,1,0,2,0,,0,850,1,2,0,2021-05-11 10:23:23,Add prepared statement support to causal_reads,"The current causal_reads mechanism relies on the SQL in question being modifiable. As this is not possible for COM_STMT_EXECUTE queries, a different synchronization mechanism must be created for causal_reads=local and causal_reads=global.

One option is to route a normal COM_QUERY packet with the MASTER_GTID_WAIT to the server and then sending the COM_STMT_EXECUTE without waiting for the response. The result of the MASTER_GTID_WAIT would be ignored and the query would never be retried if the MASTER_GTID_WAIT failed. This mode would have a ""best-effort"" guarantee of the causality of the reads.

Another option is to just wait for the COM_QUERY to finish before sending the COM_STMT_EXECUTE to the server. This would still improve read scaling but it would roughly double the latency for each query.

The third option is to use the BEGIN NOT ATOMIC syntax combined with immediate execution of prepared statements. This retains the same limitations and benefits that the COM_QUERY ONE does with the exception of causing an extra prepare-execute-close cycle to occur in the server.


----
*Original description:*

Hi

Maxscale is not able to deal with causal read and read/write split. This force to use only one node of the topology in place. 


",,0,0,0,0,0.0,"Add prepared statement support to causal_reads $end$ The current causal_reads mechanism relies on the SQL in question being modifiable. As this is not possible for COM_STMT_EXECUTE queries, a different synchronization mechanism must be created for causal_reads=local and causal_reads=global.

One option is to route a normal COM_QUERY packet with the MASTER_GTID_WAIT to the server and then sending the COM_STMT_EXECUTE without waiting for the response. The result of the MASTER_GTID_WAIT would be ignored and the query would never be retried if the MASTER_GTID_WAIT failed. This mode would have a ""best-effort"" guarantee of the causality of the reads.

Another option is to just wait for the COM_QUERY to finish before sending the COM_STMT_EXECUTE to the server. This would still improve read scaling but it would roughly double the latency for each query.

The third option is to use the BEGIN NOT ATOMIC syntax combined with immediate execution of prepared statements. This retains the same limitations and benefits that the COM_QUERY ONE does with the exception of causing an extra prepare-execute-close cycle to occur in the server.


----
*Original description:*

Hi

Maxscale is not able to deal with causal read and read/write split. This force to use only one node of the topology in place. 


 $acceptance criteria:$",0,0,0,0,0,0,0,673.85,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1563,MXS-3500,Sub-Task,MXS,2021-04-14 12:45:39,,0,Create Schema list column with mock data,"User should be able to see all  schemas, table names and columns. 
* The list should be represented as tree view where user can toggle and copy schema. 
* Search input to filter schemas objects
* Option to preview schema data, view schema details, place schema as sql to editor view",,"Create Schema list column with mock data $end$ User should be able to see all  schemas, table names and columns. 
* The list should be represented as tree view where user can toggle and copy schema. 
* Search input to filter schemas objects
* Option to preview schema data, view schema details, place schema as sql to editor view $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-14 12:45:39,Create Schema list column with mock data,"User should be able to see all  schemas, table names and columns. 
* The list should be represented as tree view where user can toggle and copy schema. 
* Search input to filter schemas objects
* Option to preview schema data, view schema details, place schema as sql to editor view",,0,0,0,0,0.0,"Create Schema list column with mock data $end$ User should be able to see all  schemas, table names and columns. 
* The list should be represented as tree view where user can toggle and copy schema. 
* Search input to filter schemas objects
* Option to preview schema data, view schema details, place schema as sql to editor view $acceptance criteria:$",0,0,0,0,0,0,0,0.0,16,7,0.4375,6,0.375,4,0.25,3,0.1875,3,0.1875
1564,MXS-3502,Sub-Task,MXS,2021-04-14 13:03:34,,0,Create basic layout,"Left column: Schema list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable.",,"Create basic layout $end$ Left column: Schema list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-14 13:03:34,Create basic layout,"Left column: Schema list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable.",,0,0,0,0,0.0,"Create basic layout $end$ Left column: Schema list section.
Right column: Editor view at top position and query result view at bottom.
The width and height of the layout should be resizable. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,17,7,0.411765,6,0.352941,4,0.235294,3,0.176471,3,0.176471
1565,MXS-3507,Task,MXS,2021-04-21 08:31:22,MXS-3387,0,Implement getLastError and resetLastError,,,Implement getLastError and resetLastError $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-26 10:31:54,Implement getLastError and resetLastError,,,0,0,0,0,0.0,Implement getLastError and resetLastError $end$ $acceptance criteria:$,0,0,0,0,0,0,0,122.0,439,40,0.0911162,17,0.0387244,12,0.0273349,9,0.0205011,9,0.0205011
1566,MXS-3510,Sub-Task,MXS,2021-04-22 13:33:24,,0,Refactor data-table,"`data-table` which is the wrapper component of `v-data-table`
 having performance issue when rendering large data. e.g. 10000 rows.
With `v-data-table`, this is not an issue but with `data-table`, it shows latency and lag in rendering data",,"Refactor data-table $end$ `data-table` which is the wrapper component of `v-data-table`
 having performance issue when rendering large data. e.g. 10000 rows.
With `v-data-table`, this is not an issue but with `data-table`, it shows latency and lag in rendering data $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-04-22 13:33:24,Refactor data-table,"`data-table` which is the wrapper component of `v-data-table`
 having performance issue when rendering large data. e.g. 10000 rows.
With `v-data-table`, this is not an issue but with `data-table`, it shows latency and lag in rendering data",,0,0,0,0,0.0,"Refactor data-table $end$ `data-table` which is the wrapper component of `v-data-table`
 having performance issue when rendering large data. e.g. 10000 rows.
With `v-data-table`, this is not an issue but with `data-table`, it shows latency and lag in rendering data $acceptance criteria:$",0,0,0,0,0,0,1,0.0,18,7,0.388889,6,0.333333,4,0.222222,3,0.166667,3,0.166667
1567,MXS-3511,Task,MXS,2021-04-26 07:29:39,,0,Update config.guess and config.sub of sqlite3 in qc_sqlite,"They are from 2007 so are not that good at guessing anymore. Update them from the latest sqlite3 release.
",,"Update config.guess and config.sub of sqlite3 in qc_sqlite $end$ They are from 2007 so are not that good at guessing anymore. Update them from the latest sqlite3 release.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-26 07:29:39,Update config.guess and config.sub of sqlite3 in qc_sqlite,"They are from 2007 so are not that good at guessing anymore. Update them from the latest sqlite3 release.
",,0,0,0,0,0.0,"Update config.guess and config.sub of sqlite3 in qc_sqlite $end$ They are from 2007 so are not that good at guessing anymore. Update them from the latest sqlite3 release.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,440,40,0.0909091,17,0.0386364,12,0.0272727,9,0.0204545,9,0.0204545
1568,MXS-3512,Sub-Task,MXS,2021-04-26 08:38:21,,0,Add togglable table columns ,,,Add togglable table columns  $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,10,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-04-26 08:38:21,Add togglable table columns ,,,0,0,0,0,0.0,Add togglable table columns  $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,19,7,0.368421,6,0.315789,4,0.210526,3,0.157895,3,0.157895
1569,MXS-3516,Sub-Task,MXS,2021-04-26 10:32:02,,0,Migrate vuetify to 2.3 version,2.3 version contains v-virtual-scroll which is needed to create a custom table for displaying query results.,,Migrate vuetify to 2.3 version $end$ 2.3 version contains v-virtual-scroll which is needed to create a custom table for displaying query results. $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-04-26 10:32:02,Migrate vuetify to 2.3 version,2.3 version contains v-virtual-scroll which is needed to create a custom table for displaying query results.,,0,0,0,0,0.0,Migrate vuetify to 2.3 version $end$ 2.3 version contains v-virtual-scroll which is needed to create a custom table for displaying query results. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,20,7,0.35,6,0.3,4,0.2,3,0.15,3,0.15
1570,MXS-3517,Sub-Task,MXS,2021-04-26 10:33:34,,0,Create custom table using div that utilises v-virtual-scroll,A custom table that have all the styles of current data-table but having virtual scroll for rows,,Create custom table using div that utilises v-virtual-scroll $end$ A custom table that have all the styles of current data-table but having virtual scroll for rows $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,2,0,,0,850,0,0,0,2021-04-26 10:33:34,Create custom table using div that utilises v-virtual-scroll,A custom table that have all the styles of current data-table.,,0,2,0,8,0.318182,Create custom table using div that utilises v-virtual-scroll $end$ A custom table that have all the styles of current data-table. $acceptance criteria:$,2,1,1,0,0,0,1,0.0,21,7,0.333333,6,0.285714,4,0.190476,3,0.142857,3,0.142857
1571,MXS-3518,Task,MXS,2021-04-26 10:42:36,,0,Add query endpoint to REST-API ,,,Add query endpoint to REST-API  $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-04-26 10:42:36,Add query endpoint to REST-API ,,,0,0,0,0,0.0,Add query endpoint to REST-API  $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,441,40,0.0907029,17,0.0385488,12,0.0272109,9,0.0204082,9,0.0204082
1572,MXS-3519,Task,MXS,2021-04-26 10:43:30,,0,Backend implementation of REST-API query endpoint,,,Backend implementation of REST-API query endpoint $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-04-26 10:43:30,Backend implementation of REST-API query endpoint,,,0,0,0,0,0.0,Backend implementation of REST-API query endpoint $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,442,40,0.0904977,17,0.0384615,12,0.0271493,9,0.020362,9,0.020362
1573,MXS-3520,Sub-Task,MXS,2021-04-27 06:29:45,,0,Dynamic import monaco editor,"The current bundle size is significantly large, approximately 7.5 MB.",,"Dynamic import monaco editor $end$ The current bundle size is significantly large, approximately 7.5 MB. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,2,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-04-27 06:29:45,Dynamic import monaco editor,"The current bundle size is significantly large, approximately 7.5 MB.",,0,0,0,0,0.0,"Dynamic import monaco editor $end$ The current bundle size is significantly large, approximately 7.5 MB. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,22,8,0.363636,7,0.318182,4,0.181818,3,0.136364,3,0.136364
1574,MXS-3521,Task,MXS,2021-04-27 14:48:42,MXS-3464,0,Integrate with query API,"Open connection and send query via the following endpoint
{code:java}
/query
{code}
Request body params: user, password, db, sql and timeout",,"Integrate with query API $end$ Open connection and send query via the following endpoint
{code:java}
/query
{code}
Request body params: user, password, db, sql and timeout $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,3,0,1,1,,0,850,0,0,0,2021-04-27 14:48:42,Integrate with query API,"Open connection and send query via the following endpoint
{code:java}
/servers/:server/query
{code}
Request body params: user, password, db, sql and timeout",,0,1,0,2,0.0357143,"Integrate with query API $end$ Open connection and send query via the following endpoint
{code:java}
/servers/:server/query
{code}
Request body params: user, password, db, sql and timeout $acceptance criteria:$",1,1,0,0,0,0,1,0.0,23,8,0.347826,7,0.304348,4,0.173913,3,0.130435,3,0.130435
1575,MXS-3522,Task,MXS,2021-04-27 14:50:11,MXS-3464,0,Dynamic import monaco editor,"The current bundle size is significantly large, approximately 7.5 MB.",,"Dynamic import monaco editor $end$ The current bundle size is significantly large, approximately 7.5 MB. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-05-18 12:22:35,Dynamic import monaco editor,"The current bundle size is significantly large, approximately 7.5 MB.",,0,0,0,0,0.0,"Dynamic import monaco editor $end$ The current bundle size is significantly large, approximately 7.5 MB. $acceptance criteria:$",0,0,0,0,0,0,0,501.533,24,9,0.375,7,0.291667,4,0.166667,3,0.125,3,0.125
1576,MXS-3539,Sub-Task,MXS,2021-05-10 12:25:55,,0,Create `truncate-string` component,"Auto truncate text if content width is larger than element width and show full text
in tooltip",,"Create `truncate-string` component $end$ Auto truncate text if content width is larger than element width and show full text
in tooltip $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-05-10 12:25:55,Create `truncate-string` component,"Auto truncate text if content width is larger than element width and show full text
in tooltip",,0,0,0,0,0.0,"Create `truncate-string` component $end$ Auto truncate text if content width is larger than element width and show full text
in tooltip $acceptance criteria:$",0,0,0,0,0,0,1,0.0,25,9,0.36,7,0.28,4,0.16,3,0.12,3,0.12
1577,MXS-3540,Sub-Task,MXS,2021-05-10 12:26:58,,0,Add re-sizable feature  for table columns,,,Add re-sizable feature  for table columns $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-05-10 12:26:58,Add re-sizable feature  for table columns,,,0,0,0,0,0.0,Add re-sizable feature  for table columns $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,26,9,0.346154,7,0.269231,4,0.153846,3,0.115385,3,0.115385
1578,MXS-3541,Task,MXS,2021-05-11 05:22:57,MXS-3387,0,Extract id from _id in document.,,,Extract id from _id in document. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-05-11 05:22:57,Extract id from _id in document.,,,0,0,0,0,0.0,Extract id from _id in document. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,443,40,0.0902935,17,0.0383747,12,0.027088,9,0.020316,9,0.020316
1579,MXS-3542,Task,MXS,2021-05-11 09:11:11,MXS-3387,0,Factor out table creation,,,Factor out table creation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,1,0,,0,850,0,0,0,2021-05-11 09:11:11,Add command mxsCreateCollection,,,1,0,0,7,0.666667,Add command mxsCreateCollection $end$ $acceptance criteria:$,1,1,1,0,0,0,1,0.0,444,40,0.0900901,17,0.0382883,12,0.027027,9,0.0202703,9,0.0202703
1580,MXS-3543,Sub-Task,MXS,2021-05-11 11:31:39,,0,Show result sets of query statements in several tables,"Show result sets of query statements in several tables.
There should be a way to navigate between result sets",,"Show result sets of query statements in several tables $end$ Show result sets of query statements in several tables.
There should be a way to navigate between result sets $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-05-11 11:31:39,Show result sets of query statements in several tables,"Show result sets of query statements in several tables.
There should be a way to navigate between result sets",,0,0,0,0,0.0,"Show result sets of query statements in several tables $end$ Show result sets of query statements in several tables.
There should be a way to navigate between result sets $acceptance criteria:$",0,0,0,0,0,0,1,0.0,27,9,0.333333,7,0.259259,4,0.148148,3,0.111111,3,0.111111
1581,MXS-3545,Sub-Task,MXS,2021-05-14 09:02:09,,0,Add data export feature,"User can export and download query result to different file formats such as csv, xlsx, json",,"Add data export feature $end$ User can export and download query result to different file formats such as csv, xlsx, json $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-05-14 09:02:09,Add data export feature,"User can export and download query result to different file formats such as csv, xlsx, json",,0,0,0,0,0.0,"Add data export feature $end$ User can export and download query result to different file formats such as csv, xlsx, json $acceptance criteria:$",0,0,0,0,0,0,1,0.0,28,9,0.321429,7,0.25,4,0.142857,3,0.107143,3,0.107143
1582,MXS-3553,Sub-Task,MXS,2021-05-18 07:22:24,,0,Pagination query,,,Pagination query $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,2,,0,1,0,3,0,0,0,,0,850,1,0,0,2021-05-18 07:22:24,Pagination query,,,0,0,0,0,0.0,Pagination query $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,29,9,0.310345,7,0.241379,4,0.137931,3,0.103448,3,0.103448
1583,MXS-3555,Task,MXS,2021-05-19 05:29:20,MXS-3387,0,Add support for $size and $all,,,Add support for $size and $all $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-05-24 09:36:20,Add support for $size and $all,,,0,0,0,0,0.0,Add support for $size and $all $end$ $acceptance criteria:$,0,0,0,0,0,0,0,124.117,445,41,0.0921348,18,0.0404494,12,0.0269663,9,0.0202247,9,0.0202247
1584,MXS-3557,Task,MXS,2021-05-19 11:47:28,,0,Track average query duration for services and servers,"Currently the average query duration is tracked for servers only when a readwritesplit service uses it and is configured with {{slave_selection_criteria=ADAPTIVE_ROUTING}}. In addition, only queries classified as reads are tracked.

As query latency is a good metric for measuring the overall performance of a cluster, it should be tracked both at the service level as well as on the server level. Initially, the average should take into account all queries regardless of whether they are writes or reads.

The new statistic should be separate from the existing {{adaptive_avg_select_time}} statistic exposed in the servers REST API resource. This will keep the semantics of the existing statistic the same as it was in older releases.",,"Track average query duration for services and servers $end$ Currently the average query duration is tracked for servers only when a readwritesplit service uses it and is configured with {{slave_selection_criteria=ADAPTIVE_ROUTING}}. In addition, only queries classified as reads are tracked.

As query latency is a good metric for measuring the overall performance of a cluster, it should be tracked both at the service level as well as on the server level. Initially, the average should take into account all queries regardless of whether they are writes or reads.

The new statistic should be separate from the existing {{adaptive_avg_select_time}} statistic exposed in the servers REST API resource. This will keep the semantics of the existing statistic the same as it was in older releases. $acceptance criteria:$",,markus makela,markus makela,Major,9,,0,1,0,3,0,0,0,,0,850,1,0,0,2021-06-07 10:42:46,Track average query duration for services and servers,"Currently the average query duration is tracked for servers only when a readwritesplit service uses it and is configured with {{slave_selection_criteria=ADAPTIVE_ROUTING}}. In addition, only queries classified as reads are tracked.

As query latency is a good metric for measuring the overall performance of a cluster, it should be tracked both at the service level as well as on the server level. Initially, the average should take into account all queries regardless of whether they are writes or reads.

The new statistic should be separate from the existing {{adaptive_avg_select_time}} statistic exposed in the servers REST API resource. This will keep the semantics of the existing statistic the same as it was in older releases.",,0,0,0,0,0.0,"Track average query duration for services and servers $end$ Currently the average query duration is tracked for servers only when a readwritesplit service uses it and is configured with {{slave_selection_criteria=ADAPTIVE_ROUTING}}. In addition, only queries classified as reads are tracked.

As query latency is a good metric for measuring the overall performance of a cluster, it should be tracked both at the service level as well as on the server level. Initially, the average should take into account all queries regardless of whether they are writes or reads.

The new statistic should be separate from the existing {{adaptive_avg_select_time}} statistic exposed in the servers REST API resource. This will keep the semantics of the existing statistic the same as it was in older releases. $acceptance criteria:$",0,0,0,0,0,0,1,454.917,112,15,0.133929,10,0.0892857,8,0.0714286,8,0.0714286,7,0.0625
1585,MXS-3558,Sub-Task,MXS,2021-05-19 12:32:57,,0,Vertical result table,"There should be a button to convert the ""normal"" result table to vertical table",,"Vertical result table $end$ There should be a button to convert the ""normal"" result table to vertical table $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-05-19 12:32:57,Vertical result table,"There should be a button to convert the ""normal"" result table to vertical table",,0,0,0,0,0.0,"Vertical result table $end$ There should be a button to convert the ""normal"" result table to vertical table $acceptance criteria:$",0,0,0,0,0,0,1,0.0,30,9,0.3,7,0.233333,4,0.133333,3,0.1,3,0.1
1586,MXS-3562,Sub-Task,MXS,2021-05-21 06:01:07,,0,Add client side sorting feature to virtual scroll table,"Sorting should be enabled when total rows <= 10000 for ""normal"" table ( not vertical table)",,"Add client side sorting feature to virtual scroll table $end$ Sorting should be enabled when total rows <= 10000 for ""normal"" table ( not vertical table) $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,4,0,1,0,,0,850,0,0,0,2021-05-21 06:01:07,Add sorting feature to virtual scroll table,"Sorting should be enabled when total rows <= 10000 for ""normal"" table ( not vertical table)",,1,0,0,2,0.0769231,"Add sorting feature to virtual scroll table $end$ Sorting should be enabled when total rows <= 10000 for ""normal"" table ( not vertical table) $acceptance criteria:$",1,1,0,0,0,0,1,0.0,31,9,0.290323,7,0.225806,4,0.129032,3,0.0967742,3,0.0967742
1587,MXS-3563,Sub-Task,MXS,2021-05-21 06:04:42,,0,Add maximize/minimize button to query result section,,,Add maximize/minimize button to query result section $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,2,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-05-21 06:04:42,Add maximize/minimize button to query result section,,,0,0,0,0,0.0,Add maximize/minimize button to query result section $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,32,10,0.3125,7,0.21875,4,0.125,3,0.09375,3,0.09375
1588,MXS-3564,Sub-Task,MXS,2021-05-21 06:08:16,,0,Add collapse button to collapse query result section,,,Add collapse button to collapse query result section $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,3,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-05-21 06:08:16,Add collapse button to collapse query result section,,,0,0,0,0,0.0,Add collapse button to collapse query result section $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,33,10,0.30303,7,0.212121,4,0.121212,3,0.0909091,3,0.0909091
1589,MXS-3568,Task,MXS,2021-05-24 07:17:33,MXS-3464,0,Add context button in toolbar,"Allow users to see the current database or choose one for subsequent statements.
",,"Add context button in toolbar $end$ Allow users to see the current database or choose one for subsequent statements.
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-05-24 10:43:12,Add context button in toolbar,"Allow users to see the current database or choose one for subsequent statements.
",,0,0,0,0,0.0,"Add context button in toolbar $end$ Allow users to see the current database or choose one for subsequent statements.
 $acceptance criteria:$",0,0,0,0,0,0,0,3.41667,34,10,0.294118,7,0.205882,4,0.117647,3,0.0882353,3,0.0882353
1590,MXS-3569,Task,MXS,2021-05-24 08:43:58,MXS-3464,0,Add graph visualization feature,,,Add graph visualization feature $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2021-05-24 10:44:25,Add graph visualization feature,,,0,0,0,0,0.0,Add graph visualization feature $end$ $acceptance criteria:$,0,0,0,0,0,0,1,2.0,35,10,0.285714,7,0.2,4,0.114286,3,0.0857143,3,0.0857143
1591,MXS-3570,Task,MXS,2021-05-24 09:37:16,MXS-3387,0,Create mongodbprotocol tests using the MongoDB NodeJS client library.,,,Create mongodbprotocol tests using the MongoDB NodeJS client library. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-05-24 09:37:16,Create mongodbprotocol tests using the MongoDB NodeJS client library.,,,0,0,0,0,0.0,Create mongodbprotocol tests using the MongoDB NodeJS client library. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,446,41,0.0919283,18,0.0403587,12,0.0269058,9,0.0201794,9,0.0201794
1592,MXS-3571,Sub-Task,MXS,2021-05-24 09:40:31,,0,Show total duration time of querying ,,,Show total duration time of querying  $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-05-24 09:40:31,Show total duration time of querying ,,,0,0,0,0,0.0,Show total duration time of querying  $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,36,10,0.277778,7,0.194444,4,0.111111,3,0.0833333,3,0.0833333
1593,MXS-3583,Task,MXS,2021-06-01 07:47:02,MXS-3387,0,Take actual MongoDB test-suite into use,,,Take actual MongoDB test-suite into use $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,25,,1,1,1,8,0,0,0,,0,850,1,0,0,2021-08-09 08:06:24,Take actual MongoDB test-suite into use,,,0,0,0,0,0.0,Take actual MongoDB test-suite into use $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1656.32,447,41,0.0917226,18,0.0402685,12,0.0268456,9,0.0201342,9,0.0201342
1594,MXS-3587,Task,MXS,2021-06-02 04:33:14,MXS-3387,0,Replace working name with final name,,,Replace working name with final name $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-06-07 09:52:18,Replace working name with final name,,,0,0,0,0,0.0,Replace working name with final name $end$ $acceptance criteria:$,0,0,0,0,0,0,0,125.317,448,41,0.0915179,18,0.0401786,12,0.0267857,9,0.0200893,9,0.0200893
1595,MXS-359,New Feature,MXS,2015-09-10 10:59:34,,0,keepalive client connection on master failover  ,"h2. Planned Tasks

* Allow re-connection to master servers
* Allow the master server to change mid-session

h2. Original Issue
The case that we try to solve are very frequent in 3 tiers Java Applications , we have long running connections in a 1 to 1 model majority of connection in state Idle. Loosing the DB backend would generate JDBC errors leaving some application that don't catch exception  in a failed situation , despite we could have hidden this to all sessions that have committed  just before the failure   

Connection context tracking will be used (encoding, variables prepared statement ), 
Maxscale maintain session variable keepalive = 0 

On event 
If tmp table set keepalive = 1 
If SP Call  set keepalive = 2
If keepalive not in (1,2)  and autocomit =1 and statement set keepalive = 0 
If keepalive not in (1,2)  and autocomit =1 and query=ANY set keepalive = 0 

If keepalive not in (1,2)  and autocomit =1 and query=BEGIN set keepalive = 3
If keepalive not in (1,2)  and autocomit =1 and  keepalive = 3 and  query=ANY set keepalive = 4
If keepalive not in (1,2)  and autocomit =1 and query=COMMIT set keepalive =0 
 
If keepalive not in (1,2)  and autocomit =0 and query=COMMIT set keepalive =3 
If keepalive not in (1,2)  and autocomit =0 and query=ANY set keepalive =4 

We can keepalive in keepalive =0 and keepalive=3
If  keepalive=3  we need to START TRANSACTION on the new backend 
",,"keepalive client connection on master failover   $end$ h2. Planned Tasks

* Allow re-connection to master servers
* Allow the master server to change mid-session

h2. Original Issue
The case that we try to solve are very frequent in 3 tiers Java Applications , we have long running connections in a 1 to 1 model majority of connection in state Idle. Loosing the DB backend would generate JDBC errors leaving some application that don't catch exception  in a failed situation , despite we could have hidden this to all sessions that have committed  just before the failure   

Connection context tracking will be used (encoding, variables prepared statement ), 
Maxscale maintain session variable keepalive = 0 

On event 
If tmp table set keepalive = 1 
If SP Call  set keepalive = 2
If keepalive not in (1,2)  and autocomit =1 and statement set keepalive = 0 
If keepalive not in (1,2)  and autocomit =1 and query=ANY set keepalive = 0 

If keepalive not in (1,2)  and autocomit =1 and query=BEGIN set keepalive = 3
If keepalive not in (1,2)  and autocomit =1 and  keepalive = 3 and  query=ANY set keepalive = 4
If keepalive not in (1,2)  and autocomit =1 and query=COMMIT set keepalive =0 
 
If keepalive not in (1,2)  and autocomit =0 and query=COMMIT set keepalive =3 
If keepalive not in (1,2)  and autocomit =0 and query=ANY set keepalive =4 

We can keepalive in keepalive =0 and keepalive=3
If  keepalive=3  we need to START TRANSACTION on the new backend 
 $acceptance criteria:$",,VAROQUI Stephane,VAROQUI Stephane,Major,11,,1,1,3,1,0,2,0,,0,850,0,2,0,2018-03-06 10:28:08,keepalive client connection on master failover  ,"h2. Planned Tasks

* Allow re-connection to master servers
* Allow the master server to change mid-session

h2. Original Issue
The case that we try to solve are very frequent in 3 tiers Java Applications , we have long running connections in a 1 to 1 model majority of connection in state Idle. Loosing the DB backend would generate JDBC errors leaving some application that don't catch exception  in a failed situation , despite we could have hidden this to all sessions that have committed  just before the failure   

Connection context tracking will be used (encoding, variables prepared statement ), 
Maxscale maintain session variable keepalive = 0 

On event 
If tmp table set keepalive = 1 
If SP Call  set keepalive = 2
If keepalive not in (1,2)  and autocomit =1 and statement set keepalive = 0 
If keepalive not in (1,2)  and autocomit =1 and query=ANY set keepalive = 0 

If keepalive not in (1,2)  and autocomit =1 and query=BEGIN set keepalive = 3
If keepalive not in (1,2)  and autocomit =1 and  keepalive = 3 and  query=ANY set keepalive = 4
If keepalive not in (1,2)  and autocomit =1 and query=COMMIT set keepalive =0 
 
If keepalive not in (1,2)  and autocomit =0 and query=COMMIT set keepalive =3 
If keepalive not in (1,2)  and autocomit =0 and query=ANY set keepalive =4 

We can keepalive in keepalive =0 and keepalive=3
If  keepalive=3  we need to START TRANSACTION on the new backend 
",,0,0,0,0,0.0,"keepalive client connection on master failover   $end$ h2. Planned Tasks

* Allow re-connection to master servers
* Allow the master server to change mid-session

h2. Original Issue
The case that we try to solve are very frequent in 3 tiers Java Applications , we have long running connections in a 1 to 1 model majority of connection in state Idle. Loosing the DB backend would generate JDBC errors leaving some application that don't catch exception  in a failed situation , despite we could have hidden this to all sessions that have committed  just before the failure   

Connection context tracking will be used (encoding, variables prepared statement ), 
Maxscale maintain session variable keepalive = 0 

On event 
If tmp table set keepalive = 1 
If SP Call  set keepalive = 2
If keepalive not in (1,2)  and autocomit =1 and statement set keepalive = 0 
If keepalive not in (1,2)  and autocomit =1 and query=ANY set keepalive = 0 

If keepalive not in (1,2)  and autocomit =1 and query=BEGIN set keepalive = 3
If keepalive not in (1,2)  and autocomit =1 and  keepalive = 3 and  query=ANY set keepalive = 4
If keepalive not in (1,2)  and autocomit =1 and query=COMMIT set keepalive =0 
 
If keepalive not in (1,2)  and autocomit =0 and query=COMMIT set keepalive =3 
If keepalive not in (1,2)  and autocomit =0 and query=ANY set keepalive =4 

We can keepalive in keepalive =0 and keepalive=3
If  keepalive=3  we need to START TRANSACTION on the new backend 
 $acceptance criteria:$",0,0,0,0,0,0,0,21791.5,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1596,MXS-3593,Task,MXS,2021-06-04 07:09:22,MXS-3387,0,Use multi statement when inserting documents,"Currently, if the {{insert_behavior}} is {{as_nosql}}, that is, _nosqlprotocol_ behaves just like MongoDB, there will be as many INSERT statements *and* roundtrips (between MaxScale and the backend) as there are documents.

If multi-statements are used, the number of roundtrips could be cut down to 1, which should have a significant impact on the performance.

If {{insert_behavior}} is {{as_mariadb}}, there will be just one INSERT statement and 1 roundtrip, which is likely to continue providing the best performance.",,"Use multi statement when inserting documents $end$ Currently, if the {{insert_behavior}} is {{as_nosql}}, that is, _nosqlprotocol_ behaves just like MongoDB, there will be as many INSERT statements *and* roundtrips (between MaxScale and the backend) as there are documents.

If multi-statements are used, the number of roundtrips could be cut down to 1, which should have a significant impact on the performance.

If {{insert_behavior}} is {{as_mariadb}}, there will be just one INSERT statement and 1 roundtrip, which is likely to continue providing the best performance. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-06-07 09:48:29,Use multi statement when inserting documents,"Currently, if the {{insert_behavior}} is {{as_nosql}}, that is, _nosqlprotocol_ behaves just like MongoDB, there will be as many INSERT statements *and* roundtrips (between MaxScale and the backend) as there are documents.

If multi-statements are used, the number of roundtrips could be cut down to 1, which should have a significant impact on the performance.

If {{insert_behavior}} is {{as_mariadb}}, there will be just one INSERT statement and 1 roundtrip, which is likely to continue providing the best performance.",,0,0,0,0,0.0,"Use multi statement when inserting documents $end$ Currently, if the {{insert_behavior}} is {{as_nosql}}, that is, _nosqlprotocol_ behaves just like MongoDB, there will be as many INSERT statements *and* roundtrips (between MaxScale and the backend) as there are documents.

If multi-statements are used, the number of roundtrips could be cut down to 1, which should have a significant impact on the performance.

If {{insert_behavior}} is {{as_mariadb}}, there will be just one INSERT statement and 1 roundtrip, which is likely to continue providing the best performance. $acceptance criteria:$",0,0,0,0,0,0,0,74.65,449,41,0.091314,18,0.0400891,12,0.0267261,9,0.0200445,9,0.0200445
1597,MXS-3596,Task,MXS,2021-06-07 05:47:15,,0,Create tests for configuration sync,Create a comprehensive test suite for the config_sync feature.,,Create tests for configuration sync $end$ Create a comprehensive test suite for the config_sync feature. $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-06-07 06:24:52,Create tests for configuration sync,Create a comprehensive test suite for the config_sync feature.,,0,0,0,0,0.0,Create tests for configuration sync $end$ Create a comprehensive test suite for the config_sync feature. $acceptance criteria:$,0,0,0,0,0,0,0,0.616667,113,15,0.132743,10,0.0884956,8,0.0707965,8,0.0707965,7,0.0619469
1598,MXS-3597,Task,MXS,2021-06-07 06:25:55,,0,Document configuration synchronization,Document the new parameters and how they can be used to synchronize the configurations between MaxScale instances.,,Document configuration synchronization $end$ Document the new parameters and how they can be used to synchronize the configurations between MaxScale instances. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-06-07 06:25:57,Document configuration synchronization,Document the new parameters and how they can be used to synchronize the configurations between MaxScale instances.,,0,0,0,0,0.0,Document configuration synchronization $end$ Document the new parameters and how they can be used to synchronize the configurations between MaxScale instances. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,114,15,0.131579,10,0.0877193,8,0.0701754,8,0.0701754,7,0.0614035
1599,MXS-3598,Task,MXS,2021-06-07 09:26:11,MXS-3387,0,Handle packets > 16MB.,,,Handle packets > 16MB. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-06-07 09:48:10,Handle packets > 16MB.,,,0,0,0,0,0.0,Handle packets > 16MB. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.35,450,41,0.0911111,18,0.04,12,0.0266667,9,0.02,9,0.02
1600,MXS-3599,Task,MXS,2021-06-07 09:50:20,MXS-3387,0,Profile and optimize nosqlprotocol,,,Profile and optimize nosqlprotocol $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-06-07 09:52:22,Profile and optimize nosqlprotocol,,,0,0,0,0,0.0,Profile and optimize nosqlprotocol $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0333333,451,41,0.0909091,18,0.0399113,12,0.0266075,9,0.0199557,9,0.0199557
1601,MXS-3605,New Feature,MXS,2021-06-10 14:53:49,,0,Maximum number of threads is limited to 100,"The number of routing threads in Maxscale is hard-coded to 100. This is starting to become something that could soon be exceeded on some of the larger machines that the cloud providers offer. For example the {{m5.24xlarge}} instance on AWS has 96 vCPUs and the new ARM instances ({{m6g.16xlarge}}) offer 64 cores.

Since there's no real reason to limit the number of cores if the system does indeed have more than 100 threads, the limit should be at some point either removed or at least doubled.",,"Maximum number of threads is limited to 100 $end$ The number of routing threads in Maxscale is hard-coded to 100. This is starting to become something that could soon be exceeded on some of the larger machines that the cloud providers offer. For example the {{m5.24xlarge}} instance on AWS has 96 vCPUs and the new ARM instances ({{m6g.16xlarge}}) offer 64 cores.

Since there's no real reason to limit the number of cores if the system does indeed have more than 100 threads, the limit should be at some point either removed or at least doubled. $acceptance criteria:$",,markus makela,markus makela,Minor,30,,0,0,1,1,0,1,0,,0,850,0,1,0,2022-02-14 07:54:45,Maximum number of threads is limited to 100,"The number of routing threads in Maxscale is hard-coded to 100. This is starting to become something that could soon be exceeded on some of the larger machines that the cloud providers offer. For example the {{m5.24xlarge}} instance on AWS has 96 vCPUs and the new ARM instances ({{m6g.16xlarge}}) offer 64 cores.

Since there's no real reason to limit the number of cores if the system does indeed have more than 100 threads, the limit should be at some point either removed or at least doubled.",,0,0,0,0,0.0,"Maximum number of threads is limited to 100 $end$ The number of routing threads in Maxscale is hard-coded to 100. This is starting to become something that could soon be exceeded on some of the larger machines that the cloud providers offer. For example the {{m5.24xlarge}} instance on AWS has 96 vCPUs and the new ARM instances ({{m6g.16xlarge}}) offer 64 cores.

Since there's no real reason to limit the number of cores if the system does indeed have more than 100 threads, the limit should be at some point either removed or at least doubled. $acceptance criteria:$",0,0,0,0,0,0,0,5969.0,115,15,0.130435,10,0.0869565,8,0.0695652,8,0.0695652,7,0.0608696
1602,MXS-3613,New Feature,MXS,2021-06-15 10:05:01,,0,Support PS with metadata skip (i.e. MARIADB_CLIENT_CACHE_METADATA),Add support for the MARIADB_CLIENT_CACHE_METADATA capability added in 10.6.,,Support PS with metadata skip (i.e. MARIADB_CLIENT_CACHE_METADATA) $end$ Add support for the MARIADB_CLIENT_CACHE_METADATA capability added in 10.6. $acceptance criteria:$,,markus makela,markus makela,Major,16,,0,2,1,1,0,2,0,,0,850,1,2,0,2021-10-11 09:04:52,Support PS with metadata skip (i.e. MARIADB_CLIENT_CACHE_METADATA),Add support for the MARIADB_CLIENT_CACHE_METADATA capability added in 10.6.,,0,0,0,0,0.0,Support PS with metadata skip (i.e. MARIADB_CLIENT_CACHE_METADATA) $end$ Add support for the MARIADB_CLIENT_CACHE_METADATA capability added in 10.6. $acceptance criteria:$,0,0,0,0,0,0,0,2830.98,116,15,0.12931,10,0.0862069,8,0.0689655,8,0.0689655,7,0.0603448
1603,MXS-3627,Task,MXS,2021-06-21 04:30:51,MXS-3464,0,Add trend line for graphs,,,Add trend line for graphs $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-06-21 08:53:50,Add trend line for graphs,,,0,0,0,0,0.0,Add trend line for graphs $end$ $acceptance criteria:$,0,0,0,0,0,0,1,4.36667,37,10,0.27027,7,0.189189,4,0.108108,3,0.0810811,3,0.0810811
1604,MXS-3629,Task,MXS,2021-06-21 09:36:44,,0,Make MaxScale 6.0.0 Beta release.,,,Make MaxScale 6.0.0 Beta release. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-06-21 09:36:44,Make MaxScale 6.0.0 Beta release.,,,0,0,0,0,0.0,Make MaxScale 6.0.0 Beta release. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,452,41,0.090708,18,0.039823,12,0.0265487,9,0.0199115,9,0.0199115
1605,MXS-3632,New Feature,MXS,2021-06-23 06:44:57,MXS-3631,0,Add right click context menu to schema tree,,,Add right click context menu to schema tree $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,11,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-06-23 07:53:26,Add right click context menu to schema tree,,,0,0,0,0,0.0,Add right click context menu to schema tree $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1.13333,38,10,0.263158,7,0.184211,4,0.105263,3,0.0789474,3,0.0789474
1606,MXS-3633,New Feature,MXS,2021-06-23 06:55:38,MXS-3631,0,Add more options to context menu in schema tree,"Add options such as Select top 1000 rows, Alter table, Drop table
",,"Add more options to context menu in schema tree $end$ Add options such as Select top 1000 rows, Alter table, Drop table
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,17,,0,0,1,2,0,2,1,,0,850,0,0,0,2021-10-25 09:10:27,Add more options to context menu in schema tree,"Add options such as Select top 1000 rows, Alter table, Table inspector.",,0,2,0,4,0.0833333,"Add more options to context menu in schema tree $end$ Add options such as Select top 1000 rows, Alter table, Table inspector. $acceptance criteria:$",2,1,0,0,0,0,1,2978.23,39,10,0.25641,7,0.179487,4,0.102564,3,0.0769231,3,0.0769231
1607,MXS-3634,New Feature,MXS,2021-06-23 06:59:27,MXS-3631,0,Add DDL editor,,,Add DDL editor $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,12,,0,0,2,4,0,0,5,,0,850,0,0,0,2021-09-10 12:45:14,Add DDL editor,,,0,0,0,0,0.0,Add DDL editor $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1901.75,40,11,0.275,7,0.175,4,0.1,3,0.075,3,0.075
1608,MXS-3635,New Feature,MXS,2021-06-23 07:21:21,MXS-3631,0,"Quicker access to ""Place Schema in Editor""","Being able to drag and drop the table name/ column name into the editor.
Potential solution: https://github.com/microsoft/monaco-editor/issues/1050",,"Quicker access to ""Place Schema in Editor"" $end$ Being able to drag and drop the table name/ column name into the editor.
Potential solution: https://github.com/microsoft/monaco-editor/issues/1050 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,6,,0,0,0,1,0,1,0,,0,850,0,0,0,2021-07-12 05:40:50,"Quicker access to ""Place Schema in Editor""","Being able to drag and drop the table name/ column name into the editor.  OR double click it to put it in the editor for faster access.

Potential solution: https://github.com/microsoft/monaco-editor/issues/1050",,0,1,0,13,0.325,"Quicker access to ""Place Schema in Editor"" $end$ Being able to drag and drop the table name/ column name into the editor.  OR double click it to put it in the editor for faster access.

Potential solution: https://github.com/microsoft/monaco-editor/issues/1050 $acceptance criteria:$",1,1,1,1,0,0,1,454.317,41,11,0.268293,7,0.170732,4,0.097561,3,0.0731707,3,0.0731707
1609,MXS-3636,New Feature,MXS,2021-06-23 07:29:23,MXS-3631,0,Filter result by specific column,Need to be able to filter on specific column or columns instead of (or in addition to) searching entire result set,,Filter result by specific column $end$ Need to be able to filter on specific column or columns instead of (or in addition to) searching entire result set $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-08-02 08:07:31,Filter result by specific column,Need to be able to filter on specific column or columns instead of (or in addition to) searching entire result set,,0,0,0,0,0.0,Filter result by specific column $end$ Need to be able to filter on specific column or columns instead of (or in addition to) searching entire result set $acceptance criteria:$,0,0,0,0,0,0,1,960.633,42,12,0.285714,8,0.190476,5,0.119048,3,0.0714286,3,0.0714286
1610,MXS-3637,Task,MXS,2021-06-23 07:40:53,MXS-3631,0,Highlight active viewed table in the schema tree,"When on the data preview tab, simply clicking on the table name should bring up the preview. Creates a more fluid experience than having to click on the context menu then preview data button - however that button can still stay there for those that are not on the data preview to get there without first having to click on the data preview tab down below.                                             furthermore, would be nice if the active table being data previewed is highlighted or bolded to visually remind what table you are viewing",,"Highlight active viewed table in the schema tree $end$ When on the data preview tab, simply clicking on the table name should bring up the preview. Creates a more fluid experience than having to click on the context menu then preview data button - however that button can still stay there for those that are not on the data preview to get there without first having to click on the data preview tab down below.                                             furthermore, would be nice if the active table being data previewed is highlighted or bolded to visually remind what table you are viewing $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-07-12 05:39:47,Highlight active viewed table in the schema tree,"When on the data preview tab, simply clicking on the table name should bring up the preview. Creates a more fluid experience than having to click on the context menu then preview data button - however that button can still stay there for those that are not on the data preview to get there without first having to click on the data preview tab down below.                                             furthermore, would be nice if the active table being data previewed is highlighted or bolded to visually remind what table you are viewing",,0,0,0,0,0.0,"Highlight active viewed table in the schema tree $end$ When on the data preview tab, simply clicking on the table name should bring up the preview. Creates a more fluid experience than having to click on the context menu then preview data button - however that button can still stay there for those that are not on the data preview to get there without first having to click on the data preview tab down below.                                             furthermore, would be nice if the active table being data previewed is highlighted or bolded to visually remind what table you are viewing $acceptance criteria:$",0,0,0,0,0,0,0,453.967,43,12,0.27907,8,0.186047,5,0.116279,3,0.0697674,3,0.0697674
1611,MXS-3638,New Feature,MXS,2021-06-23 07:42:59,MXS-3631,0,Multi-tab query editor,One tab for one query connection.,,Multi-tab query editor $end$ One tab for one query connection. $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,6,,0,0,0,1,0,0,4,,0,850,0,0,0,2021-07-12 05:42:33,Multi-tab query editor,One tab for one query connection.,,0,0,0,0,0.0,Multi-tab query editor $end$ One tab for one query connection. $acceptance criteria:$,0,0,0,0,0,0,0,453.983,44,12,0.272727,8,0.181818,5,0.113636,3,0.0681818,3,0.0681818
1612,MXS-3639,New Feature,MXS,2021-06-23 07:48:49,MXS-3631,0,Show Stored procedures and Triggers  in schema tree,,,Show Stored procedures and Triggers  in schema tree $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,2,0,0,4,,0,850,0,0,0,2021-06-23 08:11:10,Show Stored procedures and Triggers  in schema tree,,,0,0,0,0,0.0,Show Stored procedures and Triggers  in schema tree $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.366667,45,12,0.266667,8,0.177778,5,0.111111,3,0.0666667,3,0.0666667
1613,MXS-3640,Sub-Task,MXS,2021-06-23 07:52:29,,0,Refactor the schemas tree,"Refactor the schemas tree to have `Tables`, `Columns`, `Stored procedures` and `Triggers`
nodes.",,"Refactor the schemas tree $end$ Refactor the schemas tree to have `Tables`, `Columns`, `Stored procedures` and `Triggers`
nodes. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-06-23 07:52:29,Refactor the schemas tree,"Refactor the schemas tree to have `Tables`, `Columns`, `Stored procedures` and `Triggers`
nodes.",,0,0,0,0,0.0,"Refactor the schemas tree $end$ Refactor the schemas tree to have `Tables`, `Columns`, `Stored procedures` and `Triggers`
nodes. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,46,12,0.26087,8,0.173913,5,0.108696,3,0.0652174,3,0.0652174
1614,MXS-3641,Sub-Task,MXS,2021-06-23 13:19:33,,0,Show stored procedures,,,Show stored procedures $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-06-23 13:19:33,Show stored procedures,,,0,0,0,0,0.0,Show stored procedures $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,47,12,0.255319,8,0.170213,5,0.106383,3,0.0638298,3,0.0638298
1615,MXS-3642,New Feature,MXS,2021-06-23 17:50:14,,0,Control replication with drag and drop in MaxGUI,"We should be able to see all servers and their topology in a GUI interface. It should show which listeners feed through which filters down to the servers.

It should support multi-tiered chains of replication with auto discovery and repoint on drag and drop using GTID.",,"Control replication with drag and drop in MaxGUI $end$ We should be able to see all servers and their topology in a GUI interface. It should show which listeners feed through which filters down to the servers.

It should support multi-tiered chains of replication with auto discovery and repoint on drag and drop using GTID. $acceptance criteria:$",,Manjot Singh,Manjot Singh,Major,19,,0,2,1,6,0,1,2,,0,850,2,0,0,2022-01-17 09:06:37,Extend GUI to control replication with drag and drop,"We should be able to see all servers and their topology in a GUI interface. It should show which listeners feed through which filters down to the servers.

It should support multi-tiered chains of replication with auto discovery and repoint on drag and drop using GTID.",,1,0,0,7,0.103448,"Extend GUI to control replication with drag and drop $end$ We should be able to see all servers and their topology in a GUI interface. It should show which listeners feed through which filters down to the servers.

It should support multi-tiered chains of replication with auto discovery and repoint on drag and drop using GTID. $acceptance criteria:$",1,1,1,0,0,0,1,4983.27,2,1,0.5,0,0.0,0,0.0,0,0.0,0,0.0
1616,MXS-3645,New Feature,MXS,2021-06-23 18:57:25,,0,Make Transaction Performance Monitoring Filter non-experimental,"A potential customer is highly interested in using https://mariadb.com/kb/en/mariadb-maxscale-25-transaction-performance-monitoring-filter/ but this is currently (despite docs not stating this) {{experimental}} only, which means it cannot be safely used in production.

The customer's specific statement for their interest in TPM is-

{quote}there was a goal of plotting query timings and providing some insight into transactions and the TPM looked to accomplish some of these goals{quote}

Customer also asked if this will be available properly in v2.6 (v6), but I am not seeing any roadmap for this feature nor inclusion on v6 release notes.

Can we consider taking steps necessary to make this prod-ready and to release it in an upcoming MaxScale version? If not, are there fundamental problems with the filter as-is and can we make a new feature which would address customer demand for the problems the TPM filter solves?",,"Make Transaction Performance Monitoring Filter non-experimental $end$ A potential customer is highly interested in using https://mariadb.com/kb/en/mariadb-maxscale-25-transaction-performance-monitoring-filter/ but this is currently (despite docs not stating this) {{experimental}} only, which means it cannot be safely used in production.

The customer's specific statement for their interest in TPM is-

{quote}there was a goal of plotting query timings and providing some insight into transactions and the TPM looked to accomplish some of these goals{quote}

Customer also asked if this will be available properly in v2.6 (v6), but I am not seeing any roadmap for this feature nor inclusion on v6 release notes.

Can we consider taking steps necessary to make this prod-ready and to release it in an upcoming MaxScale version? If not, are there fundamental problems with the filter as-is and can we make a new feature which would address customer demand for the problems the TPM filter solves? $acceptance criteria:$",,Rob Schwyzer,Rob Schwyzer,Major,12,,0,2,0,1,0,0,0,,0,850,1,0,0,2021-10-11 10:53:38,Make Transaction Performance Monitoring Filter non-experimental,"A potential customer is highly interested in using https://mariadb.com/kb/en/mariadb-maxscale-25-transaction-performance-monitoring-filter/ but this is currently (despite docs not stating this) {{experimental}} only, which means it cannot be safely used in production.

The customer's specific statement for their interest in TPM is-

{quote}there was a goal of plotting query timings and providing some insight into transactions and the TPM looked to accomplish some of these goals{quote}

Customer also asked if this will be available properly in v2.6 (v6), but I am not seeing any roadmap for this feature nor inclusion on v6 release notes.

Can we consider taking steps necessary to make this prod-ready and to release it in an upcoming MaxScale version? If not, are there fundamental problems with the filter as-is and can we make a new feature which would address customer demand for the problems the TPM filter solves?",,0,0,0,0,0.0,"Make Transaction Performance Monitoring Filter non-experimental $end$ A potential customer is highly interested in using https://mariadb.com/kb/en/mariadb-maxscale-25-transaction-performance-monitoring-filter/ but this is currently (despite docs not stating this) {{experimental}} only, which means it cannot be safely used in production.

The customer's specific statement for their interest in TPM is-

{quote}there was a goal of plotting query timings and providing some insight into transactions and the TPM looked to accomplish some of these goals{quote}

Customer also asked if this will be available properly in v2.6 (v6), but I am not seeing any roadmap for this feature nor inclusion on v6 release notes.

Can we consider taking steps necessary to make this prod-ready and to release it in an upcoming MaxScale version? If not, are there fundamental problems with the filter as-is and can we make a new feature which would address customer demand for the problems the TPM filter solves? $acceptance criteria:$",0,0,0,0,0,0,0,2631.93,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1617,MXS-3646,Sub-Task,MXS,2021-06-24 08:22:51,,0,Show triggers of a table,,,Show triggers of a table $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-06-24 08:22:51,Show triggers of a table,,,0,0,0,0,0.0,Show triggers of a table $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,48,12,0.25,8,0.166667,5,0.104167,3,0.0625,3,0.0625
1618,MXS-3647,Sub-Task,MXS,2021-06-24 08:46:19,,0,Refactor schemas tree to get info from `INFORMATION_SCHEMA`,,,Refactor schemas tree to get info from `INFORMATION_SCHEMA` $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-06-24 08:46:19,Refactor schemas tree to get info from `INFORMATION_SCHEMA`,,,0,0,0,0,0.0,Refactor schemas tree to get info from `INFORMATION_SCHEMA` $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,49,12,0.244898,8,0.163265,5,0.102041,3,0.0612245,3,0.0612245
1619,MXS-3649,New Feature,MXS,2021-06-29 12:39:03,,0,Add service reload time somewhere in maxctrl output,"It would be good if you can add service reload time somewhere in maxctrl output. For now, we can set log_info=1 so those details can be log in error.log. ",,"Add service reload time somewhere in maxctrl output $end$ It would be good if you can add service reload time somewhere in maxctrl output. For now, we can set log_info=1 so those details can be log in error.log.  $acceptance criteria:$",,Nilnandan Joshi,Nilnandan Joshi,Major,11,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-09-12 10:05:51,Add service reload time somewhere in maxctrl output,"It would be good if you can add service reload time somewhere in maxctrl output. For now, we can set log_info=1 so those details can be log in error.log. ",,0,0,0,0,0.0,"Add service reload time somewhere in maxctrl output $end$ It would be good if you can add service reload time somewhere in maxctrl output. For now, we can set log_info=1 so those details can be log in error.log.  $acceptance criteria:$",0,0,0,0,0,0,0,10557.4,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1620,MXS-3656,Task,MXS,2021-07-02 05:42:27,,0,Fix MaxScale 6 system tests.,,,Fix MaxScale 6 system tests. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-07-02 05:42:27,Fix MaxScale 6 system tests.,,,0,0,0,0,0.0,Fix MaxScale 6 system tests. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,453,41,0.0905077,18,0.0397351,12,0.0264901,9,0.0198675,9,0.0198675
1621,MXS-3659,New Feature,MXS,2021-07-07 04:58:31,,0,Show Slave Status in GUI,"When Replication is stopped or Lagging, there is currently no way to see that in the maxgui. It only says Running with a red X but doesn't show any status about replication on either the dashboard or on the server or monitor pages. 

Adding Information to the GUI about Slave Status including if it's in Error, if Slave SQL and Slave IO threads are running, and Seconds behind master at the very least would be helpful.  Would like to be able to drill into the red X (or the server that's in a red state) and find the error for why it's red. Maybe even make some information about the error visible on the dash besides the Red text and X.
If I have this dash page on a display screen in an office, I want to be able to look at it and clearly see the problem that 
a) that replication is stopped and why (Connectivity error? Duplicate error?  Can't find Binlog? etc) 
or
 b) that replication is lagging by 300 seconds but still running (and watch that lag increase or decrease to know if I need to intervene or not)",,"Show Slave Status in GUI $end$ When Replication is stopped or Lagging, there is currently no way to see that in the maxgui. It only says Running with a red X but doesn't show any status about replication on either the dashboard or on the server or monitor pages. 

Adding Information to the GUI about Slave Status including if it's in Error, if Slave SQL and Slave IO threads are running, and Seconds behind master at the very least would be helpful.  Would like to be able to drill into the red X (or the server that's in a red state) and find the error for why it's red. Maybe even make some information about the error visible on the dash besides the Red text and X.
If I have this dash page on a display screen in an office, I want to be able to look at it and clearly see the problem that 
a) that replication is stopped and why (Connectivity error? Duplicate error?  Can't find Binlog? etc) 
or
 b) that replication is lagging by 300 seconds but still running (and watch that lag increase or decrease to know if I need to intervene or not) $acceptance criteria:$",,Kathryn Sizemore,Kathryn Sizemore,Major,10,,0,0,0,3,0,0,0,,0,850,0,0,0,2021-08-16 09:48:30,Show Slave Status in GUI,"When Replication is stopped or Lagging, there is currently no way to see that in the maxgui. It only says Running with a red X but doesn't show any status about replication on either the dashboard or on the server or monitor pages. 

Adding Information to the GUI about Slave Status including if it's in Error, if Slave SQL and Slave IO threads are running, and Seconds behind master at the very least would be helpful.  Would like to be able to drill into the red X (or the server that's in a red state) and find the error for why it's red. Maybe even make some information about the error visible on the dash besides the Red text and X.
If I have this dash page on a display screen in an office, I want to be able to look at it and clearly see the problem that 
a) that replication is stopped and why (Connectivity error? Duplicate error?  Can't find Binlog? etc) 
or
 b) that replication is lagging by 300 seconds but still running (and watch that lag increase or decrease to know if I need to intervene or not)",,0,0,0,0,0.0,"Show Slave Status in GUI $end$ When Replication is stopped or Lagging, there is currently no way to see that in the maxgui. It only says Running with a red X but doesn't show any status about replication on either the dashboard or on the server or monitor pages. 

Adding Information to the GUI about Slave Status including if it's in Error, if Slave SQL and Slave IO threads are running, and Seconds behind master at the very least would be helpful.  Would like to be able to drill into the red X (or the server that's in a red state) and find the error for why it's red. Maybe even make some information about the error visible on the dash besides the Red text and X.
If I have this dash page on a display screen in an office, I want to be able to look at it and clearly see the problem that 
a) that replication is stopped and why (Connectivity error? Duplicate error?  Can't find Binlog? etc) 
or
 b) that replication is lagging by 300 seconds but still running (and watch that lag increase or decrease to know if I need to intervene or not) $acceptance criteria:$",0,0,0,0,0,0,1,964.817,2,1,0.5,1,0.5,1,0.5,1,0.5,1,0.5
1622,MXS-3663,New Feature,MXS,2021-07-08 20:43:19,,0,Causal Reads Global Cluster Distribution,"Causal reads currently only works in a single MaxScale instance environment however for the purposes of HA, many customers are running multiple MaxScale servers.  If the application is going though a load balancer like F5 or NLB, those connections are round robin'd  to the various MaxScale instance and the benefit of causal reads is lost.

We should devise a strategy to sync that information among MaxScale nodes. ",,"Causal Reads Global Cluster Distribution $end$ Causal reads currently only works in a single MaxScale instance environment however for the purposes of HA, many customers are running multiple MaxScale servers.  If the application is going though a load balancer like F5 or NLB, those connections are round robin'd  to the various MaxScale instance and the benefit of causal reads is lost.

We should devise a strategy to sync that information among MaxScale nodes.  $acceptance criteria:$",,Todd Stoffel,Todd Stoffel,Major,17,,0,2,2,4,0,0,0,,0,850,1,0,0,2021-12-08 10:44:01,Causal Reads Global Cluster Distribution,"Causal reads currently only works in a single MaxScale instance environment however for the purposes of HA, many customers are running multiple MaxScale servers.  If the application is going though a load balancer like F5 or NLB, those connections are round robin'd  to the various MaxScale instance and the benefit of causal reads is lost.

We should devise a strategy to sync that information among MaxScale nodes. ",,0,0,0,0,0.0,"Causal Reads Global Cluster Distribution $end$ Causal reads currently only works in a single MaxScale instance environment however for the purposes of HA, many customers are running multiple MaxScale servers.  If the application is going though a load balancer like F5 or NLB, those connections are round robin'd  to the various MaxScale instance and the benefit of causal reads is lost.

We should devise a strategy to sync that information among MaxScale nodes.  $acceptance criteria:$",0,0,0,0,0,0,1,3662.0,9,3,0.333333,3,0.333333,2,0.222222,2,0.222222,1,0.111111
1623,MXS-3667,Sub-Task,MXS,2021-07-13 13:17:28,,0,Design component pattern to have multiple query editor,"The query pane components needs to be refactored to support multi-tab query editor.  `QueryEditor`, `QueryResult`, `VisualizeSideBar` and `ChartContainer` should be composed into a `Worksheet` component.  
There will a wrapper component of `Worksheet` to control navigating, creating, and deleting `Worksheet`.
Query vuex module state also needs to be refactored as all states used in
`Worksheet` and its child components should have it standalone states.",,"Design component pattern to have multiple query editor $end$ The query pane components needs to be refactored to support multi-tab query editor.  `QueryEditor`, `QueryResult`, `VisualizeSideBar` and `ChartContainer` should be composed into a `Worksheet` component.  
There will a wrapper component of `Worksheet` to control navigating, creating, and deleting `Worksheet`.
Query vuex module state also needs to be refactored as all states used in
`Worksheet` and its child components should have it standalone states. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-07-13 13:17:28,Design component pattern to have multiple query editor,"The query pane components needs to be refactored to support multi-tab query editor.  `QueryEditor`, `QueryResult`, `VisualizeSideBar` and `ChartContainer` should be composed into a `Worksheet` component.  
There will a wrapper component of `Worksheet` to control navigating, creating, and deleting `Worksheet`.
Query vuex module state also needs to be refactored as all states used in
`Worksheet` and its child components should have it standalone states.",,0,0,0,0,0.0,"Design component pattern to have multiple query editor $end$ The query pane components needs to be refactored to support multi-tab query editor.  `QueryEditor`, `QueryResult`, `VisualizeSideBar` and `ChartContainer` should be composed into a `Worksheet` component.  
There will a wrapper component of `Worksheet` to control navigating, creating, and deleting `Worksheet`.
Query vuex module state also needs to be refactored as all states used in
`Worksheet` and its child components should have it standalone states. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,50,12,0.24,8,0.16,5,0.1,3,0.06,3,0.06
1624,MXS-3668,Sub-Task,MXS,2021-07-13 13:21:46,,0,Create tab layout for navigating between worksheets,"User should be able to
* Create new worksheets
* Delete worksheet
* Name the tab
* Show arrow navigation when worksheet tabs are overflowed",,"Create tab layout for navigating between worksheets $end$ User should be able to
* Create new worksheets
* Delete worksheet
* Name the tab
* Show arrow navigation when worksheet tabs are overflowed $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,1,0,1,0,,0,850,0,0,0,2021-07-13 13:21:46,Create tab layout for navigating between worksheets( query editors),"User should be able to
* Create new worksheets
* Delete worksheet
* Name the tab
* Show arrow navigation when worksheet tabs are overflowed",,1,0,0,4,0.0810811,"Create tab layout for navigating between worksheets( query editors) $end$ User should be able to
* Create new worksheets
* Delete worksheet
* Name the tab
* Show arrow navigation when worksheet tabs are overflowed $acceptance criteria:$",1,1,0,0,0,0,0,0.0,51,12,0.235294,8,0.156863,5,0.0980392,3,0.0588235,3,0.0588235
1625,MXS-3669,Sub-Task,MXS,2021-07-14 13:06:21,,0,"Compose query editor pane, result pane, visualize pane into a worksheet component",,,"Compose query editor pane, result pane, visualize pane into a worksheet component $end$ $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2021-07-14 13:06:21,"Decompose query editor pane, result pane, visualize pane into a worksheet component",,,1,0,0,2,0.0666667,"Decompose query editor pane, result pane, visualize pane into a worksheet component $end$ $acceptance criteria:$",1,1,0,0,0,0,0,0.0,52,13,0.25,8,0.153846,5,0.0961538,3,0.0576923,3,0.0576923
1626,MXS-3671,Sub-Task,MXS,2021-07-15 06:06:37,,0,Refactor query vuex module state,"Query vuex module state needs to be refactored in a way that all states used in `Worksheet`
and its child components are standalone.
Possibly an array of objects representing multiple worksheets.",,"Refactor query vuex module state $end$ Query vuex module state needs to be refactored in a way that all states used in `Worksheet`
and its child components are standalone.
Possibly an array of objects representing multiple worksheets. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-07-15 06:06:37,Refactor query vuex module state,"Query vuex module state needs to be refactored in a way that all states used in `Worksheet`
and its child components are standalone.
Possibly an array of objects representing multiple worksheets.",,0,0,0,0,0.0,"Refactor query vuex module state $end$ Query vuex module state needs to be refactored in a way that all states used in `Worksheet`
and its child components are standalone.
Possibly an array of objects representing multiple worksheets. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,53,14,0.264151,8,0.150943,5,0.0943396,3,0.0566038,3,0.0566038
1627,MXS-3675,New Feature,MXS,2021-07-19 10:54:48,MXS-3631,0,History/favorite queries,"History/favorite queries will be stored in the browser's localStorage.
",,"History/favorite queries $end$ History/favorite queries will be stored in the browser's localStorage.
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,2,0,1,6,,0,850,0,0,0,2021-08-16 09:48:32,Store history/favorite queries,"History/favorite queries will be stored in the browser's localStorage.
",,1,0,0,3,0.133333,"Store history/favorite queries $end$ History/favorite queries will be stored in the browser's localStorage.
 $acceptance criteria:$",1,1,0,0,0,0,1,670.883,54,14,0.259259,8,0.148148,5,0.0925926,3,0.0555556,3,0.0555556
1628,MXS-3676,Task,MXS,2021-07-19 12:02:00,,0,Upgrade to Connector/C 3.2,Upgrade to Connector/C 3.2 as replaces 3.1 as the maintained version.,,Upgrade to Connector/C 3.2 $end$ Upgrade to Connector/C 3.2 as replaces 3.1 as the maintained version. $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-08-02 07:43:49,Upgrade to Connector/C 3.2,Upgrade to Connector/C 3.2 as replaces 3.1 as the maintained version.,,0,0,0,0,0.0,Upgrade to Connector/C 3.2 $end$ Upgrade to Connector/C 3.2 as replaces 3.1 as the maintained version. $acceptance criteria:$,0,0,0,0,0,0,0,331.683,117,15,0.128205,10,0.0854701,8,0.0683761,8,0.0683761,7,0.0598291
1629,MXS-3678,Task,MXS,2021-07-19 13:26:35,MXS-3631,0,Preselect resource name to be connected,,,Preselect resource name to be connected $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-08-02 08:07:21,Preselect resource name to be connected,,,0,0,0,0,0.0,Preselect resource name to be connected $end$ $acceptance criteria:$,0,0,0,0,0,0,1,330.667,55,15,0.272727,8,0.145455,5,0.0909091,3,0.0545455,3,0.0545455
1630,MXS-3681,New Feature,MXS,2021-07-22 05:56:58,MXS-3631,0,Refactor SQL GUI to support multiple  SQL connections,,,Refactor SQL GUI to support multiple  SQL connections $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,14,,0,0,1,4,0,2,4,,0,850,0,1,0,2021-07-23 10:58:30,Refactor SQL GUI to support multiple database connection,,,1,0,0,4,0.181818,Refactor SQL GUI to support multiple database connection $end$ $acceptance criteria:$,1,1,0,0,0,0,1,29.0167,56,15,0.267857,8,0.142857,5,0.0892857,3,0.0535714,3,0.0535714
1631,MXS-3683,Task,MXS,2021-07-23 05:44:56,,0,Make filter parameter table editable in MaxGUI,,,Make filter parameter table editable in MaxGUI $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,12,,1,0,1,3,0,0,0,,0,850,0,0,0,2021-08-16 09:59:04,Make filter parameter table editable in MaxGUI,,,0,0,0,0,0.0,Make filter parameter table editable in MaxGUI $end$ $acceptance criteria:$,0,0,0,0,0,0,1,580.233,57,16,0.280702,8,0.140351,5,0.0877193,3,0.0526316,3,0.0526316
1632,MXS-3684,Sub-Task,MXS,2021-07-26 08:41:22,,0,Refactor query state to make it work with multiple connections,,,Refactor query state to make it work with multiple connections $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,4,0,1,0,,0,850,0,0,0,2021-07-26 08:41:22,Clean up query state to make it work with multiple connections,,,1,0,0,3,0.142857,Clean up query state to make it work with multiple connections $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,58,16,0.275862,8,0.137931,5,0.0862069,3,0.0517241,3,0.0517241
1633,MXS-3691,Sub-Task,MXS,2021-07-27 08:57:07,,0,Refactor `connection-manager` component to work with multiple connections interface,,,Refactor `connection-manager` component to work with multiple connections interface $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-07-27 08:57:07,Refactor `connection-manager` component to work with multiple connections interface,,,0,0,0,0,0.0,Refactor `connection-manager` component to work with multiple connections interface $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,59,17,0.288136,8,0.135593,5,0.0847458,3,0.0508475,3,0.0508475
1634,MXS-37,New Feature,MXS,2015-01-04 09:30:00,,0,bugzillaId-719: mandatory SELECT privilege on db level?,"This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=719
	
+Description lisu87 2015-02-04 09:30:00 UTC+
After upgrading from 1.0.1beta to 1.0.4 GA we found out we cannot longer use maxscale with mysql users which have SELECT privilege on table level.

When trying to connect to mysql server via maxscale without specifying default db it works, but with default db specified we always get ""Access denied for user"".

The solution that worked for us was to grant SELECT privilege for user on db level instead of doing so for specific tables in the database.

Please let me know if this behaviour is intended?",,"bugzillaId-719: mandatory SELECT privilege on db level? $end$ This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=719
	
+Description lisu87 2015-02-04 09:30:00 UTC+
After upgrading from 1.0.1beta to 1.0.4 GA we found out we cannot longer use maxscale with mysql users which have SELECT privilege on table level.

When trying to connect to mysql server via maxscale without specifying default db it works, but with default db specified we always get ""Access denied for user"".

The solution that worked for us was to grant SELECT privilege for user on db level instead of doing so for specific tables in the database.

Please let me know if this behaviour is intended? $acceptance criteria:$",,Dipti Joshi,lisu87,Minor,18,,0,4,0,1,0,3,0,,0,850,2,3,0,2016-02-09 12:53:46,bugzillaId-719: mandatory SELECT privilege on db level?,"This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=719
	
+Description lisu87 2015-02-04 09:30:00 UTC+
After upgrading from 1.0.1beta to 1.0.4 GA we found out we cannot longer use maxscale with mysql users which have SELECT privilege on table level.

When trying to connect to mysql server via maxscale without specifying default db it works, but with default db specified we always get ""Access denied for user"".

The solution that worked for us was to grant SELECT privilege for user on db level instead of doing so for specific tables in the database.

Please let me know if this behaviour is intended?",,0,0,0,0,0.0,"bugzillaId-719: mandatory SELECT privilege on db level? $end$ This is imported from bugzilla item: http://bugs.mariadb.com/show_bug.cgi?id=719
	
+Description lisu87 2015-02-04 09:30:00 UTC+
After upgrading from 1.0.1beta to 1.0.4 GA we found out we cannot longer use maxscale with mysql users which have SELECT privilege on table level.

When trying to connect to mysql server via maxscale without specifying default db it works, but with default db specified we always get ""Access denied for user"".

The solution that worked for us was to grant SELECT privilege for user on db level instead of doing so for specific tables in the database.

Please let me know if this behaviour is intended? $acceptance criteria:$",0,0,0,0,0,0,0,9627.38,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1635,MXS-3700,Task,MXS,2021-07-29 05:36:51,,0,Benchmark the overhead of qlafilter,"The qlafilter provides valuable information but it comes at some performance cost. This task is for measuring that cost and determining whether it is reasonable enough to allow it to be enabled on production systems.

As a part of this benchmarking, an evaluation of strategies for reducing the performance cost can be done. This includes technical improvements to the logging mechanism, sparser sampling of clients, conditional sampling etc..
",,"Benchmark the overhead of qlafilter $end$ The qlafilter provides valuable information but it comes at some performance cost. This task is for measuring that cost and determining whether it is reasonable enough to allow it to be enabled on production systems.

As a part of this benchmarking, an evaluation of strategies for reducing the performance cost can be done. This includes technical improvements to the logging mechanism, sparser sampling of clients, conditional sampling etc..
 $acceptance criteria:$",,markus makela,markus makela,Major,14,,0,1,0,2,0,1,0,,0,850,1,1,0,2021-08-16 11:18:43,Benchmark the overhead of qlafilter,"The qlafilter provides valuable information but it comes at some performance cost. This task is for measuring that cost and determining whether it is reasonable enough to allow it to be enabled on production systems.

As a part of this benchmarking, an evaluation of strategies for reducing the performance cost can be done. This includes technical improvements to the logging mechanism, sparser sampling of clients, conditional sampling etc..
",,0,0,0,0,0.0,"Benchmark the overhead of qlafilter $end$ The qlafilter provides valuable information but it comes at some performance cost. This task is for measuring that cost and determining whether it is reasonable enough to allow it to be enabled on production systems.

As a part of this benchmarking, an evaluation of strategies for reducing the performance cost can be done. This includes technical improvements to the logging mechanism, sparser sampling of clients, conditional sampling etc..
 $acceptance criteria:$",0,0,0,0,0,0,1,437.683,118,15,0.127119,10,0.0847458,8,0.0677966,8,0.0677966,7,0.059322
1636,MXS-3701,New Feature,MXS,2021-07-29 05:37:42,,0,Add canonical query form into qlafilter,This would allow the queries to be logged more safely without having to expose the values passed along in the SQL.,,Add canonical query form into qlafilter $end$ This would allow the queries to be logged more safely without having to expose the values passed along in the SQL. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-08-16 11:19:40,Add canonical query form into qlafilter,This would allow the queries to be logged more safely without having to expose the values passed along in the SQL.,,0,0,0,0,0.0,Add canonical query form into qlafilter $end$ This would allow the queries to be logged more safely without having to expose the values passed along in the SQL. $acceptance criteria:$,0,0,0,0,0,0,0,437.683,119,15,0.12605,10,0.0840336,8,0.0672269,8,0.0672269,7,0.0588235
1637,MXS-3708,New Feature,MXS,2021-08-02 11:37:01,,0,Make cache filter modifiable at runtime,"The cache filter cannot be modified at runtime and it should be made modifiable now that the API for it is in place.

Due to the complexity of the cache filter, this can be considered a new feature as it isn't expected to work with only minimal changes.",,"Make cache filter modifiable at runtime $end$ The cache filter cannot be modified at runtime and it should be made modifiable now that the API for it is in place.

Due to the complexity of the cache filter, this can be considered a new feature as it isn't expected to work with only minimal changes. $acceptance criteria:$",,markus makela,markus makela,Major,29,,1,0,1,5,0,2,0,,0,850,0,2,0,2022-11-21 11:37:20,Make cache filter modifiable at runtime,"The cache filter cannot be modified at runtime and it should be made modifiable now that the API for it is in place.

Due to the complexity of the cache filter, this can be considered a new feature as it isn't expected to work with only minimal changes.",,0,0,0,0,0.0,"Make cache filter modifiable at runtime $end$ The cache filter cannot be modified at runtime and it should be made modifiable now that the API for it is in place.

Due to the complexity of the cache filter, this can be considered a new feature as it isn't expected to work with only minimal changes. $acceptance criteria:$",0,0,0,0,0,0,1,11424.0,120,15,0.125,10,0.0833333,8,0.0666667,8,0.0666667,7,0.0583333
1638,MXS-3723,New Feature,MXS,2021-08-12 23:47:10,,0,Save and Load .sql files in the Query Editor,"The IDE should allow users to save and edit .sql files from the user's machine. Additionally, with a configuration, allow saving and editing .sql files on the maxscale server in a specific jailed directory (similar to mariadb's directory for outfile or load data).",,"Save and Load .sql files in the Query Editor $end$ The IDE should allow users to save and edit .sql files from the user's machine. Additionally, with a configuration, allow saving and editing .sql files on the maxscale server in a specific jailed directory (similar to mariadb's directory for outfile or load data). $acceptance criteria:$",,Manjot Singh,Manjot Singh,Major,25,,0,1,4,4,0,1,0,,0,850,1,0,0,2022-04-25 07:22:52,Save and Load .sql files in IDE,"The IDE should allow users to save and edit .sql files from the user's machine. Additionally, with a configuration, allow saving and editing .sql files on the maxscale server in a specific jailed directory (similar to mariadb's directory for outfile or load data).",,1,0,0,4,0.0566038,"Save and Load .sql files in IDE $end$ The IDE should allow users to save and edit .sql files from the user's machine. Additionally, with a configuration, allow saving and editing .sql files on the maxscale server in a specific jailed directory (similar to mariadb's directory for outfile or load data). $acceptance criteria:$",1,1,0,0,0,0,1,6127.58,3,2,0.666667,1,0.333333,0,0.0,0,0.0,0,0.0
1639,MXS-3724,Task,MXS,2021-08-12 23:47:59,,0,IDE should remember last open sql for a user,"When a user closes out of the browser or clicks away from IDE, the IDE should remember the last open files (even if unsaved) and restore them on next load.",,"IDE should remember last open sql for a user $end$ When a user closes out of the browser or clicks away from IDE, the IDE should remember the last open files (even if unsaved) and restore them on next load. $acceptance criteria:$",,Manjot Singh,Manjot Singh,Major,29,,0,0,2,3,0,0,0,,0,850,0,0,0,2022-04-25 07:23:16,IDE should remember last open sql for a user,"When a user closes out of the browser or clicks away from IDE, the IDE should remember the last open files (even if unsaved) and restore them on next load.",,0,0,0,0,0.0,"IDE should remember last open sql for a user $end$ When a user closes out of the browser or clicks away from IDE, the IDE should remember the last open files (even if unsaved) and restore them on next load. $acceptance criteria:$",0,0,0,0,0,0,1,6127.58,4,3,0.75,1,0.25,0,0.0,0,0.0,0,0.0
1640,MXS-3725,New Feature,MXS,2021-08-12 23:50:52,,0,Allow storing query as snippets in the Query Editor,Add a macro menu in IDE that allows easy input and use of frequently used queries.,,Allow storing query as snippets in the Query Editor $end$ Add a macro menu in IDE that allows easy input and use of frequently used queries. $acceptance criteria:$,,Manjot Singh,Manjot Singh,Major,31,,0,2,0,1,0,3,0,,0,850,2,0,0,2022-04-11 09:43:56,IDE macros,Add a macro menu in IDE that allows easy input and use of frequently used queries.,,3,0,0,11,0.428571,IDE macros $end$ Add a macro menu in IDE that allows easy input and use of frequently used queries. $acceptance criteria:$,3,1,1,1,0,0,1,5793.88,5,3,0.6,1,0.2,0,0.0,0,0.0,0,0.0
1641,MXS-3731,Sub-Task,MXS,2021-08-16 11:03:47,,0,"Once a connection is used by a worksheet, it shouldn't be used by other worksheet","Once a connection is used by a worksheet, it shouldn't be used by other worksheet.
In the tab navigation, there should be a way to show which connection is being used by a worksheet.",,"Once a connection is used by a worksheet, it shouldn't be used by other worksheet $end$ Once a connection is used by a worksheet, it shouldn't be used by other worksheet.
In the tab navigation, there should be a way to show which connection is being used by a worksheet. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,4,0,1,0,,0,850,0,0,0,2021-08-16 11:03:47,"Once a connection is used by a worksheet, it shouldn't be used by other worksheet","Once a connection is used by a worksheet, it shouldn't be used by other worksheet.
In the tab navigation, there should be an info icon to show which connection is being used by a worksheet.",,0,1,0,5,0.0566038,"Once a connection is used by a worksheet, it shouldn't be used by other worksheet $end$ Once a connection is used by a worksheet, it shouldn't be used by other worksheet.
In the tab navigation, there should be an info icon to show which connection is being used by a worksheet. $acceptance criteria:$",1,1,1,0,0,0,1,0.0,60,17,0.283333,8,0.133333,5,0.0833333,3,0.05,3,0.05
1642,MXS-3732,Sub-Task,MXS,2021-08-16 11:05:26,,0,Allow executing queries parallelly among worksheets,,,Allow executing queries parallelly among worksheets $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,4,0,1,0,,0,850,0,0,0,2021-08-16 11:05:26,Allow to execute queries parallelly among worksheets,,,1,0,0,3,0.2,Allow to execute queries parallelly among worksheets $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,61,18,0.295082,9,0.147541,5,0.0819672,3,0.0491803,3,0.0491803
1643,MXS-3733,New Feature,MXS,2021-08-16 17:24:44,,0,Add keytab filepath configuration option to GSSAPI authenticator ,Add an authenticator option which sets the filepath instead of forcing users to use system default path. The server includes a similar option.,,Add keytab filepath configuration option to GSSAPI authenticator  $end$ Add an authenticator option which sets the filepath instead of forcing users to use system default path. The server includes a similar option. $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,7,,0,0,1,1,0,0,0,,0,850,0,0,0,2021-08-16 17:25:32,Add keytab filepath configuration option to GSSAPI authenticator ,Add an authenticator option which sets the filepath instead of forcing users to use system default path. The server includes a similar option.,,0,0,0,0,0.0,Add keytab filepath configuration option to GSSAPI authenticator  $end$ Add an authenticator option which sets the filepath instead of forcing users to use system default path. The server includes a similar option. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,23,2,0.0869565,1,0.0434783,0,0.0,0,0.0,0,0.0
1644,MXS-3739,Sub-Task,MXS,2021-08-24 08:37:00,,0,Find out an appropriate place to show history/favorite queries,Needs to have a place to show user history/favorite queries,,Find out an appropriate place to show history/favorite queries $end$ Needs to have a place to show user history/favorite queries $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,2,0,3,0,,0,850,0,0,0,2021-08-24 08:37:00,Find out a place to show history/favorite queries,Needs to have a place to show user history/favorite queries,,3,0,0,3,0.0952381,Find out a place to show history/favorite queries $end$ Needs to have a place to show user history/favorite queries $acceptance criteria:$,3,1,0,0,0,0,1,0.0,62,19,0.306452,9,0.145161,5,0.0806452,3,0.0483871,3,0.0483871
1645,MXS-3740,Sub-Task,MXS,2021-08-24 08:48:25,,0,Control saving favorite queries,"User can click on a button to save current queries in the editor as favorite and name it or
using shortcut keys .i.e. Ctrl/CMD + S to save it. 
A favorite query object contains the following keys:
* name: Favorite query name
* sql: Query text
* createdDate: Created date (RFC2822 format: ddd, DD MMM YYYY HH:mm:ss)",,"Control saving favorite queries $end$ User can click on a button to save current queries in the editor as favorite and name it or
using shortcut keys .i.e. Ctrl/CMD + S to save it. 
A favorite query object contains the following keys:
* name: Favorite query name
* sql: Query text
* createdDate: Created date (RFC2822 format: ddd, DD MMM YYYY HH:mm:ss) $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,2,0,3,0,,0,850,0,0,0,2021-08-24 08:48:25,Control saving favorite queries,"User can click on a button to save current queries in the editor as favorite and name it or
using shortcut keys .i.e. Ctrl/CMD + S to save it. ",,0,3,0,28,0.777778,"Control saving favorite queries $end$ User can click on a button to save current queries in the editor as favorite and name it or
using shortcut keys .i.e. Ctrl/CMD + S to save it.  $acceptance criteria:$",3,1,1,1,1,1,1,0.0,63,20,0.31746,9,0.142857,5,0.0793651,3,0.047619,3,0.047619
1646,MXS-3741,Sub-Task,MXS,2021-08-24 08:57:06,,0,Control saving history queries,"History queries should be stored automatically.
A history object contains the following keys:
* date: Date when executing the query (ddd, DD MMM YYYY )
* connName: Connection name
* time: Time when executing the query (HH:mm:ss)
* exeTime: Execution time
* sql: Query text
",,"Control saving history queries $end$ History queries should be stored automatically.
A history object contains the following keys:
* date: Date when executing the query (ddd, DD MMM YYYY )
* connName: Connection name
* time: Time when executing the query (HH:mm:ss)
* exeTime: Execution time
* sql: Query text
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,9,,0,0,0,2,0,5,0,,0,850,0,0,0,2021-08-24 08:57:06,Control saving history queries,"History queries should be stored automatically.
",,0,5,0,39,3.0,"Control saving history queries $end$ History queries should be stored automatically.
 $acceptance criteria:$",5,1,1,1,1,1,1,0.0,64,21,0.328125,10,0.15625,6,0.09375,4,0.0625,4,0.0625
1647,MXS-3742,Sub-Task,MXS,2021-08-24 10:48:42,,0,Showing history/favorite in table layout,"*Table for showing history*
* Layout can be resizable
* Auto group by date
* Having checkbox to select multiple items to delete or delete all
*Right click to open context menu
*Having two options in the context menu:  Copy to clipboard, Place SQL in Editor
* Drag/Drop sql  to the editor.

*Table for showing favorite*
* Having checkbox to select multiple items to delete or delete all
*Right click to open context menu
*Having two options in the context menu:  Copy to clipboard, Place SQL in Editor
* Drag/Drop sql  to the editor.",,"Showing history/favorite in table layout $end$ *Table for showing history*
* Layout can be resizable
* Auto group by date
* Having checkbox to select multiple items to delete or delete all
*Right click to open context menu
*Having two options in the context menu:  Copy to clipboard, Place SQL in Editor
* Drag/Drop sql  to the editor.

*Table for showing favorite*
* Having checkbox to select multiple items to delete or delete all
*Right click to open context menu
*Having two options in the context menu:  Copy to clipboard, Place SQL in Editor
* Drag/Drop sql  to the editor. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,2,0,3,0,,0,850,0,0,0,2021-08-24 10:48:42,Showing history/favorite in table layout,"The layout contains two tables
*  A table on the left to show the list ( show name of the query if it is favorite query, otherwise shows date list for history query
* A table on the right to show details of the query
* Layout can be resizable
Should have the following features for the table on the left
* Having checkbox to select multiple items to delete or delete all
* First item on should be pre-selected
* Clicking on the name of item , it should show details of the query on the right table.

Should have the following features for the table on the right
*Right click to open context menu
*Having two options in the context menu:  Copy to clipboard, Place SQL in Editor
* Drag/Drop feature to the editor.",,0,3,0,144,0.659722,"Showing history/favorite in table layout $end$ The layout contains two tables
*  A table on the left to show the list ( show name of the query if it is favorite query, otherwise shows date list for history query
* A table on the right to show details of the query
* Layout can be resizable
Should have the following features for the table on the left
* Having checkbox to select multiple items to delete or delete all
* First item on should be pre-selected
* Clicking on the name of item , it should show details of the query on the right table.

Should have the following features for the table on the right
*Right click to open context menu
*Having two options in the context menu:  Copy to clipboard, Place SQL in Editor
* Drag/Drop feature to the editor. $acceptance criteria:$",3,1,1,1,1,1,1,0.0,65,22,0.338462,11,0.169231,7,0.107692,5,0.0769231,5,0.0769231
1648,MXS-3745,Sub-Task,MXS,2021-08-25 08:20:24,,0,Handle clear history/favorite queries,"When storing history/favorite queries to localStorage, the maximum storage size can not be determined easily as different browsers have different size and user
can even increase the size.So, the most appropriate way is to show a snackbar indicating error and inform user to delete some to create more space.
There could be a setting in `Query Configuration` dialog to automatically clear history queries after certain days. e.g. 30 days.",,"Handle clear history/favorite queries $end$ When storing history/favorite queries to localStorage, the maximum storage size can not be determined easily as different browsers have different size and user
can even increase the size.So, the most appropriate way is to show a snackbar indicating error and inform user to delete some to create more space.
There could be a setting in `Query Configuration` dialog to automatically clear history queries after certain days. e.g. 30 days. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,1,0,,0,850,0,0,0,2021-08-25 08:20:24,Handle clear history/favorite queries,"Since history and favorite queries are stored in localStorage, the maximum size should be determined and should not be exceed. 
For history queries, when it reaches maximum size, older history should be cleared.
For favorite queries, there should be a limit.",,0,1,0,90,1.29167,"Handle clear history/favorite queries $end$ Since history and favorite queries are stored in localStorage, the maximum size should be determined and should not be exceed. 
For history queries, when it reaches maximum size, older history should be cleared.
For favorite queries, there should be a limit. $acceptance criteria:$",1,1,1,1,1,1,1,0.0,66,23,0.348485,12,0.181818,8,0.121212,6,0.0909091,6,0.0909091
1649,MXS-3749,Task,MXS,2021-08-30 09:54:59,MXS-3387,0,Add iterator support to OP_QUERY,,,Add iterator support to OP_QUERY $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-08-30 09:54:59,Add iterator support to OP_QUERY,,,0,0,0,0,0.0,Add iterator support to OP_QUERY $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,454,41,0.0903084,18,0.0396476,12,0.0264317,9,0.0198238,9,0.0198238
1650,MXS-3751,Task,MXS,2021-08-31 13:40:38,,0,Improve custom user creation,"The fast fix implemented for MXS-3475 can be improved upon. The custom users should be read from a text file (json?), allowing the user to define all relevant fields. ",,"Improve custom user creation $end$ The fast fix implemented for MXS-3475 can be improved upon. The custom users should be read from a text file (json?), allowing the user to define all relevant fields.  $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,13,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-08-31 13:41:31,Improve custom user creation,"The fast fix implemented for MXS-3475 can be improved upon. The custom users should be read from a text file (json?), allowing the user to define all relevant fields. ",,0,0,0,0,0.0,"Improve custom user creation $end$ The fast fix implemented for MXS-3475 can be improved upon. The custom users should be read from a text file (json?), allowing the user to define all relevant fields.  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,24,2,0.0833333,1,0.0416667,0,0.0,0,0.0,0,0.0
1651,MXS-3758,Task,MXS,2021-09-06 10:49:21,,0,Replace pipe based worker messaging with one based upon eventfd.,"An eventfd based worker messaging mechanism is likely to consume less kernel-space memory, be faster and be easier to debug.",,"Replace pipe based worker messaging with one based upon eventfd. $end$ An eventfd based worker messaging mechanism is likely to consume less kernel-space memory, be faster and be easier to debug. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-10-11 10:42:34,Replace pipe based worker messaging with one based upon eventfd.,"An eventfd based worker messaging mechanism is likely to consume less kernel-space memory, be faster and be easier to debug.",,0,0,0,0,0.0,"Replace pipe based worker messaging with one based upon eventfd. $end$ An eventfd based worker messaging mechanism is likely to consume less kernel-space memory, be faster and be easier to debug. $acceptance criteria:$",0,0,0,0,0,0,0,839.883,455,41,0.0901099,18,0.0395604,12,0.0263736,9,0.0197802,9,0.0197802
1652,MXS-3763,Task,MXS,2021-09-13 10:43:45,,0,Fix tests in 6,,,Fix tests in 6 $end$ $acceptance criteria:$,,markus makela,markus makela,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-09-13 10:43:45,Fix tests in 6,,,0,0,0,0,0.0,Fix tests in 6 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,121,15,0.123967,10,0.0826446,8,0.0661157,8,0.0661157,7,0.0578512
1653,MXS-3771,New Feature,MXS,2021-09-16 14:56:42,,0,Add row count to qlafilter,"The number of rows returned for SELECT statements would be a valuable addition to the qlafilter. This combined with the query latency should be a pretty good performance indicator.

Additional metrics that could be added that are already available in {{mxs::Reply}}:
* Result size in bytes
* Number of warnings
* The error message, if any",,"Add row count to qlafilter $end$ The number of rows returned for SELECT statements would be a valuable addition to the qlafilter. This combined with the query latency should be a pretty good performance indicator.

Additional metrics that could be added that are already available in {{mxs::Reply}}:
* Result size in bytes
* Number of warnings
* The error message, if any $acceptance criteria:$",,markus makela,markus makela,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-09-28 11:03:32,Add row count to qlafilter,"The number of rows returned for SELECT statements would be a valuable addition to the qlafilter. This combined with the query latency should be a pretty good performance indicator.

Additional metrics that could be added that are already available in {{mxs::Reply}}:
* Result size in bytes
* Number of warnings
* The error message, if any",,0,0,0,0,0.0,"Add row count to qlafilter $end$ The number of rows returned for SELECT statements would be a valuable addition to the qlafilter. This combined with the query latency should be a pretty good performance indicator.

Additional metrics that could be added that are already available in {{mxs::Reply}}:
* Result size in bytes
* Number of warnings
* The error message, if any $acceptance criteria:$",0,0,0,0,0,0,0,284.1,122,15,0.122951,10,0.0819672,8,0.0655738,8,0.0655738,7,0.057377
1654,MXS-3776,Task,MXS,2021-09-20 11:11:12,,0,Researching on making query editor as a library,"The research aims to investigate and estimate time needed to detach query editor and make it as a library.

",,"Researching on making query editor as a library $end$ The research aims to investigate and estimate time needed to detach query editor and make it as a library.

 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-09-20 11:11:12,Researching on making query editor as a library,"The research aims to investigate and estimate time needed to detach query editor and make it as a library.

",,0,0,0,0,0.0,"Researching on making query editor as a library $end$ The research aims to investigate and estimate time needed to detach query editor and make it as a library.

 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,67,24,0.358209,13,0.19403,9,0.134328,7,0.104478,7,0.104478
1655,MXS-3781,Task,MXS,2021-09-23 12:27:30,,0,Hide system databases in the schema sidebar,"By default, it should hide system databases. Users can configure this option by clicking the Settings icon on the top right corner.",,"Hide system databases in the schema sidebar $end$ By default, it should hide system databases. Users can configure this option by clicking the Settings icon on the top right corner. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,15,,0,0,0,1,0,3,0,,0,850,0,0,0,2021-10-25 09:10:05,Hides system tables in the schema sidebar,By default it should hide system tables. User can configure this option by clicking Settings icon on the top right corner.,,2,1,0,11,0.193548,Hides system tables in the schema sidebar $end$ By default it should hide system tables. User can configure this option by clicking Settings icon on the top right corner. $acceptance criteria:$,3,1,1,1,0,0,1,764.7,68,24,0.352941,13,0.191176,9,0.132353,7,0.102941,7,0.102941
1656,MXS-3783,New Feature,MXS,2021-09-24 10:43:57,,0,User access control in MaxGUI,"Currently the GUI allows you to attempt all operations that modify the configuration even if we know they will fail due to a lack of permissions. Graying out the buttons that do these modifications would signal the user that it requires more privileges to attempt it.

*Original description:*

----

Hi,

i created a basic user, followed these guide: 

https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-administration-tutorial/#managing-maxctrl-and-rest-api-users

But in MaxGui and CLI im able to use maxctrl or do some modifications via GUI with my ""test"" user.

Is this a bug or did i something wrong?

my passwd looks like:

[{""name"": ""test"", ""account"": ""basic"", ""password"": ""$6$MX...yz1""}, 
{""name"": ""admin"", ""account"": ""admin"", ""password"": ""$6$MX...yz1""}]

I just need a user for dashboard readonly.

Thy",,"User access control in MaxGUI $end$ Currently the GUI allows you to attempt all operations that modify the configuration even if we know they will fail due to a lack of permissions. Graying out the buttons that do these modifications would signal the user that it requires more privileges to attempt it.

*Original description:*

----

Hi,

i created a basic user, followed these guide: 

https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-administration-tutorial/#managing-maxctrl-and-rest-api-users

But in MaxGui and CLI im able to use maxctrl or do some modifications via GUI with my ""test"" user.

Is this a bug or did i something wrong?

my passwd looks like:

[{""name"": ""test"", ""account"": ""basic"", ""password"": ""$6$MX...yz1""}, 
{""name"": ""admin"", ""account"": ""admin"", ""password"": ""$6$MX...yz1""}]

I just need a user for dashboard readonly.

Thy $acceptance criteria:$",,M.B.,M.B.,Minor,17,,0,8,1,1,0,4,0,,0,850,0,2,0,2022-04-11 07:48:34,MaxGUI - Basic user are allowed to attempt modifications even if they will fail,"Currently the GUI allows you to attempt all operations that modify the configuration even if we know they will fail due to a lack of permissions. Graying out the buttons that do these modifications would signal the user that it requires more privileges to attempt it.

*Original description:*

----

Hi,

i created a basic user, followed these guide: 

https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-administration-tutorial/#managing-maxctrl-and-rest-api-users

But in MaxGui and CLI im able to use maxctrl or do some modifications via GUI with my ""test"" user.

Is this a bug or did i something wrong?

my passwd looks like:

[{""name"": ""test"", ""account"": ""basic"", ""password"": ""$6$MX...yz1""}, 
{""name"": ""admin"", ""account"": ""admin"", ""password"": ""$6$MX...yz1""}]

I just need a user for dashboard readonly.

Thy",,2,0,0,17,0.107692,"MaxGUI - Basic user are allowed to attempt modifications even if they will fail $end$ Currently the GUI allows you to attempt all operations that modify the configuration even if we know they will fail due to a lack of permissions. Graying out the buttons that do these modifications would signal the user that it requires more privileges to attempt it.

*Original description:*

----

Hi,

i created a basic user, followed these guide: 

https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-administration-tutorial/#managing-maxctrl-and-rest-api-users

But in MaxGui and CLI im able to use maxctrl or do some modifications via GUI with my ""test"" user.

Is this a bug or did i something wrong?

my passwd looks like:

[{""name"": ""test"", ""account"": ""basic"", ""password"": ""$6$MX...yz1""}, 
{""name"": ""admin"", ""account"": ""admin"", ""password"": ""$6$MX...yz1""}]

I just need a user for dashboard readonly.

Thy $acceptance criteria:$",2,1,1,1,1,0,1,4773.07,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1657,MXS-3784,Sub-Task,MXS,2021-09-27 07:01:58,,0,Determine scope,"Define features and functions of DDL to be implemented into MaxGUI.
",,"Determine scope $end$ Define features and functions of DDL to be implemented into MaxGUI.
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-09-27 07:01:58,Determine scope,"Define features and functions of DDL to be implemented into MaxGUI.
",,0,0,0,0,0.0,"Determine scope $end$ Define features and functions of DDL to be implemented into MaxGUI.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,69,25,0.362319,14,0.202899,10,0.144928,7,0.101449,7,0.101449
1658,MXS-3785,Sub-Task,MXS,2021-09-27 09:05:01,,0,Create DDL editor basic layout,"Layout of the  DDL editor:
# Table related functions
# Tab navigation to navigate between Columns, Triggers
# Tab content section contains all functions of chosen tab. ",,"Create DDL editor basic layout $end$ Layout of the  DDL editor:
# Table related functions
# Tab navigation to navigate between Columns, Triggers
# Tab content section contains all functions of chosen tab.  $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,1,0,,0,850,0,0,0,2021-09-27 09:05:01,Create DDL editor basic layout,"Layout of the  DDL editor:
# Table related functions
      * Input to change table name ALTER TABLE t2 RENAME t1
    * A drop-down to select ""Character Set ""
    * A drop-down to select ""Collation""
    * A drop-down to select ""Engine""
    * A text input to add table comment
# Tab navigation to navigate between Columns, Triggers
# Tab content section contains all functions of chosen tab. ",,0,1,0,39,0.527027,"Create DDL editor basic layout $end$ Layout of the  DDL editor:
# Table related functions
      * Input to change table name ALTER TABLE t2 RENAME t1
    * A drop-down to select ""Character Set ""
    * A drop-down to select ""Collation""
    * A drop-down to select ""Engine""
    * A text input to add table comment
# Tab navigation to navigate between Columns, Triggers
# Tab content section contains all functions of chosen tab.  $acceptance criteria:$",1,1,1,1,1,1,1,0.0,70,25,0.357143,14,0.2,10,0.142857,7,0.1,7,0.1
1659,MXS-3786,Sub-Task,MXS,2021-09-27 09:05:51,,0,Add `Alter Table` option to schema context menu,"By selecting ""Alter Table "" option in schema context menu,  it opens DDL editor for chosen table.",,"Add `Alter Table` option to schema context menu $end$ By selecting ""Alter Table "" option in schema context menu,  it opens DDL editor for chosen table. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,4,0,2,0,,0,850,0,0,0,2021-09-27 09:05:51,Add `Alter Table...` option to schema context menu,"By selecting ""Alter Table... "" option in schema context menu,  it creates a new worksheet with DDL editor. The name of the worksheet would be ${nameOfTbl} - Table",,1,1,0,25,0.461538,"Add `Alter Table...` option to schema context menu $end$ By selecting ""Alter Table... "" option in schema context menu,  it creates a new worksheet with DDL editor. The name of the worksheet would be ${nameOfTbl} - Table $acceptance criteria:$",2,1,1,1,1,1,1,0.0,71,26,0.366197,15,0.211268,11,0.15493,8,0.112676,8,0.112676
1660,MXS-3787,Task,MXS,2021-09-27 09:23:19,,0,Document COM_CHANGE_USER limitations,"If a faulty COM_CHANGE_USER is accepted by MaxScale but rejected by server, the session fails. In an ideal situation, the command could be canceled also when receiving the server error packet. Fixing this is not trivial, so just document it for now.",,"Document COM_CHANGE_USER limitations $end$ If a faulty COM_CHANGE_USER is accepted by MaxScale but rejected by server, the session fails. In an ideal situation, the command could be canceled also when receiving the server error packet. Fixing this is not trivial, so just document it for now. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Minor,14,,0,0,0,1,0,3,0,,0,850,0,0,0,2021-09-28 11:14:27,COM_CHANGE_USER cannot always be canceled properly,"If a faulty COM_CHANGE_USER is accepted by MaxScale but rejected by server, the session fails. In an ideal situation, the command could be canceled also when receiving the server error packet.",,1,2,0,18,0.425,"COM_CHANGE_USER cannot always be canceled properly $end$ If a faulty COM_CHANGE_USER is accepted by MaxScale but rejected by server, the session fails. In an ideal situation, the command could be canceled also when receiving the server error packet. $acceptance criteria:$",3,1,1,1,1,0,1,25.85,25,2,0.08,1,0.04,0,0.0,0,0.0,0,0.0
1661,MXS-3789,Sub-Task,MXS,2021-09-28 07:05:17,,0,Add table related functions,"Input to change table name
A drop-down to select ""Character Set ""
A drop-down to select ""Collation""
A drop-down to select ""Engine""
A text input to add table comment",,"Add table related functions $end$ Input to change table name
A drop-down to select ""Character Set ""
A drop-down to select ""Collation""
A drop-down to select ""Engine""
A text input to add table comment $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-09-28 07:05:17,Add table related functions,"Input to change table name
A drop-down to select ""Character Set ""
A drop-down to select ""Collation""
A drop-down to select ""Engine""
A text input to add table comment",,0,0,0,0,0.0,"Add table related functions $end$ Input to change table name
A drop-down to select ""Character Set ""
A drop-down to select ""Collation""
A drop-down to select ""Engine""
A text input to add table comment $acceptance criteria:$",0,0,0,0,0,0,1,0.0,72,27,0.375,16,0.222222,12,0.166667,9,0.125,9,0.125
1662,MXS-3792,Task,MXS,2021-09-28 10:55:03,,0,add tests with ES and CS 10.5 - 10.7 backend to BB,"regular testing with both ES and CS backend is needed:

2.5, 6 with ES 10.5, 10.6, CS 10.5 - 10.7
2.4 with ES 10.6, CS 10.6

on push tests - consider ""one out of 10 commits"" is tested with all backends",,"add tests with ES and CS 10.5 - 10.7 backend to BB $end$ regular testing with both ES and CS backend is needed:

2.5, 6 with ES 10.5, 10.6, CS 10.5 - 10.7
2.4 with ES 10.6, CS 10.6

on push tests - consider ""one out of 10 commits"" is tested with all backends $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-09-28 11:18:42,add tests with ES and CS 10.5 - 10.7 backend to BB,"regular testing with both ES and CS backend is needed:

2.5, 6 with ES 10.5, 10.6, CS 10.5 - 10.7
2.4 with ES 10.6, CS 10.6

on push tests - consider ""one out of 10 commits"" is tested with all backends",,0,0,0,0,0.0,"add tests with ES and CS 10.5 - 10.7 backend to BB $end$ regular testing with both ES and CS backend is needed:

2.5, 6 with ES 10.5, 10.6, CS 10.5 - 10.7
2.4 with ES 10.6, CS 10.6

on push tests - consider ""one out of 10 commits"" is tested with all backends $acceptance criteria:$",0,0,0,0,0,0,0,0.383333,92,1,0.0108696,0,0.0,0,0.0,0,0.0,0,0.0
1663,MXS-3797,Task,MXS,2021-09-28 14:31:12,,0,Improve COM_CHANGE_USER handling,Should ideally work with different authenticators and user mapping.,,Improve COM_CHANGE_USER handling $end$ Should ideally work with different authenticators and user mapping. $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-10-01 10:46:36,Improve COM_CHANGE_USER handling,Should ideally work with different authenticators and user mapping.,,0,0,0,0,0.0,Improve COM_CHANGE_USER handling $end$ Should ideally work with different authenticators and user mapping. $acceptance criteria:$,0,0,0,0,0,0,0,68.25,26,3,0.115385,2,0.0769231,1,0.0384615,1,0.0384615,0,0.0
1664,MXS-3802,Sub-Task,MXS,2021-10-01 12:14:47,,0,Add columns alter specifications,"Create a table to alter columns
",,"Add columns alter specifications $end$ Create a table to alter columns
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2021-10-01 12:14:47,Add columns alter specifications,"Create a table to alter columns
",,0,0,0,0,0.0,"Add columns alter specifications $end$ Create a table to alter columns
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,73,27,0.369863,16,0.219178,12,0.164384,9,0.123288,9,0.123288
1665,MXS-3806,New Feature,MXS,2021-10-07 14:15:48,,0,Provide filtering for the KafkaCDC Router,Provide a means to filter on database and/or table the replication stream of the KafkaCDC Router from a primary or Galera cluster.,,Provide filtering for the KafkaCDC Router $end$ Provide a means to filter on database and/or table the replication stream of the KafkaCDC Router from a primary or Galera cluster. $acceptance criteria:$,,Kyle Joiner,Kyle Joiner,Major,13,,0,13,0,1,0,1,0,,0,850,5,1,0,2022-03-02 08:06:29,Provide filtering for the KafkaCDC Router,Provide a means to filter on database and/or table the replication stream of the KafkaCDC Router from a primary or Galera cluster.,,0,0,0,0,0.0,Provide filtering for the KafkaCDC Router $end$ Provide a means to filter on database and/or table the replication stream of the KafkaCDC Router from a primary or Galera cluster. $acceptance criteria:$,0,0,0,0,0,0,0,3497.83,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1666,MXS-381,New Feature,MXS,2015-09-25 11:19:28,,0,support for transparent proxy,"Hi,

is it possible to implement ""transparent-proxy"" feature in maxscale ?

https://www.percona.com/doc/percona-xtradb-cluster/5.5/howtos/haproxy.html

It means basically, at new connection to backend from maxscale:
Sending to the db backend the originating IP via a protocol, so that the original IP can be displayed in show processlist, and used in authentification definition.

Currently, only percona server supports this feature.

Thanks !
Joffrey",,"support for transparent proxy $end$ Hi,

is it possible to implement ""transparent-proxy"" feature in maxscale ?

https://www.percona.com/doc/percona-xtradb-cluster/5.5/howtos/haproxy.html

It means basically, at new connection to backend from maxscale:
Sending to the db backend the originating IP via a protocol, so that the original IP can be displayed in show processlist, and used in authentification definition.

Currently, only percona server supports this feature.

Thanks !
Joffrey $acceptance criteria:$",,Joffrey MICHAIE,Joffrey MICHAIE,Major,8,,0,2,2,1,0,0,0,,0,850,1,0,0,2017-04-19 09:59:08,support for transparent proxy,"Hi,

is it possible to implement ""transparent-proxy"" feature in maxscale ?

https://www.percona.com/doc/percona-xtradb-cluster/5.5/howtos/haproxy.html

It means basically, at new connection to backend from maxscale:
Sending to the db backend the originating IP via a protocol, so that the original IP can be displayed in show processlist, and used in authentification definition.

Currently, only percona server supports this feature.

Thanks !
Joffrey",,0,0,0,0,0.0,"support for transparent proxy $end$ Hi,

is it possible to implement ""transparent-proxy"" feature in maxscale ?

https://www.percona.com/doc/percona-xtradb-cluster/5.5/howtos/haproxy.html

It means basically, at new connection to backend from maxscale:
Sending to the db backend the originating IP via a protocol, so that the original IP can be displayed in show processlist, and used in authentification definition.

Currently, only percona server supports this feature.

Thanks !
Joffrey $acceptance criteria:$",0,0,0,0,0,0,0,13726.7,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
1667,MXS-3818,Task,MXS,2021-10-15 05:44:50,,0,Upgrade to Connector-C 3.1.15,"The 3.1.15 version has a bug fix that is needed in order for the fix to MXS-3801 to be tested. Once 3.1.15 is released, the attached patch should be applied to {{system_tests/binary_ps_cursor.cpp}}.",,"Upgrade to Connector-C 3.1.15 $end$ The 3.1.15 version has a bug fix that is needed in order for the fix to MXS-3801 to be tested. Once 3.1.15 is released, the attached patch should be applied to {{system_tests/binary_ps_cursor.cpp}}. $acceptance criteria:$",,markus makela,markus makela,Major,11,,2,0,2,1,0,0,0,,0,850,0,0,0,2021-11-22 09:07:35,Upgrade to Connector-C 3.1.15,"The 3.1.15 version has a bug fix that is needed in order for the fix to MXS-3801 to be tested. Once 3.1.15 is released, the attached patch should be applied to {{system_tests/binary_ps_cursor.cpp}}.",,0,0,0,0,0.0,"Upgrade to Connector-C 3.1.15 $end$ The 3.1.15 version has a bug fix that is needed in order for the fix to MXS-3801 to be tested. Once 3.1.15 is released, the attached patch should be applied to {{system_tests/binary_ps_cursor.cpp}}. $acceptance criteria:$",0,0,0,0,0,0,0,915.367,123,15,0.121951,10,0.0813008,8,0.0650406,8,0.0650406,7,0.0569106
1668,MXS-3822,New Feature,MXS,2021-10-20 18:44:24,,0,MaxScale Global Memory Use Indicator,"Some of our customers are frustrated by the inability to monitor global memory usage from MaxScale using any internal reporting such as status variables. 

Occasionally MaxScale uses a larger than expected amount of memory, be it because large transactions are in-flight or because the history of commands gets unexpectedly large, for example, or due to memory leaks in a current version of a MaxScale service or supporting tool such as SQLite.

The desire is to have a global variable in MaxScale reporting on the current cumulative memory use off all MaxScale services, including the SQLite footprint, so that automated monitoring could become aware of potential problem situations and address them proactively.",,"MaxScale Global Memory Use Indicator $end$ Some of our customers are frustrated by the inability to monitor global memory usage from MaxScale using any internal reporting such as status variables. 

Occasionally MaxScale uses a larger than expected amount of memory, be it because large transactions are in-flight or because the history of commands gets unexpectedly large, for example, or due to memory leaks in a current version of a MaxScale service or supporting tool such as SQLite.

The desire is to have a global variable in MaxScale reporting on the current cumulative memory use off all MaxScale services, including the SQLite footprint, so that automated monitoring could become aware of potential problem situations and address them proactively. $acceptance criteria:$",,Juan,Juan,Major,30,,0,3,2,5,0,0,0,,0,850,2,0,0,2022-06-06 09:55:20,MaxScale Global Memory Use Indicator,"Some of our customers are frustrated by the inability to monitor global memory usage from MaxScale using any internal reporting such as status variables. 

Occasionally MaxScale uses a larger than expected amount of memory, be it because large transactions are in-flight or because the history of commands gets unexpectedly large, for example, or due to memory leaks in a current version of a MaxScale service or supporting tool such as SQLite.

The desire is to have a global variable in MaxScale reporting on the current cumulative memory use off all MaxScale services, including the SQLite footprint, so that automated monitoring could become aware of potential problem situations and address them proactively.",,0,0,0,0,0.0,"MaxScale Global Memory Use Indicator $end$ Some of our customers are frustrated by the inability to monitor global memory usage from MaxScale using any internal reporting such as status variables. 

Occasionally MaxScale uses a larger than expected amount of memory, be it because large transactions are in-flight or because the history of commands gets unexpectedly large, for example, or due to memory leaks in a current version of a MaxScale service or supporting tool such as SQLite.

The desire is to have a global variable in MaxScale reporting on the current cumulative memory use off all MaxScale services, including the SQLite footprint, so that automated monitoring could become aware of potential problem situations and address them proactively. $acceptance criteria:$",0,0,0,0,0,0,1,5487.17,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1669,MXS-3827,New Feature,MXS,2021-10-25 08:38:46,,0,Audit log for the REST API,Being able to know what operations have been done via the REST API can be useful for auditing purposes. This could be added as a separate log file or as a new log facility.,,Audit log for the REST API $end$ Being able to know what operations have been done via the REST API can be useful for auditing purposes. This could be added as a separate log file or as a new log facility. $acceptance criteria:$,,markus makela,markus makela,Major,13,,0,0,1,2,0,0,0,,0,850,0,0,0,2023-01-16 10:52:33,Audit log for the REST API,Being able to know what operations have been done via the REST API can be useful for auditing purposes. This could be added as a separate log file or as a new log facility.,,0,0,0,0,0.0,Audit log for the REST API $end$ Being able to know what operations have been done via the REST API can be useful for auditing purposes. This could be added as a separate log file or as a new log facility. $acceptance criteria:$,0,0,0,0,0,0,1,10754.2,124,15,0.120968,10,0.0806452,8,0.0645161,8,0.0645161,7,0.0564516
1670,MXS-3828,Task,MXS,2021-10-25 09:26:06,,0,Explore custom SkySQL images,,,Explore custom SkySQL images $end$ $acceptance criteria:$,,markus makela,markus makela,Major,8,,0,0,1,2,0,0,0,,0,850,0,0,0,2021-10-25 09:26:06,Explore custom SkySQL images,,,0,0,0,0,0.0,Explore custom SkySQL images $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,125,15,0.12,10,0.08,8,0.064,8,0.064,7,0.056
1671,MXS-3830,Task,MXS,2021-10-25 10:49:43,,0,Store routing hints in a vector,,,Store routing hints in a vector $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-10-25 10:49:43,Store routing hints in a vector,,,0,0,0,0,0.0,Store routing hints in a vector $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,456,41,0.0899123,18,0.0394737,12,0.0263158,9,0.0197368,9,0.0197368
1672,MXS-3837,Task,MXS,2021-10-26 10:47:06,,0,Pre-select Listener as the default option in SQL connection dialog,,,Pre-select Listener as the default option in SQL connection dialog $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Minor,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-11-22 07:56:07,Pre-select Listener as the default option in SQL connection dialog,,,0,0,0,0,0.0,Pre-select Listener as the default option in SQL connection dialog $end$ $acceptance criteria:$,0,0,0,0,0,0,0,645.15,74,27,0.364865,16,0.216216,12,0.162162,9,0.121622,9,0.121622
1673,MXS-3843,Task,MXS,2021-10-29 11:23:34,,0,Update inih-library and move it to maxbase,,,Update inih-library and move it to maxbase $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-10-29 11:25:02,Update inih-library and move it to maxbase,,,0,0,0,0,0.0,Update inih-library and move it to maxbase $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,27,3,0.111111,2,0.0740741,1,0.037037,1,0.037037,0,0.0
1674,MXS-3844,New Feature,MXS,2021-10-30 02:24:32,,0,Cooperative Monitor Indicator,Add an indicator to the GUI dashboard that displays which MaxScale node is currently in control when Cooperative Monitoring is enabled.,,Cooperative Monitor Indicator $end$ Add an indicator to the GUI dashboard that displays which MaxScale node is currently in control when Cooperative Monitoring is enabled. $acceptance criteria:$,,Todd Stoffel,Todd Stoffel,Major,18,,0,1,0,2,0,0,0,,0,850,1,0,0,2021-12-07 08:10:15,Cooperative Monitor Indicator,Add an indicator to the GUI dashboard that displays which MaxScale node is currently in control when Cooperative Monitoring is enabled.,,0,0,0,0,0.0,Cooperative Monitor Indicator $end$ Add an indicator to the GUI dashboard that displays which MaxScale node is currently in control when Cooperative Monitoring is enabled. $acceptance criteria:$,0,0,0,0,0,0,1,917.75,10,3,0.3,3,0.3,2,0.2,2,0.2,1,0.1
1675,MXS-3848,Sub-Task,MXS,2021-11-02 11:26:47,,0,Create a confirm dialog to execute sql action,,,Create a confirm dialog to execute sql action $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2021-11-02 11:26:47,Create a confirm dialog to execute sql action,,,0,0,0,0,0.0,Create a confirm dialog to execute sql action $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,75,27,0.36,16,0.213333,12,0.16,9,0.12,9,0.12
1676,MXS-3853,New Feature,MXS,2021-11-04 08:15:00,,0,Manage MaxScale users in MaxGUI,Provide a new feature to manage the Maxscale GUI user creation or user modification user password updating or user privileges updating  and show the current users and their status.,,Manage MaxScale users in MaxGUI $end$ Provide a new feature to manage the Maxscale GUI user creation or user modification user password updating or user privileges updating  and show the current users and their status. $acceptance criteria:$,,Naresh Chandra,Naresh Chandra,Major,25,,0,3,1,1,0,4,0,,0,850,3,2,0,2022-04-11 07:48:29,Add a GUI To create or manage the users and users privileges for Maxscale GUI login,Provide a new feature to manage the Maxscale GUI user creation or user modification user password updating or user privileges updating  and show the current users and their status.,,2,0,0,19,0.3125,Add a GUI To create or manage the users and users privileges for Maxscale GUI login $end$ Provide a new feature to manage the Maxscale GUI user creation or user modification user password updating or user privileges updating  and show the current users and their status. $acceptance criteria:$,2,1,1,1,1,0,1,3791.55,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1677,MXS-3874,Task,MXS,2021-11-12 13:59:34,,0,Use mxb::ini to read configuration files,,,Use mxb::ini to read configuration files $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Minor,9,,0,0,0,3,0,0,0,,0,850,0,0,0,2021-11-15 15:54:07,Use mxb::ini to read configuration files,,,0,0,0,0,0.0,Use mxb::ini to read configuration files $end$ $acceptance criteria:$,0,0,0,0,0,0,1,73.9,28,3,0.107143,2,0.0714286,1,0.0357143,1,0.0357143,0,0.0
1678,MXS-3888,Task,MXS,2021-11-22 08:24:00,MXS-3387,0,Implement SCRAM/RFC5802,"Currently {{nosqlprotocol}} supports no authentication whatsoever, but all clients access the database using the credentials specified in {{maxscale.cnf}}.

Authentication support should be added so that each client can be identified. It may no be possible to directly map MariaDB server users to nosqlprotocol client users, but a mapping in MaxScale may be needed.
",,"Implement SCRAM/RFC5802 $end$ Currently {{nosqlprotocol}} supports no authentication whatsoever, but all clients access the database using the credentials specified in {{maxscale.cnf}}.

Authentication support should be added so that each client can be identified. It may no be possible to directly map MariaDB server users to nosqlprotocol client users, but a mapping in MaxScale may be needed.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,1,1,0,1,0,,0,850,0,0,0,2021-12-07 09:52:14,Implement authentication,"Currently {{nosqlprotocol}} supports no authentication whatsoever, but all clients access the database using the credentials specified in {{maxscale.cnf}}.

Authentication support should be added so that each client can be identified. It may no be possible to directly map MariaDB server users to nosqlprotocol client users, but a mapping in MaxScale may be needed.
",,1,0,0,2,0.0172414,"Implement authentication $end$ Currently {{nosqlprotocol}} supports no authentication whatsoever, but all clients access the database using the credentials specified in {{maxscale.cnf}}.

Authentication support should be added so that each client can be identified. It may no be possible to directly map MariaDB server users to nosqlprotocol client users, but a mapping in MaxScale may be needed.
 $acceptance criteria:$",1,1,0,0,0,0,0,361.467,457,41,0.0897155,18,0.0393873,12,0.0262582,9,0.0196937,9,0.0196937
1679,MXS-3889,Task,MXS,2021-11-22 08:56:06,MXS-3387,0,Investigate SCRAM and RFC5802,SCRAM is the authentication mechanism that should be implemented for {{nosqlprotocol}}.,,Investigate SCRAM and RFC5802 $end$ SCRAM is the authentication mechanism that should be implemented for {{nosqlprotocol}}. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,1,2,0,0,0,,0,850,0,0,0,2021-11-22 08:57:18,Investigate SCRAM and RFC5802,SCRAM is the authentication mechanism that should be implemented for {{nosqlprotocol}}.,,0,0,0,0,0.0,Investigate SCRAM and RFC5802 $end$ SCRAM is the authentication mechanism that should be implemented for {{nosqlprotocol}}. $acceptance criteria:$,0,0,0,0,0,0,1,0.0166667,458,42,0.0917031,18,0.0393013,12,0.0262009,9,0.0196507,9,0.0196507
1680,MXS-3890,Task,MXS,2021-11-22 09:29:49,,0,Move Log view to a separate page,"Instead of placing it on the settings page, move it to the Log page.
Use the same log icon as monitoring UI log https://www.figma.com/file/iRo294QJuNfQ3tyZboFWBq/%F0%9F%91%812.07.2021-%7C-Monitoring-Logs",,"Move Log view to a separate page $end$ Instead of placing it on the settings page, move it to the Log page.
Use the same log icon as monitoring UI log https://www.figma.com/file/iRo294QJuNfQ3tyZboFWBq/%F0%9F%91%812.07.2021-%7C-Monitoring-Logs $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2021-11-22 09:29:49,Move Log view to a separate page,"Instead of placing it on the settings page, move it to the Log page.
Use the same log icon as monitoring UI log https://www.figma.com/file/iRo294QJuNfQ3tyZboFWBq/%F0%9F%91%812.07.2021-%7C-Monitoring-Logs",,0,0,0,0,0.0,"Move Log view to a separate page $end$ Instead of placing it on the settings page, move it to the Log page.
Use the same log icon as monitoring UI log https://www.figma.com/file/iRo294QJuNfQ3tyZboFWBq/%F0%9F%91%812.07.2021-%7C-Monitoring-Logs $acceptance criteria:$",0,0,0,0,0,0,0,0.0,76,27,0.355263,16,0.210526,12,0.157895,9,0.118421,9,0.118421
1681,MXS-3891,Task,MXS,2021-11-22 11:46:50,,0,Benchmark 6.2,,,Benchmark 6.2 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2021-11-22 11:46:50,Benchmark 6.2,,,0,0,0,0,0.0,Benchmark 6.2 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,459,42,0.0915033,18,0.0392157,12,0.0261438,9,0.0196078,9,0.0196078
1682,MXS-3894,New Feature,MXS,2021-11-24 12:50:47,,0,Invent a configuration option to allow transaction replay ignore checksum check,"In some workflows, particularly the one in one of the important POCs today, unconditional application of the checksum test prevents replay from being used, even though the transaction logic does not in any way merit it.

The off-the-top proposal is to allow me to blacklist a bunch of tables and standard functions. MaxScale would skip the checksum if running into a SELECT from any one of these tables. It should be limited to selects without joins.

For the POC, a lot less is enough. Skip checksum test on SELECT LAST_INSERT_ID.",,"Invent a configuration option to allow transaction replay ignore checksum check $end$ In some workflows, particularly the one in one of the important POCs today, unconditional application of the checksum test prevents replay from being used, even though the transaction logic does not in any way merit it.

The off-the-top proposal is to allow me to blacklist a bunch of tables and standard functions. MaxScale would skip the checksum if running into a SELECT from any one of these tables. It should be limited to selects without joins.

For the POC, a lot less is enough. Skip checksum test on SELECT LAST_INSERT_ID. $acceptance criteria:$",,Gregory Dorman,Gregory Dorman,Critical,9,,0,2,2,1,0,0,0,,0,850,1,0,0,2021-12-07 07:24:31,Invent a configuration option to allow transaction replay ignore checksum check,"In some workflows, particularly the one in one of the important POCs today, unconditional application of the checksum test prevents replay from being used, even though the transaction logic does not in any way merit it.

The off-the-top proposal is to allow me to blacklist a bunch of tables and standard functions. MaxScale would skip the checksum if running into a SELECT from any one of these tables. It should be limited to selects without joins.

For the POC, a lot less is enough. Skip checksum test on SELECT LAST_INSERT_ID.",,0,0,0,0,0.0,"Invent a configuration option to allow transaction replay ignore checksum check $end$ In some workflows, particularly the one in one of the important POCs today, unconditional application of the checksum test prevents replay from being used, even though the transaction logic does not in any way merit it.

The off-the-top proposal is to allow me to blacklist a bunch of tables and standard functions. MaxScale would skip the checksum if running into a SELECT from any one of these tables. It should be limited to selects without joins.

For the POC, a lot less is enough. Skip checksum test on SELECT LAST_INSERT_ID. $acceptance criteria:$",0,0,0,0,0,0,0,306.55,1,1,1.0,0,0.0,0,0.0,0,0.0,0,0.0
1683,MXS-3898,New Feature,MXS,2021-12-03 09:57:11,,0,Add a way to define and switch different timeouts for primary and replicate during failover,"There are good reasons to have different settings for various timeouts like:

- max_statement_time
- wait_timeout
- lock_wait_timeout
- innodb_lock_wait_timeout
- etc

for primary/master server vs slave/replica. When failover happens these settings should be changed and assuming Maxscale does and controls failover process, it should get options to do this. I mean separate settings for ""primary"" and ""replica"" timeouts or maybe any/all global variables to enforce on a node that switches to that status.",,"Add a way to define and switch different timeouts for primary and replicate during failover $end$ There are good reasons to have different settings for various timeouts like:

- max_statement_time
- wait_timeout
- lock_wait_timeout
- innodb_lock_wait_timeout
- etc

for primary/master server vs slave/replica. When failover happens these settings should be changed and assuming Maxscale does and controls failover process, it should get options to do this. I mean separate settings for ""primary"" and ""replica"" timeouts or maybe any/all global variables to enforce on a node that switches to that status. $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,18,,0,4,0,1,0,0,0,,0,850,0,0,0,2022-09-12 10:14:02,Add a way to define and switch different timeouts for primary and replicate during failover,"There are good reasons to have different settings for various timeouts like:

- max_statement_time
- wait_timeout
- lock_wait_timeout
- innodb_lock_wait_timeout
- etc

for primary/master server vs slave/replica. When failover happens these settings should be changed and assuming Maxscale does and controls failover process, it should get options to do this. I mean separate settings for ""primary"" and ""replica"" timeouts or maybe any/all global variables to enforce on a node that switches to that status.",,0,0,0,0,0.0,"Add a way to define and switch different timeouts for primary and replicate during failover $end$ There are good reasons to have different settings for various timeouts like:

- max_statement_time
- wait_timeout
- lock_wait_timeout
- innodb_lock_wait_timeout
- etc

for primary/master server vs slave/replica. When failover happens these settings should be changed and assuming Maxscale does and controls failover process, it should get options to do this. I mean separate settings for ""primary"" and ""replica"" timeouts or maybe any/all global variables to enforce on a node that switches to that status. $acceptance criteria:$",0,0,0,0,0,0,0,6792.27,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1684,MXS-3902,New Feature,MXS,2021-12-07 09:48:28,,0,Limit total number of connections to backend,"When the MaxScale pooling feature is enabled, idle connections are put to a pool from which connections are taken when needed. However, if there are no connections in the pool, then a new backend connection will always created. Consequently, the presence of the pool does not provide an upper limit for the total number of backend connections.

There are situations where providing an upper limit on the number of backend connections, while simultaneously not restricting the number of clients, is what is wanted. The pooling mechanism should be extended so that it also optionally would imply the maximum number of connections that are created.",,"Limit total number of connections to backend $end$ When the MaxScale pooling feature is enabled, idle connections are put to a pool from which connections are taken when needed. However, if there are no connections in the pool, then a new backend connection will always created. Consequently, the presence of the pool does not provide an upper limit for the total number of backend connections.

There are situations where providing an upper limit on the number of backend connections, while simultaneously not restricting the number of clients, is what is wanted. The pooling mechanism should be extended so that it also optionally would imply the maximum number of connections that are created. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,0,1,7,0,0,0,,0,850,0,0,0,2021-12-07 09:49:28,Limit total number of connections to backend,"When the MaxScale pooling feature is enabled, idle connections are put to a pool from which connections are taken when needed. However, if there are no connections in the pool, then a new backend connection will always created. Consequently, the presence of the pool does not provide an upper limit for the total number of backend connections.

There are situations where providing an upper limit on the number of backend connections, while simultaneously not restricting the number of clients, is what is wanted. The pooling mechanism should be extended so that it also optionally would imply the maximum number of connections that are created.",,0,0,0,0,0.0,"Limit total number of connections to backend $end$ When the MaxScale pooling feature is enabled, idle connections are put to a pool from which connections are taken when needed. However, if there are no connections in the pool, then a new backend connection will always created. Consequently, the presence of the pool does not provide an upper limit for the total number of backend connections.

There are situations where providing an upper limit on the number of backend connections, while simultaneously not restricting the number of clients, is what is wanted. The pooling mechanism should be extended so that it also optionally would imply the maximum number of connections that are created. $acceptance criteria:$",0,0,0,0,0,0,1,0.0166667,460,42,0.0913043,18,0.0391304,12,0.026087,9,0.0195652,9,0.0195652
1685,MXS-3903,Task,MXS,2021-12-07 09:55:12,,0,Investigate Postgres wire protocol,Take a look at the Postgres wire protocol and provide a rough estimate of what kind of effort would be needed to support it. ,,Investigate Postgres wire protocol $end$ Take a look at the Postgres wire protocol and provide a rough estimate of what kind of effort would be needed to support it.  $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,1,0,4,0,0,0,,0,850,1,0,0,2021-12-07 09:55:12,Investigate Postgres wire protocol,Take a look at the Postgres wire protocol and provide a rough estimate of what kind of effort would be needed to support it. ,,0,0,0,0,0.0,Investigate Postgres wire protocol $end$ Take a look at the Postgres wire protocol and provide a rough estimate of what kind of effort would be needed to support it.  $acceptance criteria:$,0,0,0,0,0,0,1,0.0,461,42,0.0911063,18,0.0390456,12,0.0260304,9,0.0195228,9,0.0195228
1686,MXS-3905,Task,MXS,2021-12-08 11:26:54,,0,"Create Docker Registry in GCloud, push all CI builds into it",push CI Maxscale Docker images to the registry accessible by Enterprise toke ,,"Create Docker Registry in GCloud, push all CI builds into it $end$ push CI Maxscale Docker images to the registry accessible by Enterprise toke  $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,3,0,0,1,,0,850,1,0,0,2021-12-08 11:47:58,"Create Docker Registry in GCloud, push all CI builds into it",push CI Maxscale Docker images to the registry accessible by Enterprise toke ,,0,0,0,0,0.0,"Create Docker Registry in GCloud, push all CI builds into it $end$ push CI Maxscale Docker images to the registry accessible by Enterprise toke  $acceptance criteria:$",0,0,0,0,0,0,1,0.35,93,1,0.0107527,0,0.0,0,0.0,0,0.0,0,0.0
1687,MXS-3906,Sub-Task,MXS,2021-12-08 11:29:27,,0,use internal connection to get Maxscale packages without user/password in the URL,"currently Docker image build for Maxscale uses https:/user:password@... URL and this URL is stored forever in the Docker image and visible in the `docker historicity`, BuildBot should use internal GCloud URL in the repo config",,"use internal connection to get Maxscale packages without user/password in the URL $end$ currently Docker image build for Maxscale uses https:/user:password@... URL and this URL is stored forever in the Docker image and visible in the `docker historicity`, BuildBot should use internal GCloud URL in the repo config $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2021-12-08 11:29:27,use internal connection to get Maxscale packages without user/password in the URL,"currently Docker image build for Maxscale uses https:/user:password@... URL and this URL is stored forever in the Docker image and visible in the `docker historicity`, BuildBot should use internal GCloud URL in the repo config",,0,0,0,0,0.0,"use internal connection to get Maxscale packages without user/password in the URL $end$ currently Docker image build for Maxscale uses https:/user:password@... URL and this URL is stored forever in the Docker image and visible in the `docker historicity`, BuildBot should use internal GCloud URL in the repo config $acceptance criteria:$",0,0,0,0,0,0,1,0.0,94,1,0.0106383,0,0.0,0,0.0,0,0.0,0,0.0
1688,MXS-391,New Feature,MXS,2015-10-05 11:01:14,,0,Add support for wildcards in hostnames,"h2. User grant information warning messages (not able to process grants properly?) 

I notice on startup of MaxScale when using the binlog router module the following messages:

{code}
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host1.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host2.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2%, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user3@host2% for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: mysql
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: performance_schema
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host %host4%.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user5@%host4%.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
{code}
Note: these are modified to not show real user names.

It seems clear from the logging that MaxScale is not able to handle usernames of the following types:
* with explicit access to multiple databases
* with hostnames containing % wildcard values
* it also looks like it it tries to resolve the grants on startup and this is not dynamic.

Behaviour therefore does not seem to match MySQL/MariaDB. The error messages are confusing and you do not allow grants for several users which if I was trying to provide access to maxscale as a proxy would be problematic.  Grants are often hard to change and compliance may make it difficult to provide more access than currently configured, so enabling maxscale to recognise these cases better would be most helpful.

For the binlog router this is not such an issue as access credentials and the number of users involved is much smaller but even so seeing these messages every time MaxScale starts up looks confusing.",,"Add support for wildcards in hostnames $end$ h2. User grant information warning messages (not able to process grants properly?) 

I notice on startup of MaxScale when using the binlog router module the following messages:

{code}
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host1.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host2.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2%, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user3@host2% for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: mysql
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: performance_schema
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host %host4%.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user5@%host4%.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
{code}
Note: these are modified to not show real user names.

It seems clear from the logging that MaxScale is not able to handle usernames of the following types:
* with explicit access to multiple databases
* with hostnames containing % wildcard values
* it also looks like it it tries to resolve the grants on startup and this is not dynamic.

Behaviour therefore does not seem to match MySQL/MariaDB. The error messages are confusing and you do not allow grants for several users which if I was trying to provide access to maxscale as a proxy would be problematic.  Grants are often hard to change and compliance may make it difficult to provide more access than currently configured, so enabling maxscale to recognise these cases better would be most helpful.

For the binlog router this is not such an issue as access credentials and the number of users involved is much smaller but even so seeing these messages every time MaxScale starts up looks confusing. $acceptance criteria:$",,Simon J Mudd,Simon J Mudd,Minor,13,,0,1,3,1,0,3,0,,0,850,0,3,0,2017-01-04 10:47:43,Add support for wildcards in hostnames,"h2. User grant information warning messages (not able to process grants properly?) 

I notice on startup of MaxScale when using the binlog router module the following messages:

{code}
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host1.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host2.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2%, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user3@host2% for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: mysql
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: performance_schema
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host %host4%.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user5@%host4%.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
{code}
Note: these are modified to not show real user names.

It seems clear from the logging that MaxScale is not able to handle usernames of the following types:
* with explicit access to multiple databases
* with hostnames containing % wildcard values
* it also looks like it it tries to resolve the grants on startup and this is not dynamic.

Behaviour therefore does not seem to match MySQL/MariaDB. The error messages are confusing and you do not allow grants for several users which if I was trying to provide access to maxscale as a proxy would be problematic.  Grants are often hard to change and compliance may make it difficult to provide more access than currently configured, so enabling maxscale to recognise these cases better would be most helpful.

For the binlog router this is not such an issue as access credentials and the number of users involved is much smaller but even so seeing these messages every time MaxScale starts up looks confusing.",,0,0,0,0,0.0,"Add support for wildcards in hostnames $end$ h2. User grant information warning messages (not able to process grants properly?) 

I notice on startup of MaxScale when using the binlog router module the following messages:

{code}
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host1.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user1@host2.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host host2%, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user3@host2% for service [binlog_service]. This user will be unavailable via MaxScale.
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: mysql
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Duplicate MySQL user found for service [binlog_service]: user4@10.% for database: performance_schema
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Error: Failed to obtain address for host %host4%.example.com, Name or service not known
Oct  5 09:42:36 maxscale.example.com MaxScale[41563]: Warning: Failed to add user user5@%host4%.example.com for service [binlog_service]. This user will be unavailable via MaxScale.
{code}
Note: these are modified to not show real user names.

It seems clear from the logging that MaxScale is not able to handle usernames of the following types:
* with explicit access to multiple databases
* with hostnames containing % wildcard values
* it also looks like it it tries to resolve the grants on startup and this is not dynamic.

Behaviour therefore does not seem to match MySQL/MariaDB. The error messages are confusing and you do not allow grants for several users which if I was trying to provide access to maxscale as a proxy would be problematic.  Grants are often hard to change and compliance may make it difficult to provide more access than currently configured, so enabling maxscale to recognise these cases better would be most helpful.

For the binlog router this is not such an issue as access credentials and the number of users involved is much smaller but even so seeing these messages every time MaxScale starts up looks confusing. $acceptance criteria:$",0,0,0,0,0,0,0,10967.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1689,MXS-3912,New Feature,MXS,2021-12-14 05:19:57,,0,maxctrl command to check last time login details for any Maxscale user ,"Currently we have only option to list out the users with in Maxscale. 

{code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Date/Time* column can help to understand when which user logged in with in Maxscale. ",,"maxctrl command to check last time login details for any Maxscale user  $end$ Currently we have only option to list out the users with in Maxscale. 

{code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Date/Time* column can help to understand when which user logged in with in Maxscale.  $acceptance criteria:$",,Pramod Mahto,Pramod Mahto,Minor,14,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-04-25 08:13:25,maxctrl command to check last time login details for any Maxscale user ,"Currently we have only option to list out the users with in Maxscale. 

{code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Date/Time* column can help to understand when which user logged in with in Maxscale. ",,0,0,0,0,0.0,"maxctrl command to check last time login details for any Maxscale user  $end$ Currently we have only option to list out the users with in Maxscale. 

{code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Date/Time* column can help to understand when which user logged in with in Maxscale.  $acceptance criteria:$",0,0,0,0,0,0,0,3170.88,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1690,MXS-3913,New Feature,MXS,2021-12-14 05:27:14,,0,maxctrl command to check last time Password Change  details for any Maxscale user ,"Currently we have only option to list out the users with in Maxscale.

 {code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Last Password Change* column can help to understand when/which user password changed  with in Maxscale.",,"maxctrl command to check last time Password Change  details for any Maxscale user  $end$ Currently we have only option to list out the users with in Maxscale.

 {code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Last Password Change* column can help to understand when/which user password changed  with in Maxscale. $acceptance criteria:$",,Pramod Mahto,Pramod Mahto,Major,13,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-04-25 08:13:36,maxctrl command to check last time Password Change  details for any Maxscale user ,"Currently we have only option to list out the users with in Maxscale.

 {code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Last Password Change* column can help to understand when/which user password changed  with in Maxscale.",,0,0,0,0,0.0,"maxctrl command to check last time Password Change  details for any Maxscale user  $end$ Currently we have only option to list out the users with in Maxscale.

 {code}

[root@c8_maxscale_01 ~]# maxctrl list users
┌───────────────┬──────┬────────────┐
│ Name          │ Type │ Privileges │
├───────────────┼──────┼────────────┤
│ maxscaleadmin │ inet │ basic      │
├───────────────┼──────┼────────────┤
│ admin         │ inet │ admin      │
└───────────────┴──────┴────────────┘

{code}

Last Access *Last Password Change* column can help to understand when/which user password changed  with in Maxscale. $acceptance criteria:$",0,0,0,0,0,0,0,3170.77,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1691,MXS-3918,New Feature,MXS,2021-12-16 07:03:44,,0,Add stop query button in the Query Editor,"After preparing the query we have only run button but if we want stop/pause the the query execution, then we need a stop/pause button. 
So can you we made change for run button as RUN/STOP button, whenever we run the query the same button we can use it for stop instead of two buttons.

 !image-2021-12-16-12-33-30-010.png|thumbnail! ",,"Add stop query button in the Query Editor $end$ After preparing the query we have only run button but if we want stop/pause the the query execution, then we need a stop/pause button. 
So can you we made change for run button as RUN/STOP button, whenever we run the query the same button we can use it for stop instead of two buttons.

 !image-2021-12-16-12-33-30-010.png|thumbnail!  $acceptance criteria:$",,Naresh Chandra,Naresh Chandra,Major,16,,0,9,0,1,0,1,0,,0,850,3,0,0,2022-03-23 12:12:25,Implement Stop button on the Query Editor GUI For Stopping the Query Execution,"After preparing the query we have only run button but if we want stop/pause the the query execution, then we need a stop/pause button. 
So can you we made change for run button as RUN/STOP button, whenever we run the query the same button we can use it for stop instead of two buttons.

 !image-2021-12-16-12-33-30-010.png|thumbnail! ",,1,0,0,13,0.140845,"Implement Stop button on the Query Editor GUI For Stopping the Query Execution $end$ After preparing the query we have only run button but if we want stop/pause the the query execution, then we need a stop/pause button. 
So can you we made change for run button as RUN/STOP button, whenever we run the query the same button we can use it for stop instead of two buttons.

 !image-2021-12-16-12-33-30-010.png|thumbnail!  $acceptance criteria:$",1,1,1,1,0,0,1,2333.13,1,1,1.0,1,1.0,1,1.0,1,1.0,0,0.0
1692,MXS-3925,New Feature,MXS,2021-12-20 10:48:39,MXS-3387,0,Implement authentication,Add support  for MongoDB SCRAM authentication using the SCRAM-SHA-1 and SCRAM-SHA-256 mechanisms. ,,Implement authentication $end$ Add support  for MongoDB SCRAM authentication using the SCRAM-SHA-1 and SCRAM-SHA-256 mechanisms.  $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,3,0,2,0,,0,850,0,0,0,2021-12-20 10:48:39,Implement authentication,Add support  for SCRAM-1 and SCRAM-256 authentication. ,,0,2,0,11,0.666667,Implement authentication $end$ Add support  for SCRAM-1 and SCRAM-256 authentication.  $acceptance criteria:$,2,1,1,1,0,0,1,0.0,462,42,0.0909091,18,0.038961,12,0.025974,9,0.0194805,9,0.0194805
1693,MXS-3946,New Feature,MXS,2022-01-14 07:18:44,,0,Highlight text when filtering (GUI),"The text should be highlighted when filtering data in
`SCHEMAS` tree, result table, and other places having filter input.
This improves UX.",,"Highlight text when filtering (GUI) $end$ The text should be highlighted when filtering data in
`SCHEMAS` tree, result table, and other places having filter input.
This improves UX. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,11,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-09-26 10:33:09,Highlight text when filtering (GUI),"The text should be highlighted when filtering data in
`SCHEMAS` tree, result table, and other places having filter input.
This improves UX.",,0,0,0,0,0.0,"Highlight text when filtering (GUI) $end$ The text should be highlighted when filtering data in
`SCHEMAS` tree, result table, and other places having filter input.
This improves UX. $acceptance criteria:$",0,0,0,0,0,0,0,6123.23,77,27,0.350649,16,0.207792,12,0.155844,9,0.116883,9,0.116883
1694,MXS-3950,Task,MXS,2022-01-17 08:51:42,MXS-3387,0,Add support for SCRAM-SHA-256,,,Add support for SCRAM-SHA-256 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-01-17 08:51:42,Add support for SCRAM-SHA-256,,,0,0,0,0,0.0,Add support for SCRAM-SHA-256 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,463,43,0.0928726,19,0.0410367,13,0.0280778,9,0.0194384,9,0.0194384
1695,MXS-3951,Task,MXS,2022-01-17 11:35:32,,0,Use new config in monitors,Take the new configuration mechanism in use in monitors and remove the old one.,,Use new config in monitors $end$ Take the new configuration mechanism in use in monitors and remove the old one. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-01-17 11:35:32,Use new config in monitors,Take the new configuration mechanism in use in monitors and remove the old one.,,0,0,0,0,0.0,Use new config in monitors $end$ Take the new configuration mechanism in use in monitors and remove the old one. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,126,15,0.119048,10,0.0793651,8,0.0634921,8,0.0634921,7,0.0555556
1696,MXS-3960,Sub-Task,MXS,2022-01-24 08:10:40,,0,Create configuration page,A standalone page for showing maxscale's chain configuration,,Create configuration page $end$ A standalone page for showing maxscale's chain configuration $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,6,0,1,0,,0,850,0,0,0,2022-01-24 08:10:40,Create chain configuration page,A standalone page for showing maxscale's chain configuration,,1,0,0,1,0.0666667,Create chain configuration page $end$ A standalone page for showing maxscale's chain configuration $acceptance criteria:$,1,1,0,0,0,0,1,0.0,78,27,0.346154,16,0.205128,12,0.153846,9,0.115385,9,0.115385
1697,MXS-3961,Sub-Task,MXS,2022-01-24 08:13:13,,0,Create clusters discovery page,"A standalone page for showing clusters. Clicking on a cluster, it will navigate to a cluster page where the cluster is visualized as a tree graph. 
The user can control the replication by dragging the node of the tree. Each node should show some info and have a button to open dialog for configuring replication options.",,"Create clusters discovery page $end$ A standalone page for showing clusters. Clicking on a cluster, it will navigate to a cluster page where the cluster is visualized as a tree graph. 
The user can control the replication by dragging the node of the tree. Each node should show some info and have a button to open dialog for configuring replication options. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,10,,0,1,0,6,0,2,0,,0,850,1,0,0,2022-01-24 08:13:13,Create topology discovery page,A standalone page for showing topologies graphs where the user can control replication of a topology.,,1,1,0,48,1.91304,Create topology discovery page $end$ A standalone page for showing topologies graphs where the user can control replication of a topology. $acceptance criteria:$,2,1,1,1,1,1,1,0.0,79,28,0.35443,16,0.202532,12,0.151899,9,0.113924,9,0.113924
1698,MXS-3967,Task,MXS,2022-01-28 07:46:58,MXS-3387,0,Add support for re-authentication,"Currently it is possible to authenticate, but not to re-authenticate, that is, to change used credentials without reconnecting. It is trivial to support that in the connection between the client and MaxScale, but any changes there have to be reflected in the connections between MaxScale and the backend servers. That can either be supported by generating a COM_CHANGE_USER packet or simpler by just closing the backend connections and reconnecting with new MariaDB credentials.",,"Add support for re-authentication $end$ Currently it is possible to authenticate, but not to re-authenticate, that is, to change used credentials without reconnecting. It is trivial to support that in the connection between the client and MaxScale, but any changes there have to be reflected in the connections between MaxScale and the backend servers. That can either be supported by generating a COM_CHANGE_USER packet or simpler by just closing the backend connections and reconnecting with new MariaDB credentials. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-01-28 07:46:58,Add support for re-authentication,"Currently it is possible to authenticate, but not to re-authenticate, that is, to change used credentials without reconnecting. It is trivial to support that in the connection between the client and MaxScale, but any changes there have to be reflected in the connections between MaxScale and the backend servers. That can either be supported by generating a COM_CHANGE_USER packet or simpler by just closing the backend connections and reconnecting with new MariaDB credentials.",,0,0,0,0,0.0,"Add support for re-authentication $end$ Currently it is possible to authenticate, but not to re-authenticate, that is, to change used credentials without reconnecting. It is trivial to support that in the connection between the client and MaxScale, but any changes there have to be reflected in the connections between MaxScale and the backend servers. That can either be supported by generating a COM_CHANGE_USER packet or simpler by just closing the backend connections and reconnecting with new MariaDB credentials. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,464,43,0.0926724,19,0.0409483,13,0.0280172,9,0.0193966,9,0.0193966
1699,MXS-3968,New Feature,MXS,2022-01-28 07:58:07,MXS-3387,0,Add support for SSL,"With SCRAM authentication supported, SSL support is the missing piece for making the communication between the client and nosqlprotocol fully secure.",,"Add support for SSL $end$ With SCRAM authentication supported, SSL support is the missing piece for making the communication between the client and nosqlprotocol fully secure. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-01-28 07:58:07,Add support for SSL,"With SCRAM authentication supported, SSL support is the missing piece for making the communication between the client and nosqlprotocol fully secure.",,0,0,0,0,0.0,"Add support for SSL $end$ With SCRAM authentication supported, SSL support is the missing piece for making the communication between the client and nosqlprotocol fully secure. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,465,43,0.0924731,19,0.0408602,13,0.027957,9,0.0193548,9,0.0193548
1700,MXS-3969,Task,MXS,2022-01-28 12:06:48,,0,Create test sutup with several client test machines for Maxscale high load test,"example scenario:
- run one normal Maxscale test from system-test set to create setup
- separately run MDBCI to create a number of VMs to run clients
- run Maxscale clients on the new VMs
- monitor Maxscale (until crash)

it should be long running test

",,"Create test sutup with several client test machines for Maxscale high load test $end$ example scenario:
- run one normal Maxscale test from system-test set to create setup
- separately run MDBCI to create a number of VMs to run clients
- run Maxscale clients on the new VMs
- monitor Maxscale (until crash)

it should be long running test

 $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-01-31 11:50:22,Create test sutup with several client test machines for Maxscale high load test,"example scenario:
- run one normal Maxscale test from system-test set to create setup
- separately run MDBCI to create a number of VMs to run clients
- run Maxscale clients on the new VMs
- monitor Maxscale (until crash)

it should be long running test

",,0,0,0,0,0.0,"Create test sutup with several client test machines for Maxscale high load test $end$ example scenario:
- run one normal Maxscale test from system-test set to create setup
- separately run MDBCI to create a number of VMs to run clients
- run Maxscale clients on the new VMs
- monitor Maxscale (until crash)

it should be long running test

 $acceptance criteria:$",0,0,0,0,0,0,0,71.7167,95,1,0.0105263,0,0.0,0,0.0,0,0.0,0,0.0
1701,MXS-3981,Task,MXS,2022-02-02 11:35:32,MXS-3387,0,Store only hashed versions of passwords,,,Store only hashed versions of passwords $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-02-02 11:35:43,Store only hashed versions of passwords,,,0,0,0,0,0.0,Store only hashed versions of passwords $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,466,43,0.0922747,19,0.0407725,13,0.027897,9,0.0193133,9,0.0193133
1702,MXS-3985,Task,MXS,2022-02-04 08:21:42,,0,Make mariadbmon operations asynchronous," Commands such as `release-locks, rejoin, failover, reset-replication` should support asynchronous calls. Otherwise, the GUI won't know when the operation is successfully done and be responsible during that time. ",,"Make mariadbmon operations asynchronous $end$  Commands such as `release-locks, rejoin, failover, reset-replication` should support asynchronous calls. Otherwise, the GUI won't know when the operation is successfully done and be responsible during that time.  $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Blocker,10,,0,0,1,1,0,1,0,,0,850,0,1,0,2022-03-08 09:08:15,Make mariadbmon operations asynchronous," Commands such as `release-locks, rejoin, failover, reset-replication` should support asynchronous calls. Otherwise, the GUI won't know when the operation is successfully done and be responsible during that time. ",,0,0,0,0,0.0,"Make mariadbmon operations asynchronous $end$  Commands such as `release-locks, rejoin, failover, reset-replication` should support asynchronous calls. Otherwise, the GUI won't know when the operation is successfully done and be responsible during that time.  $acceptance criteria:$",0,0,0,0,0,0,0,768.767,80,29,0.3625,17,0.2125,13,0.1625,10,0.125,10,0.125
1703,MXS-399,New Feature,MXS,2015-10-07 13:23:06,,0,maxadmin -p should behave same way as mysql/mariadb client,"Hi, can the maxadmin command with -p option (no password provided)  behave the same way as mysql or mariadb client, which is to ask for a password, instead of the help error ?

[root@vm-liv-olk-db-3 ~]# maxadmin -p
maxadmin: option requires an argument -- 'p'
maxadmin Version 1.2.1
The MaxScale administrative and monitor client.

Usage: maxadmin [-u user] [-p password] [-h hostname] [-P port] [<command file> | <command>]

  -u|--user=...	        The user name to use for the connection, default
			is admin.
  -p|--password=...	The user password, if not given the password will
			be prompted for interactively
  -h|--hostname=...	The maxscale host to connecto to. The default is
			localhost
  -P|--port=...       	The port to use for the connection, the default
			port is 6603.
  -v|--version          print version information and exit
  -?|--help		Print this help text.
Any remaining arguments are treated as MaxScale commands or a file
containing commands to execute.

# maxadmin
Password:

Thanks in advance,
Joffrey
",,"maxadmin -p should behave same way as mysql/mariadb client $end$ Hi, can the maxadmin command with -p option (no password provided)  behave the same way as mysql or mariadb client, which is to ask for a password, instead of the help error ?

[root@vm-liv-olk-db-3 ~]# maxadmin -p
maxadmin: option requires an argument -- 'p'
maxadmin Version 1.2.1
The MaxScale administrative and monitor client.

Usage: maxadmin [-u user] [-p password] [-h hostname] [-P port] [<command file> | <command>]

  -u|--user=...	        The user name to use for the connection, default
			is admin.
  -p|--password=...	The user password, if not given the password will
			be prompted for interactively
  -h|--hostname=...	The maxscale host to connecto to. The default is
			localhost
  -P|--port=...       	The port to use for the connection, the default
			port is 6603.
  -v|--version          print version information and exit
  -?|--help		Print this help text.
Any remaining arguments are treated as MaxScale commands or a file
containing commands to execute.

# maxadmin
Password:

Thanks in advance,
Joffrey
 $acceptance criteria:$",,Joffrey MICHAIE,Joffrey MICHAIE,Minor,9,,0,0,1,1,0,0,0,,0,850,0,0,0,2016-11-02 10:51:10,maxadmin -p should behave same way as mysql/mariadb client,"Hi, can the maxadmin command with -p option (no password provided)  behave the same way as mysql or mariadb client, which is to ask for a password, instead of the help error ?

[root@vm-liv-olk-db-3 ~]# maxadmin -p
maxadmin: option requires an argument -- 'p'
maxadmin Version 1.2.1
The MaxScale administrative and monitor client.

Usage: maxadmin [-u user] [-p password] [-h hostname] [-P port] [<command file> | <command>]

  -u|--user=...	        The user name to use for the connection, default
			is admin.
  -p|--password=...	The user password, if not given the password will
			be prompted for interactively
  -h|--hostname=...	The maxscale host to connecto to. The default is
			localhost
  -P|--port=...       	The port to use for the connection, the default
			port is 6603.
  -v|--version          print version information and exit
  -?|--help		Print this help text.
Any remaining arguments are treated as MaxScale commands or a file
containing commands to execute.

# maxadmin
Password:

Thanks in advance,
Joffrey
",,0,0,0,0,0.0,"maxadmin -p should behave same way as mysql/mariadb client $end$ Hi, can the maxadmin command with -p option (no password provided)  behave the same way as mysql or mariadb client, which is to ask for a password, instead of the help error ?

[root@vm-liv-olk-db-3 ~]# maxadmin -p
maxadmin: option requires an argument -- 'p'
maxadmin Version 1.2.1
The MaxScale administrative and monitor client.

Usage: maxadmin [-u user] [-p password] [-h hostname] [-P port] [<command file> | <command>]

  -u|--user=...	        The user name to use for the connection, default
			is admin.
  -p|--password=...	The user password, if not given the password will
			be prompted for interactively
  -h|--hostname=...	The maxscale host to connecto to. The default is
			localhost
  -P|--port=...       	The port to use for the connection, the default
			port is 6603.
  -v|--version          print version information and exit
  -?|--help		Print this help text.
Any remaining arguments are treated as MaxScale commands or a file
containing commands to execute.

# maxadmin
Password:

Thanks in advance,
Joffrey
 $acceptance criteria:$",0,0,0,0,0,0,0,9405.47,2,1,0.5,0,0.0,0,0.0,0,0.0,0,0.0
1704,MXS-3990,Task,MXS,2022-02-09 08:25:40,,0,Fix issues with the rebalancing feature,"The rebalancing functionality that moves sessions from one thread to another in order to balance the load must take the implications of delayed calls and handle that so that a moved session does not end up being called from multiple threads.
",,"Fix issues with the rebalancing feature $end$ The rebalancing functionality that moves sessions from one thread to another in order to balance the load must take the implications of delayed calls and handle that so that a moved session does not end up being called from multiple threads.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,1,0,1,3,0,0,0,,0,850,0,0,0,2022-02-11 13:08:17,Fix issues with the rebalancing feature,"The rebalancing functionality that moves sessions from one thread to another in order to balance the load must take the implications of delayed calls and handle that so that a moved session does not end up being called from multiple threads.
",,0,0,0,0,0.0,"Fix issues with the rebalancing feature $end$ The rebalancing functionality that moves sessions from one thread to another in order to balance the load must take the implications of delayed calls and handle that so that a moved session does not end up being called from multiple threads.
 $acceptance criteria:$",0,0,0,0,0,0,1,52.7,467,43,0.0920771,19,0.0406852,13,0.0278373,9,0.0192719,9,0.0192719
1705,MXS-3993,Task,MXS,2022-02-10 05:25:05,,0,Use native capability API from C/C,"Now that we found out that one is available in CONC-579, we ought to take it into use.

{code:c++}
unsigned long ServerCapabilities, ServerExtCapabilities;
mariadb_get_infov(mariadb, MARIADB_CONNECTION_SERVER_CAPABILITIES, (void*)&ServerCapabilities);
mariadb_get_infov(mariadb, MARIADB_CONNECTION_EXTENDED_SERVER_CAPABILITIES, (void*)&ServerExtCapabilities);
{code}",,"Use native capability API from C/C $end$ Now that we found out that one is available in CONC-579, we ought to take it into use.

{code:c++}
unsigned long ServerCapabilities, ServerExtCapabilities;
mariadb_get_infov(mariadb, MARIADB_CONNECTION_SERVER_CAPABILITIES, (void*)&ServerCapabilities);
mariadb_get_infov(mariadb, MARIADB_CONNECTION_EXTENDED_SERVER_CAPABILITIES, (void*)&ServerExtCapabilities);
{code} $acceptance criteria:$",,markus makela,markus makela,Minor,9,,0,1,0,1,0,1,0,,0,850,1,0,0,2022-02-14 10:26:31,Use native capability API from C/C,"Now that we found out that one is available in CONC-579, we ought to take it into use.",,0,1,0,12,0.444444,"Use native capability API from C/C $end$ Now that we found out that one is available in CONC-579, we ought to take it into use. $acceptance criteria:$",1,1,1,1,0,0,1,101.017,127,15,0.11811,10,0.0787402,8,0.0629921,8,0.0629921,7,0.0551181
1706,MXS-3994,Task,MXS,2022-02-11 13:09:10,MXS-3387,0,Integrate NoSQL tests into the system-test,,,Integrate NoSQL tests into the system-test $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-02-11 13:09:10,Integrate NoSQL tests into the system-test,,,0,0,0,0,0.0,Integrate NoSQL tests into the system-test $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,468,43,0.0918803,19,0.0405983,13,0.0277778,9,0.0192308,9,0.0192308
1707,MXS-3996,Task,MXS,2022-02-14 08:10:08,,0,Tune functional tests of nosqlprotocol,,,Tune functional tests of nosqlprotocol $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-02-14 08:10:08,Tune functional tests of nosqlprotocol,,,0,0,0,0,0.0,Tune functional tests of nosqlprotocol $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,469,43,0.0916844,19,0.0405117,13,0.0277185,9,0.0191898,9,0.0191898
1708,MXS-3997,New Feature,MXS,2022-02-14 11:36:55,,0,Name threads for better CPU usage view,,,Name threads for better CPU usage view $end$ $acceptance criteria:$,,Niclas Antti,Niclas Antti,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-02-14 11:48:56,Name threads for better CPU usage view,,,0,0,0,0,0.0,Name threads for better CPU usage view $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.2,18,1,0.0555556,0,0.0,0,0.0,0,0.0,0,0.0
1709,MXS-4010,New Feature,MXS,2022-02-17 18:23:22,,0,provide a method to purge avro files,Provide an automatic and manual method to purge avro log files from the avro router.,,provide a method to purge avro files $end$ Provide an automatic and manual method to purge avro log files from the avro router. $acceptance criteria:$,,Kyle Joiner,Kyle Joiner,Major,10,,0,1,1,1,0,0,0,,0,850,1,0,0,2022-03-28 07:33:21,provide a method to purge avro files,Provide an automatic and manual method to purge avro log files from the avro router.,,0,0,0,0,0.0,provide a method to purge avro files $end$ Provide an automatic and manual method to purge avro log files from the avro router. $acceptance criteria:$,0,0,0,0,0,0,0,925.15,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1710,MXS-4013,New Feature,MXS,2022-02-18 11:14:52,,0,"Add Views, Functions and Indexes to Query Editor schema sidebar","Hi Team,

We are unable to see the below objects in the query editor.

Currently we are seeing only Tables and Procedures, so we need the below objects along with them.

1. Views
2. Functions
3. Triggers
4. Indexes (Primar key, Unique Key, Composite keys and Normal Indexes)",,"Add Views, Functions and Indexes to Query Editor schema sidebar $end$ Hi Team,

We are unable to see the below objects in the query editor.

Currently we are seeing only Tables and Procedures, so we need the below objects along with them.

1. Views
2. Functions
3. Triggers
4. Indexes (Primar key, Unique Key, Composite keys and Normal Indexes) $acceptance criteria:$",,Naresh Chandra,Naresh Chandra,Major,16,,0,4,0,1,0,5,0,,0,850,3,0,0,2022-10-24 07:11:49,"Query Editor Unable See The Views, Functions and Triggers","Hi Team,

We are unable to see the below objects in the query editor.

Currently we are seeing only Tables and Procedures, so we need the below objects along with them.

1. Views
2. Functions
3. Triggers",,3,2,0,24,0.428571,"Query Editor Unable See The Views, Functions and Triggers $end$ Hi Team,

We are unable to see the below objects in the query editor.

Currently we are seeing only Tables and Procedures, so we need the below objects along with them.

1. Views
2. Functions
3. Triggers $acceptance criteria:$",5,1,1,1,1,1,1,5947.93,2,2,1.0,2,1.0,2,1.0,1,0.5,0,0.0
1711,MXS-4020,Task,MXS,2022-02-22 12:31:31,,0,Change the label for Setting a Server to Maintenance Mode,"Currently, hovering on the pause icon shows the ""Maintain server"" tooltip. In the dialog,
the title is also ""Maintain server"".
To be more accurate, the label should be ""Set maintenance mode"", then the button for
confirming is changed from ""Maintain"" to ""Set""

""Maintain server"" ->  ""Set maintenance mode""
""Maintain"" -> ""Set""",,"Change the label for Setting a Server to Maintenance Mode $end$ Currently, hovering on the pause icon shows the ""Maintain server"" tooltip. In the dialog,
the title is also ""Maintain server"".
To be more accurate, the label should be ""Set maintenance mode"", then the button for
confirming is changed from ""Maintain"" to ""Set""

""Maintain server"" ->  ""Set maintenance mode""
""Maintain"" -> ""Set"" $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-02-22 14:20:03,Change the label for Setting a Server to Maintenance Mode,"Currently, hovering on the pause icon shows the ""Maintain server"" tooltip. In the dialog,
the title is also ""Maintain server"".
To be more accurate, the label should be ""Set maintenance mode"", then the button for
confirming is changed from ""Maintain"" to ""Set""

""Maintain server"" ->  ""Set maintenance mode""
""Maintain"" -> ""Set""",,0,0,0,0,0.0,"Change the label for Setting a Server to Maintenance Mode $end$ Currently, hovering on the pause icon shows the ""Maintain server"" tooltip. In the dialog,
the title is also ""Maintain server"".
To be more accurate, the label should be ""Set maintenance mode"", then the button for
confirming is changed from ""Maintain"" to ""Set""

""Maintain server"" ->  ""Set maintenance mode""
""Maintain"" -> ""Set"" $acceptance criteria:$",0,0,0,0,0,0,0,1.8,81,29,0.358025,17,0.209877,13,0.160494,10,0.123457,10,0.123457
1712,MXS-4025,New Feature,MXS,2022-02-23 08:54:18,,0,Multiple query tabs on the same worksheet,"Implement number of tabs within the Worksheet like SQL yog. So that we can run multiple query's from multiple tabs for one worksheet. Otherwise we have to open the multiple browser tabs to connect one DB.

 !image-2022-02-23-14-24-13-799.png|thumbnail! ",,"Multiple query tabs on the same worksheet $end$ Implement number of tabs within the Worksheet like SQL yog. So that we can run multiple query's from multiple tabs for one worksheet. Otherwise we have to open the multiple browser tabs to connect one DB.

 !image-2022-02-23-14-24-13-799.png|thumbnail!  $acceptance criteria:$",,Naresh Chandra,Naresh Chandra,Major,20,,0,3,2,2,0,2,1,,0,850,3,0,0,2022-05-09 18:06:07,Within Worksheet We Need Multiple Query Editors Tabs To Run Different Queries On the Same Work Sheet,"Implement number of tabs within the Worksheet like SQL yog. So that we can run multiple query's from multiple tabs for one worksheet. Otherwise we have top open the multiple browser tabs to connect one DB.

 !image-2022-02-23-14-24-13-799.png|thumbnail! ",,1,1,0,22,0.280702,"Within Worksheet We Need Multiple Query Editors Tabs To Run Different Queries On the Same Work Sheet $end$ Implement number of tabs within the Worksheet like SQL yog. So that we can run multiple query's from multiple tabs for one worksheet. Otherwise we have top open the multiple browser tabs to connect one DB.

 !image-2022-02-23-14-24-13-799.png|thumbnail!  $acceptance criteria:$",2,1,1,1,1,1,1,1809.18,3,3,1.0,3,1.0,3,1.0,2,0.666667,1,0.333333
1713,MXS-4036,Task,MXS,2022-03-02 11:57:46,,0,Test MaxScale multiplexing/pooling against ProxySql,,,Test MaxScale multiplexing/pooling against ProxySql $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-03-02 11:57:46,Test MaxScale multiplexing/pooling against ProxySql,,,0,0,0,0,0.0,Test MaxScale multiplexing/pooling against ProxySql $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,470,43,0.0914894,19,0.0404255,13,0.0276596,9,0.0191489,9,0.0191489
1714,MXS-4037,Task,MXS,2022-03-02 11:58:59,,0,Cleanup MariaDB protocol packet reading,,,Cleanup MariaDB protocol packet reading $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,12,,0,0,0,6,0,2,0,,0,850,0,0,0,2022-03-02 11:58:59,Cleanup the protocol packet reading,,,2,0,0,2,0.125,Cleanup the protocol packet reading $end$ $acceptance criteria:$,2,1,0,0,0,0,1,0.0,471,43,0.0912951,19,0.0403397,13,0.0276008,9,0.0191083,9,0.0191083
1715,MXS-4041,New Feature,MXS,2022-03-10 05:34:32,,0,Support reloading of the REST API TLS certificates,"With MXS-3982 fixed for listeners and servers, the only remaining part is the REST API whose certificates can only be reloaded by restarting MaxScale. The {{maxctrl reload tls}} command, added for version 7, could also reload the TLS certificates of the REST API or at least schedule for it to take place.

The hardest part will most likely be the fact that we don't directly control the network layer of the HTTP daemon as libmicrohttpd handles that. There are also some concurrency issues if the websocket interface is being used to stream the logs.",,"Support reloading of the REST API TLS certificates $end$ With MXS-3982 fixed for listeners and servers, the only remaining part is the REST API whose certificates can only be reloaded by restarting MaxScale. The {{maxctrl reload tls}} command, added for version 7, could also reload the TLS certificates of the REST API or at least schedule for it to take place.

The hardest part will most likely be the fact that we don't directly control the network layer of the HTTP daemon as libmicrohttpd handles that. There are also some concurrency issues if the websocket interface is being used to stream the logs. $acceptance criteria:$",,markus makela,markus makela,Major,10,,0,2,2,1,0,0,0,,0,850,2,0,0,2022-04-25 06:21:30,Support reloading of the REST API TLS certificates,"With MXS-3982 fixed for listeners and servers, the only remaining part is the REST API whose certificates can only be reloaded by restarting MaxScale. The {{maxctrl reload tls}} command, added for version 7, could also reload the TLS certificates of the REST API or at least schedule for it to take place.

The hardest part will most likely be the fact that we don't directly control the network layer of the HTTP daemon as libmicrohttpd handles that. There are also some concurrency issues if the websocket interface is being used to stream the logs.",,0,0,0,0,0.0,"Support reloading of the REST API TLS certificates $end$ With MXS-3982 fixed for listeners and servers, the only remaining part is the REST API whose certificates can only be reloaded by restarting MaxScale. The {{maxctrl reload tls}} command, added for version 7, could also reload the TLS certificates of the REST API or at least schedule for it to take place.

The hardest part will most likely be the fact that we don't directly control the network layer of the HTTP daemon as libmicrohttpd handles that. There are also some concurrency issues if the websocket interface is being used to stream the logs. $acceptance criteria:$",0,0,0,0,0,0,0,1104.77,128,16,0.125,11,0.0859375,9,0.0703125,8,0.0625,7,0.0546875
1716,MXS-4044,New Feature,MXS,2022-03-14 04:10:55,,0,Log a message whenever a server is set into maintenance mode.,"Currently after enabling* log_info=1* under [maxscale] section only display information about maintenance mode. 

Command : 

{code}
maxctrl set server vmc_c maintenance
maxctrl clear server vmc_c maintenance

{code}


Output :- 

{code}
2022-03-10 23:40:53   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/set
2022-03-10 23:41:12   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/clear
2022-03-10 23:41:18   info   : Accept authentication from 'admin', using password. Request: /v1/servers

{code}

This can be changed to default without enabling log_info=1  for  exact message to be logged as it doesn't tell what state the server was put into.",,"Log a message whenever a server is set into maintenance mode. $end$ Currently after enabling* log_info=1* under [maxscale] section only display information about maintenance mode. 

Command : 

{code}
maxctrl set server vmc_c maintenance
maxctrl clear server vmc_c maintenance

{code}


Output :- 

{code}
2022-03-10 23:40:53   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/set
2022-03-10 23:41:12   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/clear
2022-03-10 23:41:18   info   : Accept authentication from 'admin', using password. Request: /v1/servers

{code}

This can be changed to default without enabling log_info=1  for  exact message to be logged as it doesn't tell what state the server was put into. $acceptance criteria:$",,Pramod Mahto,Pramod Mahto,Minor,15,,0,0,0,1,0,2,0,,0,850,0,0,0,2022-09-12 10:12:25, Log a message whenever a server is set into maintenance mode. ,"Currently after enabling* log_info=1* under [maxscale] section only display information about maintenance mode. 

Command : 

{code}
maxctrl set server vmc_c maintenance
maxctrl clear server vmc_c maintenance

{code}


Output :- 

{code}
2022-03-10 23:40:53   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/set
2022-03-10 23:41:12   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/clear
2022-03-10 23:41:18   info   : Accept authentication from 'admin', using password. Request: /v1/servers

{code}

This can be changed to default without enabling log_info=1  for  exact message to be logged as it doesn't tell what state the server was put into.",,2,0,0,0,0.0," Log a message whenever a server is set into maintenance mode.  $end$ Currently after enabling* log_info=1* under [maxscale] section only display information about maintenance mode. 

Command : 

{code}
maxctrl set server vmc_c maintenance
maxctrl clear server vmc_c maintenance

{code}


Output :- 

{code}
2022-03-10 23:40:53   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/set
2022-03-10 23:41:12   info   : Accept authentication from 'admin', using password. Request: /v1/servers/vmc_c/clear
2022-03-10 23:41:18   info   : Accept authentication from 'admin', using password. Request: /v1/servers

{code}

This can be changed to default without enabling log_info=1  for  exact message to be logged as it doesn't tell what state the server was put into. $acceptance criteria:$",2,0,0,0,0,0,0,4374.02,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1717,MXS-4047,Task,MXS,2022-03-14 11:46:13,,0,Test rebalancing feature,,,Test rebalancing feature $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-03-14 11:46:13,Test rebalancing feature,,,0,0,0,0,0.0,Test rebalancing feature $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,472,44,0.0932203,19,0.0402542,13,0.0275424,9,0.0190678,9,0.0190678
1718,MXS-4048,Task,MXS,2022-03-14 11:48:52,,0,Benchmark develop,,,Benchmark develop $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,13,,0,0,0,6,0,0,0,,0,850,0,0,0,2022-03-14 11:48:52,Benchmark develop,,,0,0,0,0,0.0,Benchmark develop $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,473,44,0.0930233,19,0.0401691,13,0.0274841,9,0.0190275,9,0.0190275
1719,MXS-4054,Task,MXS,2022-03-23 10:55:51,,0,Expose query classifier information using std::string_view.,"Currently the query classifier explicitly allocates memory for storing fields, etc.. If instead, the canonical version of a query were stored then all field information could be made available as {{std::string_view}}, which would not require any memory to be allocated and would be faster.",,"Expose query classifier information using std::string_view. $end$ Currently the query classifier explicitly allocates memory for storing fields, etc.. If instead, the canonical version of a query were stored then all field information could be made available as {{std::string_view}}, which would not require any memory to be allocated and would be faster. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,12,,0,0,0,5,0,0,0,,0,850,0,0,0,2022-03-23 10:55:51,Expose query classifier information using std::string_view.,"Currently the query classifier explicitly allocates memory for storing fields, etc.. If instead, the canonical version of a query were stored then all field information could be made available as {{std::string_view}}, which would not require any memory to be allocated and would be faster.",,0,0,0,0,0.0,"Expose query classifier information using std::string_view. $end$ Currently the query classifier explicitly allocates memory for storing fields, etc.. If instead, the canonical version of a query were stored then all field information could be made available as {{std::string_view}}, which would not require any memory to be allocated and would be faster. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,474,44,0.092827,19,0.0400844,13,0.0274262,9,0.0189873,9,0.0189873
1720,MXS-406,New Feature,MXS,2015-10-14 10:32:14,,0,Galeramon to set the preferred donor nodes,"Galeramon should be able to control which nodes are the preferred as the donor with the wsrep_sst_donor variable. This would allow joining nodes to choose ""slave"" nodes which will allow MaxScale to influence the donor choosing process.

The monitor could execute {{SELECT @@wsrep_node_name;}} on a non-master node of the cluster and then assign the preferred SST donor node with {{SET GLOBAL wsrep_sst_donor=my_donor_node1}}.

{code}
MariaDB [(none)]> select @@wsrep_node_name;
+-----------------------+
| @@wsrep_node_name     |
+-----------------------+
| localhost.localdomain |
+-----------------------+
1 row in set (0.00 sec)

MariaDB [(none)]> set global wsrep_sst_donor = ""localhost.localdomain"";
Query OK, 0 rows affected (0.00 sec)
{code}",,"Galeramon to set the preferred donor nodes $end$ Galeramon should be able to control which nodes are the preferred as the donor with the wsrep_sst_donor variable. This would allow joining nodes to choose ""slave"" nodes which will allow MaxScale to influence the donor choosing process.

The monitor could execute {{SELECT @@wsrep_node_name;}} on a non-master node of the cluster and then assign the preferred SST donor node with {{SET GLOBAL wsrep_sst_donor=my_donor_node1}}.

{code}
MariaDB [(none)]> select @@wsrep_node_name;
+-----------------------+
| @@wsrep_node_name     |
+-----------------------+
| localhost.localdomain |
+-----------------------+
1 row in set (0.00 sec)

MariaDB [(none)]> set global wsrep_sst_donor = ""localhost.localdomain"";
Query OK, 0 rows affected (0.00 sec)
{code} $acceptance criteria:$",,markus makela,markus makela,Major,10,,0,2,0,1,0,2,0,,0,850,1,0,0,2017-01-04 11:08:59,Galeramon to set the preferred donor nodes,"Galeramon should be able to control which nodes are the preferred as the donor with the wsrep_sst_donor variable. This would allow joining nodes to choose ""slave"" nodes which will allow MaxScale to influence the donor choosing process.",,0,2,0,60,1.2766,"Galeramon to set the preferred donor nodes $end$ Galeramon should be able to control which nodes are the preferred as the donor with the wsrep_sst_donor variable. This would allow joining nodes to choose ""slave"" nodes which will allow MaxScale to influence the donor choosing process. $acceptance criteria:$",2,1,1,1,1,1,1,10752.6,4,2,0.5,2,0.5,2,0.5,2,0.5,2,0.5
1721,MXS-4060,Task,MXS,2022-03-24 07:47:18,,0,Add support for loop-calls,"Currently the _delayed call_ mechanism that is intended for periodic callbacks to be called after a certain longish (milliseconds) delay, is also used for callbacks that merely needs to be called via the event loop. For this purpose the delayed call mechanism is quite inefficient as it always introduced a delay of at least 1ms. A separate mechanism is needed for callbacks to be called as quickly as possible via the event loop.",,"Add support for loop-calls $end$ Currently the _delayed call_ mechanism that is intended for periodic callbacks to be called after a certain longish (milliseconds) delay, is also used for callbacks that merely needs to be called via the event loop. For this purpose the delayed call mechanism is quite inefficient as it always introduced a delay of at least 1ms. A separate mechanism is needed for callbacks to be called as quickly as possible via the event loop. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-03-24 07:47:18,Add support for loop-calls,"Currently the _delayed call_ mechanism that is intended for periodic callbacks to be called after a certain longish (milliseconds) delay, is also used for callbacks that merely needs to be called via the event loop. For this purpose the delayed call mechanism is quite inefficient as it always introduced a delay of at least 1ms. A separate mechanism is needed for callbacks to be called as quickly as possible via the event loop.",,0,0,0,0,0.0,"Add support for loop-calls $end$ Currently the _delayed call_ mechanism that is intended for periodic callbacks to be called after a certain longish (milliseconds) delay, is also used for callbacks that merely needs to be called via the event loop. For this purpose the delayed call mechanism is quite inefficient as it always introduced a delay of at least 1ms. A separate mechanism is needed for callbacks to be called as quickly as possible via the event loop. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,475,44,0.0926316,19,0.04,13,0.0273684,9,0.0189474,9,0.0189474
1722,MXS-4063,Task,MXS,2022-03-25 08:58:57,,0,Add support for limiting the amount of data read.,"MaxScale uses epoll together with non-blocking socket descriptors. When MaxScale is notified there is something to read on a socket, it will read and handle the data until EAGAIN is returned. That is necessary as the sockets are added to epoll in edge-triggered mode. Now, if a client is capable of writing so fast that MaxScale never is able to empty the socket, then that client will be able to monopolize the thread handling that particular socket. Thus, it should be possible to not ready everything before returning to epoll and checking for sockets ready to be read.",,"Add support for limiting the amount of data read. $end$ MaxScale uses epoll together with non-blocking socket descriptors. When MaxScale is notified there is something to read on a socket, it will read and handle the data until EAGAIN is returned. That is necessary as the sockets are added to epoll in edge-triggered mode. Now, if a client is capable of writing so fast that MaxScale never is able to empty the socket, then that client will be able to monopolize the thread handling that particular socket. Thus, it should be possible to not ready everything before returning to epoll and checking for sockets ready to be read. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,4,0,0,0,,0,850,0,0,0,2022-03-25 08:58:57,Add support for limiting the amount of data read.,"MaxScale uses epoll together with non-blocking socket descriptors. When MaxScale is notified there is something to read on a socket, it will read and handle the data until EAGAIN is returned. That is necessary as the sockets are added to epoll in edge-triggered mode. Now, if a client is capable of writing so fast that MaxScale never is able to empty the socket, then that client will be able to monopolize the thread handling that particular socket. Thus, it should be possible to not ready everything before returning to epoll and checking for sockets ready to be read.",,0,0,0,0,0.0,"Add support for limiting the amount of data read. $end$ MaxScale uses epoll together with non-blocking socket descriptors. When MaxScale is notified there is something to read on a socket, it will read and handle the data until EAGAIN is returned. That is necessary as the sockets are added to epoll in edge-triggered mode. Now, if a client is capable of writing so fast that MaxScale never is able to empty the socket, then that client will be able to monopolize the thread handling that particular socket. Thus, it should be possible to not ready everything before returning to epoll and checking for sockets ready to be read. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,476,44,0.092437,19,0.039916,13,0.0273109,9,0.0189076,9,0.0189076
1723,MXS-4066,Task,MXS,2022-03-28 10:39:06,,0,Test new pooling mechanism,,,Test new pooling mechanism $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-03-28 10:39:06,Test new pooling mechanism,,,0,0,0,0,0.0,Test new pooling mechanism $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,477,44,0.0922432,19,0.0398323,13,0.0272537,9,0.0188679,9,0.0188679
1724,MXS-4079,Task,MXS,2022-04-06 05:41:17,,0,Document cooperative monitoring conflict probability,The documentation currently is not clear enough about how the feature works and why complex conflict resolution strategies are not needed. Explaning that the probability of a conflict is small due to the randomness involved in the lock acquisition goes a long way to help the users understand why MaxScale behaves the way it does.,,Document cooperative monitoring conflict probability $end$ The documentation currently is not clear enough about how the feature works and why complex conflict resolution strategies are not needed. Explaning that the probability of a conflict is small due to the randomness involved in the lock acquisition goes a long way to help the users understand why MaxScale behaves the way it does. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,1,1,1,0,0,0,,0,850,1,0,0,2022-04-25 10:34:10,Document cooperative monitoring conflict probability,The documentation currently is not clear enough about how the feature works and why complex conflict resolution strategies are not needed. Explaning that the probability of a conflict is small due to the randomness involved in the lock acquisition goes a long way to help the users understand why MaxScale behaves the way it does.,,0,0,0,0,0.0,Document cooperative monitoring conflict probability $end$ The documentation currently is not clear enough about how the feature works and why complex conflict resolution strategies are not needed. Explaning that the probability of a conflict is small due to the randomness involved in the lock acquisition goes a long way to help the users understand why MaxScale behaves the way it does. $acceptance criteria:$,0,0,0,0,0,0,0,460.867,129,16,0.124031,11,0.0852713,9,0.0697674,8,0.0620155,7,0.0542636
1725,MXS-4081,Task,MXS,2022-04-06 13:48:03,,0,Use ESLint for MaxCtrl,,,Use ESLint for MaxCtrl $end$ $acceptance criteria:$,,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-04-25 06:21:33,Use ESLint for MaxCtrl,,,0,0,0,0,0.0,Use ESLint for MaxCtrl $end$ $acceptance criteria:$,0,0,0,0,0,0,0,448.55,130,16,0.123077,11,0.0846154,9,0.0692308,8,0.0615385,7,0.0538462
1726,MXS-4087,New Feature,MXS,2022-04-08 10:33:03,,0,Kill session in MaxGUI,"{code:java}
DELETE /v1/sessions/:id
{code}
The above endpoint is now available. The GUI should add a delete button in the
sessions table to kill it.",,"Kill session in MaxGUI $end$ {code:java}
DELETE /v1/sessions/:id
{code}
The above endpoint is now available. The GUI should add a delete button in the
sessions table to kill it. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,12,,0,0,1,1,0,2,0,,0,850,0,0,0,2022-04-25 06:24:47,Kill session via  MaxGUI,"{code:java}
DELETE /v1/sessions/:id
{code}
The above endpoint is now available. The GUI should add a delete button in the
sessions table to kill it.",,2,0,0,2,0.0322581,"Kill session via  MaxGUI $end$ {code:java}
DELETE /v1/sessions/:id
{code}
The above endpoint is now available. The GUI should add a delete button in the
sessions table to kill it. $acceptance criteria:$",2,1,0,0,0,0,0,403.85,82,29,0.353659,17,0.207317,13,0.158537,10,0.121951,10,0.121951
1727,MXS-410,Task,MXS,2015-10-16 04:08:11,,0,need doc on how to write and install custom plugin,"Need doc on how to write and install custom plugin.

It would be nice to have a few examples.",,"need doc on how to write and install custom plugin $end$ Need doc on how to write and install custom plugin.

It would be nice to have a few examples. $acceptance criteria:$",,Rick James,Rick James,Minor,11,,0,1,0,2,0,0,0,,0,850,0,0,0,2017-02-01 10:35:58,need doc on how to write and install custom plugin,"Need doc on how to write and install custom plugin.

It would be nice to have a few examples.",,0,0,0,0,0.0,"need doc on how to write and install custom plugin $end$ Need doc on how to write and install custom plugin.

It would be nice to have a few examples. $acceptance criteria:$",0,0,0,0,0,0,1,11382.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1728,MXS-4104,Task,MXS,2022-04-20 13:03:40,,0,Rename to new version number,Change all references to MaxScale 7 to Maxscale 22.08 where needed. At the same time all old references to MaxScale 2.6 could also be changed to Maxscale 6.,,Rename to new version number $end$ Change all references to MaxScale 7 to Maxscale 22.08 where needed. At the same time all old references to MaxScale 2.6 could also be changed to Maxscale 6. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-04-25 06:21:44,Rename to new version number,Change all references to MaxScale 7 to Maxscale 22.08 where needed. At the same time all old references to MaxScale 2.6 could also be changed to Maxscale 6.,,0,0,0,0,0.0,Rename to new version number $end$ Change all references to MaxScale 7 to Maxscale 22.08 where needed. At the same time all old references to MaxScale 2.6 could also be changed to Maxscale 6. $acceptance criteria:$,0,0,0,0,0,0,0,113.3,131,16,0.122137,11,0.0839695,9,0.0687023,8,0.0610687,7,0.0534351
1729,MXS-4106,New Feature,MXS,2022-04-21 06:19:22,,0,Feature request: support for Redis password AUTH,"Feature request: add support for the simple password authentication feature provided by Redis

see also:

https://redis.io/docs/manual/security/#authentication
https://redis.io/commands/auth/",,"Feature request: support for Redis password AUTH $end$ Feature request: add support for the simple password authentication feature provided by Redis

see also:

https://redis.io/docs/manual/security/#authentication
https://redis.io/commands/auth/ $acceptance criteria:$",,Hartmut Holzgraefe,Hartmut Holzgraefe,Major,15,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-11-21 10:28:35,Feature request: support for Redis password AUTH,"Feature request: add support for the simple password authentication feature provided by Redis

see also:

https://redis.io/docs/manual/security/#authentication
https://redis.io/commands/auth/",,0,0,0,0,0.0,"Feature request: support for Redis password AUTH $end$ Feature request: add support for the simple password authentication feature provided by Redis

see also:

https://redis.io/docs/manual/security/#authentication
https://redis.io/commands/auth/ $acceptance criteria:$",0,0,0,0,0,0,1,5140.15,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1730,MXS-4107,New Feature,MXS,2022-04-21 06:23:10,,0,Feature request: support TLS encrypted Redis connections,"Feature request: add support for TLS encrypted Redis connections

With being able to use data-in-transit encryption between MariaDB Server, Maxscale and clients (and between Galera nodes) we should also be able to encrypt communication with the cache backend as this basically has the same information going over the wire as we have between Server/Maxscale and actual clients.

See also: https://redis.io/docs/manual/security/encryption/",,"Feature request: support TLS encrypted Redis connections $end$ Feature request: add support for TLS encrypted Redis connections

With being able to use data-in-transit encryption between MariaDB Server, Maxscale and clients (and between Galera nodes) we should also be able to encrypt communication with the cache backend as this basically has the same information going over the wire as we have between Server/Maxscale and actual clients.

See also: https://redis.io/docs/manual/security/encryption/ $acceptance criteria:$",,Hartmut Holzgraefe,Hartmut Holzgraefe,Major,11,,0,1,0,1,0,0,0,,0,850,1,0,0,2022-12-15 10:18:08,Feature request: support TLS encrypted Redis connections,"Feature request: add support for TLS encrypted Redis connections

With being able to use data-in-transit encryption between MariaDB Server, Maxscale and clients (and between Galera nodes) we should also be able to encrypt communication with the cache backend as this basically has the same information going over the wire as we have between Server/Maxscale and actual clients.

See also: https://redis.io/docs/manual/security/encryption/",,0,0,0,0,0.0,"Feature request: support TLS encrypted Redis connections $end$ Feature request: add support for TLS encrypted Redis connections

With being able to use data-in-transit encryption between MariaDB Server, Maxscale and clients (and between Galera nodes) we should also be able to encrypt communication with the cache backend as this basically has the same information going over the wire as we have between Server/Maxscale and actual clients.

See also: https://redis.io/docs/manual/security/encryption/ $acceptance criteria:$",0,0,0,0,0,0,0,5715.9,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1731,MXS-4117,Task,MXS,2022-04-29 07:53:43,,0,Modulize Query Editor query module state,"The query module state needs to be split into smaller modules so that new features can be implemented and maintained easier. 
This is required to make query tabs in a worksheet, so it blocks [MXS-4025|https://jira.mariadb.org/browse/MXS-4025].
In addition, to save and load .sql files in IDE, it's technically possible to implement it, but from the UX point of view,  with the current design, when loading a file to the editor,  there is no good way to show the name of the loaded file. By having a query tab inside a worksheet and using the loaded file name as that query tab name, it explicitly tells which file is loaded. So then MXS-4025 blocks  [MXS-3723|https://jira.mariadb.org/browse/MXS-3723].",,"Modulize Query Editor query module state $end$ The query module state needs to be split into smaller modules so that new features can be implemented and maintained easier. 
This is required to make query tabs in a worksheet, so it blocks [MXS-4025|https://jira.mariadb.org/browse/MXS-4025].
In addition, to save and load .sql files in IDE, it's technically possible to implement it, but from the UX point of view,  with the current design, when loading a file to the editor,  there is no good way to show the name of the loaded file. By having a query tab inside a worksheet and using the loaded file name as that query tab name, it explicitly tells which file is loaded. So then MXS-4025 blocks  [MXS-3723|https://jira.mariadb.org/browse/MXS-3723]. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,13,,0,3,2,1,0,1,0,,0,850,3,0,0,2022-04-29 08:23:11,Refactor Query Editor query module state,"The query module state needs to be split into smaller modules so that new features can be implemented and maintained easier. 
This is required to make query tabs in a worksheet, so it blocks [MXS-4025|https://jira.mariadb.org/browse/MXS-4025].
In addition, to save and load .sql files in IDE, it's technically possible to implement it, but from the UX point of view,  with the current design, when loading a file to the editor,  there is no good way to show the name of the loaded file. By having a query tab inside a worksheet and using the loaded file name as that query tab name, it explicitly tells which file is loaded. So then MXS-4025 blocks  [MXS-3723|https://jira.mariadb.org/browse/MXS-3723].",,1,0,0,2,0.00819672,"Refactor Query Editor query module state $end$ The query module state needs to be split into smaller modules so that new features can be implemented and maintained easier. 
This is required to make query tabs in a worksheet, so it blocks [MXS-4025|https://jira.mariadb.org/browse/MXS-4025].
In addition, to save and load .sql files in IDE, it's technically possible to implement it, but from the UX point of view,  with the current design, when loading a file to the editor,  there is no good way to show the name of the loaded file. By having a query tab inside a worksheet and using the loaded file name as that query tab name, it explicitly tells which file is loaded. So then MXS-4025 blocks  [MXS-3723|https://jira.mariadb.org/browse/MXS-3723]. $acceptance criteria:$",1,1,0,0,0,0,0,0.483333,83,30,0.361446,17,0.204819,13,0.156627,10,0.120482,10,0.120482
1732,MXS-4119,Sub-Task,MXS,2022-05-04 10:00:37,,0,Add querySession state module,"Once [MXS-4117|https://jira.mariadb.org/browse/MXS-4117] is done,  the current state structure can be visualized as this  
!query_editor_states_diagram.png|thumbnail! 
To support multiple query tabs in a worksheet, it needs to be refactored to this 
!query_editor_with_session_module_diagram.png|thumbnail!  
or this 
 !query_editor_with_session_module_diagram_v2.png|thumbnail! 
",,"Add querySession state module $end$ Once [MXS-4117|https://jira.mariadb.org/browse/MXS-4117] is done,  the current state structure can be visualized as this  
!query_editor_states_diagram.png|thumbnail! 
To support multiple query tabs in a worksheet, it needs to be refactored to this 
!query_editor_with_session_module_diagram.png|thumbnail!  
or this 
 !query_editor_with_session_module_diagram_v2.png|thumbnail! 
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,11,,0,0,0,2,0,4,0,,0,850,0,0,0,2022-05-04 10:00:37,Add querySession state module,"Once [MXS-4117|https://jira.mariadb.org/browse/MXS-4117] is done,  the current state structure can be visualized as it's illustrated in  !query_editor_states_diagram.png|thumbnail! 
To support multiple query tabs in a worksheet, it needs to be refactored to this  !query_editor_with_session_module_diagram.png|thumbnail! ",,0,4,0,7,0.153846,"Add querySession state module $end$ Once [MXS-4117|https://jira.mariadb.org/browse/MXS-4117] is done,  the current state structure can be visualized as it's illustrated in  !query_editor_states_diagram.png|thumbnail! 
To support multiple query tabs in a worksheet, it needs to be refactored to this  !query_editor_with_session_module_diagram.png|thumbnail!  $acceptance criteria:$",4,1,1,0,0,0,1,0.0,84,31,0.369048,17,0.202381,13,0.154762,10,0.119048,10,0.119048
1733,MXS-4122,New Feature,MXS,2022-05-09 07:49:01,,0,Fast Global causal_reads,"Request to implement a `causal_reads=fast_global` to have global causal reads scope but no wait for the timeout before routing the SELECT to the primary node.

The use-case covers the majority of cases where the user would want to have a fast query response globally without the wait for replication synchronization. ",,"Fast Global causal_reads $end$ Request to implement a `causal_reads=fast_global` to have global causal reads scope but no wait for the timeout before routing the SELECT to the primary node.

The use-case covers the majority of cases where the user would want to have a fast query response globally without the wait for replication synchronization.  $acceptance criteria:$",,Faisal Saeed,Faisal Saeed,Major,12,,0,1,1,1,0,0,0,,0,850,0,0,0,2022-09-12 10:07:56,Fast Global causal_reads,"Request to implement a `causal_reads=fast_global` to have global causal reads scope but no wait for the timeout before routing the SELECT to the primary node.

The use-case covers the majority of cases where the user would want to have a fast query response globally without the wait for replication synchronization. ",,0,0,0,0,0.0,"Fast Global causal_reads $end$ Request to implement a `causal_reads=fast_global` to have global causal reads scope but no wait for the timeout before routing the SELECT to the primary node.

The use-case covers the majority of cases where the user would want to have a fast query response globally without the wait for replication synchronization.  $acceptance criteria:$",0,0,0,0,0,0,0,3026.3,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1734,MXS-4129,Sub-Task,MXS,2022-05-11 05:30:35,,0,Add encryption to binlogrouter,"Add encryption of binlog events to the binlogrouter. A similar approach can be taken that was used with MaxScale 2.4  where the following steps were taken to encrypt an event:
* Store the real event length in memory
* Move the 4 byte timestamp into the 4 byte event length field
* Encrypt the data, excluding the 4 bytes from the start of the file that contain the timestamp (now stored where the length is stored)
* Move the 4 bytes of encrypted data stored at the event length field into the 4 bytes where the timestamp is stored
* Store the encrypted event length in the 4 byte event length field ",,"Add encryption to binlogrouter $end$ Add encryption of binlog events to the binlogrouter. A similar approach can be taken that was used with MaxScale 2.4  where the following steps were taken to encrypt an event:
* Store the real event length in memory
* Move the 4 byte timestamp into the 4 byte event length field
* Encrypt the data, excluding the 4 bytes from the start of the file that contain the timestamp (now stored where the length is stored)
* Move the 4 bytes of encrypted data stored at the event length field into the 4 bytes where the timestamp is stored
* Store the encrypted event length in the 4 byte event length field  $acceptance criteria:$",,markus makela,markus makela,Major,4,,0,0,0,6,0,0,0,,0,850,0,0,0,2022-05-11 05:30:35,Add encryption to binlogrouter,"Add encryption of binlog events to the binlogrouter. A similar approach can be taken that was used with MaxScale 2.4  where the following steps were taken to encrypt an event:
* Store the real event length in memory
* Move the 4 byte timestamp into the 4 byte event length field
* Encrypt the data, excluding the 4 bytes from the start of the file that contain the timestamp (now stored where the length is stored)
* Move the 4 bytes of encrypted data stored at the event length field into the 4 bytes where the timestamp is stored
* Store the encrypted event length in the 4 byte event length field ",,0,0,0,0,0.0,"Add encryption to binlogrouter $end$ Add encryption of binlog events to the binlogrouter. A similar approach can be taken that was used with MaxScale 2.4  where the following steps were taken to encrypt an event:
* Store the real event length in memory
* Move the 4 byte timestamp into the 4 byte event length field
* Encrypt the data, excluding the 4 bytes from the start of the file that contain the timestamp (now stored where the length is stored)
* Move the 4 bytes of encrypted data stored at the event length field into the 4 bytes where the timestamp is stored
* Store the encrypted event length in the 4 byte event length field  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,132,16,0.121212,11,0.0833333,9,0.0681818,8,0.0606061,7,0.0530303
1735,MXS-4130,Sub-Task,MXS,2022-05-11 05:33:42,,0,Create a generic encryption facility,"Currently the only encryption that is done in MaxScale is the encryption of passwords. As the code used for it contains most of what is required to encrypt the binlog data, it can be reused and made slightly more generic. The things that are needed are the selection of the encryption ciper and the key file and IV.",,"Create a generic encryption facility $end$ Currently the only encryption that is done in MaxScale is the encryption of passwords. As the code used for it contains most of what is required to encrypt the binlog data, it can be reused and made slightly more generic. The things that are needed are the selection of the encryption ciper and the key file and IV. $acceptance criteria:$",,markus makela,markus makela,Major,4,,0,0,0,6,0,0,0,,0,850,0,0,0,2022-05-11 05:33:42,Create a generic encryption facility,"Currently the only encryption that is done in MaxScale is the encryption of passwords. As the code used for it contains most of what is required to encrypt the binlog data, it can be reused and made slightly more generic. The things that are needed are the selection of the encryption ciper and the key file and IV.",,0,0,0,0,0.0,"Create a generic encryption facility $end$ Currently the only encryption that is done in MaxScale is the encryption of passwords. As the code used for it contains most of what is required to encrypt the binlog data, it can be reused and made slightly more generic. The things that are needed are the selection of the encryption ciper and the key file and IV. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,133,16,0.120301,11,0.0827068,9,0.0676692,8,0.0601504,7,0.0526316
1736,MXS-4131,Sub-Task,MXS,2022-05-11 05:41:07,,0,Create a generic key management facility,"Storing encryption keys on the same filesystem where the encryped data is located is not the most secure way of handling things but above all it is not a convenient way to enforce policies on key rotation or expiration. Making the key retrieval process generic enough that it can be extended to support multiple providers would make implementing them easier. For starters, the existing file-based encryption is the only one that is needed to verify that the encryption in the binlogrouter works. ",,"Create a generic key management facility $end$ Storing encryption keys on the same filesystem where the encryped data is located is not the most secure way of handling things but above all it is not a convenient way to enforce policies on key rotation or expiration. Making the key retrieval process generic enough that it can be extended to support multiple providers would make implementing them easier. For starters, the existing file-based encryption is the only one that is needed to verify that the encryption in the binlogrouter works.  $acceptance criteria:$",,markus makela,markus makela,Major,4,,0,0,0,6,0,0,0,,0,850,0,0,0,2022-05-11 05:41:07,Create a generic key management facility,"Storing encryption keys on the same filesystem where the encryped data is located is not the most secure way of handling things but above all it is not a convenient way to enforce policies on key rotation or expiration. Making the key retrieval process generic enough that it can be extended to support multiple providers would make implementing them easier. For starters, the existing file-based encryption is the only one that is needed to verify that the encryption in the binlogrouter works. ",,0,0,0,0,0.0,"Create a generic key management facility $end$ Storing encryption keys on the same filesystem where the encryped data is located is not the most secure way of handling things but above all it is not a convenient way to enforce policies on key rotation or expiration. Making the key retrieval process generic enough that it can be extended to support multiple providers would make implementing them easier. For starters, the existing file-based encryption is the only one that is needed to verify that the encryption in the binlogrouter works.  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,134,16,0.119403,11,0.0820896,9,0.0671642,8,0.0597015,7,0.0522388
1737,MXS-4145,New Feature,MXS,2022-05-23 07:46:27,MXS-3387,0,Add support for multi-MaxScale usage.,"The authentication mechanisms used by NoSQL and MariaDB are sufficiently dissimilar that {{nosqlprotocol}} needs direct access to the SHA1 password of the client, to be able to log into MariaDB on behalf of the it.

Currently, the SHA1 password is stored in a local sqlite3 database on the MaxScale host. This presents a problem when multiple MaxScale instances are used in front of the same database cluster, as a NoSQL user created via one MaxScale instance is not available on the other.

This problem can be solved by storing the SHA1 password in a table in the MariaDB server/cluster. That way, irrespective of which MaxScale instance a NoSQL user was created on, it would immediately also be available on the other.

As anyone with access to that table would be able to impersonate every user in that table, the SHA1 password should be encrypted using a key available only to the MaxScale instances, e.g. by specifying the encryption key in the MaxScale configuration file. That way, the setup would be just as secure/insecure as the current sqlite3 arrangement.",,"Add support for multi-MaxScale usage. $end$ The authentication mechanisms used by NoSQL and MariaDB are sufficiently dissimilar that {{nosqlprotocol}} needs direct access to the SHA1 password of the client, to be able to log into MariaDB on behalf of the it.

Currently, the SHA1 password is stored in a local sqlite3 database on the MaxScale host. This presents a problem when multiple MaxScale instances are used in front of the same database cluster, as a NoSQL user created via one MaxScale instance is not available on the other.

This problem can be solved by storing the SHA1 password in a table in the MariaDB server/cluster. That way, irrespective of which MaxScale instance a NoSQL user was created on, it would immediately also be available on the other.

As anyone with access to that table would be able to impersonate every user in that table, the SHA1 password should be encrypted using a key available only to the MaxScale instances, e.g. by specifying the encryption key in the MaxScale configuration file. That way, the setup would be just as secure/insecure as the current sqlite3 arrangement. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-06-13 09:56:55,Add support for multi-MaxScale usage.,"The authentication mechanisms used by NoSQL and MariaDB are sufficiently dissimilar that {{nosqlprotocol}} needs direct access to the SHA1 password of the client, to be able to log into MariaDB on behalf of the it.

Currently, the SHA1 password is stored in a local sqlite3 database on the MaxScale host. This presents a problem when multiple MaxScale instances are used in front of the same database cluster, as a NoSQL user created via one MaxScale instance is not available on the other.

This problem can be solved by storing the SHA1 password in a table in the MariaDB server/cluster. That way, irrespective of which MaxScale instance a NoSQL user was created on, it would immediately also be available on the other.

As anyone with access to that table would be able to impersonate every user in that table, the SHA1 password should be encrypted using a key available only to the MaxScale instances, e.g. by specifying the encryption key in the MaxScale configuration file. That way, the setup would be just as secure/insecure as the current sqlite3 arrangement.",,0,0,0,0,0.0,"Add support for multi-MaxScale usage. $end$ The authentication mechanisms used by NoSQL and MariaDB are sufficiently dissimilar that {{nosqlprotocol}} needs direct access to the SHA1 password of the client, to be able to log into MariaDB on behalf of the it.

Currently, the SHA1 password is stored in a local sqlite3 database on the MaxScale host. This presents a problem when multiple MaxScale instances are used in front of the same database cluster, as a NoSQL user created via one MaxScale instance is not available on the other.

This problem can be solved by storing the SHA1 password in a table in the MariaDB server/cluster. That way, irrespective of which MaxScale instance a NoSQL user was created on, it would immediately also be available on the other.

As anyone with access to that table would be able to impersonate every user in that table, the SHA1 password should be encrypted using a key available only to the MaxScale instances, e.g. by specifying the encryption key in the MaxScale configuration file. That way, the setup would be just as secure/insecure as the current sqlite3 arrangement. $acceptance criteria:$",0,0,0,0,0,0,1,506.167,478,44,0.0920502,19,0.039749,13,0.0271967,9,0.0188285,9,0.0188285
1738,MXS-4146,Task,MXS,2022-05-24 21:54:57,,0,Xpand MaxScale Tutorial in KB doesn't work,"This was originally reported in [the docs-talk slack channel by Luke Smith|https://mariadb.slack.com/archives/CHAMKU9PG/p1653427736091489].

Luke reported a problem with [this KB documentation page|https://mariadb.com/kb/en/mariadb-maxscale-6-maxscale-and-xpand-tutorial/] about using MaxScale with Xpand, which is generated from [this page in GitHub|https://github.com/mariadb-corporation/MaxScale/blob/6.3/Documentation/Tutorials/MaxScale-Xpand-Tutorial.md].

This document does not list the full set of required privileges.

When a user tries to connect with the privileges listed in the document, they will see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:21:56   error  : Failed to query server '@@Xpand:node-1' for user account info. Query 'SELECT * FROM system.users; SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role); SHOW DATABASES;' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`users`.; transaction aborted.
{noformat}

If the SELECT privilege is granted on the `system`.`users` table, users still see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:23:35   error  : Failed to query server '@@Xpand:node-1' for user account info. Multiquery element 'SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role);' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`user_acl`.; transaction aborted.
{noformat}

If the SELECT privilege is also granted on the `system`.`user_acl` table, then connections are successful.

However, there might be other permissions missing as the ""maxscale"" user still can't use customer databases due to the following error in the MaxScale error log:

{noformat}
2022-05-24 23:48:26   warning: (26) [MariaDBProtocol] Authentication failed for user 'horizonApp'@[10.70.120.51] to service 'Xpand-Service'. Originating listener: 'xpand_listener'. MariaDB error: 'Unknown database 'database_name''.
{noformat}",,"Xpand MaxScale Tutorial in KB doesn't work $end$ This was originally reported in [the docs-talk slack channel by Luke Smith|https://mariadb.slack.com/archives/CHAMKU9PG/p1653427736091489].

Luke reported a problem with [this KB documentation page|https://mariadb.com/kb/en/mariadb-maxscale-6-maxscale-and-xpand-tutorial/] about using MaxScale with Xpand, which is generated from [this page in GitHub|https://github.com/mariadb-corporation/MaxScale/blob/6.3/Documentation/Tutorials/MaxScale-Xpand-Tutorial.md].

This document does not list the full set of required privileges.

When a user tries to connect with the privileges listed in the document, they will see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:21:56   error  : Failed to query server '@@Xpand:node-1' for user account info. Query 'SELECT * FROM system.users; SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role); SHOW DATABASES;' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`users`.; transaction aborted.
{noformat}

If the SELECT privilege is granted on the `system`.`users` table, users still see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:23:35   error  : Failed to query server '@@Xpand:node-1' for user account info. Multiquery element 'SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role);' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`user_acl`.; transaction aborted.
{noformat}

If the SELECT privilege is also granted on the `system`.`user_acl` table, then connections are successful.

However, there might be other permissions missing as the ""maxscale"" user still can't use customer databases due to the following error in the MaxScale error log:

{noformat}
2022-05-24 23:48:26   warning: (26) [MariaDBProtocol] Authentication failed for user 'horizonApp'@[10.70.120.51] to service 'Xpand-Service'. Originating listener: 'xpand_listener'. MariaDB error: 'Unknown database 'database_name''.
{noformat} $acceptance criteria:$",,Anne Strasser,Anne Strasser,Major,11,,0,3,0,1,0,0,0,,0,850,2,0,0,2022-05-30 11:05:25,Xpand MaxScale Tutorial in KB doesn't work,"This was originally reported in [the docs-talk slack channel by Luke Smith|https://mariadb.slack.com/archives/CHAMKU9PG/p1653427736091489].

Luke reported a problem with [this KB documentation page|https://mariadb.com/kb/en/mariadb-maxscale-6-maxscale-and-xpand-tutorial/] about using MaxScale with Xpand, which is generated from [this page in GitHub|https://github.com/mariadb-corporation/MaxScale/blob/6.3/Documentation/Tutorials/MaxScale-Xpand-Tutorial.md].

This document does not list the full set of required privileges.

When a user tries to connect with the privileges listed in the document, they will see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:21:56   error  : Failed to query server '@@Xpand:node-1' for user account info. Query 'SELECT * FROM system.users; SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role); SHOW DATABASES;' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`users`.; transaction aborted.
{noformat}

If the SELECT privilege is granted on the `system`.`users` table, users still see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:23:35   error  : Failed to query server '@@Xpand:node-1' for user account info. Multiquery element 'SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role);' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`user_acl`.; transaction aborted.
{noformat}

If the SELECT privilege is also granted on the `system`.`user_acl` table, then connections are successful.

However, there might be other permissions missing as the ""maxscale"" user still can't use customer databases due to the following error in the MaxScale error log:

{noformat}
2022-05-24 23:48:26   warning: (26) [MariaDBProtocol] Authentication failed for user 'horizonApp'@[10.70.120.51] to service 'Xpand-Service'. Originating listener: 'xpand_listener'. MariaDB error: 'Unknown database 'database_name''.
{noformat}",,0,0,0,0,0.0,"Xpand MaxScale Tutorial in KB doesn't work $end$ This was originally reported in [the docs-talk slack channel by Luke Smith|https://mariadb.slack.com/archives/CHAMKU9PG/p1653427736091489].

Luke reported a problem with [this KB documentation page|https://mariadb.com/kb/en/mariadb-maxscale-6-maxscale-and-xpand-tutorial/] about using MaxScale with Xpand, which is generated from [this page in GitHub|https://github.com/mariadb-corporation/MaxScale/blob/6.3/Documentation/Tutorials/MaxScale-Xpand-Tutorial.md].

This document does not list the full set of required privileges.

When a user tries to connect with the privileges listed in the document, they will see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:21:56   error  : Failed to query server '@@Xpand:node-1' for user account info. Query 'SELECT * FROM system.users; SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role); SHOW DATABASES;' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`users`.; transaction aborted.
{noformat}

If the SELECT privilege is granted on the `system`.`users` table, users still see the following error in the Maxscale error log:

{noformat}
2022-05-24 23:23:35   error  : Failed to query server '@@Xpand:node-1' for user account info. Multiquery element 'SELECT u.username, u.host, a.dbname, a.privileges FROM system.user_acl AS a LEFT JOIN system.users AS u ON (u.user = a.role);' failed. Error 1045: [11281] Permission denied: User 'maxscale'@'10.70.120.%' is missing SELECT on `system`.`user_acl`.; transaction aborted.
{noformat}

If the SELECT privilege is also granted on the `system`.`user_acl` table, then connections are successful.

However, there might be other permissions missing as the ""maxscale"" user still can't use customer databases due to the following error in the MaxScale error log:

{noformat}
2022-05-24 23:48:26   warning: (26) [MariaDBProtocol] Authentication failed for user 'horizonApp'@[10.70.120.51] to service 'Xpand-Service'. Originating listener: 'xpand_listener'. MariaDB error: 'Unknown database 'database_name''.
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,133.167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1739,MXS-4150,Task,MXS,2022-06-02 06:11:17,,0,Measure the impact of exact-match and regex based rules on the caching.,"When the caching was implemented, the query classifier cache was not present. That meant that if a rule required the presence or absence of a database/table, then the statement had to be parsed and the parsing had a significant impact on the performance. Now with the query classifier cache the classification information is in general available without having to parse the statement. Hence, the impact of rules on the performance should be much smaller than what it used to be. Measure what the impact is and document it.",,"Measure the impact of exact-match and regex based rules on the caching. $end$ When the caching was implemented, the query classifier cache was not present. That meant that if a rule required the presence or absence of a database/table, then the statement had to be parsed and the parsing had a significant impact on the performance. Now with the query classifier cache the classification information is in general available without having to parse the statement. Hence, the impact of rules on the performance should be much smaller than what it used to be. Measure what the impact is and document it. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-06-02 06:11:17,Measure the impact of exact-match and regex based rules on the caching.,"When the caching was implemented, the query classifier cache was not present. That meant that if a rule required the presence or absence of a database/table, then the statement had to be parsed and the parsing had a significant impact on the performance. Now with the query classifier cache the classification information is in general available without having to parse the statement. Hence, the impact of rules on the performance should be much smaller than what it used to be. Measure what the impact is and document it.",,0,0,0,0,0.0,"Measure the impact of exact-match and regex based rules on the caching. $end$ When the caching was implemented, the query classifier cache was not present. That meant that if a rule required the presence or absence of a database/table, then the statement had to be parsed and the parsing had a significant impact on the performance. Now with the query classifier cache the classification information is in general available without having to parse the statement. Hence, the impact of rules on the performance should be much smaller than what it used to be. Measure what the impact is and document it. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,479,44,0.091858,19,0.039666,13,0.0271399,9,0.0187891,9,0.0187891
1740,MXS-4157,Sub-Task,MXS,2022-06-06 07:20:54,,0,Document encryption key management in MaxScale,,,Document encryption key management in MaxScale $end$ $acceptance criteria:$,,markus makela,markus makela,Major,1,,0,0,0,6,0,0,0,,0,850,0,0,0,2022-06-06 07:20:54,Document encryption key management in MaxScale,,,0,0,0,0,0.0,Document encryption key management in MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,135,16,0.118519,11,0.0814815,9,0.0666667,8,0.0592593,7,0.0518519
1741,MXS-4158,Sub-Task,MXS,2022-06-06 07:21:13,,0,Test pinloki encryption,,,Test pinloki encryption $end$ $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,6,0,0,0,,0,850,0,0,0,2022-06-06 07:21:13,Test pinloki encryption,,,0,0,0,0,0.0,Test pinloki encryption $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,136,16,0.117647,11,0.0808824,9,0.0661765,8,0.0588235,7,0.0514706
1742,MXS-4161,New Feature,MXS,2022-06-07 20:36:41,,0,MaxScale System Diagnostics,"MaxScale should provides means using which it is easy to ascertain that the configuration is compatible with the resources available to MaxScale. Especially when MaxScale is running in a container, it is possible that the resources - cores and memory - are limited compared to what is available on the machine. If that is the case and the automatic configuration (in particular {{threads}} and {{query_classifier_cache_size}}) is reliead upon, then MaxScale may end up using far more resources than what is available, with crashes as the result.

Original description
================
MaxScale Allocated Memory Usage Estimation

While MXS-3822 will tackle showing customers the current memory usage MaxScale is causing, we should also add an additional feature which shows customers ""allocated"" memory usage. This would be more of an estimate, but would give users direct feedback on how their MaxScale configuration creates the potential for memory usage.

For example, we know that {{query_classifier_cache_size}} is pretty straightforward. We also know that each thread spawned by {{threads=N}} has its own cache which comes with it. We should be able to show a worst-case estimate of memory usage based on this information.

h2. Why?
MaxScale has many automatic configuration parameters (such as {{threads=auto}} or {{query_classifier_cache_size}} defaulting to 15% of detected memory). In most cases, these work well and set sensible defaults. However, in some cases other factors obstruct MaxScale's ability to properly detect underlying resources and default memory allocation can be overzealous. This behavior is non-obvious to most customers, and because MaxScale does not immediately use these memory allocations when it first starts up, many customers end up complaining about ""memory leaks"" or other ""memory problems"" as connections come in and MaxScale begins actively using the memory its configurations allow it to.

By showing customers clearly what MaxScale is configured to use, it will clue customers immediately in when something is wrong at a configuration level, so instead of customers immediately assuming a leak or other problem is occurring, customers will instead ask MariaDB why MaxScale is expecting to use so much more memory than the customer has allocated to the VM/node/etc, which creates a much more productive conversation with MariaDB.

h2. Starting Point
This request could be seen as all-encompassing which could make it difficult to implement properly. For example, calculating expected memory usage for various filters may be impossible or extremely difficult. Likewise for estimating memory usage based on a potentially infinite number of incoming connections.

Initial feature delivery could dodge complicated situations by scoping appropriately and communicating that scoping to users.

For connection-count-based estimations, a simple answer to start could be to let the customer enter that value so customers can see expected worst-case memory usage (excepting impact of filters) for various concurrent connection targets which customers could then use for planning purposes. Down the road, it may be feasible to harvest or prepopulate sensible values based on backend servers' {{max_connections}} or based on the configuration of {{max_routing_connections}} when that is enabled.",,"MaxScale System Diagnostics $end$ MaxScale should provides means using which it is easy to ascertain that the configuration is compatible with the resources available to MaxScale. Especially when MaxScale is running in a container, it is possible that the resources - cores and memory - are limited compared to what is available on the machine. If that is the case and the automatic configuration (in particular {{threads}} and {{query_classifier_cache_size}}) is reliead upon, then MaxScale may end up using far more resources than what is available, with crashes as the result.

Original description
================
MaxScale Allocated Memory Usage Estimation

While MXS-3822 will tackle showing customers the current memory usage MaxScale is causing, we should also add an additional feature which shows customers ""allocated"" memory usage. This would be more of an estimate, but would give users direct feedback on how their MaxScale configuration creates the potential for memory usage.

For example, we know that {{query_classifier_cache_size}} is pretty straightforward. We also know that each thread spawned by {{threads=N}} has its own cache which comes with it. We should be able to show a worst-case estimate of memory usage based on this information.

h2. Why?
MaxScale has many automatic configuration parameters (such as {{threads=auto}} or {{query_classifier_cache_size}} defaulting to 15% of detected memory). In most cases, these work well and set sensible defaults. However, in some cases other factors obstruct MaxScale's ability to properly detect underlying resources and default memory allocation can be overzealous. This behavior is non-obvious to most customers, and because MaxScale does not immediately use these memory allocations when it first starts up, many customers end up complaining about ""memory leaks"" or other ""memory problems"" as connections come in and MaxScale begins actively using the memory its configurations allow it to.

By showing customers clearly what MaxScale is configured to use, it will clue customers immediately in when something is wrong at a configuration level, so instead of customers immediately assuming a leak or other problem is occurring, customers will instead ask MariaDB why MaxScale is expecting to use so much more memory than the customer has allocated to the VM/node/etc, which creates a much more productive conversation with MariaDB.

h2. Starting Point
This request could be seen as all-encompassing which could make it difficult to implement properly. For example, calculating expected memory usage for various filters may be impossible or extremely difficult. Likewise for estimating memory usage based on a potentially infinite number of incoming connections.

Initial feature delivery could dodge complicated situations by scoping appropriately and communicating that scoping to users.

For connection-count-based estimations, a simple answer to start could be to let the customer enter that value so customers can see expected worst-case memory usage (excepting impact of filters) for various concurrent connection targets which customers could then use for planning purposes. Down the road, it may be feasible to harvest or prepopulate sensible values based on backend servers' {{max_connections}} or based on the configuration of {{max_routing_connections}} when that is enabled. $acceptance criteria:$",,Rob Schwyzer,Rob Schwyzer,Major,11,,0,3,1,1,0,3,0,,0,850,1,1,0,2022-09-12 09:47:26,MaxScale Allocated Memory Usage Estimation,"While MXS-3822 will tackle showing customers the current memory usage MaxScale is causing, we should also add an additional feature which shows customers ""allocated"" memory usage. This would be more of an estimate, but would give users direct feedback on how their MaxScale configuration creates the potential for memory usage.

For example, we know that {{query_classifier_cache_size}} is pretty straightforward. We also know that each thread spawned by {{threads=N}} has its own cache which comes with it. We should be able to show a worst-case estimate of memory usage based on this information.

h2. Why?
MaxScale has many automatic configuration parameters (such as {{threads=auto}} or {{query_classifier_cache_size}} defaulting to 15% of detected memory). In most cases, these work well and set sensible defaults. However, in some cases other factors obstruct MaxScale's ability to properly detect underlying resources and default memory allocation can be overzealous. This behavior is non-obvious to most customers, and because MaxScale does not immediately use these memory allocations when it first starts up, many customers end up complaining about ""memory leaks"" or other ""memory problems"" as connections come in and MaxScale begins actively using the memory its configurations allow it to.

By showing customers clearly what MaxScale is configured to use, it will clue customers immediately in when something is wrong at a configuration level, so instead of customers immediately assuming a leak or other problem is occurring, customers will instead ask MariaDB why MaxScale is expecting to use so much more memory than the customer has allocated to the VM/node/etc, which creates a much more productive conversation with MariaDB.

h2. Starting Point
This request could be seen as all-encompassing which could make it difficult to implement properly. For example, calculating expected memory usage for various filters may be impossible or extremely difficult. Likewise for estimating memory usage based on a potentially infinite number of incoming connections.

Initial feature delivery could dodge complicated situations by scoping appropriately and communicating that scoping to users.

For connection-count-based estimations, a simple answer to start could be to let the customer enter that value so customers can see expected worst-case memory usage (excepting impact of filters) for various concurrent connection targets which customers could then use for planning purposes. Down the road, it may be feasible to harvest or prepopulate sensible values based on backend servers' {{max_connections}} or based on the configuration of {{max_routing_connections}} when that is enabled.",,1,1,0,100,0.233251,"MaxScale Allocated Memory Usage Estimation $end$ While MXS-3822 will tackle showing customers the current memory usage MaxScale is causing, we should also add an additional feature which shows customers ""allocated"" memory usage. This would be more of an estimate, but would give users direct feedback on how their MaxScale configuration creates the potential for memory usage.

For example, we know that {{query_classifier_cache_size}} is pretty straightforward. We also know that each thread spawned by {{threads=N}} has its own cache which comes with it. We should be able to show a worst-case estimate of memory usage based on this information.

h2. Why?
MaxScale has many automatic configuration parameters (such as {{threads=auto}} or {{query_classifier_cache_size}} defaulting to 15% of detected memory). In most cases, these work well and set sensible defaults. However, in some cases other factors obstruct MaxScale's ability to properly detect underlying resources and default memory allocation can be overzealous. This behavior is non-obvious to most customers, and because MaxScale does not immediately use these memory allocations when it first starts up, many customers end up complaining about ""memory leaks"" or other ""memory problems"" as connections come in and MaxScale begins actively using the memory its configurations allow it to.

By showing customers clearly what MaxScale is configured to use, it will clue customers immediately in when something is wrong at a configuration level, so instead of customers immediately assuming a leak or other problem is occurring, customers will instead ask MariaDB why MaxScale is expecting to use so much more memory than the customer has allocated to the VM/node/etc, which creates a much more productive conversation with MariaDB.

h2. Starting Point
This request could be seen as all-encompassing which could make it difficult to implement properly. For example, calculating expected memory usage for various filters may be impossible or extremely difficult. Likewise for estimating memory usage based on a potentially infinite number of incoming connections.

Initial feature delivery could dodge complicated situations by scoping appropriately and communicating that scoping to users.

For connection-count-based estimations, a simple answer to start could be to let the customer enter that value so customers can see expected worst-case memory usage (excepting impact of filters) for various concurrent connection targets which customers could then use for planning purposes. Down the road, it may be feasible to harvest or prepopulate sensible values based on backend servers' {{max_connections}} or based on the configuration of {{max_routing_connections}} when that is enabled. $acceptance criteria:$",2,1,1,1,1,1,1,2317.17,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1743,MXS-4167,New Feature,MXS,2022-06-15 07:54:45,,0,Show filter diagnostics in MaxGUI,,,Show filter diagnostics in MaxGUI $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Minor,6,,0,0,0,2,0,1,0,,0,850,0,0,0,2022-06-17 06:08:20,Show filter diagnostics in the GUI,,,1,0,0,3,0.222222,Show filter diagnostics in the GUI $end$ $acceptance criteria:$,1,1,0,0,0,0,1,46.2167,85,32,0.376471,18,0.211765,13,0.152941,10,0.117647,10,0.117647
1744,MXS-4168,Task,MXS,2022-06-15 23:35:05,,0,Allow copying logs in the GUI without losing its style format,"At the moment, directly select and copy logs in the GUI, the result looks like this

{code:java}
2022-06-15 10:56:44
info
: Found matching user 'maxuser'@'127.0.0.1' for client 'maxuser'@'::ffff:127.0.0.1' with sufficient privileges.
2022-06-15 10:56:44
info
: Target connection counts:
2022-06-15 10:56:44
info
: current operations : 0 in server1 Master, Running
2022-06-15 10:56:44
info
: current operations : 0 in server2 Slave, Running
{code}

A log should be inline.",,"Allow copying logs in the GUI without losing its style format $end$ At the moment, directly select and copy logs in the GUI, the result looks like this

{code:java}
2022-06-15 10:56:44
info
: Found matching user 'maxuser'@'127.0.0.1' for client 'maxuser'@'::ffff:127.0.0.1' with sufficient privileges.
2022-06-15 10:56:44
info
: Target connection counts:
2022-06-15 10:56:44
info
: current operations : 0 in server1 Master, Running
2022-06-15 10:56:44
info
: current operations : 0 in server2 Slave, Running
{code}

A log should be inline. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-06-16 01:11:12,Allow copying logs in the GUI without losing its style format,"At the moment, directly select and copy logs in the GUI, the result looks like this

{code:java}
2022-06-15 10:56:44
info
: Found matching user 'maxuser'@'127.0.0.1' for client 'maxuser'@'::ffff:127.0.0.1' with sufficient privileges.
2022-06-15 10:56:44
info
: Target connection counts:
2022-06-15 10:56:44
info
: current operations : 0 in server1 Master, Running
2022-06-15 10:56:44
info
: current operations : 0 in server2 Slave, Running
{code}

A log should be inline.",,0,0,0,0,0.0,"Allow copying logs in the GUI without losing its style format $end$ At the moment, directly select and copy logs in the GUI, the result looks like this

{code:java}
2022-06-15 10:56:44
info
: Found matching user 'maxuser'@'127.0.0.1' for client 'maxuser'@'::ffff:127.0.0.1' with sufficient privileges.
2022-06-15 10:56:44
info
: Target connection counts:
2022-06-15 10:56:44
info
: current operations : 0 in server1 Master, Running
2022-06-15 10:56:44
info
: current operations : 0 in server2 Slave, Running
{code}

A log should be inline. $acceptance criteria:$",0,0,0,0,0,0,0,1.6,86,33,0.383721,18,0.209302,13,0.151163,10,0.116279,10,0.116279
1745,MXS-4178,Task,MXS,2022-06-27 09:09:37,,0,Replace localStorage with localForage for maxgui,"*localStorage* has a size limit of 5MB for most modern browsers unless the user increases it. So, it has a limitation regarding the size limit. 
There is a better storage in the browser which is *IndexedDB*. It's suitable for storing larger amounts of structured data. 
It can store both objects and key-value pairs,  so it saves time converting objects to strings that are needed when using *localStorage*.
Its maximum size limit is also larger than *localStorage*

There is a library called *localForage*  which will automatically use localStorage in browsers with no IndexedDB or WebSQL support.
",,"Replace localStorage with localForage for maxgui $end$ *localStorage* has a size limit of 5MB for most modern browsers unless the user increases it. So, it has a limitation regarding the size limit. 
There is a better storage in the browser which is *IndexedDB*. It's suitable for storing larger amounts of structured data. 
It can store both objects and key-value pairs,  so it saves time converting objects to strings that are needed when using *localStorage*.
Its maximum size limit is also larger than *localStorage*

There is a library called *localForage*  which will automatically use localStorage in browsers with no IndexedDB or WebSQL support.
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-06-27 09:09:37,Replace localStorage with localForage for maxgui,"*localStorage* has a size limit of 5MB for most modern browsers unless the user increases it. So, it has a limitation regarding the size limit. 
There is a better storage in the browser which is *IndexedDB*. It's suitable for storing larger amounts of structured data. 
It can store both objects and key-value pairs,  so it saves time converting objects to strings that are needed when using *localStorage*.
Its maximum size limit is also larger than *localStorage*

There is a library called *localForage*  which will automatically use localStorage in browsers with no IndexedDB or WebSQL support.
",,0,0,0,0,0.0,"Replace localStorage with localForage for maxgui $end$ *localStorage* has a size limit of 5MB for most modern browsers unless the user increases it. So, it has a limitation regarding the size limit. 
There is a better storage in the browser which is *IndexedDB*. It's suitable for storing larger amounts of structured data. 
It can store both objects and key-value pairs,  so it saves time converting objects to strings that are needed when using *localStorage*.
Its maximum size limit is also larger than *localStorage*

There is a library called *localForage*  which will automatically use localStorage in browsers with no IndexedDB or WebSQL support.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,87,33,0.37931,18,0.206897,13,0.149425,10,0.114943,10,0.114943
1746,MXS-4182,New Feature,MXS,2022-06-30 18:27:07,,0,Session load indicator.,"Was: MaxScale Global CPU Usage Indicator

At some times it's very hard to know why MaxScale is consuming High CPU utilization and which part in particular is causing it. We need an indication as to where the CPU is going. Which query or session is causing more CPU would be good and alert in the GUI with Critical/Warning/Notice in the GUI and through auto mail alert would be a good and it will be very useful feature for us.
",,"Session load indicator. $end$ Was: MaxScale Global CPU Usage Indicator

At some times it's very hard to know why MaxScale is consuming High CPU utilization and which part in particular is causing it. We need an indication as to where the CPU is going. Which query or session is causing more CPU would be good and alert in the GUI with Critical/Warning/Notice in the GUI and through auto mail alert would be a good and it will be very useful feature for us.
 $acceptance criteria:$",,Naresh Chandra,Naresh Chandra,Major,16,,0,3,1,1,0,3,0,,0,850,3,1,0,2023-01-30 08:59:59,MaxScale Global CPU Usage Indicator,"At some times it's very hard to know why MaxScale is consuming High CPU utilization and which part in particular is causing it. We need an indication as to where the CPU is going. Which query or session is causing more CPU would be good and alert in the GUI with Critical/Warning/Notice in the GUI and through auto mail alert would be a good and it will be very useful feature for us.
",,1,1,0,14,0.0740741,"MaxScale Global CPU Usage Indicator $end$ At some times it's very hard to know why MaxScale is consuming High CPU utilization and which part in particular is causing it. We need an indication as to where the CPU is going. Which query or session is causing more CPU would be good and alert in the GUI with Critical/Warning/Notice in the GUI and through auto mail alert would be a good and it will be very useful feature for us.
 $acceptance criteria:$",2,1,1,1,0,0,1,5126.53,4,4,1.0,4,1.0,4,1.0,3,0.75,2,0.5
1747,MXS-4186,Task,MXS,2022-07-04 08:21:16,,0,"Add ""Memory"" column to the session display in MaxGUI",MXS-2724 adds to the returned JSON session information object a field {{attributes.memory}} containing an object with memory statistics for the session. In that object there's a field {{total}} that is the combined total of the data. A new column _Memory_ should be added to the MaxGUI session display where the {{attributes.memory.total}} value is displayed.,,"Add ""Memory"" column to the session display in MaxGUI $end$ MXS-2724 adds to the returned JSON session information object a field {{attributes.memory}} containing an object with memory statistics for the session. In that object there's a field {{total}} that is the combined total of the data. A new column _Memory_ should be added to the MaxGUI session display where the {{attributes.memory.total}} value is displayed. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,1,1,0,0,0,,0,850,0,0,0,2022-07-04 08:21:41,"Add ""Memory"" column to the session display in MaxGUI",MXS-2724 adds to the returned JSON session information object a field {{attributes.memory}} containing an object with memory statistics for the session. In that object there's a field {{total}} that is the combined total of the data. A new column _Memory_ should be added to the MaxGUI session display where the {{attributes.memory.total}} value is displayed.,,0,0,0,0,0.0,"Add ""Memory"" column to the session display in MaxGUI $end$ MXS-2724 adds to the returned JSON session information object a field {{attributes.memory}} containing an object with memory statistics for the session. In that object there's a field {{total}} that is the combined total of the data. A new column _Memory_ should be added to the MaxGUI session display where the {{attributes.memory.total}} value is displayed. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,480,44,0.0916667,19,0.0395833,13,0.0270833,9,0.01875,9,0.01875
1748,MXS-4187,Task,MXS,2022-07-04 08:27:38,,0,Investigate and fix unit-test 'test_worker',The `test_worker` unit-test fails consistently on some platforms and intermittently on others. Figure out and fix the cause.,,Investigate and fix unit-test 'test_worker' $end$ The `test_worker` unit-test fails consistently on some platforms and intermittently on others. Figure out and fix the cause. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,12,,0,1,0,2,0,0,0,,0,850,1,0,0,2022-07-04 08:27:38,Investigate and fix unit-test 'test_worker',The `test_worker` unit-test fails consistently on some platforms and intermittently on others. Figure out and fix the cause.,,0,0,0,0,0.0,Investigate and fix unit-test 'test_worker' $end$ The `test_worker` unit-test fails consistently on some platforms and intermittently on others. Figure out and fix the cause. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,481,44,0.0914761,19,0.039501,13,0.027027,9,0.018711,9,0.018711
1749,MXS-4206,Task,MXS,2022-07-18 05:19:40,,0,Layering QueryEditor components,"All the components in the QueryPage should be restructured and placed into container components and presentational components. This would make things easier to re-use
the presentational components and write unit tests",,"Layering QueryEditor components $end$ All the components in the QueryPage should be restructured and placed into container components and presentational components. This would make things easier to re-use
the presentational components and write unit tests $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,11,,0,0,0,3,0,0,0,,0,850,0,0,0,2022-07-18 10:20:34,Layering QueryEditor components,"All the components in the QueryPage should be restructured and placed into container components and presentational components. This would make things easier to re-use
the presentational components and write unit tests",,0,0,0,0,0.0,"Layering QueryEditor components $end$ All the components in the QueryPage should be restructured and placed into container components and presentational components. This would make things easier to re-use
the presentational components and write unit tests $acceptance criteria:$",0,0,0,0,0,0,1,5.0,88,33,0.375,18,0.204545,13,0.147727,10,0.113636,10,0.113636
1750,MXS-4208,Task,MXS,2022-07-18 10:31:01,,0,Make tests work with RHEL 8 and RHEL 9,The system tests do not work due to them expecting a RHEL 7 environment.,,Make tests work with RHEL 8 and RHEL 9 $end$ The system tests do not work due to them expecting a RHEL 7 environment. $acceptance criteria:$,,markus makela,markus makela,Major,10,,0,3,0,1,0,0,0,,0,850,3,0,0,2022-07-18 10:31:01,Make tests work with RHEL 8 and RHEL 9,The system tests do not work due to them expecting a RHEL 7 environment.,,0,0,0,0,0.0,Make tests work with RHEL 8 and RHEL 9 $end$ The system tests do not work due to them expecting a RHEL 7 environment. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,137,16,0.116788,11,0.080292,9,0.0656934,8,0.0583942,7,0.0510949
1751,MXS-421,New Feature,MXS,2015-10-23 13:11:06,,0,improved log facility,"Setting of various log facilities
 
-   Log authentication failure into one facility ( separate auth deny from server vs proxy) 
log_facility_deny_auth=LOG_USER
log_level_deny_auth=WARNING

-  Log query execution privileges failure into one facility 
log_facility_deny_privileges=LOG_USER
log_level_deny_privileges=WARNING

- Log Firewall Filters into one facility
Log_facility_deny_firewall=LOG_USER
log_level_deny_firewall=NOTICE

To be compiled with other ideas
 ",,"improved log facility $end$ Setting of various log facilities
 
-   Log authentication failure into one facility ( separate auth deny from server vs proxy) 
log_facility_deny_auth=LOG_USER
log_level_deny_auth=WARNING

-  Log query execution privileges failure into one facility 
log_facility_deny_privileges=LOG_USER
log_level_deny_privileges=WARNING

- Log Firewall Filters into one facility
Log_facility_deny_firewall=LOG_USER
log_level_deny_firewall=NOTICE

To be compiled with other ideas
  $acceptance criteria:$",,VAROQUI Stephane,VAROQUI Stephane,Major,28,,0,4,0,5,0,0,0,,0,850,0,0,0,2017-07-27 09:59:07,improved log facility,"Setting of various log facilities
 
-   Log authentication failure into one facility ( separate auth deny from server vs proxy) 
log_facility_deny_auth=LOG_USER
log_level_deny_auth=WARNING

-  Log query execution privileges failure into one facility 
log_facility_deny_privileges=LOG_USER
log_level_deny_privileges=WARNING

- Log Firewall Filters into one facility
Log_facility_deny_firewall=LOG_USER
log_level_deny_firewall=NOTICE

To be compiled with other ideas
 ",,0,0,0,0,0.0,"improved log facility $end$ Setting of various log facilities
 
-   Log authentication failure into one facility ( separate auth deny from server vs proxy) 
log_facility_deny_auth=LOG_USER
log_level_deny_auth=WARNING

-  Log query execution privileges failure into one facility 
log_facility_deny_privileges=LOG_USER
log_level_deny_privileges=WARNING

- Log Firewall Filters into one facility
Log_facility_deny_firewall=LOG_USER
log_level_deny_firewall=NOTICE

To be compiled with other ideas
  $acceptance criteria:$",0,0,0,0,0,0,1,15428.8,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1752,MXS-422,Task,MXS,2015-10-23 14:35:42,,0,Internal Developer Documentation,There are a number of different tasks for which instructions would be needed.,,Internal Developer Documentation $end$ There are a number of different tasks for which instructions would be needed. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,4,,0,850,0,0,0,2016-01-12 18:03:08,Internal Developer Documentation,There are a number of different tasks for which instructions would be needed.,,0,0,0,0,0.0,Internal Developer Documentation $end$ There are a number of different tasks for which instructions would be needed. $acceptance criteria:$,0,0,0,0,0,0,0,1947.45,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1753,MXS-4223,Task,MXS,2022-07-28 06:53:44,,0,Show all table rows by default in MaxGUI dashboard,"When the table rows >=10,  it limits the number of rows to be shown to 10. The user has to click on the dropdown at the footer of the table to change the value to show all. This is not convenient for the user.
It should show all by default.",,"Show all table rows by default in MaxGUI dashboard $end$ When the table rows >=10,  it limits the number of rows to be shown to 10. The user has to click on the dropdown at the footer of the table to change the value to show all. This is not convenient for the user.
It should show all by default. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-07-29 07:59:31,Show all table rows by default in MaxGUI dashboard,"When the table rows >=10,  it limits the number of rows to be shown to 10. The user has to click on the dropdown at the footer of the table to change the value to show all. This is not convenient for the user.
It should show all by default.",,0,0,0,0,0.0,"Show all table rows by default in MaxGUI dashboard $end$ When the table rows >=10,  it limits the number of rows to be shown to 10. The user has to click on the dropdown at the footer of the table to change the value to show all. This is not convenient for the user.
It should show all by default. $acceptance criteria:$",0,0,0,0,0,0,0,25.0833,89,33,0.370787,18,0.202247,13,0.146067,10,0.11236,10,0.11236
1754,MXS-423,Sub-Task,MXS,2015-10-23 14:57:55,,0,Instructions for running Jenkins,Anybody should be able to start a Jenkins job. Instructions for how to do that should be created.,,Instructions for running Jenkins $end$ Anybody should be able to start a Jenkins job. Instructions for how to do that should be created. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2015-10-23 14:57:55,Instructions for running Jenkins,Anybody should be able to start a Jenkins job. Instructions for how to do that should be created.,,0,0,0,0,0.0,Instructions for running Jenkins $end$ Anybody should be able to start a Jenkins job. Instructions for how to do that should be created. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1755,MXS-4234,Task,MXS,2022-08-08 06:06:10,,0,Packaging Query Editor,A directory will be created in maxgui to contain all components related to the query editor. This directory will be set up to be suitable for packaging the query editor. ,,Packaging Query Editor $end$ A directory will be created in maxgui to contain all components related to the query editor. This directory will be set up to be suitable for packaging the query editor.  $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,7,,0,0,0,3,0,0,2,,0,850,0,0,0,2022-08-08 09:14:33,Packaging Query Editor,A directory will be created in maxgui to contain all components related to the query editor. This directory will be set up to be suitable for packaging the query editor. ,,0,0,0,0,0.0,Packaging Query Editor $end$ A directory will be created in maxgui to contain all components related to the query editor. This directory will be set up to be suitable for packaging the query editor.  $acceptance criteria:$,0,0,0,0,0,0,1,3.13333,90,33,0.366667,18,0.2,13,0.144444,10,0.111111,10,0.111111
1756,MXS-4235,Sub-Task,MXS,2022-08-08 09:19:18,,0,Replace the usage of `::v-deep` selector` in Query Editor,"SkySql UI uses Vue 2.7. to avoid compatibility issues, migrate Vue to its latest minor version",,"Replace the usage of `::v-deep` selector` in Query Editor $end$ SkySql UI uses Vue 2.7. to avoid compatibility issues, migrate Vue to its latest minor version $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,6,,0,0,0,3,0,2,0,,0,850,0,0,0,2022-08-08 09:19:18,Migrate from Vue 2.6 to 2.7,"To avoid compatibility issues, migrate vue to its latest minor version",,1,1,0,24,0.8,"Migrate from Vue 2.6 to 2.7 $end$ To avoid compatibility issues, migrate vue to its latest minor version $acceptance criteria:$",2,1,1,1,1,1,1,0.0,91,33,0.362637,18,0.197802,13,0.142857,10,0.10989,10,0.10989
1757,MXS-4236,Sub-Task,MXS,2022-08-09 06:14:32,,0,Remove query editor routes,"Query editor depends on route configuration to work correctly.
e.g. `/query/servers/server_0` is the route for the SQL connection to `server_0`.
The route is introduced in order to open the query editor via a quick link on the cluster detail control page. However, this is over necessary, there is another way to do a similar job.

When clicking the button to open the query editor for a specific server, it can just find the worksheet having a connection to that server and make it an active worksheet. This gets rid of the need for using routes for the query editor.",,"Remove query editor routes $end$ Query editor depends on route configuration to work correctly.
e.g. `/query/servers/server_0` is the route for the SQL connection to `server_0`.
The route is introduced in order to open the query editor via a quick link on the cluster detail control page. However, this is over necessary, there is another way to do a similar job.

When clicking the button to open the query editor for a specific server, it can just find the worksheet having a connection to that server and make it an active worksheet. This gets rid of the need for using routes for the query editor. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2022-08-09 06:14:32,Remove query editor routes,"Query editor depends on route configuration to work correctly.
e.g. `/query/servers/server_0` is the route for the SQL connection to `server_0`.
The route is introduced in order to open the query editor via a quick link on the cluster detail control page. However, this is over necessary, there is another way to do a similar job.

When clicking the button to open the query editor for a specific server, it can just find the worksheet having a connection to that server and make it an active worksheet. This gets rid of the need for using routes for the query editor.",,0,0,0,0,0.0,"Remove query editor routes $end$ Query editor depends on route configuration to work correctly.
e.g. `/query/servers/server_0` is the route for the SQL connection to `server_0`.
The route is introduced in order to open the query editor via a quick link on the cluster detail control page. However, this is over necessary, there is another way to do a similar job.

When clicking the button to open the query editor for a specific server, it can just find the worksheet having a connection to that server and make it an active worksheet. This gets rid of the need for using routes for the query editor. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,92,34,0.369565,19,0.206522,14,0.152174,11,0.119565,11,0.119565
1758,MXS-424,Sub-Task,MXS,2015-10-23 14:58:56,,0,Instructions for creating a test-case.,Instructions for creating a test case and for incorporating it into the test-suite should be created.,,Instructions for creating a test-case. $end$ Instructions for creating a test case and for incorporating it into the test-suite should be created. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2015-10-23 14:58:56,Instructions for creating a test-case.,Instructions for creating a test case and for incorporating it into the test-suite should be created.,,0,0,0,0,0.0,Instructions for creating a test-case. $end$ Instructions for creating a test case and for incorporating it into the test-suite should be created. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1759,MXS-4242,New Feature,MXS,2022-08-11 11:26:09,MXS-3387,0,At startup nosqlprotcol should optionally create a default NoSQL user.,"When _nosqlprotocol_ is started with the following configuration
{code}
...
nosqlprotocol.user=the_user
nosqlprotocol.password=the_password
nosqlprotocol.authentication_required=true
nosqlprotocol.authorization_enabled=true
{code}
it should check whether the NoSQL account database is empty and if it is, it should create a a NoSQL user {{the_user}} with the password {{the_password}}, and give it the NoSQL capabilities corresponding to what {{SHOW GRANTS}}  reports for the user.

With this functionality, the bootstrapping of a secure _nosqlprotocol_ installation is significantly simplified. All that needs to be done is:
* Create the user in MariaDB.
* Configure _nosqlprotocol_ in the MaxScale configuration file as above.
* Start MaxScale.

Without this functionality, the creation of a secure _nosqlprotocol_ installation requires MaxScale needs to be started multiple times, with the MaxScale configuration file edited in between.",,"At startup nosqlprotcol should optionally create a default NoSQL user. $end$ When _nosqlprotocol_ is started with the following configuration
{code}
...
nosqlprotocol.user=the_user
nosqlprotocol.password=the_password
nosqlprotocol.authentication_required=true
nosqlprotocol.authorization_enabled=true
{code}
it should check whether the NoSQL account database is empty and if it is, it should create a a NoSQL user {{the_user}} with the password {{the_password}}, and give it the NoSQL capabilities corresponding to what {{SHOW GRANTS}}  reports for the user.

With this functionality, the bootstrapping of a secure _nosqlprotocol_ installation is significantly simplified. All that needs to be done is:
* Create the user in MariaDB.
* Configure _nosqlprotocol_ in the MaxScale configuration file as above.
* Start MaxScale.

Without this functionality, the creation of a secure _nosqlprotocol_ installation requires MaxScale needs to be started multiple times, with the MaxScale configuration file edited in between. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-08-15 05:49:13,At startup nosqlprotcol should optionally create a default NoSQL user.,"When _nosqlprotocol_ is started with the following configuration
{code}
...
nosqlprotocol.user=the_user
nosqlprotocol.password=the_password
nosqlprotocol.authentication_required=true
nosqlprotocol.authorization_enabled=true
{code}
it should check whether the NoSQL account database is empty and if it is, it should create a a NoSQL user {{the_user}} with the password {{the_password}}, and give it the NoSQL capabilities corresponding to what {{SHOW GRANTS}}  reports for the user.

With this functionality, the bootstrapping of a secure _nosqlprotocol_ installation is significantly simplified. All that needs to be done is:
* Create the user in MariaDB.
* Configure _nosqlprotocol_ in the MaxScale configuration file as above.
* Start MaxScale.

Without this functionality, the creation of a secure _nosqlprotocol_ installation requires MaxScale needs to be started multiple times, with the MaxScale configuration file edited in between.",,0,0,0,0,0.0,"At startup nosqlprotcol should optionally create a default NoSQL user. $end$ When _nosqlprotocol_ is started with the following configuration
{code}
...
nosqlprotocol.user=the_user
nosqlprotocol.password=the_password
nosqlprotocol.authentication_required=true
nosqlprotocol.authorization_enabled=true
{code}
it should check whether the NoSQL account database is empty and if it is, it should create a a NoSQL user {{the_user}} with the password {{the_password}}, and give it the NoSQL capabilities corresponding to what {{SHOW GRANTS}}  reports for the user.

With this functionality, the bootstrapping of a secure _nosqlprotocol_ installation is significantly simplified. All that needs to be done is:
* Create the user in MariaDB.
* Configure _nosqlprotocol_ in the MaxScale configuration file as above.
* Start MaxScale.

Without this functionality, the creation of a secure _nosqlprotocol_ installation requires MaxScale needs to be started multiple times, with the MaxScale configuration file edited in between. $acceptance criteria:$",0,0,0,0,0,0,1,90.3833,482,44,0.0912863,19,0.0394191,13,0.026971,9,0.0186722,9,0.0186722
1760,MXS-4245,Task,MXS,2022-08-16 10:12:00,,0,Investigate mxs1110_16mb,,,Investigate mxs1110_16mb $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-08-16 10:12:00,Investigate mxs1110_16mb,,,0,0,0,0,0.0,Investigate mxs1110_16mb $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,483,44,0.0910973,19,0.0393375,13,0.0269151,9,0.0186335,9,0.0186335
1761,MXS-4246,Task,MXS,2022-08-16 10:24:03,,0,Benchmark 22.08,,,Benchmark 22.08 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,1,0,3,0,0,0,,0,850,1,0,0,2022-08-16 10:24:03,Benchmark 22.08,,,0,0,0,0,0.0,Benchmark 22.08 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,484,44,0.0909091,19,0.0392562,13,0.0268595,9,0.018595,9,0.018595
1762,MXS-425,Sub-Task,MXS,2015-10-23 15:02:37,,0,Instructions for analyzing test results.,"After a test-suite has been run, the failed cases need to be investigated.

Instructions for digging out the information, accessing log-files, core-dumps etc. from the test servers is needed.",,"Instructions for analyzing test results. $end$ After a test-suite has been run, the failed cases need to be investigated.

Instructions for digging out the information, accessing log-files, core-dumps etc. from the test servers is needed. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2015-10-23 15:02:37,Instructions for analyzing test results.,"After a test-suite has been run, the failed cases need to be investigated.

Instructions for digging out the information, accessing log-files, core-dumps etc. from the test servers is needed.",,0,0,0,0,0.0,"Instructions for analyzing test results. $end$ After a test-suite has been run, the failed cases need to be investigated.

Instructions for digging out the information, accessing log-files, core-dumps etc. from the test servers is needed. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1763,MXS-4261,Task,MXS,2022-08-26 11:22:59,MXS-3387,0,Extend usersInfo,The {{usersInfo}} command should support the {{showCredentials}} flag. ,,Extend usersInfo $end$ The {{usersInfo}} command should support the {{showCredentials}} flag.  $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-08-26 11:43:39,Extend usersInfo,The {{usersInfo}} command should support the {{showCredentials}} flag. ,,0,0,0,0,0.0,Extend usersInfo $end$ The {{usersInfo}} command should support the {{showCredentials}} flag.  $acceptance criteria:$,0,0,0,0,0,0,0,0.333333,485,44,0.0907217,19,0.0391753,13,0.0268041,9,0.0185567,9,0.0185567
1764,MXS-4270,New Feature,MXS,2022-09-01 11:21:21,MXS-2661,0,ed25519 authentication support,"with banking security requiremet being reviewed and upped throughout europe, more and more banks wants to move away from mariadb_nativepassword to ed25519.

bringing ed25519 to maxscale is already a most needed feature and will soon become a mandatory requirement.",,"ed25519 authentication support $end$ with banking security requiremet being reviewed and upped throughout europe, more and more banks wants to move away from mariadb_nativepassword to ed25519.

bringing ed25519 to maxscale is already a most needed feature and will soon become a mandatory requirement. $acceptance criteria:$",,Sylvain ARBAUDIE,Sylvain ARBAUDIE,Major,24,,0,3,2,3,0,1,0,,0,850,1,0,0,2022-12-07 11:19:46,Add ed25519 Authentication to maxscale,"with banking security requiremet being reviewed and upped throughout europe, more and more banks wants to move away from mariadb_nativepassword to ed25519.

bringing ed25519 to maxscale is already a most needed feature and will soon become a mandatory requirement.",,1,0,0,6,0.0851064,"Add ed25519 Authentication to maxscale $end$ with banking security requiremet being reviewed and upped throughout europe, more and more banks wants to move away from mariadb_nativepassword to ed25519.

bringing ed25519 to maxscale is already a most needed feature and will soon become a mandatory requirement. $acceptance criteria:$",1,1,1,0,0,0,1,2327.97,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1765,MXS-4277,New Feature,MXS,2022-09-07 06:49:41,,0,"iss field in JWT tokens is always ""maxscale""","The issuer field ({{iss}}) is always {{maxscale}} for all tokens. This makes it hard to figure out who actually created the token. A better alternative would be to construct it from the machine's hostname (or from {{admin_host}}) as well as the {{admin_port}} parameters to form a URL that points to the issuer.

The proposed approach with the defaults {{admin_host=127.0.0.1}} and {{admin_port=8989}} would result in the following issuer field:
{code}
""iss"": ""http://127.0.0.1:8989/v1/auth""
{code}
The only problem with this approach is that it prevents the tokens from being shared across multiple MaxScale instances which would otherwise be possible in 22.08 with a pre-shared symmetric key. For this reason, it might need to be made into a user-configurable string, especially if the value of {{admin_host}} isn't the externally visible hostname of the machine.",,"iss field in JWT tokens is always ""maxscale"" $end$ The issuer field ({{iss}}) is always {{maxscale}} for all tokens. This makes it hard to figure out who actually created the token. A better alternative would be to construct it from the machine's hostname (or from {{admin_host}}) as well as the {{admin_port}} parameters to form a URL that points to the issuer.

The proposed approach with the defaults {{admin_host=127.0.0.1}} and {{admin_port=8989}} would result in the following issuer field:
{code}
""iss"": ""http://127.0.0.1:8989/v1/auth""
{code}
The only problem with this approach is that it prevents the tokens from being shared across multiple MaxScale instances which would otherwise be possible in 22.08 with a pre-shared symmetric key. For this reason, it might need to be made into a user-configurable string, especially if the value of {{admin_host}} isn't the externally visible hostname of the machine. $acceptance criteria:$",,markus makela,markus makela,Minor,17,,0,2,0,1,0,1,0,,0,850,1,1,0,2023-04-04 10:29:08,"iss field in JWT tokens is always ""maxscale""","The issuer field ({{iss}}) is always {{maxscale}} for all tokens. This makes it hard to figure out who actually created the token. A better alternative would be to construct it from the machine's hostname (or from {{admin_host}}) as well as the {{admin_port}} parameters to form a URL that points to the issuer.

The proposed approach with the defaults {{admin_host=127.0.0.1}} and {{admin_port=8989}} would result in the following issuer field:
{code}
""iss"": ""http://127.0.0.1:8989/v1/auth""
{code}
The only problem with this approach is that it prevents the tokens from being shared across multiple MaxScale instances which would otherwise be possible in 22.08 with a pre-shared symmetric key. For this reason, it might need to be made into a user-configurable string, especially if the value of {{admin_host}} isn't the externally visible hostname of the machine.",,0,0,0,0,0.0,"iss field in JWT tokens is always ""maxscale"" $end$ The issuer field ({{iss}}) is always {{maxscale}} for all tokens. This makes it hard to figure out who actually created the token. A better alternative would be to construct it from the machine's hostname (or from {{admin_host}}) as well as the {{admin_port}} parameters to form a URL that points to the issuer.

The proposed approach with the defaults {{admin_host=127.0.0.1}} and {{admin_port=8989}} would result in the following issuer field:
{code}
""iss"": ""http://127.0.0.1:8989/v1/auth""
{code}
The only problem with this approach is that it prevents the tokens from being shared across multiple MaxScale instances which would otherwise be possible in 22.08 with a pre-shared symmetric key. For this reason, it might need to be made into a user-configurable string, especially if the value of {{admin_host}} isn't the externally visible hostname of the machine. $acceptance criteria:$",0,0,0,0,0,0,0,5019.65,138,16,0.115942,11,0.0797101,9,0.0652174,8,0.057971,7,0.0507246
1766,MXS-4285,New Feature,MXS,2022-09-08 11:34:59,,0,MaxGui should store and remember user preferences,MaxGui shoulr store and remember user preferences so that window sizes etc. are remembers from one session to the next.,,MaxGui should store and remember user preferences $end$ MaxGui shoulr store and remember user preferences so that window sizes etc. are remembers from one session to the next. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-10-10 06:21:17,MaxGui should store and remember user preferences,MaxGui shoulr store and remember user preferences so that window sizes etc. are remembers from one session to the next.,,0,0,0,0,0.0,MaxGui should store and remember user preferences $end$ MaxGui shoulr store and remember user preferences so that window sizes etc. are remembers from one session to the next. $acceptance criteria:$,0,0,0,0,0,0,0,762.767,486,44,0.090535,19,0.0390946,13,0.026749,9,0.0185185,9,0.0185185
1767,MXS-4296,Sub-Task,MXS,2022-09-16 08:52:49,,0,Show stopping state indicator,,,Show stopping state indicator $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,2,0,2,0,,0,850,0,0,0,2022-09-16 08:52:49,Stop the query immediately,"The query editor stops the query by sending `KILL QUERY thread_id` and leaves the connection itself intact. However, this may take time to stop the query and cause bad UX.
To make it faster,  the connection should be killed instead of the query. After killing the connection, clone the worksheet connection and bind it to that query tab.",,1,1,0,66,0.953846,"Stop the query immediately $end$ The query editor stops the query by sending `KILL QUERY thread_id` and leaves the connection itself intact. However, this may take time to stop the query and cause bad UX.
To make it faster,  the connection should be killed instead of the query. After killing the connection, clone the worksheet connection and bind it to that query tab. $acceptance criteria:$",2,1,1,1,1,1,1,0.0,93,34,0.365591,19,0.204301,14,0.150538,11,0.11828,11,0.11828
1768,MXS-4297,Sub-Task,MXS,2022-09-16 08:53:44,,0,Fix high memory usage between query tabs,,,Fix high memory usage between query tabs $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,0,0,2,0,1,0,,0,850,0,0,0,2022-09-16 08:53:44,Fix memory leak between query tabs,,,1,0,0,3,0.222222,Fix memory leak between query tabs $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,94,35,0.37234,20,0.212766,15,0.159574,12,0.12766,12,0.12766
1769,MXS-4298,Sub-Task,MXS,2022-09-16 08:55:24,,0,Add keyboard shortcut for stopping the query,,,Add keyboard shortcut for stopping the query $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-09-16 08:55:24,Add keyboard shortcut for stopping the query,,,0,0,0,0,0.0,Add keyboard shortcut for stopping the query $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,95,36,0.378947,20,0.210526,15,0.157895,12,0.126316,12,0.126316
1770,MXS-43,New Feature,MXS,2015-03-14 09:56:36,,0,Would you please make a official MaxScale docker image for us ?,"Would you please make a official MaxScale docker image for us ?

I tried my best to install MaxScale in these three days.
But i never success run it.

Please create a official docker image contains usable maxscale for us.
many thanks you guys.
",,"Would you please make a official MaxScale docker image for us ? $end$ Would you please make a official MaxScale docker image for us ?

I tried my best to install MaxScale in these three days.
But i never success run it.

Please create a official docker image contains usable maxscale for us.
many thanks you guys.
 $acceptance criteria:$",,zhifeng hu,zhifeng hu,Major,15,,0,1,0,3,0,0,0,,0,850,0,0,0,2017-07-27 10:02:17,Would you please make a official MaxScale docker image for us ?,"Would you please make a official MaxScale docker image for us ?

I tried my best to install MaxScale in these three days.
But i never success run it.

Please create a official docker image contains usable maxscale for us.
many thanks you guys.
",,0,0,0,0,0.0,"Would you please make a official MaxScale docker image for us ? $end$ Would you please make a official MaxScale docker image for us ?

I tried my best to install MaxScale in these three days.
But i never success run it.

Please create a official docker image contains usable maxscale for us.
many thanks you guys.
 $acceptance criteria:$",0,0,0,0,0,0,1,20784.1,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1771,MXS-4302,New Feature,MXS,2022-09-20 08:24:30,,0,Implement a way to specify a list of users or exclude a list of users,"This is required for all the clients who have multiple database users but only want to log queries for a few users or who want to exclude a few users from the list and capture queries by all others.

Currently, the QLA filter only supports one user per filter. It would be great if the user could do the following

- Implement regex support for users and exclude users


{code:java}
user=^user.*|^query_user.*|^dba.*
exclude=^dba_admin.*|user5
{code}

This would include all the user1, user2, user(n) and query_user1, query_user2 etc. but exclude user5 and all the dba_admin users. ",,"Implement a way to specify a list of users or exclude a list of users $end$ This is required for all the clients who have multiple database users but only want to log queries for a few users or who want to exclude a few users from the list and capture queries by all others.

Currently, the QLA filter only supports one user per filter. It would be great if the user could do the following

- Implement regex support for users and exclude users


{code:java}
user=^user.*|^query_user.*|^dba.*
exclude=^dba_admin.*|user5
{code}

This would include all the user1, user2, user(n) and query_user1, query_user2 etc. but exclude user5 and all the dba_admin users.  $acceptance criteria:$",,Faisal Saeed,Faisal Saeed,Major,12,,0,0,0,1,0,0,0,,0,850,0,0,0,2023-02-27 10:55:11,Implement a way to specify a list of users or exclude a list of users,"This is required for all the clients who have multiple database users but only want to log queries for a few users or who want to exclude a few users from the list and capture queries by all others.

Currently, the QLA filter only supports one user per filter. It would be great if the user could do the following

- Implement regex support for users and exclude users


{code:java}
user=^user.*|^query_user.*|^dba.*
exclude=^dba_admin.*|user5
{code}

This would include all the user1, user2, user(n) and query_user1, query_user2 etc. but exclude user5 and all the dba_admin users. ",,0,0,0,0,0.0,"Implement a way to specify a list of users or exclude a list of users $end$ This is required for all the clients who have multiple database users but only want to log queries for a few users or who want to exclude a few users from the list and capture queries by all others.

Currently, the QLA filter only supports one user per filter. It would be great if the user could do the following

- Implement regex support for users and exclude users


{code:java}
user=^user.*|^query_user.*|^dba.*
exclude=^dba_admin.*|user5
{code}

This would include all the user1, user2, user(n) and query_user1, query_user2 etc. but exclude user5 and all the dba_admin users.  $acceptance criteria:$",0,0,0,0,0,0,0,3842.5,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1772,MXS-4308,Task,MXS,2022-09-22 15:04:01,,0,Rebuild-server improvements,"1. Add more logging
 2. Improve error detection if possible. Test network link before backup transfer, check datadir after transfer.
3. Improve documentation",,"Rebuild-server improvements $end$ 1. Add more logging
 2. Improve error detection if possible. Test network link before backup transfer, check datadir after transfer.
3. Improve documentation $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-09-26 10:38:18,Rebuild-server improvements,"1. Add more logging
 2. Improve error detection if possible. Test network link before backup transfer, check datadir after transfer.
3. Improve documentation",,0,0,0,0,0.0,"Rebuild-server improvements $end$ 1. Add more logging
 2. Improve error detection if possible. Test network link before backup transfer, check datadir after transfer.
3. Improve documentation $acceptance criteria:$",0,0,0,0,0,0,1,91.5667,29,3,0.103448,2,0.0689655,1,0.0344828,1,0.0344828,0,0.0
1773,MXS-4309,Task,MXS,2022-09-23 05:52:28,,0,mxs2456_trx_replay_cap assumes a promotion order for servers,"The {{test_replay_failure}} section in {{mxs2456_trx_replay_cap}} assumes that the servers are promoted in the order they are declared. However, it does not check that failover has actually happened before proceeding to block the next node. This causes the test to fail as it assumes the third failover would be rejected but it ends up working as there is only two replay attempts. The right thing to do would be to check that failover has happened before proceeding to block the next node.",,"mxs2456_trx_replay_cap assumes a promotion order for servers $end$ The {{test_replay_failure}} section in {{mxs2456_trx_replay_cap}} assumes that the servers are promoted in the order they are declared. However, it does not check that failover has actually happened before proceeding to block the next node. This causes the test to fail as it assumes the third failover would be rejected but it ends up working as there is only two replay attempts. The right thing to do would be to check that failover has happened before proceeding to block the next node. $acceptance criteria:$",,markus makela,markus makela,Major,12,,0,0,0,4,0,0,0,,0,850,0,0,0,2022-12-05 08:01:03,mxs2456_trx_replay_cap assumes a promotion order for servers,"The {{test_replay_failure}} section in {{mxs2456_trx_replay_cap}} assumes that the servers are promoted in the order they are declared. However, it does not check that failover has actually happened before proceeding to block the next node. This causes the test to fail as it assumes the third failover would be rejected but it ends up working as there is only two replay attempts. The right thing to do would be to check that failover has happened before proceeding to block the next node.",,0,0,0,0,0.0,"mxs2456_trx_replay_cap assumes a promotion order for servers $end$ The {{test_replay_failure}} section in {{mxs2456_trx_replay_cap}} assumes that the servers are promoted in the order they are declared. However, it does not check that failover has actually happened before proceeding to block the next node. This causes the test to fail as it assumes the third failover would be rejected but it ends up working as there is only two replay attempts. The right thing to do would be to check that failover has happened before proceeding to block the next node. $acceptance criteria:$",0,0,0,0,0,0,1,1754.13,139,16,0.115108,11,0.0791367,9,0.0647482,8,0.057554,7,0.0503597
1774,MXS-4310,Sub-Task,MXS,2022-09-23 06:42:42,,0,Proof-of-Concept,Implement a minimal proof-of-concept implementation that demonstrates the idea behind the data loading.,,Proof-of-Concept $end$ Implement a minimal proof-of-concept implementation that demonstrates the idea behind the data loading. $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,12,0,0,0,,0,850,0,0,0,2022-09-23 06:42:42,Proof-of-Concept,Implement a minimal proof-of-concept implementation that demonstrates the idea behind the data loading.,,0,0,0,0,0.0,Proof-of-Concept $end$ Implement a minimal proof-of-concept implementation that demonstrates the idea behind the data loading. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,140,16,0.114286,11,0.0785714,9,0.0642857,8,0.0571429,7,0.05
1775,MXS-4311,Sub-Task,MXS,2022-09-23 06:43:29,,0,Design Document,Write a design document that outlines the plan for implementing the feature.,,Design Document $end$ Write a design document that outlines the plan for implementing the feature. $acceptance criteria:$,,markus makela,markus makela,Major,4,,0,1,0,12,0,0,0,,0,850,1,0,0,2022-09-23 06:43:29,Design Document,Write a design document that outlines the plan for implementing the feature.,,0,0,0,0,0.0,Design Document $end$ Write a design document that outlines the plan for implementing the feature. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,141,16,0.113475,11,0.0780142,9,0.0638298,8,0.0567376,7,0.0496454
1776,MXS-4320,New Feature,MXS,2022-09-27 08:06:07,,0,Let maxctrl show datetime values using local client timezone,"Right now maxctrl always shows datetime values using the GMT timezone (eg. in {{maxctrl list users}} output, using the HTTP Date header format.

This happens regardless of the local {{$TZ}} variable setting.

It would be nice to have such datetime values been shown in local time instead.",,"Let maxctrl show datetime values using local client timezone $end$ Right now maxctrl always shows datetime values using the GMT timezone (eg. in {{maxctrl list users}} output, using the HTTP Date header format.

This happens regardless of the local {{$TZ}} variable setting.

It would be nice to have such datetime values been shown in local time instead. $acceptance criteria:$",,Hartmut Holzgraefe,Hartmut Holzgraefe,Major,10,,1,0,1,1,0,0,0,,0,850,0,0,0,2023-01-23 12:44:11,Let maxctrl show datetime values using local client timezone,"Right now maxctrl always shows datetime values using the GMT timezone (eg. in {{maxctrl list users}} output, using the HTTP Date header format.

This happens regardless of the local {{$TZ}} variable setting.

It would be nice to have such datetime values been shown in local time instead.",,0,0,0,0,0.0,"Let maxctrl show datetime values using local client timezone $end$ Right now maxctrl always shows datetime values using the GMT timezone (eg. in {{maxctrl list users}} output, using the HTTP Date header format.

This happens regardless of the local {{$TZ}} variable setting.

It would be nice to have such datetime values been shown in local time instead. $acceptance criteria:$",0,0,0,0,0,0,0,2836.63,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1777,MXS-4323,Task,MXS,2022-09-28 08:25:41,,0,Review MaxGUI security,"Make sure our implementation passes the security checklist.
https://mariadbcorp.atlassian.net/wiki/spaces/SEC/pages/1650360360/Web+Application+Security
This is for SkySQL. 
",,"Review MaxGUI security $end$ Make sure our implementation passes the security checklist.
https://mariadbcorp.atlassian.net/wiki/spaces/SEC/pages/1650360360/Web+Application+Security
This is for SkySQL. 
 $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,15,,0,0,0,1,0,10,0,,0,850,0,0,0,2022-10-07 14:35:14,Review MaxGUI security,"Make sure our implementation passes the security checklist.
https://mariadbcorp.atlassian.net/wiki/spaces/SEC/pages/1650360360/Web+Application+Security",,0,10,0,4,0.266667,"Review MaxGUI security $end$ Make sure our implementation passes the security checklist.
https://mariadbcorp.atlassian.net/wiki/spaces/SEC/pages/1650360360/Web+Application+Security $acceptance criteria:$",10,1,0,0,0,0,0,222.15,96,36,0.375,20,0.208333,15,0.15625,12,0.125,12,0.125
1778,MXS-4330,New Feature,MXS,2022-09-29 12:00:25,,0,Xpand parallel replication does not work with maxscale,"I tried to setup xpand parallel replication and I this error. This is on the xpand master.

{code:java}
2022-09-29 10:54:11.949066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=214017 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56124) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_ROWS SLOW_START batch_size: 21474836480 low_bound: 15372286728091293014 high_bound: 18446744073709551615}
2022-09-29 10:54:11.951066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=215041 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56132) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_STATEMENTS SLOW_START batch_size: 21474836480 low_bound: 0 high_bound: 0}
2022-09-29 10:54:12.392068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56124): Session killed due to closed client socket: 
2022-09-29 10:54:12.393068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56132): Session killed due to closed client socket: 
{code}

Maxscale logs on the master:

{code:java}
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 334 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 197 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 120 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
{code}

I have added one database with 1 table with 2 rows in the master xpand.
Only the database and the table were replicated very slowly, but the two rows in the table never got replicated.
On the same xpands I changed the two maxscales images to version 6.2.3 then I setup the replication again and all the data was replicated.
",,"Xpand parallel replication does not work with maxscale $end$ I tried to setup xpand parallel replication and I this error. This is on the xpand master.

{code:java}
2022-09-29 10:54:11.949066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=214017 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56124) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_ROWS SLOW_START batch_size: 21474836480 low_bound: 15372286728091293014 high_bound: 18446744073709551615}
2022-09-29 10:54:11.951066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=215041 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56132) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_STATEMENTS SLOW_START batch_size: 21474836480 low_bound: 0 high_bound: 0}
2022-09-29 10:54:12.392068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56124): Session killed due to closed client socket: 
2022-09-29 10:54:12.393068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56132): Session killed due to closed client socket: 
{code}

Maxscale logs on the master:

{code:java}
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 334 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 197 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 120 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
{code}

I have added one database with 1 table with 2 rows in the master xpand.
Only the database and the table were replicated very slowly, but the two rows in the table never got replicated.
On the same xpands I changed the two maxscales images to version 6.2.3 then I setup the replication again and all the data was replicated.
 $acceptance criteria:$",,Hristiyan Nikolov,Hristiyan Nikolov,Major,23,,0,9,0,1,0,1,0,,0,850,2,1,0,2023-01-16 10:52:09,Xpand parallel replication does not work with maxscale,"I tried to setup xpand parallel replication and I this error. This is on the xpand master.

{code:java}
2022-09-29 10:54:11.949066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=214017 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56124) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_ROWS SLOW_START batch_size: 21474836480 low_bound: 15372286728091293014 high_bound: 18446744073709551615}
2022-09-29 10:54:11.951066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=215041 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56132) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_STATEMENTS SLOW_START batch_size: 21474836480 low_bound: 0 high_bound: 0}
2022-09-29 10:54:12.392068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56124): Session killed due to closed client socket: 
2022-09-29 10:54:12.393068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56132): Session killed due to closed client socket: 
{code}

Maxscale logs on the master:

{code:java}
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 334 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 197 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 120 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
{code}

I have added one database with 1 table with 2 rows in the master xpand.
Only the database and the table were replicated very slowly, but the two rows in the table never got replicated.
On the same xpands I changed the two maxscales images to version 6.2.3 then I setup the replication again and all the data was replicated.
",,0,0,0,0,0.0,"Xpand parallel replication does not work with maxscale $end$ I tried to setup xpand parallel replication and I this error. This is on the xpand master.

{code:java}
2022-09-29 10:54:11.949066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=214017 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56124) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_ROWS SLOW_START batch_size: 21474836480 low_bound: 15372286728091293014 high_bound: 18446744073709551615}
2022-09-29 10:54:11.951066 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: INFO mysql/replication/master.c:534 start_replication_master(): {conn_id=215041 closing=0 lock count=1}: Replication request from IPv4(10.156.15.221:56132) for xpand-bin: cid 0x6335778e10085806 pos { proto: clx current_commit_id: 0x6335778e10085806 current_event_id: 2 flags: WANT_STATEMENTS SLOW_START batch_size: 21474836480 low_bound: 0 high_bound: 0}
2022-09-29 10:54:12.392068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56124): Session killed due to closed client socket: 
2022-09-29 10:54:12.393068 UTC nid 1 hnn-xpand1-mxp-0.hnn-xpand1-mxp clxnode: WARNING mysql/replication/master.c:96 mysql_master_close(): Replication master error: slave IPv4(10.156.15.221:56132): Session killed due to closed client socket: 
{code}

Maxscale logs on the master:

{code:java}
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:10   error  : (54) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 334 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 197 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 120 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2
2022-09-29 12:09:29   error  : (60) Unexpected result state. cmd: 0x00, len: 28 server: @@Xpand-Monitor:node-2 (subsequent similar messages suppressed for 10000 milliseconds)
{code}

I have added one database with 1 table with 2 rows in the master xpand.
Only the database and the table were replicated very slowly, but the two rows in the table never got replicated.
On the same xpands I changed the two maxscales images to version 6.2.3 then I setup the replication again and all the data was replicated.
 $acceptance criteria:$",0,0,0,0,0,0,0,2614.85,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1779,MXS-4346,Task,MXS,2022-10-11 10:29:12,,0,mxs2750_com_stmt_exec randomly fails,,,mxs2750_com_stmt_exec randomly fails $end$ $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2022-10-11 10:29:12,mxs2750_com_stmt_exec randomly fails,,,0,0,0,0,0.0,mxs2750_com_stmt_exec randomly fails $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,142,16,0.112676,11,0.0774648,9,0.0633803,8,0.056338,7,0.0492958
1780,MXS-4357,Task,MXS,2022-10-20 05:50:25,,0,"Document how master_reconnection works, with examples","We have master_reconnect setting since 2.3.x, but current documentation:

https://mariadb.com/kb/en/mariadb-maxscale-25-readwritesplit/#master_reconnection

does not include any example on how the connection will work (what client side will get) when it's enabled. There are many preconditions listed, and one may assume (without reading that KB page) that connection to current master will be transparently re-routed to a new master, while this may not be true and the feature is more about changing read only status for the connections to promoted slave (correct me if I am wrong). 

Please, elaborate and show some use cases for this feature with examples (in KB and/or Enterprise Documentation).",,"Document how master_reconnection works, with examples $end$ We have master_reconnect setting since 2.3.x, but current documentation:

https://mariadb.com/kb/en/mariadb-maxscale-25-readwritesplit/#master_reconnection

does not include any example on how the connection will work (what client side will get) when it's enabled. There are many preconditions listed, and one may assume (without reading that KB page) that connection to current master will be transparently re-routed to a new master, while this may not be true and the feature is more about changing read only status for the connections to promoted slave (correct me if I am wrong). 

Please, elaborate and show some use cases for this feature with examples (in KB and/or Enterprise Documentation). $acceptance criteria:$",,Valerii Kravchuk,Valerii Kravchuk,Major,13,,0,1,0,1,0,1,0,,0,850,1,1,0,2022-11-07 11:21:06,"Document how master_reconnection works, with examples","We have master_reconnect setting since 2.3.x, but current documentation:

https://mariadb.com/kb/en/mariadb-maxscale-25-readwritesplit/#master_reconnection

does not include any example on how the connection will work (what client side will get) when it's enabled. There are many preconditions listed, and one may assume (without reading that KB page) that connection to current master will be transparently re-routed to a new master, while this may not be true and the feature is more about changing read only status for the connections to promoted slave (correct me if I am wrong). 

Please, elaborate and show some use cases for this feature with examples (in KB and/or Enterprise Documentation).",,0,0,0,0,0.0,"Document how master_reconnection works, with examples $end$ We have master_reconnect setting since 2.3.x, but current documentation:

https://mariadb.com/kb/en/mariadb-maxscale-25-readwritesplit/#master_reconnection

does not include any example on how the connection will work (what client side will get) when it's enabled. There are many preconditions listed, and one may assume (without reading that KB page) that connection to current master will be transparently re-routed to a new master, while this may not be true and the feature is more about changing read only status for the connections to promoted slave (correct me if I am wrong). 

Please, elaborate and show some use cases for this feature with examples (in KB and/or Enterprise Documentation). $acceptance criteria:$",0,0,0,0,0,0,0,437.5,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1781,MXS-4361,Task,MXS,2022-10-24 06:57:50,,0,Update documentation,,,Update documentation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-10-24 06:57:50,Update documentation,,,0,0,0,0,0.0,Update documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,487,44,0.0903491,19,0.0390144,13,0.026694,9,0.0184805,9,0.0184805
1782,MXS-4362,Task,MXS,2022-10-24 07:17:31,,0,Rework startup configuration handling,"At startup, MaxScale first ""sniffs"" the main configuration file, primarily in order to detect whether some default directories have been changed, makes initial configurations and starts the threads. Then it reads the configuration anew, this time processing additional static configuration files and runtime changes.
Now that the number of threads can be adjusted at runtime, it leads to MaxScale first creating as many threads as specified in the main configuration file and then later, if the number of threads at an earlier run was decreased, shut down threads. This is non-optimal. Things need to be changed so that no more than the desired number of threads are created at startup.",,"Rework startup configuration handling $end$ At startup, MaxScale first ""sniffs"" the main configuration file, primarily in order to detect whether some default directories have been changed, makes initial configurations and starts the threads. Then it reads the configuration anew, this time processing additional static configuration files and runtime changes.
Now that the number of threads can be adjusted at runtime, it leads to MaxScale first creating as many threads as specified in the main configuration file and then later, if the number of threads at an earlier run was decreased, shut down threads. This is non-optimal. Things need to be changed so that no more than the desired number of threads are created at startup. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,10,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-10-24 07:17:31,Rework startup configuration handling,"At startup, MaxScale first ""sniffs"" the main configuration file, primarily in order to detect whether some default directories have been changed, makes initial configurations and starts the threads. Then it reads the configuration anew, this time processing additional static configuration files and runtime changes.
Now that the number of threads can be adjusted at runtime, it leads to MaxScale first creating as many threads as specified in the main configuration file and then later, if the number of threads at an earlier run was decreased, shut down threads. This is non-optimal. Things need to be changed so that no more than the desired number of threads are created at startup.",,0,0,0,0,0.0,"Rework startup configuration handling $end$ At startup, MaxScale first ""sniffs"" the main configuration file, primarily in order to detect whether some default directories have been changed, makes initial configurations and starts the threads. Then it reads the configuration anew, this time processing additional static configuration files and runtime changes.
Now that the number of threads can be adjusted at runtime, it leads to MaxScale first creating as many threads as specified in the main configuration file and then later, if the number of threads at an earlier run was decreased, shut down threads. This is non-optimal. Things need to be changed so that no more than the desired number of threads are created at startup. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,488,44,0.0901639,19,0.0389344,13,0.0266393,9,0.0184426,9,0.0184426
1783,MXS-4366,Sub-Task,MXS,2022-10-25 07:16:35,,0,Allow async queries via the REST-API,"In order to implement the ETL mechanism, the REST-API must be extended with asynchronous SQL execution capabilities. This will also be of use for MXS-3727.",,"Allow async queries via the REST-API $end$ In order to implement the ETL mechanism, the REST-API must be extended with asynchronous SQL execution capabilities. This will also be of use for MXS-3727. $acceptance criteria:$",,markus makela,markus makela,Major,6,,0,1,0,12,0,1,0,,0,850,1,0,0,2022-10-25 07:16:35,Extend REST-API for ETL,"In order to implement the ETL mechanism, the REST-API must be extended with asynchronous SQL execution capabilities. This will also be of use for MXS-3727.",,1,0,0,8,0.1875,"Extend REST-API for ETL $end$ In order to implement the ETL mechanism, the REST-API must be extended with asynchronous SQL execution capabilities. This will also be of use for MXS-3727. $acceptance criteria:$",1,1,1,0,0,0,1,0.0,143,16,0.111888,11,0.0769231,9,0.0629371,8,0.0559441,7,0.048951
1784,MXS-4376,Sub-Task,MXS,2022-11-01 07:52:03,,0,Add REST-API endpoint for importing data,"Add the REST-API endpoints needed to perform the data imports.

The current design is to extend the existing {{/sql}} endpoint with a {{/sql/:id/import}} endpoint that uses async query execution to perform the import.",,"Add REST-API endpoint for importing data $end$ Add the REST-API endpoints needed to perform the data imports.

The current design is to extend the existing {{/sql}} endpoint with a {{/sql/:id/import}} endpoint that uses async query execution to perform the import. $acceptance criteria:$",,markus makela,markus makela,Major,5,,0,0,0,12,0,0,0,,0,850,0,0,0,2022-11-01 07:52:03,Add REST-API endpoint for importing data,"Add the REST-API endpoints needed to perform the data imports.

The current design is to extend the existing {{/sql}} endpoint with a {{/sql/:id/import}} endpoint that uses async query execution to perform the import.",,0,0,0,0,0.0,"Add REST-API endpoint for importing data $end$ Add the REST-API endpoints needed to perform the data imports.

The current design is to extend the existing {{/sql}} endpoint with a {{/sql/:id/import}} endpoint that uses async query execution to perform the import. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,144,17,0.118056,12,0.0833333,9,0.0625,8,0.0555556,7,0.0486111
1785,MXS-4380,Task,MXS,2022-11-07 11:23:40,,0,Debug assertion in mxs1926_killed_server,{{2022-11-07 11:33:43   error  : (2) debug assert at /home/timofey_turenko_mariadb_com/MaxScale/server/modules/protocol/MariaDB/mariadb_backend.cc:2603 failed: !session_is_load_active(m_session) || m_reply.state() == ReplyState::LOAD_DATA_END}},,Debug assertion in mxs1926_killed_server $end$ {{2022-11-07 11:33:43   error  : (2) debug assert at /home/timofey_turenko_mariadb_com/MaxScale/server/modules/protocol/MariaDB/mariadb_backend.cc:2603 failed: !session_is_load_active(m_session) || m_reply.state() == ReplyState::LOAD_DATA_END}} $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,2,0,2,0,2,0,,0,850,2,0,0,2022-11-07 11:23:40,Debug assertion in mxs1926_killed_server,"{code}
2022-11-07 11:33:43   error  : (2) debug assert at /home/timofey_turenko_mariadb_com/MaxScale/server/modules/protocol/MariaDB/mariadb_backend.cc:2603 failed: !session_is_load_active(m_session) || m_reply.state() == ReplyState::LOAD_DATA_END
{code}",,0,2,0,6,0.166667,"Debug assertion in mxs1926_killed_server $end$ {code}
2022-11-07 11:33:43   error  : (2) debug assert at /home/timofey_turenko_mariadb_com/MaxScale/server/modules/protocol/MariaDB/mariadb_backend.cc:2603 failed: !session_is_load_active(m_session) || m_reply.state() == ReplyState::LOAD_DATA_END
{code} $acceptance criteria:$",2,1,1,0,0,0,1,0.0,145,17,0.117241,12,0.0827586,9,0.062069,8,0.0551724,7,0.0482759
1786,MXS-4381,Task,MXS,2022-11-07 11:27:32,,0,Update logging,,,Update logging $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-11-07 11:27:32,Update logging,,,0,0,0,0,0.0,Update logging $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,489,44,0.0899796,19,0.0388548,13,0.0265849,9,0.0184049,9,0.0184049
1787,MXS-4382,Task,MXS,2022-11-07 11:28:31,,0,Test and document backup features,Test with real machines instead of VMs. Also add documentation.,,Test and document backup features $end$ Test with real machines instead of VMs. Also add documentation. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-11-07 11:28:31,Test and document backup features,Test with real machines instead of VMs. Also add documentation.,,0,0,0,0,0.0,Test and document backup features $end$ Test with real machines instead of VMs. Also add documentation. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,490,44,0.0897959,19,0.0387755,13,0.0265306,9,0.0183673,9,0.0183673
1788,MXS-4384,Task,MXS,2022-11-08 11:18:32,,0,Drop qc_query_has_clause,Not used anymore.,,Drop qc_query_has_clause $end$ Not used anymore. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-11-30 13:57:11,Drop qc_query_has_clause,Not used anymore.,,0,0,0,0,0.0,Drop qc_query_has_clause $end$ Not used anymore. $acceptance criteria:$,0,0,0,0,0,0,0,530.633,491,44,0.089613,19,0.0386965,13,0.0264766,9,0.0183299,9,0.0183299
1789,MXS-4394,Sub-Task,MXS,2022-11-14 07:19:35,,0,Redesign Query Editor UX,"The query editor UX needs to be redesigned so that ETL or table data compare and sync would fit with the look and feel of the query editor. Apart from that, these features would also be available in SkySQL, so the design needs to look reasonable for both.",,"Redesign Query Editor UX $end$ The query editor UX needs to be redesigned so that ETL or table data compare and sync would fit with the look and feel of the query editor. Apart from that, these features would also be available in SkySQL, so the design needs to look reasonable for both. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,4,,0,1,0,3,0,0,0,,0,850,1,0,0,2022-11-14 07:19:35,Redesign Query Editor UX,"The query editor UX needs to be redesigned so that ETL or table data compare and sync would fit with the look and feel of the query editor. Apart from that, these features would also be available in SkySQL, so the design needs to look reasonable for both.",,0,0,0,0,0.0,"Redesign Query Editor UX $end$ The query editor UX needs to be redesigned so that ETL or table data compare and sync would fit with the look and feel of the query editor. Apart from that, these features would also be available in SkySQL, so the design needs to look reasonable for both. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,97,37,0.381443,20,0.206186,15,0.154639,12,0.123711,12,0.123711
1790,MXS-4395,Sub-Task,MXS,2022-11-14 07:21:07,,0,Outline approaches for showing the diff and generate sync script,"To check if the tables are out of sync, `CHECKSUM TABLE` could be used for that.
Showing the diff may be challenging. A preliminary thought is to use the column key and `LIMIT` to navigate through the table. This approach may be slower and not user-friendly to see the diff but this will prevent MaxScale and the query editor from running out of memory.",,"Outline approaches for showing the diff and generate sync script $end$ To check if the tables are out of sync, `CHECKSUM TABLE` could be used for that.
Showing the diff may be challenging. A preliminary thought is to use the column key and `LIMIT` to navigate through the table. This approach may be slower and not user-friendly to see the diff but this will prevent MaxScale and the query editor from running out of memory. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,7,,0,1,0,3,0,2,0,,0,850,1,0,0,2022-11-14 07:21:07,Find out the way to display the diff in the query editor,"To check if the tables are out of sync, `CHECKSUM TABLE` could be used for that.
Showing the diff may be challenging. A preliminary thought is to use the column key and `LIMIT` to navigate through the table. This approach may be slower and not user-friendly to see the diff but this will prevent MaxScale
and the query editor from running out of memory.",,1,1,0,18,0.126582,"Find out the way to display the diff in the query editor $end$ To check if the tables are out of sync, `CHECKSUM TABLE` could be used for that.
Showing the diff may be challenging. A preliminary thought is to use the column key and `LIMIT` to navigate through the table. This approach may be slower and not user-friendly to see the diff but this will prevent MaxScale
and the query editor from running out of memory. $acceptance criteria:$",2,1,1,1,1,0,1,0.0,98,37,0.377551,20,0.204082,15,0.153061,12,0.122449,12,0.122449
1791,MXS-4400,Task,MXS,2022-11-16 07:13:14,,0,Adjust some texts of the Query Editor,https://jira.mariadb.org/browse/DBAAS-11851,,Adjust some texts of the Query Editor $end$ https://jira.mariadb.org/browse/DBAAS-11851 $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2022-11-16 14:38:37,Adjust Query Editor's leaving page message  ,https://jira.mariadb.org/browse/DBAAS-11851,,1,0,0,9,0.6,Adjust Query Editor's leaving page message   $end$ https://jira.mariadb.org/browse/DBAAS-11851 $acceptance criteria:$,1,1,1,0,0,0,1,7.41667,99,38,0.383838,21,0.212121,16,0.161616,13,0.131313,12,0.121212
1792,MXS-4405,Task,MXS,2022-11-21 07:34:48,,0,Replace Query Editor object states sync methods with Vuex ORM,"The state modules for Query Editor are typically relational objects. 
There is an[existing library|https://vuex-orm.org] for having ORM Object-Relational Mapping in the Vuex store. It would be better to replace the current state's synchronized methods with ORM to improve maintainability, scalability, and readability.",,"Replace Query Editor object states sync methods with Vuex ORM $end$ The state modules for Query Editor are typically relational objects. 
There is an[existing library|https://vuex-orm.org] for having ORM Object-Relational Mapping in the Vuex store. It would be better to replace the current state's synchronized methods with ORM to improve maintainability, scalability, and readability. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-11-21 07:34:54,Replace Query Editor object states sync methods with Vuex ORM,"The state modules for Query Editor are typically relational objects. 
There is an[existing library|https://vuex-orm.org] for having ORM Object-Relational Mapping in the Vuex store. It would be better to replace the current state's synchronized methods with ORM to improve maintainability, scalability, and readability.",,0,0,0,0,0.0,"Replace Query Editor object states sync methods with Vuex ORM $end$ The state modules for Query Editor are typically relational objects. 
There is an[existing library|https://vuex-orm.org] for having ORM Object-Relational Mapping in the Vuex store. It would be better to replace the current state's synchronized methods with ORM to improve maintainability, scalability, and readability. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,100,39,0.39,22,0.22,16,0.16,13,0.13,12,0.12
1793,MXS-4407,Sub-Task,MXS,2022-11-21 11:17:49,,0,Implement MariaDB migration,,,Implement MariaDB migration $end$ $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,12,0,0,0,,0,850,0,0,0,2022-11-21 11:17:49,Implement MariaDB migration,,,0,0,0,0,0.0,Implement MariaDB migration $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,146,18,0.123288,13,0.0890411,9,0.0616438,8,0.0547945,7,0.0479452
1794,MXS-4408,Sub-Task,MXS,2022-11-21 11:40:50,,0,Add PRD,PRD document for UI/UX design,,Add PRD $end$ PRD document for UI/UX design $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,3,0,0,0,,0,850,0,0,0,2022-11-21 11:40:50,Add PRD,PRD document for UI/UX design,,0,0,0,0,0.0,Add PRD $end$ PRD document for UI/UX design $acceptance criteria:$,0,0,0,0,0,0,1,0.0,101,39,0.386139,22,0.217822,16,0.158416,13,0.128713,12,0.118812
1795,MXS-4417,Sub-Task,MXS,2022-11-25 15:00:05,,0,Document new REST-API endpoints,,,Document new REST-API endpoints $end$ $acceptance criteria:$,,markus makela,markus makela,Major,4,,0,0,0,12,0,0,0,,0,850,0,0,0,2022-11-25 15:00:05,Document new REST-API endpoints,,,0,0,0,0,0.0,Document new REST-API endpoints $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,147,18,0.122449,13,0.0884354,9,0.0612245,8,0.0544218,7,0.047619
1796,MXS-4422,Task,MXS,2022-12-01 06:46:18,,0,Move the classification data from GWBUF's shared buffer to GWBUF itself.,"A proper fix for MXS-4421
",,"Move the classification data from GWBUF's shared buffer to GWBUF itself. $end$ A proper fix for MXS-4421
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,1,0,1,3,0,0,0,,0,850,0,0,0,2022-12-01 06:46:52,Move the classification data from GWBUF's shared buffer to GWBUF itself.,"A proper fix for MXS-4421
",,0,0,0,0,0.0,"Move the classification data from GWBUF's shared buffer to GWBUF itself. $end$ A proper fix for MXS-4421
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,492,44,0.0894309,19,0.0386179,13,0.0264228,9,0.0182927,9,0.0182927
1797,MXS-4424,Sub-Task,MXS,2022-12-02 14:11:47,,0,Implement Postgres migration,,,Implement Postgres migration $end$ $acceptance criteria:$,,markus makela,markus makela,Major,4,,0,0,0,12,0,0,0,,0,850,0,0,0,2022-12-02 14:11:47,Implement Postgres migration,,,0,0,0,0,0.0,Implement Postgres migration $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,148,18,0.121622,13,0.0878378,9,0.0608108,8,0.0540541,7,0.0472973
1798,MXS-4430,New Feature,MXS,2022-12-07 07:03:27,,0,ETL/Data Migration Service GUI,PRD: https://docs.google.com/document/d/1cs0P3aK46Jpq3a_8hI71at6tybU2iVRp7DNCd3jv0rg/edit?usp=sharing,,ETL/Data Migration Service GUI $end$ PRD: https://docs.google.com/document/d/1cs0P3aK46Jpq3a_8hI71at6tybU2iVRp7DNCd3jv0rg/edit?usp=sharing $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,14,,0,0,2,5,0,1,5,,0,850,0,0,0,2022-12-07 07:03:27,ETL GUI,PRD: https://docs.google.com/document/d/1cs0P3aK46Jpq3a_8hI71at6tybU2iVRp7DNCd3jv0rg/edit?usp=sharing,,1,0,0,4,0.428571,ETL GUI $end$ PRD: https://docs.google.com/document/d/1cs0P3aK46Jpq3a_8hI71at6tybU2iVRp7DNCd3jv0rg/edit?usp=sharing $acceptance criteria:$,1,1,0,0,0,0,1,0.0,102,39,0.382353,22,0.215686,16,0.156863,13,0.127451,12,0.117647
1799,MXS-4431,Task,MXS,2022-12-07 10:55:23,,0,Write additional tests for MXS-3269,,,Write additional tests for MXS-3269 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2022-12-07 10:55:23,Write additional tests for MXS-3269,,,0,0,0,0,0.0,Write additional tests for MXS-3269 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,493,44,0.0892495,19,0.0385396,13,0.0263692,9,0.0182556,9,0.0182556
1800,MXS-4433,Task,MXS,2022-12-07 11:20:18,,0,Multiarchitecture Docker builds,,,Multiarchitecture Docker builds $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2022-12-07 11:20:18,Multiarchitecture Docker builds,,,0,0,0,0,0.0,Multiarchitecture Docker builds $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,494,44,0.0890688,19,0.0384615,13,0.0263158,9,0.0182186,9,0.0182186
1801,MXS-4437,Sub-Task,MXS,2022-12-08 08:42:54,,0,Implement generic ODBC migration,"A ""fallback"" migration implementation for unknown source database types can be created by using the ODBC catalog functions. This allows all ODBC data sources to be used for ETL but the resulting SQL might require updates from the user to fully work. It'll still serve as a good starting point to which manual edits can be done. ",,"Implement generic ODBC migration $end$ A ""fallback"" migration implementation for unknown source database types can be created by using the ODBC catalog functions. This allows all ODBC data sources to be used for ETL but the resulting SQL might require updates from the user to fully work. It'll still serve as a good starting point to which manual edits can be done.  $acceptance criteria:$",,markus makela,markus makela,Major,4,,0,0,0,12,0,0,0,,0,850,0,0,0,2022-12-08 08:42:54,Implement generic ODBC migration,"A ""fallback"" migration implementation for unknown source database types can be created by using the ODBC catalog functions. This allows all ODBC data sources to be used for ETL but the resulting SQL might require updates from the user to fully work. It'll still serve as a good starting point to which manual edits can be done. ",,0,0,0,0,0.0,"Implement generic ODBC migration $end$ A ""fallback"" migration implementation for unknown source database types can be created by using the ODBC catalog functions. This allows all ODBC data sources to be used for ETL but the resulting SQL might require updates from the user to fully work. It'll still serve as a good starting point to which manual edits can be done.  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,149,18,0.120805,13,0.0872483,9,0.0604027,8,0.0536913,7,0.0469799
1802,MXS-4441,Sub-Task,MXS,2022-12-12 08:34:20,,0,ETL GUI high-level design,,,ETL GUI high-level design $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,5,0,0,0,,0,850,0,0,0,2022-12-12 08:34:20,ETL GUI high-level design,,,0,0,0,0,0.0,ETL GUI high-level design $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,103,40,0.38835,22,0.213592,16,0.15534,13,0.126214,12,0.116505
1803,MXS-4443,Sub-Task,MXS,2022-12-13 09:40:38,,0,Implement ETL task model,"A model with the following fields:
+ id: string UUID
+ name: string
+ status: string
+ active_stage_index: number",,"Implement ETL task model $end$ A model with the following fields:
+ id: string UUID
+ name: string
+ status: string
+ active_stage_index: number $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,5,0,0,0,,0,850,0,0,0,2022-12-13 09:40:38,Implement ETL task model,"A model with the following fields:
+ id: string UUID
+ name: string
+ status: string
+ active_stage_index: number",,0,0,0,0,0.0,"Implement ETL task model $end$ A model with the following fields:
+ id: string UUID
+ name: string
+ status: string
+ active_stage_index: number $acceptance criteria:$",0,0,0,0,0,0,1,0.0,104,40,0.384615,22,0.211538,16,0.153846,13,0.125,12,0.115385
1804,MXS-4444,Sub-Task,MXS,2022-12-13 09:42:24,,0,Implement ETL task functions,"etl_task functions
+ add():  Add an ETL task
+ view(id): View a running, canceled, error, or done task
+ cancel(id): Cancel a running task
+ delete(id): Delete a canceled, error, or done task",,"Implement ETL task functions $end$ etl_task functions
+ add():  Add an ETL task
+ view(id): View a running, canceled, error, or done task
+ cancel(id): Cancel a running task
+ delete(id): Delete a canceled, error, or done task $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,5,0,0,0,,0,850,0,0,0,2022-12-13 09:42:24,Implement ETL task functions,"etl_task functions
+ add():  Add an ETL task
+ view(id): View a running, canceled, error, or done task
+ cancel(id): Cancel a running task
+ delete(id): Delete a canceled, error, or done task",,0,0,0,0,0.0,"Implement ETL task functions $end$ etl_task functions
+ add():  Add an ETL task
+ view(id): View a running, canceled, error, or done task
+ cancel(id): Cancel a running task
+ delete(id): Delete a canceled, error, or done task $acceptance criteria:$",0,0,0,0,0,0,1,0.0,105,40,0.380952,22,0.209524,16,0.152381,13,0.12381,12,0.114286
1805,MXS-4445,Sub-Task,MXS,2022-12-13 09:43:55,,0,Implement connection binding function for ETL,"+ openSource(obj): ODBC inputs.
+ testSourceConnection()
+ openDestination(obj): MaxScale server connection.
+ testDestinationConnection()",,"Implement connection binding function for ETL $end$ + openSource(obj): ODBC inputs.
+ testSourceConnection()
+ openDestination(obj): MaxScale server connection.
+ testDestinationConnection() $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,5,0,0,0,,0,850,0,0,0,2022-12-13 09:43:55,Implement connection binding function for ETL,"+ openSource(obj): ODBC inputs.
+ testSourceConnection()
+ openDestination(obj): MaxScale server connection.
+ testDestinationConnection()",,0,0,0,0,0.0,"Implement connection binding function for ETL $end$ + openSource(obj): ODBC inputs.
+ testSourceConnection()
+ openDestination(obj): MaxScale server connection.
+ testDestinationConnection() $acceptance criteria:$",0,0,0,0,0,0,1,0.0,106,40,0.377358,22,0.207547,16,0.150943,13,0.122642,12,0.113208
1806,MXS-4446,Sub-Task,MXS,2022-12-13 09:45:14,,0,Implement a treeview with checkbox for schema tree,,,Implement a treeview with checkbox for schema tree $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,3,,0,0,0,5,0,0,0,,0,850,0,0,0,2022-12-13 09:45:14,Implement a treeview with checkbox for schema tree,,,0,0,0,0,0.0,Implement a treeview with checkbox for schema tree $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,107,40,0.373832,22,0.205607,16,0.149533,13,0.121495,12,0.11215
1807,MXS-4448,Task,MXS,2022-12-13 12:10:52,,0,Migrate chart.js and its plugins to use Chart.js 3.X,"chart.js to 3.x
chartjs-plugin-streaming to 2.x
chartjs-plugin-trendline to 2.x
After that `vue-moment, moment-duration-format, moment-locales-webpack-plugin` packages can be replaced with luxon.",,"Migrate chart.js and its plugins to use Chart.js 3.X $end$ chart.js to 3.x
chartjs-plugin-streaming to 2.x
chartjs-plugin-trendline to 2.x
After that `vue-moment, moment-duration-format, moment-locales-webpack-plugin` packages can be replaced with luxon. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Minor,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2023-02-21 14:15:43,Migrate chart.js and its plugins to use Chart.js 3.X,"chart.js to 3.x
chartjs-plugin-streaming to 2.x
chartjs-plugin-trendline to 2.x
After that `vue-moment, moment-duration-format, moment-locales-webpack-plugin` packages can be replaced with luxon.",,0,0,0,0,0.0,"Migrate chart.js and its plugins to use Chart.js 3.X $end$ chart.js to 3.x
chartjs-plugin-streaming to 2.x
chartjs-plugin-trendline to 2.x
After that `vue-moment, moment-duration-format, moment-locales-webpack-plugin` packages can be replaced with luxon. $acceptance criteria:$",0,0,0,0,0,0,0,1682.07,108,40,0.37037,22,0.203704,16,0.148148,13,0.12037,12,0.111111
1808,MXS-4456,Task,MXS,2022-12-16 20:42:44,,0,Modules may end up calling clientReply directly in routeQuery,"The way the routing currently works is that the client protocol's expected response count is incremented before the call to routeQuery is done. This avoids the problem where the response count goes negative if a call to clientReply is done inside the routeQuery call.

Modify the modules that do this to instead return it after the function has finished. Mechanisms for this are already available.",,"Modules may end up calling clientReply directly in routeQuery $end$ The way the routing currently works is that the client protocol's expected response count is incremented before the call to routeQuery is done. This avoids the problem where the response count goes negative if a call to clientReply is done inside the routeQuery call.

Modify the modules that do this to instead return it after the function has finished. Mechanisms for this are already available. $acceptance criteria:$",,markus makela,markus makela,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2023-02-27 11:04:22,Modules may end up calling clientReply directly in routeQuery,"The way the routing currently works is that the client protocol's expected response count is incremented before the call to routeQuery is done. This avoids the problem where the response count goes negative if a call to clientReply is done inside the routeQuery call.

Modify the modules that do this to instead return it after the function has finished. Mechanisms for this are already available.",,0,0,0,0,0.0,"Modules may end up calling clientReply directly in routeQuery $end$ The way the routing currently works is that the client protocol's expected response count is incremented before the call to routeQuery is done. This avoids the problem where the response count goes negative if a call to clientReply is done inside the routeQuery call.

Modify the modules that do this to instead return it after the function has finished. Mechanisms for this are already available. $acceptance criteria:$",0,0,0,0,0,0,0,1742.35,150,18,0.12,13,0.0866667,9,0.06,8,0.0533333,7,0.0466667
1809,MXS-4468,Sub-Task,MXS,2023-01-03 10:50:45,,0,Test MariaDB-to-MariaDB ETL implementation,,,Test MariaDB-to-MariaDB ETL implementation $end$ $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,12,0,0,0,,0,850,0,0,0,2023-01-03 10:50:45,Test MariaDB-to-MariaDB ETL implementation,,,0,0,0,0,0.0,Test MariaDB-to-MariaDB ETL implementation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,151,18,0.119205,13,0.0860927,9,0.0596026,8,0.0529801,7,0.0463576
1810,MXS-4482,Sub-Task,MXS,2023-01-20 13:16:07,,0,Test PostgreSQL-to-MariaDB ETL implementation,,,Test PostgreSQL-to-MariaDB ETL implementation $end$ $acceptance criteria:$,,markus makela,markus makela,Major,3,,0,0,0,12,0,0,0,,0,850,0,0,0,2023-01-20 13:16:07,Test PostgreSQL-to-MariaDB ETL implementation,,,0,0,0,0,0.0,Test PostgreSQL-to-MariaDB ETL implementation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,152,18,0.118421,13,0.0855263,9,0.0592105,8,0.0526316,7,0.0460526
1811,MXS-4492,Task,MXS,2023-01-30 11:48:50,,0,Update copyright message,Copyright message in MaxScale source code should be updated.,,Update copyright message $end$ Copyright message in MaxScale source code should be updated. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,1,0,1,0,0,0,,0,850,1,0,0,2023-01-30 11:48:53,Update copyright message,Copyright message in MaxScale source code should be updated.,,0,0,0,0,0.0,Update copyright message $end$ Copyright message in MaxScale source code should be updated. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,495,44,0.0888889,19,0.0383838,13,0.0262626,9,0.0181818,9,0.0181818
1812,MXS-4496,Task,MXS,2023-01-31 11:58:10,,0,Limit resultset read size,"In prior versions, MaxScale would read resultset data from server-side socket without limit. In practice, the read would end when the socket was empty, which would then allow more data to be sent by the server. This is usually not an issue as the data is then written to the client socket. However, if client connection is a bottleneck, quite a bit of data could end up stored in the client-side write buffer.

The resultset read code could instead check the size of the client-side write queue, and limit the amount of data read such that the write queue cannot significantly overshoot the writeq_high_water-setting.

Other optimizations to related IO may also be added.",,"Limit resultset read size $end$ In prior versions, MaxScale would read resultset data from server-side socket without limit. In practice, the read would end when the socket was empty, which would then allow more data to be sent by the server. This is usually not an issue as the data is then written to the client socket. However, if client connection is a bottleneck, quite a bit of data could end up stored in the client-side write buffer.

The resultset read code could instead check the size of the client-side write queue, and limit the amount of data read such that the write queue cannot significantly overshoot the writeq_high_water-setting.

Other optimizations to related IO may also be added. $acceptance criteria:$",,Esa Korhonen,Esa Korhonen,Minor,5,,1,0,1,1,0,0,0,,0,850,0,0,0,2023-01-31 11:58:21,Limit resultset read size,"In prior versions, MaxScale would read resultset data from server-side socket without limit. In practice, the read would end when the socket was empty, which would then allow more data to be sent by the server. This is usually not an issue as the data is then written to the client socket. However, if client connection is a bottleneck, quite a bit of data could end up stored in the client-side write buffer.

The resultset read code could instead check the size of the client-side write queue, and limit the amount of data read such that the write queue cannot significantly overshoot the writeq_high_water-setting.

Other optimizations to related IO may also be added.",,0,0,0,0,0.0,"Limit resultset read size $end$ In prior versions, MaxScale would read resultset data from server-side socket without limit. In practice, the read would end when the socket was empty, which would then allow more data to be sent by the server. This is usually not an issue as the data is then written to the client socket. However, if client connection is a bottleneck, quite a bit of data could end up stored in the client-side write buffer.

The resultset read code could instead check the size of the client-side write queue, and limit the amount of data read such that the write queue cannot significantly overshoot the writeq_high_water-setting.

Other optimizations to related IO may also be added. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,30,3,0.1,2,0.0666667,1,0.0333333,1,0.0333333,0,0.0
1813,MXS-4516,Task,MXS,2023-02-20 11:28:15,,0,Profile and test the cache modifications.,,,Profile and test the cache modifications. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2023-02-20 11:28:15,Profile and test the cache modifications.,,,0,0,0,0,0.0,Profile and test the cache modifications. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,496,44,0.0887097,19,0.0383065,13,0.0262097,9,0.0181452,9,0.0181452
1814,MXS-4517,Task,MXS,2023-02-20 11:41:58,,0,Profile 23.02,,,Profile 23.02 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2023-02-20 11:41:58,Profile 23.02,,,0,0,0,0,0.0,Profile 23.02 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,497,44,0.0885312,19,0.0382294,13,0.0261569,9,0.0181087,9,0.0181087
1815,MXS-4518,Task,MXS,2023-02-20 11:42:08,,0,Test semi-sync replication in connector-c,Relates to CONC-470.,,Test semi-sync replication in connector-c $end$ Relates to CONC-470. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,0,1,1,0,0,0,,0,850,0,0,0,2023-02-20 11:42:08,Test semi-sync replication in connector-c,Relates to CONC-470.,,0,0,0,0,0.0,Test semi-sync replication in connector-c $end$ Relates to CONC-470. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,153,18,0.117647,13,0.0849673,9,0.0588235,8,0.0522876,7,0.0457516
1816,MXS-4523,Task,MXS,2023-02-22 11:57:56,,0,Classifier cleanup,,,Classifier cleanup $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2023-02-22 11:58:11,Classifier cleanup,,,0,0,0,0,0.0,Classifier cleanup $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,498,44,0.0883534,19,0.0381526,13,0.0261044,9,0.0180723,9,0.0180723
1817,MXS-4528,Task,MXS,2023-02-27 07:28:15,MXS-4574,0,Postgres client protocol support,,,Postgres client protocol support $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,3,0,0,0,,0,850,0,0,0,2023-02-27 10:55:48,Postgres client protocol support,,,0,0,0,0,0.0,Postgres client protocol support $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.45,499,44,0.0881764,19,0.0380762,13,0.0260521,9,0.0180361,9,0.0180361
1818,MXS-4529,Task,MXS,2023-02-27 07:28:35,MXS-4574,0,Postgres backend protocol support,,,Postgres backend protocol support $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,9,,0,0,0,3,0,0,0,,0,850,0,0,0,2023-02-27 10:55:44,Postgres backend protocol support,,,0,0,0,0,0.0,Postgres backend protocol support $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.45,500,44,0.088,19,0.038,13,0.026,9,0.018,9,0.018
1819,MXS-4530,Task,MXS,2023-02-27 07:29:19,,0,Postgres monitor,,,Postgres monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,2,0,0,0,,0,850,1,0,0,2023-02-27 10:55:42,Postgres monitor,,,0,0,0,0,0.0,Postgres monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.43333,501,44,0.0878244,19,0.0379242,13,0.0259481,9,0.0179641,9,0.0179641
1820,MXS-4532,Task,MXS,2023-02-27 07:30:22,MXS-4574,0,Postgres filter/router support,,,Postgres filter/router support $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,0,3,0,0,0,,0,850,0,0,0,2023-02-27 10:55:37,Postgres filter/router support,,,0,0,0,0,0.0,Postgres filter/router support $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.41667,502,44,0.0876494,19,0.0378486,13,0.0258964,9,0.0179283,9,0.0179283
1821,MXS-4533,Task,MXS,2023-02-27 07:30:50,MXS-4574,0,Postgres authentication,,,Postgres authentication $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,12,,0,0,0,3,0,0,0,,0,850,0,0,0,2023-03-20 11:27:01,Postgres authentication,,,0,0,0,0,0.0,Postgres authentication $end$ $acceptance criteria:$,0,0,0,0,0,0,1,507.933,503,44,0.0874752,19,0.0377734,13,0.0258449,9,0.0178926,9,0.0178926
1822,MXS-4534,Sub-Task,MXS,2023-02-27 12:14:10,,0,Add PRD,,,Add PRD $end$ $acceptance criteria:$,,Duong Thien Ly,Duong Thien Ly,Major,4,,0,1,0,5,0,0,0,,0,850,1,0,0,2023-02-27 12:14:10,Add PRD,,,0,0,0,0,0.0,Add PRD $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,109,40,0.366972,22,0.201835,16,0.146789,13,0.119266,12,0.110092
1823,MXS-4537,Sub-Task,MXS,2023-03-02 08:24:27,,0,Create ERD worksheet,"Similar to the Data Migration task, the ERD task will have its own dedicated worksheet.",,"Create ERD worksheet $end$ Similar to the Data Migration task, the ERD task will have its own dedicated worksheet. $acceptance criteria:$",,Duong Thien Ly,Duong Thien Ly,Major,2,,0,0,0,5,0,0,0,,0,850,0,0,0,2023-03-02 08:24:27,Create ERD worksheet,"Similar to the Data Migration task, the ERD task will have its own dedicated worksheet.",,0,0,0,0,0.0,"Create ERD worksheet $end$ Similar to the Data Migration task, the ERD task will have its own dedicated worksheet. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,110,40,0.363636,22,0.2,16,0.145455,13,0.118182,12,0.109091
1824,MXS-4558,Task,MXS,2023-03-16 14:32:52,MXS-4574,0,Postgres user account manager,,,Postgres user account manager $end$ $acceptance criteria:$,,Esa Korhonen,Esa Korhonen,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2023-03-20 11:26:51,Postgres user account manager,,,0,0,0,0,0.0,Postgres user account manager $end$ $acceptance criteria:$,0,0,0,0,0,0,1,92.8833,31,3,0.0967742,2,0.0645161,1,0.0322581,1,0.0322581,0,0.0
1825,MXS-457,Sub-Task,MXS,2015-11-10 19:28:50,,0,Test MaxScale with MySQL 5.1,,,Test MaxScale with MySQL 5.1 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2015-11-10 19:28:50,Test MaxScale with MySQL 5.1,,,0,0,0,0,0.0,Test MaxScale with MySQL 5.1 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,9,4,0.444444,1,0.111111,0,0.0,0,0.0,0,0.0
1826,MXS-4576,New Feature,MXS,2023-04-04 10:33:05,MXS-4574,0,Implement xrouter,,,Implement xrouter $end$ $acceptance criteria:$,,markus makela,markus makela,Major,9,,0,0,0,2,0,0,0,,0,850,0,0,0,2023-04-04 10:33:05,Implement xrouter,,,0,0,0,0,0.0,Implement xrouter $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,154,18,0.116883,13,0.0844156,9,0.0584416,8,0.0519481,7,0.0454545
1827,MXS-458,Sub-Task,MXS,2015-11-10 19:29:28,,0,Test MaxScale with MySQL/MariaDB 5.5,,,Test MaxScale with MySQL/MariaDB 5.5 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2015-11-10 19:29:28,Test MaxScale with MySQL/MariaDB 5.5,,,0,0,0,0,0.0,Test MaxScale with MySQL/MariaDB 5.5 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,10,4,0.4,1,0.1,0,0.0,0,0.0,0,0.0
1828,MXS-459,Sub-Task,MXS,2015-11-10 19:30:00,,0,Test MaxScale with MariaDB 10.0,,,Test MaxScale with MariaDB 10.0 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,1,0,2,0,0,0,,0,850,1,0,0,2015-11-10 19:30:00,Test MaxScale with MariaDB 10.0,,,0,0,0,0,0.0,Test MaxScale with MariaDB 10.0 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,11,4,0.363636,1,0.0909091,0,0.0,0,0.0,0,0.0
1829,MXS-460,Sub-Task,MXS,2015-11-10 19:30:34,,0,Test MaxScale with MariaDB 10.1,,,Test MaxScale with MariaDB 10.1 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2015-11-10 19:30:34,Test MaxScale with MariaDB 10.1,,,0,0,0,0,0.0,Test MaxScale with MariaDB 10.1 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,12,4,0.333333,1,0.0833333,0,0.0,0,0.0,0,0.0
1830,MXS-461,Sub-Task,MXS,2015-11-10 19:32:03,,0,Test MaxScale with MySQL 5.6,,,Test MaxScale with MySQL 5.6 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2015-11-10 19:32:03,Test MaxScale with MySQL 5.6,,,0,0,0,0,0.0,Test MaxScale with MySQL 5.6 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,13,4,0.307692,1,0.0769231,0,0.0,0,0.0,0,0.0
1831,MXS-462,Sub-Task,MXS,2015-11-10 19:32:26,,0,Test MaxScale with MySQL 5.7,,,Test MaxScale with MySQL 5.7 $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2015-11-10 19:32:26,Test MaxScale with MySQL 5.7,,,0,0,0,0,0.0,Test MaxScale with MySQL 5.7 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,14,4,0.285714,1,0.0714286,0,0.0,0,0.0,0,0.0
1832,MXS-468,Sub-Task,MXS,2015-11-12 12:15:59,,0,Release testing task (Multijob task),"""one click"" MultiJob Jenkins task to run all basic configurations",,"Release testing task (Multijob task) $end$ ""one click"" MultiJob Jenkins task to run all basic configurations $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2015-11-12 12:15:59,Release testing task (Multijob task),"""one click"" MultiJob Jenkins task to run all basic configurations",,0,0,0,0,0.0,"Release testing task (Multijob task) $end$ ""one click"" MultiJob Jenkins task to run all basic configurations $acceptance criteria:$",0,0,0,0,0,0,1,0.0,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1833,MXS-483,New Feature,MXS,2015-11-23 17:57:28,MXS-482,0,Change Data Listener Protocol Plugin,"(1) Create a skeleton protocol listener plugin - that calls the hooks into authentication plugin using API defined by MXS-486
(2) Implement the [MaxScale change data listener  protocol|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] in the plugin.
(3) Create a sample client that uses the [MaxScale change data listener protocol |https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] to request change data from MaxScale",,"Change Data Listener Protocol Plugin $end$ (1) Create a skeleton protocol listener plugin - that calls the hooks into authentication plugin using API defined by MXS-486
(2) Implement the [MaxScale change data listener  protocol|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] in the plugin.
(3) Create a sample client that uses the [MaxScale change data listener protocol |https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] to request change data from MaxScale $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,23,,0,0,0,6,0,0,0,,0,850,0,0,0,2016-01-12 12:26:10,Change Data Listener Protocol Plugin,"(1) Create a skeleton protocol listener plugin - that calls the hooks into authentication plugin using API defined by MXS-486
(2) Implement the [MaxScale change data listener  protocol|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] in the plugin.
(3) Create a sample client that uses the [MaxScale change data listener protocol |https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] to request change data from MaxScale",,0,0,0,0,0.0,"Change Data Listener Protocol Plugin $end$ (1) Create a skeleton protocol listener plugin - that calls the hooks into authentication plugin using API defined by MXS-486
(2) Implement the [MaxScale change data listener  protocol|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] in the plugin.
(3) Create a sample client that uses the [MaxScale change data listener protocol |https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.wjb2vfe73ieu] to request change data from MaxScale $acceptance criteria:$",0,0,0,0,0,0,1,1194.47,15,4,0.266667,1,0.0666667,0,0.0,0,0.0,0,0.0
1834,MXS-484,New Feature,MXS,2015-11-23 17:58:02,MXS-482,0,Authentication Plugin for Change Data Listener Clients,,,Authentication Plugin for Change Data Listener Clients $end$ $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,15,,0,0,0,3,0,0,2,,0,850,0,0,0,2016-02-09 12:47:59,Authentication Plugin for Change Data Listener Clients,,,0,0,0,0,0.0,Authentication Plugin for Change Data Listener Clients $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1866.82,16,4,0.25,1,0.0625,0,0.0,0,0.0,0,0.0
1835,MXS-485,Sub-Task,MXS,2015-11-23 18:06:04,,0,Authentication plugin skeleton,"Skeleton Authentication Plugin - extracted from existing code base
",,"Authentication plugin skeleton $end$ Skeleton Authentication Plugin - extracted from existing code base
 $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,6,,0,2,1,3,0,0,0,,0,850,2,0,0,2015-11-23 18:06:04,Authentication plugin skeleton,"Skeleton Authentication Plugin - extracted from existing code base
",,0,0,0,0,0.0,"Authentication plugin skeleton $end$ Skeleton Authentication Plugin - extracted from existing code base
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,17,4,0.235294,1,0.0588235,0,0.0,0,0.0,0,0.0
1836,MXS-486,Sub-Task,MXS,2015-11-23 18:07:31,,0,Define Authentication Plugin hook API,Define authentication hooks that protocol plugins will have to invoke.,,Define Authentication Plugin hook API $end$ Define authentication hooks that protocol plugins will have to invoke. $acceptance criteria:$,,Dipti Joshi,Dipti Joshi,Major,5,,0,1,0,3,0,0,0,,0,850,1,0,0,2015-11-23 18:07:31,Define Authentication Plugin hook API,Define authentication hooks that protocol plugins will have to invoke.,,0,0,0,0,0.0,Define Authentication Plugin hook API $end$ Define authentication hooks that protocol plugins will have to invoke. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,18,4,0.222222,1,0.0555556,0,0.0,0,0.0,0,0.0
1837,MXS-488,New Feature,MXS,2015-11-24 18:44:20,MXS-482,0,Binlog Avro Translator Plugin,"Plugin per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jidnmte1crrp]

* Row based binlog Event filtering
* Converting events to avro record ( Processing of WRITE/UPDATE/DELETE event)
* Avro Schema dependency(Processing TABLE_MAP_EVENT)
* MariaDB 10 GTID handling (Extract  GTID as transaction Id)
* GTID and avro filename mapping",,"Binlog Avro Translator Plugin $end$ Plugin per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jidnmte1crrp]

* Row based binlog Event filtering
* Converting events to avro record ( Processing of WRITE/UPDATE/DELETE event)
* Avro Schema dependency(Processing TABLE_MAP_EVENT)
* MariaDB 10 GTID handling (Extract  GTID as transaction Id)
* GTID and avro filename mapping $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,15,,0,0,0,3,0,0,0,,0,850,0,0,0,2016-02-24 09:45:11,Binlog Avro Translator Plugin,"Plugin per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jidnmte1crrp]

* Row based binlog Event filtering
* Converting events to avro record ( Processing of WRITE/UPDATE/DELETE event)
* Avro Schema dependency(Processing TABLE_MAP_EVENT)
* MariaDB 10 GTID handling (Extract  GTID as transaction Id)
* GTID and avro filename mapping",,0,0,0,0,0.0,"Binlog Avro Translator Plugin $end$ Plugin per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jidnmte1crrp]

* Row based binlog Event filtering
* Converting events to avro record ( Processing of WRITE/UPDATE/DELETE event)
* Avro Schema dependency(Processing TABLE_MAP_EVENT)
* MariaDB 10 GTID handling (Extract  GTID as transaction Id)
* GTID and avro filename mapping $acceptance criteria:$",0,0,0,0,0,0,1,2199.0,19,4,0.210526,1,0.0526316,0,0.0,0,0.0,0,0.0
1838,MXS-490,New Feature,MXS,2015-11-24 18:45:35,MXS-482,0,Avro Schema File Management,"Tool(outside MaxScale) to generate Avro schema file from MariaDB Table per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jcpurbbmi5g0] 
Note: Example schema generator: https://github.com/Flipkart/aesop",,"Avro Schema File Management $end$ Tool(outside MaxScale) to generate Avro schema file from MariaDB Table per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jcpurbbmi5g0] 
Note: Example schema generator: https://github.com/Flipkart/aesop $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,18,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-02-24 09:45:11,Avro Schema File Management,"Tool(outside MaxScale) to generate Avro schema file from MariaDB Table per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jcpurbbmi5g0] 
Note: Example schema generator: https://github.com/Flipkart/aesop",,0,0,0,0,0.0,"Avro Schema File Management $end$ Tool(outside MaxScale) to generate Avro schema file from MariaDB Table per this [requirements|https://docs.google.com/document/d/1Jue8K-rIyq8uKky6nXAunC5p71m_36GTu7Ob6ic9F8c/edit#heading=h.jcpurbbmi5g0] 
Note: Example schema generator: https://github.com/Flipkart/aesop $acceptance criteria:$",0,0,0,0,0,0,1,2198.98,20,4,0.2,1,0.05,0,0.0,0,0.0,0,0.0
1839,MXS-506,Sub-Task,MXS,2015-12-18 16:07:15,,0,Documentation for test environment tools,,,Documentation for test environment tools $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2015-12-18 16:07:15,Documentation for test environment tools,,,0,0,0,0,0.0,Documentation for test environment tools $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1840,MXS-51,Task,MXS,2015-03-19 20:01:36,,0,Create functional package upgrade tests,"There should be tests to ensure that upgrades to new MaxScale packages work as expected (see MXS-49 and MXS-50 for why this is important).

Here are some ideas about what should be tested:

1. Make sure the upgrade is successful
2. Make sure the service starts after upgrade
3. Make sure the service stops after upgrade
4. Make sure the new package can be removed without problem
5. Make sure the new package can be installed again without problem
6. Make sure the package can be downgraded to the previous release

Maybe there are more?",,"Create functional package upgrade tests $end$ There should be tests to ensure that upgrades to new MaxScale packages work as expected (see MXS-49 and MXS-50 for why this is important).

Here are some ideas about what should be tested:

1. Make sure the upgrade is successful
2. Make sure the service starts after upgrade
3. Make sure the service stops after upgrade
4. Make sure the new package can be removed without problem
5. Make sure the new package can be installed again without problem
6. Make sure the package can be downgraded to the previous release

Maybe there are more? $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,20,,0,4,0,3,0,0,0,,0,850,2,0,0,2016-01-12 17:54:45,Create functional package upgrade tests,"There should be tests to ensure that upgrades to new MaxScale packages work as expected (see MXS-49 and MXS-50 for why this is important).

Here are some ideas about what should be tested:

1. Make sure the upgrade is successful
2. Make sure the service starts after upgrade
3. Make sure the service stops after upgrade
4. Make sure the new package can be removed without problem
5. Make sure the new package can be installed again without problem
6. Make sure the package can be downgraded to the previous release

Maybe there are more?",,0,0,0,0,0.0,"Create functional package upgrade tests $end$ There should be tests to ensure that upgrades to new MaxScale packages work as expected (see MXS-49 and MXS-50 for why this is important).

Here are some ideas about what should be tested:

1. Make sure the upgrade is successful
2. Make sure the service starts after upgrade
3. Make sure the service stops after upgrade
4. Make sure the new package can be removed without problem
5. Make sure the new package can be installed again without problem
6. Make sure the package can be downgraded to the previous release

Maybe there are more? $acceptance criteria:$",0,0,0,0,0,0,1,7173.88,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1841,MXS-512,Task,MXS,2016-01-05 08:53:20,MXS-509,0,Analyze query_classifier usage.,Analyze the current query_classifier usage to get a clear understanding of the current needs.,,Analyze query_classifier usage. $end$ Analyze the current query_classifier usage to get a clear understanding of the current needs. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,1,0,1,0,0,1,,0,850,1,0,0,2016-01-11 14:44:57,Analyze query_classifier usage.,Analyze the current query_classifier usage to get a clear understanding of the current needs.,,0,0,0,0,0.0,Analyze query_classifier usage. $end$ Analyze the current query_classifier usage to get a clear understanding of the current needs. $acceptance criteria:$,0,0,0,0,0,0,0,149.85,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1842,MXS-517,Task,MXS,2016-01-05 09:06:32,MXS-509,0,Load query classifier at runtime.,"Modify MaxScale so that the component providing the query classification API is loaded at runtime, and create such a component that uses the MySql embedded parser. That will allow the choice of whether to use in-process or out-of-process parsing to be made at runtime.",,"Load query classifier at runtime. $end$ Modify MaxScale so that the component providing the query classification API is loaded at runtime, and create such a component that uses the MySql embedded parser. That will allow the choice of whether to use in-process or out-of-process parsing to be made at runtime. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-01-12 11:55:01,Load query classifier at runtime.,"Modify MaxScale so that the component providing the query classification API is loaded at runtime, and create such a component that uses the MySql embedded parser. That will allow the choice of whether to use in-process or out-of-process parsing to be made at runtime.",,0,0,0,0,0.0,"Load query classifier at runtime. $end$ Modify MaxScale so that the component providing the query classification API is loaded at runtime, and create such a component that uses the MySql embedded parser. That will allow the choice of whether to use in-process or out-of-process parsing to be made at runtime. $acceptance criteria:$",0,0,0,0,0,0,1,170.8,5,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1843,MXS-542,Sub-Task,MXS,2016-01-08 10:26:03,,0,Remove explicit calls of parse_query.,"To be able to parse queries in an external process efficiently, there cannot be any direct calls from module code to do that.",,"Remove explicit calls of parse_query. $end$ To be able to parse queries in an external process efficiently, there cannot be any direct calls from module code to do that. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-01-08 10:26:03,Remove explicit calls of parse_query.,"To be able to parse queries in an external process efficiently, there cannot be any direct calls from module code to do that.",,0,0,0,0,0.0,"Remove explicit calls of parse_query. $end$ To be able to parse queries in an external process efficiently, there cannot be any direct calls from module code to do that. $acceptance criteria:$",0,0,0,0,0,0,0,0.0,6,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1844,MXS-544,New Feature,MXS,2016-01-12 12:40:05,,0,Protocol Module Investigations and Improvements,,,Protocol Module Investigations and Improvements $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,21,,0,8,0,11,0,0,0,,0,850,8,0,0,2016-01-12 12:40:05,Protocol Module Investigations and Improvements,,,0,0,0,0,0.0,Protocol Module Investigations and Improvements $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1845,MXS-547,Task,MXS,2016-01-13 08:49:16,,0,Create maxscale-experimental package,Create a package which contains experimental and non-GA modules. This way the threshold to include new modules in packages is lowered but the quality of the core MaxScale package isn't compromised.,,Create maxscale-experimental package $end$ Create a package which contains experimental and non-GA modules. This way the threshold to include new modules in packages is lowered but the quality of the core MaxScale package isn't compromised. $acceptance criteria:$,,markus makela,markus makela,Major,8,,0,1,2,2,0,0,0,,0,850,1,0,0,2016-07-05 09:43:14,Create maxscale-experimental package,Create a package which contains experimental and non-GA modules. This way the threshold to include new modules in packages is lowered but the quality of the core MaxScale package isn't compromised.,,0,0,0,0,0.0,Create maxscale-experimental package $end$ Create a package which contains experimental and non-GA modules. This way the threshold to include new modules in packages is lowered but the quality of the core MaxScale package isn't compromised. $acceptance criteria:$,0,0,0,0,0,0,1,4176.88,5,3,0.6,3,0.6,3,0.6,3,0.6,3,0.6
1846,MXS-553,New Feature,MXS,2016-01-21 15:33:49,,0,FR: Add connection id to maxadmin show sessions output,"Our customer would like to have the maxadmin ""show sessions"" to show the connection ID (from SHOW PROCESSLIST) assigned to the backend connection from the DB Host it is connected to.",,"FR: Add connection id to maxadmin show sessions output $end$ Our customer would like to have the maxadmin ""show sessions"" to show the connection ID (from SHOW PROCESSLIST) assigned to the backend connection from the DB Host it is connected to. $acceptance criteria:$",,Guillaume Lefranc,Guillaume Lefranc,Major,21,,0,2,0,2,0,0,0,,0,850,0,0,0,2017-08-22 09:45:05,FR: Add connection id to maxadmin show sessions output,"Our customer would like to have the maxadmin ""show sessions"" to show the connection ID (from SHOW PROCESSLIST) assigned to the backend connection from the DB Host it is connected to.",,0,0,0,0,0.0,"FR: Add connection id to maxadmin show sessions output $end$ Our customer would like to have the maxadmin ""show sessions"" to show the connection ID (from SHOW PROCESSLIST) assigned to the backend connection from the DB Host it is connected to. $acceptance criteria:$",0,0,0,0,0,0,1,13890.2,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1847,MXS-556,Task,MXS,2016-01-26 13:28:15,,0,Investigation and specification of firewall improvements.,"* Investigation and specification of firewall improvements.
* Their relationship to configuration management. In particular, the updating of firewall rules should be possible to do at runtime.
",,"Investigation and specification of firewall improvements. $end$ * Investigation and specification of firewall improvements.
* Their relationship to configuration management. In particular, the updating of firewall rules should be possible to do at runtime.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,2,1,0,0,0,,0,850,0,0,0,2016-01-26 13:28:15,Investigation and specification of firewall improvements.,"* Investigation and specification of firewall improvements.
* Their relationship to configuration management. In particular, the updating of firewall rules should be possible to do at runtime.
",,0,0,0,0,0.0,"Investigation and specification of firewall improvements. $end$ * Investigation and specification of firewall improvements.
* Their relationship to configuration management. In particular, the updating of firewall rules should be possible to do at runtime.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1848,MXS-557,Task,MXS,2016-01-26 13:31:53,,0,Implementation of firewall improvements.,"The contents of this depends upon the result of MXS-556.
",,"Implementation of firewall improvements. $end$ The contents of this depends upon the result of MXS-556.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,1,1,0,0,0,,0,850,0,0,0,2016-01-26 13:31:53,Implementation of firewall improvements.,"The contents of this depends upon the result of MXS-556.
",,0,0,0,0,0.0,"Implementation of firewall improvements. $end$ The contents of this depends upon the result of MXS-556.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,9,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1849,MXS-558,Task,MXS,2016-01-26 13:34:45,MXS-509,0,Investigation of using SQLite as query classifier.,Investigate whether the tokenizer/parser of SQLite can be used as query classifier.,,Investigation of using SQLite as query classifier. $end$ Investigate whether the tokenizer/parser of SQLite can be used as query classifier. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,15,,0,4,0,3,0,0,0,,0,850,4,0,0,2016-01-26 13:34:45,Investigation of using SQLite as query classifier.,Investigate whether the tokenizer/parser of SQLite can be used as query classifier.,,0,0,0,0,0.0,Investigation of using SQLite as query classifier. $end$ Investigate whether the tokenizer/parser of SQLite can be used as query classifier. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,10,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1850,MXS-567,Task,MXS,2016-02-09 09:35:54,,0,create test for 'blob' related issues,"'longblob' test is not finished and fails (sometimes with segfault), but Maxscale seems to work and problem is in the test.

Related issues:
https://mariadb.atlassian.net/browse/MXS-566
https://mariadb.atlassian.net/browse/MXS-300
https://mariadb.atlassian.net/browse/MXS-56",,"create test for 'blob' related issues $end$ 'longblob' test is not finished and fails (sometimes with segfault), but Maxscale seems to work and problem is in the test.

Related issues:
https://mariadb.atlassian.net/browse/MXS-566
https://mariadb.atlassian.net/browse/MXS-300
https://mariadb.atlassian.net/browse/MXS-56 $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-02-09 12:59:07,create test for 'blob' related issues,"'longblob' test is not finished and fails (sometimes with segfault), but Maxscale seems to work and problem is in the test.

Related issues:
https://mariadb.atlassian.net/browse/MXS-566
https://mariadb.atlassian.net/browse/MXS-300
https://mariadb.atlassian.net/browse/MXS-56",,0,0,0,0,0.0,"create test for 'blob' related issues $end$ 'longblob' test is not finished and fails (sometimes with segfault), but Maxscale seems to work and problem is in the test.

Related issues:
https://mariadb.atlassian.net/browse/MXS-566
https://mariadb.atlassian.net/browse/MXS-300
https://mariadb.atlassian.net/browse/MXS-56 $acceptance criteria:$",0,0,0,0,0,0,0,3.38333,7,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1851,MXS-568,Task,MXS,2016-02-09 09:38:10,,0,check all disabled tests,"Test package contains a number of disable test. The task is to check all of them, enable if test is valid.",,"check all disabled tests $end$ Test package contains a number of disable test. The task is to check all of them, enable if test is valid. $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-02-09 12:59:28,check all disable tests,"Test package contains a number of disable test. The task is to check all of them, enable if test is valid.",,1,0,0,2,0.0357143,"check all disable tests $end$ Test package contains a number of disable test. The task is to check all of them, enable if test is valid. $acceptance criteria:$",1,1,0,0,0,0,0,3.35,8,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1852,MXS-569,Task,MXS,2016-02-09 10:40:36,,0,create test for MXS-419,"bug description https://mariadb.atlassian.net/browse/MXS-419
",,"create test for MXS-419 $end$ bug description https://mariadb.atlassian.net/browse/MXS-419
 $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-02-09 12:58:50,create test for MXS-419,"bug description https://mariadb.atlassian.net/browse/MXS-419
",,0,0,0,0,0.0,"create test for MXS-419 $end$ bug description https://mariadb.atlassian.net/browse/MXS-419
 $acceptance criteria:$",0,0,0,0,0,0,0,2.3,9,1,0.111111,0,0.0,0,0.0,0,0.0,0,0.0
1853,MXS-572,Task,MXS,2016-02-09 12:53:15,,0,Create comprehensive tests for firewall filter.,,,Create comprehensive tests for firewall filter. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,15,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-02-09 12:53:15,Create comprehensive tests for firewall filter.,,,0,0,0,0,0.0,Create comprehensive tests for firewall filter. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,11,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1854,MXS-584,New Feature,MXS,2016-02-16 14:32:50,,0,binlog_router disconnects client after wrong sql,"Hi,

i use two different Python MySQL connectors, which both run sql commands right after the inital connection, like:

{code:sql}
set autocommit=0
or
SET @@session.autocommit = OFF
{code}

MaxScale does not support the sql from above and prints an error:
{noformat}
error  : Unexpected query from 'nagios'@'1.2.3.4': set autocommit=0
notice : my_binlog: Slave 1.2.3.4, server id 0, disconnected after 0 seconds. 1 SQL commands
{noformat}

This prevents me from getting the slave status via python scripts from maxscale binlog router.

Couldn't this just be a warning instead of disconnecting a slave with wrong sql? Maybe the problem is related to the python connector rather than maxscale.

Regards, Michael",,"binlog_router disconnects client after wrong sql $end$ Hi,

i use two different Python MySQL connectors, which both run sql commands right after the inital connection, like:

{code:sql}
set autocommit=0
or
SET @@session.autocommit = OFF
{code}

MaxScale does not support the sql from above and prints an error:
{noformat}
error  : Unexpected query from 'nagios'@'1.2.3.4': set autocommit=0
notice : my_binlog: Slave 1.2.3.4, server id 0, disconnected after 0 seconds. 1 SQL commands
{noformat}

This prevents me from getting the slave status via python scripts from maxscale binlog router.

Couldn't this just be a warning instead of disconnecting a slave with wrong sql? Maybe the problem is related to the python connector rather than maxscale.

Regards, Michael $acceptance criteria:$",,Michael Froehlich,Michael Froehlich,Major,12,,0,2,1,1,0,0,0,,0,850,1,0,0,2016-05-31 10:48:17,binlog_router disconnects client after wrong sql,"Hi,

i use two different Python MySQL connectors, which both run sql commands right after the inital connection, like:

{code:sql}
set autocommit=0
or
SET @@session.autocommit = OFF
{code}

MaxScale does not support the sql from above and prints an error:
{noformat}
error  : Unexpected query from 'nagios'@'1.2.3.4': set autocommit=0
notice : my_binlog: Slave 1.2.3.4, server id 0, disconnected after 0 seconds. 1 SQL commands
{noformat}

This prevents me from getting the slave status via python scripts from maxscale binlog router.

Couldn't this just be a warning instead of disconnecting a slave with wrong sql? Maybe the problem is related to the python connector rather than maxscale.

Regards, Michael",,0,0,0,0,0.0,"binlog_router disconnects client after wrong sql $end$ Hi,

i use two different Python MySQL connectors, which both run sql commands right after the inital connection, like:

{code:sql}
set autocommit=0
or
SET @@session.autocommit = OFF
{code}

MaxScale does not support the sql from above and prints an error:
{noformat}
error  : Unexpected query from 'nagios'@'1.2.3.4': set autocommit=0
notice : my_binlog: Slave 1.2.3.4, server id 0, disconnected after 0 seconds. 1 SQL commands
{noformat}

This prevents me from getting the slave status via python scripts from maxscale binlog router.

Couldn't this just be a warning instead of disconnecting a slave with wrong sql? Maybe the problem is related to the python connector rather than maxscale.

Regards, Michael $acceptance criteria:$",0,0,0,0,0,0,0,2516.25,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1855,MXS-591,New Feature,MXS,2016-02-25 14:51:05,,0,Inject comment into query for auditing purposes,"Hi everybody,

I would like MaxScale to add a query comment with some information that can be used for auditing.
I would like to be able to add the IP address, MaxScale service responsible. Perhaps you have some ideas for other information to add?

Thank you,
Michael",,"Inject comment into query for auditing purposes $end$ Hi everybody,

I would like MaxScale to add a query comment with some information that can be used for auditing.
I would like to be able to add the IP address, MaxScale service responsible. Perhaps you have some ideas for other information to add?

Thank you,
Michael $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,9,,0,1,0,1,0,0,0,,0,850,0,0,0,2018-06-12 08:03:52,Inject comment into query for auditing purposes,"Hi everybody,

I would like MaxScale to add a query comment with some information that can be used for auditing.
I would like to be able to add the IP address, MaxScale service responsible. Perhaps you have some ideas for other information to add?

Thank you,
Michael",,0,0,0,0,0.0,"Inject comment into query for auditing purposes $end$ Hi everybody,

I would like MaxScale to add a query comment with some information that can be used for auditing.
I would like to be able to add the IP address, MaxScale service responsible. Perhaps you have some ideas for other information to add?

Thank you,
Michael $acceptance criteria:$",0,0,0,0,0,0,0,20105.2,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1856,MXS-607,Task,MXS,2016-03-07 08:58:39,MXS-606,0,Release notes for 1.4.0,,,Release notes for 1.4.0 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-03-08 10:53:12,Release notes for 1.4,,,1,0,0,2,0.142857,Release notes for 1.4 $end$ $acceptance criteria:$,1,1,0,0,0,0,0,25.9,12,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1857,MXS-608,Task,MXS,2016-03-07 08:59:03,MXS-606,0,Change log for 1.4.0 (beta),,,Change log for 1.4.0 (beta) $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-03-08 10:53:12,Change log for 1.4,,,1,0,0,3,0.285714,Change log for 1.4 $end$ $acceptance criteria:$,1,1,0,0,0,0,0,25.9,13,1,0.0769231,0,0.0,0,0.0,0,0.0,0,0.0
1858,MXS-609,Task,MXS,2016-03-07 08:59:25,MXS-606,0,Upgrading from 1.3 to 1.4.0,Document,,Upgrading from 1.3 to 1.4.0 $end$ Document $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,0,1,0,2,0,,0,850,1,0,0,2016-03-08 10:53:12,Updating from 1.3 to 1.4.,Document,,2,0,0,4,0.222222,Updating from 1.3 to 1.4. $end$ Document $acceptance criteria:$,2,1,0,0,0,0,0,25.8833,14,2,0.142857,0,0.0,0,0.0,0,0.0,0,0.0
1859,MXS-610,Task,MXS,2016-03-07 08:59:52,MXS-606,0,1.4.0 (beta) Limitations document.,,,1.4.0 (beta) Limitations document. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-03-08 10:53:12,Limitations document.,,,1,0,0,2,0.4,Limitations document. $end$ $acceptance criteria:$,1,1,0,0,0,0,0,25.8833,15,3,0.2,0,0.0,0,0.0,0,0.0,0,0.0
1860,MXS-611,Task,MXS,2016-03-07 09:01:03,MXS-606,0,Verify 1.4.0 documentation.,,,Verify 1.4.0 documentation. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-03-08 10:53:12,Verify 1.4 documentation.,,,1,0,0,2,0.166667,Verify 1.4 documentation. $end$ $acceptance criteria:$,1,1,0,0,0,0,0,25.8667,16,4,0.25,0,0.0,0,0.0,0,0.0,0,0.0
1861,MXS-612,Task,MXS,2016-03-07 09:01:14,MXS-606,0,Build and Upload 1.4.0 binaries.,,,Build and Upload 1.4.0 binaries. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,1,0,2,0,,0,850,0,0,0,2016-03-08 10:53:13,Upload 1.4 binaries.,,,2,0,0,4,0.5,Upload 1.4 binaries. $end$ $acceptance criteria:$,2,1,0,0,0,0,0,25.85,17,5,0.294118,0,0.0,0,0.0,0,0.0,0,0.0
1862,MXS-613,Task,MXS,2016-03-07 09:01:39,MXS-606,0,Sync 1.4.0 documentation to KB.,,,Sync 1.4.0 documentation to KB. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-03-08 10:53:13,Sync documentation to KB.,,,1,0,0,1,0.142857,Sync documentation to KB. $end$ $acceptance criteria:$,1,1,0,0,0,0,0,25.85,18,6,0.333333,0,0.0,0,0.0,0,0.0,0,0.0
1863,MXS-614,Task,MXS,2016-03-07 09:02:33,MXS-606,0,1.4.0 Regression testing across all platforms.,,,1.4.0 Regression testing across all platforms. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,1,0,1,0,,0,850,1,0,0,2016-03-08 10:53:13,Regression testing across all platforms.,,,1,0,0,1,0.125,Regression testing across all platforms. $end$ $acceptance criteria:$,1,1,0,0,0,0,0,25.8333,19,7,0.368421,0,0.0,0,0.0,0,0.0,0,0.0
1864,MXS-615,New Feature,MXS,2016-03-08 09:47:58,,0,Authentication as Module,Create a new type of module for client authentication. Implement MySQL authentication as a module. Implement Avro authentication as a module.,,Authentication as Module $end$ Create a new type of module for client authentication. Implement MySQL authentication as a module. Implement Avro authentication as a module. $acceptance criteria:$,,martin brampton,martin brampton,Major,6,,0,2,1,2,0,0,0,,0,850,2,0,0,2016-03-08 10:47:33,Authentication as Module,Create a new type of module for client authentication. Implement MySQL authentication as a module. Implement Avro authentication as a module.,,0,0,0,0,0.0,Authentication as Module $end$ Create a new type of module for client authentication. Implement MySQL authentication as a module. Implement Avro authentication as a module. $acceptance criteria:$,0,0,0,0,0,0,1,0.983333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1865,MXS-617,Task,MXS,2016-03-08 11:03:32,MXS-634,0,Sqlite based query-classifier (minimal skeleton implementation),"The basic mechanism is in place. Now can basically detect selects.


",,"Sqlite based query-classifier (minimal skeleton implementation) $end$ The basic mechanism is in place. Now can basically detect selects.


 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,1,0,,0,850,0,0,0,2016-03-08 11:03:32,Sqlite based query-classifier.,"The basic mechanism is in place. Now can basically detect selects.


",,1,0,0,5,0.235294,"Sqlite based query-classifier. $end$ The basic mechanism is in place. Now can basically detect selects.


 $acceptance criteria:$",1,1,1,0,0,0,1,0.0,20,8,0.4,0,0.0,0,0.0,0,0.0,0,0.0
1866,MXS-618,Task,MXS,2016-03-08 11:04:06,,0,Split documentation into separate repository.,,,Split documentation into separate repository. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-08 11:04:06,Split documentation into separate repository.,,,0,0,0,0,0.0,Split documentation into separate repository. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,21,9,0.428571,1,0.047619,0,0.0,0,0.0,0,0.0
1867,MXS-622,New Feature,MXS,2016-03-15 11:29:57,,0,Firewall filter improvements,Follow up from MXS-167.,,Firewall filter improvements $end$ Follow up from MXS-167. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,2,0,1,0,0,2,,0,850,1,0,0,2017-01-04 09:33:28,Firewall filter improvements,Follow up from MXS-167.,,0,0,0,0,0.0,Firewall filter improvements $end$ Follow up from MXS-167. $acceptance criteria:$,0,0,0,0,0,0,0,7078.05,22,9,0.409091,1,0.0454545,0,0.0,0,0.0,0,0.0
1868,MXS-635,Task,MXS,2016-03-23 08:36:35,MXS-634,0,Create test setup for query classifier.,A test setup is needed for verifying that the sqlite based query classifier is capable of parsing and classifying everything that the MySQL based query classifier is capable of.,,Create test setup for query classifier. $end$ A test setup is needed for verifying that the sqlite based query classifier is capable of parsing and classifying everything that the MySQL based query classifier is capable of. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-04-06 07:10:01,Create test setup for query classifier.,A test setup is needed for verifying that the sqlite based query classifier is capable of parsing and classifying everything that the MySQL based query classifier is capable of.,,0,0,0,0,0.0,Create test setup for query classifier. $end$ A test setup is needed for verifying that the sqlite based query classifier is capable of parsing and classifying everything that the MySQL based query classifier is capable of. $acceptance criteria:$,0,0,0,0,0,0,0,334.55,23,9,0.391304,1,0.0434783,0,0.0,0,0.0,0,0.0
1869,MXS-636,New Feature,MXS,2016-03-23 08:48:02,,0,Continue handling read queries although master is lost,MaxScale's ReadWrite router stops processing any queries if the connection to the master is lost. It has been requested that MaxScale could/should continue processing read queries by sending them to the slaves and only stop write queries (DDL and DML) by returning an error indicating that the master is down.,,Continue handling read queries although master is lost $end$ MaxScale's ReadWrite router stops processing any queries if the connection to the master is lost. It has been requested that MaxScale could/should continue processing read queries by sending them to the slaves and only stop write queries (DDL and DML) by returning an error indicating that the master is down. $acceptance criteria:$,,Rasmus Johansson,Rasmus Johansson,Major,5,,0,2,1,1,0,0,0,,0,850,0,0,0,2016-04-19 09:19:58,Continue handling read queries although master is lost,MaxScale's ReadWrite router stops processing any queries if the connection to the master is lost. It has been requested that MaxScale could/should continue processing read queries by sending them to the slaves and only stop write queries (DDL and DML) by returning an error indicating that the master is down.,,0,0,0,0,0.0,Continue handling read queries although master is lost $end$ MaxScale's ReadWrite router stops processing any queries if the connection to the master is lost. It has been requested that MaxScale could/should continue processing read queries by sending them to the slaves and only stop write queries (DDL and DML) by returning an error indicating that the master is down. $acceptance criteria:$,0,0,0,0,0,0,0,648.517,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1870,MXS-638,Task,MXS,2016-03-23 08:56:35,MXS-637,0,1.4.1: ReleaseNotes,,,1.4.1: ReleaseNotes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:43:28,1.4.1: ReleaseNotes,,,0,0,0,0,0.0,1.4.1: ReleaseNotes $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.76667,24,9,0.375,1,0.0416667,0,0.0,0,0.0,0,0.0
1871,MXS-639,Task,MXS,2016-03-23 08:56:47,MXS-637,0,1.4.1: Change log,,,1.4.1: Change log $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:43:28,1.4.1: Change log,,,0,0,0,0,0.0,1.4.1: Change log $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.76667,25,9,0.36,1,0.04,0,0.0,0,0.0,0,0.0
1872,MXS-640,Task,MXS,2016-03-23 08:57:14,MXS-637,0,1.4.1: Upgrading To.,,,1.4.1: Upgrading To. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:43:28,1.4.1: Upgrading To.,,,0,0,0,0,0.0,1.4.1: Upgrading To. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.76667,26,9,0.346154,1,0.0384615,0,0.0,0,0.0,0,0.0
1873,MXS-641,Task,MXS,2016-03-23 08:57:25,MXS-637,0,1.4.1: Verify documentation.,,,1.4.1: Verify documentation. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:43:29,1.4.1: Verify documentation.,,,0,0,0,0,0.0,1.4.1: Verify documentation. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.76667,27,9,0.333333,1,0.037037,0,0.0,0,0.0,0,0.0
1874,MXS-642,Task,MXS,2016-03-23 08:57:42,MXS-637,0,1.4.1: Build and upload binaries,,,1.4.1: Build and upload binaries $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:43:29,1.4.1: Build and upload binaries,,,0,0,0,0,0.0,1.4.1: Build and upload binaries $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.75,28,9,0.321429,1,0.0357143,0,0.0,0,0.0,0,0.0
1875,MXS-643,Task,MXS,2016-03-23 08:57:55,MXS-637,0,1.4.1: Sync documentation to KB,,,1.4.1: Sync documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-03-23 10:43:29,1.4.1: Sync documentation to KB,,,0,0,0,0,0.0,1.4.1: Sync documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.75,29,9,0.310345,1,0.0344828,0,0.0,0,0.0,0,0.0
1876,MXS-644,Task,MXS,2016-03-23 08:58:09,MXS-637,0,1.4.1: Regression testing across all platforms.,,,1.4.1: Regression testing across all platforms. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-03-23 10:43:29,1.4.1: Regression testing across all platforms.,,,0,0,0,0,0.0,1.4.1: Regression testing across all platforms. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.75,30,9,0.3,1,0.0333333,0,0.0,0,0.0,0,0.0
1877,MXS-647,Task,MXS,2016-03-23 11:16:45,,0,Remove up-to-date state for slaves.,"Re-arrange binlogrouter so that slaves never are in up-to-date state, but are always updated as if they were in the catchup mode.",,"Remove up-to-date state for slaves. $end$ Re-arrange binlogrouter so that slaves never are in up-to-date state, but are always updated as if they were in the catchup mode. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-03-23 11:16:45,Remove up-to-date state for slaves.,"Re-arrange binlogrouter so that slaves never are in up-to-date state, but are always updated as if they were in the catchup mode.",,0,0,0,0,0.0,"Remove up-to-date state for slaves. $end$ Re-arrange binlogrouter so that slaves never are in up-to-date state, but are always updated as if they were in the catchup mode. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,31,9,0.290323,1,0.0322581,0,0.0,0,0.0,0,0.0
1878,MXS-651,New Feature,MXS,2016-03-30 11:18:16,,0,SSL for backend connections,Support for configuring SSL for the connections to backend database servers using the MySQL protocol. ,,SSL for backend connections $end$ Support for configuring SSL for the connections to backend database servers using the MySQL protocol.  $acceptance criteria:$,,martin brampton,martin brampton,Major,8,,0,9,0,4,0,0,0,,0,850,8,0,0,2016-04-19 09:23:19,SSL for backend connections,Support for configuring SSL for the connections to backend database servers using the MySQL protocol. ,,0,0,0,0,0.0,SSL for backend connections $end$ Support for configuring SSL for the connections to backend database servers using the MySQL protocol.  $acceptance criteria:$,0,0,0,0,0,0,1,478.083,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1879,MXS-659,New Feature,MXS,2016-04-04 09:19:36,MXS-482,0,Add Avro file indexing,Avro files could be indexed on the GTID to make requests to files at an offset faster.,,Add Avro file indexing $end$ Avro files could be indexed on the GTID to make requests to files at an offset faster. $acceptance criteria:$,,markus makela,markus makela,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-04 09:26:46,Add Avro file indexing,Avro files could be indexed on the GTID to make requests to files at an offset faster.,,0,0,0,0,0.0,Add Avro file indexing $end$ Avro files could be indexed on the GTID to make requests to files at an offset faster. $acceptance criteria:$,0,0,0,0,0,0,0,720.117,6,3,0.5,3,0.5,3,0.5,3,0.5,3,0.5
1880,MXS-66,New Feature,MXS,2015-03-27 18:51:57,,0,MaxScale should refuse to load modules if unexpected configuration options are found.,"This request was originally made in http://bugs.mariadb.com/show_bug.cgi?id=154 in August 2013, but I ran into it again while testing filters.

If you add an option to MaxScale.cnf that doesn't exist for a module, the module writes an error to the error log, but continues loading. For example:

{noformat}
2015-03-21 06:24:57   Error : Unexpected parameter 'filter' for object 'RW Split Router' of type 'service'.
{noformat}

The correct option is ""filters"", not ""filter"". But in this case, the router loads anyway, without the filters defined. This may mean that a user is allowed to execute queries that should be blocked. In other cases, it could have other harmful effects.

I think that the module should refuse to load if there are unexpected/unused configuration options. That is the behavior of MySQL server and it can be very dangerous to have queries going places you don't expect, or have queries allowed to execute in the first place, because you've mis-spelled a configuration variable or option or think you're setting some option that doesn't exist.",,"MaxScale should refuse to load modules if unexpected configuration options are found. $end$ This request was originally made in http://bugs.mariadb.com/show_bug.cgi?id=154 in August 2013, but I ran into it again while testing filters.

If you add an option to MaxScale.cnf that doesn't exist for a module, the module writes an error to the error log, but continues loading. For example:

{noformat}
2015-03-21 06:24:57   Error : Unexpected parameter 'filter' for object 'RW Split Router' of type 'service'.
{noformat}

The correct option is ""filters"", not ""filter"". But in this case, the router loads anyway, without the filters defined. This may mean that a user is allowed to execute queries that should be blocked. In other cases, it could have other harmful effects.

I think that the module should refuse to load if there are unexpected/unused configuration options. That is the behavior of MySQL server and it can be very dangerous to have queries going places you don't expect, or have queries allowed to execute in the first place, because you've mis-spelled a configuration variable or option or think you're setting some option that doesn't exist. $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,11,,1,2,3,1,0,1,0,,0,850,1,1,0,2016-05-17 09:46:31,MaxScale should refuse to load modules if unexpected configuration options are found.,"This request was originally made in http://bugs.mariadb.com/show_bug.cgi?id=154 in August 2013, but I ran into it again while testing filters.

If you add an option to MaxScale.cnf that doesn't exist for a module, the module writes an error to the error log, but continues loading. For example:

{noformat}
2015-03-21 06:24:57   Error : Unexpected parameter 'filter' for object 'RW Split Router' of type 'service'.
{noformat}

The correct option is ""filters"", not ""filter"". But in this case, the router loads anyway, without the filters defined. This may mean that a user is allowed to execute queries that should be blocked. In other cases, it could have other harmful effects.

I think that the module should refuse to load if there are unexpected/unused configuration options. That is the behavior of MySQL server and it can be very dangerous to have queries going places you don't expect, or have queries allowed to execute in the first place, because you've mis-spelled a configuration variable or option or think you're setting some option that doesn't exist.",,0,0,0,0,0.0,"MaxScale should refuse to load modules if unexpected configuration options are found. $end$ This request was originally made in http://bugs.mariadb.com/show_bug.cgi?id=154 in August 2013, but I ran into it again while testing filters.

If you add an option to MaxScale.cnf that doesn't exist for a module, the module writes an error to the error log, but continues loading. For example:

{noformat}
2015-03-21 06:24:57   Error : Unexpected parameter 'filter' for object 'RW Split Router' of type 'service'.
{noformat}

The correct option is ""filters"", not ""filter"". But in this case, the router loads anyway, without the filters defined. This may mean that a user is allowed to execute queries that should be blocked. In other cases, it could have other harmful effects.

I think that the module should refuse to load if there are unexpected/unused configuration options. That is the behavior of MySQL server and it can be very dangerous to have queries going places you don't expect, or have queries allowed to execute in the first place, because you've mis-spelled a configuration variable or option or think you're setting some option that doesn't exist. $acceptance criteria:$",0,0,0,0,0,0,0,9998.9,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1881,MXS-660,New Feature,MXS,2016-04-04 12:57:32,,0,Live events notification to slaves,"Binlog under big load: live updates refactoring.
Architectural streamlining and performance and reliability improvents come with a new model for live events notification",,"Live events notification to slaves $end$ Binlog under big load: live updates refactoring.
Architectural streamlining and performance and reliability improvents come with a new model for live events notification $acceptance criteria:$",,Massimiliano Pinto,Massimiliano Pinto,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-04-19 09:17:38,Live events notification to slaves,"Binlog under big load: live updates refactoring.
Architectural streamlining and performance and reliability improvents come with a new model for live events notification",,0,0,0,0,0.0,"Live events notification to slaves $end$ Binlog under big load: live updates refactoring.
Architectural streamlining and performance and reliability improvents come with a new model for live events notification $acceptance criteria:$",0,0,0,0,0,0,0,356.333,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1882,MXS-664,Task,MXS,2016-04-06 07:21:55,MXS-634,0,Ensure that qc_sqlite passes the existing rudimentary query classifier test.,,,Ensure that qc_sqlite passes the existing rudimentary query classifier test. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-04-06 07:21:55,Ensure that qc_sqlite passes the existing rudimentary query classifier test.,,,0,0,0,0,0.0,Ensure that qc_sqlite passes the existing rudimentary query classifier test. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,32,9,0.28125,1,0.03125,0,0.0,0,0.0,0,0.0
1883,MXS-665,Task,MXS,2016-04-06 07:23:28,MXS-634,0,Fix existing query classifier test.,Currently the query classifier test only checks that one particular type bit is set for each classified query. All bits need to be checked.,,Fix existing query classifier test. $end$ Currently the query classifier test only checks that one particular type bit is set for each classified query. All bits need to be checked. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-04-06 07:23:28,Fix existing query classifier test.,Currently the query classifier test only checks that one particular type bit is set for each classified query. All bits need to be checked.,,0,0,0,0,0.0,Fix existing query classifier test. $end$ Currently the query classifier test only checks that one particular type bit is set for each classified query. All bits need to be checked. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,33,9,0.272727,1,0.030303,0,0.0,0,0.0,0,0.0
1884,MXS-669,New Feature,MXS,2016-04-13 12:39:23,,0,Refactor modutil_get_complete_packets,The modutil_get_complete_packets function makes the buffers contiguous when in most cases it is not necessary.,,Refactor modutil_get_complete_packets $end$ The modutil_get_complete_packets function makes the buffers contiguous when in most cases it is not necessary. $acceptance criteria:$,,markus makela,markus makela,Major,7,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-04-19 09:18:47,Refactor modutil_get_complete_packets,The modutil_get_complete_packets function makes the buffers contiguous when in most cases it is not necessary.,,0,0,0,0,0.0,Refactor modutil_get_complete_packets $end$ The modutil_get_complete_packets function makes the buffers contiguous when in most cases it is not necessary. $acceptance criteria:$,0,0,0,0,0,0,0,140.65,7,3,0.428571,3,0.428571,3,0.428571,3,0.428571,3,0.428571
1885,MXS-676,Task,MXS,2016-04-19 07:39:46,MXS-634,0,Extend qc_sqlite so that select.test passes.,"Extend the sqlite based query classifier so that it can handle an ever increasing set of SQL the same way as the embedded library does.

In particular, ensure that select.test passes.",,"Extend qc_sqlite so that select.test passes. $end$ Extend the sqlite based query classifier so that it can handle an ever increasing set of SQL the same way as the embedded library does.

In particular, ensure that select.test passes. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,2,0,,0,850,0,0,0,2016-04-19 07:39:46,Extend qc_sqlite,Extend the sqlite based query classifier so that it can handle an ever increasing set of SQL the same way as the embedded library does.,,1,1,0,10,0.333333,Extend qc_sqlite $end$ Extend the sqlite based query classifier so that it can handle an ever increasing set of SQL the same way as the embedded library does. $acceptance criteria:$,2,1,1,1,0,0,1,0.0,34,9,0.264706,1,0.0294118,0,0.0,0,0.0,0,0.0
1886,MXS-678,Task,MXS,2016-04-19 09:22:16,,0,Create test-cases SemiSync,,,Create test-cases SemiSync $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,2,0,1,0,0,0,,0,850,2,0,0,2016-04-19 09:22:16,Create test-cases SemiSync,,,0,0,0,0,0.0,Create test-cases SemiSync $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,35,10,0.285714,2,0.0571429,1,0.0285714,0,0.0,0,0.0
1887,MXS-701,New Feature,MXS,2016-04-28 17:56:55,,0,Add binlog filtering to MaxScale,"There are many use cases where a user wants to replicate only some subset of a master's objects. In some cases, only a single table should be replicated. In other cases, a single database should be replicated. In other cases, perhaps a single table should be *excluded*.

Today, this can be done using the replicate-* options on the slave. However, those options affect the slave SQL thread. The slave IO thread still fetches the entire binlog from the master and writes the whole thing to disk in the form of relay logs. Then the IO thread skips over potentially many gigabytes of log entries to apply the limited subset that match the configured replicate-* entries. This means the master is tied up transferring enormous amounts of unnecessary data over the network, all the while waiting for the slave IO thread to write that data to disk.

An alternative is to use binlog-* options on the master to prevent it from writing certain log entries to disk. This is unacceptable. It means that the binlog is no longer than authoritative record, with the result that the binlog cannot be used for roll-forward recovery. It also precludes separate sets of slaves each with different requirements.

Another solution could use the MaxScale binlog router. If MaxScale were located close to the master, to minimize network bottlenecks, it could write binlogs locally and provide filtering to clients. This means the binlog on the master would still be authoritative, it would add MaxScale as an async (or semi-sync) DR copy of the binlog, and the CPU load of processing the per-slave filters would be handled on the MaxScale node rather than on the master.

To facilitate this use case, there are several different implementation possibilities:

1) Separate listeners, each with its own filters. Slaves that need only a specific subset of log entries would connect to the listener that was configured to serve only that subset.
2) Configuration based on slave server-id. The binlog router would be configured to serve a specific subset of objects to particular server-ids.
3) MariaDB Server could be modified to accept the same replicate-* rules that exist today, but to communicate those rules to the master when it connects. When MariaDB connected to a MaxScale binlog router as master, the binlog router would do the filtering locally. (A future enhancement could add filtering support of this kind to a MariaDB Server master, but that would not be required upfront for this slave behavior to be valuable!)

This feature would add enormous value to MaxScale and to the MaxScale binlog router.",,"Add binlog filtering to MaxScale $end$ There are many use cases where a user wants to replicate only some subset of a master's objects. In some cases, only a single table should be replicated. In other cases, a single database should be replicated. In other cases, perhaps a single table should be *excluded*.

Today, this can be done using the replicate-* options on the slave. However, those options affect the slave SQL thread. The slave IO thread still fetches the entire binlog from the master and writes the whole thing to disk in the form of relay logs. Then the IO thread skips over potentially many gigabytes of log entries to apply the limited subset that match the configured replicate-* entries. This means the master is tied up transferring enormous amounts of unnecessary data over the network, all the while waiting for the slave IO thread to write that data to disk.

An alternative is to use binlog-* options on the master to prevent it from writing certain log entries to disk. This is unacceptable. It means that the binlog is no longer than authoritative record, with the result that the binlog cannot be used for roll-forward recovery. It also precludes separate sets of slaves each with different requirements.

Another solution could use the MaxScale binlog router. If MaxScale were located close to the master, to minimize network bottlenecks, it could write binlogs locally and provide filtering to clients. This means the binlog on the master would still be authoritative, it would add MaxScale as an async (or semi-sync) DR copy of the binlog, and the CPU load of processing the per-slave filters would be handled on the MaxScale node rather than on the master.

To facilitate this use case, there are several different implementation possibilities:

1) Separate listeners, each with its own filters. Slaves that need only a specific subset of log entries would connect to the listener that was configured to serve only that subset.
2) Configuration based on slave server-id. The binlog router would be configured to serve a specific subset of objects to particular server-ids.
3) MariaDB Server could be modified to accept the same replicate-* rules that exist today, but to communicate those rules to the master when it connects. When MariaDB connected to a MaxScale binlog router as master, the binlog router would do the filtering locally. (A future enhancement could add filtering support of this kind to a MariaDB Server master, but that would not be required upfront for this slave behavior to be valuable!)

This feature would add enormous value to MaxScale and to the MaxScale binlog router. $acceptance criteria:$",,Kolbe Kegel,Kolbe Kegel,Major,20,,0,3,0,3,0,0,0,,0,850,2,0,0,2017-11-08 09:18:52,Add binlog filtering to MaxScale,"There are many use cases where a user wants to replicate only some subset of a master's objects. In some cases, only a single table should be replicated. In other cases, a single database should be replicated. In other cases, perhaps a single table should be *excluded*.

Today, this can be done using the replicate-* options on the slave. However, those options affect the slave SQL thread. The slave IO thread still fetches the entire binlog from the master and writes the whole thing to disk in the form of relay logs. Then the IO thread skips over potentially many gigabytes of log entries to apply the limited subset that match the configured replicate-* entries. This means the master is tied up transferring enormous amounts of unnecessary data over the network, all the while waiting for the slave IO thread to write that data to disk.

An alternative is to use binlog-* options on the master to prevent it from writing certain log entries to disk. This is unacceptable. It means that the binlog is no longer than authoritative record, with the result that the binlog cannot be used for roll-forward recovery. It also precludes separate sets of slaves each with different requirements.

Another solution could use the MaxScale binlog router. If MaxScale were located close to the master, to minimize network bottlenecks, it could write binlogs locally and provide filtering to clients. This means the binlog on the master would still be authoritative, it would add MaxScale as an async (or semi-sync) DR copy of the binlog, and the CPU load of processing the per-slave filters would be handled on the MaxScale node rather than on the master.

To facilitate this use case, there are several different implementation possibilities:

1) Separate listeners, each with its own filters. Slaves that need only a specific subset of log entries would connect to the listener that was configured to serve only that subset.
2) Configuration based on slave server-id. The binlog router would be configured to serve a specific subset of objects to particular server-ids.
3) MariaDB Server could be modified to accept the same replicate-* rules that exist today, but to communicate those rules to the master when it connects. When MariaDB connected to a MaxScale binlog router as master, the binlog router would do the filtering locally. (A future enhancement could add filtering support of this kind to a MariaDB Server master, but that would not be required upfront for this slave behavior to be valuable!)

This feature would add enormous value to MaxScale and to the MaxScale binlog router.",,0,0,0,0,0.0,"Add binlog filtering to MaxScale $end$ There are many use cases where a user wants to replicate only some subset of a master's objects. In some cases, only a single table should be replicated. In other cases, a single database should be replicated. In other cases, perhaps a single table should be *excluded*.

Today, this can be done using the replicate-* options on the slave. However, those options affect the slave SQL thread. The slave IO thread still fetches the entire binlog from the master and writes the whole thing to disk in the form of relay logs. Then the IO thread skips over potentially many gigabytes of log entries to apply the limited subset that match the configured replicate-* entries. This means the master is tied up transferring enormous amounts of unnecessary data over the network, all the while waiting for the slave IO thread to write that data to disk.

An alternative is to use binlog-* options on the master to prevent it from writing certain log entries to disk. This is unacceptable. It means that the binlog is no longer than authoritative record, with the result that the binlog cannot be used for roll-forward recovery. It also precludes separate sets of slaves each with different requirements.

Another solution could use the MaxScale binlog router. If MaxScale were located close to the master, to minimize network bottlenecks, it could write binlogs locally and provide filtering to clients. This means the binlog on the master would still be authoritative, it would add MaxScale as an async (or semi-sync) DR copy of the binlog, and the CPU load of processing the per-slave filters would be handled on the MaxScale node rather than on the master.

To facilitate this use case, there are several different implementation possibilities:

1) Separate listeners, each with its own filters. Slaves that need only a specific subset of log entries would connect to the listener that was configured to serve only that subset.
2) Configuration based on slave server-id. The binlog router would be configured to serve a specific subset of objects to particular server-ids.
3) MariaDB Server could be modified to accept the same replicate-* rules that exist today, but to communicate those rules to the master when it connects. When MariaDB connected to a MaxScale binlog router as master, the binlog router would do the filtering locally. (A future enhancement could add filtering support of this kind to a MariaDB Server master, but that would not be required upfront for this slave behavior to be valuable!)

This feature would add enormous value to MaxScale and to the MaxScale binlog router. $acceptance criteria:$",0,0,0,0,0,0,1,13407.3,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1888,MXS-707,Task,MXS,2016-05-04 07:50:37,MXS-634,0,Extend qc_sqlite so that insert.test passes.,,,Extend qc_sqlite so that insert.test passes. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-04 07:50:37,Extend qc_sqlite so that insert.test passes.,,,0,0,0,0,0.0,Extend qc_sqlite so that insert.test passes. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,36,10,0.277778,2,0.0555556,1,0.0277778,0,0.0,0,0.0
1889,MXS-708,Task,MXS,2016-05-04 07:50:59,MXS-634,0,Extend qc_sqlite so that update.test passes.,,,Extend qc_sqlite so that update.test passes. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-05-04 07:50:59,Extend qc_sqlite so that update.test passes.,,,0,0,0,0,0.0,Extend qc_sqlite so that update.test passes. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,37,10,0.27027,2,0.0540541,1,0.027027,0,0.0,0,0.0
1890,MXS-709,Task,MXS,2016-05-04 07:51:41,MXS-634,0,Extend qc_sqlite so that set.test passes.,"set.test was created by

{{$ cd server/mysql-test/t}}
{{$ egrep -i ""^SET"" > set.test}}

",,"Extend qc_sqlite so that set.test passes. $end$ set.test was created by

{{$ cd server/mysql-test/t}}
{{$ egrep -i ""^SET"" > set.test}}

 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,8,,0,1,0,2,0,4,0,,0,850,1,0,0,2016-05-04 07:51:41,Extend qc_sqlite so that set_statement.test passes.,"set.test was created by

{quote}
cd server/mysql-test/t
egrep -i ""^SET"" > set.test
{quote}
",,1,3,0,10,0.272727,"Extend qc_sqlite so that set_statement.test passes. $end$ set.test was created by

{quote}
cd server/mysql-test/t
egrep -i ""^SET"" > set.test
{quote}
 $acceptance criteria:$",4,1,1,1,0,0,1,0.0,38,10,0.263158,2,0.0526316,1,0.0263158,0,0.0,0,0.0
1891,MXS-722,New Feature,MXS,2016-05-13 12:02:13,,0,Implement a --config-test to check that configuration is correct,"It would be good do have a configuration sanity check in the form:

$ maxscale --config-test

or

$ /etc/init.d/maxscale configtest


It could be implemented in the following way:

When MaxScale starts with such option should collect all options(in an array) from all modules (which should expose/register them), each option could be a structure of the type  [optname, array{opttype, allowed_values} ].

Not sure that the internal array for each option is needed, but using such structures allows to use multiple types(with their type and range check) for options that can be set to more than one type, e.g.:

address= [ip | hostname]

something= 0,1,ON,OFF


Meaningful error messages could be:

Error: Unknown option 'tipe' at row 12

Error: Value '999' for option 'threads' is out of range

Error: Value '192.168.1.' for option 'address' is not valid, expected IP or HOSTNAME  (this example is tricky as the mistyped IP could also be an hostname).

",,"Implement a --config-test to check that configuration is correct $end$ It would be good do have a configuration sanity check in the form:

$ maxscale --config-test

or

$ /etc/init.d/maxscale configtest


It could be implemented in the following way:

When MaxScale starts with such option should collect all options(in an array) from all modules (which should expose/register them), each option could be a structure of the type  [optname, array{opttype, allowed_values} ].

Not sure that the internal array for each option is needed, but using such structures allows to use multiple types(with their type and range check) for options that can be set to more than one type, e.g.:

address= [ip | hostname]

something= 0,1,ON,OFF


Meaningful error messages could be:

Error: Unknown option 'tipe' at row 12

Error: Value '999' for option 'threads' is out of range

Error: Value '192.168.1.' for option 'address' is not valid, expected IP or HOSTNAME  (this example is tricky as the mistyped IP could also be an hostname).

 $acceptance criteria:$",,Claudio Nanni,Claudio Nanni,Major,7,,0,1,2,1,0,0,0,,0,850,1,0,0,2016-05-17 09:45:41,Implement a --config-test to check that configuration is correct,"It would be good do have a configuration sanity check in the form:

$ maxscale --config-test

or

$ /etc/init.d/maxscale configtest


It could be implemented in the following way:

When MaxScale starts with such option should collect all options(in an array) from all modules (which should expose/register them), each option could be a structure of the type  [optname, array{opttype, allowed_values} ].

Not sure that the internal array for each option is needed, but using such structures allows to use multiple types(with their type and range check) for options that can be set to more than one type, e.g.:

address= [ip | hostname]

something= 0,1,ON,OFF


Meaningful error messages could be:

Error: Unknown option 'tipe' at row 12

Error: Value '999' for option 'threads' is out of range

Error: Value '192.168.1.' for option 'address' is not valid, expected IP or HOSTNAME  (this example is tricky as the mistyped IP could also be an hostname).

",,0,0,0,0,0.0,"Implement a --config-test to check that configuration is correct $end$ It would be good do have a configuration sanity check in the form:

$ maxscale --config-test

or

$ /etc/init.d/maxscale configtest


It could be implemented in the following way:

When MaxScale starts with such option should collect all options(in an array) from all modules (which should expose/register them), each option could be a structure of the type  [optname, array{opttype, allowed_values} ].

Not sure that the internal array for each option is needed, but using such structures allows to use multiple types(with their type and range check) for options that can be set to more than one type, e.g.:

address= [ip | hostname]

something= 0,1,ON,OFF


Meaningful error messages could be:

Error: Unknown option 'tipe' at row 12

Error: Value '999' for option 'threads' is out of range

Error: Value '192.168.1.' for option 'address' is not valid, expected IP or HOSTNAME  (this example is tricky as the mistyped IP could also be an hostname).

 $acceptance criteria:$",0,0,0,0,0,0,0,93.7167,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1892,MXS-723,Task,MXS,2016-05-16 08:27:18,,0,test case for MXS-683,,,test case for MXS-683 $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-18 05:34:47,test case for MXS-683,,,0,0,0,0,0.0,test case for MXS-683 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,45.1167,10,1,0.1,0,0.0,0,0.0,0,0.0,0,0.0
1893,MXS-724,Task,MXS,2016-05-16 08:29:10,,0,test case for MXS-682,,,test case for MXS-682 $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,5,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-05-18 05:34:47,test case for MXS-682,,,0,0,0,0,0.0,test case for MXS-682 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,45.0833,11,1,0.0909091,0,0.0,0,0.0,0,0.0,0,0.0
1894,MXS-725,Task,MXS,2016-05-16 08:37:54,,0,test case for MXS-716,,,test case for MXS-716 $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-18 05:34:37,test case for MXS-716,,,0,0,0,0,0.0,test case for MXS-716 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,44.9333,12,1,0.0833333,0,0.0,0,0.0,0,0.0,0,0.0
1895,MXS-726,Task,MXS,2016-05-16 10:08:09,,0,test case for MXS-712,,,test case for MXS-712 $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-18 05:34:47,test case for MXS-712,,,0,0,0,0,0.0,test case for MXS-712 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,43.4333,13,1,0.0769231,0,0.0,0,0.0,0,0.0,0,0.0
1896,MXS-727,Task,MXS,2016-05-16 10:10:00,,0,test case for MXS-710,,,test case for MXS-710 $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,1,0,0,0,,0,850,0,0,0,2016-05-18 05:34:47,test case for MXS-710,,,0,0,0,0,0.0,test case for MXS-710 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,43.4,14,1,0.0714286,0,0.0,0,0.0,0,0.0,0,0.0
1897,MXS-728,Task,MXS,2016-05-16 10:13:04,,0,test case for MXS-705,,,test case for MXS-705 $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-18 05:34:47,test case for MXS-705,,,0,0,0,0,0.0,test case for MXS-705 $end$ $acceptance criteria:$,0,0,0,0,0,0,0,43.35,15,1,0.0666667,0,0.0,0,0.0,0,0.0,0,0.0
1898,MXS-729,Task,MXS,2016-05-17 09:40:46,,0,MaxAdmin security modification.,"* Access only from same host.
* Initially root can access.
* Root can add and remove user-ids.
* Thereafter possible to use maxadmin as other than root.
",,"MaxAdmin security modification. $end$ * Access only from same host.
* Initially root can access.
* Root can add and remove user-ids.
* Thereafter possible to use maxadmin as other than root.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,2,0,1,0,0,0,,0,850,2,0,0,2016-05-17 09:40:46,MaxAdmin security modification.,"* Access only from same host.
* Initially root can access.
* Root can add and remove user-ids.
* Thereafter possible to use maxadmin as other than root.
",,0,0,0,0,0.0,"MaxAdmin security modification. $end$ * Access only from same host.
* Initially root can access.
* Root can add and remove user-ids.
* Thereafter possible to use maxadmin as other than root.
 $acceptance criteria:$",0,0,0,0,0,0,0,0.0,39,11,0.282051,3,0.0769231,2,0.0512821,0,0.0,0,0.0
1899,MXS-730,Task,MXS,2016-05-17 09:42:33,,0,MaxAdmin test,,,MaxAdmin test $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-17 09:42:33,MaxAdmin test,,,0,0,0,0,0.0,MaxAdmin test $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,40,11,0.275,3,0.075,2,0.05,0,0.0,0,0.0
1900,MXS-740,New Feature,MXS,2016-05-27 12:35:57,,0,More graceful maintenance mode,"Currently, in case of a setup the readconnroute with router_options=master we redirect all traffic to the master. When enabling maintenance mode for this node with _set server test1 maintenance_, incoming connections are refused  until the next monitor_interval is hit.

It would be a lot more graceful if connections are always accepted. This could be achieved by enabling maintenance mode only with the next monitoring mode. Please implement this so enabling maintenance mode is more graceful.",,"More graceful maintenance mode $end$ Currently, in case of a setup the readconnroute with router_options=master we redirect all traffic to the master. When enabling maintenance mode for this node with _set server test1 maintenance_, incoming connections are refused  until the next monitor_interval is hit.

It would be a lot more graceful if connections are always accepted. This could be achieved by enabling maintenance mode only with the next monitoring mode. Please implement this so enabling maintenance mode is more graceful. $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,5,,0,0,1,1,0,0,0,,0,850,0,0,0,2016-12-14 10:34:14,More graceful maintenance mode,"Currently, in case of a setup the readconnroute with router_options=master we redirect all traffic to the master. When enabling maintenance mode for this node with _set server test1 maintenance_, incoming connections are refused  until the next monitor_interval is hit.

It would be a lot more graceful if connections are always accepted. This could be achieved by enabling maintenance mode only with the next monitoring mode. Please implement this so enabling maintenance mode is more graceful.",,0,0,0,0,0.0,"More graceful maintenance mode $end$ Currently, in case of a setup the readconnroute with router_options=master we redirect all traffic to the master. When enabling maintenance mode for this node with _set server test1 maintenance_, incoming connections are refused  until the next monitor_interval is hit.

It would be a lot more graceful if connections are always accepted. This could be achieved by enabling maintenance mode only with the next monitoring mode. Please implement this so enabling maintenance mode is more graceful. $acceptance criteria:$",0,0,0,0,0,0,0,4821.97,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1901,MXS-743,Task,MXS,2016-05-31 05:46:52,,0,2.0.0 release preparations.,,,2.0.0 release preparations. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,2,0,4,0,0,8,,0,850,2,0,0,2016-05-31 09:13:55,2.0.0 release preparations.,,,0,0,0,0,0.0,2.0.0 release preparations. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,3.45,41,11,0.268293,3,0.0731707,2,0.0487805,0,0.0,0,0.0
1902,MXS-744,Sub-Task,MXS,2016-05-31 05:47:22,,0,2.0.0: ReleaseNotes,,,2.0.0: ReleaseNotes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:47:22,2.0.0: ReleaseNotes,,,0,0,0,0,0.0,2.0.0: ReleaseNotes $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,42,11,0.261905,3,0.0714286,2,0.047619,0,0.0,0,0.0
1903,MXS-745,Sub-Task,MXS,2016-05-31 05:47:33,,0,2.0.0: Change Log,,,2.0.0: Change Log $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:47:33,2.0.0: Change Log,,,0,0,0,0,0.0,2.0.0: Change Log $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,43,11,0.255814,3,0.0697674,2,0.0465116,0,0.0,0,0.0
1904,MXS-746,Sub-Task,MXS,2016-05-31 05:47:48,,0,2.0.0: Upgrading To,,,2.0.0: Upgrading To $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:47:48,2.0.0: Upgrading To,,,0,0,0,0,0.0,2.0.0: Upgrading To $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,44,11,0.25,3,0.0681818,2,0.0454545,0,0.0,0,0.0
1905,MXS-747,Sub-Task,MXS,2016-05-31 05:48:00,,0,2.0.0: Verify Documentation,,,2.0.0: Verify Documentation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:48:00,2.0.0: Verify Documentation,,,0,0,0,0,0.0,2.0.0: Verify Documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,45,11,0.244444,3,0.0666667,2,0.0444444,0,0.0,0,0.0
1906,MXS-748,Sub-Task,MXS,2016-05-31 05:48:10,,0,2.0.0: Build and Upload Binaries,,,2.0.0: Build and Upload Binaries $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:48:10,2.0.0: Build and Upload Binaries,,,0,0,0,0,0.0,2.0.0: Build and Upload Binaries $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,46,11,0.23913,3,0.0652174,2,0.0434783,0,0.0,0,0.0
1907,MXS-749,Sub-Task,MXS,2016-05-31 05:48:35,,0,2.0.0: Regression Testing,,,2.0.0: Regression Testing $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:48:35,2.0.0: Regression Testing,,,0,0,0,0,0.0,2.0.0: Regression Testing $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,47,11,0.234043,3,0.0638298,2,0.0425532,0,0.0,0,0.0
1908,MXS-750,Sub-Task,MXS,2016-05-31 05:49:46,,0,2.0.0: Sync Documentation to KB,,,2.0.0: Sync Documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-05-31 05:49:46,2.0.0: Sync Documentation to KB,,,0,0,0,0,0.0,2.0.0: Sync Documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,48,11,0.229167,3,0.0625,2,0.0416667,0,0.0,0,0.0
1909,MXS-751,Task,MXS,2016-05-31 08:23:26,,0,Update documentation - Change description of MaxScale,"Where ever in the documentation or readme (https://github.com/mariadb-corporation/maxscale/blob/master/README) we are referring to MaxScale as
-  ""intelligent proxy"" needs to be replaced with ""dynamic data routing platform""
- ""database centric proxy"" needs to be replaced with ""dynamic data routing platform""
- just as ""proxy"" needs to be replaced with ""dynamic data routing platform""

Additionally where ever it says MaxScale without MariaDB - it needs to say ""MariaDB MaxScale""",,"Update documentation - Change description of MaxScale $end$ Where ever in the documentation or readme (https://github.com/mariadb-corporation/maxscale/blob/master/README) we are referring to MaxScale as
-  ""intelligent proxy"" needs to be replaced with ""dynamic data routing platform""
- ""database centric proxy"" needs to be replaced with ""dynamic data routing platform""
- just as ""proxy"" needs to be replaced with ""dynamic data routing platform""

Additionally where ever it says MaxScale without MariaDB - it needs to say ""MariaDB MaxScale"" $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,7,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-31 10:04:05,Update documentation - Change description of MaxScale,"Where ever in the documentation or readme (https://github.com/mariadb-corporation/maxscale/blob/master/README) we are referring to MaxScale as
-  ""intelligent proxy"" needs to be replaced with ""dynamic data routing platform""
- ""database centric proxy"" needs to be replaced with ""dynamic data routing platform""
- just as ""proxy"" needs to be replaced with ""dynamic data routing platform""

Additionally where ever it says MaxScale without MariaDB - it needs to say ""MariaDB MaxScale""",,0,0,0,0,0.0,"Update documentation - Change description of MaxScale $end$ Where ever in the documentation or readme (https://github.com/mariadb-corporation/maxscale/blob/master/README) we are referring to MaxScale as
-  ""intelligent proxy"" needs to be replaced with ""dynamic data routing platform""
- ""database centric proxy"" needs to be replaced with ""dynamic data routing platform""
- just as ""proxy"" needs to be replaced with ""dynamic data routing platform""

Additionally where ever it says MaxScale without MariaDB - it needs to say ""MariaDB MaxScale"" $acceptance criteria:$",0,0,0,0,0,0,0,1.66667,21,4,0.190476,1,0.047619,0,0.0,0,0.0,0,0.0
1910,MXS-752,Task,MXS,2016-05-31 09:51:46,,0,Update Avro documentation,The documentation should be updated and cleaned up.,,Update Avro documentation $end$ The documentation should be updated and cleaned up. $acceptance criteria:$,,markus makela,markus makela,Major,6,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-05-31 09:52:38,Update Avro documentation,The documentation should be updated and cleaned up.,,0,0,0,0,0.0,Update Avro documentation $end$ The documentation should be updated and cleaned up. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,8,3,0.375,3,0.375,3,0.375,3,0.375,3,0.375
1911,MXS-753,Task,MXS,2016-05-31 10:00:15,,0,backend ssl tests,,,backend ssl tests $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-05-31 10:19:25,backend ssl tests,,,0,0,0,0,0.0,backend ssl tests $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.316667,16,1,0.0625,0,0.0,0,0.0,0,0.0,0,0.0
1912,MXS-754,New Feature,MXS,2016-06-02 19:57:24,,0,Add option to Query Log All Filter to log all matches to a single file,"I ran into a situation recently where this ability might have been helpful to effectively have an equivalent to the MariaDB general query log via MaxScale. If this feature was added I'm certain I would use it. If this feature exists and I'm missing it, my apologies. I have experimented with enabling log_info to get the information I needed but even logging to a tmpfs MaxScale ran too slowly.",,"Add option to Query Log All Filter to log all matches to a single file $end$ I ran into a situation recently where this ability might have been helpful to effectively have an equivalent to the MariaDB general query log via MaxScale. If this feature was added I'm certain I would use it. If this feature exists and I'm missing it, my apologies. I have experimented with enabling log_info to get the information I needed but even logging to a tmpfs MaxScale ran too slowly. $acceptance criteria:$",,Mathew Hornbeek,Mathew Hornbeek,Minor,9,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-11-02 10:51:20,Add option to Query Log All Filter to log all matches to a single file,"I ran into a situation recently where this ability might have been helpful to effectively have an equivalent to the MariaDB general query log via MaxScale. If this feature was added I'm certain I would use it. If this feature exists and I'm missing it, my apologies. I have experimented with enabling log_info to get the information I needed but even logging to a tmpfs MaxScale ran too slowly.",,0,0,0,0,0.0,"Add option to Query Log All Filter to log all matches to a single file $end$ I ran into a situation recently where this ability might have been helpful to effectively have an equivalent to the MariaDB general query log via MaxScale. If this feature was added I'm certain I would use it. If this feature exists and I'm missing it, my apologies. I have experimented with enabling log_info to get the information I needed but even logging to a tmpfs MaxScale ran too slowly. $acceptance criteria:$",0,0,0,0,0,0,0,3662.88,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1913,MXS-755,New Feature,MXS,2016-06-03 18:54:13,,0,FR: Hint filters for stored procedures ,"This is a feature request to have hint filters working with stored procedures.
Our Customer C******* uses Stored Procedure as a practice, 90% of them are SELECTs only and are always routed by MaxScale to the server.

Please modify hints so SPs can be routed to the slave on request. ",,"FR: Hint filters for stored procedures  $end$ This is a feature request to have hint filters working with stored procedures.
Our Customer C******* uses Stored Procedure as a practice, 90% of them are SELECTs only and are always routed by MaxScale to the server.

Please modify hints so SPs can be routed to the slave on request.  $acceptance criteria:$",,Guillaume Lefranc,Guillaume Lefranc,Major,6,,0,2,0,1,0,0,0,,0,850,1,0,0,2016-07-19 09:48:38,FR: Hint filters for stored procedures ,"This is a feature request to have hint filters working with stored procedures.
Our Customer C******* uses Stored Procedure as a practice, 90% of them are SELECTs only and are always routed by MaxScale to the server.

Please modify hints so SPs can be routed to the slave on request. ",,0,0,0,0,0.0,"FR: Hint filters for stored procedures  $end$ This is a feature request to have hint filters working with stored procedures.
Our Customer C******* uses Stored Procedure as a practice, 90% of them are SELECTs only and are always routed by MaxScale to the server.

Please modify hints so SPs can be routed to the slave on request.  $acceptance criteria:$",0,0,0,0,0,0,0,1094.9,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1914,MXS-756,New Feature,MXS,2016-06-06 08:34:10,,0,Retry read after slave failure,"When a read query is interrupted by a slave failure, it should be possible to retry it on another slave or on the master. Since there's no guarantee that a statement executed on the slave and the master returns the same result, we can safely retry any query that does not modify the session state.

All _critical statements_(transactions, writes) are always sent to the master and the rest of the statements that are sent to the slaves could be classified as non-critical. This means that no hard dependencies on the data are present in any of the statements and some degree of slave lag is acceptable. This makes it safe to retry the queries in most situations.

This new feature should be configurable so that old failure behavior could be used.",,"Retry read after slave failure $end$ When a read query is interrupted by a slave failure, it should be possible to retry it on another slave or on the master. Since there's no guarantee that a statement executed on the slave and the master returns the same result, we can safely retry any query that does not modify the session state.

All _critical statements_(transactions, writes) are always sent to the master and the rest of the statements that are sent to the slaves could be classified as non-critical. This means that no hard dependencies on the data are present in any of the statements and some degree of slave lag is acceptable. This makes it safe to retry the queries in most situations.

This new feature should be configurable so that old failure behavior could be used. $acceptance criteria:$",,markus makela,markus makela,Major,7,,1,1,1,1,0,0,0,,0,850,1,0,0,2016-11-30 09:56:36,Retry read after slave failure,"When a read query is interrupted by a slave failure, it should be possible to retry it on another slave or on the master. Since there's no guarantee that a statement executed on the slave and the master returns the same result, we can safely retry any query that does not modify the session state.

All _critical statements_(transactions, writes) are always sent to the master and the rest of the statements that are sent to the slaves could be classified as non-critical. This means that no hard dependencies on the data are present in any of the statements and some degree of slave lag is acceptable. This makes it safe to retry the queries in most situations.

This new feature should be configurable so that old failure behavior could be used.",,0,0,0,0,0.0,"Retry read after slave failure $end$ When a read query is interrupted by a slave failure, it should be possible to retry it on another slave or on the master. Since there's no guarantee that a statement executed on the slave and the master returns the same result, we can safely retry any query that does not modify the session state.

All _critical statements_(transactions, writes) are always sent to the master and the rest of the statements that are sent to the slaves could be classified as non-critical. This means that no hard dependencies on the data are present in any of the statements and some degree of slave lag is acceptable. This makes it safe to retry the queries in most situations.

This new feature should be configurable so that old failure behavior could be used. $acceptance criteria:$",0,0,0,0,0,0,0,4249.37,9,3,0.333333,3,0.333333,3,0.333333,3,0.333333,3,0.333333
1915,MXS-760,New Feature,MXS,2016-06-09 11:38:12,MXS-2532,0,Allow different sets of users on backend servers,"It would be great to allow each server to have its own users, which could authenticate only to this particular server. Currently every new connection is authenticated against all backend servers.

I can see many use cases that would benefit from this feature, for example:

Creating a cluster from completely independent db servers
 - one or more Maxscale instances as entry points
 - every backend server has its own users and databases
 - migration of any database (with its own users) between backends
 - scallability: more backend servers can be added and populated with existing or new databases

 Main advantage is that you can migrate databases between backend servers without changing hostname of the database and connection strings in applications.


",,"Allow different sets of users on backend servers $end$ It would be great to allow each server to have its own users, which could authenticate only to this particular server. Currently every new connection is authenticated against all backend servers.

I can see many use cases that would benefit from this feature, for example:

Creating a cluster from completely independent db servers
 - one or more Maxscale instances as entry points
 - every backend server has its own users and databases
 - migration of any database (with its own users) between backends
 - scallability: more backend servers can be added and populated with existing or new databases

 Main advantage is that you can migrate databases between backend servers without changing hostname of the database and connection strings in applications.


 $acceptance criteria:$",,David Cigánek,David Cigánek,Major,25,,0,1,1,1,0,1,0,,0,850,1,0,0,2019-10-07 10:06:29,schema router - allow different sets of users on backend servers,"It would be great to allow each server to have its own users, which could authenticate only to this particular server. Currently every new connection is authenticated against all backend servers.

I can see many use cases that would benefit from this feature, for example:

Creating a cluster from completely independent db servers
 - one or more Maxscale instances as entry points
 - every backend server has its own users and databases
 - migration of any database (with its own users) between backends
 - scallability: more backend servers can be added and populated with existing or new databases

 Main advantage is that you can migrate databases between backend servers without changing hostname of the database and connection strings in applications.


",,1,0,0,5,0.0298507,"schema router - allow different sets of users on backend servers $end$ It would be great to allow each server to have its own users, which could authenticate only to this particular server. Currently every new connection is authenticated against all backend servers.

I can see many use cases that would benefit from this feature, for example:

Creating a cluster from completely independent db servers
 - one or more Maxscale instances as entry points
 - every backend server has its own users and databases
 - migration of any database (with its own users) between backends
 - scallability: more backend servers can be added and populated with existing or new databases

 Main advantage is that you can migrate databases between backend servers without changing hostname of the database and connection strings in applications.


 $acceptance criteria:$",1,1,1,0,0,0,1,29158.5,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1916,MXS-761,Task,MXS,2016-06-14 07:24:32,,0,Find and fix SSL unreliability,"Using SSL in the monitor and service communication seems to be unreliable in some contexts when backend SSL is used. The cause should be identified so that SSL can be used in all communication with the backend.
",,"Find and fix SSL unreliability $end$ Using SSL in the monitor and service communication seems to be unreliable in some contexts when backend SSL is used. The cause should be identified so that SSL can be used in all communication with the backend.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,2,0,2,0,0,0,,0,850,2,0,0,2016-06-14 07:24:32,Find and fix SSL unreliability,"Using SSL in the monitor and service communication seems to be unreliable in some contexts when backend SSL is used. The cause should be identified so that SSL can be used in all communication with the backend.
",,0,0,0,0,0.0,"Find and fix SSL unreliability $end$ Using SSL in the monitor and service communication seems to be unreliable in some contexts when backend SSL is used. The cause should be identified so that SSL can be used in all communication with the backend.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,49,11,0.22449,3,0.0612245,2,0.0408163,0,0.0,0,0.0
1917,MXS-762,Task,MXS,2016-06-14 09:57:27,,0,Create package.,,,Create package. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,1,0,1,0,,0,850,1,0,0,2016-06-14 09:57:27,Create booking.com package.,,,1,0,0,1,0.166667,Create booking.com package. $end$ $acceptance criteria:$,1,1,0,0,0,0,0,0.0,50,11,0.22,3,0.06,2,0.04,0,0.0,0,0.0
1918,MXS-763,Task,MXS,2016-06-14 09:59:12,,0,Improving Avro error messages and documentation,,,Improving Avro error messages and documentation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-06-14 09:59:12,Improving Avro error messages and documentation,,,0,0,0,0,0.0,Improving Avro error messages and documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,51,12,0.235294,3,0.0588235,2,0.0392157,0,0.0,0,0.0
1919,MXS-770,New Feature,MXS,2016-06-16 11:13:41,,0,"Add ""PURGE { BINARY | MASTER } LOGS"" functionality","Please add support for PURGE { BINARY | MASTER } LOGS. This is important in managing the binary logs and disk usage.

~Hareesh",,"Add ""PURGE { BINARY | MASTER } LOGS"" functionality $end$ Please add support for PURGE { BINARY | MASTER } LOGS. This is important in managing the binary logs and disk usage.

~Hareesh $acceptance criteria:$",,Hareesh Haridas,Hareesh Haridas,Major,11,,1,2,1,1,0,0,0,,0,850,2,0,0,2017-07-27 09:56:07,"Add ""PURGE { BINARY | MASTER } LOGS"" functionality","Please add support for PURGE { BINARY | MASTER } LOGS. This is important in managing the binary logs and disk usage.

~Hareesh",,0,0,0,0,0.0,"Add ""PURGE { BINARY | MASTER } LOGS"" functionality $end$ Please add support for PURGE { BINARY | MASTER } LOGS. This is important in managing the binary logs and disk usage.

~Hareesh $acceptance criteria:$",0,0,0,0,0,0,0,9742.7,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1920,MXS-781,New Feature,MXS,2016-06-23 19:22:44,,0,binlog router does not give proper error when authentication with master fails,"Let's say that I have a binlog router instance with the following configuration:

{noformat}
[maxscale]
threads=4
log_debug=1
log_trace=1

[Replication]
type=service
router=binlogrouter
version_string=10.0.25-log
user=maxscale
passwd=password
router_options=server_id=2,mariadb10-compatibility=1,binlogdir=/var/log/binlogs

[Replication Listener]
type=listener
service=Replication
protocol=MySQLClient
port=3306
{noformat}

And then I create a replication user on the master:

{noformat}
GRANT ALL PRIVILEGES ON *.* TO 'repl'@'%' IDENTIFIED BY 'password';
{noformat}

Now let's say that I execute CHANGE MASTER TO on the binlog router instance, but I supply the wrong password for master_password:

{noformat}
CHANGE MASTER TO master_host='172.31.32.117',master_port=3306,master_user='repl',master_password='password1', MASTER_LOG_FILE='mysqld-bin.000002',MASTER_LOG_POS=4;
START SLAVE;
{noformat}

This will fail to connect due to the incorrect password, but MaxScale seems to ignore this in SHOW SLAVE STATUS. Instead, it suggests that Slave_IO_State is ""Timestamp retrieval"" and Last_Errno is 0.

{noformat}
MySQL [(none)]> SHOW SLAVE STATUS\G
*************************** 1. row ***************************
               Slave_IO_State: Timestamp retrieval
                  Master_Host: 172.31.32.117
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysqld-bin.000002
          Read_Master_Log_Pos: 4
               Relay_Log_File: mysqld-bin.000002
                Relay_Log_Pos: 4
        Relay_Master_Log_File: mysqld-bin.000002
             Slave_IO_Running: Connecting
            Slave_SQL_Running: Yes
              Replicate_Do_DB:
          Replicate_Ignore_DB:
           Replicate_Do_Table:
       Replicate_Ignore_Table:
      Replicate_Wild_Do_Table:
  Replicate_Wild_Ignore_Table:
                   Last_Errno: 0
                   Last_Error:
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 4
              Relay_Log_Space: 4
              Until_Condition: None
               Until_Log_File:
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File:
           Master_SSL_CA_Path:
              Master_SSL_Cert:
            Master_SSL_Cipher:
               Master_SSL_Key:
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error:
               Last_SQL_Errno: 0
               Last_SQL_Error:
  Replicate_Ignore_Server_Ids:
             Master_Server_Id: 0
                  Master_UUID: 57e87f43-3975-11e6-9925-06916daa38f7
             Master_Info_File:
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave running
           Master_Retry_Count: 1000
                  Master_Bind:
      Last_IO_Error_TimeStamp:
     Last_SQL_Error_Timestamp:
               Master_SSL_Crl:
           Master_SSL_Crlpath:
           Retrieved_Gtid_Set:
            Executed_Gtid_Set:
                Auto_Position:
1 row in set (0.00 sec)
{noformat}

The log does seem to contain an error message when this happens, so it seems that MaxScale is aware of the failure:

{noformat}
2016-06-23 15:06:31   [2]  info   : Execute statement (truncated, it contains password) from the slave 'change master to master_host='172.31.32.117',master_port=3306,master_user='repl',master_...'
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_USER is [repl]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_HOST is [172.31.32.117]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_PORT is [3306]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_FILE is [mysqld-bin.000002]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_POS is [4]
2016-06-23 15:06:31   notice : Replication: 'CHANGE MASTER TO executed'. Previous state MASTER_HOST='_none_', MASTER_PORT=3306, MASTER_LOG_FILE='', MASTER_LOG_POS=0, MASTER_USER='maxscale'. New state is MASTER_HOST='172.31.32.117', MASTER_PORT=3306, MASTER_LOG_FILE='mysqld-bin.000002', MASTER_LOG_POS=4, MASTER_USER='repl'
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_write] Append to writequeue. 4 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
...snip...
2016-06-23 15:06:49   [2]  info   : Execute statement from the slave 'start slave'
2016-06-23 15:06:49   [2]  info   : Started session [0] for Replication service
2016-06-23 15:06:49   notice : Loaded module MySQLBackend: V2.0.0 from /usr/lib64/maxscale/libMySQLBackend.so
2016-06-23 15:06:49   debug  : 140013001512704 [gw_do_connect_to_backend] Connected to backend server 172.31.32.117:3306, fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_create_backend_connection] Connection pending to 172.31.32.117:3306, protocol fd 13 client fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_connect] Connected to server 172.31.32.117:3306, from backend dcb 0x7f5744000ee0, client dcp 0x7f5744000b00 fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_add_dcb] Added dcb 0x7f5744000ee0 in state DCB_STATE_POLLING to poll set.
2016-06-23 15:06:49   notice : Replication: attempting to connect to master server 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4
2016-06-23 15:06:49   debug  : 140013001512704 [gw_MySQLWrite_backend] delayed write to dcb 0x7f5744000ee0 fd 13 protocol state MYSQL_PENDING_CONNECT.
2016-06-23 15:06:49   notice : Replication: START SLAVE executed by maxscale@127.0.0.1. Trying connection to master 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4, transaction safe pos 4
2016-06-23 15:06:49   debug  : 140013442922464 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013012002560 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : Loading data from backend database [172.31.32.117:3306] for service [Replication]
2016-06-23 15:06:49   debug  : Replication: Adding database db1 to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database information_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database mysql to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database performance_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database test to the resouce hash.
2016-06-23 15:06:49   debug  : Loaded 5 MySQL Database Names for service [Replication]
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@localhost for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@% for database no db added to service user table.
2016-06-23 15:06:49   warning: Duplicate MySQL user found for service [Replication]: maxscale@% for database: (null)
2016-06-23 15:06:49   [2]  info   : Replication: User repl@% for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User sst@% for database no db added to service user table.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 121 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 5 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 2, MYSQL_CONNECTED.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 99 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 1 writes buffered for dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 84 Bytes to dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Read zero bytes from backend dcb 0x7f5744000ee0 fd 13 in state DCB_STATE_POLLING. n 0, head (nil), len 0
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] gw_receive_backend_auth read successfully nothing. dcb 0x7f5744000ee0 fd 13, user repl.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 8197 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 4, MYSQL_AUTH_RECV.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 110 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Invalid authentication message from backend dcb 0x7f5744000ee0 fd 13, ptr[4] = 255, error 28000, msg Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES).
2016-06-23 15:06:49   error  : Invalid authentication message from backend. Error : 28000, Msg : Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES)
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] after gw_receive_backend_authentication fd 13, state = MYSQL_AUTH_FAILED.
2016-06-23 15:06:49   error  : Backend server didn't accept authentication for user repl.
{noformat}",,"binlog router does not give proper error when authentication with master fails $end$ Let's say that I have a binlog router instance with the following configuration:

{noformat}
[maxscale]
threads=4
log_debug=1
log_trace=1

[Replication]
type=service
router=binlogrouter
version_string=10.0.25-log
user=maxscale
passwd=password
router_options=server_id=2,mariadb10-compatibility=1,binlogdir=/var/log/binlogs

[Replication Listener]
type=listener
service=Replication
protocol=MySQLClient
port=3306
{noformat}

And then I create a replication user on the master:

{noformat}
GRANT ALL PRIVILEGES ON *.* TO 'repl'@'%' IDENTIFIED BY 'password';
{noformat}

Now let's say that I execute CHANGE MASTER TO on the binlog router instance, but I supply the wrong password for master_password:

{noformat}
CHANGE MASTER TO master_host='172.31.32.117',master_port=3306,master_user='repl',master_password='password1', MASTER_LOG_FILE='mysqld-bin.000002',MASTER_LOG_POS=4;
START SLAVE;
{noformat}

This will fail to connect due to the incorrect password, but MaxScale seems to ignore this in SHOW SLAVE STATUS. Instead, it suggests that Slave_IO_State is ""Timestamp retrieval"" and Last_Errno is 0.

{noformat}
MySQL [(none)]> SHOW SLAVE STATUS\G
*************************** 1. row ***************************
               Slave_IO_State: Timestamp retrieval
                  Master_Host: 172.31.32.117
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysqld-bin.000002
          Read_Master_Log_Pos: 4
               Relay_Log_File: mysqld-bin.000002
                Relay_Log_Pos: 4
        Relay_Master_Log_File: mysqld-bin.000002
             Slave_IO_Running: Connecting
            Slave_SQL_Running: Yes
              Replicate_Do_DB:
          Replicate_Ignore_DB:
           Replicate_Do_Table:
       Replicate_Ignore_Table:
      Replicate_Wild_Do_Table:
  Replicate_Wild_Ignore_Table:
                   Last_Errno: 0
                   Last_Error:
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 4
              Relay_Log_Space: 4
              Until_Condition: None
               Until_Log_File:
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File:
           Master_SSL_CA_Path:
              Master_SSL_Cert:
            Master_SSL_Cipher:
               Master_SSL_Key:
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error:
               Last_SQL_Errno: 0
               Last_SQL_Error:
  Replicate_Ignore_Server_Ids:
             Master_Server_Id: 0
                  Master_UUID: 57e87f43-3975-11e6-9925-06916daa38f7
             Master_Info_File:
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave running
           Master_Retry_Count: 1000
                  Master_Bind:
      Last_IO_Error_TimeStamp:
     Last_SQL_Error_Timestamp:
               Master_SSL_Crl:
           Master_SSL_Crlpath:
           Retrieved_Gtid_Set:
            Executed_Gtid_Set:
                Auto_Position:
1 row in set (0.00 sec)
{noformat}

The log does seem to contain an error message when this happens, so it seems that MaxScale is aware of the failure:

{noformat}
2016-06-23 15:06:31   [2]  info   : Execute statement (truncated, it contains password) from the slave 'change master to master_host='172.31.32.117',master_port=3306,master_user='repl',master_...'
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_USER is [repl]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_HOST is [172.31.32.117]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_PORT is [3306]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_FILE is [mysqld-bin.000002]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_POS is [4]
2016-06-23 15:06:31   notice : Replication: 'CHANGE MASTER TO executed'. Previous state MASTER_HOST='_none_', MASTER_PORT=3306, MASTER_LOG_FILE='', MASTER_LOG_POS=0, MASTER_USER='maxscale'. New state is MASTER_HOST='172.31.32.117', MASTER_PORT=3306, MASTER_LOG_FILE='mysqld-bin.000002', MASTER_LOG_POS=4, MASTER_USER='repl'
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_write] Append to writequeue. 4 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
...snip...
2016-06-23 15:06:49   [2]  info   : Execute statement from the slave 'start slave'
2016-06-23 15:06:49   [2]  info   : Started session [0] for Replication service
2016-06-23 15:06:49   notice : Loaded module MySQLBackend: V2.0.0 from /usr/lib64/maxscale/libMySQLBackend.so
2016-06-23 15:06:49   debug  : 140013001512704 [gw_do_connect_to_backend] Connected to backend server 172.31.32.117:3306, fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_create_backend_connection] Connection pending to 172.31.32.117:3306, protocol fd 13 client fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_connect] Connected to server 172.31.32.117:3306, from backend dcb 0x7f5744000ee0, client dcp 0x7f5744000b00 fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_add_dcb] Added dcb 0x7f5744000ee0 in state DCB_STATE_POLLING to poll set.
2016-06-23 15:06:49   notice : Replication: attempting to connect to master server 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4
2016-06-23 15:06:49   debug  : 140013001512704 [gw_MySQLWrite_backend] delayed write to dcb 0x7f5744000ee0 fd 13 protocol state MYSQL_PENDING_CONNECT.
2016-06-23 15:06:49   notice : Replication: START SLAVE executed by maxscale@127.0.0.1. Trying connection to master 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4, transaction safe pos 4
2016-06-23 15:06:49   debug  : 140013442922464 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013012002560 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : Loading data from backend database [172.31.32.117:3306] for service [Replication]
2016-06-23 15:06:49   debug  : Replication: Adding database db1 to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database information_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database mysql to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database performance_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database test to the resouce hash.
2016-06-23 15:06:49   debug  : Loaded 5 MySQL Database Names for service [Replication]
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@localhost for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@% for database no db added to service user table.
2016-06-23 15:06:49   warning: Duplicate MySQL user found for service [Replication]: maxscale@% for database: (null)
2016-06-23 15:06:49   [2]  info   : Replication: User repl@% for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User sst@% for database no db added to service user table.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 121 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 5 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 2, MYSQL_CONNECTED.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 99 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 1 writes buffered for dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 84 Bytes to dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Read zero bytes from backend dcb 0x7f5744000ee0 fd 13 in state DCB_STATE_POLLING. n 0, head (nil), len 0
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] gw_receive_backend_auth read successfully nothing. dcb 0x7f5744000ee0 fd 13, user repl.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 8197 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 4, MYSQL_AUTH_RECV.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 110 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Invalid authentication message from backend dcb 0x7f5744000ee0 fd 13, ptr[4] = 255, error 28000, msg Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES).
2016-06-23 15:06:49   error  : Invalid authentication message from backend. Error : 28000, Msg : Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES)
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] after gw_receive_backend_authentication fd 13, state = MYSQL_AUTH_FAILED.
2016-06-23 15:06:49   error  : Backend server didn't accept authentication for user repl.
{noformat} $acceptance criteria:$",,Geoff Montee,Geoff Montee,Minor,8,,0,3,0,1,0,0,0,,0,850,2,0,0,2016-07-05 09:56:22,binlog router does not give proper error when authentication with master fails,"Let's say that I have a binlog router instance with the following configuration:

{noformat}
[maxscale]
threads=4
log_debug=1
log_trace=1

[Replication]
type=service
router=binlogrouter
version_string=10.0.25-log
user=maxscale
passwd=password
router_options=server_id=2,mariadb10-compatibility=1,binlogdir=/var/log/binlogs

[Replication Listener]
type=listener
service=Replication
protocol=MySQLClient
port=3306
{noformat}

And then I create a replication user on the master:

{noformat}
GRANT ALL PRIVILEGES ON *.* TO 'repl'@'%' IDENTIFIED BY 'password';
{noformat}

Now let's say that I execute CHANGE MASTER TO on the binlog router instance, but I supply the wrong password for master_password:

{noformat}
CHANGE MASTER TO master_host='172.31.32.117',master_port=3306,master_user='repl',master_password='password1', MASTER_LOG_FILE='mysqld-bin.000002',MASTER_LOG_POS=4;
START SLAVE;
{noformat}

This will fail to connect due to the incorrect password, but MaxScale seems to ignore this in SHOW SLAVE STATUS. Instead, it suggests that Slave_IO_State is ""Timestamp retrieval"" and Last_Errno is 0.

{noformat}
MySQL [(none)]> SHOW SLAVE STATUS\G
*************************** 1. row ***************************
               Slave_IO_State: Timestamp retrieval
                  Master_Host: 172.31.32.117
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysqld-bin.000002
          Read_Master_Log_Pos: 4
               Relay_Log_File: mysqld-bin.000002
                Relay_Log_Pos: 4
        Relay_Master_Log_File: mysqld-bin.000002
             Slave_IO_Running: Connecting
            Slave_SQL_Running: Yes
              Replicate_Do_DB:
          Replicate_Ignore_DB:
           Replicate_Do_Table:
       Replicate_Ignore_Table:
      Replicate_Wild_Do_Table:
  Replicate_Wild_Ignore_Table:
                   Last_Errno: 0
                   Last_Error:
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 4
              Relay_Log_Space: 4
              Until_Condition: None
               Until_Log_File:
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File:
           Master_SSL_CA_Path:
              Master_SSL_Cert:
            Master_SSL_Cipher:
               Master_SSL_Key:
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error:
               Last_SQL_Errno: 0
               Last_SQL_Error:
  Replicate_Ignore_Server_Ids:
             Master_Server_Id: 0
                  Master_UUID: 57e87f43-3975-11e6-9925-06916daa38f7
             Master_Info_File:
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave running
           Master_Retry_Count: 1000
                  Master_Bind:
      Last_IO_Error_TimeStamp:
     Last_SQL_Error_Timestamp:
               Master_SSL_Crl:
           Master_SSL_Crlpath:
           Retrieved_Gtid_Set:
            Executed_Gtid_Set:
                Auto_Position:
1 row in set (0.00 sec)
{noformat}

The log does seem to contain an error message when this happens, so it seems that MaxScale is aware of the failure:

{noformat}
2016-06-23 15:06:31   [2]  info   : Execute statement (truncated, it contains password) from the slave 'change master to master_host='172.31.32.117',master_port=3306,master_user='repl',master_...'
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_USER is [repl]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_HOST is [172.31.32.117]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_PORT is [3306]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_FILE is [mysqld-bin.000002]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_POS is [4]
2016-06-23 15:06:31   notice : Replication: 'CHANGE MASTER TO executed'. Previous state MASTER_HOST='_none_', MASTER_PORT=3306, MASTER_LOG_FILE='', MASTER_LOG_POS=0, MASTER_USER='maxscale'. New state is MASTER_HOST='172.31.32.117', MASTER_PORT=3306, MASTER_LOG_FILE='mysqld-bin.000002', MASTER_LOG_POS=4, MASTER_USER='repl'
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_write] Append to writequeue. 4 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
...snip...
2016-06-23 15:06:49   [2]  info   : Execute statement from the slave 'start slave'
2016-06-23 15:06:49   [2]  info   : Started session [0] for Replication service
2016-06-23 15:06:49   notice : Loaded module MySQLBackend: V2.0.0 from /usr/lib64/maxscale/libMySQLBackend.so
2016-06-23 15:06:49   debug  : 140013001512704 [gw_do_connect_to_backend] Connected to backend server 172.31.32.117:3306, fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_create_backend_connection] Connection pending to 172.31.32.117:3306, protocol fd 13 client fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_connect] Connected to server 172.31.32.117:3306, from backend dcb 0x7f5744000ee0, client dcp 0x7f5744000b00 fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_add_dcb] Added dcb 0x7f5744000ee0 in state DCB_STATE_POLLING to poll set.
2016-06-23 15:06:49   notice : Replication: attempting to connect to master server 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4
2016-06-23 15:06:49   debug  : 140013001512704 [gw_MySQLWrite_backend] delayed write to dcb 0x7f5744000ee0 fd 13 protocol state MYSQL_PENDING_CONNECT.
2016-06-23 15:06:49   notice : Replication: START SLAVE executed by maxscale@127.0.0.1. Trying connection to master 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4, transaction safe pos 4
2016-06-23 15:06:49   debug  : 140013442922464 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013012002560 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : Loading data from backend database [172.31.32.117:3306] for service [Replication]
2016-06-23 15:06:49   debug  : Replication: Adding database db1 to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database information_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database mysql to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database performance_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database test to the resouce hash.
2016-06-23 15:06:49   debug  : Loaded 5 MySQL Database Names for service [Replication]
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@localhost for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@% for database no db added to service user table.
2016-06-23 15:06:49   warning: Duplicate MySQL user found for service [Replication]: maxscale@% for database: (null)
2016-06-23 15:06:49   [2]  info   : Replication: User repl@% for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User sst@% for database no db added to service user table.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 121 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 5 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 2, MYSQL_CONNECTED.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 99 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 1 writes buffered for dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 84 Bytes to dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Read zero bytes from backend dcb 0x7f5744000ee0 fd 13 in state DCB_STATE_POLLING. n 0, head (nil), len 0
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] gw_receive_backend_auth read successfully nothing. dcb 0x7f5744000ee0 fd 13, user repl.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 8197 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 4, MYSQL_AUTH_RECV.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 110 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Invalid authentication message from backend dcb 0x7f5744000ee0 fd 13, ptr[4] = 255, error 28000, msg Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES).
2016-06-23 15:06:49   error  : Invalid authentication message from backend. Error : 28000, Msg : Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES)
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] after gw_receive_backend_authentication fd 13, state = MYSQL_AUTH_FAILED.
2016-06-23 15:06:49   error  : Backend server didn't accept authentication for user repl.
{noformat}",,0,0,0,0,0.0,"binlog router does not give proper error when authentication with master fails $end$ Let's say that I have a binlog router instance with the following configuration:

{noformat}
[maxscale]
threads=4
log_debug=1
log_trace=1

[Replication]
type=service
router=binlogrouter
version_string=10.0.25-log
user=maxscale
passwd=password
router_options=server_id=2,mariadb10-compatibility=1,binlogdir=/var/log/binlogs

[Replication Listener]
type=listener
service=Replication
protocol=MySQLClient
port=3306
{noformat}

And then I create a replication user on the master:

{noformat}
GRANT ALL PRIVILEGES ON *.* TO 'repl'@'%' IDENTIFIED BY 'password';
{noformat}

Now let's say that I execute CHANGE MASTER TO on the binlog router instance, but I supply the wrong password for master_password:

{noformat}
CHANGE MASTER TO master_host='172.31.32.117',master_port=3306,master_user='repl',master_password='password1', MASTER_LOG_FILE='mysqld-bin.000002',MASTER_LOG_POS=4;
START SLAVE;
{noformat}

This will fail to connect due to the incorrect password, but MaxScale seems to ignore this in SHOW SLAVE STATUS. Instead, it suggests that Slave_IO_State is ""Timestamp retrieval"" and Last_Errno is 0.

{noformat}
MySQL [(none)]> SHOW SLAVE STATUS\G
*************************** 1. row ***************************
               Slave_IO_State: Timestamp retrieval
                  Master_Host: 172.31.32.117
                  Master_User: repl
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysqld-bin.000002
          Read_Master_Log_Pos: 4
               Relay_Log_File: mysqld-bin.000002
                Relay_Log_Pos: 4
        Relay_Master_Log_File: mysqld-bin.000002
             Slave_IO_Running: Connecting
            Slave_SQL_Running: Yes
              Replicate_Do_DB:
          Replicate_Ignore_DB:
           Replicate_Do_Table:
       Replicate_Ignore_Table:
      Replicate_Wild_Do_Table:
  Replicate_Wild_Ignore_Table:
                   Last_Errno: 0
                   Last_Error:
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 4
              Relay_Log_Space: 4
              Until_Condition: None
               Until_Log_File:
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File:
           Master_SSL_CA_Path:
              Master_SSL_Cert:
            Master_SSL_Cipher:
               Master_SSL_Key:
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error:
               Last_SQL_Errno: 0
               Last_SQL_Error:
  Replicate_Ignore_Server_Ids:
             Master_Server_Id: 0
                  Master_UUID: 57e87f43-3975-11e6-9925-06916daa38f7
             Master_Info_File:
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave running
           Master_Retry_Count: 1000
                  Master_Bind:
      Last_IO_Error_TimeStamp:
     Last_SQL_Error_Timestamp:
               Master_SSL_Crl:
           Master_SSL_Crlpath:
           Retrieved_Gtid_Set:
            Executed_Gtid_Set:
                Auto_Position:
1 row in set (0.00 sec)
{noformat}

The log does seem to contain an error message when this happens, so it seems that MaxScale is aware of the failure:

{noformat}
2016-06-23 15:06:31   [2]  info   : Execute statement (truncated, it contains password) from the slave 'change master to master_host='172.31.32.117',master_port=3306,master_user='repl',master_...'
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_USER is [repl]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_HOST is [172.31.32.117]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_PORT is [3306]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_FILE is [mysqld-bin.000002]
2016-06-23 15:06:31   [2]  info   : Replication: New MASTER_LOG_POS is [4]
2016-06-23 15:06:31   notice : Replication: 'CHANGE MASTER TO executed'. Previous state MASTER_HOST='_none_', MASTER_PORT=3306, MASTER_LOG_FILE='', MASTER_LOG_POS=0, MASTER_USER='maxscale'. New state is MASTER_HOST='172.31.32.117', MASTER_PORT=3306, MASTER_LOG_FILE='mysqld-bin.000002', MASTER_LOG_POS=4, MASTER_USER='repl'
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_write] Append to writequeue. 4 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:31   debug  : 140013012002560 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
...snip...
2016-06-23 15:06:49   [2]  info   : Execute statement from the slave 'start slave'
2016-06-23 15:06:49   [2]  info   : Started session [0] for Replication service
2016-06-23 15:06:49   notice : Loaded module MySQLBackend: V2.0.0 from /usr/lib64/maxscale/libMySQLBackend.so
2016-06-23 15:06:49   debug  : 140013001512704 [gw_do_connect_to_backend] Connected to backend server 172.31.32.117:3306, fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_create_backend_connection] Connection pending to 172.31.32.117:3306, protocol fd 13 client fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_connect] Connected to server 172.31.32.117:3306, from backend dcb 0x7f5744000ee0, client dcp 0x7f5744000b00 fd -1.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_add_dcb] Added dcb 0x7f5744000ee0 in state DCB_STATE_POLLING to poll set.
2016-06-23 15:06:49   notice : Replication: attempting to connect to master server 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4
2016-06-23 15:06:49   debug  : 140013001512704 [gw_MySQLWrite_backend] delayed write to dcb 0x7f5744000ee0 fd 13 protocol state MYSQL_PENDING_CONNECT.
2016-06-23 15:06:49   notice : Replication: START SLAVE executed by maxscale@127.0.0.1. Trying connection to master 172.31.32.117:3306, binlog mysqld-bin.000002, pos 4, transaction safe pos 4
2016-06-23 15:06:49   debug  : 140013442922464 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013012002560 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : Loading data from backend database [172.31.32.117:3306] for service [Replication]
2016-06-23 15:06:49   debug  : Replication: Adding database db1 to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database information_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database mysql to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database performance_schema to the resouce hash.
2016-06-23 15:06:49   debug  : Replication: Adding database test to the resouce hash.
2016-06-23 15:06:49   debug  : Loaded 5 MySQL Database Names for service [Replication]
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@localhost for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User appuser@% for database no db added to service user table.
2016-06-23 15:06:49   warning: Duplicate MySQL user found for service [Replication]: maxscale@% for database: (null)
2016-06-23 15:06:49   [2]  info   : Replication: User repl@% for database no db added to service user table.
2016-06-23 15:06:49   [2]  info   : Replication: User sst@% for database no db added to service user table.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 121 writes buffered for dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 11 Bytes to dcb 0x7f574c000a40 in state DCB_STATE_POLLING fd 4
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 5 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 2, MYSQL_CONNECTED.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 99 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_write] Append to writequeue. 1 writes buffered for dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_drain_writeq] Wrote 84 Bytes to dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Read zero bytes from backend dcb 0x7f5744000ee0 fd 13 in state DCB_STATE_POLLING. n 0, head (nil), len 0
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] gw_receive_backend_auth read successfully nothing. dcb 0x7f5744000ee0 fd 13, user repl.
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] epoll_wait found 1 fds
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] event 8197 dcb 0x7f5744000ee0 role DCB_ROLE_REQUEST_HANDLER
2016-06-23 15:06:49   debug  : 140013001512704 [gw_write_backend_event] wrote to dcb 0x7f5744000ee0 fd 13, return 1
2016-06-23 15:06:49   debug  : 140013001512704 [poll_waitevents] Read in dcb 0x7f5744000ee0 fd 13
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] Read dcb 0x7f5744000ee0 fd 13 protocol state 4, MYSQL_AUTH_RECV.
2016-06-23 15:06:49   debug  : 140013001512704 [dcb_read] Read 110 bytes from dcb 0x7f5744000ee0 in state DCB_STATE_POLLING fd 13.
2016-06-23 15:06:49   debug  : 140013001512704 [gw_receive_backend_auth] Invalid authentication message from backend dcb 0x7f5744000ee0 fd 13, ptr[4] = 255, error 28000, msg Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES).
2016-06-23 15:06:49   error  : Invalid authentication message from backend. Error : 28000, Msg : Access denied for user 'repl'@'ip-172-31-47-243.us-west-2.compute.internal' (using password: YES)
2016-06-23 15:06:49   debug  : 140013001512704 [gw_read_backend_event] after gw_receive_backend_authentication fd 13, state = MYSQL_AUTH_FAILED.
2016-06-23 15:06:49   error  : Backend server didn't accept authentication for user repl.
{noformat} $acceptance criteria:$",0,0,0,0,0,0,0,278.55,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1921,MXS-788,Task,MXS,2016-07-05 09:58:45,,0,Enable SSL for master registration in binlog server.,,,Enable SSL for master registration in binlog server. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,4,0,1,0,0,0,,0,850,4,0,0,2016-07-05 09:58:45,Enable SSL for master registration in binlog server.,,,0,0,0,0,0.0,Enable SSL for master registration in binlog server. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,52,12,0.230769,3,0.0576923,2,0.0384615,0,0.0,0,0.0
1922,MXS-793,New Feature,MXS,2016-07-07 06:40:14,,0,Secured Binlog Server,,,Secured Binlog Server $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,24,,0,9,0,8,0,1,4,,0,850,9,0,0,2016-07-19 09:56:33,Encrypt binlog files,,,1,0,0,6,0.5,Encrypt binlog files $end$ $acceptance criteria:$,1,1,1,0,0,0,1,291.267,53,12,0.226415,3,0.0566038,2,0.0377358,0,0.0,0,0.0
1923,MXS-796,Sub-Task,MXS,2016-07-07 06:43:37,,0,Aurora Replication Monitoring,,,Aurora Replication Monitoring $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,7,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-08-17 09:18:17,Aurora Replication Monitoring,,,0,0,0,0,0.0,Aurora Replication Monitoring $end$ $acceptance criteria:$,0,0,0,0,0,0,1,986.567,54,13,0.240741,4,0.0740741,2,0.037037,0,0.0,0,0.0
1924,MXS-797,New Feature,MXS,2016-07-07 06:55:30,,0,Caching Filter,,,Caching Filter $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,27,,0,0,0,10,0,0,8,,0,850,0,0,0,2016-08-17 08:51:36,Caching Filter,,,0,0,0,0,0.0,Caching Filter $end$ $acceptance criteria:$,0,0,0,0,0,0,1,985.933,55,13,0.236364,4,0.0727273,2,0.0363636,0,0.0,0,0.0
1925,MXS-802,Sub-Task,MXS,2016-07-11 16:54:07,,0,Log current working directory in daemon mode,Log the current working directory to the log files on startup.,,Log current working directory in daemon mode $end$ Log the current working directory to the log files on startup. $acceptance criteria:$,,markus makela,markus makela,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-07-11 16:54:07,Log current working directory in daemon mode,Log the current working directory to the log files on startup.,,0,0,0,0,0.0,Log current working directory in daemon mode $end$ Log the current working directory to the log files on startup. $acceptance criteria:$,0,0,0,0,0,0,0,0.0,10,3,0.3,3,0.3,3,0.3,3,0.3,3,0.3
1926,MXS-807,New Feature,MXS,2016-07-19 09:58:15,,0,Refactor Read-Write Split router,Improve read-write split router code and eliminate dependencies on protocols.,,Refactor Read-Write Split router $end$ Improve read-write split router code and eliminate dependencies on protocols. $acceptance criteria:$,,martin brampton,martin brampton,Major,12,,0,1,0,7,0,0,0,,0,850,1,0,0,2016-07-19 10:00:21,Refactor Read-Write Split router,Improve read-write split router code and eliminate dependencies on protocols.,,0,0,0,0,0.0,Refactor Read-Write Split router $end$ Improve read-write split router code and eliminate dependencies on protocols. $acceptance criteria:$,0,0,0,0,0,0,1,0.0333333,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1927,MXS-815,Sub-Task,MXS,2016-08-02 07:45:25,,0,2.0.0: Repository preparations,,,2.0.0: Repository preparations $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-08-02 07:45:25,2.0.0: Repository preparations,,,0,0,0,0,0.0,2.0.0: Repository preparations $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,56,13,0.232143,4,0.0714286,2,0.0357143,0,0.0,0,0.0
1928,MXS-817,New Feature,MXS,2016-08-02 10:07:15,,0,Move loading of user data into authenticator modules,The loading of user data i.e. database users is hard coded into service.c. This should be moved into the authenticator modules so that different authentication mechanisms should have their own way of loading data.,,Move loading of user data into authenticator modules $end$ The loading of user data i.e. database users is hard coded into service.c. This should be moved into the authenticator modules so that different authentication mechanisms should have their own way of loading data. $acceptance criteria:$,,markus makela,markus makela,Major,13,,0,2,0,2,0,0,0,,0,850,2,0,0,2016-08-02 10:07:15,Move loading of user data into authenticator modules,The loading of user data i.e. database users is hard coded into service.c. This should be moved into the authenticator modules so that different authentication mechanisms should have their own way of loading data.,,0,0,0,0,0.0,Move loading of user data into authenticator modules $end$ The loading of user data i.e. database users is hard coded into service.c. This should be moved into the authenticator modules so that different authentication mechanisms should have their own way of loading data. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,11,3,0.272727,3,0.272727,3,0.272727,3,0.272727,3,0.272727
1929,MXS-818,Task,MXS,2016-08-02 10:14:50,,0,Design and Document Restful API,,,Design and Document Restful API $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,1,1,,0,850,0,0,0,2016-08-02 10:14:50,Design and Implement Restful API,,,1,0,0,2,0.125,Design and Implement Restful API $end$ $acceptance criteria:$,1,1,0,0,0,0,1,0.0,57,13,0.22807,4,0.0701754,2,0.0350877,0,0.0,0,0.0
1930,MXS-819,Sub-Task,MXS,2016-08-02 10:15:15,,0,Create API documentation,,,Create API documentation $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-08-02 10:15:15,Create API documentation,,,0,0,0,0,0.0,Create API documentation $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,58,14,0.241379,4,0.0689655,2,0.0344828,0,0.0,0,0.0
1931,MXS-838,New Feature,MXS,2016-08-25 12:53:53,,0,Drop sessions after server is put into maintenance mode,"Hi,

I believe it is logical and wanted that all sessions to a certain backend server are dropped if that backend server is put into maintenance mode. I have tested if this is the case in ReadConnRoute and it is not, I did not check the more advanced routers.

I think MXS-740 needs to be implemented before this, otherwise there will be (unneeded) downtime when a server is put into maintenance mode.

What do you think?

Cheers,
Michaël",,"Drop sessions after server is put into maintenance mode $end$ Hi,

I believe it is logical and wanted that all sessions to a certain backend server are dropped if that backend server is put into maintenance mode. I have tested if this is the case in ReadConnRoute and it is not, I did not check the more advanced routers.

I think MXS-740 needs to be implemented before this, otherwise there will be (unneeded) downtime when a server is put into maintenance mode.

What do you think?

Cheers,
Michaël $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,5,,0,1,1,1,0,0,0,,0,850,0,0,0,2016-12-14 10:34:23,Drop sessions after server is put into maintenance mode,"Hi,

I believe it is logical and wanted that all sessions to a certain backend server are dropped if that backend server is put into maintenance mode. I have tested if this is the case in ReadConnRoute and it is not, I did not check the more advanced routers.

I think MXS-740 needs to be implemented before this, otherwise there will be (unneeded) downtime when a server is put into maintenance mode.

What do you think?

Cheers,
Michaël",,0,0,0,0,0.0,"Drop sessions after server is put into maintenance mode $end$ Hi,

I believe it is logical and wanted that all sessions to a certain backend server are dropped if that backend server is put into maintenance mode. I have tested if this is the case in ReadConnRoute and it is not, I did not check the more advanced routers.

I think MXS-740 needs to be implemented before this, otherwise there will be (unneeded) downtime when a server is put into maintenance mode.

What do you think?

Cheers,
Michaël $acceptance criteria:$",0,0,0,0,0,0,0,2661.67,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1932,MXS-843,New Feature,MXS,2016-08-26 11:58:31,,0,Log output of executed event command,"Hi,

It would be a great improvement if the log could output the exact command that was executed in case of an event.
This way a DBA can much more easily debug the result of the script.

Thanks,
Michaël",,"Log output of executed event command $end$ Hi,

It would be a great improvement if the log could output the exact command that was executed in case of an event.
This way a DBA can much more easily debug the result of the script.

Thanks,
Michaël $acceptance criteria:$",,Michaël de groot,Michaël de groot,Major,8,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-11-16 10:49:34,Log output of executed event command,"Hi,

It would be a great improvement if the log could output the exact command that was executed in case of an event.
This way a DBA can much more easily debug the result of the script.

Thanks,
Michaël",,0,0,0,0,0.0,"Log output of executed event command $end$ Hi,

It would be a great improvement if the log could output the exact command that was executed in case of an event.
This way a DBA can much more easily debug the result of the script.

Thanks,
Michaël $acceptance criteria:$",0,0,0,0,0,0,0,1966.85,4,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1933,MXS-848,New Feature,MXS,2016-08-28 21:04:10,,0,Provide more info in qla log,"QLA Filter log write now shows the query time and, the client user and query text for each query passing through MaxScale.

Include the following additional information for each query in the log as well
- backend server name and port where the query was sent 
- The service that was used for the query

Also for each client connection there is separate log file - instead there should be one log file per service",,"Provide more info in qla log $end$ QLA Filter log write now shows the query time and, the client user and query text for each query passing through MaxScale.

Include the following additional information for each query in the log as well
- backend server name and port where the query was sent 
- The service that was used for the query

Also for each client connection there is separate log file - instead there should be one log file per service $acceptance criteria:$",,Dipti Joshi,Dipti Joshi,Major,9,,0,1,1,1,0,1,0,,0,850,1,1,0,2016-11-30 09:13:21,Provide more info in qla log,"QLA Filter log write now shows the query time and, the client user and query text for each query passing through MaxScale.

Include the following additional information for each query in the log as well
- backend server name and port where the query was sent 
- The service that was used for the query

Also for each client connection there is separate log file - instead there should be one log file per service",,0,0,0,0,0.0,"Provide more info in qla log $end$ QLA Filter log write now shows the query time and, the client user and query text for each query passing through MaxScale.

Include the following additional information for each query in the log as well
- backend server name and port where the query was sent 
- The service that was used for the query

Also for each client connection there is separate log file - instead there should be one log file per service $acceptance criteria:$",0,0,0,0,0,0,0,2244.15,22,4,0.181818,1,0.0454545,0,0.0,0,0.0,0,0.0
1934,MXS-849,Sub-Task,MXS,2016-08-30 08:12:38,,0,Create Initial Top-level Caching Plugin,"The caching mechanism will be in two levels; a generic one handling the rules and a specific one handling the actual storage.

The generic one will be common to all storage implementations and the specific one will be created for each storage component.",,"Create Initial Top-level Caching Plugin $end$ The caching mechanism will be in two levels; a generic one handling the rules and a specific one handling the actual storage.

The generic one will be common to all storage implementations and the specific one will be created for each storage component. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,2,,0,0,0,10,0,0,0,,0,850,0,0,0,2016-08-30 08:12:38,Create Initial Top-level Caching Plugin,"The caching mechanism will be in two levels; a generic one handling the rules and a specific one handling the actual storage.

The generic one will be common to all storage implementations and the specific one will be created for each storage component.",,0,0,0,0,0.0,"Create Initial Top-level Caching Plugin $end$ The caching mechanism will be in two levels; a generic one handling the rules and a specific one handling the actual storage.

The generic one will be common to all storage implementations and the specific one will be created for each storage component. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,59,14,0.237288,4,0.0677966,2,0.0338983,0,0.0,0,0.0
1935,MXS-850,Sub-Task,MXS,2016-08-30 08:13:37,,0,Create initial storage implementation on top of RocksDB.,,,Create initial storage implementation on top of RocksDB. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,10,0,0,0,,0,850,0,0,0,2016-08-30 08:13:37,Create initial storage implementation on top of RocksDB.,,,0,0,0,0,0.0,Create initial storage implementation on top of RocksDB. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,60,14,0.233333,4,0.0666667,2,0.0333333,0,0.0,0,0.0
1936,MXS-851,Sub-Task,MXS,2016-08-30 08:14:48,,0,Implement rule handling.,,,Implement rule handling. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,10,0,0,0,,0,850,0,0,0,2016-08-30 08:14:48,Implement rule handling.,,,0,0,0,0,0.0,Implement rule handling. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,61,14,0.229508,4,0.0655738,2,0.0327869,0,0.0,0,0.0
1937,MXS-852,New Feature,MXS,2016-08-30 19:29:00,,0,Maxscale prepared statement routing,"All of our prepared statements are being routed to master even if they are read only select. I see this is a known limitation of MaxScale. 

Can you please implement route to slave for read only prepared statements?",,"Maxscale prepared statement routing $end$ All of our prepared statements are being routed to master even if they are read only select. I see this is a known limitation of MaxScale. 

Can you please implement route to slave for read only prepared statements? $acceptance criteria:$",,Chris Calender,Chris Calender,Major,10,,0,1,0,2,0,0,0,,0,850,0,0,0,2017-06-14 09:17:29,Maxscale prepared statement routing,"All of our prepared statements are being routed to master even if they are read only select. I see this is a known limitation of MaxScale. 

Can you please implement route to slave for read only prepared statements?",,0,0,0,0,0.0,"Maxscale prepared statement routing $end$ All of our prepared statements are being routed to master even if they are read only select. I see this is a known limitation of MaxScale. 

Can you please implement route to slave for read only prepared statements? $acceptance criteria:$",0,0,0,0,0,0,1,6901.8,0,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1938,MXS-861,Sub-Task,MXS,2016-09-13 06:57:36,,0,Implement result handling.,"The cache needs to understand the results streaming through it so that it can properly store a result, in case it is subject to caching. And ignore it, for instance, if it is too large.
",,"Implement result handling. $end$ The cache needs to understand the results streaming through it so that it can properly store a result, in case it is subject to caching. And ignore it, for instance, if it is too large.
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,3,,0,0,0,10,0,0,0,,0,850,0,0,0,2016-09-13 06:57:36,Implement result handling.,"The cache needs to understand the results streaming through it so that it can properly store a result, in case it is subject to caching. And ignore it, for instance, if it is too large.
",,0,0,0,0,0.0,"Implement result handling. $end$ The cache needs to understand the results streaming through it so that it can properly store a result, in case it is subject to caching. And ignore it, for instance, if it is too large.
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,62,14,0.225806,4,0.0645161,2,0.0322581,0,0.0,0,0.0
1939,MXS-862,Task,MXS,2016-09-13 08:48:56,,0,GSSAPI Authentication Plugin,,,GSSAPI Authentication Plugin $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,8,,0,0,0,3,0,1,1,,0,850,0,0,0,2016-09-13 10:08:05,LDAP/Kerberos Authentication Plugin,,,1,0,0,2,0.166667,LDAP/Kerberos Authentication Plugin $end$ $acceptance criteria:$,1,1,0,0,0,0,1,1.31667,63,14,0.222222,4,0.0634921,2,0.031746,0,0.0,0,0.0
1940,MXS-865,Task,MXS,2016-09-13 10:09:48,,0,"Create tests for Gatekeeper, Auroramon and CCRfilter",,,"Create tests for Gatekeeper, Auroramon and CCRfilter $end$ $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,0,0,0,3,0,0,0,,0,850,0,0,0,2016-09-13 10:09:48,"Create tests for Gatekeeper, Auroramon and CCRfilter",,,0,0,0,0,0.0,"Create tests for Gatekeeper, Auroramon and CCRfilter $end$ $acceptance criteria:$",0,0,0,0,0,0,1,0.0,64,15,0.234375,4,0.0625,2,0.03125,0,0.0,0,0.0
1941,MXS-867,Sub-Task,MXS,2016-09-15 09:40:44,,0,File based Key Management for binlog encryption,,,File based Key Management for binlog encryption $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,4,,0,1,0,8,0,0,0,,0,850,1,0,0,2016-09-15 09:40:44,File based Key Management for binlog encryption,,,0,0,0,0,0.0,File based Key Management for binlog encryption $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,1,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1942,MXS-868,Sub-Task,MXS,2016-09-15 09:41:47,,0,Add encrcyption/decryption to read/write binlog record routines,,,Add encrcyption/decryption to read/write binlog record routines $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,4,,0,0,0,8,0,0,0,,0,850,0,0,0,2016-09-15 09:41:47,Add encrcyption/decryption to read/write binlog record routines,,,0,0,0,0,0.0,Add encrcyption/decryption to read/write binlog record routines $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,2,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1943,MXS-869,Sub-Task,MXS,2016-09-15 09:47:08,,0,Create START_ENCRYPTION_EVENT in binlog and skip it while sending data to slaves,,,Create START_ENCRYPTION_EVENT in binlog and skip it while sending data to slaves $end$ $acceptance criteria:$,,Massimiliano Pinto,Massimiliano Pinto,Major,4,,0,0,0,8,0,0,0,,0,850,0,0,0,2016-09-15 09:47:08,Create START_ENCRYPTION_EVENT in binlog and skip it while sending data to slaves,,,0,0,0,0,0.0,Create START_ENCRYPTION_EVENT in binlog and skip it while sending data to slaves $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,3,0,0.0,0,0.0,0,0.0,0,0.0,0,0.0
1944,MXS-879,Sub-Task,MXS,2016-10-05 06:33:43,,0,Cache: Implement user matching,"If a user rule is used in the cache configuration file, the user should be matched following the format specified here: https://dev.mysql.com/doc/refman/5.7/en/account-names.html
",,"Cache: Implement user matching $end$ If a user rule is used in the cache configuration file, the user should be matched following the format specified here: https://dev.mysql.com/doc/refman/5.7/en/account-names.html
 $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,6,,0,0,0,10,0,0,0,,0,850,0,0,0,2016-10-05 06:34:38,Cache: Implement user matching,"If a user rule is used in the cache configuration file, the user should be matched following the format specified here: https://dev.mysql.com/doc/refman/5.7/en/account-names.html
",,0,0,0,0,0.0,"Cache: Implement user matching $end$ If a user rule is used in the cache configuration file, the user should be matched following the format specified here: https://dev.mysql.com/doc/refman/5.7/en/account-names.html
 $acceptance criteria:$",0,0,0,0,0,0,1,0.0,65,15,0.230769,4,0.0615385,2,0.0307692,0,0.0,0,0.0
1945,MXS-884,Task,MXS,2016-10-05 08:07:44,MXS-881,0,QC: Provide more detailed field information when it is available.,"Currently the query classifier returns the affected fields as a simple string, always without the table name and without distinguising whether a field is used in the select part or the where part.

For instance, the affected fields of a query like
{code}
select x.a, y.b from x join y where x.c = 3
{code}
is reported as ""a b c"".

Extend the API so that no information is filtered.",,"QC: Provide more detailed field information when it is available. $end$ Currently the query classifier returns the affected fields as a simple string, always without the table name and without distinguising whether a field is used in the select part or the where part.

For instance, the affected fields of a query like
{code}
select x.a, y.b from x join y where x.c = 3
{code}
is reported as ""a b c"".

Extend the API so that no information is filtered. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,0,0,1,3,0,0,0,,0,850,0,0,0,2016-10-05 08:52:12,QC: Provide more detailed field information when it is available.,"Currently the query classifier returns the affected fields as a simple string, always without the table name and without distinguising whether a field is used in the select part or the where part.

For instance, the affected fields of a query like
{code}
select x.a, y.b from x join y where x.c = 3
{code}
is reported as ""a b c"".

Extend the API so that no information is filtered.",,0,0,0,0,0.0,"QC: Provide more detailed field information when it is available. $end$ Currently the query classifier returns the affected fields as a simple string, always without the table name and without distinguising whether a field is used in the select part or the where part.

For instance, the affected fields of a query like
{code}
select x.a, y.b from x join y where x.c = 3
{code}
is reported as ""a b c"".

Extend the API so that no information is filtered. $acceptance criteria:$",0,0,0,0,0,0,1,0.733333,66,15,0.227273,4,0.0606061,2,0.030303,0,0.0,0,0.0
1946,MXS-890,Task,MXS,2016-10-12 04:41:28,,0,Configure minimal test set to be executed on evey push,essential tests have to be executed on every push to defined set of branches. Tests should be executed on constantly running VMs ,,Configure minimal test set to be executed on evey push $end$ essential tests have to be executed on every push to defined set of branches. Tests should be executed on constantly running VMs  $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-10-18 09:20:25,Configure minimal test set to be executed on evey push,essential tests have to be executed on every push to defined set of branches. Tests should be executed on constantly running VMs ,,0,0,0,0,0.0,Configure minimal test set to be executed on evey push $end$ essential tests have to be executed on every push to defined set of branches. Tests should be executed on constantly running VMs  $acceptance criteria:$,0,0,0,0,0,0,0,148.633,17,1,0.0588235,0,0.0,0,0.0,0,0.0,0,0.0
1947,MXS-891,Task,MXS,2016-10-12 04:43:30,,0,build for CentOS 7 triggered b push should be executed on constantly running VM,build takes long time due to ned to install all build-deps and cmake compilation. it is not acceptable for smoke tests,,build for CentOS 7 triggered b push should be executed on constantly running VM $end$ build takes long time due to ned to install all build-deps and cmake compilation. it is not acceptable for smoke tests $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,0,0,3,0,0,0,,0,850,0,0,0,2016-11-02 10:53:05,build for CentOS 7 triggered b push should be executed on constantly running VM,build takes long time due to ned to install all build-deps and cmake compilation. it is not acceptable for smoke tests,,0,0,0,0,0.0,build for CentOS 7 triggered b push should be executed on constantly running VM $end$ build takes long time due to ned to install all build-deps and cmake compilation. it is not acceptable for smoke tests $acceptance criteria:$,0,0,0,0,0,0,1,510.15,18,1,0.0555556,0,0.0,0,0.0,0,0.0,0,0.0
1948,MXS-892,Task,MXS,2016-10-12 04:47:46,,0,add backend check at the end of the failed test,"in case of test falure due to backend failure:
1. it should be clearly stated
2. attempt to re-run test should be done",,"add backend check at the end of the failed test $end$ in case of test falure due to backend failure:
1. it should be clearly stated
2. attempt to re-run test should be done $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,9,,0,3,0,5,0,0,0,,0,850,3,0,0,2017-03-01 11:31:23,add backend check at the end of the failed test,"in case of test falure due to backend failure:
1. it should be clearly stated
2. attempt to re-run test should be done",,0,0,0,0,0.0,"add backend check at the end of the failed test $end$ in case of test falure due to backend failure:
1. it should be clearly stated
2. attempt to re-run test should be done $acceptance criteria:$",0,0,0,0,0,0,1,3366.72,19,1,0.0526316,0,0.0,0,0.0,0,0.0,0,0.0
1949,MXS-893,Task,MXS,2016-10-12 04:49:35,,0,use 'snapshot' to restore backend between tests,"add option to use snapshot vagrant feature to restore backend after tests

investigate feasibility to restore VMs by 'snapshot restore' after every tests to be sure backend is good in any case",,"use 'snapshot' to restore backend between tests $end$ add option to use snapshot vagrant feature to restore backend after tests

investigate feasibility to restore VMs by 'snapshot restore' after every tests to be sure backend is good in any case $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,7,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-11-16 10:52:50,use 'snapshot' to restore backend between tests,"add option to use snapshot vagrant feature to restore backend after tests

investigate feasibility to restore VMs by 'snapshot restore' after every tests to be sure backend is good in any case",,0,0,0,0,0.0,"use 'snapshot' to restore backend between tests $end$ add option to use snapshot vagrant feature to restore backend after tests

investigate feasibility to restore VMs by 'snapshot restore' after every tests to be sure backend is good in any case $acceptance criteria:$",0,0,0,0,0,0,1,846.05,20,1,0.05,0,0.0,0,0.0,0,0.0,0,0.0
1950,MXS-895,Task,MXS,2016-10-12 04:54:47,,0,setup regular automatic performance test run ,"using solution delivered by FRUCT:
- setup daily perf test run
- create example SQL to show result
",,"setup regular automatic performance test run  $end$ using solution delivered by FRUCT:
- setup daily perf test run
- create example SQL to show result
 $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,12,,0,0,0,7,0,0,0,,0,850,0,0,0,2016-10-18 09:20:17,setup regular automatic performance test run ,"using solution delivered by FRUCT:
- setup daily perf test run
- create example SQL to show result
",,0,0,0,0,0.0,"setup regular automatic performance test run  $end$ using solution delivered by FRUCT:
- setup daily perf test run
- create example SQL to show result
 $acceptance criteria:$",0,0,0,0,0,0,1,148.417,21,1,0.047619,0,0.0,0,0.0,0,0.0,0,0.0
1951,MXS-902,Task,MXS,2016-10-12 05:11:32,,0,LDAP/Kerberos test environment,prepare environment for testing authentication features,,LDAP/Kerberos test environment $end$ prepare environment for testing authentication features $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,8,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-11-30 10:21:14,LDAP/Kerberos test environment,prepare environment for testing authentication features,,0,0,0,0,0.0,LDAP/Kerberos test environment $end$ prepare environment for testing authentication features $acceptance criteria:$,0,0,0,0,0,0,1,1181.15,22,1,0.0454545,0,0.0,0,0.0,0,0.0,0,0.0
1952,MXS-904,Task,MXS,2016-10-12 05:14:50,,0,Easy setup for local testing ,"it should be possible to create test VMs on developers machine in easy way (""one button"")",,"Easy setup for local testing  $end$ it should be possible to create test VMs on developers machine in easy way (""one button"") $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,26,,0,3,0,17,0,0,0,,0,850,3,0,0,2017-02-15 10:54:40,Easy setup for local testing ,"it should be possible to create test VMs on developers machine in easy way (""one button"")",,0,0,0,0,0.0,"Easy setup for local testing  $end$ it should be possible to create test VMs on developers machine in easy way (""one button"") $acceptance criteria:$",0,0,0,0,0,0,1,3029.65,23,1,0.0434783,0,0.0,0,0.0,0,0.0,0,0.0
1953,MXS-907,Task,MXS,2016-10-12 05:25:03,,0,cleanup of Jenkins job parameters,"a number of parameters (e.g ci_release, restart Maxscale, etc) are not in use any more and have to be removed",,"cleanup of Jenkins job parameters $end$ a number of parameters (e.g ci_release, restart Maxscale, etc) are not in use any more and have to be removed $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,1,0,0,0,,0,850,1,0,0,2016-10-18 09:20:09,cleanup of Jenkins job parameters,"a number of parameters (e.g ci_release, restart Maxscale, etc) are not in use any more and have to be removed",,0,0,0,0,0.0,"cleanup of Jenkins job parameters $end$ a number of parameters (e.g ci_release, restart Maxscale, etc) are not in use any more and have to be removed $acceptance criteria:$",0,0,0,0,0,0,0,147.917,24,1,0.0416667,0,0.0,0,0.0,0,0.0,0,0.0
1954,MXS-910,New Feature,MXS,2016-10-12 05:35:37,,0,Selective Masking,"Data Security Standards such as PCI(Payment Card Industry) and  HIPAA(Health Insurance Portability and Accountability Act) require that sensitive/personal information should be kept private
Example Personal Identity information  to be kept private
* Social Security Number
* Credit Card number
* Driver License number
* Name
* Birthdate

What is Selective Data Masking ?: Data masking or data obfuscation is the process of hiding original data with random characters or data. Selective masking is to mask the result based on query result column.",,"Selective Masking $end$ Data Security Standards such as PCI(Payment Card Industry) and  HIPAA(Health Insurance Portability and Accountability Act) require that sensitive/personal information should be kept private
Example Personal Identity information  to be kept private
* Social Security Number
* Credit Card number
* Driver License number
* Name
* Birthdate

What is Selective Data Masking ?: Data masking or data obfuscation is the process of hiding original data with random characters or data. Selective masking is to mask the result based on query result column. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,13,,0,0,1,5,0,0,0,,0,850,0,0,0,2016-11-02 10:42:31,Selective Masking,"Data Security Standards such as PCI(Payment Card Industry) and  HIPAA(Health Insurance Portability and Accountability Act) require that sensitive/personal information should be kept private
Example Personal Identity information  to be kept private
* Social Security Number
* Credit Card number
* Driver License number
* Name
* Birthdate

What is Selective Data Masking ?: Data masking or data obfuscation is the process of hiding original data with random characters or data. Selective masking is to mask the result based on query result column.",,0,0,0,0,0.0,"Selective Masking $end$ Data Security Standards such as PCI(Payment Card Industry) and  HIPAA(Health Insurance Portability and Accountability Act) require that sensitive/personal information should be kept private
Example Personal Identity information  to be kept private
* Social Security Number
* Credit Card number
* Driver License number
* Name
* Birthdate

What is Selective Data Masking ?: Data masking or data obfuscation is the process of hiding original data with random characters or data. Selective masking is to mask the result based on query result column. $acceptance criteria:$",0,0,0,0,0,0,1,509.1,67,15,0.223881,4,0.0597015,2,0.0298507,0,0.0,0,0.0
1955,MXS-912,New Feature,MXS,2016-10-13 09:10:10,,0,maxscale-dev package,,,maxscale-dev package $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,11,,0,0,0,3,0,0,2,,0,850,0,0,0,2017-01-18 10:14:35,maxscale-dev package,,,0,0,0,0,0.0,maxscale-dev package $end$ $acceptance criteria:$,0,0,0,0,0,0,1,2329.07,68,15,0.220588,4,0.0588235,2,0.0294118,0,0.0,0,0.0
1956,MXS-913,Sub-Task,MXS,2016-10-13 09:13:33,,0,Move headers to .../include/maxscale,Move all headers from server/include to include/maxscale.,,Move headers to .../include/maxscale $end$ Move all headers from server/include to include/maxscale. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2016-10-13 09:13:33,Move headers to .../include/maxscale,Move all headers from server/include to include/maxscale.,,0,0,0,0,0.0,Move headers to .../include/maxscale $end$ Move all headers from server/include to include/maxscale. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,69,15,0.217391,4,0.057971,2,0.0289855,0,0.0,0,0.0
1957,MXS-914,Sub-Task,MXS,2016-10-13 15:09:34,,0,Move server/modules/include headers to appropriate places.,,,Move server/modules/include headers to appropriate places. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,3,0,0,0,,0,850,0,0,0,2016-10-13 15:09:34,Move server/modules/include headers to appropriate places.,,,0,0,0,0,0.0,Move server/modules/include headers to appropriate places. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,70,15,0.214286,4,0.0571429,2,0.0285714,0,0.0,0,0.0
1958,MXS-922,New Feature,MXS,2016-10-18 06:45:53,,0,Easy Installation/Configuration of MaxScale,,,Easy Installation/Configuration of MaxScale $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,11,,0,1,0,2,0,0,6,,0,850,1,0,0,2016-11-02 10:44:55,Easy Installation/Configuration of MaxScale,,,0,0,0,0,0.0,Easy Installation/Configuration of MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,1,363.983,71,15,0.211268,4,0.056338,2,0.028169,0,0.0,0,0.0
1959,MXS-923,Sub-Task,MXS,2016-10-18 06:47:11,,0,Creating/Deleting a Server,"Make it possible to dynamically create/delete a server via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,"Creating/Deleting a Server $end$ Make it possible to dynamically create/delete a server via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-10-18 06:47:11,Creating/Deleting a Server,"Make it possible to dynamically create/delete a server via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,0,0,0,0,0.0,"Creating/Deleting a Server $end$ Make it possible to dynamically create/delete a server via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,72,15,0.208333,4,0.0555556,2,0.0277778,0,0.0,0,0.0
1960,MXS-924,Sub-Task,MXS,2016-10-18 06:48:21,,0,Adding/Removing a Server to/from a Monitor,"Make it possible to dynamically add/remove a server to/from a monitor via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,"Adding/Removing a Server to/from a Monitor $end$ Make it possible to dynamically add/remove a server to/from a monitor via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-10-18 06:48:21,Adding/Removing a Server to/from a Monitor,"Make it possible to dynamically add/remove a server to/from a monitor via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted.",,0,0,0,0,0.0,"Adding/Removing a Server to/from a Monitor $end$ Make it possible to dynamically add/remove a server to/from a monitor via MaxAdmin.

The changed state should be persisted so that the state is the same also when MaxScale is restarted. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,73,15,0.205479,4,0.0547945,2,0.0273973,0,0.0,0,0.0
1961,MXS-925,Sub-Task,MXS,2016-10-18 06:50:31,,0,Adding/Removing a Server to/from a Service.,"Make it possible to dynamically add/remove a server to/from a service via MaxAdmin.

The change should be persisted so that the changed state is present also when MaxScale is restarted.",,"Adding/Removing a Server to/from a Service. $end$ Make it possible to dynamically add/remove a server to/from a service via MaxAdmin.

The change should be persisted so that the changed state is present also when MaxScale is restarted. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,4,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-10-18 06:50:31,Adding/Removing a Server to/from a Service.,"Make it possible to dynamically add/remove a server to/from a service via MaxAdmin.

The change should be persisted so that the changed state is present also when MaxScale is restarted.",,0,0,0,0,0.0,"Adding/Removing a Server to/from a Service. $end$ Make it possible to dynamically add/remove a server to/from a service via MaxAdmin.

The change should be persisted so that the changed state is present also when MaxScale is restarted. $acceptance criteria:$",0,0,0,0,0,0,1,0.0,74,15,0.202703,4,0.0540541,2,0.027027,0,0.0,0,0.0
1962,MXS-927,New Feature,MXS,2016-10-18 07:20:50,,0,Amazon Aurora Monitor,,,Amazon Aurora Monitor $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,10,,0,0,0,2,0,0,2,,0,850,0,0,0,2016-11-30 10:21:17,Amazon Aurora Monitor,,,0,0,0,0,0.0,Amazon Aurora Monitor $end$ $acceptance criteria:$,0,0,0,0,0,0,1,1035.0,75,15,0.2,4,0.0533333,2,0.0266667,0,0.0,0,0.0
1963,MXS-928,Sub-Task,MXS,2016-10-18 07:21:37,,0,Tests for Aurora Monitor,Documentation is here: https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Monitors/Aurora-Monitor.md,,Tests for Aurora Monitor $end$ Documentation is here: https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Monitors/Aurora-Monitor.md $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 07:21:37,Tests for Aurora Monitor,Documentation is here: https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Monitors/Aurora-Monitor.md,,0,0,0,0,0.0,Tests for Aurora Monitor $end$ Documentation is here: https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Monitors/Aurora-Monitor.md $acceptance criteria:$,0,0,0,0,0,0,1,0.0,76,15,0.197368,4,0.0526316,2,0.0263158,0,0.0,0,0.0
1964,MXS-929,New Feature,MXS,2016-10-18 07:40:22,,0,Dynamic Configuration of Firewall,"h2. Dynamic configuration of the firewall filter
The user should be able to modify the rules of the dbfwfilter. This is a likely scenario as security is not a static concept.

h2. Example use case
The developer changes a name of a temporary table column and this is blocked by the firewall filter. The DBA then modifies the rules file and reloads it. MaxScale then uses the new rule file to block queries that do not match the temporary table column.

h2. Required code changes
Each module needs to expose either an update or an extension entry point. Exposing an update entry point would allow for a more structured way of updating modules. Exposing a custom command entry point would allow modules to implement only the operations that they can and should do.

One option is to expose both an update and a custom command (an {{extension}} entry point in some sense) in the API.

It is also possible, that custom commands could be implemented as callbacks that are registered to the core by the modules. These callbacks could then be called via some common gateway. It would allow the modules to expose new commands and the diagnostic interface would be defined by the modules that are in use.

h3. Benefits of custom module commands
Having an entry point in the API that allows modules to implement actions which aren't in the module API. For example, the qlafilter could rotate log files with a {{rotate logs}} entry point, the cache filter could drop caches and the schemarouter could reload all database maps.

h3. Benefits of an update command
Being able to call a standard entry point would make it easier to implement runtime changes to modules. It would also guide the developer of a module to expect changes to the system. New modules could be designed with a more defined update policy which would make for a more dynamic user experience.

h3. Benefits of custom callback registration
Registering a callback keeps the API definition small and compact. This lowers the threshold of developing new modules and makes the API easier to understand.

h2. Implementation
The chosen design was the custom callback registration as it allows the greatest amount of flexibility. Modules can register different sorts of functions without breaking the module API.",,"Dynamic Configuration of Firewall $end$ h2. Dynamic configuration of the firewall filter
The user should be able to modify the rules of the dbfwfilter. This is a likely scenario as security is not a static concept.

h2. Example use case
The developer changes a name of a temporary table column and this is blocked by the firewall filter. The DBA then modifies the rules file and reloads it. MaxScale then uses the new rule file to block queries that do not match the temporary table column.

h2. Required code changes
Each module needs to expose either an update or an extension entry point. Exposing an update entry point would allow for a more structured way of updating modules. Exposing a custom command entry point would allow modules to implement only the operations that they can and should do.

One option is to expose both an update and a custom command (an {{extension}} entry point in some sense) in the API.

It is also possible, that custom commands could be implemented as callbacks that are registered to the core by the modules. These callbacks could then be called via some common gateway. It would allow the modules to expose new commands and the diagnostic interface would be defined by the modules that are in use.

h3. Benefits of custom module commands
Having an entry point in the API that allows modules to implement actions which aren't in the module API. For example, the qlafilter could rotate log files with a {{rotate logs}} entry point, the cache filter could drop caches and the schemarouter could reload all database maps.

h3. Benefits of an update command
Being able to call a standard entry point would make it easier to implement runtime changes to modules. It would also guide the developer of a module to expect changes to the system. New modules could be designed with a more defined update policy which would make for a more dynamic user experience.

h3. Benefits of custom callback registration
Registering a callback keeps the API definition small and compact. This lowers the threshold of developing new modules and makes the API easier to understand.

h2. Implementation
The chosen design was the custom callback registration as it allows the greatest amount of flexibility. Modules can register different sorts of functions without breaking the module API. $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,18,,0,1,0,1,0,10,0,,0,850,1,3,0,2016-11-16 10:45:53,Dynamic Configuration of Firewall,"The user should be able to modify the rules of the dbfwfilter.

An example is that the developer changes a name of a temporary table column and this is blocked by the firewall filter. The DBA then modifies the rules file and reloads it. MaxScale then uses the new rule file to block queries.",,0,7,0,340,5.45902,"Dynamic Configuration of Firewall $end$ The user should be able to modify the rules of the dbfwfilter.

An example is that the developer changes a name of a temporary table column and this is blocked by the firewall filter. The DBA then modifies the rules file and reloads it. MaxScale then uses the new rule file to block queries. $acceptance criteria:$",7,1,1,1,1,1,1,699.083,77,15,0.194805,4,0.0519481,2,0.025974,0,0.0,0,0.0
1965,MXS-930,New Feature,MXS,2016-10-18 07:46:18,,0,MaxRows Filter,"The purpose of the filter is to put a hard limit on the maximum number of rows that will be returned to a client. 

Example: With following mask filter, only query result set with 150 or fewer rows will be returned to the client. If the result set is more than 150 rows, 0 rows will be returned to the client.
{code}
[Mask Filter]
type=filter
module=maskfilter
maxrows=150
{code}",,"MaxRows Filter $end$ The purpose of the filter is to put a hard limit on the maximum number of rows that will be returned to a client. 

Example: With following mask filter, only query result set with 150 or fewer rows will be returned to the client. If the result set is more than 150 rows, 0 rows will be returned to the client.
{code}
[Mask Filter]
type=filter
module=maskfilter
maxrows=150
{code} $acceptance criteria:$",,Johan Wikman,Johan Wikman,Major,9,,0,1,0,2,0,0,0,,0,850,1,0,0,2016-10-18 09:32:10,MaxRows Filter,"The purpose of the filter is to put a hard limit on the maximum number of rows that will be returned to a client. 

Example: With following mask filter, only query result set with 150 or fewer rows will be returned to the client. If the result set is more than 150 rows, 0 rows will be returned to the client.
{code}
[Mask Filter]
type=filter
module=maskfilter
maxrows=150
{code}",,0,0,0,0,0.0,"MaxRows Filter $end$ The purpose of the filter is to put a hard limit on the maximum number of rows that will be returned to a client. 

Example: With following mask filter, only query result set with 150 or fewer rows will be returned to the client. If the result set is more than 150 rows, 0 rows will be returned to the client.
{code}
[Mask Filter]
type=filter
module=maskfilter
maxrows=150
{code} $acceptance criteria:$",0,0,0,0,0,0,1,1.75,78,16,0.205128,5,0.0641026,3,0.0384615,1,0.0128205,1,0.0128205
1966,MXS-932,Sub-Task,MXS,2016-10-18 07:51:02,,0,Create tests for the Cache,,,Create tests for the Cache $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,10,0,0,0,,0,850,0,0,0,2016-10-18 07:51:02,Create tests for the Cache,,,0,0,0,0,0.0,Create tests for the Cache $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,79,16,0.202532,5,0.0632911,3,0.0379747,1,0.0126582,1,0.0126582
1967,MXS-934,Task,MXS,2016-10-18 08:12:43,MXS-881,0,QC: Provide support for prepared statements.,Provide support for getting information about prepared statements.,,QC: Provide support for prepared statements. $end$ Provide support for getting information about prepared statements. $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,1,1,1,0,0,0,,0,850,1,0,0,2016-10-18 09:29:31,QC: Provide support for prepared statements.,Provide support for getting information about prepared statements.,,0,0,0,0,0.0,QC: Provide support for prepared statements. $end$ Provide support for getting information about prepared statements. $acceptance criteria:$,0,0,0,0,0,0,0,1.26667,80,16,0.2,5,0.0625,3,0.0375,1,0.0125,1,0.0125
1968,MXS-935,Sub-Task,MXS,2016-10-18 08:18:23,,0,Implement column matching.,,,Implement column matching. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,1,10,0,0,0,,0,850,0,0,0,2016-10-18 08:18:23,Implement column matching.,,,0,0,0,0,0.0,Implement column matching. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,81,16,0.197531,5,0.0617284,3,0.037037,1,0.0123457,1,0.0123457
1969,MXS-936,Task,MXS,2016-10-18 09:26:37,,0,Add shutdown hooks,,,Add shutdown hooks $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 09:26:37,Add shutdown hooks,,,0,0,0,0,0.0,Add shutdown hooks $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,82,16,0.195122,5,0.0609756,3,0.0365854,1,0.0121951,1,0.0121951
1970,MXS-937,Sub-Task,MXS,2016-10-18 09:29:35,,0,Load a list of database users for pre-authentication,The GSSAPI client side authenticator should load a list of database users so that it can provide better error messages to the clients.,,Load a list of database users for pre-authentication $end$ The GSSAPI client side authenticator should load a list of database users so that it can provide better error messages to the clients. $acceptance criteria:$,,markus makela,markus makela,Major,2,,0,0,0,3,0,0,0,,0,850,0,0,0,2016-10-18 09:29:35,Load a list of database users for pre-authentication,The GSSAPI client side authenticator should load a list of database users so that it can provide better error messages to the clients.,,0,0,0,0,0.0,Load a list of database users for pre-authentication $end$ The GSSAPI client side authenticator should load a list of database users so that it can provide better error messages to the clients. $acceptance criteria:$,0,0,0,0,0,0,1,0.0,12,3,0.25,3,0.25,3,0.25,3,0.25,3,0.25
1971,MXS-938,Task,MXS,2016-10-18 09:30:23,,0,Track transaction state.,,,Track transaction state. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-10-18 09:30:23,Track transaction state.,,,0,0,0,0,0.0,Track transaction state. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,83,16,0.192771,5,0.060241,3,0.0361446,1,0.0120482,1,0.0120482
1972,MXS-939,Task,MXS,2016-10-18 09:34:16,,0,Investigate MaxScale random test failures.,,,Investigate MaxScale random test failures. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,1,0,3,0,0,0,,0,850,1,0,0,2016-10-18 09:34:16,Investigate MaxScale random test failures.,,,0,0,0,0,0.0,Investigate MaxScale random test failures. $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,84,16,0.190476,5,0.0595238,3,0.0357143,1,0.0119048,1,0.0119048
1973,MXS-940,Task,MXS,2016-10-18 09:40:56,,0,Further readwritesplit cleanup.,,,Further readwritesplit cleanup. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-10-18 09:40:56,Further readwritesplit cleanup.,,,0,0,0,0,0.0,Further readwritesplit cleanup. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,85,16,0.188235,5,0.0588235,3,0.0352941,1,0.0117647,1,0.0117647
1974,MXS-941,Task,MXS,2016-10-18 09:43:46,,0,Prepare 1.4.4,,,Prepare 1.4.4 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,8,,0,850,0,0,0,2016-10-18 09:43:46,Prepare 1.4.4,,,0,0,0,0,0.0,Prepare 1.4.4 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,86,16,0.186047,5,0.0581395,3,0.0348837,1,0.0116279,1,0.0116279
1975,MXS-943,Sub-Task,MXS,2016-10-18 17:06:53,,0,1.4.4: Create branch,,,1.4.4: Create branch $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:06:53,1.4.4: Create branch,,,0,0,0,0,0.0,1.4.4: Create branch $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,87,16,0.183908,5,0.0574713,3,0.0344828,1,0.0114943,1,0.0114943
1976,MXS-944,Sub-Task,MXS,2016-10-18 17:07:39,,0,1.4.4: Merge all post 1.4.3 binlog patches to branch 1.4.4,,,1.4.4: Merge all post 1.4.3 binlog patches to branch 1.4.4 $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,6,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:07:39,1.4.4: Merge all post 1.4.3 binlog patches to branch 1.4.4,,,0,0,0,0,0.0,1.4.4: Merge all post 1.4.3 binlog patches to branch 1.4.4 $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,88,16,0.181818,5,0.0568182,3,0.0340909,1,0.0113636,1,0.0113636
1977,MXS-945,Sub-Task,MXS,2016-10-18 17:10:12,,0,1.4.4: Change Log,,,1.4.4: Change Log $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:10:12,1.4.4: Change Log,,,0,0,0,0,0.0,1.4.4: Change Log $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,89,16,0.179775,5,0.0561798,3,0.0337079,1,0.011236,1,0.011236
1978,MXS-946,Sub-Task,MXS,2016-10-18 17:10:27,,0,1.4.4: Upgrading To,,,1.4.4: Upgrading To $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,2,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:10:27,1.4.4: Upgrading To,,,0,0,0,0,0.0,1.4.4: Upgrading To $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,90,16,0.177778,5,0.0555556,3,0.0333333,1,0.0111111,1,0.0111111
1979,MXS-947,Sub-Task,MXS,2016-10-18 17:10:50,,0,1.4.4: Release Notes,,,1.4.4: Release Notes $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:10:50,1.4.4: Release Notes,,,0,0,0,0,0.0,1.4.4: Release Notes $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,91,16,0.175824,5,0.0549451,3,0.032967,1,0.010989,1,0.010989
1980,MXS-948,Sub-Task,MXS,2016-10-18 17:11:19,,0,1.4.4: Build and upload binaries,,,1.4.4: Build and upload binaries $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,4,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:11:19,1.4.4: Build and upload binaries,,,0,0,0,0,0.0,1.4.4: Build and upload binaries $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,92,16,0.173913,5,0.0543478,3,0.0326087,1,0.0108696,1,0.0108696
1981,MXS-949,Sub-Task,MXS,2016-10-18 17:12:09,,0,1.4.4: Regression testing,,,1.4.4: Regression testing $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,2,0,2,0,0,0,,0,850,2,0,0,2016-10-18 17:12:09,1.4.4: Regression testing,,,0,0,0,0,0.0,1.4.4: Regression testing $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,93,16,0.172043,5,0.0537634,3,0.0322581,1,0.0107527,1,0.0107527
1982,MXS-950,Sub-Task,MXS,2016-10-18 17:13:24,,0,1.4.4: Sync documentation to KB,,,1.4.4: Sync documentation to KB $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,3,,0,0,0,2,0,0,0,,0,850,0,0,0,2016-10-18 17:13:24,1.4.4: Sync documentation to KB,,,0,0,0,0,0.0,1.4.4: Sync documentation to KB $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,94,16,0.170213,5,0.0531915,3,0.0319149,1,0.0106383,1,0.0106383
1983,MXS-971,Task,MXS,2016-11-16 09:13:22,,0,Implement LRU mechanism for Cache storage.,,,Implement LRU mechanism for Cache storage. $end$ $acceptance criteria:$,,Johan Wikman,Johan Wikman,Major,5,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-11-16 10:44:54,Implement LRU mechanism for Cache storage.,,,0,0,0,0,0.0,Implement LRU mechanism for Cache storage. $end$ $acceptance criteria:$,0,0,0,0,0,0,0,1.51667,95,16,0.168421,5,0.0526316,3,0.0315789,1,0.0105263,1,0.0105263
1984,MXS-980,Task,MXS,2016-11-18 16:23:07,,0,Regular build packages for all OS,Build and install test should be executed for all active branches and all distros on regular basis,,Regular build packages for all OS $end$ Build and install test should be executed for all active branches and all distros on regular basis $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,7,,0,0,0,4,0,0,5,,0,850,0,0,0,2017-02-01 10:53:37,Regular build packages for all OS,Build and install test should be executed for all active branches and all distros on regular basis,,0,0,0,0,0.0,Regular build packages for all OS $end$ Build and install test should be executed for all active branches and all distros on regular basis $acceptance criteria:$,0,0,0,0,0,0,1,1794.5,25,1,0.04,0,0.0,0,0.0,0,0.0,0,0.0
1985,MXS-981,Sub-Task,MXS,2016-11-18 16:23:45,,0,Jenkins job to trigger regular build,,,Jenkins job to trigger regular build $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,6,,0,0,0,4,0,0,0,,0,850,0,0,0,2016-11-18 16:23:45,Jenkins job to trigger regular build,,,0,0,0,0,0.0,Jenkins job to trigger regular build $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,26,1,0.0384615,0,0.0,0,0.0,0,0.0,0,0.0
1986,MXS-983,Sub-Task,MXS,2016-11-18 16:30:23,,0,Jenkins job to trigger install test,,,Jenkins job to trigger install test $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,4,,0,1,0,4,0,0,0,,0,850,1,0,0,2016-11-18 16:30:23,Jenkins job to trigger install test,,,0,0,0,0,0.0,Jenkins job to trigger install test $end$ $acceptance criteria:$,0,0,0,0,0,0,1,0.0,27,1,0.037037,0,0.0,0,0.0,0,0.0,0,0.0
1987,MXS-985,Sub-Task,MXS,2016-11-18 16:35:07,,0,Scheduler for builds and install tests,"regular builds and regular install tests for active branches ('develop', 2.0., 1.4, etc) can create significant load on Jenkins server and it should be fine-tuned to avoid interfering with other testing

this tasks includes also solution testing on real Jenkins",,"Scheduler for builds and install tests $end$ regular builds and regular install tests for active branches ('develop', 2.0., 1.4, etc) can create significant load on Jenkins server and it should be fine-tuned to avoid interfering with other testing

this tasks includes also solution testing on real Jenkins $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,4,,0,2,0,4,0,0,0,,0,850,2,0,0,2016-11-18 16:35:07,Scheduler for builds and install tests,"regular builds and regular install tests for active branches ('develop', 2.0., 1.4, etc) can create significant load on Jenkins server and it should be fine-tuned to avoid interfering with other testing

this tasks includes also solution testing on real Jenkins",,0,0,0,0,0.0,"Scheduler for builds and install tests $end$ regular builds and regular install tests for active branches ('develop', 2.0., 1.4, etc) can create significant load on Jenkins server and it should be fine-tuned to avoid interfering with other testing

this tasks includes also solution testing on real Jenkins $acceptance criteria:$",0,0,0,0,0,0,1,0.0,28,1,0.0357143,0,0.0,0,0.0,0,0.0,0,0.0
1988,MXS-986,Sub-Task,MXS,2016-11-18 16:40:30,,0,Monitoring and automatic clean-up for builds and install tests,"In case of automatic regular builds it is important to clean up automatically  failed Jenkins job, VMs,, etc ",,"Monitoring and automatic clean-up for builds and install tests $end$ In case of automatic regular builds it is important to clean up automatically  failed Jenkins job, VMs,, etc  $acceptance criteria:$",,Timofey Turenko,Timofey Turenko,Major,2,,0,1,0,4,0,0,0,,0,850,1,0,0,2016-11-18 16:40:30,Monitoring and automatic clean-up for builds and install tests,"In case of automatic regular builds it is important to clean up automatically  failed Jenkins job, VMs,, etc ",,0,0,0,0,0.0,"Monitoring and automatic clean-up for builds and install tests $end$ In case of automatic regular builds it is important to clean up automatically  failed Jenkins job, VMs,, etc  $acceptance criteria:$",0,0,0,0,0,0,1,0.0,29,1,0.0344828,0,0.0,0,0.0,0,0.0,0,0.0
1989,MXS-993,Task,MXS,2016-11-18 19:22:34,,0,Move maxscale-system-test repository into the MaxScale,,,Move maxscale-system-test repository into the MaxScale $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,3,,0,0,0,1,0,0,1,,0,850,0,0,0,2017-05-18 09:35:04,Move maxscale-system-test repository into the MaxScale,,,0,0,0,0,0.0,Move maxscale-system-test repository into the MaxScale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,4334.2,30,1,0.0333333,0,0.0,0,0.0,0,0.0,0,0.0
1990,MXS-995,Sub-Task,MXS,2016-11-18 19:24:36,,0,move tests to Maxscale repo. modify Jenkins to use tests from Maxscale,,,move tests to Maxscale repo. modify Jenkins to use tests from Maxscale $end$ $acceptance criteria:$,,Timofey Turenko,Timofey Turenko,Major,2,,0,0,0,1,0,0,0,,0,850,0,0,0,2016-11-18 19:24:36,move tests to Maxscale repo. modify Jenkins to use tests from Maxscale,,,0,0,0,0,0.0,move tests to Maxscale repo. modify Jenkins to use tests from Maxscale $end$ $acceptance criteria:$,0,0,0,0,0,0,0,0.0,31,1,0.0322581,0,0.0,0,0.0,0,0.0,0,0.0
