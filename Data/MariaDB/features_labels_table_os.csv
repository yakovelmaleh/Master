,issue_key,project_key,author,id,created,body,chronological_number,clean_comment
0,CONJ-125,CONJ,Massimo Siani,66558,2014-12-10 13:53:59,"Hi Lennart,
not sure whether you already tried, but adding
{code}
statement.setFetchSize(Integer.MIN_VALUE);
{code}
decreases the memory need by a factor of 100 (uses the stream ResultSet feature). Maybe it doesn't solve your problem, though.

In any case, I'm investigating how to decrease the memory need for CachedResultSet objects. Let me share that, as far as I was able to understand, it seems that the problem is due to the large number of MySQLValueObject objects. I guess it's worth spending some time to understand if we can avoid such object.

Any comment is very welcomed!",1,"Hi Lennart,
not sure whether you already tried, but adding
{code}
statement.setFetchSize(Integer.MIN_VALUE);
{code}
decreases the memory need by a factor of 100 (uses the stream ResultSet feature). Maybe it doesn't solve your problem, though.

In any case, I'm investigating how to decrease the memory need for CachedResultSet objects. Let me share that, as far as I was able to understand, it seems that the problem is due to the large number of MySQLValueObject objects. I guess it's worth spending some time to understand if we can avoid such object.

Any comment is very welcomed!"
1,CONJ-125,CONJ,Lennart Schedin,66562,2014-12-11 14:02:34,"Yes I know about the ResultSet stream feature (setFetchSize(Integer.MIN_VALUE)). But I did not get that to work (because of either a bug in my application code or in MariaDB JDBC). I will write a ticket if I find any specific bug in MariaDB JDBC.

For the specific problem with CachedResultSet I currently don't have any implementation proposal. The only thing I have a high level idea: only cache the binary data received for each row. Then as the application code calls the resultSet.next() method each binary data row is converted into the MySQLValueObject-objects data structure. Each subsequent call to resultSet.next() can overwrite the previous  MySQLValueObject-objects data structure. With this idea only one row at a time would be “inflated” into Java objects at a time. All the other rows could remain in a byte array. Alternative solution is to inflate only 1, 5, 10, 100 or 1000 (etc) rows at a time.

I suspect that it could take same time and will require a large refactoring of the code. I would estimate the risk of fixing this ticket to be high (it may cause other bugs).
",2,"Yes I know about the ResultSet stream feature (setFetchSize(Integer.MIN_VALUE)). But I did not get that to work (because of either a bug in my application code or in MariaDB JDBC). I will write a ticket if I find any specific bug in MariaDB JDBC.

For the specific problem with CachedResultSet I currently don't have any implementation proposal. The only thing I have a high level idea: only cache the binary data received for each row. Then as the application code calls the resultSet.next() method each binary data row is converted into the MySQLValueObject-objects data structure. Each subsequent call to resultSet.next() can overwrite the previous  MySQLValueObject-objects data structure. With this idea only one row at a time would be “inflated” into Java objects at a time. All the other rows could remain in a byte array. Alternative solution is to inflate only 1, 5, 10, 100 or 1000 (etc) rows at a time.

I suspect that it could take same time and will require a large refactoring of the code. I would estimate the risk of fixing this ticket to be high (it may cause other bugs).
"
2,CONJ-125,CONJ,Massimo Siani,66563,2014-12-11 14:20:22,"Could you refactor your code where you can't use the stream feature so that we can add a test case for your real case?
Would you be interested in that?",3,"Could you refactor your code where you can't use the stream feature so that we can add a test case for your real case?
Would you be interested in that?"
3,CONJ-125,CONJ,Diego Dupin,82398,2016-03-31 07:24:49,test result show now 26m footprint with version1.4.0,4,test result show now 26m footprint with version1.4.0
4,CONJ-141,CONJ,Diego Dupin,73848,2015-07-22 19:39:21,"adding test case that handle different possibilities : 

{code:java}
/**
     * CONJ-141 : Batch Statement Rewrite: Support for ON DUPLICATE KEY
     * @throws SQLException
     */
    @Test
    public void rewriteBatchedStatementsWithQueryFirstAndLAst() throws SQLException {
        Properties props = new Properties();
        props.setProperty(""rewriteBatchedStatements"", ""true"");
        Connection tmpConnection = null;
        try {
            tmpConnection = openNewConnection(connURI, props);
            Statement st = tmpConnection.createStatement();
            st.executeUpdate(""drop table if exists t3_dupp"");
            st.executeUpdate(""create table t3_dupp(col1 int, pkey int NOT NULL, col2 int, col3 int, col4 int, PRIMARY KEY (`pkey`))"");

            PreparedStatement sqlInsert = connection.prepareStatement(""INSERT INTO t3_dupp(col1, pkey,col2,col3,col4) VALUES (9, ?, 5, ?, 8) ON DUPLICATE KEY UPDATE pkey=pkey+10"");
            sqlInsert.setInt(1, 1);
            sqlInsert.setInt(2, 2);
            sqlInsert.addBatch();

            sqlInsert.setInt(1, 2);
            sqlInsert.setInt(2, 5);
            sqlInsert.addBatch();

            sqlInsert.setInt(1, 7);
            sqlInsert.setInt(2, 6);
            sqlInsert.addBatch();
            sqlInsert.executeBatch();
        } finally {
            if (tmpConnection != null) tmpConnection.close();
        }
    }
{code}

Resulting query send to database : 
INSERT INTO t3_dupp(col1, pkey,col2,col3,col4) VALUES (9, 1, 5, 2, 8),(9, 2, 5, 5, 8),(9, 7, 5, 6, 8) ON DUPLICATE KEY UPDATE pkey=pkey+10

",1,"adding test case that handle different possibilities : 

{code:java}
/**
     * CONJ-141 : Batch Statement Rewrite: Support for ON DUPLICATE KEY
     * @throws SQLException
     */
    @Test
    public void rewriteBatchedStatementsWithQueryFirstAndLAst() throws SQLException {
        Properties props = new Properties();
        props.setProperty(""rewriteBatchedStatements"", ""true"");
        Connection tmpConnection = null;
        try {
            tmpConnection = openNewConnection(connURI, props);
            Statement st = tmpConnection.createStatement();
            st.executeUpdate(""drop table if exists t3_dupp"");
            st.executeUpdate(""create table t3_dupp(col1 int, pkey int NOT NULL, col2 int, col3 int, col4 int, PRIMARY KEY (`pkey`))"");

            PreparedStatement sqlInsert = connection.prepareStatement(""INSERT INTO t3_dupp(col1, pkey,col2,col3,col4) VALUES (9, ?, 5, ?, 8) ON DUPLICATE KEY UPDATE pkey=pkey+10"");
            sqlInsert.setInt(1, 1);
            sqlInsert.setInt(2, 2);
            sqlInsert.addBatch();

            sqlInsert.setInt(1, 2);
            sqlInsert.setInt(2, 5);
            sqlInsert.addBatch();

            sqlInsert.setInt(1, 7);
            sqlInsert.setInt(2, 6);
            sqlInsert.addBatch();
            sqlInsert.executeBatch();
        } finally {
            if (tmpConnection != null) tmpConnection.close();
        }
    }
{code}

Resulting query send to database : 
INSERT INTO t3_dupp(col1, pkey,col2,col3,col4) VALUES (9, 1, 5, 2, 8),(9, 2, 5, 5, 8),(9, 7, 5, 6, 8) ON DUPLICATE KEY UPDATE pkey=pkey+10

"
5,CONJ-145,CONJ,Diego Dupin,76115,2015-09-21 17:06:13,"functionnality added in the next version 1.3.0 to complete failover implementation.
",1,"functionnality added in the next version 1.3.0 to complete failover implementation.
"
6,CONJ-163,CONJ,Diego Dupin,73363,2015-07-11 15:41:27,"
getColumnLabel must be your solution : 

{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_1
:  getColumnLabel return col_A
{code}

JDBC-4.0 indicate that there is 2 differents things : 
- getColumnName will have the physical name
- getColumnLabel will be the alias if defined, the physical name if not. 

Does it not answer your needs ?
",1,"
getColumnLabel must be your solution : 

{code:java}
select col_1 as col_A from test_table;
:  getColumnName return col_1
:  getColumnLabel return col_A
{code}

JDBC-4.0 indicate that there is 2 differents things : 
- getColumnName will have the physical name
- getColumnLabel will be the alias if defined, the physical name if not. 

Does it not answer your needs ?
"
7,CONJ-163,CONJ,seung hoon yoo,73382,2015-07-13 03:27:20,"getColumnLabel is solution. 
I agree,
BUT,
1. mysql client supports useOldAliasMetadataBehavior option about column alias
2. Some package solution application code was used getColumnName instead of getColumnLagel
   (using with useOldAliasMetadataBehavior=true)

My situation some package developer says,
"" Our package run in mysql envirment(useOldAliasMetadataBehavior=ture), but mariadb does not""
so I want mariadb support more (old) package or appliation.
and it's more Compatible with mysql feature.

",2,"getColumnLabel is solution. 
I agree,
BUT,
1. mysql client supports useOldAliasMetadataBehavior option about column alias
2. Some package solution application code was used getColumnName instead of getColumnLagel
   (using with useOldAliasMetadataBehavior=true)

My situation some package developer says,
"" Our package run in mysql envirment(useOldAliasMetadataBehavior=ture), but mariadb does not""
so I want mariadb support more (old) package or appliation.
and it's more Compatible with mysql feature.

"
8,CONJ-163,CONJ,Diego Dupin,73843,2015-07-22 16:44:16,done as requested for mysql compatibility,3,done as requested for mysql compatibility
9,CONJ-22,CONJ,Vladislav Vaintroub,29869,2013-02-06 23:58:04,"This is a feature request, a rather big feature (i.e connector-big , not server-big:)

The driver documentation https://kb.askmonty.org/en/about-the-mariadb-java-client/  lists all supported parameters and also states
""The driver only uses text protocol to communicate with the database. Prepared statements (parameter substitution) is handled by the driver, on the client side.""

This is basically what  this bug is all about.  The only unique feature that server side prepared statement allows is possibility to get result set metadata of the prepared statement without execution (CONJ-21)",1,"This is a feature request, a rather big feature (i.e connector-big , not server-big:)

The driver documentation URL  lists all supported parameters and also states
""The driver only uses text protocol to communicate with the database. Prepared statements (parameter substitution) is handled by the driver, on the client side.""

This is basically what  this bug is all about.  The only unique feature that server side prepared statement allows is possibility to get result set metadata of the prepared statement without execution (CONJ-21)"
10,CONJ-22,CONJ,Elena Stepanova,29870,2013-02-07 00:27:09,"Strictly speaking, apart from the unique feature, there is probably also performance at stake (I didn't run benchmarks, but I expect server-side prepared statements to be faster); but otherwise I agree, it's a feature request, and not even a critical one, considering the number of problems with prepared statements on the server side. Sorry for not finding it on the KB page.

Unfortunately, the lack of the feature described in CONJ-21 means that client applications relying on it and trying to switch from connector/j to mariadb library will start throwing rather ugly null pointer exceptions. ",2,"Strictly speaking, apart from the unique feature, there is probably also performance at stake (I didn't run benchmarks, but I expect server-side prepared statements to be faster); but otherwise I agree, it's a feature request, and not even a critical one, considering the number of problems with prepared statements on the server side. Sorry for not finding it on the KB page.

Unfortunately, the lack of the feature described in CONJ-21 means that client applications relying on it and trying to switch from connector/j to mariadb library will start throwing rather ugly null pointer exceptions. "
11,CONJ-22,CONJ,Andy Shulman,35768,2013-11-03 03:25:07,I'd just like to +1 this issue. I just switched from the Oracle JDBC driver because it didn't support PreparedStatement rewriting. I'm sad to lose server-side PreparedStatements as they are quite a bit faster for my application (though rewriting is a much bigger increase). Getting both features in one driver would be fantastic.,3,I'd just like to +1 this issue. I just switched from the Oracle JDBC driver because it didn't support PreparedStatement rewriting. I'm sad to lose server-side PreparedStatements as they are quite a bit faster for my application (though rewriting is a much bigger increase). Getting both features in one driver would be fantastic.
12,CONJ-22,CONJ,Paolo Bazzi,73200,2015-07-06 10:16:58,"Using the new MariaDB JDBC Driver 1.1.9 I'm still not able to show that server-prepared statements are working. 

Using the attached test case from above still prints out the following:
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 0

If I switch the testcase and use the mysql-connector-java-5.1.32-bin.jar JDBC driver, the output is as expected:
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 1

Could you check again, if this issues was really solved with 1.1.9?
",4,"Using the new MariaDB JDBC Driver 1.1.9 I'm still not able to show that server-prepared statements are working. 

Using the attached test case from above still prints out the following:
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 0

If I switch the testcase and use the mysql-connector-java-5.1.32-bin.jar JDBC driver, the output is as expected:
Prepared_stmt_count before prepare: 0
Prepared_stmt_count after prepare: 1

Could you check again, if this issues was really solved with 1.1.9?
"
13,CONJ-22,CONJ,diego dupin,73202,2015-07-06 11:39:57,"The PreparedStatement rewriting has been fixed, but that doesn't include this issue.
it is indeed not fixed. 
",5,"The PreparedStatement rewriting has been fixed, but that doesn't include this issue.
it is indeed not fixed. 
"
14,CONJ-22,CONJ,Marcel Schneider,79882,2016-01-15 16:39:21,"I verified this with version 1.3.3, and it seems that the useServerPrepStmts parameter still does not work.
I suggest to reopen the ticket.",6,"I verified this with version 1.3.3, and it seems that the useServerPrepStmts parameter still does not work.
I suggest to reopen the ticket."
15,CONJ-22,CONJ,Marcel Schneider,80410,2016-01-27 17:30:36,Shall i make a new ticket?,7,Shall i make a new ticket?
16,CONJ-22,CONJ,Diego Dupin,80444,2016-01-29 10:43:20,"Hi, 
I check again and this work. 
Can you elaborate on the problem you have ?

some tests case i've run:
{code:java}
    @Test
    public void serverExecutionTest() throws SQLException {
        createTable(""ServerPrepareStatementTestt"", ""id int not null primary key auto_increment, test boolean"");
        try (Connection connection = setConnection()){
            Statement statement = connection.createStatement();
            ResultSet rs = statement.executeQuery(""show global status like 'Prepared_stmt_count'"");
            assertTrue(rs.next());
            final int nbStatementCount = rs.getInt(2);

            PreparedStatement ps = connection.prepareStatement(
                    ""INSERT INTO ServerPrepareStatementTestt (test) VALUES (?)"");
            ps.setBoolean(1, true);
            ps.addBatch();
            ps.execute();

            rs = statement.executeQuery(""show global status like 'Prepared_stmt_count'"");
            assertTrue(rs.next());
            assertTrue(rs.getInt(2) == nbStatementCount + 1);
        }
    }
{code}

{code:java}
    @Test
    public void testCache() throws SQLException {
        createTable(""ServerPrepareStatementPrepareCache"", ""id int not null primary key auto_increment, test varchar(20)"");
        try (Connection connection = setConnection()) {
            final String QUERY = ""INSERT INTO ServerPrepareStatementPrepareCache(test) VALUES (?)"";
            final long startTime = System.nanoTime();
            PreparedStatement pstmt = connection.prepareStatement(QUERY);
            pstmt.setString(1, ""test1"");
            pstmt.execute();
            final long executionTime = System.nanoTime() - startTime;

            final long startTimeSecond = System.nanoTime();
            PreparedStatement pstmt2 = connection.prepareStatement(QUERY);
            pstmt2.setString(1, ""test2"");
            pstmt2.execute();
            final long executionTimeSecond = System.nanoTime() - startTimeSecond;

            System.out.println(""total time : "" + (executionTimeSecond) + "" first : "" + executionTime);
            Assert.assertTrue(executionTimeSecond  * 10 < executionTime);

            ResultSet resultSet = connection.createStatement().executeQuery(""SELECT * FROM ServerPrepareStatementPrepareCache"");
            if (resultSet.next()) {
                Assert.assertEquals(""test1"", resultSet.getString(2));
                if (resultSet.next()) {
                    Assert.assertEquals(""test2"", resultSet.getString(2));
                } else {
                    Assert.fail(""Must have a result"");
                }
            } else {
                Assert.fail(""Must have a result"");
            }
        }
    }
{code}
console result is : 
second query is total time : 2788435 first : 59721444
 ",8,"Hi, 
I check again and this work. 
Can you elaborate on the problem you have ?

some tests case i've run:
{code:java}
    @Test
    public void serverExecutionTest() throws SQLException {
        createTable(""ServerPrepareStatementTestt"", ""id int not null primary key auto_increment, test boolean"");
        try (Connection connection = setConnection()){
            Statement statement = connection.createStatement();
            ResultSet rs = statement.executeQuery(""show global status like 'Prepared_stmt_count'"");
            assertTrue(rs.next());
            final int nbStatementCount = rs.getInt(2);

            PreparedStatement ps = connection.prepareStatement(
                    ""INSERT INTO ServerPrepareStatementTestt (test) VALUES (?)"");
            ps.setBoolean(1, true);
            ps.addBatch();
            ps.execute();

            rs = statement.executeQuery(""show global status like 'Prepared_stmt_count'"");
            assertTrue(rs.next());
            assertTrue(rs.getInt(2) == nbStatementCount + 1);
        }
    }
{code}

{code:java}
    @Test
    public void testCache() throws SQLException {
        createTable(""ServerPrepareStatementPrepareCache"", ""id int not null primary key auto_increment, test varchar(20)"");
        try (Connection connection = setConnection()) {
            final String QUERY = ""INSERT INTO ServerPrepareStatementPrepareCache(test) VALUES (?)"";
            final long startTime = System.nanoTime();
            PreparedStatement pstmt = connection.prepareStatement(QUERY);
            pstmt.setString(1, ""test1"");
            pstmt.execute();
            final long executionTime = System.nanoTime() - startTime;

            final long startTimeSecond = System.nanoTime();
            PreparedStatement pstmt2 = connection.prepareStatement(QUERY);
            pstmt2.setString(1, ""test2"");
            pstmt2.execute();
            final long executionTimeSecond = System.nanoTime() - startTimeSecond;

            System.out.println(""total time : "" + (executionTimeSecond) + "" first : "" + executionTime);
            Assert.assertTrue(executionTimeSecond  * 10 < executionTime);

            ResultSet resultSet = connection.createStatement().executeQuery(""SELECT * FROM ServerPrepareStatementPrepareCache"");
            if (resultSet.next()) {
                Assert.assertEquals(""test1"", resultSet.getString(2));
                if (resultSet.next()) {
                    Assert.assertEquals(""test2"", resultSet.getString(2));
                } else {
                    Assert.fail(""Must have a result"");
                }
            } else {
                Assert.fail(""Must have a result"");
            }
        }
    }
{code}
console result is : 
second query is total time : 2788435 first : 59721444
 "
17,CONJ-22,CONJ,Marcel Schneider,80446,2016-01-29 13:35:02,"I can reproduce your test cases: they work as intended. So I went back to my case which doesn't work. After some analyses I found this:
the caching does work if you use a statement with a bind, like this:
""select count(1) from ServerPrepareStatementTestt where id=?"".
However, the caching does not work if you don't use a bind, like this:
""select count(1) from ServerPrepareStatementTestt where 1=1"".
When I switch to the mysql driver, the caching works in both cases.",9,"I can reproduce your test cases: they work as intended. So I went back to my case which doesn't work. After some analyses I found this:
the caching does work if you use a statement with a bind, like this:
""select count(1) from ServerPrepareStatementTestt where id=?"".
However, the caching does not work if you don't use a bind, like this:
""select count(1) from ServerPrepareStatementTestt where 1=1"".
When I switch to the mysql driver, the caching works in both cases."
18,CONJ-22,CONJ,Marcel Schneider,83391,2016-05-12 11:47:23,Were you able to reproduce my results (using literals instead of binds)? Showing that the issue is still open?,10,Were you able to reproduce my results (using literals instead of binds)? Showing that the issue is still open?
19,CONJ-26,CONJ,Vladislav Vaintroub,31598,2013-04-28 20:15:44,"I think fetch direction can safely be ignored. after looking again at spec. this is a hint to the driver, which has no visible effects during runtime (i.e ResultSet.next() will still move forward, and previous() will move backward).  We cannot use this hint - to move backwards with ResultSet.previous(), we have to read and cache the whole result,  no way around it.",1,"I think fetch direction can safely be ignored. after looking again at spec. this is a hint to the driver, which has no visible effects during runtime (i.e ResultSet.next() will still move forward, and previous() will move backward).  We cannot use this hint - to move backwards with ResultSet.previous(), we have to read and cache the whole result,  no way around it."
20,CONJ-26,CONJ,Paolo Bazzi,81268,2016-02-24 17:53:54,"+1 for solving this issue....  
We run into a similiar problem while reading a large set of data from a MariaDB and were forced to switch to the streamed mode, since we ran into OutOfMemory exceptions when fetching all records at once (~8m records). Other JDBC drivers (like Oracle) only fetch the records, when iterating over the result set and therefore require a lot less of memory if the records are processed in a loop and then discarded. 
It would be great to support a configurable fetch size instead of force the user to decide wether to read all or nothing.",2,"+1 for solving this issue....  
We run into a similiar problem while reading a large set of data from a MariaDB and were forced to switch to the streamed mode, since we ran into OutOfMemory exceptions when fetching all records at once (~8m records). Other JDBC drivers (like Oracle) only fetch the records, when iterating over the result set and therefore require a lot less of memory if the records are processed in a loop and then discarded. 
It would be great to support a configurable fetch size instead of force the user to decide wether to read all or nothing."
21,CONJ-26,CONJ,Diego Dupin,81269,2016-02-24 17:56:58,Some good news Paolo : that's in the roadmap for next version 1.4.0.,3,Some good news Paolo : that's in the roadmap for next version 1.4.0.
22,CONJ-26,CONJ,Vladislav Vaintroub,81270,2016-02-24 17:57:57,"[~bazzip] ,alas, is no principal difference between streamed mode  and configurable fetch size. streaming mode requires least memory though :) 
Are you unhappy to be ""forced"" into this mode?",4,"[~bazzip] ,alas, is no principal difference between streamed mode  and configurable fetch size. streaming mode requires least memory though :) 
Are you unhappy to be ""forced"" into this mode?"
23,CONJ-26,CONJ,Paolo Bazzi,81272,2016-02-24 18:10:42,"@Diego very nice to hear!

@Vladislav 
Two issues with the streamed mode
* We use shared java code which is executed on both MariaDB and Oracle databases (some kind of data replication). The fetch size is set for each statement according to business logic and expected statement result size. With this setup we run into OutOfMemoryException problems with large data sets and the MariaDB JDBC driver, since the driver tried to load all data into memory. We were forced to implement a ""if oracle then use fetchSize else use Integer.MIN_VALUE"" Hack to solve the problem
* I would expect a performance gain using an adequate fetch size instead of the streaming mode, which requires a JDBC driver <-> database server network round trip for each fetched result row",5,"@Diego very nice to hear!

@Vladislav 
Two issues with the streamed mode
* We use shared java code which is executed on both MariaDB and Oracle databases (some kind of data replication). The fetch size is set for each statement according to business logic and expected statement result size. With this setup we run into OutOfMemoryException problems with large data sets and the MariaDB JDBC driver, since the driver tried to load all data into memory. We were forced to implement a ""if oracle then use fetchSize else use Integer.MIN_VALUE"" Hack to solve the problem
* I would expect a performance gain using an adequate fetch size instead of the streaming mode, which requires a JDBC driver  database server network round trip for each fetched result row"
24,CONJ-26,CONJ,Vladislav Vaintroub,81273,2016-02-24 18:32:07,"I agree on portability, but I doubt you will gain any performance
 
 the driver does exactly the same amount of network reads, and the server the same amount of writes.
The server writes whole result set, the client reads the whole result set, Oracle may and actually does perform very differently.
",6,"I agree on portability, but I doubt you will gain any performance
 
 the driver does exactly the same amount of network reads, and the server the same amount of writes.
The server writes whole result set, the client reads the whole result set, Oracle may and actually does perform very differently.
"
25,CONJ-26,CONJ,Diego Dupin,82508,2016-04-04 20:43:22,"This is now implemented on version 1.4.0.

Like Vladislav say, since all datas have to be read, performance doesn't change a lot, 
 
JMH results (source https://codeshare.io/OlBRO) when streaming 100,000 rows 

Bench.fetchSizeBy1000       : 50.274 ± 0.206  ms/op (read with fetch size 1000)
Bench.fetchSizeAll               : 50.593 ± 0.252  ms/op (read all data)
Bench.fetchSizeOneByOne : 51.641 ± 0.299  ms/op (fetch one by one)

No big difference, but avoiding to create a big buffer permit to gain a small 1%, ( and avoid loading all in memory)
",7,"This is now implemented on version 1.4.0.

Like Vladislav say, since all datas have to be read, performance doesn't change a lot, 
 
JMH results (source URL when streaming 100,000 rows 

Bench.fetchSizeBy1000       : 50.274 ± 0.206  ms/op (read with fetch size 1000)
Bench.fetchSizeAll               : 50.593 ± 0.252  ms/op (read all data)
Bench.fetchSizeOneByOne : 51.641 ± 0.299  ms/op (fetch one by one)

No big difference, but avoiding to create a big buffer permit to gain a small 1%, ( and avoid loading all in memory)
"
26,CONJ-322,CONJ,Archimedes Trajano,95376,2017-05-16 19:45:49,Does MariaDB support updating resultsets meaning it just needs to be implemented in the Java connector or does it fundamentally not support it?,1,Does MariaDB support updating resultsets meaning it just needs to be implemented in the Java connector or does it fundamentally not support it?
27,CONJ-322,CONJ,Diego Dupin,95422,2017-05-17 14:10:00,"Protocol doesn't support it. 
But still, MySQL Driver support this functionality by executing lot of underlying queries. 
It's currently in study for next version.

Using UPDATE query will always have better performance. ",2,"Protocol doesn't support it. 
But still, MySQL Driver support this functionality by executing lot of underlying queries. 
It's currently in study for next version.

Using UPDATE query will always have better performance. "
28,CONJ-322,CONJ,Archimedes Trajano,95423,2017-05-17 14:19:35,"Well if underlying it does the queries then for sure it will have better performance.  Perhaps MariaDB should implement it on the server level instead basically when it requests a result set that is {{ResultSet.CONCUR_UPDATABLE}} the rows are ""locked"" on the database side which will then take in a non standard SQL construct of multi-update.  A further optimization can be done by stating...

Updatable Result Sets must contain all the primary key columns of a table AND must not be the result of a {{join}} AND updates on the primary key(s) is not allowed.",3,"Well if underlying it does the queries then for sure it will have better performance.  Perhaps MariaDB should implement it on the server level instead basically when it requests a result set that is {{ResultSet.CONCUR_UPDATABLE}} the rows are ""locked"" on the database side which will then take in a non standard SQL construct of multi-update.  A further optimization can be done by stating...

Updatable Result Sets must contain all the primary key columns of a table AND must not be the result of a {{join}} AND updates on the primary key(s) is not allowed."
29,CONJ-322,CONJ,Diego Dupin,97895,2017-07-23 13:18:44,"available using SNAPSHOT : 

{code:java}
<repositories>
    <repository>
        <id>sonatype-nexus-snapshots</id>
        <name>Sonatype Nexus Snapshots</name>
        <url>https://oss.sonatype.org/content/repositories/snapshots</url>
    </repository>
</repositories>

<dependencies>
    <dependency>
        <groupId>org.mariadb.jdbc</groupId>
        <artifactId>mariadb-java-client</artifactId>
        <version>2.1.0-SNAPSHOT</version>
    </dependency>
</dependencies>
{code}

There is different restrictions : 
- cannot be use if different tables are in resultset
- for update/delete, primary key must be in resultset
- for insert, primary key is not mandatory if generated with AUTO_INCREMENT
",4,"available using SNAPSHOT : 

{code:java}

    
        sonatype-nexus-snapshots
        Sonatype Nexus Snapshots
        URL
    



    
        org.mariadb.jdbc
        mariadb-java-client
        2.1.0-SNAPSHOT
    

{code}

There is different restrictions : 
- cannot be use if different tables are in resultset
- for update/delete, primary key must be in resultset
- for insert, primary key is not mandatory if generated with AUTO_INCREMENT
"
30,CONJ-327,CONJ,Diego Dupin,101074,2017-10-03 08:34:20,reported until MDEV-9804 is implemented,1,reported until MDEV-9804 is implemented
31,CONJ-400,CONJ,Shashi Mall,103735,2017-11-28 16:21:16,"Hi Diego,

Thanks for your work on this issue but I am somewhat confused by the implementation.

Firstly, the query used to determine the cluster status does not, in my opinion, return the desired information.

Using the mysql client, I see the following:

MariaDB [(none)]> select @@wsrep_cluster_status;
ERROR 1193 (HY000): Unknown system variable 'wsrep_cluster_status'

However, the desired information can be obtained (apologies for the lack of formatting):

MariaDB [(none)]> show status like 'wsrep_cluster_status';
+----------------------+---------+
| Variable_name        | Value   |
+----------------------+---------+
| wsrep_cluster_status | Primary |
+----------------------+---------+
1 row in set (0.00 sec)

Please let me know where I am going wrong in my analysis.

Secondly, this issue was going to deal with the wsrep_ready status variable but this is not implemented in the code and was removed from the description.  Can you please explain what happened?",1,"Hi Diego,

Thanks for your work on this issue but I am somewhat confused by the implementation.

Firstly, the query used to determine the cluster status does not, in my opinion, return the desired information.

Using the mysql client, I see the following:

MariaDB [(none)]> select @@wsrep_cluster_status;
ERROR 1193 (HY000): Unknown system variable 'wsrep_cluster_status'

However, the desired information can be obtained (apologies for the lack of formatting):

MariaDB [(none)]> show status like 'wsrep_cluster_status';
+----------------------+---------+
| Variable_name        | Value   |
+----------------------+---------+
| wsrep_cluster_status | Primary |
+----------------------+---------+
1 row in set (0.00 sec)

Please let me know where I am going wrong in my analysis.

Secondly, this issue was going to deal with the wsrep_ready status variable but this is not implemented in the code and was removed from the description.  Can you please explain what happened?"
32,CONJ-400,CONJ,Diego Dupin,104857,2017-12-22 13:52:06,"Implementation in 2.2.0 was never using the variable wsrep_cluster_status, but standard COM_PING.
This is now corrected in release 2.2.1 and 1.7.1 and well tested with galera.
",2,"Implementation in 2.2.0 was never using the variable wsrep_cluster_status, but standard COM_PING.
This is now corrected in release 2.2.1 and 1.7.1 and well tested with galera.
"
33,CONJ-400,CONJ,Shashi Mall,105193,2018-01-03 14:44:32,"Hi Diego,

Thank you for working on this and I can confirm that the isValid() method is working as described.  The implementation certainly ensures that only a connection to a primary component node is considered valid.

However, I feel that this is not a comprehensive solution since the wsrep_ready status must also be examined in order to determine if the node can accept queries.  Indeed, this was stated in earlier descriptions of this issue.

I would be very grateful if you could explain why the decision has been made to not examine the wsrep_ready status.  I am sure there must be a good reason but I cannot work it out.",3,"Hi Diego,

Thank you for working on this and I can confirm that the isValid() method is working as described.  The implementation certainly ensures that only a connection to a primary component node is considered valid.

However, I feel that this is not a comprehensive solution since the wsrep_ready status must also be examined in order to determine if the node can accept queries.  Indeed, this was stated in earlier descriptions of this issue.

I would be very grateful if you could explain why the decision has been made to not examine the wsrep_ready status.  I am sure there must be a good reason but I cannot work it out."
34,CONJ-430,CONJ,Diego Dupin,101073,2017-10-03 08:33:30,already done in 1.5.8,1,already done in 1.5.8
35,CONJ-492,CONJ,Diego Dupin,97963,2017-07-25 13:54:16,"this evolution works only for MariaDB server, since MySQL server don't make distinction (KILL command return ER_QUERY_INTERRUPTED (error 1317) )",1,"this evolution works only for MariaDB server, since MySQL server don't make distinction (KILL command return ER_QUERY_INTERRUPTED (error 1317) )"
36,MCOL-1,MCOL,David Hill,83057,2016-04-27 15:10:27,"This will be addressed with a Documentation change to tell the user not to do any database changes via cpimport, DDl, or DML while the redistributeDB command is running..

So no code changes required..",1,"This will be addressed with a Documentation change to tell the user not to do any database changes via cpimport, DDl, or DML while the redistributeDB command is running..

So no code changes required.."
37,MCOL-1,MCOL,Dipti Joshi,83813,2016-05-31 07:02:12,[~hill]Since redistributeDB is not part of storage engine or server code  using different component for this,2,[~hill]Since redistributeDB is not part of storage engine or server code  using different component for this
38,MCOL-1,MCOL,David Hill,83850,2016-05-31 13:19:30,"FYI - redistributeDB is an Enterprise tool, so it will NOT be included with the MariaDB ColumnStore product since we aren't providing the InfiniDB Enterprise tools.
At least not in the Alpha phase..",3,"FYI - redistributeDB is an Enterprise tool, so it will NOT be included with the MariaDB ColumnStore product since we aren't providing the InfiniDB Enterprise tools.
At least not in the Alpha phase.."
39,MCOL-1,MCOL,David Thompson,88509,2016-11-18 19:47:54,[~David.Hall] - make sure this is covered in the documentation for redistribute.,4,[~David.Hall] - make sure this is covered in the documentation for redistribute.
40,MCOL-1,MCOL,David Thompson,88864,2016-11-29 21:25:13,"Based on our discussion - first assuming that redistribute works in read only mode, we should issue an error with a descriptive action to go enable read only mode. This is consistent with other command behavior. If it does not work in read only mode then we can just document the limitation and we should also understand if actual writes can be done in parallel.",5,"Based on our discussion - first assuming that redistribute works in read only mode, we should issue an error with a descriptive action to go enable read only mode. This is consistent with other command behavior. If it does not work in read only mode then we can just document the limitation and we should also understand if actual writes can be done in parallel."
41,MCOL-1,MCOL,David Hall,89614,2016-12-15 20:56:01,"Regarding read only mode. cpimport does look at the setting and should fail if read only.

All the tests for read only are early in the processing, not at the access level, so redistributeData will work with read only set.",6,"Regarding read only mode. cpimport does look at the setting and should fail if read only.

All the tests for read only are early in the processing, not at the access level, so redistributeData will work with read only set."
42,MCOL-1,MCOL,David Hall,90610,2017-01-14 15:56:20,"Empirical tests prove that redistributeData works fine with suspenDatabaseWrites set on (Read Only mode). Since redistributeData works in an asynchronous model, it is not appropriate for redistributeData to set this by itself, as there's no way for it to reset it when done. The solution is to add code to check and inform the user what needs to be done.",7,"Empirical tests prove that redistributeData works fine with suspenDatabaseWrites set on (Read Only mode). Since redistributeData works in an asynchronous model, it is not appropriate for redistributeData to set this by itself, as there's no way for it to reset it when done. The solution is to add code to check and inform the user what needs to be done."
43,MCOL-1,MCOL,David Hall,90743,2017-01-17 22:46:59,Added a check for read only mode and won't let redistributeData continue without it.,8,Added a check for read only mode and won't let redistributeData continue without it.
44,MCOL-1,MCOL,David Hall,90870,2017-01-20 16:58:06,There's a problem with using isReadOnly() to determine is suspendwrites was called. Use getSystemSuspended() instead.,9,There's a problem with using isReadOnly() to determine is suspendwrites was called. Use getSystemSuspended() instead.
45,MCOL-1,MCOL,Daniel Lee,92433,2017-03-01 17:11:57,"Build tested: GitHub source

[root@localhost mariadb-columnstore-server]# git show
commit 3da188e5c8a2630019ea810fb8c1bd3ece5e058b
Merge: 5d9686c 53c1df7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Feb 10 15:07:31 2017 +0000

    Merge pull request #31 from jbfavre/fix_deb_package_dependency
    
    MCOL-562 Fix Debian package dependencies

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 16cef50caedd9ec7585b04c096996a9441bdf2d5
Author: David Hill <david.hill@mariadb.com>
Date:   Wed Mar 1 10:39:11 2017 -0600

    change the check for prompt back to the previous code

mcsadmin> redistributedata start
redistributedata   Wed Mar  1 16:50:16 2017
redistributeData START 
Source dbroots: 1
Destination dbroots: 1

WriteEngineServer returned status 1: Cleared.
WriteEngineServer returned status 2: Redistribute is started.
mcsadmin> resumeDatabaseWrites
resumedatabasewrites   Wed Mar  1 16:58:11 2017

This command resumes the DDL/DML writes to the MariaDB ColumnStore Database
           Do you want to proceed: (y or n) [n]: y

Resume MariaDB ColumnStore Database Writes Request successfully completed
mcsadmin> redistributedata start
redistributedata   Wed Mar  1 16:58:19 2017
redistributeData START 
Source dbroots: 1
Destination dbroots: 1

The system must be in read only mode for redistribeData to work
You must run suspendDatabaseWrites before running redistributeData
Be sure to run resumeDatabaseWrites when redistributeData status shows complete
",10,"Build tested: GitHub source

[root@localhost mariadb-columnstore-server]# git show
commit 3da188e5c8a2630019ea810fb8c1bd3ece5e058b
Merge: 5d9686c 53c1df7
Author: Andrew Hutchings 
Date:   Fri Feb 10 15:07:31 2017 +0000

    Merge pull request #31 from jbfavre/fix_deb_package_dependency
    
    MCOL-562 Fix Debian package dependencies

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 16cef50caedd9ec7585b04c096996a9441bdf2d5
Author: David Hill 
Date:   Wed Mar 1 10:39:11 2017 -0600

    change the check for prompt back to the previous code

mcsadmin> redistributedata start
redistributedata   Wed Mar  1 16:50:16 2017
redistributeData START 
Source dbroots: 1
Destination dbroots: 1

WriteEngineServer returned status 1: Cleared.
WriteEngineServer returned status 2: Redistribute is started.
mcsadmin> resumeDatabaseWrites
resumedatabasewrites   Wed Mar  1 16:58:11 2017

This command resumes the DDL/DML writes to the MariaDB ColumnStore Database
           Do you want to proceed: (y or n) [n]: y

Resume MariaDB ColumnStore Database Writes Request successfully completed
mcsadmin> redistributedata start
redistributedata   Wed Mar  1 16:58:19 2017
redistributeData START 
Source dbroots: 1
Destination dbroots: 1

The system must be in read only mode for redistribeData to work
You must run suspendDatabaseWrites before running redistributeData
Be sure to run resumeDatabaseWrites when redistributeData status shows complete
"
46,MCOL-1000,MCOL,Daniel Lee,102402,2017-11-02 14:08:34,"Build verified: 1.1.1-1 GitHub source

/root/columnstore/mariadb-columnstore-server
commit 8db18b893413d89dbed55b070b371fb9d918f6a8
Merge: 3df02f4 734b7e3
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Nov 1 09:45:13 2017 -0500

    Merge pull request #75 from mariadb-corporation/MCOL-1000
    
    MCOL-1000 merge MariaDB 10.2.10

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 646f330bf7ccaffa456c97a2bcf2f1a5d7d28ffb
Author: david hill <david.hill@mariadb.com>
Date:   Tue Oct 31 13:41:48 2017 -0500

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.10-MariaDB-log Columnstore 1.1.1-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
",1,"Build verified: 1.1.1-1 GitHub source

/root/columnstore/mariadb-columnstore-server
commit 8db18b893413d89dbed55b070b371fb9d918f6a8
Merge: 3df02f4 734b7e3
Author: David.Hall 
Date:   Wed Nov 1 09:45:13 2017 -0500

    Merge pull request #75 from mariadb-corporation/MCOL-1000
    
    MCOL-1000 merge MariaDB 10.2.10

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 646f330bf7ccaffa456c97a2bcf2f1a5d7d28ffb
Author: david hill 
Date:   Tue Oct 31 13:41:48 2017 -0500

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.10-MariaDB-log Columnstore 1.1.1-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
"
47,MCOL-1015,MCOL,David Hill,102916,2017-11-08 20:31:00,"Markus making some additions, added rpm and binary build instructions and creating develop-1.1 and develop branch to match ours.",1,"Markus making some additions, added rpm and binary build instructions and creating develop-1.1 and develop branch to match ours."
48,MCOL-1015,MCOL,David Hill,108262,2018-03-12 15:37:35,"building successfully on centos7, debian8 and debian9. 

ubuntu16 still had 1 package issue. working on that now..",2,"building successfully on centos7, debian8 and debian9. 

ubuntu16 still had 1 package issue. working on that now.."
49,MCOL-1015,MCOL,David Hill,108266,2018-03-12 15:58:40,"ubuntu16 now building.... currently just building cdc and kafka.. kafka-avro-adapter not being built.

submitted a jira to hopefully get a top level cmake build where it will build all data adapters in the repo..",3,"ubuntu16 now building.... currently just building cdc and kafka.. kafka-avro-adapter not being built.

submitted a jira to hopefully get a top level cmake build where it will build all data adapters in the repo.."
50,MCOL-1015,MCOL,David Hill,109576,2018-04-11 14:06:40,close is completed..,4,close is completed..
51,MCOL-102,MCOL,Daniel Lee,83988,2016-06-03 17:24:22,"Attempted to download packages and got the following:

Home
MariaDB
High availability, scalability and performance beyond MySQL
Error

The website encountered an unexpected error. Please try again later.
",1,"Attempted to download packages and got the following:

Home
MariaDB
High availability, scalability and performance beyond MySQL
Error

The website encountered an unexpected error. Please try again later.
"
52,MCOL-102,MCOL,Dipti Joshi,84025,2016-06-06 02:58:05,[~dleeyh]it is fixed now - please try again.,2,[~dleeyh]it is fixed now - please try again.
53,MCOL-102,MCOL,Daniel Lee,84043,2016-06-06 16:04:37,"Build tested:
mariadb-columnstore-1.0.0-centos6.x86_64.rpm.tar


mscadmin> getsoft
getsoftwareinfo   Mon Jun  6 09:25:16 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Fri 03 Jun 2016 05:12:55 PM CDT
Install Date: Mon 06 Jun 2016 09:19:10 AM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-0.src.rpm
Size        : 13125074                         License: Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.

1) Downloaded all 4 packages, both bin and rpm packages for CentOS 6 and 7, from the portal and tested on a single server installation.
2) Verified packages from the portal are the same as the ones from the shared directory using the diff command.
3) The packages from the shared directory have been tested on 1um-2pm configuration last Friday.


",3,"Build tested:
mariadb-columnstore-1.0.0-centos6.x86_64.rpm.tar


mscadmin> getsoft
getsoftwareinfo   Mon Jun  6 09:25:16 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Fri 03 Jun 2016 05:12:55 PM CDT
Install Date: Mon 06 Jun 2016 09:19:10 AM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-0.src.rpm
Size        : 13125074                         License: Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.

1) Downloaded all 4 packages, both bin and rpm packages for CentOS 6 and 7, from the portal and tested on a single server installation.
2) Verified packages from the portal are the same as the ones from the shared directory using the diff command.
3) The packages from the shared directory have been tested on 1um-2pm configuration last Friday.


"
54,MCOL-1032,MCOL,Andrew Hutchings,103385,2017-11-20 09:37:23,Relatively simple merge this time around.,1,Relatively simple merge this time around.
55,MCOL-1032,MCOL,Daniel Lee,104045,2017-12-04 21:04:36,"
Build verified: 1.0.12-1 GitHub source

[root@localhost ~]# cat mariadb-columnstore-1.0.12-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 25e9d054cd3d05683fade1b974e1730316d256ed
Merge: 89b2ea1 7c52a83
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Nov 21 10:49:11 2017 -0600

    Merge pull request #79 from mariadb-corporation/MCOL-954-1.0
    
    MCOL-954 Init vtable state

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b112e826a2793228f5f3c1312fec5291fc1d8bf5
Merge: 7c2640f b657938
Author: David.Hall <david.hall@mariadb.com>
Date:   Fri Dec 1 16:17:28 2017 -0600

    Merge pull request #338 from mariadb-corporation/MCOL-1068
    
    MCOL-1068 Improve compression_ratio() procedure


[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 6
Server version: 10.1.29-MariaDB Columnstore 1.0.12-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

",2,"
Build verified: 1.0.12-1 GitHub source

[root@localhost ~]# cat mariadb-columnstore-1.0.12-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 25e9d054cd3d05683fade1b974e1730316d256ed
Merge: 89b2ea1 7c52a83
Author: David.Hall 
Date:   Tue Nov 21 10:49:11 2017 -0600

    Merge pull request #79 from mariadb-corporation/MCOL-954-1.0
    
    MCOL-954 Init vtable state

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b112e826a2793228f5f3c1312fec5291fc1d8bf5
Merge: 7c2640f b657938
Author: David.Hall 
Date:   Fri Dec 1 16:17:28 2017 -0600

    Merge pull request #338 from mariadb-corporation/MCOL-1068
    
    MCOL-1068 Improve compression_ratio() procedure


[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 6
Server version: 10.1.29-MariaDB Columnstore 1.0.12-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

"
56,MCOL-104,MCOL,Bharath Bokka,134202,2019-09-16 12:18:44,"Build verified: 1.4.0-1

[root@localhost ~]# cat gitversionInfo.txt
server commit:
67452bc
engine commit:
4d2a159

MariaDB [(none)]> show engines;
+--------------------+---------+-------------------------------------------------------------------------------------------------+--------------+------+------------+
| Engine             | Support | Comment                                                                                         | Transactions | XA   | Savepoints |
+--------------------+---------+-------------------------------------------------------------------------------------------------+--------------+------+------------+
| Columnstore        | YES     | Columnstore storage engine                                                                      | YES          | NO   | NO         |
| MRG_MyISAM         | YES     | Collection of identical MyISAM tables                                                           | NO           | NO   | NO         |
| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables                                       | NO           | NO   | NO         |
| Aria               | YES     | Crash-safe tables with MyISAM heritage. Used for internal temporary tables and privilege tables | NO           | NO   | NO         |
| MyISAM             | YES     | Non-transactional engine with good performance and small data footprint                         | NO           | NO   | NO         |
| SEQUENCE           | YES     | Generated tables filled with sequential values                                                  | YES          | NO   | YES        |
| InnoDB             | DEFAULT | Supports transactions, row-level locking, foreign keys and encryption for tables                | YES          | YES  | YES        |
| PERFORMANCE_SCHEMA | YES     | Performance Schema                                                                              | NO           | NO   | NO         |
| CSV                | YES     | Stores tables as CSV files                                                                      | NO           | NO   | NO         |
+--------------------+---------+-------------------------------------------------------------------------------------------------+--------------+------+------------+
9 rows in set (0.001 sec)

No INFINIDB in the list of engines.",1,"Build verified: 1.4.0-1

[root@localhost ~]# cat gitversionInfo.txt
server commit:
67452bc
engine commit:
4d2a159

MariaDB [(none)]> show engines;
+--------------------+---------+-------------------------------------------------------------------------------------------------+--------------+------+------------+
| Engine             | Support | Comment                                                                                         | Transactions | XA   | Savepoints |
+--------------------+---------+-------------------------------------------------------------------------------------------------+--------------+------+------------+
| Columnstore        | YES     | Columnstore storage engine                                                                      | YES          | NO   | NO         |
| MRG_MyISAM         | YES     | Collection of identical MyISAM tables                                                           | NO           | NO   | NO         |
| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables                                       | NO           | NO   | NO         |
| Aria               | YES     | Crash-safe tables with MyISAM heritage. Used for internal temporary tables and privilege tables | NO           | NO   | NO         |
| MyISAM             | YES     | Non-transactional engine with good performance and small data footprint                         | NO           | NO   | NO         |
| SEQUENCE           | YES     | Generated tables filled with sequential values                                                  | YES          | NO   | YES        |
| InnoDB             | DEFAULT | Supports transactions, row-level locking, foreign keys and encryption for tables                | YES          | YES  | YES        |
| PERFORMANCE_SCHEMA | YES     | Performance Schema                                                                              | NO           | NO   | NO         |
| CSV                | YES     | Stores tables as CSV files                                                                      | NO           | NO   | NO         |
+--------------------+---------+-------------------------------------------------------------------------------------------------+--------------+------+------------+
9 rows in set (0.001 sec)

No INFINIDB in the list of engines."
57,MCOL-105,MCOL,David Hill,91266,2017-01-31 21:33:47,"Rasmus had centos 7 working.

I just got Centos 6 working, a bit of doing since the install packages was different than centos 7 setup.
Plus learning the system and how it all works..",1,"Rasmus had centos 7 working.

I just got Centos 6 working, a bit of doing since the install packages was different than centos 7 setup.
Plus learning the system and how it all works.."
58,MCOL-105,MCOL,David Hill,91331,2017-02-01 20:43:37,Ubuntu 16.04 buildbot working.... Woo Hoo,2,Ubuntu 16.04 buildbot working.... Woo Hoo
59,MCOL-105,MCOL,David Hill,91340,2017-02-01 22:52:20,"Debian 8 buildbot working...

 things to continue to work, master script setup to build rpms. Just need to learn to do both rpm an debian packages... Then it will be time to work for a clean build for SUSE then follow it with buildbot worker",3,"Debian 8 buildbot working...

 things to continue to work, master script setup to build rpms. Just need to learn to do both rpm an debian packages... Then it will be time to work for a clean build for SUSE then follow it with buildbot worker"
60,MCOL-105,MCOL,David Hill,91495,2017-02-06 15:47:44,"Got MCS to build and run on SuSE 12....

Next will be to create the buildbot worker and get it into the build process

And to run the full regression test on it...

",4,"Got MCS to build and run on SuSE 12....

Next will be to create the buildbot worker and get it into the build process

And to run the full regression test on it...

"
61,MCOL-105,MCOL,David Hill,91563,2017-02-07 15:27:39,"1. now generating both binary and rpm packages
2. completed nightly regression test, passes just like centos 7.
3. have a buildbot worker getting good makes.
4. Its build against the 1.0.7 with no additional code changes. So we could just release it to the 1.0.7 repo.

I plan to do some multi-node installs with the rpm and binary packages today... And daniel is performing qa testing on the packages.

So we could plan to have it out by the end of this week, if all goes well.",5,"1. now generating both binary and rpm packages
2. completed nightly regression test, passes just like centos 7.
3. have a buildbot worker getting good makes.
4. Its build against the 1.0.7 with no additional code changes. So we could just release it to the 1.0.7 repo.

I plan to do some multi-node installs with the rpm and binary packages today... And daniel is performing qa testing on the packages.

So we could plan to have it out by the end of this week, if all goes well."
62,MCOL-105,MCOL,David Hill,91710,2017-02-09 22:47:29,"now genertaing both rpm and debian packages.... So all is complete with the JIRA.
Another JIRA has the regression testing addition..",6,"now genertaing both rpm and debian packages.... So all is complete with the JIRA.
Another JIRA has the regression testing addition.."
63,MCOL-105,MCOL,David Hill,91711,2017-02-09 22:47:52,complete..,7,complete..
64,MCOL-1052,MCOL,Roman,110506,2018-05-03 19:49:21,Please review this.,1,Please review this.
65,MCOL-1052,MCOL,Andrew Hutchings,110574,2018-05-05 05:32:18,Great work! I think at a later date we should maybe look at breaking out the larger code segments from places like ha_calpont_execplan.cpp into separate files. But that is more of a cleanup task. Very happy to see this!,2,Great work! I think at a later date we should maybe look at breaking out the larger code segments from places like ha_calpont_execplan.cpp into separate files. But that is more of a cleanup task. Very happy to see this!
66,MCOL-1052,MCOL,Andrew Hutchings,110575,2018-05-05 05:37:50,"For QA: This basically pushes down GROUP BY queries into ColumnStore when vtable mode is disabled. Whilst this is only slightly useful now (much better performance when vtable is off) it will be very useful when we get to a point where we can get rid of vtable. There is a new set of regression tests called ""test022"" which covers this.",3,"For QA: This basically pushes down GROUP BY queries into ColumnStore when vtable mode is disabled. Whilst this is only slightly useful now (much better performance when vtable is off) it will be very useful when we get to a point where we can get rid of vtable. There is a new set of regression tests called ""test022"" which covers this."
67,MCOL-1052,MCOL,Daniel Lee,111770,2018-05-30 21:26:28,"/root/columnstore/mariadb-columnstore-server
commit 4334641df0df040e0f53332c11dc8e29dc34b8b7
Merge: 960853c cd5e845
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Apr 9 13:23:44 2018 -0500

    Merge pull request #107 from mariadb-corporation/dev-merge-up-20180409
    
    Dev merge up 20180409

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit e79daac8a9b1c8047aeabc3d73ae137cc151f84e
Merge: 8790177 8a3c5e0
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue May 15 15:34:19 2018 +0100

    Merge pull request #472 from drrtuy/MCOL-1073
    
    MCOL-1073 Removed DecomSrv

ssd=31.250
hdd=88.530
usb=193.684

1.1.3-1 10g dbt3

select l_shipmode from lineitem where l_shipmode='AIR' group by l_shipmode,l_receiptdate;
vtablemode=1
disk run: 2.32 sec
cache run: 1.43 sec
vtablemode=0
disk run: 7.17 sec
cache run: 6.85 sec


1.2.0-1 10g dbt3

select l_shipmode from lineitem where l_shipmode='AIR' group by l_shipmode,l_receiptdate;
vtablemode=1
disk run: 2.10 sec
cache run: 1.48 sec
vtablemode=0
disk run: 2.23 sec
cache run: 1.18 sec

Attempted running regression test022 and it is not yet working.  Possibly code is not yet merged.  Will try regression test later.
",4,"/root/columnstore/mariadb-columnstore-server
commit 4334641df0df040e0f53332c11dc8e29dc34b8b7
Merge: 960853c cd5e845
Author: David.Hall 
Date:   Mon Apr 9 13:23:44 2018 -0500

    Merge pull request #107 from mariadb-corporation/dev-merge-up-20180409
    
    Dev merge up 20180409

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit e79daac8a9b1c8047aeabc3d73ae137cc151f84e
Merge: 8790177 8a3c5e0
Author: Andrew Hutchings 
Date:   Tue May 15 15:34:19 2018 +0100

    Merge pull request #472 from drrtuy/MCOL-1073
    
    MCOL-1073 Removed DecomSrv

ssd=31.250
hdd=88.530
usb=193.684

1.1.3-1 10g dbt3

select l_shipmode from lineitem where l_shipmode='AIR' group by l_shipmode,l_receiptdate;
vtablemode=1
disk run: 2.32 sec
cache run: 1.43 sec
vtablemode=0
disk run: 7.17 sec
cache run: 6.85 sec


1.2.0-1 10g dbt3

select l_shipmode from lineitem where l_shipmode='AIR' group by l_shipmode,l_receiptdate;
vtablemode=1
disk run: 2.10 sec
cache run: 1.48 sec
vtablemode=0
disk run: 2.23 sec
cache run: 1.18 sec

Attempted running regression test022 and it is not yet working.  Possibly code is not yet merged.  Will try regression test later.
"
68,MCOL-1052,MCOL,Daniel Lee,111855,2018-05-31 19:46:10,"Build verified: 1.2.0-1 source


/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 29346339adf385d21cb5f9b1cae4c830f8cddb12
Merge: e79daac 1a1f3ea
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu May 31 13:33:49 2018 +0100

    Merge pull request #486 from drrtuy/MCOL-1449
    
    MCOL 1449 Fix the regression in sorting orientation.

There is an issue with regression test022 due to unrelated issue.  Close this ticket since the fix is working.  Will check for regression in future builds.",5,"Build verified: 1.2.0-1 source


/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 29346339adf385d21cb5f9b1cae4c830f8cddb12
Merge: e79daac 1a1f3ea
Author: Andrew Hutchings 
Date:   Thu May 31 13:33:49 2018 +0100

    Merge pull request #486 from drrtuy/MCOL-1449
    
    MCOL 1449 Fix the regression in sorting orientation.

There is an issue with regression test022 due to unrelated issue.  Close this ticket since the fix is working.  Will check for regression in future builds."
69,MCOL-1058,MCOL,David Hill,105766,2018-01-16 21:34:36,"added  code to check if these mariadb packages are installed based on OS and report error if one of them is installed.

CENTOS6/7 = ""mariadb-libs""
SUSE12 = ""mariadb"" , ""libmariadb18""
UBUNTU16 = ""mariadb-server"" ""libmariadb18""
DEBIAN 8/9 = ""mariadb-server"" ""libmariadb18""

-------------------------------------------

results when not installed

./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System test tool ***

** Validate local OS is supported

Local Node OS System Name : Debian GNU/Linux 9 (stretch)

** Run MariaDB ColumnStore Dependent Package Check

Local Node - Passed, all dependency packages are installed
Local Node - Passed, all packages that should not be installed aren't installed


*** Finished Validation of the Cluster, all Test Passed ***

RESULTS WHEN ONE IS INSTALLED

./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System test tool ***

** Validate local OS is supported

Local Node OS System Name : Debian GNU/Linux 9 (stretch)

** Run MariaDB ColumnStore Dependent Package Check

Local Node - Passed, all dependency packages are installed
Failed, Local Node package mariadb-server is installed, please un-install

Failure occurred, do you want to continue? (y,n) > y


*** Finished Validation of the Cluster, Failures occurred. Check for Error/Failed test results ***",1,"added  code to check if these mariadb packages are installed based on OS and report error if one of them is installed.

CENTOS6/7 = ""mariadb-libs""
SUSE12 = ""mariadb"" , ""libmariadb18""
UBUNTU16 = ""mariadb-server"" ""libmariadb18""
DEBIAN 8/9 = ""mariadb-server"" ""libmariadb18""

-------------------------------------------

results when not installed

./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System test tool ***

** Validate local OS is supported

Local Node OS System Name : Debian GNU/Linux 9 (stretch)

** Run MariaDB ColumnStore Dependent Package Check

Local Node - Passed, all dependency packages are installed
Local Node - Passed, all packages that should not be installed aren't installed


*** Finished Validation of the Cluster, all Test Passed ***

RESULTS WHEN ONE IS INSTALLED

./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System test tool ***

** Validate local OS is supported

Local Node OS System Name : Debian GNU/Linux 9 (stretch)

** Run MariaDB ColumnStore Dependent Package Check

Local Node - Passed, all dependency packages are installed
Failed, Local Node package mariadb-server is installed, please un-install

Failure occurred, do you want to continue? (y,n) > y


*** Finished Validation of the Cluster, Failures occurred. Check for Error/Failed test results ***"
70,MCOL-1058,MCOL,David Hill,105786,2018-01-17 17:20:36,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/369,2,URL
71,MCOL-1058,MCOL,Ben Thompson,105789,2018-01-17 17:46:21,Reviewed / Merged,3,Reviewed / Merged
72,MCOL-1058,MCOL,David Hill,105793,2018-01-17 19:04:43,"for the MariaDB password check, I put in code that would do the following and only report an issue if the error code of password miss was reported, Error 1045.

On local and remote servers...
1. start mysqld
2. try to log into console and check for error return code of 1045 (password required)
    If this error is reported, there is a password set and no /root/.my.cnf file.
    script will then report this as an error..

",4,"for the MariaDB password check, I put in code that would do the following and only report an issue if the error code of password miss was reported, Error 1045.

On local and remote servers...
1. start mysqld
2. try to log into console and check for error return code of 1045 (password required)
    If this error is reported, there is a password set and no /root/.my.cnf file.
    script will then report this as an error..

"
73,MCOL-1058,MCOL,Daniel Lee,106090,2018-01-23 17:25:41,"Build verified: 1.1.3-1 Github source
/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Fri Jan 19 12:39:05 2018 -0600
Merge pull request #84 from mariadb-corporation/MCOL-1159
MCOL-1159 Merge mariadb-10.2.12
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 6c930a03070b3815995d1282233b06100a09bbb9
Merge: 63224a0 c1035da
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Mon Jan 22 16:26:34 2018 -0600
Merge pull request #374 from mariadb-corporation/MCOL-1060
MCOL-1060 - test tool change #2
",5,"Build verified: 1.1.3-1 Github source
/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 
Date: Fri Jan 19 12:39:05 2018 -0600
Merge pull request #84 from mariadb-corporation/MCOL-1159
MCOL-1159 Merge mariadb-10.2.12
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 6c930a03070b3815995d1282233b06100a09bbb9
Merge: 63224a0 c1035da
Author: benthompson15 
Date: Mon Jan 22 16:26:34 2018 -0600
Merge pull request #374 from mariadb-corporation/MCOL-1060
MCOL-1060 - test tool change #2
"
74,MCOL-1060,MCOL,David Hill,105799,2018-01-17 21:42:05,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/370,1,URL
75,MCOL-1060,MCOL,Ben Thompson,105861,2018-01-18 22:49:24,Reviewed / Merged,2,Reviewed / Merged
76,MCOL-1060,MCOL,David Hill,106031,2018-01-22 20:37:00,"https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/374

there were 2 places that needed to be changed for test tool",3,"URL

there were 2 places that needed to be changed for test tool"
77,MCOL-1060,MCOL,Ben Thompson,106037,2018-01-22 22:26:45,"Reviewed / Merged

",4,"Reviewed / Merged

"
78,MCOL-1060,MCOL,Daniel Lee,106086,2018-01-23 17:12:57,"Build verified: 1.1.3-1 Github source

/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Fri Jan 19 12:39:05 2018 -0600

    Merge pull request #84 from mariadb-corporation/MCOL-1159
    
    MCOL-1159 Merge mariadb-10.2.12

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 6c930a03070b3815995d1282233b06100a09bbb9
Merge: 63224a0 c1035da
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Jan 22 16:26:34 2018 -0600

    Merge pull request #374 from mariadb-corporation/MCOL-1060
    
    MCOL-1060 - test tool change #2

",5,"Build verified: 1.1.3-1 Github source

/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 
Date:   Fri Jan 19 12:39:05 2018 -0600

    Merge pull request #84 from mariadb-corporation/MCOL-1159
    
    MCOL-1159 Merge mariadb-10.2.12

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 6c930a03070b3815995d1282233b06100a09bbb9
Merge: 63224a0 c1035da
Author: benthompson15 
Date:   Mon Jan 22 16:26:34 2018 -0600

    Merge pull request #374 from mariadb-corporation/MCOL-1060
    
    MCOL-1060 - test tool change #2

"
79,MCOL-1069,MCOL,Andrew Hutchings,104212,2017-12-07 14:51:48,Merge appears to be pretty simple this time around.,1,Merge appears to be pretty simple this time around.
80,MCOL-1069,MCOL,Daniel Lee,105798,2018-01-17 21:14:34,"Build verified: 1.1.3-1 Github source

/root/columnstore/mariadb-columnstore-server
commit 21ec50194ea3103c922489aa2d9d1d8b61f66343
Merge: 0b3b260 a4f993e
Author: David.Hall <david.hall@mariadb.com>
Date:   Fri Dec 8 12:41:57 2017 -0600

    Merge pull request #82 from mariadb-corporation/MCOL-1069
    
    MCOL-1069 Merge MariaDB 10.2.11

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 04f78bb4e357cf6280171e54d57dbe8e9a325f8b
Merge: 6d14d60 7bc2e24
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Wed Jan 17 11:45:42 2018 -0600

    Merge pull request #368 from mariadb-corporation/MCOL-1066
    
    MCOL-1066 - changed directory path for non-root install local disk

root@jessie:~# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.11-MariaDB-log Columnstore 1.1.3-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.




",2,"Build verified: 1.1.3-1 Github source

/root/columnstore/mariadb-columnstore-server
commit 21ec50194ea3103c922489aa2d9d1d8b61f66343
Merge: 0b3b260 a4f993e
Author: David.Hall 
Date:   Fri Dec 8 12:41:57 2017 -0600

    Merge pull request #82 from mariadb-corporation/MCOL-1069
    
    MCOL-1069 Merge MariaDB 10.2.11

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 04f78bb4e357cf6280171e54d57dbe8e9a325f8b
Merge: 6d14d60 7bc2e24
Author: benthompson15 
Date:   Wed Jan 17 11:45:42 2018 -0600

    Merge pull request #368 from mariadb-corporation/MCOL-1066
    
    MCOL-1066 - changed directory path for non-root install local disk

root@jessie:~# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.11-MariaDB-log Columnstore 1.1.3-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.




"
81,MCOL-1073,MCOL,Roman,110962,2018-05-14 18:52:48,Please review,1,Please review
82,MCOL-1073,MCOL,Daniel Lee,111772,2018-05-30 21:45:41,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 4334641df0df040e0f53332c11dc8e29dc34b8b7
Merge: 960853c cd5e845
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Apr 9 13:23:44 2018 -0500

    Merge pull request #107 from mariadb-corporation/dev-merge-up-20180409
    
    Dev merge up 20180409

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit e79daac8a9b1c8047aeabc3d73ae137cc151f84e
Merge: 8790177 8a3c5e0
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue May 15 15:34:19 2018 +0100

    Merge pull request #472 from drrtuy/MCOL-1073
    
    MCOL-1073 Removed DecomSrv


Checked release 1.1.3-1 and found the the DecomSvr process running and showing in OAM.  Verified that it has been removed in 1.2.0-1.",2,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 4334641df0df040e0f53332c11dc8e29dc34b8b7
Merge: 960853c cd5e845
Author: David.Hall 
Date:   Mon Apr 9 13:23:44 2018 -0500

    Merge pull request #107 from mariadb-corporation/dev-merge-up-20180409
    
    Dev merge up 20180409

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit e79daac8a9b1c8047aeabc3d73ae137cc151f84e
Merge: 8790177 8a3c5e0
Author: Andrew Hutchings 
Date:   Tue May 15 15:34:19 2018 +0100

    Merge pull request #472 from drrtuy/MCOL-1073
    
    MCOL-1073 Removed DecomSrv


Checked release 1.1.3-1 and found the the DecomSvr process running and showing in OAM.  Verified that it has been removed in 1.2.0-1."
83,MCOL-1075,MCOL,Andrew Hutchings,104036,2017-12-04 17:21:58,"Thank you for your input. We will take these edits on-board.

As far as batch size goes, 10,000 is more optimal for cpimport whereas at least 100,000 is more optimal for the API (there is a trade-off between RAM and network rtt/compression time). This is due to the different ways they handle network communications. In addition we want to have a way for the API to not rely on Columnstore.xml in a future release. This means that you would need a way to set the batch size anyway. We will make sure this is documented.",1,"Thank you for your input. We will take these edits on-board.

As far as batch size goes, 10,000 is more optimal for cpimport whereas at least 100,000 is more optimal for the API (there is a trade-off between RAM and network rtt/compression time). This is due to the different ways they handle network communications. In addition we want to have a way for the API to not rely on Columnstore.xml in a future release. This means that you would need a way to set the batch size anyway. We will make sure this is documented."
84,MCOL-1075,MCOL,Andrew Hutchings,104119,2017-12-05 21:11:37,Two minor document fixes in pull request,2,Two minor document fixes in pull request
85,MCOL-1075,MCOL,David Thompson,104124,2017-12-06 00:31:29,verified the changes are in the generated doc.,3,verified the changes are in the generated doc.
86,MCOL-1085,MCOL,Andrew Hutchings,105091,2017-12-31 15:56:44,We also need -fno-omit-frame-pointer,1,We also need -fno-omit-frame-pointer
87,MCOL-1085,MCOL,Andrew Hutchings,105184,2018-01-03 09:28:52,"Patch for develop-1.0 to be merged up.

It will create trace files automatically when a ColumnStore process crashes and put them in /var/log/mariadb/columnstore/trace. The file will contain the date/time, signal number and the stack at time of crash.

The file names are in ""ProcessName.PID.log"" format.

For QA: try ""kill -11"" on a ColumnStore daemon process and look for the new log files.",2,"Patch for develop-1.0 to be merged up.

It will create trace files automatically when a ColumnStore process crashes and put them in /var/log/mariadb/columnstore/trace. The file will contain the date/time, signal number and the stack at time of crash.

The file names are in ""ProcessName.PID.log"" format.

For QA: try ""kill -11"" on a ColumnStore daemon process and look for the new log files."
88,MCOL-1085,MCOL,Daniel Lee,106025,2018-01-22 19:16:58,"
Build verified: 1.1.3-1 Github source

/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Fri Jan 19 12:39:05 2018 -0600

    Merge pull request #84 from mariadb-corporation/MCOL-1159
    
    MCOL-1159 Merge mariadb-10.2.12

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c74d5de21d6571c0b0e9a12dacaf77856d332e63
Merge: 201813d 63adbd0
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Jan 22 09:42:34 2018 -0600

    Merge pull request #375 from mariadb-corporation/dev-1.1-build-fix
    
    Fix missing compiler flag from 1.0 -> 1.1 merge


[root@localhost trace]# cat PrimProc.9715.log 
Date/time: 2018-01-22 19:15:32
Signal: 11

[0x7ff284c63200]
/lib64/libpthread.so.0(+0xf100)[0x7ff27f040100]
/lib64/libpthread.so.0(pthread_cond_wait+0xc5)[0x7ff27f03c6d5]
/usr/local/mariadb/columnstore/lib/libthreadpool.so.1(_ZN10threadpool10ThreadPool4waitEv+0x13f)[0x7ff281110a2f]
[0x7ff284c5426d]
[0x7ff284bfd6f5]
/lib64/libc.so.6(__libc_start_main+0xf5)[0x7ff27e061b15]
[0x7ff284c1727d]

",3,"
Build verified: 1.1.3-1 Github source

/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 
Date:   Fri Jan 19 12:39:05 2018 -0600

    Merge pull request #84 from mariadb-corporation/MCOL-1159
    
    MCOL-1159 Merge mariadb-10.2.12

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c74d5de21d6571c0b0e9a12dacaf77856d332e63
Merge: 201813d 63adbd0
Author: benthompson15 
Date:   Mon Jan 22 09:42:34 2018 -0600

    Merge pull request #375 from mariadb-corporation/dev-1.1-build-fix
    
    Fix missing compiler flag from 1.0 -> 1.1 merge


[root@localhost trace]# cat PrimProc.9715.log 
Date/time: 2018-01-22 19:15:32
Signal: 11

[0x7ff284c63200]
/lib64/libpthread.so.0(+0xf100)[0x7ff27f040100]
/lib64/libpthread.so.0(pthread_cond_wait+0xc5)[0x7ff27f03c6d5]
/usr/local/mariadb/columnstore/lib/libthreadpool.so.1(_ZN10threadpool10ThreadPool4waitEv+0x13f)[0x7ff281110a2f]
[0x7ff284c5426d]
[0x7ff284bfd6f5]
/lib64/libc.so.6(__libc_start_main+0xf5)[0x7ff27e061b15]
[0x7ff284c1727d]

"
89,MCOL-1085,MCOL,Daniel Lee,106028,2018-01-22 20:07:29,"1.0.13-1:

/root/columnstore/mariadb-columnstore-server
commit 8877490c9d8ea2155b2788d6afadb50f0dd9c260
Merge: 7ec285d e8ea173
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Jan 10 22:28:23 2018 +0000

    Merge pull request #83 from mariadb-corporation/MCOL-258
    
    MCOL-258 Add using statements to eliminate warnings

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit cc2bbe88bc525e585cc523d7c680868d395ff328
Merge: 9b35056 403e0ef
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Mon Jan 22 16:39:21 2018 +0000

    Merge pull request #376 from mariadb-corporation/MCOL-1114
    
    MCOL-1114: Change cmake minimum versions.


",4,"1.0.13-1:

/root/columnstore/mariadb-columnstore-server
commit 8877490c9d8ea2155b2788d6afadb50f0dd9c260
Merge: 7ec285d e8ea173
Author: Andrew Hutchings 
Date:   Wed Jan 10 22:28:23 2018 +0000

    Merge pull request #83 from mariadb-corporation/MCOL-258
    
    MCOL-258 Add using statements to eliminate warnings

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit cc2bbe88bc525e585cc523d7c680868d395ff328
Merge: 9b35056 403e0ef
Author: Andrew Hutchings 
Date:   Mon Jan 22 16:39:21 2018 +0000

    Merge pull request #376 from mariadb-corporation/MCOL-1114
    
    MCOL-1114: Change cmake minimum versions.


"
90,MCOL-1094,MCOL,Jens Röwekamp,119407,2018-11-16 21:55:57,"4 functions shall be implemented to view and clear table locks in ColumnStoreDriver
- listTableLocks() , returns all current table locks in an array of \[lockId, sessionId, database, table\]
- isTableLocked(std::string database, std::string table), returns true if the table is locked by another process and false if not
- clearTableLock(uint16_t id), clears a table lock with the specific lock Id and rolls back the killed transaction
- clearTableLock(std::string database, std::string table), clears a table lock for the specified database table combination and rolls back the killed transaction",1,"4 functions shall be implemented to view and clear table locks in ColumnStoreDriver
- listTableLocks() , returns all current table locks in an array of \[lockId, sessionId, database, table\]
- isTableLocked(std::string database, std::string table), returns true if the table is locked by another process and false if not
- clearTableLock(uint16_t id), clears a table lock with the specific lock Id and rolls back the killed transaction
- clearTableLock(std::string database, std::string table), clears a table lock for the specified database table combination and rolls back the killed transaction"
91,MCOL-1094,MCOL,Jens Röwekamp,119771,2018-11-25 10:27:27,"Introducing 6 new functions to ColumnStoreDriver:

- std::vector listTableLocks();
- bool isTableLocked(const std::string & db, const std::string & table, TableLockInfo& rtn);
- bool isTableLocked(const std::string & db, const std::string & table);
- void clearTableLock(uint64_t lockId);
- void clearTableLock(const std::string & db, const std::string & table);
- void clearTableLock(TableLockInfo tbi);

Introducing a new datatype mcsapi::TableLockInfo to store table lock information.

mcsapi's introduced functions and data types ported to pymcsapi and javamcsapi

Introducing C++, Java, and Python tests for listTableLocks(), isTableLocked(), and clearTableLock()

Added new functions to documentation plus general documentation updates for javamcsapi and pymcsapi

*Known limitations:*
- mcsapi's isTableLocked() function is affected by MCOL-1218 and only detects locks of tables that were existent when the ColumnStoreDriver was initialized
- javamcsapi and pymcsapi don't yet have methods to access the dbroot array returned by TableLockInfo's getDbrootList()
- javamcsapi and pymcsapi don't yet have methods to access the the creation time returned by TableLockInfo's getCreationTime()
- pymcsapi's isTableLocked() function only returns bool but not also the additional TableLockInfo object mcsapi returns

Limitations addressed in MCOL-1961

*For QA*
- run regression test suite on CentOS 7, Windows 10, and one Debian/Ubuntu
- verify that the documentation is correct",2,"Introducing 6 new functions to ColumnStoreDriver:

- std::vector listTableLocks();
- bool isTableLocked(const std::string & db, const std::string & table, TableLockInfo& rtn);
- bool isTableLocked(const std::string & db, const std::string & table);
- void clearTableLock(uint64_t lockId);
- void clearTableLock(const std::string & db, const std::string & table);
- void clearTableLock(TableLockInfo tbi);

Introducing a new datatype mcsapi::TableLockInfo to store table lock information.

mcsapi's introduced functions and data types ported to pymcsapi and javamcsapi

Introducing C++, Java, and Python tests for listTableLocks(), isTableLocked(), and clearTableLock()

Added new functions to documentation plus general documentation updates for javamcsapi and pymcsapi

*Known limitations:*
- mcsapi's isTableLocked() function is affected by MCOL-1218 and only detects locks of tables that were existent when the ColumnStoreDriver was initialized
- javamcsapi and pymcsapi don't yet have methods to access the dbroot array returned by TableLockInfo's getDbrootList()
- javamcsapi and pymcsapi don't yet have methods to access the the creation time returned by TableLockInfo's getCreationTime()
- pymcsapi's isTableLocked() function only returns bool but not also the additional TableLockInfo object mcsapi returns

Limitations addressed in MCOL-1961

*For QA*
- run regression test suite on CentOS 7, Windows 10, and one Debian/Ubuntu
- verify that the documentation is correct"
92,MCOL-1099,MCOL,David Thompson,104407,2017-12-12 22:44:33,"updated doc source, will be available in next 1.1.3 release.",1,"updated doc source, will be available in next 1.1.3 release."
93,MCOL-11,MCOL,David Hill,83216,2016-05-04 20:38:21,using mcsadmin,1,using mcsadmin
94,MCOL-11,MCOL,David Hill,83304,2016-05-09 19:30:50,"removing alias cc, use mcsadmin instead",2,"removing alias cc, use mcsadmin instead"
95,MCOL-11,MCOL,David Hill,83307,2016-05-09 20:03:25,required changes to the nightly regression test suite,3,required changes to the nightly regression test suite
96,MCOL-11,MCOL,David Hill,83310,2016-05-09 20:13:18,"changes made, need QA review and tested",4,"changes made, need QA review and tested"
97,MCOL-11,MCOL,Daniel Lee,83508,2016-05-18 16:30:38,"Build verified:

InfiniDB> getcalpontsoft
getcalpontsoftwareinfo   Wed May 18 11:21:29 2016

Name        : infinidb-platform            Relocations: (not relocatable)
Version     : 5.0                               Vendor: MariaDB, Inc.
Release     : 0                             Build Date: Sun 15 May 2016 07:24:29 PM CDT
Install Date: Mon 16 May 2016 05:21:51 PM CDT      Build Host: srvbuilder

Verified that alias cc has been changed to mcsadmin (also has a shorter form ma), as well as calpontConsole binary has been renamed as mcsadmin.

MariaDB [(none)]> quit
Bye
[root@columnStore ~]# alias
alias core='cd /var/log/Calpont/corefiles'
alias cp='cp -i'
alias dbrm='cd /usr/local/Calpont/data1/systemFiles/dbrm'
alias home='cd /usr/local/Calpont/'
alias l.='ls -d .* --color=auto'
alias ll='ls -l --color=auto'
alias log='cd /var/log/Calpont/'
alias ls='ls --color=auto'
alias ma='/usr/local/Calpont/bin/mcsadmin'
alias mcsadmin='/usr/local/Calpont/bin/mcsadmin'
alias mcsmysql='/usr/local/Calpont/mysql/bin/mysql --defaults-file=/usr/local/Calpont/mysql/my.cnf -u root'
alias module='cat /usr/local/Calpont/local/module'
alias mv='mv -i'
alias rm='rm -i'
alias tdebug='tail -f /var/log/Calpont/debug.log'
alias tinfo='tail -f /var/log/Calpont/info.log'
alias tmsg='tail -f /var/log/messages'
alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'


[root@columnStore ~]# ls -al /usr/local/Calpont/bin/mcs*
-rwxr-xr-x. 1 root root 630486 May 15 19:20 /usr/local/Calpont/bin/mcsadmin
",5,"Build verified:

InfiniDB> getcalpontsoft
getcalpontsoftwareinfo   Wed May 18 11:21:29 2016

Name        : infinidb-platform            Relocations: (not relocatable)
Version     : 5.0                               Vendor: MariaDB, Inc.
Release     : 0                             Build Date: Sun 15 May 2016 07:24:29 PM CDT
Install Date: Mon 16 May 2016 05:21:51 PM CDT      Build Host: srvbuilder

Verified that alias cc has been changed to mcsadmin (also has a shorter form ma), as well as calpontConsole binary has been renamed as mcsadmin.

MariaDB [(none)]> quit
Bye
[root@columnStore ~]# alias
alias core='cd /var/log/Calpont/corefiles'
alias cp='cp -i'
alias dbrm='cd /usr/local/Calpont/data1/systemFiles/dbrm'
alias home='cd /usr/local/Calpont/'
alias l.='ls -d .* --color=auto'
alias ll='ls -l --color=auto'
alias log='cd /var/log/Calpont/'
alias ls='ls --color=auto'
alias ma='/usr/local/Calpont/bin/mcsadmin'
alias mcsadmin='/usr/local/Calpont/bin/mcsadmin'
alias mcsmysql='/usr/local/Calpont/mysql/bin/mysql --defaults-file=/usr/local/Calpont/mysql/my.cnf -u root'
alias module='cat /usr/local/Calpont/local/module'
alias mv='mv -i'
alias rm='rm -i'
alias tdebug='tail -f /var/log/Calpont/debug.log'
alias tinfo='tail -f /var/log/Calpont/info.log'
alias tmsg='tail -f /var/log/messages'
alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'


[root@columnStore ~]# ls -al /usr/local/Calpont/bin/mcs*
-rwxr-xr-x. 1 root root 630486 May 15 19:20 /usr/local/Calpont/bin/mcsadmin
"
98,MCOL-1101,MCOL,Roman,123526,2019-02-18 14:56:14,Please review.,1,Please review.
99,MCOL-1101,MCOL,Daniel Lee,134465,2019-09-19 19:05:44,"Build tested: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
975463c

Verified that:
16 infinidb_ variables have been renamed to columnstore_.
infinidb_vtable_mode has been removed
4 new ones added:
columnstore_derived_handler
columnstore_processing_handlers_fallback
columnstore_select_handler
columnstore_group_by_handler

MariaDB [(none)]> show variables like ""%columnstore%"";
+------------------------------------------------+--------+
| Variable_name                                  | Value  |
+------------------------------------------------+--------+
| columnstore_compression_type                   | SNAPPY |
| columnstore_decimal_scale                      | 8      |
| columnstore_derived_handler                    | ON     |
| columnstore_diskjoin_bucketsize                | 100    |
| columnstore_diskjoin_largesidelimit            | 0      |
| columnstore_diskjoin_smallsidelimit            | 0      |
| columnstore_double_for_decimal_math            | OFF    |
| columnstore_group_by_handler                   | ON     |
| columnstore_import_for_batchinsert_delimiter   | 7      |
| columnstore_import_for_batchinsert_enclosed_by | 17     |
| columnstore_local_query                        | 0      |
| columnstore_ordered_only                       | OFF    |
| columnstore_processing_handlers_fallback       | OFF    |
| columnstore_select_handler                     | ON     |
| columnstore_string_scan_threshold              | 10     |
| columnstore_stringtable_threshold              | 20     |
| columnstore_um_mem_limit                       | 0      |
| columnstore_use_decimal_scale                  | OFF    |
| columnstore_use_import_for_batchinsert         | ON     |
| columnstore_varbin_always_hex                  | OFF    |
+------------------------------------------------+--------+
20 rows in set (0.004 sec)
",2,"Build tested: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
975463c

Verified that:
16 infinidb_ variables have been renamed to columnstore_.
infinidb_vtable_mode has been removed
4 new ones added:
columnstore_derived_handler
columnstore_processing_handlers_fallback
columnstore_select_handler
columnstore_group_by_handler

MariaDB [(none)]> show variables like ""%columnstore%"";
+------------------------------------------------+--------+
| Variable_name                                  | Value  |
+------------------------------------------------+--------+
| columnstore_compression_type                   | SNAPPY |
| columnstore_decimal_scale                      | 8      |
| columnstore_derived_handler                    | ON     |
| columnstore_diskjoin_bucketsize                | 100    |
| columnstore_diskjoin_largesidelimit            | 0      |
| columnstore_diskjoin_smallsidelimit            | 0      |
| columnstore_double_for_decimal_math            | OFF    |
| columnstore_group_by_handler                   | ON     |
| columnstore_import_for_batchinsert_delimiter   | 7      |
| columnstore_import_for_batchinsert_enclosed_by | 17     |
| columnstore_local_query                        | 0      |
| columnstore_ordered_only                       | OFF    |
| columnstore_processing_handlers_fallback       | OFF    |
| columnstore_select_handler                     | ON     |
| columnstore_string_scan_threshold              | 10     |
| columnstore_stringtable_threshold              | 20     |
| columnstore_um_mem_limit                       | 0      |
| columnstore_use_decimal_scale                  | OFF    |
| columnstore_use_import_for_batchinsert         | ON     |
| columnstore_varbin_always_hex                  | OFF    |
+------------------------------------------------+--------+
20 rows in set (0.004 sec)
"
100,MCOL-1101,MCOL,Daniel Lee,134466,2019-09-19 19:08:39,Close based on the previous comment.  Documentation of the 4 new variables are being worked on.,3,Close based on the previous comment.  Documentation of the 4 new variables are being worked on.
101,MCOL-1107,MCOL,David Thompson,104522,2017-12-14 23:21:20,"ran unit tests and also verified could load simple test table with dates like 29-May-17 using:
java -Djava.library.path=../../java/ -jar ./build/libs/CpImport-all.jar test dates dates.txt dd-MMM-yy
",1,"ran unit tests and also verified could load simple test table with dates like 29-May-17 using:
java -Djava.library.path=../../java/ -jar ./build/libs/CpImport-all.jar test dates dates.txt dd-MMM-yy
"
102,MCOL-1119,MCOL,Jens Röwekamp,105767,2018-01-17 00:55:36,"Added a spark-connector that uses mcsapi to export a DataFrame to ColumnStore.

Supported are Python2/3 and Scala.

Automatic build and basic tests have been included in CMakeLists.txt and are executed successfully in Ubuntu 16.04.",1,"Added a spark-connector that uses mcsapi to export a DataFrame to ColumnStore.

Supported are Python2/3 and Scala.

Automatic build and basic tests have been included in CMakeLists.txt and are executed successfully in Ubuntu 16.04."
103,MCOL-1119,MCOL,Jens Röwekamp,105794,2018-01-17 19:53:52,"Attached my docker test environment build file.

cd spark-dev
docker-compose up -d

One can access Jupyter on http://localhost:8888
Login password: mariadb",2,"Attached my docker test environment build file.

cd spark-dev
docker-compose up -d

One can access Jupyter on URL
Login password: mariadb"
104,MCOL-1119,MCOL,Andrew Hutchings,105844,2018-01-18 15:57:56,"Looks great!

Moved to DT to test as he understands the requirements for this better than me.",3,"Looks great!

Moved to DT to test as he understands the requirements for this better than me."
105,MCOL-112,MCOL,Andrew Hutchings,85960,2016-08-30 14:10:05,Stalled until imminent release tag of 10.1.17,1,Stalled until imminent release tag of 10.1.17
106,MCOL-112,MCOL,Andrew Hutchings,86005,2016-08-31 14:29:28,Pull request for this: https://github.com/mariadb-corporation/mariadb-columnstore-server/pull/4,2,Pull request for this: URL
107,MCOL-112,MCOL,Andrew Hutchings,86013,2016-08-31 15:29:54,Now merged into the server tree. Just needs the regression suite running. Assigning to Daniel for QA.,3,Now merged into the server tree. Just needs the regression suite running. Assigning to Daniel for QA.
108,MCOL-112,MCOL,Daniel Lee,86441,2016-09-13 22:13:03,"Assigned it to Mr. Hall for verification.

Thanks",4,"Assigned it to Mr. Hall for verification.

Thanks"
109,MCOL-112,MCOL,David Thompson,86637,2016-09-20 22:21:58,verified by regression and autopilot test.,5,verified by regression and autopilot test.
110,MCOL-1121,MCOL,Andrew Hutchings,106005,2018-01-22 15:33:56,Some comments in the review that should be looked at,1,Some comments in the review that should be looked at
111,MCOL-1142,MCOL,David Hill,105475,2018-01-10 14:50:31,"Will need a yum and apt-get repo for libcdc_connector to get the data adapters to install

",1,"Will need a yum and apt-get repo for libcdc_connector to get the data adapters to install

"
112,MCOL-1142,MCOL,David Hill,105586,2018-01-12 16:53:41,"Good news, the Maxscale repos already exist, the latest is 2.1.13... I install downloads to make sure they work and will document in the ColumnStore REPO KB doc.",2,"Good news, the Maxscale repos already exist, the latest is 2.1.13... I install downloads to make sure they work and will document in the ColumnStore REPO KB doc."
113,MCOL-1142,MCOL,Dipti Joshi,105587,2018-01-12 16:57:41,[~hill]Please create separate KB article for AX install then the ColumnStore Repo KB doc,3,[~hill]Please create separate KB article for AX install then the ColumnStore Repo KB doc
114,MCOL-1142,MCOL,David Hill,107054,2018-02-08 16:58:39,"UPDATE- I have Maxscale for debian and ubuntu packages repo working now.. just had to get the right keydownload, which I figured out from Kolbes repo script shown in this doc.

https://mariadb.com/kb/en/library/mariadb-package-repository-setup-and-usage/",4,"UPDATE- I have Maxscale for debian and ubuntu packages repo working now.. just had to get the right keydownload, which I figured out from Kolbes repo script shown in this doc.

URL"
115,MCOL-1142,MCOL,David Hill,107899,2018-03-05 17:28:30,"Documentation shows how to install packages individually, needs example for each os on how to install all at the same time per os",5,"Documentation shows how to install packages individually, needs example for each os on how to install all at the same time per os"
116,MCOL-1142,MCOL,David Hill,108112,2018-03-08 16:06:29,finished document by added sections by OS on how to install all package at the same time.,6,finished document by added sections by OS on how to install all package at the same time.
117,MCOL-1142,MCOL,David Hill,108113,2018-03-08 16:07:15,"please review 

https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-package-repositories/",7,"please review 

URL"
118,MCOL-1142,MCOL,Daniel Lee,109086,2018-03-29 13:35:43,"Sorry, I totally missed this.  No, I don't have a test case for this.
",8,"Sorry, I totally missed this.  No, I don't have a test case for this.
"
119,MCOL-1142,MCOL,Daniel Lee,123973,2019-03-01 20:41:55,"Spot check tests on the instruction link.
",9,"Spot check tests on the instruction link.
"
120,MCOL-1143,MCOL,Ben Thompson,106547,2018-01-30 20:22:32,Added cmake to tools repo and commands for packaging BIN/DEB/RPM,1,Added cmake to tools repo and commands for packaging BIN/DEB/RPM
121,MCOL-1143,MCOL,David Hill,106560,2018-01-30 22:49:33,"successfull built centos 7 rpm packages and installed

",2,"successfull built centos 7 rpm packages and installed

"
122,MCOL-1143,MCOL,David Hill,106561,2018-01-30 22:53:52,successfuly built and install debian 8 packages..,3,successfuly built and install debian 8 packages..
123,MCOL-1143,MCOL,David Hill,106562,2018-01-30 22:54:40,Check README on how to build packages,4,Check README on how to build packages
124,MCOL-1143,MCOL,Daniel Lee,106620,2018-01-31 20:48:04,"Build verified: Github source for tools

/root/mariadb-columnstore-tools
[root@localhost mariadb-columnstore-tools]# git show
commit 2a87184362c890f9dedd8fdf825be4df3b96a6d1
Merge: 7b310a8 52a4d25
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jan 30 16:32:08 2018 -0600

    Merge pull request #3 from mariadb-corporation/MCOL-1143
    
    MCOL-1143


Verified instruction in README file for making bin, rpm, and deb packages.


Binary:
./package.sh

RPM:
cmake . -DRPM=1
make package

DEB:
cmake . -DDEB=1
make package

",5,"Build verified: Github source for tools

/root/mariadb-columnstore-tools
[root@localhost mariadb-columnstore-tools]# git show
commit 2a87184362c890f9dedd8fdf825be4df3b96a6d1
Merge: 7b310a8 52a4d25
Author: david hill 
Date:   Tue Jan 30 16:32:08 2018 -0600

    Merge pull request #3 from mariadb-corporation/MCOL-1143
    
    MCOL-1143


Verified instruction in README file for making bin, rpm, and deb packages.


Binary:
./package.sh

RPM:
cmake . -DRPM=1
make package

DEB:
cmake . -DDEB=1
make package

"
125,MCOL-1145,MCOL,David Hill,113561,2018-07-05 19:39:15,"Implementation options: 

1. Adding an options to postConfigure for a user to provide the 'quick-install' for single-server and 'system name'. Example is shown here:

/home/david_hill/mariadb/columnstore/bin/postConfigure -i /home/david_hill/mariadb/columnstore -qs -sn david

2. Adding a new script that a user can enter where it will run both the post-install and postConfigure scripts. This works for both root and non-root installs.
Here is an example run of that for a non-root install

./quick_installer_single_server.sh 
Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/david_hill/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/david_hill/mariadb/columnstore/lib:/home/david_hill/mariadb/columnstore/mysql/lib
/home/david_hill/mariadb/columnstore/bin/postConfigure -i /home/david_hill/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/david_hill/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/david_hill/mariadb/columnstore/lib:/home/david_hill/mariadb/columnstore/mysql/lib
columnstore/bin/columnstore start

Run postConfigure script


This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1


===== Quick Single-Server Install Configuration =====

Single-Server install is used when there will only be 1 server configured
on the system. It can also be used for production systems, if the plan is
to stay single-server.


===== Performing Configuration Setup and MariaDB ColumnStore Startup =====

NOTE: Setting 'NumBlocksPct' to 50%
      Setting 'TotalUmMemory' to 25% of total memory. 

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

Starting MariaDB Columnstore Database Platform

MariaDB ColumnStore Database Platform Starting, please wait ...... DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /home/david_hill/mariadb/columnstore/bin/columnstoreAlias

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

NOTE: The MariaDB ColumnStore Alias Commands are in /etc/profile.d/columnstoreAlias.sh

david_hill@ubuntu-18-ss:~/mariadb/columnstore/bin$",1,"Implementation options: 

1. Adding an options to postConfigure for a user to provide the 'quick-install' for single-server and 'system name'. Example is shown here:

/home/david_hill/mariadb/columnstore/bin/postConfigure -i /home/david_hill/mariadb/columnstore -qs -sn david

2. Adding a new script that a user can enter where it will run both the post-install and postConfigure scripts. This works for both root and non-root installs.
Here is an example run of that for a non-root install

./quick_installer_single_server.sh 
Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/david_hill/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/david_hill/mariadb/columnstore/lib:/home/david_hill/mariadb/columnstore/mysql/lib
/home/david_hill/mariadb/columnstore/bin/postConfigure -i /home/david_hill/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/david_hill/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/david_hill/mariadb/columnstore/lib:/home/david_hill/mariadb/columnstore/mysql/lib
columnstore/bin/columnstore start

Run postConfigure script


This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1


===== Quick Single-Server Install Configuration =====

Single-Server install is used when there will only be 1 server configured
on the system. It can also be used for production systems, if the plan is
to stay single-server.


===== Performing Configuration Setup and MariaDB ColumnStore Startup =====

NOTE: Setting 'NumBlocksPct' to 50%
      Setting 'TotalUmMemory' to 25% of total memory. 

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

Starting MariaDB Columnstore Database Platform

MariaDB ColumnStore Database Platform Starting, please wait ...... DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /home/david_hill/mariadb/columnstore/bin/columnstoreAlias

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

NOTE: The MariaDB ColumnStore Alias Commands are in /etc/profile.d/columnstoreAlias.sh

david_hill@ubuntu-18-ss:~/mariadb/columnstore/bin$"
126,MCOL-1145,MCOL,David Hill,113607,2018-07-06 14:12:59,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/514,2,URL
127,MCOL-1145,MCOL,David Hill,113683,2018-07-09 15:19:13,"to test. install CS package on single server, root or non-root and run

.../mariadb/columnstore/bin/quick_installer_single_server.sh 

",3,"to test. install CS package on single server, root or non-root and run

.../mariadb/columnstore/bin/quick_installer_single_server.sh 

"
128,MCOL-1145,MCOL,Daniel Lee,113993,2018-07-16 18:54:01,"Build tested: GitHub source

/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 32bd7d4e270f46a6052df64cff871f2c3371bb3a
Merge: af6108d 400ae51
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Thu Jul 12 15:36:42 2018 -0500

    Merge pull request #518 from mariadb-corporation/MCOL-1146
    
    Mcol 1146 - multi node quick install

root installation using rpm.  succeeded
non-root installation using bin, library error reported.  It seems to be a path issue
Using the same VM, manual installation by running postConfigure was successful.

[guest@localhost bin]$ ./quick_installer_single_server.sh 
Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script

/home/guest/mariadb/columnstore/bin/postConfigure: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory


",4,"Build tested: GitHub source

/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 32bd7d4e270f46a6052df64cff871f2c3371bb3a
Merge: af6108d 400ae51
Author: benthompson15 
Date:   Thu Jul 12 15:36:42 2018 -0500

    Merge pull request #518 from mariadb-corporation/MCOL-1146
    
    Mcol 1146 - multi node quick install

root installation using rpm.  succeeded
non-root installation using bin, library error reported.  It seems to be a path issue
Using the same VM, manual installation by running postConfigure was successful.

[guest@localhost bin]$ ./quick_installer_single_server.sh 
Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script

/home/guest/mariadb/columnstore/bin/postConfigure: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory


"
129,MCOL-1145,MCOL,Daniel Lee,114519,2018-07-26 12:55:35,Reopen per last test result,5,Reopen per last test result
130,MCOL-1145,MCOL,David Hill,114723,2018-08-01 14:25:45,"I reproduced Daniels failed test case, investigating

guest@ip-172-31-37-130:~/mariadb/columnstore/bin$ ./quick_installer_single_server.sh 
Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script

/home/guest/mariadb/columnstore/bin/postConfigure: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory
guest@ip-172-31-37-130:~/mariadb/columnstore/bin$ 
",6,"I reproduced Daniels failed test case, investigating

guest@ip-172-31-37-130:~/mariadb/columnstore/bin$ ./quick_installer_single_server.sh 
Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script

/home/guest/mariadb/columnstore/bin/postConfigure: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory
guest@ip-172-31-37-130:~/mariadb/columnstore/bin$ 
"
131,MCOL-1145,MCOL,David Hill,114732,2018-08-01 15:19:10,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/530,7,URL
132,MCOL-1145,MCOL,Daniel Lee,114990,2018-08-07 21:01:43,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ee40c3ac050ad7b64302673fc4ab08640f64892f
Merge: 0df1b92 979d00a
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Aug 6 13:02:08 2018 -0500

    Merge pull request #523 from mariadb-corporation/MCOL-1579
    
    MCOL-1579 Remove chmod of /dev/shm
",8,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings 
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ee40c3ac050ad7b64302673fc4ab08640f64892f
Merge: 0df1b92 979d00a
Author: benthompson15 
Date:   Mon Aug 6 13:02:08 2018 -0500

    Merge pull request #523 from mariadb-corporation/MCOL-1579
    
    MCOL-1579 Remove chmod of /dev/shm
"
133,MCOL-1146,MCOL,David Hill,105378,2018-01-08 17:41:14,"get infinidb amazon multi-node installes, we had a script called amazonInstaller where you just configure an XML file, run the amazonInstaller and it would do the rest including running postConfigure. This would be a good place to start with a general multi-node install script.",1,"get infinidb amazon multi-node installes, we had a script called amazonInstaller where you just configure an XML file, run the amazonInstaller and it would do the rest including running postConfigure. This would be a good place to start with a general multi-node install script."
134,MCOL-1146,MCOL,Assen Totin,107407,2018-02-16 10:20:13,"I'd suggest we extend the postConfigure with the option to read an XML file (since CS uses XML for configuration), verify its contents and then run the usual tasks. 

The only thing here, aside from the actual work (which should be trivial, I could take it after M18), is to ask whether there are any plans to move CS to INI-style configs like the TX has (since there is a trend to converge the two). If yes, then it will make more sense to make this kickstart file for postConfigure use INI style; if not, we can stick with XML. For what it's worth, INI is easier for humans to write and will be less error-prone than XML. ",2,"I'd suggest we extend the postConfigure with the option to read an XML file (since CS uses XML for configuration), verify its contents and then run the usual tasks. 

The only thing here, aside from the actual work (which should be trivial, I could take it after M18), is to ask whether there are any plans to move CS to INI-style configs like the TX has (since there is a trend to converge the two). If yes, then it will make more sense to make this kickstart file for postConfigure use INI style; if not, we can stick with XML. For what it's worth, INI is easier for humans to write and will be less error-prone than XML. "
135,MCOL-1146,MCOL,David Hill,113564,2018-07-05 20:46:39,"Non AWS install configuration items that postConfigure normal ask for:

Module Config type [separate or combined]
Enable Local Query [y,n]   // separate systems only
Enable MariaDB ColumnStore Schema Sync Feature [y,n]
Data Storage [internal, external]
Data Storage type [standard, Data-Redundancy]
UM HostNames nnnnnn,nnnnnn
UM IP Addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx // option is not on DNS
PM HostNames nnnnnn,nnnnnn
PM IP Addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx // option is not on DNS
Number of DBroots per PM [1-x]

And if Data Redundancy, these
Data Redundancy Copies [2-MAX-Pms]
Data Redundancy Network [existing, dedicated]

",3,"Non AWS install configuration items that postConfigure normal ask for:

Module Config type [separate or combined]
Enable Local Query [y,n]   // separate systems only
Enable MariaDB ColumnStore Schema Sync Feature [y,n]
Data Storage [internal, external]
Data Storage type [standard, Data-Redundancy]
UM HostNames nnnnnn,nnnnnn
UM IP Addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx // option is not on DNS
PM HostNames nnnnnn,nnnnnn
PM IP Addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx // option is not on DNS
Number of DBroots per PM [1-x]

And if Data Redundancy, these
Data Redundancy Copies [2-MAX-Pms]
Data Redundancy Network [existing, dedicated]

"
136,MCOL-1146,MCOL,Dipti Joshi,113566,2018-07-05 20:51:42,"Simply ask for UM and PM IP addresses and non-root user id /password on the command line. For rest of the post config options take defaults. Also assume non-distributed install.

This should be non-interactive. The user should not be required to edit any config file. 

 See here https://github.com/mariadb-corporation/mariadb-columnstore-docker/blob/master/columnstore/Dockerfile - how David Thompson has done it for a single node. ",4,"Simply ask for UM and PM IP addresses and non-root user id /password on the command line. For rest of the post config options take defaults. Also assume non-distributed install.

This should be non-interactive. The user should not be required to edit any config file. 

 See here URL - how David Thompson has done it for a single node. "
137,MCOL-1146,MCOL,David Hill,113568,2018-07-05 20:58:07,"Ok, so for 1.1.6, this would be the default multi-node install. User would just provide UM and PM ip addresses

Module Config type [separate]
Enable Local Query [n]
Enable MariaDB ColumnStore Schema Sync Feature [y]
Data Storage [internal]
Data Storage type [standard]
Number of DBroots per PM [1]

and the Ip-addresses would be used as the host-names",5,"Ok, so for 1.1.6, this would be the default multi-node install. User would just provide UM and PM ip addresses

Module Config type [separate]
Enable Local Query [n]
Enable MariaDB ColumnStore Schema Sync Feature [y]
Data Storage [internal]
Data Storage type [standard]
Number of DBroots per PM [1]

and the Ip-addresses would be used as the host-names"
138,MCOL-1146,MCOL,David Hill,113570,2018-07-05 21:16:03,"I can implement it like I did for MCOL-1145. a few new command line arguments for postConfigure and a quick-install script where user provides UM and PM IP address. If they only provide PM, then its a combo install.

example on postConfigure change

./postConfigure -qm -um-ip-addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx -pm-ip-addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx

And the quick install script:

./quick_installer_multi_node.sh --um-ip-addresses=xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx --pm-ip-addresses=xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx

or for a combined system

./quick_installer_multi_node.sh --pm-ip-addresses=xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx

SO going this route, the data just gets passed into postConfigure, dont need to prefill in a Columnstore.xml",6,"I can implement it like I did for MCOL-1145. a few new command line arguments for postConfigure and a quick-install script where user provides UM and PM IP address. If they only provide PM, then its a combo install.

example on postConfigure change

./postConfigure -qm -um-ip-addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx -pm-ip-addresses xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx

And the quick install script:

./quick_installer_multi_node.sh --um-ip-addresses=xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx --pm-ip-addresses=xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx

or for a combined system

./quick_installer_multi_node.sh --pm-ip-addresses=xxxx.xxxx.xxxx.xxxx,xxxx.xxxx.xxxx.xxxx

SO going this route, the data just gets passed into postConfigure, dont need to prefill in a Columnstore.xml"
139,MCOL-1146,MCOL,David Hill,113757,2018-07-10 16:52:54,"Distrubuted and Non-distributed. The default will be Non-distributed, which will require users to preinstall packages on all servers. But I did add an option to do Distributed installs, which is something I would use more.. So it will be good to have the option of both both ways..

 here is an example and a full output:

/usr/local/mariadb/columnstore/bin/quick_installer_multi_server.sh --pm-ip-addresses=172.31.46.144,172.31.33.138 --um-ip-addresses=172.31.40.253 --dist-install

NOTE: Performing a Multi-Server Seperate install with um and pm running on seperate servers

Run post-install script

The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distributed option:

/usr/local/mariadb/columnstore/bin/columnstore start


Run postConfigure script


This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1

With the no-Prompting Option being specified, you will be required to have the following:

 1. Root user ssh keys setup between all nodes in the system or
    use the password command line option.
 2. A Configure File to use to retrieve configure data, default to Columnstore.xml.rpmsave
    or use the '-c' option to point to a configuration file.


===== Quick Install Multi-Server Configuration =====



===== Setup System Module Type Configuration =====

There are 2 options when configuring the System Module Type: separate and combined

  'separate' - User and Performance functionality on separate servers.

  'combined' - User and Performance functionality on the same server

Select the type of System Module Install [1=separate, 2=combined] (1) > 

Seperate Server Installation will be performed.

NOTE: Local Query Feature allows the ability to query data from a single Performance
      Module. Check MariaDB ColumnStore Admin Guide for additional information.

Enable Local Query feature? [y,n] (n) > 

NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
      schemas and InnoDB tables across the User Module nodes. This feature can be enabled
      or disabled, for example, if you wish to configure your own replication post installation.

MariaDB ColumnStore Schema Sync feature is Enabled, do you want to leave enabled? [y,n] (y) > 


NOTE: MariaDB ColumnStore Replication Feature is enabled

Enter System Name (columnstore-1) > 


===== Setup Storage Configuration =====


----- Setup Performance Module DBRoot Data Storage Mount Configuration -----

There are 2 options when configuring the storage: internal or external

  'internal' -    This is specified when a local disk is used for the DBRoot storage.
                  High Availability Server Failover is not Supported in this mode

  'external' -    This is specified when the DBRoot directories are mounted.
                  High Availability Server Failover is Supported in this mode.

Select the type of Data Storage [1=internal, 2=external] (1) > 

===== Setup Memory Configuration =====

NOTE: Setting 'NumBlocksPct' to 70%
      Setting 'TotalUmMemory' to 50%

===== Setup the Module Configuration =====


----- User Module Configuration -----

Enter number of User Modules [1,1024] (1) > 

*** User Module #1 Configuration ***

Enter Nic Interface #1 Host Name (172.31.40.253) > 
Enter Nic Interface #1 IP Address of 172.31.40.253 (172.31.40.253) > 

----- Performance Module Configuration -----

Enter number of Performance Modules [1,1024] (2) > 

*** Parent OAM Module Performance Module #1 Configuration ***

Enter Nic Interface #1 Host Name (172.31.46.144) > 
Enter Nic Interface #1 IP Address of 172.31.46.144 (172.31.46.144) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm1' (1) > 

*** Performance Module #2 Configuration ***

Enter Nic Interface #1 Host Name (172.31.33.138) > 
Enter Nic Interface #1 IP Address of 172.31.33.138 (172.31.33.138) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm2' (2) > 

===== System Installation =====

System Configuration is complete.
Performing System Installation.

Performing a MariaDB ColumnStore System install using a Binary package
located in the /root directory.


Next step is to enter the password to access the other Servers.
This is either your password or you can default to using a ssh key
If using a password, the password needs to be the same on all Servers.

Enter password, hit 'enter' to default to using a ssh key, or 'exit' > 


----- Performing Install on 'um1 / 172.31.40.253' -----

Install log file is located here: /tmp/um1_binary_install.log


----- Performing Install on 'pm2 / 172.31.33.138' -----

Install log file is located here: /tmp/pm2_binary_install.log


MariaDB ColumnStore Package being installed, please wait ...  DONE

===== Checking MariaDB ColumnStore System Logging Functionality =====

The MariaDB ColumnStore system logging is setup and working on local server

===== MariaDB ColumnStore System Startup =====

System Configuration is complete.
Performing System Installation.

----- Starting MariaDB ColumnStore on local server -----

MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait .......... DONE

System Catalog Successfully Created

Run MariaDB ColumnStore Replication Setup..  DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /usr/local/mariadb/columnstore/bin/columnstoreAlias

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

NOTE: The MariaDB ColumnStore Alias Commands are in /etc/profile.d/columnstoreAlias.sh

root@ip-172-31-46-144:~# 
",7,"Distrubuted and Non-distributed. The default will be Non-distributed, which will require users to preinstall packages on all servers. But I did add an option to do Distributed installs, which is something I would use more.. So it will be good to have the option of both both ways..

 here is an example and a full output:

/usr/local/mariadb/columnstore/bin/quick_installer_multi_server.sh --pm-ip-addresses=172.31.46.144,172.31.33.138 --um-ip-addresses=172.31.40.253 --dist-install

NOTE: Performing a Multi-Server Seperate install with um and pm running on seperate servers

Run post-install script

The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distributed option:

/usr/local/mariadb/columnstore/bin/columnstore start


Run postConfigure script


This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1

With the no-Prompting Option being specified, you will be required to have the following:

 1. Root user ssh keys setup between all nodes in the system or
    use the password command line option.
 2. A Configure File to use to retrieve configure data, default to Columnstore.xml.rpmsave
    or use the '-c' option to point to a configuration file.


===== Quick Install Multi-Server Configuration =====



===== Setup System Module Type Configuration =====

There are 2 options when configuring the System Module Type: separate and combined

  'separate' - User and Performance functionality on separate servers.

  'combined' - User and Performance functionality on the same server

Select the type of System Module Install [1=separate, 2=combined] (1) > 

Seperate Server Installation will be performed.

NOTE: Local Query Feature allows the ability to query data from a single Performance
      Module. Check MariaDB ColumnStore Admin Guide for additional information.

Enable Local Query feature? [y,n] (n) > 

NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
      schemas and InnoDB tables across the User Module nodes. This feature can be enabled
      or disabled, for example, if you wish to configure your own replication post installation.

MariaDB ColumnStore Schema Sync feature is Enabled, do you want to leave enabled? [y,n] (y) > 


NOTE: MariaDB ColumnStore Replication Feature is enabled

Enter System Name (columnstore-1) > 


===== Setup Storage Configuration =====


----- Setup Performance Module DBRoot Data Storage Mount Configuration -----

There are 2 options when configuring the storage: internal or external

  'internal' -    This is specified when a local disk is used for the DBRoot storage.
                  High Availability Server Failover is not Supported in this mode

  'external' -    This is specified when the DBRoot directories are mounted.
                  High Availability Server Failover is Supported in this mode.

Select the type of Data Storage [1=internal, 2=external] (1) > 

===== Setup Memory Configuration =====

NOTE: Setting 'NumBlocksPct' to 70%
      Setting 'TotalUmMemory' to 50%

===== Setup the Module Configuration =====


----- User Module Configuration -----

Enter number of User Modules [1,1024] (1) > 

*** User Module #1 Configuration ***

Enter Nic Interface #1 Host Name (172.31.40.253) > 
Enter Nic Interface #1 IP Address of 172.31.40.253 (172.31.40.253) > 

----- Performance Module Configuration -----

Enter number of Performance Modules [1,1024] (2) > 

*** Parent OAM Module Performance Module #1 Configuration ***

Enter Nic Interface #1 Host Name (172.31.46.144) > 
Enter Nic Interface #1 IP Address of 172.31.46.144 (172.31.46.144) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm1' (1) > 

*** Performance Module #2 Configuration ***

Enter Nic Interface #1 Host Name (172.31.33.138) > 
Enter Nic Interface #1 IP Address of 172.31.33.138 (172.31.33.138) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm2' (2) > 

===== System Installation =====

System Configuration is complete.
Performing System Installation.

Performing a MariaDB ColumnStore System install using a Binary package
located in the /root directory.


Next step is to enter the password to access the other Servers.
This is either your password or you can default to using a ssh key
If using a password, the password needs to be the same on all Servers.

Enter password, hit 'enter' to default to using a ssh key, or 'exit' > 


----- Performing Install on 'um1 / 172.31.40.253' -----

Install log file is located here: /tmp/um1_binary_install.log


----- Performing Install on 'pm2 / 172.31.33.138' -----

Install log file is located here: /tmp/pm2_binary_install.log


MariaDB ColumnStore Package being installed, please wait ...  DONE

===== Checking MariaDB ColumnStore System Logging Functionality =====

The MariaDB ColumnStore system logging is setup and working on local server

===== MariaDB ColumnStore System Startup =====

System Configuration is complete.
Performing System Installation.

----- Starting MariaDB ColumnStore on local server -----

MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait .......... DONE

System Catalog Successfully Created

Run MariaDB ColumnStore Replication Setup..  DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /usr/local/mariadb/columnstore/bin/columnstoreAlias

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

NOTE: The MariaDB ColumnStore Alias Commands are in /etc/profile.d/columnstoreAlias.sh

root@ip-172-31-46-144:~# 
"
140,MCOL-1146,MCOL,David Hill,113859,2018-07-12 14:32:15,"Multi-node quick install completed, here is the help

./quick_installer_multi_server.sh --help
Usage ./quick_installer_multi_server.sh [OPTION]

Quick Installer for a Multi Server MariaDB ColumnStore Install

Defaults to non-distrubuted install, meaning MariaDB Columnstore
needs to be preinstalled on all nodes in the system

Performace Module (pm) IP addresses are required
User Module (um) IP addresses are option
When only pm IP addresses provided, system is combined setup
When both pm/um IP addresses provided, system is seperate setup

--pm-ip-addresses=xxx.xxx.xxx.xxx,xxx.xxx.xxx.xxx
--um-ip-addresses=xxx.xxx.xxx.xxx,xxx.xxx.xxx.xxx, optional
--dist-install Use Distributed Install, optional
--system-name=nnnn System Name, optional

Amazon AMI quick install completed, here is the help

./quick_installer_amazon.sh --help
Usage ./quick_installer_amazon.sh [OPTION]

Quick Installer for an Amazon MariaDB ColumnStore Install
This requires to be run on a MariaDB ColumnStore AMI

Performace Module (pm) number is required
User Module (um) number is option
When only pm counts provided, system is combined setup
When both pm/um counts provided, system is seperate setup

--pm-count=x Number of pm instances to create
--um-count=x Number of um instances to create, optional
--system-name=nnnn System Name, optional
",8,"Multi-node quick install completed, here is the help

./quick_installer_multi_server.sh --help
Usage ./quick_installer_multi_server.sh [OPTION]

Quick Installer for a Multi Server MariaDB ColumnStore Install

Defaults to non-distrubuted install, meaning MariaDB Columnstore
needs to be preinstalled on all nodes in the system

Performace Module (pm) IP addresses are required
User Module (um) IP addresses are option
When only pm IP addresses provided, system is combined setup
When both pm/um IP addresses provided, system is seperate setup

--pm-ip-addresses=xxx.xxx.xxx.xxx,xxx.xxx.xxx.xxx
--um-ip-addresses=xxx.xxx.xxx.xxx,xxx.xxx.xxx.xxx, optional
--dist-install Use Distributed Install, optional
--system-name=nnnn System Name, optional

Amazon AMI quick install completed, here is the help

./quick_installer_amazon.sh --help
Usage ./quick_installer_amazon.sh [OPTION]

Quick Installer for an Amazon MariaDB ColumnStore Install
This requires to be run on a MariaDB ColumnStore AMI

Performace Module (pm) number is required
User Module (um) number is option
When only pm counts provided, system is combined setup
When both pm/um counts provided, system is seperate setup

--pm-count=x Number of pm instances to create
--um-count=x Number of um instances to create, optional
--system-name=nnnn System Name, optional
"
141,MCOL-1146,MCOL,David Hill,113861,2018-07-12 14:33:42,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/518,9,URL
142,MCOL-1146,MCOL,Ben Thompson,113876,2018-07-12 20:37:30,Merged,10,Merged
143,MCOL-1146,MCOL,Daniel Lee,113995,2018-07-16 19:43:43,"Build tested: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 32bd7d4e270f46a6052df64cff871f2c3371bb3a
Merge: af6108d 400ae51
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Thu Jul 12 15:36:42 2018 -0500

    Merge pull request #518 from mariadb-corporation/MCOL-1146
    
    Mcol 1146 - multi node quick install

root installation, distributed worked
non-distributed.  root and non-root,  Procmons did not start on um1 and pm2
none-root installation, distributed reported library error.

[guest@localhost bin]$ ./quick_installer_multi_server.sh --pm-ip-addresses=10.0.0.15,10.0.0.16 --um-ip-addresses=10.0.0.11 --dist-install

NOTE: Performing a Multi-Server Seperate install with um and pm running on seperate servers

Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script

/home/guest/mariadb/columnstore/bin/postConfigure: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory

using the same VMs, manually running postConfigure installed successfully.
",11,"Build tested: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 32bd7d4e270f46a6052df64cff871f2c3371bb3a
Merge: af6108d 400ae51
Author: benthompson15 
Date:   Thu Jul 12 15:36:42 2018 -0500

    Merge pull request #518 from mariadb-corporation/MCOL-1146
    
    Mcol 1146 - multi node quick install

root installation, distributed worked
non-distributed.  root and non-root,  Procmons did not start on um1 and pm2
none-root installation, distributed reported library error.

[guest@localhost bin]$ ./quick_installer_multi_server.sh --pm-ip-addresses=10.0.0.15,10.0.0.16 --um-ip-addresses=10.0.0.11 --dist-install

NOTE: Performing a Multi-Server Seperate install with um and pm running on seperate servers

Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script

/home/guest/mariadb/columnstore/bin/postConfigure: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory

using the same VMs, manually running postConfigure installed successfully.
"
144,MCOL-1146,MCOL,Daniel Lee,114520,2018-07-26 13:06:06,reopen per last test result,12,reopen per last test result
145,MCOL-1146,MCOL,David Hill,114731,2018-08-01 15:16:03,"reproduce issue and fixed... Also added another command that user needs to run after postconfigure is finished to set env and alias's

guest@ip-172-31-37-130:~$ mariadb/columnstore/bin/quick_installer_multi_server.sh --pm-ip-addresses=172.31.37.130

NOTE: Performing a Multi-Server Combined install with um/pm running on some server

Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script


This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1

With the no-Prompting Option being specified, you will be required to have the following:

 1. Root user ssh keys setup between all nodes in the system or
    use the password command line option.
 2. A Configure File to use to retrieve configure data, default to Columnstore.xml.rpmsave
    or use the '-c' option to point to a configuration file.


===== Quick Install Multi-Server Configuration =====



===== Setup System Module Type Configuration =====

There are 2 options when configuring the System Module Type: separate and combined

  'separate' - User and Performance functionality on separate servers.

  'combined' - User and Performance functionality on the same server

Select the type of System Module Install [1=separate, 2=combined] (2) > 

Combined Server Installation will be performed.
The Server will be configured as a Performance Module.
All MariaDB ColumnStore Processes will run on the Performance Modules.

NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
      schemas and InnoDB tables across the User Module nodes. This feature can be enabled
      or disabled, for example, if you wish to configure your own replication post installation.

MariaDB ColumnStore Schema Sync feature is Enabled, do you want to leave enabled? [y,n] (y) > 


NOTE: MariaDB ColumnStore Replication Feature is enabled

NOTE: MariaDB ColumnStore Non-Distributed Install Feature is enabled

Enter System Name (columnstore-1) > 


===== Setup Storage Configuration =====


----- Setup Performance Module DBRoot Data Storage Mount Configuration -----

There are 2 options when configuring the storage: internal or external

  'internal' -    This is specified when a local disk is used for the DBRoot storage.
                  High Availability Server Failover is not Supported in this mode

  'external' -    This is specified when the DBRoot directories are mounted.
                  High Availability Server Failover is Supported in this mode.

Select the type of Data Storage [1=internal, 2=external] (1) > 

===== Setup Memory Configuration =====


NOTE: Setting 'NumBlocksPct' to 50%
      Setting 'TotalUmMemory' to 25%

===== Setup the Module Configuration =====


----- Performance Module Configuration -----

Enter number of Performance Modules [1,1024] (1) > 

*** Parent OAM Module Performance Module #1 Configuration ***

Enter Nic Interface #1 Host Name (172.31.37.130) > 
Enter Nic Interface #1 IP Address of 172.31.37.130 (172.31.37.130) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm1' (1) > 

===== Running the MariaDB ColumnStore MariaDB Server setup scripts =====


post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

===== Checking MariaDB ColumnStore System Logging Functionality =====

The MariaDB ColumnStore system logging is setup and working on local server

MariaDB ColumnStore System Configuration and Installation is Completed

===== MariaDB ColumnStore System Startup =====

System Configuration is complete.
Performing System Installation.

----- Starting MariaDB ColumnStore on local Server 'pm1' -----


MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait ....... DONE

System Catalog Successfully Created

Run MariaDB ColumnStore Replication Setup..  DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /etc/profile.d/columnstoreEnv.sh
. /etc/profile.d/columnstoreAlias.sh

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

NOTE: The MariaDB ColumnStore Alias Commands are in /etc/profile.d/columnstoreAlias.sh

guest@ip-172-31-37-130:~$ 

. /etc/profile.d/columnstoreEnv.sh
. /etc/profile.d/columnstoreAlias.sh

 ma

MariaDB ColumnStore Admin Console
   enter 'help' for list of commands
   enter 'exit' to exit the MariaDB ColumnStore Command Console
   use up/down arrows to recall commands


Active Alarm Counts: Critical = 0, Major = 0, Minor = 0, Warning = 0, Info = 0

Critical Active Alarms:



",13,"reproduce issue and fixed... Also added another command that user needs to run after postconfigure is finished to set env and alias's

guest@ip-172-31-37-130:~$ mariadb/columnstore/bin/quick_installer_multi_server.sh --pm-ip-addresses=172.31.37.130

NOTE: Performing a Multi-Server Combined install with um/pm running on some server

Run post-install script

The next steps are:

If installing on a pm1 node:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/postConfigure -i /home/guest/mariadb/columnstore

If installing on a non-pm1 using the non-distributed option:

export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
/home/guest/mariadb/columnstore/bin/columnstore start

Run postConfigure script


This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1

With the no-Prompting Option being specified, you will be required to have the following:

 1. Root user ssh keys setup between all nodes in the system or
    use the password command line option.
 2. A Configure File to use to retrieve configure data, default to Columnstore.xml.rpmsave
    or use the '-c' option to point to a configuration file.


===== Quick Install Multi-Server Configuration =====



===== Setup System Module Type Configuration =====

There are 2 options when configuring the System Module Type: separate and combined

  'separate' - User and Performance functionality on separate servers.

  'combined' - User and Performance functionality on the same server

Select the type of System Module Install [1=separate, 2=combined] (2) > 

Combined Server Installation will be performed.
The Server will be configured as a Performance Module.
All MariaDB ColumnStore Processes will run on the Performance Modules.

NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
      schemas and InnoDB tables across the User Module nodes. This feature can be enabled
      or disabled, for example, if you wish to configure your own replication post installation.

MariaDB ColumnStore Schema Sync feature is Enabled, do you want to leave enabled? [y,n] (y) > 


NOTE: MariaDB ColumnStore Replication Feature is enabled

NOTE: MariaDB ColumnStore Non-Distributed Install Feature is enabled

Enter System Name (columnstore-1) > 


===== Setup Storage Configuration =====


----- Setup Performance Module DBRoot Data Storage Mount Configuration -----

There are 2 options when configuring the storage: internal or external

  'internal' -    This is specified when a local disk is used for the DBRoot storage.
                  High Availability Server Failover is not Supported in this mode

  'external' -    This is specified when the DBRoot directories are mounted.
                  High Availability Server Failover is Supported in this mode.

Select the type of Data Storage [1=internal, 2=external] (1) > 

===== Setup Memory Configuration =====


NOTE: Setting 'NumBlocksPct' to 50%
      Setting 'TotalUmMemory' to 25%

===== Setup the Module Configuration =====


----- Performance Module Configuration -----

Enter number of Performance Modules [1,1024] (1) > 

*** Parent OAM Module Performance Module #1 Configuration ***

Enter Nic Interface #1 Host Name (172.31.37.130) > 
Enter Nic Interface #1 IP Address of 172.31.37.130 (172.31.37.130) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm1' (1) > 

===== Running the MariaDB ColumnStore MariaDB Server setup scripts =====


post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

===== Checking MariaDB ColumnStore System Logging Functionality =====

The MariaDB ColumnStore system logging is setup and working on local server

MariaDB ColumnStore System Configuration and Installation is Completed

===== MariaDB ColumnStore System Startup =====

System Configuration is complete.
Performing System Installation.

----- Starting MariaDB ColumnStore on local Server 'pm1' -----


MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait ....... DONE

System Catalog Successfully Created

Run MariaDB ColumnStore Replication Setup..  DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /etc/profile.d/columnstoreEnv.sh
. /etc/profile.d/columnstoreAlias.sh

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

NOTE: The MariaDB ColumnStore Alias Commands are in /etc/profile.d/columnstoreAlias.sh

guest@ip-172-31-37-130:~$ 

. /etc/profile.d/columnstoreEnv.sh
. /etc/profile.d/columnstoreAlias.sh

 ma

MariaDB ColumnStore Admin Console
   enter 'help' for list of commands
   enter 'exit' to exit the MariaDB ColumnStore Command Console
   use up/down arrows to recall commands


Active Alarm Counts: Critical = 0, Major = 0, Minor = 0, Warning = 0, Info = 0

Critical Active Alarms:



"
146,MCOL-1146,MCOL,David Hill,114733,2018-08-01 15:19:29,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/530,14,URL
147,MCOL-1146,MCOL,Daniel Lee,114991,2018-08-07 21:07:55,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ee40c3ac050ad7b64302673fc4ab08640f64892f
Merge: 0df1b92 979d00a
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Aug 6 13:02:08 2018 -0500

    Merge pull request #523 from mariadb-corporation/MCOL-1579
    
    MCOL-1579 Remove chmod of /dev/shm
",15,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings 
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ee40c3ac050ad7b64302673fc4ab08640f64892f
Merge: 0df1b92 979d00a
Author: benthompson15 
Date:   Mon Aug 6 13:02:08 2018 -0500

    Merge pull request #523 from mariadb-corporation/MCOL-1579
    
    MCOL-1579 Remove chmod of /dev/shm
"
148,MCOL-1158,MCOL,Jens Röwekamp,118115,2018-10-19 01:32:59,"Changed the flag for Python 3.

For QA:
- execute pymcsapi's regression test suite at least on CentOS 7, one Debian, one Ubuntu, and Windows 10.",1,"Changed the flag for Python 3.

For QA:
- execute pymcsapi's regression test suite at least on CentOS 7, one Debian, one Ubuntu, and Windows 10."
149,MCOL-1159,MCOL,Ben Thompson,105926,2018-01-19 19:46:36,Merged to 1.1,1,Merged to 1.1
150,MCOL-1159,MCOL,Daniel Lee,105929,2018-01-19 20:27:18,"Build verified: Github source 1.1.3-1

/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Fri Jan 19 12:39:05 2018 -0600

    Merge pull request #84 from mariadb-corporation/MCOL-1159
    
    MCOL-1159 Merge mariadb-10.2.12

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 201813d63d63e99e50f2474d1bf7d8428ac72119
Merge: 3748036 a002d33
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Jan 19 19:40:18 2018 +0000

    Merge pull request #373 from mariadb-corporation/MergeFix
    
    Merge deleted change to include columnstoreversion.h



[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.12-MariaDB-log Columnstore 1.1.3-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.


",2,"Build verified: Github source 1.1.3-1

/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 
Date:   Fri Jan 19 12:39:05 2018 -0600

    Merge pull request #84 from mariadb-corporation/MCOL-1159
    
    MCOL-1159 Merge mariadb-10.2.12

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 201813d63d63e99e50f2474d1bf7d8428ac72119
Merge: 3748036 a002d33
Author: Andrew Hutchings 
Date:   Fri Jan 19 19:40:18 2018 +0000

    Merge pull request #373 from mariadb-corporation/MergeFix
    
    Merge deleted change to include columnstoreversion.h



[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.12-MariaDB-log Columnstore 1.1.3-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.


"
151,MCOL-1171,MCOL,Jens Röwekamp,105938,2018-01-20 01:22:56,"Added benchmarks for Scala and Python2/3 comparing the API with JDBC's write to ColumnStore and JDBC's write to InnoDB.

For Scala execute ./gradlew benchmark
For Python execute ./benchmark.sh PATH_TO_MARIADB_JDBC_CLIENT_JAR
in the representative directories.

Currently the second benchmark creates a java.lang.OutOfMemoryError which is assumed to be caused by MCOL-1091. ",1,"Added benchmarks for Scala and Python2/3 comparing the API with JDBC's write to ColumnStore and JDBC's write to InnoDB.

For Scala execute ./gradlew benchmark
For Python execute ./benchmark.sh PATH_TO_MARIADB_JDBC_CLIENT_JAR
in the representative directories.

Currently the second benchmark creates a java.lang.OutOfMemoryError which is assumed to be caused by MCOL-1091. "
152,MCOL-1171,MCOL,Daniel Lee,106137,2018-01-24 03:25:39,"Build verified: mariadb-columnstore-api

[root@localhost mariadb-columnstore-api]# git show
commit cb69ca0eafa218510f2d78a327b5d84641abe564
Merge: 55e181c ef3f3c0
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Mon Jan 22 15:03:34 2018 +0000

    Merge pull request #40 from mariadb-corporation/MCOL-1171
    
    Mcol 1171

Verified on Centos7 using the following steps:

sudo yum install python-pip pytest python34-setuptools python34-devel
sudo pip install --upgrade pip
sudo pip install mysql-connector==2.1.6 six pytest pyspark
sudo easy_install-3.4 pip
sudo pip3 install mysql-connector==2.1.6 pytest six pyspark
git clone https://github.com/mariadb-corporation/mariadb-columnstore-api.git
cd mariadb-columnstore-api
git checkout develop-1.1
scl enable devtoolset-4 bash
cmake -DTEST_RUNNER=ON -DSPARK_CONNECTOR=ON .
make
sudo make install
make test


[root@localhost mariadb-columnstore-api]# make test
Running tests...
Test project /root/build/mariadb-columnstore-api
      Start  1: version
 1/23 Test  #1: version ..........................   Passed    0.00 sec
      Start  2: commit
 2/23 Test  #2: commit ...........................   Passed    6.13 sec
      Start  3: million-row
 3/23 Test  #3: million-row ......................   Passed    1.41 sec
      Start  4: chained
 4/23 Test  #4: chained ..........................   Passed    0.53 sec
      Start  5: dataconvert-int
 5/23 Test  #5: dataconvert-int ..................   Passed    0.95 sec
      Start  6: dataconvert-double
 6/23 Test  #6: dataconvert-double ...............   Passed    0.94 sec
      Start  7: dataconvert-string
 7/23 Test  #7: dataconvert-string ...............   Passed    1.04 sec
      Start  8: dataconvert-datetime
 8/23 Test  #8: dataconvert-datetime .............   Passed    0.93 sec
      Start  9: dataconvert-decimal
 9/23 Test  #9: dataconvert-decimal ..............   Passed    2.15 sec
      Start 10: dataconvert-null
10/23 Test #10: dataconvert-null .................   Passed    0.98 sec
      Start 11: bad-usage
11/23 Test #11: bad-usage ........................   Passed    6.78 sec
      Start 12: truncation
12/23 Test #12: truncation .......................   Passed    1.14 sec
      Start 13: summary
13/23 Test #13: summary ..........................   Passed    3.97 sec
      Start 14: system-catalog
14/23 Test #14: system-catalog ...................   Passed    0.27 sec
      Start 15: char4
15/23 Test #15: char4 ............................   Passed    0.75 sec
      Start 16: test_basic
16/23 Test #16: test_basic .......................   Passed   17.16 sec
      Start 17: test_million_row
17/23 Test #17: test_million_row .................   Passed    4.63 sec
      Start 18: test_basic_python3
18/23 Test #18: test_basic_python3 ...............   Passed   17.00 sec
      Start 19: test_million_row_python3
19/23 Test #19: test_million_row_python3 .........   Passed    9.56 sec
      Start 20: Java_BasicTest
20/23 Test #20: Java_BasicTest ...................   Passed    3.07 sec
      Start 21: SparkScalaConnector_BasicTest
21/23 Test #21: SparkScalaConnector_BasicTest ....   Passed   11.01 sec
      Start 22: pythonColumnStoreExporter
22/23 Test #22: pythonColumnStoreExporter ........   Passed    4.89 sec
      Start 23: python3ColumnStoreExporter
23/23 Test #23: python3ColumnStoreExporter .......   Passed    4.43 sec

100% tests passed, 0 tests failed out of 23

Total Test time (real) =  99.73 sec
[root@localhost mariadb-columnstore-api]# 
",2,"Build verified: mariadb-columnstore-api

[root@localhost mariadb-columnstore-api]# git show
commit cb69ca0eafa218510f2d78a327b5d84641abe564
Merge: 55e181c ef3f3c0
Author: Andrew Hutchings 
Date:   Mon Jan 22 15:03:34 2018 +0000

    Merge pull request #40 from mariadb-corporation/MCOL-1171
    
    Mcol 1171

Verified on Centos7 using the following steps:

sudo yum install python-pip pytest python34-setuptools python34-devel
sudo pip install --upgrade pip
sudo pip install mysql-connector==2.1.6 six pytest pyspark
sudo easy_install-3.4 pip
sudo pip3 install mysql-connector==2.1.6 pytest six pyspark
git clone URL
cd mariadb-columnstore-api
git checkout develop-1.1
scl enable devtoolset-4 bash
cmake -DTEST_RUNNER=ON -DSPARK_CONNECTOR=ON .
make
sudo make install
make test


[root@localhost mariadb-columnstore-api]# make test
Running tests...
Test project /root/build/mariadb-columnstore-api
      Start  1: version
 1/23 Test  #1: version ..........................   Passed    0.00 sec
      Start  2: commit
 2/23 Test  #2: commit ...........................   Passed    6.13 sec
      Start  3: million-row
 3/23 Test  #3: million-row ......................   Passed    1.41 sec
      Start  4: chained
 4/23 Test  #4: chained ..........................   Passed    0.53 sec
      Start  5: dataconvert-int
 5/23 Test  #5: dataconvert-int ..................   Passed    0.95 sec
      Start  6: dataconvert-double
 6/23 Test  #6: dataconvert-double ...............   Passed    0.94 sec
      Start  7: dataconvert-string
 7/23 Test  #7: dataconvert-string ...............   Passed    1.04 sec
      Start  8: dataconvert-datetime
 8/23 Test  #8: dataconvert-datetime .............   Passed    0.93 sec
      Start  9: dataconvert-decimal
 9/23 Test  #9: dataconvert-decimal ..............   Passed    2.15 sec
      Start 10: dataconvert-null
10/23 Test #10: dataconvert-null .................   Passed    0.98 sec
      Start 11: bad-usage
11/23 Test #11: bad-usage ........................   Passed    6.78 sec
      Start 12: truncation
12/23 Test #12: truncation .......................   Passed    1.14 sec
      Start 13: summary
13/23 Test #13: summary ..........................   Passed    3.97 sec
      Start 14: system-catalog
14/23 Test #14: system-catalog ...................   Passed    0.27 sec
      Start 15: char4
15/23 Test #15: char4 ............................   Passed    0.75 sec
      Start 16: test_basic
16/23 Test #16: test_basic .......................   Passed   17.16 sec
      Start 17: test_million_row
17/23 Test #17: test_million_row .................   Passed    4.63 sec
      Start 18: test_basic_python3
18/23 Test #18: test_basic_python3 ...............   Passed   17.00 sec
      Start 19: test_million_row_python3
19/23 Test #19: test_million_row_python3 .........   Passed    9.56 sec
      Start 20: Java_BasicTest
20/23 Test #20: Java_BasicTest ...................   Passed    3.07 sec
      Start 21: SparkScalaConnector_BasicTest
21/23 Test #21: SparkScalaConnector_BasicTest ....   Passed   11.01 sec
      Start 22: pythonColumnStoreExporter
22/23 Test #22: pythonColumnStoreExporter ........   Passed    4.89 sec
      Start 23: python3ColumnStoreExporter
23/23 Test #23: python3ColumnStoreExporter .......   Passed    4.43 sec

100% tests passed, 0 tests failed out of 23

Total Test time (real) =  99.73 sec
[root@localhost mariadb-columnstore-api]# 
"
153,MCOL-1172,MCOL,Jens Röwekamp,105936,2018-01-19 22:43:15,Will be fixed in MCOL-1171,1,Will be fixed in MCOL-1171
154,MCOL-1172,MCOL,Jens Röwekamp,105939,2018-01-20 02:44:40,Fixed in branch MCOL-1171,2,Fixed in branch MCOL-1171
155,MCOL-1172,MCOL,Andrew Hutchings,106700,2018-02-02 10:19:57,Nothing really to QA here.,3,Nothing really to QA here.
156,MCOL-1175,MCOL,Zdravelina Sokolovska,106148,2018-01-24 08:49:59,"You may try with
sudo ${COLUMNSTORE_INSTALL_DIR}/mariadb/columnstore/bin/setConfig CrossEngineSupport User Your_User_Name
sudo ${COLUMNSTORE_INSTALL_DIR}l/mariadb/columnstore/bin/setConfig CrossEngineSupport Password ""Your_Password""",1,"You may try with
sudo ${COLUMNSTORE_INSTALL_DIR}/mariadb/columnstore/bin/setConfig CrossEngineSupport User Your_User_Name
sudo ${COLUMNSTORE_INSTALL_DIR}l/mariadb/columnstore/bin/setConfig CrossEngineSupport Password ""Your_Password"""
157,MCOL-1175,MCOL,Andy Allaway,106183,2018-01-24 15:55:10,"Hi @winstone, that worked great, although, I still dont like entering free text into cmd prompt, is there some kind of way to make ./setConfig do something like:
./setConfig CrossEngineSupport Password -p
$please enter your password:
$******

Thanks,
Andy
",2,"Hi @winstone, that worked great, although, I still dont like entering free text into cmd prompt, is there some kind of way to make ./setConfig do something like:
./setConfig CrossEngineSupport Password -p
$please enter your password:
$******

Thanks,
Andy
"
158,MCOL-1175,MCOL,Roman,171807,2020-11-11 14:48:05,"I am curios if certificates could help with this issue?
There is a possibility to authenticate oneself against MDB using certificates not loging/password pair.
",3,"I am curios if certificates could help with this issue?
There is a possibility to authenticate oneself against MDB using certificates not loging/password pair.
"
159,MCOL-1175,MCOL,David Hall,194323,2021-07-13 20:30:46,QA: See the link https://mariadb.com/kb/en/mariadb-maxscale-25-encrypting-passwords/ for how to set up encryption.,4,QA: See the link URL for how to set up encryption.
160,MCOL-1175,MCOL,Daniel Lee,194422,2021-07-14 16:07:56,Build verified: 6.1.1 (#2796),5,Build verified: 6.1.1 (#2796)
161,MCOL-1179,MCOL,Jens Röwekamp,107445,2018-02-16 22:11:16,"Temporary fixes for MCOL-1218 included, that can be removed once MCOL-1218 is closed.",1,"Temporary fixes for MCOL-1218 included, that can be removed once MCOL-1218 is closed."
162,MCOL-1179,MCOL,Jens Röwekamp,107666,2018-02-22 00:17:18,"Influenced by MCOL-1228, need to be verified if PDI still wants to ALTER TEXT types once MCOL-1228 is fixed.",2,"Influenced by MCOL-1228, need to be verified if PDI still wants to ALTER TEXT types once MCOL-1228 is fixed."
163,MCOL-1179,MCOL,Jens Röwekamp,107667,2018-02-22 00:19:04,"Includes a temporary fix for MCOL-1213, which should be removed once MCOL-1213 is fixed.",3,"Includes a temporary fix for MCOL-1213, which should be removed once MCOL-1213 is fixed."
164,MCOL-1179,MCOL,Jens Röwekamp,107668,2018-02-22 00:21:12,All functionality for the beta release is implemented.,4,All functionality for the beta release is implemented.
165,MCOL-1179,MCOL,David Thompson,107914,2018-03-05 20:09:29,as discussed will add documentation for pentaho manual dep and cmake so plugin will build automatically. I'll test this out on a clean machine.,5,as discussed will add documentation for pentaho manual dep and cmake so plugin will build automatically. I'll test this out on a clean machine.
166,MCOL-1179,MCOL,David Thompson,107924,2018-03-05 21:21:29,Also can you back this out of api and move to data-adapters,6,Also can you back this out of api and move to data-adapters
167,MCOL-1179,MCOL,Jens Röwekamp,108189,2018-03-10 00:18:21,Influenced by MCOL-1257,7,Influenced by MCOL-1257
168,MCOL-1199,MCOL,Jens Röwekamp,106834,2018-02-05 21:53:54,"Created a subclass of java.lang.RuntimeException called ColumnStoreException to which a C++ ColumnStoreError is forwarded to. Added the related glue code in javamcsapi.i.

Added a simple test to the test suite that tries to access a nonexistent table and therefore throws a ColumnStoreException.

Exceptions of type java.lang.RuntimeException are unchecked. If we need to use checked Exceptions we have to specify each C++ function where a ColumnStoreError is thrown. Haven't found an easy way to do this automated with Swig. Therefore, I chose java.lang.RuntimeException. 

What do you think?",1,"Created a subclass of java.lang.RuntimeException called ColumnStoreException to which a C++ ColumnStoreError is forwarded to. Added the related glue code in javamcsapi.i.

Added a simple test to the test suite that tries to access a nonexistent table and therefore throws a ColumnStoreException.

Exceptions of type java.lang.RuntimeException are unchecked. If we need to use checked Exceptions we have to specify each C++ function where a ColumnStoreError is thrown. Haven't found an easy way to do this automated with Swig. Therefore, I chose java.lang.RuntimeException. 

What do you think?"
169,MCOL-1199,MCOL,Andrew Hutchings,106838,2018-02-05 22:09:18,"Looks good to me. If we figure out a better way later we can change it.

For QA: There is a test in the API's test suite for this.",2,"Looks good to me. If we figure out a better way later we can change it.

For QA: There is a test in the API's test suite for this."
170,MCOL-1199,MCOL,Daniel Lee,107006,2018-02-07 22:26:02,"Build verified: Github source
API
[root@localhost mariadb-columnstore-api]# git show
commit c1b2a3c972d868806f29469541db52cb43ffdd45
Merge: 53fcb95 17fa76f
Author: David Thompson <dthompson@mariadb.com>
Date: Tue Feb 6 10:05:44 2018 -0800
Merge pull request #49 from mariadb-corporation/MCOL-1200
MCOL-1200 C++ Exceptions of type ColumnStoreError are now forwarded t…
MCS 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e5499e513d88a3dfefbe9a356e20a1bceb1bde38
Merge: 99cdb0a 4840a43
Author: david hill <david.hill@mariadb.com>
Date: Wed Jan 31 16:53:52 2018 -0600
Merge pull request #92 from mariadb-corporation/MCOL-1152
MCOL-1152: Change the debian package names.
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ffcc4e94ccd07b6bbdfc47531c4c2f065f3fab47
Merge: 05431bf 3fb5dde
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Mon Feb 5 22:37:05 2018 -0600
Merge pull request #398 from mariadb-corporation/statuscontrol-fix
add ProcStatusControl to postConfigure setup
Made build and performance build-in test suite.",3,"Build verified: Github source
API
[root@localhost mariadb-columnstore-api]# git show
commit c1b2a3c972d868806f29469541db52cb43ffdd45
Merge: 53fcb95 17fa76f
Author: David Thompson 
Date: Tue Feb 6 10:05:44 2018 -0800
Merge pull request #49 from mariadb-corporation/MCOL-1200
MCOL-1200 C++ Exceptions of type ColumnStoreError are now forwarded t…
MCS 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e5499e513d88a3dfefbe9a356e20a1bceb1bde38
Merge: 99cdb0a 4840a43
Author: david hill 
Date: Wed Jan 31 16:53:52 2018 -0600
Merge pull request #92 from mariadb-corporation/MCOL-1152
MCOL-1152: Change the debian package names.
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ffcc4e94ccd07b6bbdfc47531c4c2f065f3fab47
Merge: 05431bf 3fb5dde
Author: benthompson15 
Date: Mon Feb 5 22:37:05 2018 -0600
Merge pull request #398 from mariadb-corporation/statuscontrol-fix
add ProcStatusControl to postConfigure setup
Made build and performance build-in test suite."
171,MCOL-1200,MCOL,Jens Röwekamp,106841,2018-02-05 22:59:40,"C++ Exceptions of type ColumnStoreError are now forwarded as Python exceptions of type RuntimeError.

Added a test case in the test suite for QA.",1,"C++ Exceptions of type ColumnStoreError are now forwarded as Python exceptions of type RuntimeError.

Added a test case in the test suite for QA."
172,MCOL-1200,MCOL,Andrew Hutchings,106875,2018-02-06 11:33:23,[~dthompson] left a comment on the review so I'll wait until that is resolved,2,[~dthompson] left a comment on the review so I'll wait until that is resolved
173,MCOL-1200,MCOL,Andrew Hutchings,106907,2018-02-06 18:07:58,David has merged.,3,David has merged.
174,MCOL-1200,MCOL,Daniel Lee,107005,2018-02-07 22:24:33,"Build verified: Github source

API

[root@localhost mariadb-columnstore-api]# git show
commit c1b2a3c972d868806f29469541db52cb43ffdd45
Merge: 53fcb95 17fa76f
Author: David Thompson <dthompson@mariadb.com>
Date: Tue Feb 6 10:05:44 2018 -0800
Merge pull request #49 from mariadb-corporation/MCOL-1200
MCOL-1200 C++ Exceptions of type ColumnStoreError are now forwarded t…
MCS 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e5499e513d88a3dfefbe9a356e20a1bceb1bde38
Merge: 99cdb0a 4840a43
Author: david hill <david.hill@mariadb.com>
Date: Wed Jan 31 16:53:52 2018 -0600
Merge pull request #92 from mariadb-corporation/MCOL-1152
MCOL-1152: Change the debian package names.
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ffcc4e94ccd07b6bbdfc47531c4c2f065f3fab47
Merge: 05431bf 3fb5dde
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Mon Feb 5 22:37:05 2018 -0600
Merge pull request #398 from mariadb-corporation/statuscontrol-fix
add ProcStatusControl to postConfigure setup

Made build performed built-in test suite.",4,"Build verified: Github source

API

[root@localhost mariadb-columnstore-api]# git show
commit c1b2a3c972d868806f29469541db52cb43ffdd45
Merge: 53fcb95 17fa76f
Author: David Thompson 
Date: Tue Feb 6 10:05:44 2018 -0800
Merge pull request #49 from mariadb-corporation/MCOL-1200
MCOL-1200 C++ Exceptions of type ColumnStoreError are now forwarded t…
MCS 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e5499e513d88a3dfefbe9a356e20a1bceb1bde38
Merge: 99cdb0a 4840a43
Author: david hill 
Date: Wed Jan 31 16:53:52 2018 -0600
Merge pull request #92 from mariadb-corporation/MCOL-1152
MCOL-1152: Change the debian package names.
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ffcc4e94ccd07b6bbdfc47531c4c2f065f3fab47
Merge: 05431bf 3fb5dde
Author: benthompson15 
Date: Mon Feb 5 22:37:05 2018 -0600
Merge pull request #398 from mariadb-corporation/statuscontrol-fix
add ProcStatusControl to postConfigure setup

Made build performed built-in test suite."
175,MCOL-1201,MCOL,David Hall,114359,2018-07-23 20:07:57,"To test, use the function regr_avgx, which has been included as a test function.

regr_avgx(y,x) takes two arguments. The x argument must be numeric. It should error out if it is not. The y can be anything. This function should also work with constants in either position. It should error if it gets the wrong number of arguments.

If regr_avgx() shows as not a function then, it must be created:
CREATE AGGREGATE FUNCTION regr_avgx RETURNS REAL soname='libudf_mysql.so';

It works just like avg() and should be able to pass any test avg() can do. The difference is, if y OR x is NULL, the x value is not considered as part of the avg.

regr_avgx() doesn't fully test the capabilities of this feature. It will be further tested as part of MCOL-521 as more functions are added to the library.",1,"To test, use the function regr_avgx, which has been included as a test function.

regr_avgx(y,x) takes two arguments. The x argument must be numeric. It should error out if it is not. The y can be anything. This function should also work with constants in either position. It should error if it gets the wrong number of arguments.

If regr_avgx() shows as not a function then, it must be created:
CREATE AGGREGATE FUNCTION regr_avgx RETURNS REAL soname='libudf_mysql.so';

It works just like avg() and should be able to pass any test avg() can do. The difference is, if y OR x is NULL, the x value is not considered as part of the avg.

regr_avgx() doesn't fully test the capabilities of this feature. It will be further tested as part of MCOL-521 as more functions are added to the library."
176,MCOL-1201,MCOL,Daniel Lee,117264,2018-10-01 21:30:30,"Build tested: Github source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Thu Sep 27 20:37:03 2018 +0100
Merge pull request #134 from mariadb-corporation/versionCmakeFix
port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3326be00de5f53ec365910f07a7fd882ba193d4d
Merge: ebbeb30 5cab6c4
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Tue Sep 18 13:57:17 2018 +0100
Merge pull request #565 from drrtuy/MCOL-1601
MCOL-1601 GROUP BY now supports subqueries in HAVING.

For now, I am just testing the regr_avgx function that's built into the distribution, 

The following test seemed to be working fine with ColumnStore tables, except that the result returned as decimal values, instead of integer values.

MariaDB [mytest]> create table TestTable (
    ->     x    integer,
    ->     y    integer
    ->         ) engine=columnstore;
Query OK, 0 rows affected (0.094 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
+------+------+
9 rows in set (0.048 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x,y) FROM TestTable;
+----------------+
| REGR_AVGX(x,y) |
+----------------+
|        24.2222 |
+----------------+
1 row in set (0.013 sec)


MariaDB [mytest]> insert into TestTable values (10, NULL );
Query OK, 1 row affected (0.032 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
+------+------+
10 rows in set (0.017 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.013 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x,y) FROM TestTable;
+----------------+
| REGR_AVGX(x,y) |
+----------------+
|        24.2222 |
+----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> insert into TestTable values (NULL, 94 );
Query OK, 1 row affected (0.033 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
| NULL |   94 |
+------+------+
11 rows in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x, y) FROM TestTable;
+-----------------+
| REGR_AVGX(x, y) |
+-----------------+
|         24.2222 |
+-----------------+
1 row in set (0.019 sec)

MariaDB [mytest]> insert into TestTable values (NULL, NULL );
Query OK, 1 row affected (0.035 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
| NULL |   94 |
| NULL | NULL |
+------+------+
12 rows in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x, y) FROM TestTable;
+-----------------+
| REGR_AVGX(x, y) |
+-----------------+
|         24.2222 |
+-----------------+
1 row in set (0.010 sec)


For InnoDB tables, the function caused mysqld to crash when column values are NULL.

MariaDB [mytest]> create table TestTable (
    ->     x    integer,
    ->     y    integer
    ->         ) engine=innodb;

.
.
.
MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
| NULL |   94 |
+------+------+
11 rows in set (0.002 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x,y) FROM TestTable;
ERROR 2013 (HY000): Lost connection to MySQL server during query
MariaDB [mytest]> SELECT REGR_AVGX(y,x) FROM TestTable;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    9
Current database: mytest

ERROR 2013 (HY000): Lost connection to MySQL server during query
MariaDB [mytest]> 





",2,"Build tested: Github source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date: Thu Sep 27 20:37:03 2018 +0100
Merge pull request #134 from mariadb-corporation/versionCmakeFix
port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3326be00de5f53ec365910f07a7fd882ba193d4d
Merge: ebbeb30 5cab6c4
Author: Andrew Hutchings 
Date: Tue Sep 18 13:57:17 2018 +0100
Merge pull request #565 from drrtuy/MCOL-1601
MCOL-1601 GROUP BY now supports subqueries in HAVING.

For now, I am just testing the regr_avgx function that's built into the distribution, 

The following test seemed to be working fine with ColumnStore tables, except that the result returned as decimal values, instead of integer values.

MariaDB [mytest]> create table TestTable (
    ->     x    integer,
    ->     y    integer
    ->         ) engine=columnstore;
Query OK, 0 rows affected (0.094 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
+------+------+
9 rows in set (0.048 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x,y) FROM TestTable;
+----------------+
| REGR_AVGX(x,y) |
+----------------+
|        24.2222 |
+----------------+
1 row in set (0.013 sec)


MariaDB [mytest]> insert into TestTable values (10, NULL );
Query OK, 1 row affected (0.032 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
+------+------+
10 rows in set (0.017 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.013 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x,y) FROM TestTable;
+----------------+
| REGR_AVGX(x,y) |
+----------------+
|        24.2222 |
+----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> insert into TestTable values (NULL, 94 );
Query OK, 1 row affected (0.033 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
| NULL |   94 |
+------+------+
11 rows in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x, y) FROM TestTable;
+-----------------+
| REGR_AVGX(x, y) |
+-----------------+
|         24.2222 |
+-----------------+
1 row in set (0.019 sec)

MariaDB [mytest]> insert into TestTable values (NULL, NULL );
Query OK, 1 row affected (0.035 sec)

MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
| NULL |   94 |
| NULL | NULL |
+------+------+
12 rows in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(y, x) FROM TestTable;
+-----------------+
| REGR_AVGX(y, x) |
+-----------------+
|          5.0000 |
+-----------------+
1 row in set (0.010 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x, y) FROM TestTable;
+-----------------+
| REGR_AVGX(x, y) |
+-----------------+
|         24.2222 |
+-----------------+
1 row in set (0.010 sec)


For InnoDB tables, the function caused mysqld to crash when column values are NULL.

MariaDB [mytest]> create table TestTable (
    ->     x    integer,
    ->     y    integer
    ->         ) engine=innodb;

.
.
.
MariaDB [mytest]> select * from TestTable;
+------+------+
| x    | y    |
+------+------+
|    1 |    7 |
|    2 |    1 |
|    3 |    2 |
|    4 |    5 |
|    5 |    7 |
|    6 |   34 |
|    7 |   32 |
|    8 |   43 |
|    9 |   87 |
|   10 | NULL |
| NULL |   94 |
+------+------+
11 rows in set (0.002 sec)

MariaDB [mytest]> SELECT REGR_AVGX(x,y) FROM TestTable;
ERROR 2013 (HY000): Lost connection to MySQL server during query
MariaDB [mytest]> SELECT REGR_AVGX(y,x) FROM TestTable;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    9
Current database: mytest

ERROR 2013 (HY000): Lost connection to MySQL server during query
MariaDB [mytest]> 





"
177,MCOL-1201,MCOL,Roman,117714,2018-10-10 18:35:43,"Tested with 1000 columns UDAF:
{noformat}
MariaDB [test]> select five_piece(c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63,c64,c65,c66,c67,c68,c69,c70,c71,c72,c73,c74,c75,c76,c77,c78,c79,c80,c81,c82,c83,c84,c85,c86,c87,c88,c89,c90,c91,c92,c93,c94,c95,c96,c97,c98,c99,c100,c101,c102,c103,c104,c105,c106,c107,c108,c109,c110,c111,c112,c113,c114,c115,c116,c117,c118,c119,c120,c121,c122,c123,c124,c125,c126,c127,c128,c129,c130,c131,c132,c133,c134,c135,c136,c137,c138,c139,c140,c141,c142,c143,c144,c145,c146,c147,c148,c149,c150,c151,c152,c153,c154,c155,c156,c157,c158,c159,c160,c161,c162,c163,c164,c165,c166,c167,c168,c169,c170,c171,c172,c173,c174,c175,c176,c177,c178,c179,c180,c181,c182,c183,c184,c185,c186,c187,c188,c189,c190,c191,c192,c193,c194,c195,c196,c197,c198,c199,c200,c201,c202,c203,c204,c205,c206,c207,c208,c209,c210,c211,c212,c213,c214,c215,c216,c217,c218,c219,c220,c221,c222,c223,c224,c225,c226,c227,c228,c229,c230,c231,c232,c233,c234,c235,c236,c237,c238,c239,c240,c241,c242,c243,c244,c245,c246,c247,c248,c249,c250,c251,c252,c253,c254,c255,c256,c257,c258,c259,c260,c261,c262,c263,c264,c265,c266,c267,c268,c269,c270,c271,c272,c273,c274,c275,c276,c277,c278,c279,c280,c281,c282,c283,c284,c285,c286,c287,c288,c289,c290,c291,c292,c293,c294,c295,c296,c297,c298,c299,c300,c301,c302,c303,c304,c305,c306,c307,c308,c309,c310,c311,c312,c313,c314,c315,c316,c317,c318,c319,c320,c321,c322,c323,c324,c325,c326,c327,c328,c329,c330,c331,c332,c333,c334,c335,c336,c337,c338,c339,c340,c341,c342,c343,c344,c345,c346,c347,c348,c349,c350,c351,c352,c353,c354,c355,c356,c357,c358,c359,c360,c361,c362,c363,c364,c365,c366,c367,c368,c369,c370,c371,c372,c373,c374,c375,c376,c377,c378,c379,c380,c381,c382,c383,c384,c385,c386,c387,c388,c389,c390,c391,c392,c393,c394,c395,c396,c397,c398,c399,c400,c401,c402,c403,c404,c405,c406,c407,c408,c409,c410,c411,c412,c413,c414,c415,c416,c417,c418,c419,c420,c421,c422,c423,c424,c425,c426,c427,c428,c429,c430,c431,c432,c433,c434,c435,c436,c437,c438,c439,c440,c441,c442,c443,c444,c445,c446,c447,c448,c449,c450,c451,c452,c453,c454,c455,c456,c457,c458,c459,c460,c461,c462,c463,c464,c465,c466,c467,c468,c469,c470,c471,c472,c473,c474,c475,c476,c477,c478,c479,c480,c481,c482,c483,c484,c485,c486,c487,c488,c489,c490,c491,c492,c493,c494,c495,c496,c497,c498,c499,c500,c501,c502,c503,c504,c505,c506,c507,c508,c509,c510,c511,c512,c513,c514,c515,c516,c517,c518,c519,c520,c521,c522,c523,c524,c525,c526,c527,c528,c529,c530,c531,c532,c533,c534,c535,c536,c537,c538,c539,c540,c541,c542,c543,c544,c545,c546,c547,c548,c549,c550,c551,c552,c553,c554,c555,c556,c557,c558,c559,c560,c561,c562,c563,c564,c565,c566,c567,c568,c569,c570,c571,c572,c573,c574,c575,c576,c577,c578,c579,c580,c581,c582,c583,c584,c585,c586,c587,c588,c589,c590,c591,c592,c593,c594,c595,c596,c597,c598,c599,c600,c601,c602,c603,c604,c605,c606,c607,c608,c609,c610,c611,c612,c613,c614,c615,c616,c617,c618,c619,c620,c621,c622,c623,c624,c625,c626,c627,c628,c629,c630,c631,c632,c633,c634,c635,c636,c637,c638,c639,c640,c641,c642,c643,c644,c645,c646,c647,c648,c649,c650,c651,c652,c653,c654,c655,c656,c657,c658,c659,c660,c661,c662,c663,c664,c665,c666,c667,c668,c669,c670,c671,c672,c673,c674,c675,c676,c677,c678,c679,c680,c681,c682,c683,c684,c685,c686,c687,c688,c689,c690,c691,c692,c693,c694,c695,c696,c697,c698,c699,c700,c701,c702,c703,c704,c705,c706,c707,c708,c709,c710,c711,c712,c713,c714,c715,c716,c717,c718,c719,c720,c721,c722,c723,c724,c725,c726,c727,c728,c729,c730,c731,c732,c733,c734,c735,c736,c737,c738,c739,c740,c741,c742,c743,c744,c745,c746,c747,c748,c749,c750,c751,c752,c753,c754,c755,c756,c757,c758,c759,c760,c761,c762,c763,c764,c765,c766,c767,c768,c769,c770,c771,c772,c773,c774,c775,c776,c777,c778,c779,c780,c781,c782,c783,c784,c785,c786,c787,c788,c789,c790,c791,c792,c793,c794,c795,c796,c797,c798,c799,c800,c801,c802,c803,c804,c805,c806,c807,c808,c809,c810,c811,c812,c813,c814,c815,c816,c817,c818,c819,c820,c821,c822,c823,c824,c825,c826,c827,c828,c829,c830,c831,c832,c833,c834,c835,c836,c837,c838,c839,c840,c841,c842,c843,c844,c845,c846,c847,c848,c849,c850,c851,c852,c853,c854,c855,c856,c857,c858,c859,c860,c861,c862,c863,c864,c865,c866,c867,c868,c869,c870,c871,c872,c873,c874,c875,c876,c877,c878,c879,c880,c881,c882,c883,c884,c885,c886,c887,c888,c889,c890,c891,c892,c893,c894,c895,c896,c897,c898,c899,c900,c901,c902,c903,c904,c905,c906,c907,c908,c909,c910,c911,c912,c913,c914,c915,c916,c917,c918,c919,c920,c921,c922,c923,c924,c925,c926,c927,c928,c929,c930,c931,c932,c933,c934,c935,c936,c937,c938,c939,c940,c941,c942,c943,c944,c945,c946,c947,c948,c949,c950,c951,c952,c953,c954,c955,c956,c957,c958,c959,c960,c961,c962,c963,c964,c965,c966,c967,c968,c969,c970,c971,c972,c973,c974,c975,c976,c977,c978,c979,c980,c981,c982,c983,c984,c985,c986,c987,c988,c989,c990,c991,c992,c993,c994,c995,c996,c997,c998,c999) from cs1001;
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| five_piece(c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63 |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                         499.5000 |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{noformat}",3,"Tested with 1000 columns UDAF:
{noformat}
MariaDB [test]> select five_piece(c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63,c64,c65,c66,c67,c68,c69,c70,c71,c72,c73,c74,c75,c76,c77,c78,c79,c80,c81,c82,c83,c84,c85,c86,c87,c88,c89,c90,c91,c92,c93,c94,c95,c96,c97,c98,c99,c100,c101,c102,c103,c104,c105,c106,c107,c108,c109,c110,c111,c112,c113,c114,c115,c116,c117,c118,c119,c120,c121,c122,c123,c124,c125,c126,c127,c128,c129,c130,c131,c132,c133,c134,c135,c136,c137,c138,c139,c140,c141,c142,c143,c144,c145,c146,c147,c148,c149,c150,c151,c152,c153,c154,c155,c156,c157,c158,c159,c160,c161,c162,c163,c164,c165,c166,c167,c168,c169,c170,c171,c172,c173,c174,c175,c176,c177,c178,c179,c180,c181,c182,c183,c184,c185,c186,c187,c188,c189,c190,c191,c192,c193,c194,c195,c196,c197,c198,c199,c200,c201,c202,c203,c204,c205,c206,c207,c208,c209,c210,c211,c212,c213,c214,c215,c216,c217,c218,c219,c220,c221,c222,c223,c224,c225,c226,c227,c228,c229,c230,c231,c232,c233,c234,c235,c236,c237,c238,c239,c240,c241,c242,c243,c244,c245,c246,c247,c248,c249,c250,c251,c252,c253,c254,c255,c256,c257,c258,c259,c260,c261,c262,c263,c264,c265,c266,c267,c268,c269,c270,c271,c272,c273,c274,c275,c276,c277,c278,c279,c280,c281,c282,c283,c284,c285,c286,c287,c288,c289,c290,c291,c292,c293,c294,c295,c296,c297,c298,c299,c300,c301,c302,c303,c304,c305,c306,c307,c308,c309,c310,c311,c312,c313,c314,c315,c316,c317,c318,c319,c320,c321,c322,c323,c324,c325,c326,c327,c328,c329,c330,c331,c332,c333,c334,c335,c336,c337,c338,c339,c340,c341,c342,c343,c344,c345,c346,c347,c348,c349,c350,c351,c352,c353,c354,c355,c356,c357,c358,c359,c360,c361,c362,c363,c364,c365,c366,c367,c368,c369,c370,c371,c372,c373,c374,c375,c376,c377,c378,c379,c380,c381,c382,c383,c384,c385,c386,c387,c388,c389,c390,c391,c392,c393,c394,c395,c396,c397,c398,c399,c400,c401,c402,c403,c404,c405,c406,c407,c408,c409,c410,c411,c412,c413,c414,c415,c416,c417,c418,c419,c420,c421,c422,c423,c424,c425,c426,c427,c428,c429,c430,c431,c432,c433,c434,c435,c436,c437,c438,c439,c440,c441,c442,c443,c444,c445,c446,c447,c448,c449,c450,c451,c452,c453,c454,c455,c456,c457,c458,c459,c460,c461,c462,c463,c464,c465,c466,c467,c468,c469,c470,c471,c472,c473,c474,c475,c476,c477,c478,c479,c480,c481,c482,c483,c484,c485,c486,c487,c488,c489,c490,c491,c492,c493,c494,c495,c496,c497,c498,c499,c500,c501,c502,c503,c504,c505,c506,c507,c508,c509,c510,c511,c512,c513,c514,c515,c516,c517,c518,c519,c520,c521,c522,c523,c524,c525,c526,c527,c528,c529,c530,c531,c532,c533,c534,c535,c536,c537,c538,c539,c540,c541,c542,c543,c544,c545,c546,c547,c548,c549,c550,c551,c552,c553,c554,c555,c556,c557,c558,c559,c560,c561,c562,c563,c564,c565,c566,c567,c568,c569,c570,c571,c572,c573,c574,c575,c576,c577,c578,c579,c580,c581,c582,c583,c584,c585,c586,c587,c588,c589,c590,c591,c592,c593,c594,c595,c596,c597,c598,c599,c600,c601,c602,c603,c604,c605,c606,c607,c608,c609,c610,c611,c612,c613,c614,c615,c616,c617,c618,c619,c620,c621,c622,c623,c624,c625,c626,c627,c628,c629,c630,c631,c632,c633,c634,c635,c636,c637,c638,c639,c640,c641,c642,c643,c644,c645,c646,c647,c648,c649,c650,c651,c652,c653,c654,c655,c656,c657,c658,c659,c660,c661,c662,c663,c664,c665,c666,c667,c668,c669,c670,c671,c672,c673,c674,c675,c676,c677,c678,c679,c680,c681,c682,c683,c684,c685,c686,c687,c688,c689,c690,c691,c692,c693,c694,c695,c696,c697,c698,c699,c700,c701,c702,c703,c704,c705,c706,c707,c708,c709,c710,c711,c712,c713,c714,c715,c716,c717,c718,c719,c720,c721,c722,c723,c724,c725,c726,c727,c728,c729,c730,c731,c732,c733,c734,c735,c736,c737,c738,c739,c740,c741,c742,c743,c744,c745,c746,c747,c748,c749,c750,c751,c752,c753,c754,c755,c756,c757,c758,c759,c760,c761,c762,c763,c764,c765,c766,c767,c768,c769,c770,c771,c772,c773,c774,c775,c776,c777,c778,c779,c780,c781,c782,c783,c784,c785,c786,c787,c788,c789,c790,c791,c792,c793,c794,c795,c796,c797,c798,c799,c800,c801,c802,c803,c804,c805,c806,c807,c808,c809,c810,c811,c812,c813,c814,c815,c816,c817,c818,c819,c820,c821,c822,c823,c824,c825,c826,c827,c828,c829,c830,c831,c832,c833,c834,c835,c836,c837,c838,c839,c840,c841,c842,c843,c844,c845,c846,c847,c848,c849,c850,c851,c852,c853,c854,c855,c856,c857,c858,c859,c860,c861,c862,c863,c864,c865,c866,c867,c868,c869,c870,c871,c872,c873,c874,c875,c876,c877,c878,c879,c880,c881,c882,c883,c884,c885,c886,c887,c888,c889,c890,c891,c892,c893,c894,c895,c896,c897,c898,c899,c900,c901,c902,c903,c904,c905,c906,c907,c908,c909,c910,c911,c912,c913,c914,c915,c916,c917,c918,c919,c920,c921,c922,c923,c924,c925,c926,c927,c928,c929,c930,c931,c932,c933,c934,c935,c936,c937,c938,c939,c940,c941,c942,c943,c944,c945,c946,c947,c948,c949,c950,c951,c952,c953,c954,c955,c956,c957,c958,c959,c960,c961,c962,c963,c964,c965,c966,c967,c968,c969,c970,c971,c972,c973,c974,c975,c976,c977,c978,c979,c980,c981,c982,c983,c984,c985,c986,c987,c988,c989,c990,c991,c992,c993,c994,c995,c996,c997,c998,c999) from cs1001;
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| five_piece(c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63 |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                         499.5000 |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{noformat}"
178,MCOL-1201,MCOL,Daniel Lee,117764,2018-10-11 15:49:38,"Build verified: 1.2 source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 39c283281af045e5b5fb3fe3f399b21a6b1236ca
Merge: 46775f8 19c8a2b
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Wed Oct 10 20:11:12 2018 +0300

    Merge pull request #588 from mariadb-corporation/MCOL-266
    
    MCOL-266 Support true/false DDL default values

The mysqld crashing issue has been fixed.
",4,"Build verified: 1.2 source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 39c283281af045e5b5fb3fe3f399b21a6b1236ca
Merge: 46775f8 19c8a2b
Author: Roman Nozdrin 
Date:   Wed Oct 10 20:11:12 2018 +0300

    Merge pull request #588 from mariadb-corporation/MCOL-266
    
    MCOL-266 Support true/false DDL default values

The mysqld crashing issue has been fixed.
"
179,MCOL-1206,MCOL,Daniel Lee,106927,2018-02-07 02:10:32,"Build verified: 1.0.13-1 Github source

/root/columnstore/mariadb-columnstore-server
commit da2e7b8cfaf852e08187d3ebcfddcc3482bded86
Merge: e5b122c adc20a8
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Feb 6 17:17:29 2018 -0600

    Merge pull request #93 from mariadb-corporation/MCOL-1206
    
    MCOL-1206 Merge MariaDB 10.1.31

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 06979c8d1b06d1d8640dba1f53144f9f53de9a58
Merge: cc2bbe8 def46ca
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Jan 22 14:17:38 2018 -0600

    Merge pull request #378 from mariadb-corporation/RELEASE
    
    update version
",1,"Build verified: 1.0.13-1 Github source

/root/columnstore/mariadb-columnstore-server
commit da2e7b8cfaf852e08187d3ebcfddcc3482bded86
Merge: e5b122c adc20a8
Author: David.Hall 
Date:   Tue Feb 6 17:17:29 2018 -0600

    Merge pull request #93 from mariadb-corporation/MCOL-1206
    
    MCOL-1206 Merge MariaDB 10.1.31

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 06979c8d1b06d1d8640dba1f53144f9f53de9a58
Merge: cc2bbe8 def46ca
Author: benthompson15 
Date:   Mon Jan 22 14:17:38 2018 -0600

    Merge pull request #378 from mariadb-corporation/RELEASE
    
    update version
"
180,MCOL-1232,MCOL,Jens Röwekamp,108927,2018-03-26 16:21:29,"test added in the regression suite.

Scala needs at least 5GiB of memory to pass.",1,"test added in the regression suite.

Scala needs at least 5GiB of memory to pass."
181,MCOL-1242,MCOL,Sasha V,107971,2018-03-06 14:49:14,"Could you please not extend exiting cpimport with this new functionality, but provide a separate new tool for that, say, mcsimport?",1,"Could you please not extend exiting cpimport with this new functionality, but provide a separate new tool for that, say, mcsimport?"
182,MCOL-1242,MCOL,Jens Röwekamp,107979,2018-03-06 18:21:08,"Yes, it is planned to be a separate tool",2,"Yes, it is planned to be a separate tool"
183,MCOL-1242,MCOL,Jens Röwekamp,116920,2018-09-21 01:34:19,As discussed with project management Mac OS support and the support for different Columnstore (write) modes is scoped in additional tickets and not part of this ticket any more.,3,As discussed with project management Mac OS support and the support for different Columnstore (write) modes is scoped in additional tickets and not part of this ticket any more.
184,MCOL-1242,MCOL,Jens Röwekamp,117029,2018-09-25 00:15:07,"mcsimport for Columnstore 1.2 on Linux and Windows.

Test suite executed successfully on Windows 10, Debian 9, and CentOS 7 using mcsapi 1.2.0 \[1\]  for the build, and ColumnStore 1.2.0 running on CentOS 7.

For QA:
- review if included tests are sufficient or if there are any test cases missing
- execute test suite (through ctest or the python script directly)

mcsimport needs to be build with mcsapi 1.2.0 and tested against ColumnStore 1.2.0.


\[1\] 95c70691eaf9b8ec7571ae3df3e30c97c5bc5573",4,"mcsimport for Columnstore 1.2 on Linux and Windows.

Test suite executed successfully on Windows 10, Debian 9, and CentOS 7 using mcsapi 1.2.0 \[1\]  for the build, and ColumnStore 1.2.0 running on CentOS 7.

For QA:
- review if included tests are sufficient or if there are any test cases missing
- execute test suite (through ctest or the python script directly)

mcsimport needs to be build with mcsapi 1.2.0 and tested against ColumnStore 1.2.0.


\[1\] 95c70691eaf9b8ec7571ae3df3e30c97c5bc5573"
185,MCOL-1242,MCOL,Andrew Hutchings,117335,2018-10-03 08:38:52,[~jens.rowekamp] can you please add something that will stop cpimport from being built on CentOS 6?,5,[~jens.rowekamp] can you please add something that will stop cpimport from being built on CentOS 6?
186,MCOL-1242,MCOL,Jens Röwekamp,117369,2018-10-03 17:40:20,"[~LinuxJedi] Sure. I just documented the cmake flags in README.md.

Use:
{code:shell}
cmake -DREMOTE_CPIMPORT=OFF .
{code}

to avoid building remote cpimport on CentOS 6.",6,"[~LinuxJedi] Sure. I just documented the cmake flags in README.md.

Use:
{code:shell}
cmake -DREMOTE_CPIMPORT=OFF .
{code}

to avoid building remote cpimport on CentOS 6."
187,MCOL-1242,MCOL,Andrew Hutchings,117500,2018-10-06 10:35:22,Still breaks CentOS 6 due to requiring a C++11 compiler which CentOS doesn't have. Also still appears to break all the other builders.,7,Still breaks CentOS 6 due to requiring a C++11 compiler which CentOS doesn't have. Also still appears to break all the other builders.
188,MCOL-1242,MCOL,Jens Röwekamp,117567,2018-10-08 23:04:55,"Buildbot errors were mostly caused due to removed packages.sh. I restored a modified version of packages.sh that doesn't include the folders mcsimport, build, resources and cmake to the binary package.
Currently we distributed one binary package with all platform independent shell scripts. Therefore, I don't know if we want to include mcsimport into it or not.

The buildbot errors for CentOS 6 and Suse are caused by not using the -DREMOTE_CPIMPORT=OFF flag while invoking cmake.

On CentOS 6 we can't build remote cpimport as it requires mcsimport which depends on C++ 11. On Suse buildbot currently fails as buildbot doesn't build and install mcsapi prior building the tools.

I tested the package build on CentOS 6 with above mentioned -DREMOTE_CPIMPORT=OFF flag on a local VM and the rpm package was built successfully.

{code:shell}
[jens@centos6 ~]$ cat /etc/centos-release
CentOS release 6.10 (Final)
[jens@centos6 ~]$ git clone git@github.com:mariadb-corporation/mariadb-columnstore-tools.git
Initialized empty Git repository in /home/jens/mariadb-columnstore-tools/.git/
remote: Enumerating objects: 251, done.
remote: Counting objects: 100% (251/251), done.
remote: Compressing objects: 100% (133/133), done.
remote: Total 476 (delta 113), reused 228 (delta 100), pack-reused 225
Receiving objects: 100% (476/476), 179.76 KiB, done.
Resolving deltas: 100% (229/229), done.
[jens@centos6 ~]$ cd mariadb-columnstore-tools/
[jens@centos6 mariadb-columnstore-tools]$ git checkout MCOL-1242
Branch MCOL-1242 set up to track remote branch MCOL-1242 from origin.
Switched to a new branch 'MCOL-1242'
[jens@centos6 mariadb-columnstore-tools]$ ls
backuprestore  cmake  CMakeLists.txt  COPYRIGHT.txt  LICENSE.txt  mcsimport  monitoring  README.md  resources  VERSION
[jens@centos6 mariadb-columnstore-tools]$ mkdir build && cd build
[jens@centos6 build]$ cmake .. -DREMOTE_CPIMPORT=OFF
-- The C compiler identification is GNU 4.4.7
-- The CXX compiler identification is GNU 4.4.7
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Running cmake version 2.8.12.2
-- MariaDB-Columnstore 1.2.0
-- -----------------------------------------------
-- CMAKE_INSTALL_PREFIX = /usr/local/mariadb/columnstore
-- CMAKE_BUILD_TYPE =
-- TEST_RUNNER = OFF
-- BACKUPRESTORE = ON
-- MONITORING = ON
-- REMOTE_CPIMPORT = OFF
-- RPM = OFF
-- DEB = OFF
-- Change a values with: cmake -D<Variable>=<Value>
-- ------------------------------------------------
--
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jens/mariadb-columnstore-tools/build
[jens@centos6 build]$ make
[jens@centos6 build]$ cmake .. -DRPM=centos6
-- Running cmake version 2.8.12.2
-- MariaDB-Columnstore 1.2.0
-- -----------------------------------------------
-- CMAKE_INSTALL_PREFIX = /usr/local/mariadb/columnstore
-- CMAKE_BUILD_TYPE =
-- TEST_RUNNER = OFF
-- BACKUPRESTORE = ON
-- MONITORING = ON
-- REMOTE_CPIMPORT = OFF
-- RPM = centos6
-- DEB = OFF
-- Change a values with: cmake -D<Variable>=<Value>
-- ------------------------------------------------
--
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jens/mariadb-columnstore-tools/build
[jens@centos6 build]$ sudo make package
[sudo] password for jens:
Run CPack packaging tool...
CPack: Create package using RPM
CPack: Install projects
CPack: - Run preinstall target for: Project
CPack: - Install project: Project
CPack: Create package
CPackRPM:Debug: rpmbuild version is <4.8.0>
CPackRPM:Debug: LSB_RELEASE  = lsb_release not installed/found!
CPackRPM:Debug: processing URL
CPackRPM:Debug: using CPACK_RPM_PACKAGE_URL
CPackRPM:Debug: User defined Url:
 http://www.mariadb.com
CPackRPM:Debug: processing REQUIRES
CPackRPM:Debug: using CPACK_RPM_PACKAGE_REQUIRES
CPackRPM:Debug: User defined Requires:
  libxml2 rsync
CPackRPM:Debug: processing SUGGESTS
CPackRPM:Debug: processing PROVIDES
CPackRPM:Debug: using CPACK_RPM_PACKAGE_PROVIDES
CPackRPM:Debug: User defined Provides:
  mariadb-columnstore-tools
CPackRPM:Debug: processing OBSOLETES
CPackRPM:Debug: processing PREFIX
CPackRPM:Debug: processing CONFLICTS
CPackRPM:Debug: processing AUTOPROV
CPackRPM:Debug: processing AUTOREQ
CPackRPM:Debug: processing AUTOREQPROV
CPackRPM:Debug: User defined more define spec line specified:

        %define ignore #

CPackRPM:Debug: Initial list of path to OMIT in RPM:
CPackRPM:Debug: CPACK_RPM_EXCLUDE_FROM_AUTO_FILELIST= /etc;/etc/init.d;/usr;/usr/share;/usr/share/doc;/usr/bin;/usr/lib;/usr/lib64;/usr/include
CPackRPM:Debug: Final list of path to OMIT in RPM: -o;-path;./etc;-o;-path;./etc/init.d;-o;-path;./usr;-o;-path;./usr/share;-o;-path;./usr/share/doc;-o;-path;./usr/bin;-o;-path;./usr/lib;-o;-path;./usr/lib64;-o;-path;./usr/include
CPackRPM:Debug: Handling User Filelist: </usr/local/mariadb/columnstore/tools/COPYRIGHT.txt;/usr/local/mariadb/columnstore/tools/LICENSE.txt;/usr/local/mariadb/columnstore/tools/README.md;/usr/local/mariadb/columnstore/tools/VERSION;/usr/local/mariadb/columnstore/tools/backuprestore/columnstoreBackup;/usr/local/mariadb/columnstore/tools/backuprestore/columnstoreRestore;/usr/local/mariadb/columnstore/tools/backuprestore/README.md;/usr/local/mariadb/columnstore/tools/monitoring/check_mariadbcs;/usr/local/mariadb/columnstore/tools/monitoring/README.md;%ignore /usr;%ignore /usr/local>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/COPYRIGHT.txt>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/LICENSE.txt>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/README.md>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/VERSION>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/backuprestore/columnstoreBackup>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/backuprestore/columnstoreRestore>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/backuprestore/README.md>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/monitoring/check_mariadbcs>
CPackRPM:Debug: F_PREFIX=<>, F_PATH=</usr/local/mariadb/columnstore/tools/monitoring/README.md>
CPackRPM:Debug: F_PREFIX=<%ignore>, F_PATH=</usr>
CPackRPM:Debug: F_PREFIX=<%ignore>, F_PATH=</usr/local>
CPackRPM:Debug: CPACK_TOPLEVEL_DIRECTORY          = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM
CPackRPM:Debug: CPACK_TOPLEVEL_TAG                = Linux
CPackRPM:Debug: CPACK_TEMPORARY_DIRECTORY         = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
CPackRPM:Debug: CPACK_OUTPUT_FILE_NAME            = mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm
CPackRPM:Debug: CPACK_OUTPUT_FILE_PATH            = /home/jens/mariadb-columnstore-tools/build/mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm
CPackRPM:Debug: CPACK_PACKAGE_FILE_NAME           = mariadb-columnstore-tools-1.2.0-1-amd64-centos6
CPackRPM:Debug: CPACK_RPM_BINARY_SPECFILE         = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/SPECS/mariadb-columnstore-tools.spec
CPackRPM:Debug: CPACK_PACKAGE_INSTALL_DIRECTORY   = mariadb-columnstore-tools 1.2.0
CPackRPM:Debug: CPACK_TEMPORARY_PACKAGE_FILE_NAME = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm
CPackRPM: Will use GENERATED spec file: /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/SPECS/mariadb-columnstore-tools.spec
CPackRPM:Debug: You may consult rpmbuild logs in:
CPackRPM:Debug:    - /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/rpmbuild.err
CPackRPM:Debug: *** + umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ LANG=C
+ export LANG
+ unset DISPLAY
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot
+ exit 0
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ '[' /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 '!=' / ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
++ dirname /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mkdir -p /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM
+ mkdir /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ LANG=C
+ export LANG
+ unset DISPLAY
+ '[' -e /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ /usr/lib/rpm/check-buildroot
+ /usr/lib/rpm/redhat/brp-compress
+ /usr/lib/rpm/redhat/brp-strip /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-static-archive /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump
+ /usr/lib/rpm/brp-python-bytecompile /usr/bin/python
+ /usr/lib/rpm/redhat/brp-python-hardlink
+ /usr/lib/rpm/redhat/brp-java-repack-jars
warning: Could not canonicalize hostname: centos6
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ exit 0
 ***
CPackRPM:Debug:    - /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/rpmbuild.out
CPackRPM:Debug: *** + umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ LANG=C
+ export LANG
+ unset DISPLAY
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot
+ exit 0
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ '[' /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 '!=' / ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
++ dirname /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mkdir -p /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM
+ mkdir /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ LANG=C
+ export LANG
+ unset DISPLAY
+ '[' -e /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ /usr/lib/rpm/check-buildroot
+ /usr/lib/rpm/redhat/brp-compress
+ /usr/lib/rpm/redhat/brp-strip /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-static-archive /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump
+ /usr/lib/rpm/brp-python-bytecompile /usr/bin/python
+ /usr/lib/rpm/redhat/brp-python-hardlink
+ /usr/lib/rpm/redhat/brp-java-repack-jars
warning: Could not canonicalize hostname: centos6
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ exit 0
 ***
CPack: - package: /home/jens/mariadb-columnstore-tools/build/mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm generated.
[jens@centos6 build]$ ls
backuprestore   CMakeFiles           CPackConfig.cmake  CPackSourceConfig.cmake  Makefile                                             monitoring
CMakeCache.txt  cmake_install.cmake  _CPack_Packages    install_manifest.txt     mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm  VERSION.dep
[jens@centos6 build]$ 
{code}",8,"Buildbot errors were mostly caused due to removed packages.sh. I restored a modified version of packages.sh that doesn't include the folders mcsimport, build, resources and cmake to the binary package.
Currently we distributed one binary package with all platform independent shell scripts. Therefore, I don't know if we want to include mcsimport into it or not.

The buildbot errors for CentOS 6 and Suse are caused by not using the -DREMOTE_CPIMPORT=OFF flag while invoking cmake.

On CentOS 6 we can't build remote cpimport as it requires mcsimport which depends on C++ 11. On Suse buildbot currently fails as buildbot doesn't build and install mcsapi prior building the tools.

I tested the package build on CentOS 6 with above mentioned -DREMOTE_CPIMPORT=OFF flag on a local VM and the rpm package was built successfully.

{code:shell}
[jens@centos6 ~]$ cat /etc/centos-release
CentOS release 6.10 (Final)
[jens@centos6 ~]$ git clone git@github.com:mariadb-corporation/mariadb-columnstore-tools.git
Initialized empty Git repository in /home/jens/mariadb-columnstore-tools/.git/
remote: Enumerating objects: 251, done.
remote: Counting objects: 100% (251/251), done.
remote: Compressing objects: 100% (133/133), done.
remote: Total 476 (delta 113), reused 228 (delta 100), pack-reused 225
Receiving objects: 100% (476/476), 179.76 KiB, done.
Resolving deltas: 100% (229/229), done.
[jens@centos6 ~]$ cd mariadb-columnstore-tools/
[jens@centos6 mariadb-columnstore-tools]$ git checkout MCOL-1242
Branch MCOL-1242 set up to track remote branch MCOL-1242 from origin.
Switched to a new branch 'MCOL-1242'
[jens@centos6 mariadb-columnstore-tools]$ ls
backuprestore  cmake  CMakeLists.txt  COPYRIGHT.txt  LICENSE.txt  mcsimport  monitoring  README.md  resources  VERSION
[jens@centos6 mariadb-columnstore-tools]$ mkdir build && cd build
[jens@centos6 build]$ cmake .. -DREMOTE_CPIMPORT=OFF
-- The C compiler identification is GNU 4.4.7
-- The CXX compiler identification is GNU 4.4.7
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Running cmake version 2.8.12.2
-- MariaDB-Columnstore 1.2.0
-- -----------------------------------------------
-- CMAKE_INSTALL_PREFIX = /usr/local/mariadb/columnstore
-- CMAKE_BUILD_TYPE =
-- TEST_RUNNER = OFF
-- BACKUPRESTORE = ON
-- MONITORING = ON
-- REMOTE_CPIMPORT = OFF
-- RPM = OFF
-- DEB = OFF
-- Change a values with: cmake -D=
-- ------------------------------------------------
--
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jens/mariadb-columnstore-tools/build
[jens@centos6 build]$ make
[jens@centos6 build]$ cmake .. -DRPM=centos6
-- Running cmake version 2.8.12.2
-- MariaDB-Columnstore 1.2.0
-- -----------------------------------------------
-- CMAKE_INSTALL_PREFIX = /usr/local/mariadb/columnstore
-- CMAKE_BUILD_TYPE =
-- TEST_RUNNER = OFF
-- BACKUPRESTORE = ON
-- MONITORING = ON
-- REMOTE_CPIMPORT = OFF
-- RPM = centos6
-- DEB = OFF
-- Change a values with: cmake -D=
-- ------------------------------------------------
--
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jens/mariadb-columnstore-tools/build
[jens@centos6 build]$ sudo make package
[sudo] password for jens:
Run CPack packaging tool...
CPack: Create package using RPM
CPack: Install projects
CPack: - Run preinstall target for: Project
CPack: - Install project: Project
CPack: Create package
CPackRPM:Debug: rpmbuild version is 
CPackRPM:Debug: LSB_RELEASE  = lsb_release not installed/found!
CPackRPM:Debug: processing URL
CPackRPM:Debug: using CPACK_RPM_PACKAGE_URL
CPackRPM:Debug: User defined Url:
 URL
CPackRPM:Debug: processing REQUIRES
CPackRPM:Debug: using CPACK_RPM_PACKAGE_REQUIRES
CPackRPM:Debug: User defined Requires:
  libxml2 rsync
CPackRPM:Debug: processing SUGGESTS
CPackRPM:Debug: processing PROVIDES
CPackRPM:Debug: using CPACK_RPM_PACKAGE_PROVIDES
CPackRPM:Debug: User defined Provides:
  mariadb-columnstore-tools
CPackRPM:Debug: processing OBSOLETES
CPackRPM:Debug: processing PREFIX
CPackRPM:Debug: processing CONFLICTS
CPackRPM:Debug: processing AUTOPROV
CPackRPM:Debug: processing AUTOREQ
CPackRPM:Debug: processing AUTOREQPROV
CPackRPM:Debug: User defined more define spec line specified:

        %define ignore #

CPackRPM:Debug: Initial list of path to OMIT in RPM:
CPackRPM:Debug: CPACK_RPM_EXCLUDE_FROM_AUTO_FILELIST= /etc;/etc/init.d;/usr;/usr/share;/usr/share/doc;/usr/bin;/usr/lib;/usr/lib64;/usr/include
CPackRPM:Debug: Final list of path to OMIT in RPM: -o;-path;./etc;-o;-path;./etc/init.d;-o;-path;./usr;-o;-path;./usr/share;-o;-path;./usr/share/doc;-o;-path;./usr/bin;-o;-path;./usr/lib;-o;-path;./usr/lib64;-o;-path;./usr/include
CPackRPM:Debug: Handling User Filelist: 
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=
CPackRPM:Debug: F_PREFIX=, F_PATH=
CPackRPM:Debug: F_PREFIX=, F_PATH=
CPackRPM:Debug: CPACK_TOPLEVEL_DIRECTORY          = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM
CPackRPM:Debug: CPACK_TOPLEVEL_TAG                = Linux
CPackRPM:Debug: CPACK_TEMPORARY_DIRECTORY         = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
CPackRPM:Debug: CPACK_OUTPUT_FILE_NAME            = mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm
CPackRPM:Debug: CPACK_OUTPUT_FILE_PATH            = /home/jens/mariadb-columnstore-tools/build/mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm
CPackRPM:Debug: CPACK_PACKAGE_FILE_NAME           = mariadb-columnstore-tools-1.2.0-1-amd64-centos6
CPackRPM:Debug: CPACK_RPM_BINARY_SPECFILE         = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/SPECS/mariadb-columnstore-tools.spec
CPackRPM:Debug: CPACK_PACKAGE_INSTALL_DIRECTORY   = mariadb-columnstore-tools 1.2.0
CPackRPM:Debug: CPACK_TEMPORARY_PACKAGE_FILE_NAME = /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm
CPackRPM: Will use GENERATED spec file: /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/SPECS/mariadb-columnstore-tools.spec
CPackRPM:Debug: You may consult rpmbuild logs in:
CPackRPM:Debug:    - /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/rpmbuild.err
CPackRPM:Debug: *** + umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ LANG=C
+ export LANG
+ unset DISPLAY
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot
+ exit 0
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ '[' /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 '!=' / ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
++ dirname /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mkdir -p /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM
+ mkdir /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ LANG=C
+ export LANG
+ unset DISPLAY
+ '[' -e /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ /usr/lib/rpm/check-buildroot
+ /usr/lib/rpm/redhat/brp-compress
+ /usr/lib/rpm/redhat/brp-strip /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-static-archive /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump
+ /usr/lib/rpm/brp-python-bytecompile /usr/bin/python
+ /usr/lib/rpm/redhat/brp-python-hardlink
+ /usr/lib/rpm/redhat/brp-java-repack-jars
warning: Could not canonicalize hostname: centos6
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ exit 0
 ***
CPackRPM:Debug:    - /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/rpmbuild.out
CPackRPM:Debug: *** + umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ LANG=C
+ export LANG
+ unset DISPLAY
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot
+ exit 0
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ '[' /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 '!=' / ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
++ dirname /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mkdir -p /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM
+ mkdir /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ LANG=C
+ export LANG
+ unset DISPLAY
+ '[' -e /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6 ']'
+ rm -rf /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ mv /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/tmpBBroot /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/mariadb-columnstore-tools-1.2.0-1-amd64-centos6
+ /usr/lib/rpm/check-buildroot
+ /usr/lib/rpm/redhat/brp-compress
+ /usr/lib/rpm/redhat/brp-strip /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-static-archive /usr/bin/strip
+ /usr/lib/rpm/redhat/brp-strip-comment-note /usr/bin/strip /usr/bin/objdump
+ /usr/lib/rpm/brp-python-bytecompile /usr/bin/python
+ /usr/lib/rpm/redhat/brp-python-hardlink
+ /usr/lib/rpm/redhat/brp-java-repack-jars
warning: Could not canonicalize hostname: centos6
+ umask 022
+ cd /home/jens/mariadb-columnstore-tools/build/_CPack_Packages/Linux/RPM/BUILD
+ exit 0
 ***
CPack: - package: /home/jens/mariadb-columnstore-tools/build/mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm generated.
[jens@centos6 build]$ ls
backuprestore   CMakeFiles           CPackConfig.cmake  CPackSourceConfig.cmake  Makefile                                             monitoring
CMakeCache.txt  cmake_install.cmake  _CPack_Packages    install_manifest.txt     mariadb-columnstore-tools-1.2.0-1-amd64-centos6.rpm  VERSION.dep
[jens@centos6 build]$ 
{code}"
189,MCOL-1242,MCOL,Elena Kotsinova,117810,2018-10-12 11:36:16,"Win 10 Pro msi installation and functionality works fine.
Minor issue for UI of installer is reported separately. ",9,"Win 10 Pro msi installation and functionality works fine.
Minor issue for UI of installer is reported separately. "
190,MCOL-1244,MCOL,David Thompson,112998,2018-06-25 20:18:47,One implication for this is that mcsadmin will no longer be able to truly support shutdownSystem and a startSystem after shutdown since there is no ssh access to do this. I think this is fine as long as we auto start CS processes on each node.,1,One implication for this is that mcsadmin will no longer be able to truly support shutdownSystem and a startSystem after shutdown since there is no ssh access to do this. I think this is fine as long as we auto start CS processes on each node.
191,MCOL-1244,MCOL,Dipti Joshi,117094,2018-09-26 15:51:54,Please make the prompt to show non-distributed option as default,2,Please make the prompt to show non-distributed option as default
192,MCOL-1244,MCOL,David Hill,117620,2018-10-09 16:18:59,"Question: Do we want the quick install scripts to also default to Non-Distributed install.

If not, then these 3 files will need to be changed to pass in a command line argument for 'distributed' install..",3,"Question: Do we want the quick install scripts to also default to Non-Distributed install.

If not, then these 3 files will need to be changed to pass in a command line argument for 'distributed' install.."
193,MCOL-1244,MCOL,David Thompson,117776,2018-10-11 20:38:23,"Yes, all of the installs should default to non-distributed.",4,"Yes, all of the installs should default to non-distributed."
194,MCOL-1244,MCOL,David Hill,117780,2018-10-11 21:10:36,"CORRECTION: Changes:

1. No change code change postConfigure is required.  To default to Non-DIstributed Install, meaning when no command line argument is provided, non-distributed is assumed. User would provide '-d' command line argument to perform Distributed Install
2. quick install quick - only the multi-node needs to be change - change to pass non-distributed
    as the default.
3. Change to Columnstore.xml change DistributedInstall entry from 'y' to 'n'. postConfigure
    gets the default from here. user can override it with the command line argument

",5,"CORRECTION: Changes:

1. No change code change postConfigure is required.  To default to Non-DIstributed Install, meaning when no command line argument is provided, non-distributed is assumed. User would provide '-d' command line argument to perform Distributed Install
2. quick install quick - only the multi-node needs to be change - change to pass non-distributed
    as the default.
3. Change to Columnstore.xml change DistributedInstall entry from 'y' to 'n'. postConfigure
    gets the default from here. user can override it with the command line argument

"
195,MCOL-1244,MCOL,David Hill,117782,2018-10-11 21:56:27,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/592,6,URL
196,MCOL-1244,MCOL,David Hill,117785,2018-10-11 22:42:19,"Addition change from for Bens review

mcsadmin, procmon, procmgr = change to set the DIstributedInstall default setting to 'n'

Also this will make the addModule default as non-distributed install
",7,"Addition change from for Bens review

mcsadmin, procmon, procmgr = change to set the DIstributedInstall default setting to 'n'

Also this will make the addModule default as non-distributed install
"
197,MCOL-1244,MCOL,Daniel Lee,117934,2018-10-15 23:23:13,"Build verified: 1.2.0-1a

Verified postConfigure with and without -d switch.
Performed installations on single and muti-node stacks for each.
",8,"Build verified: 1.2.0-1a

Verified postConfigure with and without -d switch.
Performed installations on single and muti-node stacks for each.
"
198,MCOL-1248,MCOL,Elena Kotsinova,108137,2018-03-09 09:57:41,I have added comments in the github. Please check there.,1,I have added comments in the github. Please check there.
199,MCOL-1248,MCOL,Elena Kotsinova,108148,2018-03-09 12:59:46,"Could you also provide clear instruction what changes user must make in Columnstore.xml in order PDI plugin to connect to standalone CS installation ( UM and PM on the same machine)?
I have changed values (IP addresses) in the following:
* DBRM_Controller  >> IPAddr 
* pm1_ProcessMonitor >> IPAddr
* pm1_WriteEngineServer >> IPAddr

and test finished successful. ",2,"Could you also provide clear instruction what changes user must make in Columnstore.xml in order PDI plugin to connect to standalone CS installation ( UM and PM on the same machine)?
I have changed values (IP addresses) in the following:
* DBRM_Controller  >> IPAddr 
* pm1_ProcessMonitor >> IPAddr
* pm1_WriteEngineServer >> IPAddr

and test finished successful. "
200,MCOL-1248,MCOL,Jens Röwekamp,108187,2018-03-09 23:52:42,Readme.md updated,3,Readme.md updated
201,MCOL-1248,MCOL,Elena Kotsinova,108338,2018-03-13 16:00:34,David Thompson has proposed additional updates to the readme.md in github.,4,David Thompson has proposed additional updates to the readme.md in github.
202,MCOL-1248,MCOL,Elena Kotsinova,108395,2018-03-14 12:41:45,"Reviewed and accepted.
It seems that [~dthompson] must also accept changes.",5,"Reviewed and accepted.
It seems that [~dthompson] must also accept changes."
203,MCOL-1258,MCOL,David Hill,108527,2018-03-16 21:55:13,"finished, now creating the rpm/deb and binary package on buildbot as part of the repo package setup.

",1,"finished, now creating the rpm/deb and binary package on buildbot as part of the repo package setup.

"
204,MCOL-1259,MCOL,Jens Röwekamp,110779,2018-05-09 21:42:17,from the kettle / pdi adatper site there is no problem with this.,1,from the kettle / pdi adatper site there is no problem with this.
205,MCOL-1259,MCOL,Jens Röwekamp,111249,2018-05-18 23:09:56,"- top level cmake for data adapters introduced
- changed all CMakeList.txt files accordingly, and included tests via make test where possible
- changed maxscale cdc docker test installer to use new maxscale-cdc-connector rpm dependency packages

For testing:
- please test each data adapter manually for functionality after make install and also their package installation after make package.

- For kettle / PDI the test suite introduced in MCOL-1401 could be used.
- Please consult [~markus makela] on how to test the other 3 connectors.
",2,"- top level cmake for data adapters introduced
- changed all CMakeList.txt files accordingly, and included tests via make test where possible
- changed maxscale cdc docker test installer to use new maxscale-cdc-connector rpm dependency packages

For testing:
- please test each data adapter manually for functionality after make install and also their package installation after make package.

- For kettle / PDI the test suite introduced in MCOL-1401 could be used.
- Please consult [~markus makela] on how to test the other 3 connectors.
"
206,MCOL-1259,MCOL,David Thompson,111250,2018-05-18 23:34:51,"Hi Ben, you are much more knowledgable on cmake than me so can you review this.  Right now the directory level cmakes fail with this change so the buildbot check fails correctly :) not sure if you have any ideas on fixing this as i think this ought to work?

Regardless we'll need to change buildbot to use the top level cmake / make after this gets checked in. Also this should be merged up to develop as well for buildbot too.",3,"Hi Ben, you are much more knowledgable on cmake than me so can you review this.  Right now the directory level cmakes fail with this change so the buildbot check fails correctly :) not sure if you have any ideas on fixing this as i think this ought to work?

Regardless we'll need to change buildbot to use the top level cmake / make after this gets checked in. Also this should be merged up to develop as well for buildbot too."
207,MCOL-1259,MCOL,Ben Thompson,111915,2018-06-04 08:39:36,Changes have been merged and works for buildbot,4,Changes have been merged and works for buildbot
208,MCOL-1270,MCOL,Ben Thompson,114349,2018-07-23 15:40:48,Pull requests now launch buildbot builds instead of individual commits.,1,Pull requests now launch buildbot builds instead of individual commits.
209,MCOL-1271,MCOL,Ben Thompson,114347,2018-07-23 15:37:41,Buildbot has been migrated to new AWS accounts,1,Buildbot has been migrated to new AWS accounts
210,MCOL-1273,MCOL,Ben Thompson,114348,2018-07-23 15:39:49,"can build from user specified branch combination with ""Test Build"" button",1,"can build from user specified branch combination with ""Test Build"" button"
211,MCOL-1275,MCOL,Jens Röwekamp,108519,2018-03-16 20:14:09,"Pull request in branch MCOL-1245, to avoid possible git merge conflicts.",1,"Pull request in branch MCOL-1245, to avoid possible git merge conflicts."
212,MCOL-1275,MCOL,Jens Röwekamp,108522,2018-03-16 20:17:40,"if no ColumnStoreDriver can be instantiated or no JDBC connection is configured, the settings tab is selected when opening the block",2,"if no ColumnStoreDriver can be instantiated or no JDBC connection is configured, the settings tab is selected when opening the block"
213,MCOL-1275,MCOL,Elena Kotsinova,108609,2018-03-20 14:04:47,"Verified:
The Settings tab is selected by default when there is no ColumnStore.xml file selected.",3,"Verified:
The Settings tab is selected by default when there is no ColumnStore.xml file selected."
214,MCOL-128,MCOL,dic,96690,2017-06-20 09:45:41,"I have the similar problem:
MariaDB [employees]> ALTER TABLE salaries ENGINE = ColumnStore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types.",1,"I have the similar problem:
MariaDB [employees]> ALTER TABLE salaries ENGINE = ColumnStore;
ERROR 1178 (42000): The storage engine for the table doesn't support The syntax or the data type(s) is not supported by Columnstore. Please check the Columnstore syntax guide for supported syntax or data types."
215,MCOL-128,MCOL,Elias maturana,135631,2019-10-09 14:53:01,"hi, They have found a solution to this problem

regards",2,"hi, They have found a solution to this problem

regards"
216,MCOL-128,MCOL,Andrew Hutchings,142471,2020-01-20 22:29:44,"PR opened, pending regression suite test creation.",3,"PR opened, pending regression suite test creation."
217,MCOL-128,MCOL,Andrew Hutchings,142553,2020-01-21 18:38:36,PR in engine and regression suite to add ALTER TABLE to/from ColumnStore and CTAS.,4,PR in engine and regression suite to add ALTER TABLE to/from ColumnStore and CTAS.
218,MCOL-128,MCOL,Patrick LeBlanc,142850,2020-01-24 15:28:02,Merged it into develop-1.4.  Merge into develop TBD.,5,Merged it into develop-1.4.  Merge into develop TBD.
219,MCOL-128,MCOL,susil.behera,144102,2020-02-11 18:24:51,"Build verified: 1.4.3-1

MariaDB [(none)]> CREATE TABLE  db1.t1 (c1 int) ENGINE=innodb;
Query OK, 0 rows affected (0.015 sec)

MariaDB [(none)]> ALTER TABLE  db1.t1  ENGINE=columnstore;
Query OK, 0 rows affected (0.179 sec)
Records: 0  Duplicates: 0  Warnings: 0

MariaDB [(none)]> SHOW CREATE TABLE db1.t1;
+-------+---------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                |
+-------+---------------------------------------------------------------------------------------------+
| t1    | CREATE TABLE `t1` (
  `c1` int(11) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+---------------------------------------------------------------------------------------------+
1 row in set (0.000 sec)

MariaDB [(none)]> ALTER TABLE  db1.t1  ENGINE=innodb;
Query OK, 0 rows affected (0.338 sec)
Records: 0  Duplicates: 0  Warnings: 0

MariaDB [(none)]> SHOW CREATE TABLE db1.t1;
+-------+----------------------------------------------------------------------------------------+
| Table | Create Table                                                                           |
+-------+----------------------------------------------------------------------------------------+
| t1    | CREATE TABLE `t1` (
  `c1` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 |
+-------+----------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [(none)]>",6,"Build verified: 1.4.3-1

MariaDB [(none)]> CREATE TABLE  db1.t1 (c1 int) ENGINE=innodb;
Query OK, 0 rows affected (0.015 sec)

MariaDB [(none)]> ALTER TABLE  db1.t1  ENGINE=columnstore;
Query OK, 0 rows affected (0.179 sec)
Records: 0  Duplicates: 0  Warnings: 0

MariaDB [(none)]> SHOW CREATE TABLE db1.t1;
+-------+---------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                |
+-------+---------------------------------------------------------------------------------------------+
| t1    | CREATE TABLE `t1` (
  `c1` int(11) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+---------------------------------------------------------------------------------------------+
1 row in set (0.000 sec)

MariaDB [(none)]> ALTER TABLE  db1.t1  ENGINE=innodb;
Query OK, 0 rows affected (0.338 sec)
Records: 0  Duplicates: 0  Warnings: 0

MariaDB [(none)]> SHOW CREATE TABLE db1.t1;
+-------+----------------------------------------------------------------------------------------+
| Table | Create Table                                                                           |
+-------+----------------------------------------------------------------------------------------+
| t1    | CREATE TABLE `t1` (
  `c1` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 |
+-------+----------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [(none)]>"
220,MCOL-1281,MCOL,Jens Röwekamp,114411,2018-07-25 02:08:55,"- mcsapi, javamcsapi, pymcsapi, and spark-connectors ported to Windows
- tests modified to be executed against a remote ColumnStore instance through environment variables
- support for packaging/installation as Windows .msi packet
- Readme.md expanded to contain Windows build information
- test suite ran successfully on Windows 10, CentOS 7, and Debian 9

For QA:
- build the bulk write sdk on all relevant platforms and execute the regression test suite
- build the bulk write sdk on Windows 10 according to the documentation in Readme.md and execute the regression suite.

Feel free to contact me if you have questions.",1,"- mcsapi, javamcsapi, pymcsapi, and spark-connectors ported to Windows
- tests modified to be executed against a remote ColumnStore instance through environment variables
- support for packaging/installation as Windows .msi packet
- Readme.md expanded to contain Windows build information
- test suite ran successfully on Windows 10, CentOS 7, and Debian 9

For QA:
- build the bulk write sdk on all relevant platforms and execute the regression test suite
- build the bulk write sdk on Windows 10 according to the documentation in Readme.md and execute the regression suite.

Feel free to contact me if you have questions."
221,MCOL-1281,MCOL,Andrew Hutchings,114421,2018-07-25 07:27:48,Fix version is 1.2.0 but you have done a pull request against develop-1.1. Is that intentional?,2,Fix version is 1.2.0 but you have done a pull request against develop-1.1. Is that intentional?
222,MCOL-1281,MCOL,Jens Röwekamp,114866,2018-08-04 00:17:26,"Implemented the suggestions of dt and dipti during our last meeting.

Here is a complete list what has been done:
- mcsapi, javamcsapi, pymcsapi, and spark-connectors ported to Windows
- tests modified to be executed against a remote ColumnStore instance through environment variables
- support for packaging/installation as (signed) Windows .msi packet
- Readme.md expanded to contain Windows build information
- out of source builds with documentation build and regression tests run successfully on CentOS 7, Debian 8, Debian 9, Ubuntu 16.04, Ubuntu 18.04
- in source builds with documentation build and regression tests run successfully on CentOS 7 (the other OSes weren't executed)
- out of source build and in source build in Windows 10 with subsequent successful run of the regression tests
- installation of the MSI packet on a a clean Windows 10 machine and successful execution of the documented examples

For QA:
- build the bulk write sdk on all relevant platforms and execute the regression test suite
- build the bulk write sdk on Windows 10 according to the documentation in Readme.md and execute the regression suite.

Feel free to contact me if you have any questions. The Windows setup might be a bit tricky.",3,"Implemented the suggestions of dt and dipti during our last meeting.

Here is a complete list what has been done:
- mcsapi, javamcsapi, pymcsapi, and spark-connectors ported to Windows
- tests modified to be executed against a remote ColumnStore instance through environment variables
- support for packaging/installation as (signed) Windows .msi packet
- Readme.md expanded to contain Windows build information
- out of source builds with documentation build and regression tests run successfully on CentOS 7, Debian 8, Debian 9, Ubuntu 16.04, Ubuntu 18.04
- in source builds with documentation build and regression tests run successfully on CentOS 7 (the other OSes weren't executed)
- out of source build and in source build in Windows 10 with subsequent successful run of the regression tests
- installation of the MSI packet on a a clean Windows 10 machine and successful execution of the documented examples

For QA:
- build the bulk write sdk on all relevant platforms and execute the regression test suite
- build the bulk write sdk on Windows 10 according to the documentation in Readme.md and execute the regression suite.

Feel free to contact me if you have any questions. The Windows setup might be a bit tricky."
223,MCOL-1283,MCOL,Elena Kotsinova,109115,2018-03-30 13:43:43,"Yes, installations process is simplified.

",1,"Yes, installations process is simplified.

"
224,MCOL-1293,MCOL,David Hill,109757,2018-04-16 21:24:41,"has been updated

https://mariadb.com/kb/en/library/preparing-for-columnstore-installation-11x/

2 pull request

https://github.com/mariadb-corporation/mariadb-columnstore-server/pull/110

https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/447",1,"has been updated

URL

2 pull request

URL

URL"
225,MCOL-1293,MCOL,David Hill,109758,2018-04-16 21:25:46,"changed server README.md

engine - columnstoreClusterTester.sh",2,"changed server README.md

engine - columnstoreClusterTester.sh"
226,MCOL-1293,MCOL,Daniel Lee,109833,2018-04-17 19:15:03,"Build verified: 1.1.4-1 source

/root/columnstore/mariadb-columnstore-server
commit 6b8a6745bd84b0230875fb94b526d9426ba999f7
Merge: 5199dd1 2089aad
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Apr 16 18:50:35 2018 -0500

    Merge pull request #110 from mariadb-corporation/MCOL-1293
    
    MCOL-1293

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3e8fed91704effff5e442148916d1c3611dd38c4
Merge: 1586394 f2d748c
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Apr 16 18:50:05 2018 -0500

    Merge pull request #447 from mariadb-corporation/MCOL-1293
    
    MCOL-1293

Verified documentation and columnstoreClusterTester.sh





",3,"Build verified: 1.1.4-1 source

/root/columnstore/mariadb-columnstore-server
commit 6b8a6745bd84b0230875fb94b526d9426ba999f7
Merge: 5199dd1 2089aad
Author: benthompson15 
Date:   Mon Apr 16 18:50:35 2018 -0500

    Merge pull request #110 from mariadb-corporation/MCOL-1293
    
    MCOL-1293

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3e8fed91704effff5e442148916d1c3611dd38c4
Merge: 1586394 f2d748c
Author: benthompson15 
Date:   Mon Apr 16 18:50:05 2018 -0500

    Merge pull request #447 from mariadb-corporation/MCOL-1293
    
    MCOL-1293

Verified documentation and columnstoreClusterTester.sh





"
227,MCOL-1296,MCOL,Andrew Hutchings,108795,2018-03-23 10:24:57,"[~jens.rowekamp] I've pushed up branch MCOL-1296 to the API tree which adds ColumnStoreDriver::setDebug(bool enabled). Can you please add this to the other connectors on that branch?

I've documented what it does in the .rst files but basically it is spits out the same stuff to stderr that a debug build used to do. You now need to enable this option in debug and release builds to get the debugging output.",1,"[~jens.rowekamp] I've pushed up branch MCOL-1296 to the API tree which adds ColumnStoreDriver::setDebug(bool enabled). Can you please add this to the other connectors on that branch?

I've documented what it does in the .rst files but basically it is spits out the same stuff to stderr that a debug build used to do. You now need to enable this option in debug and release builds to get the debugging output."
228,MCOL-1296,MCOL,Jens Röwekamp,108983,2018-03-28 00:43:14,"Swig took care of it. No changes needed.

Found and fixed a not handled forwarded exception for std::bad_alloc that caused Java and Python to terminate once it was raised.",2,"Swig took care of it. No changes needed.

Found and fixed a not handled forwarded exception for std::bad_alloc that caused Java and Python to terminate once it was raised."
229,MCOL-1312,MCOL,Jens Röwekamp,109046,2018-03-28 20:52:14,"Added revision information to version.txt and the manifest.

Invoke mcsapi.setDebug(true) in PDI log mode row level.",1,"Added revision information to version.txt and the manifest.

Invoke mcsapi.setDebug(true) in PDI log mode row level."
230,MCOL-1312,MCOL,Jens Röwekamp,109047,2018-03-28 20:53:11,"Pushed to branch MCOL-1283, to avoid merge conflicts.",2,"Pushed to branch MCOL-1283, to avoid merge conflicts."
231,MCOL-1312,MCOL,Elena Kotsinova,109116,2018-03-30 13:45:11,"Excellent!
Verified for: 
Version: 1.1.4
Revision: 2b9b9e1",3,"Excellent!
Verified for: 
Version: 1.1.4
Revision: 2b9b9e1"
232,MCOL-1318,MCOL,David Hill,109359,2018-04-05 18:28:35,"changed to report warning if firewall is enabled.. and reference the port test for additional information.

** Run Firewall Services check

Local Node iptables service is Not Active
Local Node ufw service is Not Active
Local Node firewalld service is Not Active
Local Node firewall service is Not Active

172.31.36.237 Node iptables service is Not Enabled
172.31.36.237 Node ufw service is Not Enabled
Warning, 172.31.36.237 Node firewalld service is Active, check port test results
172.31.36.237 Node firewall service is Not Enabled


** Run MariaDB ColumnStore Port (8600-8630,8700,8800,3306) availibility test

172.31.36.237  Node Failed port test, check and disable any firewalls or open ports in firewall
All 34 scanned ports on ip-172-31-36-237.us-west-2.compute.internal (172.31.36.237) are filtered

Failure occurred, do you want to continue? (y,n) > n
",1,"changed to report warning if firewall is enabled.. and reference the port test for additional information.

** Run Firewall Services check

Local Node iptables service is Not Active
Local Node ufw service is Not Active
Local Node firewalld service is Not Active
Local Node firewall service is Not Active

172.31.36.237 Node iptables service is Not Enabled
172.31.36.237 Node ufw service is Not Enabled
Warning, 172.31.36.237 Node firewalld service is Active, check port test results
172.31.36.237 Node firewall service is Not Enabled


** Run MariaDB ColumnStore Port (8600-8630,8700,8800,3306) availibility test

172.31.36.237  Node Failed port test, check and disable any firewalls or open ports in firewall
All 34 scanned ports on ip-172-31-36-237.us-west-2.compute.internal (172.31.36.237) are filtered

Failure occurred, do you want to continue? (y,n) > n
"
233,MCOL-1318,MCOL,David Hill,109360,2018-04-05 18:29:00,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/436,2,URL
234,MCOL-1318,MCOL,David Hill,109361,2018-04-05 18:30:44,"how to test - example for centos 7

enable firewall and run

[root@ip-172-31-36-237 ~]# systemctl stop firewalld.service

disable firewall and run
[root@ip-172-31-36-237 ~]# systemctl start firewalld.service
",3,"how to test - example for centos 7

enable firewall and run

[root@ip-172-31-36-237 ~]# systemctl stop firewalld.service

disable firewall and run
[root@ip-172-31-36-237 ~]# systemctl start firewalld.service
"
235,MCOL-1318,MCOL,Daniel Lee,109756,2018-04-16 20:54:03,"Build verified: 1.1.4-1 source
/root/columnstore/mariadb-columnstore-server
commit 5199dd1a096fd3457e8fc0508bf5fb24cedec435
Merge: fce3c5e e554e04
Author: David.Hall <david.hall@mariadb.com>
Date: Wed Apr 11 11:04:46 2018 -0500
Merge pull request #108 from mariadb-corporation/MCOL-1331
MCOL-1331 Fix CASE1.DM.sql
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
[root@localhost mariadb-columnstore-engine]# git show
commit ae04b8a6877c87f3ed3566f2bf721bf285ca625f
Merge: 2ab632c dbcbd6c
Author: david hill <david.hill@mariadb.com>
Date: Tue Apr 10 10:55:56 2018 -0400
Merge pull request #438 from mariadb-corporation/MCOL-1323
MCOL-1323 cpimport Splitter has incorrect SIGPIPE mapping
diff --cc writeengine/splitter/we_splitterapp.cpp
index f52f362,0077ebd..402d2b0
mode 100755,100644..100755
— a/writeengine/splitter/we_splitterapp.cpp
+++ b/writeengine/splitter/we_splitterapp.cpp

Verified along with MCOL-1317.

** Run MariaDB ColumnStore Port (8600-8630,8700,8800,3306) availability test

s1um1  Node Failed port test, check and disable any firewalls or open ports in firewall
All 34 scanned ports on s1um1 (10.0.0.11) are filtered
",4,"Build verified: 1.1.4-1 source
/root/columnstore/mariadb-columnstore-server
commit 5199dd1a096fd3457e8fc0508bf5fb24cedec435
Merge: fce3c5e e554e04
Author: David.Hall 
Date: Wed Apr 11 11:04:46 2018 -0500
Merge pull request #108 from mariadb-corporation/MCOL-1331
MCOL-1331 Fix CASE1.DM.sql
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
[root@localhost mariadb-columnstore-engine]# git show
commit ae04b8a6877c87f3ed3566f2bf721bf285ca625f
Merge: 2ab632c dbcbd6c
Author: david hill 
Date: Tue Apr 10 10:55:56 2018 -0400
Merge pull request #438 from mariadb-corporation/MCOL-1323
MCOL-1323 cpimport Splitter has incorrect SIGPIPE mapping
diff --cc writeengine/splitter/we_splitterapp.cpp
index f52f362,0077ebd..402d2b0
mode 100755,100644..100755
— a/writeengine/splitter/we_splitterapp.cpp
+++ b/writeengine/splitter/we_splitterapp.cpp

Verified along with MCOL-1317.

** Run MariaDB ColumnStore Port (8600-8630,8700,8800,3306) availability test

s1um1  Node Failed port test, check and disable any firewalls or open ports in firewall
All 34 scanned ports on s1um1 (10.0.0.11) are filtered
"
236,MCOL-1319,MCOL,Andrew Hutchings,109386,2018-04-06 10:55:02,Simple merge this time. Seems to run fine.,1,Simple merge this time. Seems to run fine.
237,MCOL-1319,MCOL,Daniel Lee,109735,2018-04-16 14:11:29,"Build verified: 1.1.4-1 source

/root/columnstore/mariadb-columnstore-server
commit 5199dd1a096fd3457e8fc0508bf5fb24cedec435
Merge: fce3c5e e554e04
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Apr 11 11:04:46 2018 -0500

    Merge pull request #108 from mariadb-corporation/MCOL-1331
    
    MCOL-1331 Fix CASE1.DM.sql

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
[root@localhost mariadb-columnstore-engine]# git show
commit ae04b8a6877c87f3ed3566f2bf721bf285ca625f
Merge: 2ab632c dbcbd6c
Author: david hill <david.hill@mariadb.com>
Date:   Tue Apr 10 10:55:56 2018 -0400

    Merge pull request #438 from mariadb-corporation/MCOL-1323
    
    MCOL-1323 cpimport Splitter has incorrect SIGPIPE mapping

diff --cc writeengine/splitter/we_splitterapp.cpp
index f52f362,0077ebd..402d2b0
mode 100755,100644..100755
--- a/writeengine/splitter/we_splitterapp.cpp
+++ b/writeengine/splitter/we_splitterapp.cpp

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.14-MariaDB-log Columnstore 1.1.4-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

",2,"Build verified: 1.1.4-1 source

/root/columnstore/mariadb-columnstore-server
commit 5199dd1a096fd3457e8fc0508bf5fb24cedec435
Merge: fce3c5e e554e04
Author: David.Hall 
Date:   Wed Apr 11 11:04:46 2018 -0500

    Merge pull request #108 from mariadb-corporation/MCOL-1331
    
    MCOL-1331 Fix CASE1.DM.sql

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
[root@localhost mariadb-columnstore-engine]# git show
commit ae04b8a6877c87f3ed3566f2bf721bf285ca625f
Merge: 2ab632c dbcbd6c
Author: david hill 
Date:   Tue Apr 10 10:55:56 2018 -0400

    Merge pull request #438 from mariadb-corporation/MCOL-1323
    
    MCOL-1323 cpimport Splitter has incorrect SIGPIPE mapping

diff --cc writeengine/splitter/we_splitterapp.cpp
index f52f362,0077ebd..402d2b0
mode 100755,100644..100755
--- a/writeengine/splitter/we_splitterapp.cpp
+++ b/writeengine/splitter/we_splitterapp.cpp

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.14-MariaDB-log Columnstore 1.1.4-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

"
238,MCOL-1330,MCOL,Andrew Hutchings,112978,2018-06-25 14:13:28,"For QA: there is a new '-d' option available for WriteEngine and PrimProc, it will all valgrind to work with them. The flag already exists for ExeMgr and this has been modified to allow valgrind to work.",1,"For QA: there is a new '-d' option available for WriteEngine and PrimProc, it will all valgrind to work with them. The flag already exists for ExeMgr and this has been modified to allow valgrind to work."
239,MCOL-1330,MCOL,Daniel Lee,113204,2018-06-28 20:23:41,"Build verified: 1.1.6-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.6-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c4e9b0ce3a396edd2353eb44532336e9db857f86
Merge: ac45350 ffb76bb
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Jun 27 17:36:13 2018 -0500

    Merge pull request #511 from mariadb-corporation/MCOL-1467-1
    
    MCOL-1467 - changes to get back to 1.1.6

Confirmed that the -d options on 1.1.5-1 does not work for Valgrind.
Verified that the -d options for both processes do work with Valgrind.
",2,"Build verified: 1.1.6-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.6-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c4e9b0ce3a396edd2353eb44532336e9db857f86
Merge: ac45350 ffb76bb
Author: David.Hall 
Date:   Wed Jun 27 17:36:13 2018 -0500

    Merge pull request #511 from mariadb-corporation/MCOL-1467-1
    
    MCOL-1467 - changes to get back to 1.1.6

Confirmed that the -d options on 1.1.5-1 does not work for Valgrind.
Verified that the -d options for both processes do work with Valgrind.
"
240,MCOL-1333,MCOL,David Hill,109614,2018-04-12 15:26:22,"Also will look into changing to use the rpm and not the tar.gz. Actually I thought this was done. Maybe postConfigure only and not addmodule..

",1,"Also will look into changing to use the rpm and not the tar.gz. Actually I thought this was done. Maybe postConfigure only and not addmodule..

"
241,MCOL-1333,MCOL,David Hill,109658,2018-04-13 13:33:45,will go ahead and fix in 1.1.4,2,will go ahead and fix in 1.1.4
242,MCOL-1333,MCOL,David Hill,109662,2018-04-13 15:31:08,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/442,3,URL
243,MCOL-1333,MCOL,Ben Thompson,109872,2018-04-18 15:53:36,Merged,4,Merged
244,MCOL-1333,MCOL,Daniel Lee,109924,2018-04-19 19:36:04,"Build verified: 1.1.4-1 source

/root/columnstore/mariadb-columnstore-server
commit 6b8a6745bd84b0230875fb94b526d9426ba999f7
Merge: 5199dd1 2089aad
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Apr 16 18:50:35 2018 -0500

    Merge pull request #110 from mariadb-corporation/MCOL-1293
    
    MCOL-1293

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3e8fed91704effff5e442148916d1c3611dd38c4
Merge: 1586394 f2d748c
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Apr 16 18:50:05 2018 -0500

    Merge pull request #447 from mariadb-corporation/MCOL-1293
    
    MCOL-1293




",5,"Build verified: 1.1.4-1 source

/root/columnstore/mariadb-columnstore-server
commit 6b8a6745bd84b0230875fb94b526d9426ba999f7
Merge: 5199dd1 2089aad
Author: benthompson15 
Date:   Mon Apr 16 18:50:35 2018 -0500

    Merge pull request #110 from mariadb-corporation/MCOL-1293
    
    MCOL-1293

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3e8fed91704effff5e442148916d1c3611dd38c4
Merge: 1586394 f2d748c
Author: benthompson15 
Date:   Mon Apr 16 18:50:05 2018 -0500

    Merge pull request #447 from mariadb-corporation/MCOL-1293
    
    MCOL-1293




"
245,MCOL-1337,MCOL,David Thompson,111023,2018-05-15 16:46:47,This is included in the source and docker hub image.,1,This is included in the source and docker hub image.
246,MCOL-1344,MCOL,Jens Röwekamp,109927,2018-04-19 22:45:04,"method generateTableStatement(DataFrame, [""database"", ""table"", determineTypeLength]) added for Python and Scala.

Regression tests included.",1,"method generateTableStatement(DataFrame, [""database"", ""table"", determineTypeLength]) added for Python and Scala.

Regression tests included."
247,MCOL-1362,MCOL,Jens Röwekamp,118565,2018-11-01 18:27:49,In today's call with [~dshjoshi] and [~dthompson] we decided to implement sequential write from the Spark Workers to Columnstore to avoid the extra step of collecting the DataFrame at the Spark Driver.,1,In today's call with [~dshjoshi] and [~dthompson] we decided to implement sequential write from the Spark Workers to Columnstore to avoid the extra step of collecting the DataFrame at the Spark Driver.
248,MCOL-1362,MCOL,Jens Röwekamp,118998,2018-11-10 01:21:46,"Introduced a new Scala Spark Connector function:

*ColumnStoreExporter.exportFromWorkers(""database"", ""table"", RDD, \[partitions\], \[path to Columnstore.xml\])*

It exports a RDD from the Spark worker nodes instead of the Spark driver. This decreased the ingestion time by 59% in my test case compared to ColumnStoreExporter.export()

I wasn't able to implement it for PySpark as its runJob() function seems to behave different than Scala's.

To use exportFromWorkers() the Bulk Write SDK and Columnstore.xml need to be installed on the Spark driver and workers.
When using exportFromWorkers() sequential transactions from the workers are executed. This means that we don't have only one transaction (that rolls back in an error case) as in the case of export(), but as many transactions as the RDD to export has partitions. I added an optional partition parameter to be able to only ingest the failed partitions in an error case.

I added simple test to the regression suite (derived from export()'s) and executed it successfully on CentOS 7 and Windows 10.

*For QA:*
- execute regression test suite
- if you want to: set up a multi node spark cluster and test manual recovery cases by using the optimal partition parameter (to ingest only the failed partitions)",2,"Introduced a new Scala Spark Connector function:

*ColumnStoreExporter.exportFromWorkers(""database"", ""table"", RDD, \[partitions\], \[path to Columnstore.xml\])*

It exports a RDD from the Spark worker nodes instead of the Spark driver. This decreased the ingestion time by 59% in my test case compared to ColumnStoreExporter.export()

I wasn't able to implement it for PySpark as its runJob() function seems to behave different than Scala's.

To use exportFromWorkers() the Bulk Write SDK and Columnstore.xml need to be installed on the Spark driver and workers.
When using exportFromWorkers() sequential transactions from the workers are executed. This means that we don't have only one transaction (that rolls back in an error case) as in the case of export(), but as many transactions as the RDD to export has partitions. I added an optional partition parameter to be able to only ingest the failed partitions in an error case.

I added simple test to the regression suite (derived from export()'s) and executed it successfully on CentOS 7 and Windows 10.

*For QA:*
- execute regression test suite
- if you want to: set up a multi node spark cluster and test manual recovery cases by using the optimal partition parameter (to ingest only the failed partitions)"
249,MCOL-1364,MCOL,Andrew Hutchings,110378,2018-05-01 12:14:00,[~jens.rowekamp] can you please add this to the sprint and add a fix version so it can be reviewed?,1,[~jens.rowekamp] can you please add this to the sprint and add a fix version so it can be reviewed?
250,MCOL-1376,MCOL,Andrew Hutchings,110413,2018-05-02 08:48:38,For QA: develop should now compile in Ubuntu 18.04,1,For QA: develop should now compile in Ubuntu 18.04
251,MCOL-1376,MCOL,David Hill,110964,2018-05-14 19:17:49,building a local build machine to build all packages and a buildbot worker node to build all packages.,2,building a local build machine to build all packages and a buildbot worker node to build all packages.
252,MCOL-1376,MCOL,David Hill,111284,2018-05-21 15:03:33,"successfully build

ColumnStore
API
Tools
Data-Adapters:
  kafka-avro
  kettle-columnstore-bulk-exporter-plugin
  
NOT BUILT

maxscale-cdc-adapter - requires Maxscale and CDC connector for Ubuntu 18.04.

",3,"successfully build

ColumnStore
API
Tools
Data-Adapters:
  kafka-avro
  kettle-columnstore-bulk-exporter-plugin
  
NOT BUILT

maxscale-cdc-adapter - requires Maxscale and CDC connector for Ubuntu 18.04.

"
253,MCOL-1376,MCOL,David Hill,111288,2018-05-21 15:23:58,"locate Maxscale CDC connector for ubuntu 18 on web07. install it on build machine and now

SUCCESSFULLY BUILT - maxscale-cdc-adapter

So on my local Ubuntu 18 build machine, I can not build all MCS package.

Next will create Buildbot worker for Ubuntu 18 for Ben and I to build on.",4,"locate Maxscale CDC connector for ubuntu 18 on web07. install it on build machine and now

SUCCESSFULLY BUILT - maxscale-cdc-adapter

So on my local Ubuntu 18 build machine, I can not build all MCS package.

Next will create Buildbot worker for Ubuntu 18 for Ben and I to build on."
254,MCOL-1376,MCOL,David Hill,111479,2018-05-24 20:43:03,"completed creating an ubuntu 18.04 buildbot worker node.
It currently running and 2 issues. Ben sayd he version will work, so I will let him take this over into hi setup to work instead of my spending time on why mine isnt.

1. current issue - CSC data adapter needs the mcsapi to be installed. mcsapi gets cleanly built and uses sudo make install, make package to generate package. But the package install command fails. Then when the CDC runs cmake, it fails because it sayd the mcsapi is not installed. (this is the part Ben said he has working.

but the rest of the packages build and pass regression test.",5,"completed creating an ubuntu 18.04 buildbot worker node.
It currently running and 2 issues. Ben sayd he version will work, so I will let him take this over into hi setup to work instead of my spending time on why mine isnt.

1. current issue - CSC data adapter needs the mcsapi to be installed. mcsapi gets cleanly built and uses sudo make install, make package to generate package. But the package install command fails. Then when the CDC runs cmake, it fails because it sayd the mcsapi is not installed. (this is the part Ben said he has working.

but the rest of the packages build and pass regression test."
255,MCOL-1376,MCOL,David Hill,111924,2018-06-04 13:35:37,"assigning to Ben, work neede still is Ben getting Ubuntu 18 buildbot working in his setup",6,"assigning to Ben, work neede still is Ben getting Ubuntu 18 buildbot working in his setup"
256,MCOL-1376,MCOL,Ben Thompson,114346,2018-07-23 15:34:18,Buildbot is working with ubuntu18,7,Buildbot is working with ubuntu18
257,MCOL-1378,MCOL,Andrew Hutchings,112907,2018-06-22 20:31:45,For QA: ColumnStore should still compile/run after this.,1,For QA: ColumnStore should still compile/run after this.
258,MCOL-1378,MCOL,Andrew Hutchings,112908,2018-06-22 20:50:38,"Moved back to me, the new flags have found bad bugs",2,"Moved back to me, the new flags have found bad bugs"
259,MCOL-1378,MCOL,Andrew Hutchings,112953,2018-06-25 10:30:11,For QA pt2: this will cause additional test failures which are addressed in MCOL-1496,3,For QA pt2: this will cause additional test failures which are addressed in MCOL-1496
260,MCOL-138,MCOL,David Hill,84947,2016-07-13 20:08:15,"changed postConfigure, mcsadmin, columnstoreSupport files. Anywhere where it mentioned mysql to the customer, it was changed.

mariadb columnstore mysql to mariadb columnstore
mysql to mariadb columnstore

there are still scripts and other areas where mysql is used, but only made changes that a user would see",1,"changed postConfigure, mcsadmin, columnstoreSupport files. Anywhere where it mentioned mysql to the customer, it was changed.

mariadb columnstore mysql to mariadb columnstore
mysql to mariadb columnstore

there are still scripts and other areas where mysql is used, but only made changes that a user would see"
261,MCOL-138,MCOL,David Hill,84948,2016-07-13 20:10:33,"please review

commit c954b34337f5525a48b8fc5274d50f583b14b080
",2,"please review

commit c954b34337f5525a48b8fc5274d50f583b14b080
"
262,MCOL-138,MCOL,Ben Thompson,85317,2016-08-01 18:41:55,Reviewed,3,Reviewed
263,MCOL-1385,MCOL,Andrew Hutchings,114824,2018-08-03 12:21:46,"Three branches:

* regression suite: see pull request
* engine: see pull request
* server: MCOL-1385-server branch

MCOL-1385-server branch is taken from the 10.3.8 tag and then a ColumnStore patch applied on top. This means when a new release comes out we can easily rebase instead of merging. This also means that we will need to replace develop with this branch in the server tree instead of merging.

Therefore if the review is good please approve and not merge and I will coordinate with DT to get all the pieces in place.

The only remaining test failures I can see at the moment are due to things that haven't been merged up into develop and things we already know are broken in develop / develop regression suite.",1,"Three branches:

* regression suite: see pull request
* engine: see pull request
* server: MCOL-1385-server branch

MCOL-1385-server branch is taken from the 10.3.8 tag and then a ColumnStore patch applied on top. This means when a new release comes out we can easily rebase instead of merging. This also means that we will need to replace develop with this branch in the server tree instead of merging.

Therefore if the review is good please approve and not merge and I will coordinate with DT to get all the pieces in place.

The only remaining test failures I can see at the moment are due to things that haven't been merged up into develop and things we already know are broken in develop / develop regression suite."
264,MCOL-1385,MCOL,Daniel Lee,116490,2018-09-11 19:09:04,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit c2052269ef995332eedbef4fbd3ecc4a5e8e8cf0
Merge: b8fded1 d0385eb
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Sep 10 09:07:03 2018 -0500

    Merge pull request #131 from mariadb-corporation/build_fix
    
    Revert ""MDEV-11036 Add link wsrep_sst_rsync_wan -> wsrep_sst_rsync""

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 764090ba0cf0bbb99092f5bfd9f8014464a136b1
Merge: 73b45ac 5b682a5
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Mon Sep 3 16:20:47 2018 +0100

    Merge pull request #552 from drrtuy/MCOL-1510_4
    
    MCOL-1510: Add CalpontSelectExecutionPlan::serialize() changes.

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.3.8-MariaDB-log Columnstore 1.2.0-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

This test only verified the code is merged.  Any issues found, if any, will be track separately.

",2,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit c2052269ef995332eedbef4fbd3ecc4a5e8e8cf0
Merge: b8fded1 d0385eb
Author: benthompson15 
Date:   Mon Sep 10 09:07:03 2018 -0500

    Merge pull request #131 from mariadb-corporation/build_fix
    
    Revert ""MDEV-11036 Add link wsrep_sst_rsync_wan -> wsrep_sst_rsync""

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 764090ba0cf0bbb99092f5bfd9f8014464a136b1
Merge: 73b45ac 5b682a5
Author: Andrew Hutchings 
Date:   Mon Sep 3 16:20:47 2018 +0100

    Merge pull request #552 from drrtuy/MCOL-1510_4
    
    MCOL-1510: Add CalpontSelectExecutionPlan::serialize() changes.

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.3.8-MariaDB-log Columnstore 1.2.0-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

This test only verified the code is merged.  Any issues found, if any, will be track separately.

"
265,MCOL-1392,MCOL,Jens Röwekamp,111679,2018-05-29 23:10:24,"Changed datatype for timestamps from java.util.date to java.sql.timestamp to support microseconds.

Changed the all-datatype-test case accordingly.

It currently fails due to an issue in mcsapi's time implementation (MCOL-1427).",1,"Changed datatype for timestamps from java.util.date to java.sql.timestamp to support microseconds.

Changed the all-datatype-test case accordingly.

It currently fails due to an issue in mcsapi's time implementation (MCOL-1427)."
266,MCOL-1392,MCOL,Jens Röwekamp,117713,2018-10-10 18:20:55,Fixed the Timestamp csv input bug for develop.,2,Fixed the Timestamp csv input bug for develop.
267,MCOL-1392,MCOL,Elena Kotsinova,117765,2018-10-11 16:09:15,fixed. ,3,fixed. 
268,MCOL-140,MCOL,Dipti Joshi,84443,2016-06-22 01:21:20,This may be related to MCOL-66 being worked upon right now by [~David.Hall],1,This may be related to MCOL-66 being worked upon right now by [~David.Hall]
269,MCOL-140,MCOL,David Hall,85090,2016-07-21 13:19:43,"This is caused by a combination of two things. First, the VSS can't handle transactions arriving out of order if they are acting on the same block. Second, since VSS access is down the execution path, simultaneous threaded transactions arrive in unpredictable order based on cpu cycles given to each thread by the scheduler. Sometimes these two properties clash and result in a transaction being rejected.



",2,"This is caused by a combination of two things. First, the VSS can't handle transactions arriving out of order if they are acting on the same block. Second, since VSS access is down the execution path, simultaneous threaded transactions arrive in unpredictable order based on cpu cycles given to each thread by the scheduler. Sometimes these two properties clash and result in a transaction being rejected.



"
270,MCOL-140,MCOL,David Hall,85244,2016-07-28 15:12:21,"I've created a mechanism to serialize transactions only within each table. Transactions on different tables continue concurrently. The mechanism is surrounded by an ifdef, so it will be easy to turn off when we start testing a new VSS.

The mechanism is thus (as documented in the code):
// Blocks a thread if there is another trx working on the same fTableOid
// return 1 when thread should continue.
// return 0 if error. Right now, no error detection is implemented.
//
// txnid was being created before the call to this function. This caused race conditions
// so creation is delayed until we're inside the lock here. Nothing needs it before
// this point in the execution.
// 
// The algorithm is this. When the first txn for a given fTableOid arrives, start a queue 
// containing a list of waiting or working txnId. Put this txnId into the queue (working)
// Put the queue into a map keyed on fTableOid.
// 
// When the next txn for this fTableOid arrives, it finds the queue in the map and adds itself,
// then waits for condition.
// When a thread finishes, it removes its txnId from the queue and notifies all. If the queue is
// empty, it removes the entry from the map.
// Upon wakeup from wait(), a thread checks to see if it's next in the queue. If so, it is released
// to do work. Otherwise it goes back to wait.
// 
// There's a chance (CTRL+C) for instance, that the txn is no longer in the queue. Release it to work.
// Rollback will most likely be next.
// 
// A tranasaction for one fTableOid is not blocked by a txn for a different fTableOid.

A new test212 has been added to the regression suite to test multiple threads doing DML on single tables. The test runs up multiple tables and multiple threads for each table and then slams DML at them.",3,"I've created a mechanism to serialize transactions only within each table. Transactions on different tables continue concurrently. The mechanism is surrounded by an ifdef, so it will be easy to turn off when we start testing a new VSS.

The mechanism is thus (as documented in the code):
// Blocks a thread if there is another trx working on the same fTableOid
// return 1 when thread should continue.
// return 0 if error. Right now, no error detection is implemented.
//
// txnid was being created before the call to this function. This caused race conditions
// so creation is delayed until we're inside the lock here. Nothing needs it before
// this point in the execution.
// 
// The algorithm is this. When the first txn for a given fTableOid arrives, start a queue 
// containing a list of waiting or working txnId. Put this txnId into the queue (working)
// Put the queue into a map keyed on fTableOid.
// 
// When the next txn for this fTableOid arrives, it finds the queue in the map and adds itself,
// then waits for condition.
// When a thread finishes, it removes its txnId from the queue and notifies all. If the queue is
// empty, it removes the entry from the map.
// Upon wakeup from wait(), a thread checks to see if it's next in the queue. If so, it is released
// to do work. Otherwise it goes back to wait.
// 
// There's a chance (CTRL+C) for instance, that the txn is no longer in the queue. Release it to work.
// Rollback will most likely be next.
// 
// A tranasaction for one fTableOid is not blocked by a txn for a different fTableOid.

A new test212 has been added to the regression suite to test multiple threads doing DML on single tables. The test runs up multiple tables and multiple threads for each table and then slams DML at them."
271,MCOL-140,MCOL,David Hall,85245,2016-07-28 15:14:08,"This is really a Kludge. Eventually we need to re-engineer the VSS to not need things arriving in order. However, that is an expensive and risky operation, so we have this kludge. Have fun following the flow!",4,"This is really a Kludge. Eventually we need to re-engineer the VSS to not need things arriving in order. However, that is an expensive and risky operation, so we have this kludge. Have fun following the flow!"
272,MCOL-140,MCOL,Ben Thompson,85319,2016-08-01 19:12:03,Review Completed,5,Review Completed
273,MCOL-140,MCOL,Dipti Joshi,85589,2016-08-16 02:45:21,[~David.Hall] Has this been run through regression ? If so please assign to [~dleeyh] for auto pilot testing,6,[~David.Hall] Has this been run through regression ? If so please assign to [~dleeyh] for auto pilot testing
274,MCOL-140,MCOL,Daniel Lee,85786,2016-08-23 19:59:51,Have been testing in Autopilot's concurrent DDL and DML tests.  All good.,7,Have been testing in Autopilot's concurrent DDL and DML tests.  All good.
275,MCOL-1401,MCOL,Jens Röwekamp,110889,2018-05-11 20:09:35,"test.sh added for continious integration of PDI plugin

The script downloads PDI 7, and installs the build plugin + dependencies. Afterwards all defined tests from the test directory (./test/*.kjb) are executed.",1,"test.sh added for continious integration of PDI plugin

The script downloads PDI 7, and installs the build plugin + dependencies. Afterwards all defined tests from the test directory (./test/*.kjb) are executed."
276,MCOL-1401,MCOL,Jens Röwekamp,110890,2018-05-11 20:14:21,"For testing:
Test if the script executes the tests successfully in Ubuntu 16.04 and CentOS7. Afterwards modify the test that it fails, and check if it fails for CentOS7 and Ubuntu16.04.

---
After review and test are passed, the test script should be added to buildbot's CI suite.",2,"For testing:
Test if the script executes the tests successfully in Ubuntu 16.04 and CentOS7. Afterwards modify the test that it fails, and check if it fails for CentOS7 and Ubuntu16.04.

---
After review and test are passed, the test script should be added to buildbot's CI suite."
277,MCOL-1401,MCOL,Jens Röwekamp,110954,2018-05-14 16:54:01,"more tests will be integrated tomorrow, then it'll get back to review.",3,"more tests will be integrated tomorrow, then it'll get back to review."
278,MCOL-1401,MCOL,Jens Röwekamp,111057,2018-05-15 22:27:13,"Added a test.sh script that checks the tests sub-directories for kettle jobs and executes them. The defined jobs can have additional parameters defined in job.parameters which will be added during runtime.

Two CI tests are included so far:
- all type data ingestion check
- csv ingestion check -(without verification, just checks if csv files can be injected)-",4,"Added a test.sh script that checks the tests sub-directories for kettle jobs and executes them. The defined jobs can have additional parameters defined in job.parameters which will be added during runtime.

Two CI tests are included so far:
- all type data ingestion check
- csv ingestion check -(without verification, just checks if csv files can be injected)-"
279,MCOL-1401,MCOL,Andrew Hutchings,112081,2018-06-06 18:49:30,This was merged so assuming review was good. Moving to test...,5,This was merged so assuming review was good. Moving to test...
280,MCOL-1401,MCOL,Elena Kotsinova,112437,2018-06-13 23:31:04,"My assumption is that these tests will be part of the CI and regression tests for the version.
Let me know is this correct or I have to run them as separate test suite from somewhere.",6,"My assumption is that these tests will be part of the CI and regression tests for the version.
Let me know is this correct or I have to run them as separate test suite from somewhere."
281,MCOL-1401,MCOL,Jens Röwekamp,112438,2018-06-13 23:38:59,"Hi, yeah that's correct. The tests are part of the data-adapters regression suite invoked through make test.

You can manually execute just the regression test for the kettle adapter by running test.sh in the test directory.",7,"Hi, yeah that's correct. The tests are part of the data-adapters regression suite invoked through make test.

You can manually execute just the regression test for the kettle adapter by running test.sh in the test directory."
282,MCOL-1401,MCOL,Elena Kotsinova,112439,2018-06-14 03:16:49,"1. Rename the adapter package name and related parts in test.sh script to refrect the recent changes. (mariadb-columnstore-kettle-bulk-exporter-plugin)
2. The check for installed pdi must include check also for installed mariadb connector. Currenlty if pdi is already installed the connectior will not be installed.

",8,"1. Rename the adapter package name and related parts in test.sh script to refrect the recent changes. (mariadb-columnstore-kettle-bulk-exporter-plugin)
2. The check for installed pdi must include check also for installed mariadb connector. Currenlty if pdi is already installed the connectior will not be installed.

"
283,MCOL-1401,MCOL,Jens Röwekamp,112489,2018-06-14 16:07:05,"Made requested changes and included name change of MCOL-1470.

Test is the changed test.sh (to invoke manually our through make test)",9,"Made requested changes and included name change of MCOL-1470.

Test is the changed test.sh (to invoke manually our through make test)"
284,MCOL-1401,MCOL,Elena Kotsinova,115180,2018-08-13 10:38:23,"Works fine, minor adjustments are needed. Will be reported as separate issues.",10,"Works fine, minor adjustments are needed. Will be reported as separate issues."
285,MCOL-1412,MCOL,Daniel Lee,111355,2018-05-22 21:57:08,Performed few installations with the provided 1.1.5-1 deb packages (pre-release).,1,Performed few installations with the provided 1.1.5-1 deb packages (pre-release).
286,MCOL-1417,MCOL,Andrew Hutchings,112107,2018-06-07 11:28:47,Added to a bunch of upcoming MCOL-392 fixes.,1,Added to a bunch of upcoming MCOL-392 fixes.
287,MCOL-1417,MCOL,Andrew Hutchings,112112,2018-06-07 11:47:38,Pull request in engine to cover this.,2,Pull request in engine to cover this.
288,MCOL-1417,MCOL,Daniel Lee,112632,2018-06-18 21:27:30,"Build tested: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

Tested cpimport and NULL value and empty row indicators are still saturated to -838:59:59.  I used the following values for the indicators.  Are they correct?

NULL value = -2047_4095:255:255.16777215
Empty value = -2047_4095:255:255.16777214
",3,"Build tested: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall 
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

Tested cpimport and NULL value and empty row indicators are still saturated to -838:59:59.  I used the following values for the indicators.  Are they correct?

NULL value = -2047_4095:255:255.16777215
Empty value = -2047_4095:255:255.16777214
"
289,MCOL-1417,MCOL,Andrew Hutchings,112668,2018-06-19 14:48:02,If you used the word NULL without setting cpimport -n1 then that is correct. Empty values are NULL. What is an empty row indicator?,4,If you used the word NULL without setting cpimport -n1 then that is correct. Empty values are NULL. What is an empty row indicator?
290,MCOL-1417,MCOL,Andrew Hutchings,112669,2018-06-19 14:51:01,"Also, both the values you gave there will be saturated to -838:59:59.",5,"Also, both the values you gave there will be saturated to -838:59:59."
291,MCOL-1417,MCOL,Andrew Hutchings,112692,2018-06-19 16:57:58,"ok, so, lots to break down here...

When I said ""-2047 4095:255:255.16777215"" I mentioned it was purely theoretical and not a valid input, it fails several immediate sanity checks. There is no physical way for you to enter the stored NULL and empty values for the TIME data type.

Now, with your input ('-2047_4095:255:255.16777215') MariaDB will interpret this as an attempt to input the integer -2047 which translates into -00:20:47. This is normal behaviour since MariaDB allows integer representations of datetime/time.

cpimport is traditionally stricter with inputs than MariaDB. It will see this as an attempt to input -2047 hours and abort with the saturation at that point. If we want the same behaviour as MariaDB it should also be implemented for DATETIME too. Which puts it out of scope for this work.",6,"ok, so, lots to break down here...

When I said ""-2047 4095:255:255.16777215"" I mentioned it was purely theoretical and not a valid input, it fails several immediate sanity checks. There is no physical way for you to enter the stored NULL and empty values for the TIME data type.

Now, with your input ('-2047_4095:255:255.16777215') MariaDB will interpret this as an attempt to input the integer -2047 which translates into -00:20:47. This is normal behaviour since MariaDB allows integer representations of datetime/time.

cpimport is traditionally stricter with inputs than MariaDB. It will see this as an attempt to input -2047 hours and abort with the saturation at that point. If we want the same behaviour as MariaDB it should also be implemented for DATETIME too. Which puts it out of scope for this work."
292,MCOL-1417,MCOL,Daniel Lee,112701,2018-06-19 19:38:34,"The issue is in cpimport, not the implementation of the TIME data type.  MCOL-1485 has been opened to track this issue separately.

",7,"The issue is in cpimport, not the implementation of the TIME data type.  MCOL-1485 has been opened to track this issue separately.

"
293,MCOL-1418,MCOL,Daniel Lee,111049,2018-05-15 21:00:26,The issue occurs even when not saturating.  LDI the min value of -838:59:59 would also end up with 838:59:59.,1,The issue occurs even when not saturating.  LDI the min value of -838:59:59 would also end up with 838:59:59.
294,MCOL-1418,MCOL,Andrew Hutchings,112108,2018-06-07 11:42:59,MySQL binary time conversion was missing negative flag. Fixed in a branch full of fixes for MCOL-392,2,MySQL binary time conversion was missing negative flag. Fixed in a branch full of fixes for MCOL-392
295,MCOL-1418,MCOL,Andrew Hutchings,112109,2018-06-07 11:46:39,Pull request in engine and regression suite to cover this.,3,Pull request in engine and regression suite to cover this.
296,MCOL-1418,MCOL,Daniel Lee,112631,2018-06-18 21:15:32,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

",4,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall 
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

"
297,MCOL-1419,MCOL,Andrew Hutchings,111993,2018-06-05 12:00:08,Fix added to a bunch of pending fixes for MCOL-392,1,Fix added to a bunch of pending fixes for MCOL-392
298,MCOL-1419,MCOL,Andrew Hutchings,112110,2018-06-07 11:46:59,Pull request in engine and regression suite to cover this.,2,Pull request in engine and regression suite to cover this.
299,MCOL-1419,MCOL,Daniel Lee,112630,2018-06-18 21:08:09,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

",3,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall 
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

"
300,MCOL-1427,MCOL,Andrew Hutchings,111935,2018-06-04 15:16:15,"Microseconds are being stored correctly, but when being converted back for retrieval we are padding right instead of left.",1,"Microseconds are being stored correctly, but when being converted back for retrieval we are padding right instead of left."
301,MCOL-1427,MCOL,Andrew Hutchings,111954,2018-06-04 18:59:30,"Fix pending, batching with other MCOL-392 fixes.",2,"Fix pending, batching with other MCOL-392 fixes."
302,MCOL-1427,MCOL,Andrew Hutchings,112111,2018-06-07 11:47:16,Pull request in engine and regression suite to cover this.,3,Pull request in engine and regression suite to cover this.
303,MCOL-1427,MCOL,Daniel Lee,112629,2018-06-18 20:48:33,"Build verified: 1.1.2-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

",4,"Build verified: 1.1.2-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall 
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

"
304,MCOL-1428,MCOL,Andrew Hutchings,111959,2018-06-04 20:17:57,Confirmed only when SUBTIME is a WHERE condition,1,Confirmed only when SUBTIME is a WHERE condition
305,MCOL-1428,MCOL,Andrew Hutchings,111961,2018-06-04 21:07:56,Fix pending in a bunch of fixes for MCOL-392,2,Fix pending in a bunch of fixes for MCOL-392
306,MCOL-1428,MCOL,Andrew Hutchings,112113,2018-06-07 11:47:55,Pull request in engine and regression suite to cover this.,3,Pull request in engine and regression suite to cover this.
307,MCOL-1428,MCOL,Daniel Lee,112627,2018-06-18 20:32:12,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

",4,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall 
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

"
308,MCOL-1429,MCOL,Daniel Lee,111245,2018-05-18 18:29:10,"MONTHNAME() also has the same issue

select cidx, CTIME, MONTHNAME(CTIME) from datatypetestm order by cidx;
",1,"MONTHNAME() also has the same issue

select cidx, CTIME, MONTHNAME(CTIME) from datatypetestm order by cidx;
"
309,MCOL-1429,MCOL,Andrew Hutchings,111957,2018-06-04 19:42:10,Actually an old bug in those two functions triggered by TIME datatype. Array out of bounds problem.,2,Actually an old bug in those two functions triggered by TIME datatype. Array out of bounds problem.
310,MCOL-1429,MCOL,Andrew Hutchings,111958,2018-06-04 19:55:27,Fix pending batched with other MCOL-392 tests,3,Fix pending batched with other MCOL-392 tests
311,MCOL-1429,MCOL,Andrew Hutchings,112114,2018-06-07 11:48:10,Pull request in engine and regression suite to cover this.,4,Pull request in engine and regression suite to cover this.
312,MCOL-1429,MCOL,Daniel Lee,112626,2018-06-18 20:27:47,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

",5,"Build verified: 1.2.0-1 source

/root/columnstore/mariadb-columnstore-server
commit 3bfc3ad5f1e91e6f325b89fefaf62a6b38ca889a
Merge: 4334641 742af4d
Author: David.Hall 
Date:   Thu May 31 09:40:12 2018 -0500

    Merge pull request #119 from mariadb-corporation/1.1-merge-up-20180531
    
    Merge develop-1.1 into develop

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 1caa98ff46777a389cdfd4ae369865391a884c27
Merge: 60a4d14 d9e6ba9
Author: David.Hall 
Date:   Mon Jun 11 10:20:49 2018 -0500

    Merge pull request #494 from mariadb-corporation/MCOL-1433
    
    MCOL-1433 Fix several functions for TIME handling

"
313,MCOL-1433,MCOL,Andrew Hutchings,112115,2018-06-07 11:54:03,"Can you please provide your CAST, CONVERT, TIMEDIFF, STR_TO_DATE, NULLIF, MAKEDATE, DATE_FORMAT, DATEDIFF and TIMESTAMPDIFF tests?",1,"Can you please provide your CAST, CONVERT, TIMEDIFF, STR_TO_DATE, NULLIF, MAKEDATE, DATE_FORMAT, DATEDIFF and TIMESTAMPDIFF tests?"
314,MCOL-1433,MCOL,Andrew Hutchings,112137,2018-06-07 14:58:47,"The following were moved to MCOL-1461 as they are broken for DATETIME too and it appear to be by initial design:

period_add()
degrees()
inet_ntoa()

This puts them out of scope of MCOL-392.",2,"The following were moved to MCOL-1461 as they are broken for DATETIME too and it appear to be by initial design:

period_add()
degrees()
inet_ntoa()

This puts them out of scope of MCOL-392."
315,MCOL-1433,MCOL,Andrew Hutchings,112187,2018-06-08 14:04:27,Have a pull request that fixes everything else. Just need feedback from [~dleeyh] on the requested functions.,3,Have a pull request that fixes everything else. Just need feedback from [~dleeyh] on the requested functions.
316,MCOL-1433,MCOL,Andrew Hutchings,112284,2018-06-11 14:57:35,"CAST, CONVERT, TIMEDIFF, STR_TO_DATE, NULLIF, MAKEDATE, DATE_FORMAT, DATEDIFF and TIMESTAMPDIFF do not seem to cause a problem in develop any more.",4,"CAST, CONVERT, TIMEDIFF, STR_TO_DATE, NULLIF, MAKEDATE, DATE_FORMAT, DATEDIFF and TIMESTAMPDIFF do not seem to cause a problem in develop any more."
317,MCOL-1433,MCOL,Andrew Hutchings,112285,2018-06-11 14:58:45,Pull request in engine fixes most of the problem functions. The rest are either no longer a problem or are in MCOL-1461 which is a lower priority.,5,Pull request in engine fixes most of the problem functions. The rest are either no longer a problem or are in MCOL-1461 which is a lower priority.
318,MCOL-1433,MCOL,Daniel Lee,112751,2018-06-20 17:14:49,"Some functions still returning incorrect results

1.  CAST, CONVERT, STR_TO_DATE

When casting/converting TIME to DATE and DATETIME, it returns NULL.  It should return DATE and DATETIME value, respectively.

     1	--------------
     2	select cidx, CTIME, CAST(CTIME AS DATE) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	CAST(CTIME AS DATE)
     6	1	13:00:00	NULL
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, CAST(CTIME AS DATETIME) from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	CAST(CTIME AS DATETIME)
    14	1	13:00:00	NULL
    15	1 row in set

[root@localhost functions]# cat -n CNX/STR_TO_DATE.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATE,'USA')), GET_FORMAT(DATE,'USA')) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATE,'USA')), GET_FORMAT(DATE,'USA'))
     6	1	13:00:00	NULL
     7	1 row in set
.
.

    42	select cidx, CTIME, STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATETIME,'USA')), GET_FORMAT(DATETIME,'USA')) from datatypetestm order by cidx
    43	--------------
    44	
    45	cidx	CTIME	STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATETIME,'USA')), GET_FORMAT(DATETIME,'USA'))
    46	1	13:00:00	NULL
    47	1 row in set


2. TIMEDIFF

When timediff with a literal date time value, it returns the minimum TIME value, instead of NULL

[root@localhost functions]# cat -n CNX/TIMEDIFF.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	TIMEDIFF(CTIME,'2007-02-28 22:23:0')
     6	1	13:00:00	-838:59:59
     7	1 row in set

[root@localhost functions]# cat -n CNX/TIMEDIFF.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	TIMEDIFF(CTIME,'2007-02-28 22:23:0')
     6	1	13:00:00	NULL
     7	1 row in set

3. NULLIF

CTIME and CDATE are not the same they it should not return NULL

[root@localhost functions]# cat -n CNX/NULLIF.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, NULLIF(CTIME,CDATE) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	NULLIF(CTIME,CDATE)
     6	1	13:00:00	NULL
     7	1 row in set

[root@localhost functions]# cat -n CNX/NULLIF.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, NULLIF(CTIME,CDATE) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	NULLIF(CTIME,CDATE)
     6	1	13:00:00	13:00:00
     7	1 row in set

4. MAKEDATE

[root@localhost functions]# cat -n CNX/MAKEDATE.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	MAKEDATE(2010, CTIME)
     6	1	13:00:00	NULL
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	MAKEDATE(2011, CTIME)
    14	1	13:00:00	NULL
    15	1 row in set


[root@localhost functions]# cat -n CNX/MAKEDATE.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	MAKEDATE(2010, CTIME)
     6	1	13:00:00	2365-12-05
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	MAKEDATE(2011, CTIME)
    14	1	13:00:00	2366-12-05
    15	1 row in set


5. DATE_FORMAT

[root@localhost functions]# cat -n CNX/DATE_FORMAT.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATE_FORMAT(CTIME,'%W %M %Y')
     6	1	13:00:00	NULL
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATE_FORMAT(CTIME,'%H:%i:%s') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATE_FORMAT(CTIME,'%H:%i:%s')
    14	1	13:00:00	NULL
    15	1 row in set
    16	
    17	--------------
    18	select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx
    19	--------------
    20	
    21	cidx	CTIME	DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j')
    22	1	13:00:00	NULL
    23	1 row in set

[root@localhost functions]# cat -n CNX/DATE_FORMAT.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATE_FORMAT(CTIME,'%W %M %Y')
     6	1	13:00:00	Wednesday June 2018
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATE_FORMAT(CTIME,'%H:%i:%s') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATE_FORMAT(CTIME,'%H:%i:%s')
    14	1	13:00:00	13:00:00
    15	1 row in set
    16	
    17	--------------
    18	select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx
    19	--------------
    20	
    21	cidx	CTIME	DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j')
    22	1	13:00:00	20th 18 Wed 20 06 Jun 171
    23	1 row in set


6. DATEDIFF

The DATEDIFF output is off by 1

[root@localhost functions]# cat -n CNX/DATEDIFF.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, DATEDIFF(CTIME,'2007-02-28') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATEDIFF(CTIME,'2007-02-28')
     6	1	13:00:00	4131
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATEDIFF(CTIME,'2007-07-04') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATEDIFF(CTIME,'2007-07-04')
    14	1	13:00:00	4005
    15	1 row in set

[root@localhost functions]# cat -n CNX/DATEDIFF.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, DATEDIFF(CTIME,'2007-02-28') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATEDIFF(CTIME,'2007-02-28')
     6	1	13:00:00	4130
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATEDIFF(CTIME,'2007-07-04') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATEDIFF(CTIME,'2007-07-04')
    14	1	13:00:00	4004
",6,"Some functions still returning incorrect results

1.  CAST, CONVERT, STR_TO_DATE

When casting/converting TIME to DATE and DATETIME, it returns NULL.  It should return DATE and DATETIME value, respectively.

     1	--------------
     2	select cidx, CTIME, CAST(CTIME AS DATE) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	CAST(CTIME AS DATE)
     6	1	13:00:00	NULL
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, CAST(CTIME AS DATETIME) from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	CAST(CTIME AS DATETIME)
    14	1	13:00:00	NULL
    15	1 row in set

[root@localhost functions]# cat -n CNX/STR_TO_DATE.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATE,'USA')), GET_FORMAT(DATE,'USA')) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATE,'USA')), GET_FORMAT(DATE,'USA'))
     6	1	13:00:00	NULL
     7	1 row in set
.
.

    42	select cidx, CTIME, STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATETIME,'USA')), GET_FORMAT(DATETIME,'USA')) from datatypetestm order by cidx
    43	--------------
    44	
    45	cidx	CTIME	STR_TO_DATE(DATE_FORMAT(CTIME,GET_FORMAT(DATETIME,'USA')), GET_FORMAT(DATETIME,'USA'))
    46	1	13:00:00	NULL
    47	1 row in set


2. TIMEDIFF

When timediff with a literal date time value, it returns the minimum TIME value, instead of NULL

[root@localhost functions]# cat -n CNX/TIMEDIFF.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	TIMEDIFF(CTIME,'2007-02-28 22:23:0')
     6	1	13:00:00	-838:59:59
     7	1 row in set

[root@localhost functions]# cat -n CNX/TIMEDIFF.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	TIMEDIFF(CTIME,'2007-02-28 22:23:0')
     6	1	13:00:00	NULL
     7	1 row in set

3. NULLIF

CTIME and CDATE are not the same they it should not return NULL

[root@localhost functions]# cat -n CNX/NULLIF.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, NULLIF(CTIME,CDATE) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	NULLIF(CTIME,CDATE)
     6	1	13:00:00	NULL
     7	1 row in set

[root@localhost functions]# cat -n CNX/NULLIF.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, NULLIF(CTIME,CDATE) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	NULLIF(CTIME,CDATE)
     6	1	13:00:00	13:00:00
     7	1 row in set

4. MAKEDATE

[root@localhost functions]# cat -n CNX/MAKEDATE.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	MAKEDATE(2010, CTIME)
     6	1	13:00:00	NULL
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	MAKEDATE(2011, CTIME)
    14	1	13:00:00	NULL
    15	1 row in set


[root@localhost functions]# cat -n CNX/MAKEDATE.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	MAKEDATE(2010, CTIME)
     6	1	13:00:00	2365-12-05
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	MAKEDATE(2011, CTIME)
    14	1	13:00:00	2366-12-05
    15	1 row in set


5. DATE_FORMAT

[root@localhost functions]# cat -n CNX/DATE_FORMAT.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATE_FORMAT(CTIME,'%W %M %Y')
     6	1	13:00:00	NULL
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATE_FORMAT(CTIME,'%H:%i:%s') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATE_FORMAT(CTIME,'%H:%i:%s')
    14	1	13:00:00	NULL
    15	1 row in set
    16	
    17	--------------
    18	select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx
    19	--------------
    20	
    21	cidx	CTIME	DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j')
    22	1	13:00:00	NULL
    23	1 row in set

[root@localhost functions]# cat -n CNX/DATE_FORMAT.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATE_FORMAT(CTIME,'%W %M %Y')
     6	1	13:00:00	Wednesday June 2018
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATE_FORMAT(CTIME,'%H:%i:%s') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATE_FORMAT(CTIME,'%H:%i:%s')
    14	1	13:00:00	13:00:00
    15	1 row in set
    16	
    17	--------------
    18	select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx
    19	--------------
    20	
    21	cidx	CTIME	DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j')
    22	1	13:00:00	20th 18 Wed 20 06 Jun 171
    23	1 row in set


6. DATEDIFF

The DATEDIFF output is off by 1

[root@localhost functions]# cat -n CNX/DATEDIFF.DM.sql.tst.log
     1	--------------
     2	select cidx, CTIME, DATEDIFF(CTIME,'2007-02-28') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATEDIFF(CTIME,'2007-02-28')
     6	1	13:00:00	4131
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATEDIFF(CTIME,'2007-07-04') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATEDIFF(CTIME,'2007-07-04')
    14	1	13:00:00	4005
    15	1 row in set

[root@localhost functions]# cat -n CNX/DATEDIFF.DM.sql.ref.log
     1	--------------
     2	select cidx, CTIME, DATEDIFF(CTIME,'2007-02-28') from datatypetestm order by cidx
     3	--------------
     4	
     5	cidx	CTIME	DATEDIFF(CTIME,'2007-02-28')
     6	1	13:00:00	4130
     7	1 row in set
     8	
     9	--------------
    10	select cidx, CTIME, DATEDIFF(CTIME,'2007-07-04') from datatypetestm order by cidx
    11	--------------
    12	
    13	cidx	CTIME	DATEDIFF(CTIME,'2007-07-04')
    14	1	13:00:00	4004
"
319,MCOL-1433,MCOL,Andrew Hutchings,113857,2018-07-12 14:18:31,All of the flagged problems in Daniel's comment should now be fixed in the MCOL-1433 branch.,7,All of the flagged problems in Daniel's comment should now be fixed in the MCOL-1433 branch.
320,MCOL-1433,MCOL,Daniel Lee,117256,2018-10-01 18:13:44,"Build tested: Github source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Thu Sep 27 20:37:03 2018 +0100
Merge pull request #134 from mariadb-corporation/versionCmakeFix
port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3326be00de5f53ec365910f07a7fd882ba193d4d
Merge: ebbeb30 5cab6c4
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Tue Sep 18 13:57:17 2018 +0100
Merge pull request #565 from drrtuy/MCOL-1601
MCOL-1601 GROUP BY now supports subqueries in HAVING.
Regression test test001 passed, but test014 failed. Further investigation indicated that test014 failed due to a syntax error in the mcol1428.sql script. I manually verified the sublime() works. The failure is not related to this ticket and the sublime() function.

Some of the functions were fixed.  There are still a few with non-matching results.  They are TIMEDIFF(), MAKE_DATE(), and DATE_FORMAT(). 

STR_TO_DATE() seems to be correct.  The test return incorrect result because the DATE_FORMAT is used in the query.


TIMEDIFF()

Reference:

MariaDB [mytest]> select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx;
+------+----------+--------------------------------------+
| cidx | CTIME    | TIMEDIFF(CTIME,'2007-02-28 22:23:0') |
+------+----------+--------------------------------------+
|    1 | 13:00:00 | NULL                                 |
+------+----------+--------------------------------------+
1 row in set (0.001 sec)


ColumnStore:

MariaDB [mytest]> select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx;
+------+----------+--------------------------------------+
| cidx | CTIME    | TIMEDIFF(CTIME,'2007-02-28 22:23:0') |
+------+----------+--------------------------------------+
|    1 | 13:00:00 | -838:59:59                           |
+------+----------+--------------------------------------+
1 row in set (0.020 sec)


MAKEDATE()

Reference:

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2010, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2365-12-05            |
+------+----------+-----------------------+
1 row in set (0.001 sec)

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2011, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2366-12-05            |
+------+----------+-----------------------+
1 row in set (0.001 sec)


ColumnStore:

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2010, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2013-07-23            |
+------+----------+-----------------------+
1 row in set (0.056 sec)

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2011, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2014-07-23            |
+------+----------+-----------------------+
1 row in set (0.012 sec)


DATE_FORMAT()

Reference:

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx;
+------+----------+-------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%W %M %Y') |
+------+----------+-------------------------------+
|    1 | 13:00:00 | NULL                          |
+------+----------+-------------------------------+
1 row in set (0.000 sec)

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx;
+------+----------+-------------------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') |
+------+----------+-------------------------------------------+
|    1 | 13:00:00 | NULL                                      |
+------+----------+-------------------------------------------+
1 row in set (0.000 sec)


ColumnStore:

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx;
+------+----------+-------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%W %M %Y') |
+------+----------+-------------------------------+
|    1 | 13:00:00 | Monday October 2018           |
+------+----------+-------------------------------+
1 row in set (0.031 sec)

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx;
+------+----------+-------------------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') |
+------+----------+-------------------------------------------+
|    1 | 13:00:00 | 1st 18 Mon 01 10 Oct 274                  |
+------+----------+-------------------------------------------+
1 row in set (0.007 sec)",8,"Build tested: Github source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date: Thu Sep 27 20:37:03 2018 +0100
Merge pull request #134 from mariadb-corporation/versionCmakeFix
port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3326be00de5f53ec365910f07a7fd882ba193d4d
Merge: ebbeb30 5cab6c4
Author: Andrew Hutchings 
Date: Tue Sep 18 13:57:17 2018 +0100
Merge pull request #565 from drrtuy/MCOL-1601
MCOL-1601 GROUP BY now supports subqueries in HAVING.
Regression test test001 passed, but test014 failed. Further investigation indicated that test014 failed due to a syntax error in the mcol1428.sql script. I manually verified the sublime() works. The failure is not related to this ticket and the sublime() function.

Some of the functions were fixed.  There are still a few with non-matching results.  They are TIMEDIFF(), MAKE_DATE(), and DATE_FORMAT(). 

STR_TO_DATE() seems to be correct.  The test return incorrect result because the DATE_FORMAT is used in the query.


TIMEDIFF()

Reference:

MariaDB [mytest]> select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx;
+------+----------+--------------------------------------+
| cidx | CTIME    | TIMEDIFF(CTIME,'2007-02-28 22:23:0') |
+------+----------+--------------------------------------+
|    1 | 13:00:00 | NULL                                 |
+------+----------+--------------------------------------+
1 row in set (0.001 sec)


ColumnStore:

MariaDB [mytest]> select cidx, CTIME, TIMEDIFF(CTIME,'2007-02-28 22:23:0') from datatypetestm order by cidx;
+------+----------+--------------------------------------+
| cidx | CTIME    | TIMEDIFF(CTIME,'2007-02-28 22:23:0') |
+------+----------+--------------------------------------+
|    1 | 13:00:00 | -838:59:59                           |
+------+----------+--------------------------------------+
1 row in set (0.020 sec)


MAKEDATE()

Reference:

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2010, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2365-12-05            |
+------+----------+-----------------------+
1 row in set (0.001 sec)

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2011, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2366-12-05            |
+------+----------+-----------------------+
1 row in set (0.001 sec)


ColumnStore:

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2010, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2010, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2013-07-23            |
+------+----------+-----------------------+
1 row in set (0.056 sec)

MariaDB [mytest]> select cidx, CTIME, MAKEDATE(2011, CTIME) from datatypetestm order by cidx;
+------+----------+-----------------------+
| cidx | CTIME    | MAKEDATE(2011, CTIME) |
+------+----------+-----------------------+
|    1 | 13:00:00 | 2014-07-23            |
+------+----------+-----------------------+
1 row in set (0.012 sec)


DATE_FORMAT()

Reference:

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx;
+------+----------+-------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%W %M %Y') |
+------+----------+-------------------------------+
|    1 | 13:00:00 | NULL                          |
+------+----------+-------------------------------+
1 row in set (0.000 sec)

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx;
+------+----------+-------------------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') |
+------+----------+-------------------------------------------+
|    1 | 13:00:00 | NULL                                      |
+------+----------+-------------------------------------------+
1 row in set (0.000 sec)


ColumnStore:

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%W %M %Y') from datatypetestm order by cidx;
+------+----------+-------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%W %M %Y') |
+------+----------+-------------------------------+
|    1 | 13:00:00 | Monday October 2018           |
+------+----------+-------------------------------+
1 row in set (0.031 sec)

MariaDB [mytest]> select cidx, CTIME, DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') from datatypetestm order by cidx;
+------+----------+-------------------------------------------+
| cidx | CTIME    | DATE_FORMAT(CTIME,'%D %y %a %d %m %b %j') |
+------+----------+-------------------------------------------+
|    1 | 13:00:00 | 1st 18 Mon 01 10 Oct 274                  |
+------+----------+-------------------------------------------+
1 row in set (0.007 sec)"
321,MCOL-1433,MCOL,Andrew Hutchings,117411,2018-10-04 14:58:19,I've moved DATE_FORMAT() to MCOL-1768 since it will require a complete re-write to support MariaDB's behaviour. Not something we intent to do for 1.2.0. The rest will be fixed here,9,I've moved DATE_FORMAT() to MCOL-1768 since it will require a complete re-write to support MariaDB's behaviour. Not something we intent to do for 1.2.0. The rest will be fixed here
322,MCOL-1433,MCOL,Andrew Hutchings,117418,2018-10-04 17:21:58,"Fixed MAKEDATE and TIMEDIFF.

TIMEDIFF doesn't like mixed DATETIME/TIME type input in MariaDB so we now block that. MAKEDATE was using the wrong integer representation of time for the conversion.",10,"Fixed MAKEDATE and TIMEDIFF.

TIMEDIFF doesn't like mixed DATETIME/TIME type input in MariaDB so we now block that. MAKEDATE was using the wrong integer representation of time for the conversion."
323,MCOL-1433,MCOL,Daniel Lee,117630,2018-10-09 17:20:48,"Build tested: 1.2.0-1 source

Build verified: 1.2.0-1 source
/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Thu Sep 27 20:37:03 2018 +0100
Merge pull request #134 from mariadb-corporation/versionCmakeFix
port changes for mysql_version cmake to fix columnstore RPM packaging
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ca7cb9a7b13cdfd7a5dc0e72efcaf7bb1a4a9a7d
Merge: 6ee1224 a127f84
Author: David.Hall <david.hall@mariadb.com>
Date: Mon Oct 8 11:56:26 2018 -0500
Merge pull request #581 from mariadb-corporation/MCOL-1433b
MCOL-1433 Fix TIME for MAKEDATE/TIMEDIFF

test and reference results matched.


",11,"Build tested: 1.2.0-1 source

Build verified: 1.2.0-1 source
/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date: Thu Sep 27 20:37:03 2018 +0100
Merge pull request #134 from mariadb-corporation/versionCmakeFix
port changes for mysql_version cmake to fix columnstore RPM packaging
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ca7cb9a7b13cdfd7a5dc0e72efcaf7bb1a4a9a7d
Merge: 6ee1224 a127f84
Author: David.Hall 
Date: Mon Oct 8 11:56:26 2018 -0500
Merge pull request #581 from mariadb-corporation/MCOL-1433b
MCOL-1433 Fix TIME for MAKEDATE/TIMEDIFF

test and reference results matched.


"
324,MCOL-1434,MCOL,Daniel Lee,111660,2018-05-29 17:58:39,"Build verified: 1.0.14-1

Verified all distribution package files.",1,"Build verified: 1.0.14-1

Verified all distribution package files."
325,MCOL-1435,MCOL,Daniel Lee,111934,2018-06-04 14:54:39,"Build verified: 1.1.5-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.5-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 3e6e2b692a5847cc6e89fb200558ebc781b14b09
Merge: 0c983bf 1f94211
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu May 31 09:30:12 2018 -0500

    Merge pull request #118 from mariadb-corporation/MCOL-1435
    
    Merge MariaDB 10.2.15 into develop-1.1

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3ab634bde692e5c96f823aef933120eb09c72465
Merge: 12f2e11 efbf297
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu May 31 13:15:46 2018 +0100

    Merge pull request #485 from drrtuy/MCOL-1384_2
    
    MCOL-1384 Backport MCOL-573 to 1.1

Made new build for 1.1.5-1.

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 12
Server version: 10.2.15-MariaDB-log Columnstore 1.1.5-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
",1,"Build verified: 1.1.5-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.5-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 3e6e2b692a5847cc6e89fb200558ebc781b14b09
Merge: 0c983bf 1f94211
Author: David.Hall 
Date:   Thu May 31 09:30:12 2018 -0500

    Merge pull request #118 from mariadb-corporation/MCOL-1435
    
    Merge MariaDB 10.2.15 into develop-1.1

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3ab634bde692e5c96f823aef933120eb09c72465
Merge: 12f2e11 efbf297
Author: Andrew Hutchings 
Date:   Thu May 31 13:15:46 2018 +0100

    Merge pull request #485 from drrtuy/MCOL-1384_2
    
    MCOL-1384 Backport MCOL-573 to 1.1

Made new build for 1.1.5-1.

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 12
Server version: 10.2.15-MariaDB-log Columnstore 1.1.5-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
"
326,MCOL-1439,MCOL,Jens Röwekamp,113821,2018-07-11 20:28:14,"Added documentation for pymcsapi and javamcsapi and added it to the build pipeline.

Needed to introduce following new dependencies:
- Sphinx version >= 1.7.5 is required to support automated line breaks in code blocks in generated PDFs.
- javasphinx is needed to extend Sphinx with the Java domain.

Both should be installed through pip3 instead of the operating system's package manger to get the newer versions. I updated the top level README.md accordingly.

-----

For QA:
- Test if the updated dependency install commands in README.md work on all OSes.
- Build and install the new documentations on all OSes through:
{code:shell}
cmake -DBUILD_DOCS=ON -DSPARK_CONNECTOR=OFF .
make -j2
sudo make install
{code}
- Verify that the generated documentations (HTML and PDFs) are readable.",1,"Added documentation for pymcsapi and javamcsapi and added it to the build pipeline.

Needed to introduce following new dependencies:
- Sphinx version >= 1.7.5 is required to support automated line breaks in code blocks in generated PDFs.
- javasphinx is needed to extend Sphinx with the Java domain.

Both should be installed through pip3 instead of the operating system's package manger to get the newer versions. I updated the top level README.md accordingly.

-----

For QA:
- Test if the updated dependency install commands in README.md work on all OSes.
- Build and install the new documentations on all OSes through:
{code:shell}
cmake -DBUILD_DOCS=ON -DSPARK_CONNECTOR=OFF .
make -j2
sudo make install
{code}
- Verify that the generated documentations (HTML and PDFs) are readable."
327,MCOL-1439,MCOL,Daniel Lee,114042,2018-07-17 16:21:05,"Build verified: 1.1.6-1 source

/root/mariadb-columnstore-api
commit 4133c262f4b58f2ed49d69181a4b295d4c2f6764
Merge: 29a7e06 e231c95
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Jul 12 11:55:20 2018 +0100

    Merge pull request #90 from mariadb-corporation/MCOL-1439
    
    MCOL-1439 - pymcsapi and javamcsapi documentation


",2,"Build verified: 1.1.6-1 source

/root/mariadb-columnstore-api
commit 4133c262f4b58f2ed49d69181a4b295d4c2f6764
Merge: 29a7e06 e231c95
Author: Andrew Hutchings 
Date:   Thu Jul 12 11:55:20 2018 +0100

    Merge pull request #90 from mariadb-corporation/MCOL-1439
    
    MCOL-1439 - pymcsapi and javamcsapi documentation


"
328,MCOL-144,MCOL,Daniel Lee,84433,2016-06-21 14:18:38,Completed distributed functions results analysis.,1,Completed distributed functions results analysis.
329,MCOL-1446,MCOL,Daniel Lee,123975,2019-03-01 20:53:21,"Build verified: 1.2.3-1 nightly

server commit:
61f32f2
engine commit:
e849af0

",1,"Build verified: 1.2.3-1 nightly

server commit:
61f32f2
engine commit:
e849af0

"
330,MCOL-145,MCOL,David Hill,85337,2016-08-02 16:01:37,dependent on mcol-159 competing,1,dependent on mcol-159 competing
331,MCOL-145,MCOL,David Hill,85613,2016-08-16 16:26:53,Update the README.md file in server / develop repo..  Also has in it the Ubunutu build dependencies. Assign to Daniel to review and test out the new build,2,Update the README.md file in server / develop repo..  Also has in it the Ubunutu build dependencies. Assign to Daniel to review and test out the new build
332,MCOL-145,MCOL,Daniel Lee,85785,2016-08-23 19:57:21,Have been building using cents 7 with the instruction in the readme file.,3,Have been building using cents 7 with the instruction in the readme file.
333,MCOL-145,MCOL,Dipti Joshi,85789,2016-08-23 20:41:54,"[~dleeyh] Did you also tested the build instructions for Ubuntu ? (Using https://github.com/mariadb-corporation/mariadb-columnstore-server/tree/develop)
",4,"[~dleeyh] Did you also tested the build instructions for Ubuntu ? (Using URL
"
334,MCOL-146,MCOL,David Hill,84555,2016-06-24 20:52:43,"can now successfully generate builds that will install and launch. And you can geranate RPMs from. 

Currently regression test changes..",1,"can now successfully generate builds that will install and launch. And you can geranate RPMs from. 

Currently regression test changes.."
335,MCOL-146,MCOL,David Hill,84608,2016-06-27 22:21:04,looks like I have the build processing working where it generated the rpms as part of the nightly build process. Will do a run tonight and see if it fully 100% working.,2,looks like I have the build processing working where it generated the rpms as part of the nightly build process. Will do a run tonight and see if it fully 100% working.
336,MCOL-146,MCOL,David Hill,84637,2016-06-28 13:33:56,automatically generating the rpms using the new build process.,3,automatically generating the rpms using the new build process.
337,MCOL-147,MCOL,Daniel Lee,84436,2016-06-21 16:22:52,"Build verified: 1.0.1 Alpha

mscadmin> getsoft
getsoftwareinfo   Tue Jun 21 09:24:14 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 1                             Build Date: Mon 13 Jun 2016 06:50:26 PM CDT
Install Date: Fri 17 Jun 2016 10:53:22 AM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-1.src.rpm

Verified the schema sync procedure worked correctly in Columnstore

Commands used:

drop table tableName restrict;
create table tableName ......... comment='schema sync only'

",1,"Build verified: 1.0.1 Alpha

mscadmin> getsoft
getsoftwareinfo   Tue Jun 21 09:24:14 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 1                             Build Date: Mon 13 Jun 2016 06:50:26 PM CDT
Install Date: Fri 17 Jun 2016 10:53:22 AM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-1.src.rpm

Verified the schema sync procedure worked correctly in Columnstore

Commands used:

drop table tableName restrict;
create table tableName ......... comment='schema sync only'

"
338,MCOL-1475,MCOL,Andrew Hutchings,112495,2018-06-14 17:41:26,"Please do not merge until 1.1.5 has been released.

For QA: best test I've found is to create a MariaDB user that can't access the tables, use that as the cross engine user and try to do a cross engine select query. You should now get a much more verbose error message.",1,"Please do not merge until 1.1.5 has been released.

For QA: best test I've found is to create a MariaDB user that can't access the tables, use that as the cross engine user and try to do a cross engine select query. You should now get a much more verbose error message."
339,MCOL-1475,MCOL,Daniel Lee,113188,2018-06-28 16:12:59,"Build verified: 1.1.6-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.6-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c4e9b0ce3a396edd2353eb44532336e9db857f86
Merge: ac45350 ffb76bb
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Jun 27 17:36:13 2018 -0500

    Merge pull request #511 from mariadb-corporation/MCOL-1467-1
    
    MCOL-1467 - changes to get back to 1.1.6

Executed the mentioned test.

In 1.1.6-1, the error message is:
ERROR 1815 (HY000): Internal error: fatal error reading result from crossengine client lib (1044) (Access denied for user 'testuser'@'localhost' to database 'infinidb_vtable')

Confirmed the expected error message with developer.


In 1.1.5-1, the error message is:
MariaDB [mytest]> select * from orders t1, orderids t2 where t1.o_orderkey = t2.okey;
ERROR 1815 (HY000): Internal error: fatal error reading result from crossengine client lib(4294967295)(null pointer)


",2,"Build verified: 1.1.6-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.6-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c4e9b0ce3a396edd2353eb44532336e9db857f86
Merge: ac45350 ffb76bb
Author: David.Hall 
Date:   Wed Jun 27 17:36:13 2018 -0500

    Merge pull request #511 from mariadb-corporation/MCOL-1467-1
    
    MCOL-1467 - changes to get back to 1.1.6

Executed the mentioned test.

In 1.1.6-1, the error message is:
ERROR 1815 (HY000): Internal error: fatal error reading result from crossengine client lib (1044) (Access denied for user 'testuser'@'localhost' to database 'infinidb_vtable')

Confirmed the expected error message with developer.


In 1.1.5-1, the error message is:
MariaDB [mytest]> select * from orders t1, orderids t2 where t1.o_orderkey = t2.okey;
ERROR 1815 (HY000): Internal error: fatal error reading result from crossengine client lib(4294967295)(null pointer)


"
340,MCOL-1482,MCOL,Roman,112862,2018-06-22 09:18:01,"Greetings.
This is a known limitation, that won't be addressed until at least 1.4.",1,"Greetings.
This is a known limitation, that won't be addressed until at least 1.4."
341,MCOL-1482,MCOL,antoine,112874,2018-06-22 09:58:51,"ok, thanks. too bad.  ",2,"ok, thanks. too bad.  "
342,MCOL-1482,MCOL,Daniel Lee,193474,2021-07-02 16:41:53,"Build verified: 6.1.1 ( Drone #2635)

Reproduced the issue in 5.6.1 and verified the fix in 6.1.1.  Also tested update on views.
",3,"Build verified: 6.1.1 ( Drone #2635)

Reproduced the issue in 5.6.1 and verified the fix in 6.1.1.  Also tested update on views.
"
343,MCOL-1484,MCOL,Andrew Hutchings,112666,2018-06-19 14:16:51,For QA: call columnstore_info.table_usage() with a schema and/or table should be _much_ faster.,1,For QA: call columnstore_info.table_usage() with a schema and/or table should be _much_ faster.
344,MCOL-1484,MCOL,Daniel Lee,113191,2018-06-28 16:22:34,"Build verified: 1.1.6-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.6-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c4e9b0ce3a396edd2353eb44532336e9db857f86
Merge: ac45350 ffb76bb
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Jun 27 17:36:13 2018 -0500

    Merge pull request #511 from mariadb-corporation/MCOL-1467-1
    
    MCOL-1467 - changes to get back to 1.1.6

Timing is 4 times faster than 1.1.5-1


1.1.6-1

MariaDB [(none)]> call columnstore_info.table_usage(NULL,NULL);
+--------------+------------+-----------------+-----------------+-------------+
| TABLE_SCHEMA | TABLE_NAME | DATA_DISK_USAGE | DICT_DATA_USAGE | TOTAL_USAGE |
+--------------+------------+-----------------+-----------------+-------------+
| mytest       | orders     | 392.07 MB       | 132.02 MB       | 524.09 MB   |
+--------------+------------+-----------------+-----------------+-------------+
1 row in set (0.11 sec)

1.1.5-1

MariaDB [(none)]> call columnstore_info.table_usage(NULL,NULL);
+--------------+------------+-----------------+-----------------+-------------+
| TABLE_SCHEMA | TABLE_NAME | DATA_DISK_USAGE | DICT_DISK_USAGE | TOTAL_USAGE |
+--------------+------------+-----------------+-----------------+-------------+
| mytest       | orders     | 392.07 MB       | 132.02 MB       | 524.09 MB   |
+--------------+------------+-----------------+-----------------+-------------+
1 row in set (0.45 sec)
",2,"Build verified: 1.1.6-1 source

[root@localhost ~]# cat mariadb-columnstore-1.1.6-1-centos7.x86_64.bin.tar.gz.txt
/root/columnstore/mariadb-columnstore-server
commit 1741c7e7d522d1245ec9c1e4c7c7474574f09bd2
Merge: 2adc4b5 6abef48
Author: benthompson15 
Date:   Tue Jun 19 09:51:48 2018 -0500

    Merge pull request #113 from mariadb-corporation/davidhilldallas-patch-3
    
    update readme

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit c4e9b0ce3a396edd2353eb44532336e9db857f86
Merge: ac45350 ffb76bb
Author: David.Hall 
Date:   Wed Jun 27 17:36:13 2018 -0500

    Merge pull request #511 from mariadb-corporation/MCOL-1467-1
    
    MCOL-1467 - changes to get back to 1.1.6

Timing is 4 times faster than 1.1.5-1


1.1.6-1

MariaDB [(none)]> call columnstore_info.table_usage(NULL,NULL);
+--------------+------------+-----------------+-----------------+-------------+
| TABLE_SCHEMA | TABLE_NAME | DATA_DISK_USAGE | DICT_DATA_USAGE | TOTAL_USAGE |
+--------------+------------+-----------------+-----------------+-------------+
| mytest       | orders     | 392.07 MB       | 132.02 MB       | 524.09 MB   |
+--------------+------------+-----------------+-----------------+-------------+
1 row in set (0.11 sec)

1.1.5-1

MariaDB [(none)]> call columnstore_info.table_usage(NULL,NULL);
+--------------+------------+-----------------+-----------------+-------------+
| TABLE_SCHEMA | TABLE_NAME | DATA_DISK_USAGE | DICT_DISK_USAGE | TOTAL_USAGE |
+--------------+------------+-----------------+-----------------+-------------+
| mytest       | orders     | 392.07 MB       | 132.02 MB       | 524.09 MB   |
+--------------+------------+-----------------+-----------------+-------------+
1 row in set (0.45 sec)
"
345,MCOL-1496,MCOL,Andrew Hutchings,112954,2018-06-25 10:31:33,For QA: will fix ExeMgr crash in test001 when merged up to develop.,1,For QA: will fix ExeMgr crash in test001 when merged up to develop.
346,MCOL-1521,MCOL,Jens Röwekamp,113277,2018-06-29 23:33:07,For QA call the function ColumnStoreDriver.getJavaMcsapiVersion() and compare if the version matches the one given in libjavamcsapi.jar's manifest.,1,For QA call the function ColumnStoreDriver.getJavaMcsapiVersion() and compare if the version matches the one given in libjavamcsapi.jar's manifest.
347,MCOL-1521,MCOL,Daniel Lee,114054,2018-07-17 17:36:39,"Build verified: 1.1.6-1 source

/root/mariadb-columnstore-api
commit 4133c262f4b58f2ed49d69181a4b295d4c2f6764
Merge: 29a7e06 e231c95
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Jul 12 11:55:20 2018 +0100

    Merge pull request #90 from mariadb-corporation/MCOL-1439
    
    MCOL-1439 - pymcsapi and javamcsapi documentation

diff --cc java/CMakeLists.txt
index 7ca7c2e,373a20b..544dd9f
--- a/java/CMakeLists.txt
+++ b/java/CMakeLists.txt
@@@ -83,10 -83,10 +83,14 @@@ INSTALL(FILES ""${CMAKE_CURRENT_BINARY_D
  INSTALL(FILES build/libs/javamcsapi-${MCSAPI_VERSION_STRING}.jar DESTINATION  ${CMAKE_INSTALL_LIBDIR})
  INSTALL(FILES build/libs/javamcsapi.jar DESTINATION  ${CMAKE_INSTALL_LIBDIR})
  
+ if (BUILD_DOCS)
+     add_subdirectory(docs)
+ endif (BUILD_DOCS)
+ 
  IF(TEST_RUNNER)
 +    option(JAVA_COMPATIBILITY_TEST ""Run the Java mcsapi compatibility test"" ON)
      add_test(NAME Java_BasicTest COMMAND ""${CMAKE_CURRENT_SOURCE_DIR}/gradlew"" -p ${CMAKE_CURRENT_SOURCE_DIR} -Pversion=${MCSAPI_VERSION_STRING} -Pjava.library.path=${CMAKE_CURRENT_BINARY_DIR} test)
 +    IF(JAVA_COMPATIBILITY_TEST)
 +        add_test(NAME JavaMcsapi_mcsapi_compatibility_test COMMAND ""${CMAKE_CURRENT_SOURCE_DIR}/test/compatibility_test.sh"" ${VERSION_MAJOR} ${VERSION_MINOR} ${VERSION_PATCH})
 +    ENDIF(JAVA_COMPATIBILITY_TEST)
  ENDIF(TEST_RUNNER)

[root@localhost mariadb-columnstore-api]# javac -classpath "".:/usr/lib64/javamcsapi.jar"" MCSAPITest.java
[root@localhost mariadb-columnstore-api]# java -classpath "".:/usr/lib64/javamcsapi.jar"" MCSAPITest
1.1.6-4133c26
1.1.6-4133c26
[root@localhost mariadb-columnstore-api]# 
",2,"Build verified: 1.1.6-1 source

/root/mariadb-columnstore-api
commit 4133c262f4b58f2ed49d69181a4b295d4c2f6764
Merge: 29a7e06 e231c95
Author: Andrew Hutchings 
Date:   Thu Jul 12 11:55:20 2018 +0100

    Merge pull request #90 from mariadb-corporation/MCOL-1439
    
    MCOL-1439 - pymcsapi and javamcsapi documentation

diff --cc java/CMakeLists.txt
index 7ca7c2e,373a20b..544dd9f
--- a/java/CMakeLists.txt
+++ b/java/CMakeLists.txt
@@@ -83,10 -83,10 +83,14 @@@ INSTALL(FILES ""${CMAKE_CURRENT_BINARY_D
  INSTALL(FILES build/libs/javamcsapi-${MCSAPI_VERSION_STRING}.jar DESTINATION  ${CMAKE_INSTALL_LIBDIR})
  INSTALL(FILES build/libs/javamcsapi.jar DESTINATION  ${CMAKE_INSTALL_LIBDIR})
  
+ if (BUILD_DOCS)
+     add_subdirectory(docs)
+ endif (BUILD_DOCS)
+ 
  IF(TEST_RUNNER)
 +    option(JAVA_COMPATIBILITY_TEST ""Run the Java mcsapi compatibility test"" ON)
      add_test(NAME Java_BasicTest COMMAND ""${CMAKE_CURRENT_SOURCE_DIR}/gradlew"" -p ${CMAKE_CURRENT_SOURCE_DIR} -Pversion=${MCSAPI_VERSION_STRING} -Pjava.library.path=${CMAKE_CURRENT_BINARY_DIR} test)
 +    IF(JAVA_COMPATIBILITY_TEST)
 +        add_test(NAME JavaMcsapi_mcsapi_compatibility_test COMMAND ""${CMAKE_CURRENT_SOURCE_DIR}/test/compatibility_test.sh"" ${VERSION_MAJOR} ${VERSION_MINOR} ${VERSION_PATCH})
 +    ENDIF(JAVA_COMPATIBILITY_TEST)
  ENDIF(TEST_RUNNER)

[root@localhost mariadb-columnstore-api]# javac -classpath "".:/usr/lib64/javamcsapi.jar"" MCSAPITest.java
[root@localhost mariadb-columnstore-api]# java -classpath "".:/usr/lib64/javamcsapi.jar"" MCSAPITest
1.1.6-4133c26
1.1.6-4133c26
[root@localhost mariadb-columnstore-api]# 
"
348,MCOL-1524,MCOL,Jens Röwekamp,113569,2018-07-05 21:01:07,"The test executes the Java mcsapi tests with different combinations of mcsapi and javamcsapi of the same minor version (e.g. 1.1.x) to ensure compatibility. It uses the git tags columnstore-* to differentiate between versions.

Currently versions 1.1.3 and 1.1.5 aren't tagged and therefore not tested.

For QA:
Execute the test script on all supported operating systems through make test.",1,"The test executes the Java mcsapi tests with different combinations of mcsapi and javamcsapi of the same minor version (e.g. 1.1.x) to ensure compatibility. It uses the git tags columnstore-* to differentiate between versions.

Currently versions 1.1.3 and 1.1.5 aren't tagged and therefore not tested.

For QA:
Execute the test script on all supported operating systems through make test."
349,MCOL-1524,MCOL,Jens Röwekamp,113880,2018-07-13 00:52:36,"Just discovered that the test actually fails in out of source builds, but still states that it passed.

Therefore, I'll bounce the ticket back to me to fix it.",2,"Just discovered that the test actually fails in out of source builds, but still states that it passed.

Therefore, I'll bounce the ticket back to me to fix it."
350,MCOL-1524,MCOL,Jens Röwekamp,113989,2018-07-16 17:34:09,"- Fixed the out of source build issue
- CTestfile.cmake was pointing to the original test instead of the regarding backward/forward compatibility one. Fixed it as well. Now all tests are executed instead of executing three times the original/baseline test.",3,"- Fixed the out of source build issue
- CTestfile.cmake was pointing to the original test instead of the regarding backward/forward compatibility one. Fixed it as well. Now all tests are executed instead of executing three times the original/baseline test."
351,MCOL-1524,MCOL,Daniel Lee,114530,2018-07-26 18:35:35,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit f9f6dc43dd15ad3f2ca2d9e515b1e44028a16183
Merge: ced7eb4 1170b4e
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue Jul 24 18:06:24 2018 +0100

    Merge pull request #526 from mariadb-corporation/MCOL-1535
    
    Mcol 1535

/root/mariadb-columnstore-tools
commit 0d1ae73afa9521df7002d32b208f859510d54c0a
/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit f9f6dc43dd15ad3f2ca2d9e515b1e44028a16183
Merge: ced7eb4 1170b4e
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue Jul 24 18:06:24 2018 +0100

    Merge pull request #526 from mariadb-corporation/MCOL-1535
    
    Mcol 1535

/root/mariadb-columnstore-tools
commit 0d1ae73afa9521df7002d32b208f859510d54c0a
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jun 18 17:05:29 2018 -0500

    update version


All tests passed on Centos7",4,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings 
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit f9f6dc43dd15ad3f2ca2d9e515b1e44028a16183
Merge: ced7eb4 1170b4e
Author: Andrew Hutchings 
Date:   Tue Jul 24 18:06:24 2018 +0100

    Merge pull request #526 from mariadb-corporation/MCOL-1535
    
    Mcol 1535

/root/mariadb-columnstore-tools
commit 0d1ae73afa9521df7002d32b208f859510d54c0a
/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings 
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit f9f6dc43dd15ad3f2ca2d9e515b1e44028a16183
Merge: ced7eb4 1170b4e
Author: Andrew Hutchings 
Date:   Tue Jul 24 18:06:24 2018 +0100

    Merge pull request #526 from mariadb-corporation/MCOL-1535
    
    Mcol 1535

/root/mariadb-columnstore-tools
commit 0d1ae73afa9521df7002d32b208f859510d54c0a
Author: david hill 
Date:   Mon Jun 18 17:05:29 2018 -0500

    update version


All tests passed on Centos7"
352,MCOL-1532,MCOL,Roman,119252,2018-11-14 10:51:42,I was able to run ColumnStore w/o vtable using vanilla 10.2 server. There is a [branch|https://github.com/drrtuy/mariadb-columnstore-server/commits/10.2-w-columnstore] for the Server and patch for engine uploaded to google drive.,1,I was able to run ColumnStore w/o vtable using vanilla 10.2 server. There is a [branch|URL for the Server and patch for engine uploaded to google drive.
353,MCOL-1574,MCOL,Daniel Lee,117252,2018-10-01 16:46:41,"Created the features/oracleCompatibility test suite, which consists of 76 test cases at the moment.  I execute the test suite on the latest build ColumnStore 1.2.0-1. Two bugs have been identified and tickets have been created.

	MCOL-1751 Oracle Compatibility:  SELECT IF (...) INTO variable FROM DUAL result in syntax error
	MCOL-1752 Oracle Compatibility: A specific stored procedure caused mysqld to crash
",1,"Created the features/oracleCompatibility test suite, which consists of 76 test cases at the moment.  I execute the test suite on the latest build ColumnStore 1.2.0-1. Two bugs have been identified and tickets have been created.

	MCOL-1751 Oracle Compatibility:  SELECT IF (...) INTO variable FROM DUAL result in syntax error
	MCOL-1752 Oracle Compatibility: A specific stored procedure caused mysqld to crash
"
354,MCOL-1574,MCOL,Daniel Lee,117415,2018-10-04 16:54:19,Test suite has been created in Autopilot. Tests have been executed.  tickets have been open for issues found in both MariaDB 10.3.9 and ColumnStore 1.2 build. ,2,Test suite has been created in Autopilot. Tests have been executed.  tickets have been open for issues found in both MariaDB 10.3.9 and ColumnStore 1.2 build. 
355,MCOL-1577,MCOL,Ravi Prakash,115402,2018-08-17 00:41:55,Thing to notice is that source table can only be a columnstore table and hence only need to process supported syntax. I am using an already existing routine to extract the default values for a column and it generates an optional bracket for the current user function and hence the grammar was extended to allow this optional syntax. ,1,Thing to notice is that source table can only be a columnstore table and hence only need to process supported syntax. I am using an already existing routine to extract the default values for a column and it generates an optional bracket for the current user function and hence the grammar was extended to allow this optional syntax. 
356,MCOL-1577,MCOL,Daniel Lee,116499,2018-09-11 21:09:05,"Build verified:1.2.0-1 source
/root/columnstore/mariadb-columnstore-server
commit c2052269ef995332eedbef4fbd3ecc4a5e8e8cf0
Merge: b8fded1 d0385eb
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Mon Sep 10 09:07:03 2018 -0500
Merge pull request #131 from mariadb-corporation/build_fix
Revert ""MDEV-11036 Add link wsrep_sst_rsync_wan -> wsrep_sst_rsync""
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 764090ba0cf0bbb99092f5bfd9f8014464a136b1
Merge: 73b45ac 5b682a5
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Mon Sep 3 16:20:47 2018 +0100
Merge pull request #552 from drrtuy/MCOL-1510_4
MCOL-1510: Add CalpontSelectExecutionPlan::serialize() changes.

The source can also be Innodb tables, or other engines that the server already support.  This change adds the support for the ColumnStore engine.

Verified

1) syntax, on both InnoDB and ColumnStore tables
2) desc <tablename>
3) show create table <tablename>
4) entries in systable and syscolumn tables
5) tested table with all supported data types: datatypetestm table.
6) loaded data and execute queries
7) other DDL and DML statements",2,"Build verified:1.2.0-1 source
/root/columnstore/mariadb-columnstore-server
commit c2052269ef995332eedbef4fbd3ecc4a5e8e8cf0
Merge: b8fded1 d0385eb
Author: benthompson15 
Date: Mon Sep 10 09:07:03 2018 -0500
Merge pull request #131 from mariadb-corporation/build_fix
Revert ""MDEV-11036 Add link wsrep_sst_rsync_wan -> wsrep_sst_rsync""
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 764090ba0cf0bbb99092f5bfd9f8014464a136b1
Merge: 73b45ac 5b682a5
Author: Andrew Hutchings 
Date: Mon Sep 3 16:20:47 2018 +0100
Merge pull request #552 from drrtuy/MCOL-1510_4
MCOL-1510: Add CalpontSelectExecutionPlan::serialize() changes.

The source can also be Innodb tables, or other engines that the server already support.  This change adds the support for the ColumnStore engine.

Verified

1) syntax, on both InnoDB and ColumnStore tables
2) desc 
3) show create table 
4) entries in systable and syscolumn tables
5) tested table with all supported data types: datatypetestm table.
6) loaded data and execute queries
7) other DDL and DML statements"
357,MCOL-1578,MCOL,Jens Röwekamp,114566,2018-07-28 00:50:42,"- Ported the PDI plugin to Windows
- Ported PDI's test pipeline to Windows
- Adapted the build pipeline accordingly
- Added instructions to the documentation
- Tested on Windows 10 and CentOS 7

For QA:
Execute the test pipeline as documented in the top level Readme.md on CentOS, Ubuntu/Debian and Windows 10.
On Windows 10 run advanced tests as documented in the Pentaho Adapter Test Check List",1,"- Ported the PDI plugin to Windows
- Ported PDI's test pipeline to Windows
- Adapted the build pipeline accordingly
- Added instructions to the documentation
- Tested on Windows 10 and CentOS 7

For QA:
Execute the test pipeline as documented in the top level Readme.md on CentOS, Ubuntu/Debian and Windows 10.
On Windows 10 run advanced tests as documented in the Pentaho Adapter Test Check List"
358,MCOL-1578,MCOL,Jens Röwekamp,115571,2018-08-22 21:29:06,"Additional comments due to late review:
- I fixed the Windows Build in MCOL-1644 due to changed mcsapi library placements, therefore depending on with which version of mcsapi you are testing this one the build will fail. Therefore I strongly suggest to test develop-1.1 after both pull requests [51|https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/pull/51] and [57|https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/pull/57]  are merged.

For QA (addition): 
- MCOL-1606, MCOL-1643, MCOL-1644 and MCOL-1578 can be tested together after the review is complete
",2,"Additional comments due to late review:
- I fixed the Windows Build in MCOL-1644 due to changed mcsapi library placements, therefore depending on with which version of mcsapi you are testing this one the build will fail. Therefore I strongly suggest to test develop-1.1 after both pull requests [51|URL and [57|URL  are merged.

For QA (addition): 
- MCOL-1606, MCOL-1643, MCOL-1644 and MCOL-1578 can be tested together after the review is complete
"
359,MCOL-159,MCOL,Ben Thompson,85336,2016-08-02 15:59:20,"make / make install working on CentOS6 CentOS7 and Ubuntu14.04 , Need to merge with develop changes to test on Ubuntu16

Need to investigate and fix RPM generation fails.",1,"make / make install working on CentOS6 CentOS7 and Ubuntu14.04 , Need to merge with develop changes to test on Ubuntu16

Need to investigate and fix RPM generation fails."
360,MCOL-159,MCOL,David Hill,85612,2016-08-16 16:04:32,review complete and using the new build instructions to generate the build now.. So all is working with the new cmake changes,2,review complete and using the new build instructions to generate the build now.. So all is working with the new cmake changes
361,MCOL-1591,MCOL,David Hill,114747,2018-08-01 18:36:12,"how to test

//set umask to 022 and run test

umask 022
root@dhill-Z370-HD3:/usr/local/mariadb/columnstore/bin# ./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : Ubuntu 18.04.1 LTS

** Run UMASK check

UMASK local setting test passed

** Run SELINUX check

Local Node SELINUX setting is Not Enabled

** Run MariaDB Console Password check

Passed, no problems detected with a MariaDB password being set without an associated /root/.my.cnf

** Run MariaDB ColumnStore Dependent Package Check

Local Node - Passed, all dependency packages are installed
Local Node - Passed, all packages that should not be installed aren't installed


*** Finished Validation of the Cluster, all Tests Passed ***

root@dhill-Z370-HD3:/usr/local/mariadb/columnstore/bin# 

// set to non 022, like 077

umask 077
root@dhill-Z370-HD3:/usr/local/mariadb/columnstore/bin# ./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : Ubuntu 18.04.1 LTS

** Run UMASK check

Warning, UMASK test failed, check local UMASK setting. Requirement is set to 0022

Failure occurred, do you want to continue? (y,n) > 
",1,"how to test

//set umask to 022 and run test

umask 022
root@dhill-Z370-HD3:/usr/local/mariadb/columnstore/bin# ./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : Ubuntu 18.04.1 LTS

** Run UMASK check

UMASK local setting test passed

** Run SELINUX check

Local Node SELINUX setting is Not Enabled

** Run MariaDB Console Password check

Passed, no problems detected with a MariaDB password being set without an associated /root/.my.cnf

** Run MariaDB ColumnStore Dependent Package Check

Local Node - Passed, all dependency packages are installed
Local Node - Passed, all packages that should not be installed aren't installed


*** Finished Validation of the Cluster, all Tests Passed ***

root@dhill-Z370-HD3:/usr/local/mariadb/columnstore/bin# 

// set to non 022, like 077

umask 077
root@dhill-Z370-HD3:/usr/local/mariadb/columnstore/bin# ./columnstoreClusterTester.sh 

*** This is the MariaDB Columnstore Cluster System Test Tool ***

** Validate local OS is supported

Local Node OS System Name : Ubuntu 18.04.1 LTS

** Run UMASK check

Warning, UMASK test failed, check local UMASK setting. Requirement is set to 0022

Failure occurred, do you want to continue? (y,n) > 
"
362,MCOL-1591,MCOL,David Hill,114750,2018-08-01 21:36:34,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/531,2,URL
363,MCOL-1591,MCOL,Daniel Lee,115031,2018-08-08 20:06:52,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ee40c3ac050ad7b64302673fc4ab08640f64892f
Merge: 0df1b92 979d00a
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Mon Aug 6 13:02:08 2018 -0500

    Merge pull request #523 from mariadb-corporation/MCOL-1579
    
    MCOL-1579 Remove chmod of /dev/shm

The cluster tester tool now checks for umask setting.  Tested with umask values 022 and 027.
",3,"Build verified: 1.1.6-1 source

/root/columnstore/mariadb-columnstore-server
commit 513775738f72ec990d055a5d47e2511e3c0e34dd
Merge: 3c37210 9236098
Author: Andrew Hutchings 
Date:   Wed Jul 18 09:37:17 2018 +0100

    Merge pull request #123 from drrtuy/MCOL-970
    
    MCOL-970 Slow query log now contains original query even in vtable mode

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ee40c3ac050ad7b64302673fc4ab08640f64892f
Merge: 0df1b92 979d00a
Author: benthompson15 
Date:   Mon Aug 6 13:02:08 2018 -0500

    Merge pull request #523 from mariadb-corporation/MCOL-1579
    
    MCOL-1579 Remove chmod of /dev/shm

The cluster tester tool now checks for umask setting.  Tested with umask values 022 and 027.
"
364,MCOL-1596,MCOL,markus makela,114964,2018-08-07 10:15:28,"Added the {{-f}} option which takes a TSV file as an argument. The file should contain database and table names separated by tabs and terminated by newlines, one database and one table per line. The documentation for this can be found [here|https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/blob/develop-1.1-markusjm/maxscale-cdc-adapter/README.md#streaming-multiple-tables].",1,"Added the {{-f}} option which takes a TSV file as an argument. The file should contain database and table names separated by tabs and terminated by newlines, one database and one table per line. The documentation for this can be found [here|URL"
365,MCOL-160,MCOL,Ben Thompson,85335,2016-08-02 15:55:50,Ran quick test with engine cmake changes and found there are some source generating scripts that fail in the out-of-tree build. Some minor changes required to complete.,1,Ran quick test with engine cmake changes and found there are some source generating scripts that fail in the out-of-tree build. Some minor changes required to complete.
366,MCOL-160,MCOL,Andrew Hutchings,86731,2016-09-22 23:33:58,"Looks good to me. Sending to Daniel for QA, likely some test suite updates needed.

Only potential issue I can see is if you run cmake with a given path and then run cmake again without a path it may reset the path back to the default. I don't have any evidence to back that up it is just how I recall the cmake cache works.

If that is the case we can follow it up in another ticket as that would not be a usual use case.",2,"Looks good to me. Sending to Daniel for QA, likely some test suite updates needed.

Only potential issue I can see is if you run cmake with a given path and then run cmake again without a path it may reset the path back to the default. I don't have any evidence to back that up it is just how I recall the cmake cache works.

If that is the case we can follow it up in another ticket as that would not be a usual use case."
367,MCOL-160,MCOL,Daniel Lee,87176,2016-10-07 10:04:51,"Mr.  Hill, can you verify this one?",3,"Mr.  Hill, can you verify this one?"
368,MCOL-160,MCOL,David Hill,87186,2016-10-07 14:26:44,"reject testing... there is zero INFORMATION in this bug on how to test and what to look for..

Please update",4,"reject testing... there is zero INFORMATION in this bug on how to test and what to look for..

Please update"
369,MCOL-160,MCOL,Ben Thompson,87187,2016-10-07 16:01:42,"The README.md for the server has been updated with examples and how to use the new cmake options. Basically if you can create a directory and get it to successfully run make / sudo make install / make package, then it's working",5,"The README.md for the server has been updated with examples and how to use the new cmake options. Basically if you can create a directory and get it to successfully run make / sudo make install / make package, then it's working"
370,MCOL-160,MCOL,David Hill,87192,2016-10-07 18:44:16,tested and it works,6,tested and it works
371,MCOL-1615,MCOL,Daniel Lee,115485,2018-08-20 15:30:49,"Build verified: 1.1.6-1 source
/root/columnstore/mariadb-columnstore-server
commit bab181f892fdbfba7ca287115bd26581c3bd2e67
Merge: 5137757 a035b4a
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Aug 15 10:22:39 2018 -0500

    Merge pull request #126 from mariadb-corporation/MCOL-1615
    
    MCOL-1615 Merge MariaDB 10.2.17 into develop-1.1

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit fdb4ef7f796d9c8ad664b71544743da6f64f480d
Merge: b5c3888 a98aec0
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Aug 17 08:02:02 2018 +0100

    Merge pull request #542 from drrtuy/MCOL-1655
    
    MCOL-1655 removed hardcoded %debug from ddl.y.

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.2.17-MariaDB-log Columnstore 1.1.6-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
",1,"Build verified: 1.1.6-1 source
/root/columnstore/mariadb-columnstore-server
commit bab181f892fdbfba7ca287115bd26581c3bd2e67
Merge: 5137757 a035b4a
Author: David.Hall 
Date:   Wed Aug 15 10:22:39 2018 -0500

    Merge pull request #126 from mariadb-corporation/MCOL-1615
    
    MCOL-1615 Merge MariaDB 10.2.17 into develop-1.1

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit fdb4ef7f796d9c8ad664b71544743da6f64f480d
Merge: b5c3888 a98aec0
Author: Andrew Hutchings 
Date:   Fri Aug 17 08:02:02 2018 +0100

    Merge pull request #542 from drrtuy/MCOL-1655
    
    MCOL-1655 removed hardcoded %debug from ddl.y.

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.2.17-MariaDB-log Columnstore 1.1.6-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
"
372,MCOL-1617,MCOL,Jens Röwekamp,114796,2018-08-02 18:39:34,removed the building guide as discussed.,1,removed the building guide as discussed.
373,MCOL-1633,MCOL,Jens Röwekamp,115213,2018-08-13 18:13:29,"We could ship the Visual C++ Redistributable directly with our application through [merge modules|http://wixtoolset.org/documentation/manual/v3/howtos/redistributables_and_install_checks/install_vcredist.html] but [it seems|http://git.savannah.gnu.org/cgit/libiconv.git/tree/README.woe32?h=1.11.x] that such a shipment would violate GPL and the Microsoft Eula.

Otherwise we probably have to think about switching to a different toolset to build the installer.",1,"We could ship the Visual C++ Redistributable directly with our application through [merge modules|URL but [it seems|URL that such a shipment would violate GPL and the Microsoft Eula.

Otherwise we probably have to think about switching to a different toolset to build the installer."
374,MCOL-1633,MCOL,Jens Röwekamp,115442,2018-08-17 23:52:02,"Another way is to avoid using the redistributable at all and switch entirely to Microsoft's Universal CRT. But, currently I don't know how to accomplish that.",2,"Another way is to avoid using the redistributable at all and switch entirely to Microsoft's Universal CRT. But, currently I don't know how to accomplish that."
375,MCOL-1633,MCOL,Jens Röwekamp,115692,2018-08-27 17:42:18,"Added the Visual Studio C++ 2017 Redistributable (x64) to the mcsapi installer in Windows.

It installs the redistributable through merge moduls if it is not found.

As mcsapi is LGPL licensed there is no conflict with bundling the redistributable in the installer.


For QA:
- build the installer as documented and test if it installs the redistributable on a clean Windows 10 machine (check if the shipped examples are executable)
- execute the regression test suite on Windows and Linux",3,"Added the Visual Studio C++ 2017 Redistributable (x64) to the mcsapi installer in Windows.

It installs the redistributable through merge moduls if it is not found.

As mcsapi is LGPL licensed there is no conflict with bundling the redistributable in the installer.


For QA:
- build the installer as documented and test if it installs the redistributable on a clean Windows 10 machine (check if the shipped examples are executable)
- execute the regression test suite on Windows and Linux"
376,MCOL-1633,MCOL,David Thompson,117935,2018-10-15 23:34:33,"If you have either vcredist 2015 or 2017, no additional install is requested. After removing a prior install of vcredist 2015, the installer showed that it was going to install vcredist 2017 as a merge module (so it does not show up in apps and features).",4,"If you have either vcredist 2015 or 2017, no additional install is requested. After removing a prior install of vcredist 2015, the installer showed that it was going to install vcredist 2017 as a merge module (so it does not show up in apps and features)."
377,MCOL-1634,MCOL,Jens Röwekamp,115088,2018-08-10 00:58:36,"Some dependent libraries weren't able to be built through cmake, but through cygwin or batch scripts. Therefore, the automation would have been more complicated.

As a result I decided to add an archive of pre-compiled dependent libraries to the documentation whose contents can be directly copied to the Visual Studio installation. This way we ensure that we use the same libraries for every build. The archive is hosted in our ColumnStore Google Drive so that we have version control as well.

I further documented how each library of the archive was acquired or built, and if our mcsapi needs the static or shared variant. This way updates of dependent libraries should be easily possible in the future.

I further changed from dynamic googletest libraries to static googletest libraries and adapted the CMake scripts accordingly.

For QA:
Built the bulk write adapter on a clean Windows 10 machine according to the documentation and run the regression test suite.",1,"Some dependent libraries weren't able to be built through cmake, but through cygwin or batch scripts. Therefore, the automation would have been more complicated.

As a result I decided to add an archive of pre-compiled dependent libraries to the documentation whose contents can be directly copied to the Visual Studio installation. This way we ensure that we use the same libraries for every build. The archive is hosted in our ColumnStore Google Drive so that we have version control as well.

I further documented how each library of the archive was acquired or built, and if our mcsapi needs the static or shared variant. This way updates of dependent libraries should be easily possible in the future.

I further changed from dynamic googletest libraries to static googletest libraries and adapted the CMake scripts accordingly.

For QA:
Built the bulk write adapter on a clean Windows 10 machine according to the documentation and run the regression test suite."
378,MCOL-1634,MCOL,Andrew Hutchings,117929,2018-10-15 19:21:09,Buildbot proves this is working.,2,Buildbot proves this is working.
379,MCOL-1642,MCOL,Andrew Hutchings,116729,2018-09-17 15:49:22,[~markus makela] would you prefer this to be in the form of a UDF?,1,[~markus makela] would you prefer this to be in the form of a UDF?
380,MCOL-1642,MCOL,markus makela,116993,2018-09-24 09:19:09,"We just need access to it via the SQL interface. Anything else works for us but to keep it consistent, it could be a function.",2,"We just need access to it via the SQL interface. Anything else works for us but to keep it consistent, it could be a function."
381,MCOL-1642,MCOL,markus makela,118349,2018-10-29 10:52:18,I'll define the preliminary function name as {{mcsSystemPrimary}} and work based on the assumption that this function returns 1 if the system is the primary UM and 0 if it's not.,3,I'll define the preliminary function name as {{mcsSystemPrimary}} and work based on the assumption that this function returns 1 if the system is the primary UM and 0 if it's not.
382,MCOL-1642,MCOL,David Hall,118364,2018-10-29 14:18:45,I will create the function as Markus describes.,4,I will create the function as Markus describes.
383,MCOL-1642,MCOL,David Hall,118555,2018-11-01 16:08:23,"To test, on any multi-UM system run:
UM1:
MariaDB [(none)]> select mcsSystemPrimary();
+--------------------+
| mcsSystemPrimary() |
+--------------------+
|                  1 |
+--------------------+
1 row in set (0.005 sec)
UM2...N
MariaDB [(none)]> select mcsSystemPrimary();
+--------------------+
| mcsSystemPrimary() |
+--------------------+
|                  0 |
+--------------------+
1 row in set (0.004 sec)

",5,"To test, on any multi-UM system run:
UM1:
MariaDB [(none)]> select mcsSystemPrimary();
+--------------------+
| mcsSystemPrimary() |
+--------------------+
|                  1 |
+--------------------+
1 row in set (0.005 sec)
UM2...N
MariaDB [(none)]> select mcsSystemPrimary();
+--------------------+
| mcsSystemPrimary() |
+--------------------+
|                  0 |
+--------------------+
1 row in set (0.004 sec)

"
384,MCOL-1642,MCOL,Andrew Hutchings,118948,2018-11-09 08:06:37,Re-assigned to me to review since Ben is away,6,Re-assigned to me to review since Ben is away
385,MCOL-1642,MCOL,markus makela,119147,2018-11-13 08:44:17,Where are the tests for columnstore located?,7,Where are the tests for columnstore located?
386,MCOL-1642,MCOL,markus makela,119191,2018-11-13 16:03:49,Manually tested and the function seems to work. It returns 1 on the primary UM and 0 on secondary.,8,Manually tested and the function seems to work. It returns 1 on the primary UM and 0 on secondary.
387,MCOL-1642,MCOL,markus makela,119201,2018-11-13 17:02:11,Added a simple test case and assigned it for review.,9,Added a simple test case and assigned it for review.
388,MCOL-1644,MCOL,Jens Röwekamp,115570,2018-08-22 21:18:51,"- Added PDI 8.1 to the test scripts for Windows and Linux
- Fixed Windows build script due to changed mcsapi library placements

For QA: 
- MCOL-1606, MCOL-1643, MCOL-1644 and MCOL-1578 can be tested together after the review is complete
- run the regression test suite on Windows and Linux",1,"- Added PDI 8.1 to the test scripts for Windows and Linux
- Fixed Windows build script due to changed mcsapi library placements

For QA: 
- MCOL-1606, MCOL-1643, MCOL-1644 and MCOL-1578 can be tested together after the review is complete
- run the regression test suite on Windows and Linux"
389,MCOL-1644,MCOL,Elena Kotsinova,117811,2018-10-12 11:55:17,it is ok forgot to close it,2,it is ok forgot to close it
390,MCOL-1646,MCOL,David Hill,115192,2018-08-13 14:25:04,"AX install docs has been updated

https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-mariadb-download/

https://mariadb.com/kb/en/library/installing-mariadb-ax-from-the-package-repositories/",1,"AX install docs has been updated

URL

URL"
391,MCOL-1661,MCOL,markus makela,115496,2018-08-20 21:04:08,Implemented a mode that replicates changes to CS. It's slow since it uses DMLs but MCOL-1662 should improve it slightly.,1,Implemented a mode that replicates changes to CS. It's slow since it uses DMLs but MCOL-1662 should improve it slightly.
392,MCOL-1661,MCOL,Elena Kotsinova,115555,2018-08-22 12:40:51,[~markus makela] What is the Fixed version? In which version fix will be included?,2,[~markus makela] What is the Fixed version? In which version fix will be included?
393,MCOL-1661,MCOL,markus makela,115558,2018-08-22 12:54:24,Probably the next release made from {{develop-1.1}}. I'll update the fixVersion field.,3,Probably the next release made from {{develop-1.1}}. I'll update the fixVersion field.
394,MCOL-1661,MCOL,Elena Kotsinova,116470,2018-09-11 13:35:22,"Markus, I need more information on this.
Does this mean that the record updated in TX  appears updated in CS instead of inserting 2 new additional rows with event_type - ""update before"" and ""update after""?
Because I continue to see additional records for update and delete in 1.1.6",4,"Markus, I need more information on this.
Does this mean that the record updated in TX  appears updated in CS instead of inserting 2 new additional rows with event_type - ""update before"" and ""update after""?
Because I continue to see additional records for update and delete in 1.1.6"
395,MCOL-1661,MCOL,markus makela,116497,2018-09-11 20:45:49,"The new `-z` flag was added that enables the transformation mode. Was it enabled? This is required to have the adapter ""replicate"" changes to ColumnStore.",5,"The new `-z` flag was added that enables the transformation mode. Was it enabled? This is required to have the adapter ""replicate"" changes to ColumnStore."
396,MCOL-1661,MCOL,Elena Kotsinova,116529,2018-09-12 11:06:11,"I tried the -z option and I received the following failure message:

[root@maxscale-cdc QA]# mxs_adapter -s /root/QA/ -c /etc/Columnstore.xml -u cdcuser -p cdcpassword -h localhost -P 4001 -f /root/QA/2DBConfig.tsv -z
2018-09-12 14:06:08 [cdc_eli.edk] Continuing from GTID: 0-1-38:2
2018-09-12 14:06:08 [cdc_eli.edk_delta] Continuing from GTID: 0-1-33:1
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)

Do I need to setup something in addition?
The 172.20.2.240 is the server where the maxscale and adapter run. ColumnStore is on other server.

",6,"I tried the -z option and I received the following failure message:

[root@maxscale-cdc QA]# mxs_adapter -s /root/QA/ -c /etc/Columnstore.xml -u cdcuser -p cdcpassword -h localhost -P 4001 -f /root/QA/2DBConfig.tsv -z
2018-09-12 14:06:08 [cdc_eli.edk] Continuing from GTID: 0-1-38:2
2018-09-12 14:06:08 [cdc_eli.edk_delta] Continuing from GTID: 0-1-33:1
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)
2018-09-12 14:06:08 [cdc_eli.edk] Failed to connect to ColumnStore SQL interface: Access denied for user 'root'@'172.20.2.240' (using password: NO)

Do I need to setup something in addition?
The 172.20.2.240 is the server where the maxscale and adapter run. ColumnStore is on other server.

"
397,MCOL-1661,MCOL,markus makela,116531,2018-09-12 11:27:43,"The transformation mode uses the user used for cross-engine queries. The user defined in {{Columnstore.xml}} would require access to create and modify tables.

It might be that this is not a proper user to use for this and a separate one should be defined in {{Columnstore.xml}}.",7,"The transformation mode uses the user used for cross-engine queries. The user defined in {{Columnstore.xml}} would require access to create and modify tables.

It might be that this is not a proper user to use for this and a separate one should be defined in {{Columnstore.xml}}."
398,MCOL-1661,MCOL,Elena Kotsinova,116536,2018-09-12 12:43:34,"I think that this should be explicitly mentioned in the user guide.
User should prepare in addition cross engine configuration (i.e. to create proper user and grant permissions). 
I will create a separate issue about this.
",8,"I think that this should be explicitly mentioned in the user guide.
User should prepare in addition cross engine configuration (i.e. to create proper user and grant permissions). 
I will create a separate issue about this.
"
399,MCOL-1671,MCOL,Jens Röwekamp,117568,2018-10-08 23:39:55,"Including javamcsapi directly into the JRE or JDK wasn't possible as javamcsapi wouldn't be transferred to new JRE or JDK directories after a Java update.

Instead the Windows PATH is extended to mcsapi's library directory. This way Java is able to access javamcsapi's shared libraries and customer's don't have to specify the java.library.path while executing Java programs relying on javamcsapi.

The setting of the PATH entry can be deactivated in the installer if not needed or required.

For QA:
- run the regression test suite
- execute attached test.jar from the Desktop via java -jar test.jar and verify that the program doesn't exit with an error message that dependent shared libraries aren't found.",1,"Including javamcsapi directly into the JRE or JDK wasn't possible as javamcsapi wouldn't be transferred to new JRE or JDK directories after a Java update.

Instead the Windows PATH is extended to mcsapi's library directory. This way Java is able to access javamcsapi's shared libraries and customer's don't have to specify the java.library.path while executing Java programs relying on javamcsapi.

The setting of the PATH entry can be deactivated in the installer if not needed or required.

For QA:
- run the regression test suite
- execute attached test.jar from the Desktop via java -jar test.jar and verify that the program doesn't exit with an error message that dependent shared libraries aren't found."
400,MCOL-1671,MCOL,Zdravelina Sokolovska,118912,2018-11-08 12:58:32,"checked on Windows10 with the attached msi
C:\Users\elena.kotsinova\Downloads>java -version
java version ""1.8.0_191""
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) Client VM (build 25.191-b12, mixed mode, sharing)

C:\Users\elena.kotsinova\Downloads>java -jar test.jar
Native code library failed to load by parent classloader.
Ensure that it is loaded by a child classloader
java.lang.UnsatisfiedLinkError: C:\Program Files\MariaDB\ColumnStore Bulk Write SDK\lib\javamcsapi.dll: Can't load AMD 64-bit .dll on a IA 32-bit platform
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: com.mariadb.columnstore.api.javamcsapiJNI.new_ColumnStoreDriver__SWIG_1()J
        at com.mariadb.columnstore.api.javamcsapiJNI.new_ColumnStoreDriver__SWIG_1(Native Method)
        at com.mariadb.columnstore.api.ColumnStoreDriver.<init>(ColumnStoreDriver.java:78)
        at grew.Blub.main(Blub.java:8)

C:\Users\elena.kotsinova\Downloads>

",2,"checked on Windows10 with the attached msi
C:\Users\elena.kotsinova\Downloads>java -version
java version ""1.8.0_191""
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) Client VM (build 25.191-b12, mixed mode, sharing)

C:\Users\elena.kotsinova\Downloads>java -jar test.jar
Native code library failed to load by parent classloader.
Ensure that it is loaded by a child classloader
java.lang.UnsatisfiedLinkError: C:\Program Files\MariaDB\ColumnStore Bulk Write SDK\lib\javamcsapi.dll: Can't load AMD 64-bit .dll on a IA 32-bit platform
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: com.mariadb.columnstore.api.javamcsapiJNI.new_ColumnStoreDriver__SWIG_1()J
        at com.mariadb.columnstore.api.javamcsapiJNI.new_ColumnStoreDriver__SWIG_1(Native Method)
        at com.mariadb.columnstore.api.ColumnStoreDriver.(ColumnStoreDriver.java:78)
        at grew.Blub.main(Blub.java:8)

C:\Users\elena.kotsinova\Downloads>

"
401,MCOL-1671,MCOL,Jens Röwekamp,118940,2018-11-08 19:28:14,"mcsapi can only be used on 64Bit systems. Therefore, please test with a 64Bit Windows.

I opened MCOL-1865 to check why you were able to install mcsapi on a 32Bit system.",3,"mcsapi can only be used on 64Bit systems. Therefore, please test with a 64Bit Windows.

I opened MCOL-1865 to check why you were able to install mcsapi on a 32Bit system."
402,MCOL-1682,MCOL,Jens Röwekamp,115811,2018-08-29 23:48:40,"requested changes implemented and tested on CentOS 7 and Win 10.

For QA:
Run test suite on Win 10, Debian/Ubuntu and CentOS.",1,"requested changes implemented and tested on CentOS 7 and Win 10.

For QA:
Run test suite on Win 10, Debian/Ubuntu and CentOS."
403,MCOL-1698,MCOL,David Hall,120569,2018-12-10 21:26:34,"This was merged in 1.1.6 and there is no more work to do. The purpose of this feature is to implement the distinct_count() aggregate function because Server can't parse DISTINCT on UDF. When it does, this feature will be used to handle that as well.

For use as an Aggregate, the DISTINCT functionality will need to be added to the Aggregate UDAF logic, which it currently isn't. At least I don't think it would work.",1,"This was merged in 1.1.6 and there is no more work to do. The purpose of this feature is to implement the distinct_count() aggregate function because Server can't parse DISTINCT on UDF. When it does, this feature will be used to handle that as well.

For use as an Aggregate, the DISTINCT functionality will need to be added to the Aggregate UDAF logic, which it currently isn't. At least I don't think it would work."
404,MCOL-1698,MCOL,David Hall,120570,2018-12-10 21:27:20,Closing because this has been in the release for a while already.,2,Closing because this has been in the release for a while already.
405,MCOL-1713,MCOL,Jens Röwekamp,116503,2018-09-12 01:04:55,"For QA:
- build kettle adapter on Windows 10 and verify that name contains the win suffix.",1,"For QA:
- build kettle adapter on Windows 10 and verify that name contains the win suffix."
406,MCOL-1713,MCOL,Elena Kotsinova,117056,2018-09-25 15:27:19,"Win suffix is added only when build is prepared with cmake.
Building with gradlew.bat - instructions from here https://github.com/mariadb-corporation/mariadb-columnstore-data-adapters/tree/develop-1.1/kettle-columnstore-bulk-exporter-plugin
doesn't produce win suffix to the zip file",2,"Win suffix is added only when build is prepared with cmake.
Building with gradlew.bat - instructions from here URL
doesn't produce win suffix to the zip file"
407,MCOL-1713,MCOL,Jens Röwekamp,117108,2018-09-26 20:58:27,"That's true. 

We use cmake to generate the packages and during that process the plugin is renamed.

I added a note to the Readme.md to point out that during manual build via gradle no OS and release information is included in the plugin's archive's name.",3,"That's true. 

We use cmake to generate the packages and during that process the plugin is renamed.

I added a note to the Readme.md to point out that during manual build via gradle no OS and release information is included in the plugin's archive's name."
408,MCOL-1719,MCOL,Andrew Hutchings,116621,2018-09-13 14:39:31,"Rebase applied direct to develop branch, pull request not possible for these kind of rebases.",1,"Rebase applied direct to develop branch, pull request not possible for these kind of rebases."
409,MCOL-1719,MCOL,Daniel Lee,123980,2019-03-02 03:03:25,"Build verified: 1.2.3-1 nightly

It is now on 10.3.11",2,"Build verified: 1.2.3-1 nightly

It is now on 10.3.11"
410,MCOL-1731,MCOL,Jens Röwekamp,117191,2018-09-28 23:15:57,"Custom JDBC connection string added in our Informatica PowerCenter connector that overwrites the default host, port, database settings.

Tested on Informatica PowerCenter 10.2 on Windows Server 2012.

For QA:
- Build the connector as documented
- import it into PowerCenter
- test the custom JDBC connection string in PowerCenter Designer
- test the custom JDBC connection string in PowerCenter WorkFlow Manager",1,"Custom JDBC connection string added in our Informatica PowerCenter connector that overwrites the default host, port, database settings.

Tested on Informatica PowerCenter 10.2 on Windows Server 2012.

For QA:
- Build the connector as documented
- import it into PowerCenter
- test the custom JDBC connection string in PowerCenter Designer
- test the custom JDBC connection string in PowerCenter WorkFlow Manager"
411,MCOL-1731,MCOL,Andrew Hutchings,119282,2018-11-14 17:18:57,"This ticket is marked as ""IN TESTING"" but has no version. Please fix as a matter of urgency.",2,"This ticket is marked as ""IN TESTING"" but has no version. Please fix as a matter of urgency."
412,MCOL-1739,MCOL,Jens Röwekamp,119140,2018-11-13 00:44:15,"The Debian / Ubuntu / CentOS packages of mariadb-columnstore-api are now split into its components with dependencies set between each other.
On Windows there is still one MSI installer but with options to install each component separately.

On Linux we now have packages for:
- mariadb-columnstore-api-cpp (the base cpp shared libraries)
- mariadb-columnstore-api-cpp_devel (the cpp header files and examples for development)
- mariadb-columnstore-api-java (the Java shared libraries for usage and development)
- mariadb-columnstore-api-python (the Python 2.7 shared libraries for usage and development)
- mariadb-columnstore-api-python3 (the Python 3 shared libraries for usage and development)
- mariadb-columnstore-api-spark (the Spark ColumnStoreExporter for Scala)
- mariadb-columnstore-api-pyspark (the PySpark ColumnStoreExporter for Python 2.7)
- mariadb-columnstore-api-pyspark3 (the PySpark ColumnStoreExporter for Python 3)


@[~ben.thompson] I don't know if we have to apply changes to buildbot

I further don't know how this change impacts other projects. (i.e. data-adapters, mcsimport & dbaas container) We might have to update these projects as well to use the new packages.

----

*For QA*
- Do an install test with the new packages on Debian / Ubuntu / CentOS and Windows
- Further test if the dependencies are automatically resolved when installing from our repository (e.g. try just to install mariadb-columnstore-api-spark via apt and yum and check that mariadb-columnstore-api-java and mariadb-columnstore-api-cpp are also installed)",1,"The Debian / Ubuntu / CentOS packages of mariadb-columnstore-api are now split into its components with dependencies set between each other.
On Windows there is still one MSI installer but with options to install each component separately.

On Linux we now have packages for:
- mariadb-columnstore-api-cpp (the base cpp shared libraries)
- mariadb-columnstore-api-cpp_devel (the cpp header files and examples for development)
- mariadb-columnstore-api-java (the Java shared libraries for usage and development)
- mariadb-columnstore-api-python (the Python 2.7 shared libraries for usage and development)
- mariadb-columnstore-api-python3 (the Python 3 shared libraries for usage and development)
- mariadb-columnstore-api-spark (the Spark ColumnStoreExporter for Scala)
- mariadb-columnstore-api-pyspark (the PySpark ColumnStoreExporter for Python 2.7)
- mariadb-columnstore-api-pyspark3 (the PySpark ColumnStoreExporter for Python 3)


@[~ben.thompson] I don't know if we have to apply changes to buildbot

I further don't know how this change impacts other projects. (i.e. data-adapters, mcsimport & dbaas container) We might have to update these projects as well to use the new packages.

----

*For QA*
- Do an install test with the new packages on Debian / Ubuntu / CentOS and Windows
- Further test if the dependencies are automatically resolved when installing from our repository (e.g. try just to install mariadb-columnstore-api-spark via apt and yum and check that mariadb-columnstore-api-java and mariadb-columnstore-api-cpp are also installed)"
413,MCOL-1739,MCOL,Zdravelina Sokolovska,119887,2018-11-27 16:08:47,"dependencies from mcsapi are resolved automatically through mariadb-columnstore-api bundle packages

[root@localhost ~]# yum list available mariadb-columnstore-*
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Available Packages
    mariadb-columnstore-api-cpp_devel.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-java.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-pyspark.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-pyspark3.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-python.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-python3.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-spark.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-client.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-common.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-data-adapters-avro-kafka-adapter.x86_64 1.2.2-1 mariadb-columnstore-data-adapters
    mariadb-columnstore-data-adapters-maxscale-cdc-adapter.x86_64 1.2.2-1 mariadb-columnstore-data-adapters
    mariadb-columnstore-gssapi-server.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-libs.x86_64 1.2.2-1 mariadb-columnstore-1.2.2
    mariadb-columnstore-platform.x86_64 1.2.2-1 mariadb-columnstore-1.2.2
    mariadb-columnstore-rocksdb-engine.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-server.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-shared.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-storage-engine.x86_64 1.2.2-1 mariadb-columnstore-1.2.2
    mariadb-columnstore-tokudb-engine.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-tools.x86_64


{noformat}
[root@localhost ~]# yum install mariadb-columnstore-tools
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Resolving Dependencies
    --> Running transaction check
    ---> Package mariadb-columnstore-tools.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: mariadb-columnstore-api-cpp >= 1.2 for package: mariadb-columnstore-tools-1.2.2-1.x86_64
    --> Processing Dependency: libmcsapi.so.1()(64bit) for package: mariadb-columnstore-tools-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 will be installed
    --> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================================================================================
Package Arch Version Repository Size
===================================================================================================================================================================================================================================
Installing:
mariadb-columnstore-tools x86_64 1.2.2-1 Local-Install 232 k
Installing for dependencies:
mariadb-columnstore-api-cpp x86_64 1.2.2-1 Local-Install 821 k

Transaction Summary
===================================================================================================================================================================================================================================
Install 1 Package (+1 Dependent package)

Total download size: 1.0 M
Installed size: 4.5 M
Is this ok [y/d/N]: y
Downloading packages:
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 97 MB/s | 1.0 MB 00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 1/2
Installing : mariadb-columnstore-tools-1.2.2-1.x86_64 2/2
Verifying : mariadb-columnstore-tools-1.2.2-1.x86_64 1/2
Verifying : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 2/2

Installed:
mariadb-columnstore-tools.x86_64 0:1.2.2-1

Dependency Installed:
mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1

Complete!

as well

[root@localhost ~]# yum install mariadb-columnstore-api-spark
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Resolving Dependencies
    --> Running transaction check
    ---> Package mariadb-columnstore-api-spark.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: mariadb-columnstore-api-java = 1.2.2 for package: mariadb-columnstore-api-spark-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-java.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: mariadb-columnstore-api-cpp = 1.2.2 for package: mariadb-columnstore-api-java-1.2.2-1.x86_64
    --> Processing Dependency: libmcsapi.so.1()(64bit) for package: mariadb-columnstore-api-java-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 will be installed
    --> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================================================================================
Package Arch Version Repository Size
===================================================================================================================================================================================================================================
Installing:
mariadb-columnstore-api-spark x86_64 1.2.2-1 Local-Install 25 k
Installing for dependencies:
mariadb-columnstore-api-cpp x86_64 1.2.2-1 Local-Install 821 k
mariadb-columnstore-api-java x86_64 1.2.2-1 Local-Install 216 k

Transaction Summary
===================================================================================================================================================================================================================================
Install 1 Package (+2 Dependent packages)

Total download size: 1.0 M
Installed size: 4.5 M
Is this ok [y/d/N]: y
Downloading packages:
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 52 MB/s | 1.0 MB 00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 1/3
Installing : mariadb-columnstore-api-java-1.2.2-1.x86_64 2/3
Installing : mariadb-columnstore-api-spark-1.2.2-1.x86_64 3/3
Verifying : mariadb-columnstore-api-spark-1.2.2-1.x86_64 1/3
Verifying : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 2/3
Verifying : mariadb-columnstore-api-java-1.2.2-1.x86_64 3/3

Installed:
mariadb-columnstore-api-spark.x86_64 0:1.2.2-1

Dependency Installed:
mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 mariadb-columnstore-api-java.x86_64 0:1.2.2-1

Complete!

also the mcs maxscale adapter , although of the confusion package name [mariadb-columnstore-maxscale-cdc-adapters-1.2.2-1-x86_64-centos7.rpm] , is installed successfully resolving dependency from mariadb-columnstore-api-cpp

[root@localhost ~]# yum install mariadb-columnstore-maxscale-cdc-adapters
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    No package mariadb-columnstore-maxscale-cdc-adapters available.
    Error: Nothing to do
    [root@localhost ~]#


[root@localhost ~]# yum install mariadb-columnstore-data-adapters-maxscale-cdc-adapter
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Resolving Dependencies
    --> Running transaction check
    ---> Package mariadb-columnstore-data-adapters-maxscale-cdc-adapter.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: libmcsapi.so.1()(64bit) for package: mariadb-columnstore-data-adapters-maxscale-cdc-adapter-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 will be installed
    --> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================================================================================
Package Arch Version Repository Size
===================================================================================================================================================================================================================================
Installing:
mariadb-columnstore-data-adapters-maxscale-cdc-adapter x86_64 1.2.2-1 mariadb-columnstore-data-adapters 209 k
Installing for dependencies:
mariadb-columnstore-api-cpp x86_64 1.2.2-1 mariadb-columnstore-api 821 k

Transaction Summary
===================================================================================================================================================================================================================================
Install 1 Package (+1 Dependent package)

Total download size: 1.0 M
Installed size: 4.2 M
Is this ok [y/d/N]: y
Downloading packages:
(1/2): mariadb-columnstore-maxscale-cdc-adapters-1.2.2-1-x86_64-centos7.rpm | 209 kB 00:00:00
(2/2): mariadb-columnstore-api-1.2.2-1-x86_64-centos7-cpp.rpm | 821 kB 00:00:01
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 999 kB/s | 1.0 MB 00:00:01
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 1/2
Installing : mariadb-columnstore-data-adapters-maxscale-cdc-adapter-1.2.2-1.x86_64 2/2
Verifying : mariadb-columnstore-data-adapters-maxscale-cdc-adapter-1.2.2-1.x86_64 1/2
Verifying : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 2/2

Installed:
mariadb-columnstore-data-adapters-maxscale-cdc-adapter.x86_64 0:1.2.2-1

Dependency Installed:
mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1
{noformat}
",2,"dependencies from mcsapi are resolved automatically through mariadb-columnstore-api bundle packages

[root@localhost ~]# yum list available mariadb-columnstore-*
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Available Packages
    mariadb-columnstore-api-cpp_devel.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-java.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-pyspark.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-pyspark3.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-python.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-python3.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-api-spark.x86_64 1.2.2-1 mariadb-columnstore-api
    mariadb-columnstore-client.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-common.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-data-adapters-avro-kafka-adapter.x86_64 1.2.2-1 mariadb-columnstore-data-adapters
    mariadb-columnstore-data-adapters-maxscale-cdc-adapter.x86_64 1.2.2-1 mariadb-columnstore-data-adapters
    mariadb-columnstore-gssapi-server.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-libs.x86_64 1.2.2-1 mariadb-columnstore-1.2.2
    mariadb-columnstore-platform.x86_64 1.2.2-1 mariadb-columnstore-1.2.2
    mariadb-columnstore-rocksdb-engine.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-server.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-shared.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-storage-engine.x86_64 1.2.2-1 mariadb-columnstore-1.2.2
    mariadb-columnstore-tokudb-engine.x86_64 1.2.2-1.el7.centos mariadb-columnstore-1.2.2
    mariadb-columnstore-tools.x86_64


{noformat}
[root@localhost ~]# yum install mariadb-columnstore-tools
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Resolving Dependencies
    --> Running transaction check
    ---> Package mariadb-columnstore-tools.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: mariadb-columnstore-api-cpp >= 1.2 for package: mariadb-columnstore-tools-1.2.2-1.x86_64
    --> Processing Dependency: libmcsapi.so.1()(64bit) for package: mariadb-columnstore-tools-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 will be installed
    --> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================================================================================
Package Arch Version Repository Size
===================================================================================================================================================================================================================================
Installing:
mariadb-columnstore-tools x86_64 1.2.2-1 Local-Install 232 k
Installing for dependencies:
mariadb-columnstore-api-cpp x86_64 1.2.2-1 Local-Install 821 k

Transaction Summary
===================================================================================================================================================================================================================================
Install 1 Package (+1 Dependent package)

Total download size: 1.0 M
Installed size: 4.5 M
Is this ok [y/d/N]: y
Downloading packages:
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 97 MB/s | 1.0 MB 00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 1/2
Installing : mariadb-columnstore-tools-1.2.2-1.x86_64 2/2
Verifying : mariadb-columnstore-tools-1.2.2-1.x86_64 1/2
Verifying : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 2/2

Installed:
mariadb-columnstore-tools.x86_64 0:1.2.2-1

Dependency Installed:
mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1

Complete!

as well

[root@localhost ~]# yum install mariadb-columnstore-api-spark
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Resolving Dependencies
    --> Running transaction check
    ---> Package mariadb-columnstore-api-spark.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: mariadb-columnstore-api-java = 1.2.2 for package: mariadb-columnstore-api-spark-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-java.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: mariadb-columnstore-api-cpp = 1.2.2 for package: mariadb-columnstore-api-java-1.2.2-1.x86_64
    --> Processing Dependency: libmcsapi.so.1()(64bit) for package: mariadb-columnstore-api-java-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 will be installed
    --> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================================================================================
Package Arch Version Repository Size
===================================================================================================================================================================================================================================
Installing:
mariadb-columnstore-api-spark x86_64 1.2.2-1 Local-Install 25 k
Installing for dependencies:
mariadb-columnstore-api-cpp x86_64 1.2.2-1 Local-Install 821 k
mariadb-columnstore-api-java x86_64 1.2.2-1 Local-Install 216 k

Transaction Summary
===================================================================================================================================================================================================================================
Install 1 Package (+2 Dependent packages)

Total download size: 1.0 M
Installed size: 4.5 M
Is this ok [y/d/N]: y
Downloading packages:
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 52 MB/s | 1.0 MB 00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 1/3
Installing : mariadb-columnstore-api-java-1.2.2-1.x86_64 2/3
Installing : mariadb-columnstore-api-spark-1.2.2-1.x86_64 3/3
Verifying : mariadb-columnstore-api-spark-1.2.2-1.x86_64 1/3
Verifying : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 2/3
Verifying : mariadb-columnstore-api-java-1.2.2-1.x86_64 3/3

Installed:
mariadb-columnstore-api-spark.x86_64 0:1.2.2-1

Dependency Installed:
mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 mariadb-columnstore-api-java.x86_64 0:1.2.2-1

Complete!

also the mcs maxscale adapter , although of the confusion package name [mariadb-columnstore-maxscale-cdc-adapters-1.2.2-1-x86_64-centos7.rpm] , is installed successfully resolving dependency from mariadb-columnstore-api-cpp

[root@localhost ~]# yum install mariadb-columnstore-maxscale-cdc-adapters
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    No package mariadb-columnstore-maxscale-cdc-adapters available.
    Error: Nothing to do
    [root@localhost ~]#


[root@localhost ~]# yum install mariadb-columnstore-data-adapters-maxscale-cdc-adapter
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile

    base: mirrors.neterra.net
    epel: mirrors.nav.ro
    extras: mirrors.neterra.net
    ovirt-4.2: mirror.slu.cz
    ovirt-4.2-epel: mirrors.nav.ro
    updates: mirrors.neterra.net
    Resolving Dependencies
    --> Running transaction check
    ---> Package mariadb-columnstore-data-adapters-maxscale-cdc-adapter.x86_64 0:1.2.2-1 will be installed
    --> Processing Dependency: libmcsapi.so.1()(64bit) for package: mariadb-columnstore-data-adapters-maxscale-cdc-adapter-1.2.2-1.x86_64
    --> Running transaction check
    ---> Package mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1 will be installed
    --> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================================================================================================================================
Package Arch Version Repository Size
===================================================================================================================================================================================================================================
Installing:
mariadb-columnstore-data-adapters-maxscale-cdc-adapter x86_64 1.2.2-1 mariadb-columnstore-data-adapters 209 k
Installing for dependencies:
mariadb-columnstore-api-cpp x86_64 1.2.2-1 mariadb-columnstore-api 821 k

Transaction Summary
===================================================================================================================================================================================================================================
Install 1 Package (+1 Dependent package)

Total download size: 1.0 M
Installed size: 4.2 M
Is this ok [y/d/N]: y
Downloading packages:
(1/2): mariadb-columnstore-maxscale-cdc-adapters-1.2.2-1-x86_64-centos7.rpm | 209 kB 00:00:00
(2/2): mariadb-columnstore-api-1.2.2-1-x86_64-centos7-cpp.rpm | 821 kB 00:00:01
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total 999 kB/s | 1.0 MB 00:00:01
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
Installing : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 1/2
Installing : mariadb-columnstore-data-adapters-maxscale-cdc-adapter-1.2.2-1.x86_64 2/2
Verifying : mariadb-columnstore-data-adapters-maxscale-cdc-adapter-1.2.2-1.x86_64 1/2
Verifying : mariadb-columnstore-api-cpp-1.2.2-1.x86_64 2/2

Installed:
mariadb-columnstore-data-adapters-maxscale-cdc-adapter.x86_64 0:1.2.2-1

Dependency Installed:
mariadb-columnstore-api-cpp.x86_64 0:1.2.2-1
{noformat}
"
414,MCOL-1740,MCOL,Jens Röwekamp,116974,2018-09-21 23:57:06,"After a bit of research this might not be such a pressing issue as anticipated. On Debian 9 there are no additional (Java, Python etc) dependencies required for installation, and on CentOS 7 there are two unnecessary dependencies (Python 2.7 and Python 3.4) for the installation.",1,"After a bit of research this might not be such a pressing issue as anticipated. On Debian 9 there are no additional (Java, Python etc) dependencies required for installation, and on CentOS 7 there are two unnecessary dependencies (Python 2.7 and Python 3.4) for the installation."
415,MCOL-1740,MCOL,Zdravelina Sokolovska,117748,2018-10-11 12:21:27,when build mariadb-columnstore-tools-1.2.0-1  rpm package with  set flag -DREMOTE_CPIMPORT=ON appear also dependencies on yaml-cpp and yaml-cpp-devel packages,2,when build mariadb-columnstore-tools-1.2.0-1  rpm package with  set flag -DREMOTE_CPIMPORT=ON appear also dependencies on yaml-cpp and yaml-cpp-devel packages
416,MCOL-1740,MCOL,Jens Röwekamp,119204,2018-11-13 19:00:32,"Added the in MCOL-1739 introduced mariadb-columnstore-api-cpp (mcsapi only) package as dependency for package installation of mariadb-columnstore-tools for CentOS 7, Debian 9, Debian 8, Ubuntu 16.04 and Ubuntu 18.04

----

For *QA*:
- do an install test from deb/rpm package on all supported operating systems
- do an install test from the repository on all supported operating systems to check if the mariadb-columnstore-api-cpp dependency is resolved automatically
- after installation clone the github repo and run the test suite to check that mcsimport is behaving as intended",3,"Added the in MCOL-1739 introduced mariadb-columnstore-api-cpp (mcsapi only) package as dependency for package installation of mariadb-columnstore-tools for CentOS 7, Debian 9, Debian 8, Ubuntu 16.04 and Ubuntu 18.04

----

For *QA*:
- do an install test from deb/rpm package on all supported operating systems
- do an install test from the repository on all supported operating systems to check if the mariadb-columnstore-api-cpp dependency is resolved automatically
- after installation clone the github repo and run the test suite to check that mcsimport is behaving as intended"
417,MCOL-1740,MCOL,David Thompson,119572,2018-11-20 18:04:52,pr approved but holding on merging until 1739 code review feedback addressed.,4,pr approved but holding on merging until 1739 code review feedback addressed.
418,MCOL-1740,MCOL,Zdravelina Sokolovska,119740,2018-11-23 16:25:22,mariadb-columnstore-tools depends only  on  mariadb-columnstore-api-*-cpp,5,mariadb-columnstore-tools depends only  on  mariadb-columnstore-api-*-cpp
419,MCOL-1754,MCOL,Jens Röwekamp,117189,2018-09-28 20:30:57,"Changed libmysql to libmariadb for Windows tests. 
Also updated the regarding shared library download package.

For QA:
Build mcsapi with the new libmariadb library dependency and execute the regression tests.",1,"Changed libmysql to libmariadb for Windows tests. 
Also updated the regarding shared library download package.

For QA:
Build mcsapi with the new libmariadb library dependency and execute the regression tests."
420,MCOL-1759,MCOL,David Hall,117645,2018-10-09 21:58:20,Add covar_pop and covar_samp as well.,1,Add covar_pop and covar_samp as well.
421,MCOL-1759,MCOL,David Hall,117646,2018-10-09 22:08:46,"Roman, thought you might like these. May help shed light on the API.",2,"Roman, thought you might like these. May help shed light on the API."
422,MCOL-1759,MCOL,David Hall,117757,2018-10-11 15:09:25,"Test:

When used when innodb right now, these functions are rounded to two decimal places. These are not native MariaDB Server functions, but rather the result of implementing them as Server UDAF, since we need the stubs anyway.

CREATE TABLE aggr(k int, v decimal(10,2), v2 decimal(10, 2))engine=columnstore;
CREATE TABLE aggr_innodb(k int, v decimal(10,2), v2 decimal(10, 2));
INSERT INTO aggr VALUES(1, 10, NULL);
INSERT INTO aggr VALUES(2, 10, 11), (2, 20, 22), (2, 25, NULL), (2, 30, 35);
INSERT INTO aggr_innodb VALUES(1, 10, NULL);
INSERT INTO aggr_innodb VALUES(2, 10, 11), (2, 20, 22), (2, 25, NULL), (2, 30, 35);

 SELECT k, CORR(v, v2) FROM aggr GROUP BY k;
+------+--------------+
| k    | CORR(v, v2)  |
+------+--------------+
|    1 |         NULL |
|    2 | 0.9988445981 |
+------+--------------+
SELECT k, CORR(v, v2) FROM aggr_innodb GROUP BY k;
+------+-------------+
| k    | CORR(v, v2) |
+------+-------------+
|    1 |        NULL |
|    2 |        1.00 |
+------+-------------+

SELECT k, COVAR_POP(v, v2) FROM aggr GROUP BY k;
+---+------------------+
| K | COVAR_POP(V, V2) |
|---+------------------|
| 1 |           NULL   |
| 2 |             80.0 |
+---+------------------+

SELECT k, COVAR_POP(v, v2) FROM aggr_innodb GROUP BY k;
+------+------------------+
| k    | COVAR_POP(v, v2) |
+------+------------------+
|    1 |             NULL |
|    2 |            80.00 |
+------+------------------+

 SELECT k, COVAR_SAMP(v, v2) FROM aggr GROUP BY k;
+------+-------------------+
| k    | COVAR_SAMP(v, v2) |
+------+-------------------+
|    1 |              NULL |
|    2 |    120.0000000000 |
+------+-------------------+

 SELECT k, COVAR_SAMP(v, v2) FROM aggr_innodb GROUP BY k;
+------+-------------------+
| k    | COVAR_SAMP(v, v2) |
+------+-------------------+
|    1 |              NULL |
|    2 |            120.00 |
+------+-------------------+

select k, regr_sxx(v, v2) from aggr group by k;
+------+-----------------+
| k    | regr_sxx(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |  288.6666666667 |
+------+-----------------+

select k, regr_sxx(v, v2) from aggr_innodb group by k;
+------+-----------------+
| k    | regr_sxx(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |          288.67 |
+------+-----------------+

select k, regr_sxy(v, v2) from aggr group by k;
+------+-----------------+
| k    | regr_sxy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |  240.0000000000 |
+------+-----------------+

select k, regr_sxy(v, v2) from aggr_innodb group by k;
+------+-----------------+
| k    | regr_sxy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |          240.00 |
+------+-----------------+

select k, regr_syy(v, v2) from aggr group by k;
+------+-----------------+
| k    | regr_syy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |  200.0000000000 |
+------+-----------------+

select k, regr_syy(v, v2) from aggr_innodb group by k;
+------+-----------------+
| k    | regr_syy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |          200.00 |
+------+-----------------+




",3,"Test:

When used when innodb right now, these functions are rounded to two decimal places. These are not native MariaDB Server functions, but rather the result of implementing them as Server UDAF, since we need the stubs anyway.

CREATE TABLE aggr(k int, v decimal(10,2), v2 decimal(10, 2))engine=columnstore;
CREATE TABLE aggr_innodb(k int, v decimal(10,2), v2 decimal(10, 2));
INSERT INTO aggr VALUES(1, 10, NULL);
INSERT INTO aggr VALUES(2, 10, 11), (2, 20, 22), (2, 25, NULL), (2, 30, 35);
INSERT INTO aggr_innodb VALUES(1, 10, NULL);
INSERT INTO aggr_innodb VALUES(2, 10, 11), (2, 20, 22), (2, 25, NULL), (2, 30, 35);

 SELECT k, CORR(v, v2) FROM aggr GROUP BY k;
+------+--------------+
| k    | CORR(v, v2)  |
+------+--------------+
|    1 |         NULL |
|    2 | 0.9988445981 |
+------+--------------+
SELECT k, CORR(v, v2) FROM aggr_innodb GROUP BY k;
+------+-------------+
| k    | CORR(v, v2) |
+------+-------------+
|    1 |        NULL |
|    2 |        1.00 |
+------+-------------+

SELECT k, COVAR_POP(v, v2) FROM aggr GROUP BY k;
+---+------------------+
| K | COVAR_POP(V, V2) |
|---+------------------|
| 1 |           NULL   |
| 2 |             80.0 |
+---+------------------+

SELECT k, COVAR_POP(v, v2) FROM aggr_innodb GROUP BY k;
+------+------------------+
| k    | COVAR_POP(v, v2) |
+------+------------------+
|    1 |             NULL |
|    2 |            80.00 |
+------+------------------+

 SELECT k, COVAR_SAMP(v, v2) FROM aggr GROUP BY k;
+------+-------------------+
| k    | COVAR_SAMP(v, v2) |
+------+-------------------+
|    1 |              NULL |
|    2 |    120.0000000000 |
+------+-------------------+

 SELECT k, COVAR_SAMP(v, v2) FROM aggr_innodb GROUP BY k;
+------+-------------------+
| k    | COVAR_SAMP(v, v2) |
+------+-------------------+
|    1 |              NULL |
|    2 |            120.00 |
+------+-------------------+

select k, regr_sxx(v, v2) from aggr group by k;
+------+-----------------+
| k    | regr_sxx(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |  288.6666666667 |
+------+-----------------+

select k, regr_sxx(v, v2) from aggr_innodb group by k;
+------+-----------------+
| k    | regr_sxx(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |          288.67 |
+------+-----------------+

select k, regr_sxy(v, v2) from aggr group by k;
+------+-----------------+
| k    | regr_sxy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |  240.0000000000 |
+------+-----------------+

select k, regr_sxy(v, v2) from aggr_innodb group by k;
+------+-----------------+
| k    | regr_sxy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |          240.00 |
+------+-----------------+

select k, regr_syy(v, v2) from aggr group by k;
+------+-----------------+
| k    | regr_syy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |  200.0000000000 |
+------+-----------------+

select k, regr_syy(v, v2) from aggr_innodb group by k;
+------+-----------------+
| k    | regr_syy(v, v2) |
+------+-----------------+
|    1 |            NULL |
|    2 |          200.00 |
+------+-----------------+




"
423,MCOL-1759,MCOL,Daniel Lee,117933,2018-10-15 21:23:44,"Build verified: 1.2.0a

Functions were created.

Result differences are being tracked in a separate ticket.",4,"Build verified: 1.2.0a

Functions were created.

Result differences are being tracked in a separate ticket."
424,MCOL-1774,MCOL,Jens Röwekamp,118525,2018-11-01 05:05:13,"*Done in this ticket:*
- support for enclose by and escape characters added
- additional header command line parameter added to ignore the first line of the csv file if set
- csv input compatibility to RFC4180 achieved
- internal documentation in Github updated
- tests added to the regression suite
- test suite successfully executed on Win 10 and CentOS 7

*Used CSV input format definition:*
{noformat}
mcsimports CSV input file format derived from RFC4180 and cpimport

   1.  Each record is located on a separate line, delimited by a line
       break (LF or CRLF).  For example:

       aaa,bbb,ccc LF
       zzz,yyy,xxx LF
       aaa,bbb,ccc CRLF
       zzz,yyy,xxx CRLF

   2.  The last record in the file may or may not have an ending line
       break.  For example:

       aaa,bbb,ccc LF
       zzz,yyy,xxx

   3.  There maybe an optional header line appearing as the first line
       of the file with the same format as normal record lines. This
       header will contain names corresponding to the fields in the file
       and should contain the same number of fields as the records in
       the rest of the file (the presence of the header line should be
       indicated via the optional ""header"" command line argument of
	   mcsimport). If a header is specified it can be used as referrence
	   in mcsimport's mapping file. [Possible extension for the future]
	   
	   For example:

       field_name,field_name,field_name LF
       aaa,bbb,ccc LF
       zzz,yyy,xxx LF

   4.  Within the header and each record, there may be one or more
       fields, separated by a delimiter (default comma). Each line 
	   should contain the same number of fields throughout the file.
	   Spaces are considered part of a field and should not be ignored.
	   The last field in the record must not be followed by a delimiter.
	   
	   For example:

       aaa,bbb,ccc

   5.  Each field may or may not be enclosed by an enclosing character
       (default double quotes). If fields are not enclosed by an enclosing
	   character, the enclosing character may not appear inside the fields.

	   For example:

       ""aaa"",""bbb"",""ccc"" LF
       zzz,yyy,xxx
	
   6.  Fields containing line breaks (LF or CRLF), delimiters, and enclosing
       characters should be enclosed in enclosing characters.  For example:

       ""aaa"",""b LF
       bb"",""ccc"" LF
	   zzz,yyy,xxx
	
   7.  If enclosing characters are used to enclose fields, then an enclosing
       character appearing inside a field must be escaped by preceding it with
       an escaping character. (default double quotes)  For example:

       ""aaa"",""b""""bb"",""ccc""

   8.  An escaping character can escape itself when used in an enclosed field.
       For example with \ as escaping character:
	   
	   ""aaa"",""b\\bb\\"",""c\cc""
{noformat}

*For QA:*
- review added test cases and add additional test cases to the regression suite if you see fit
- execute the regression test suite on Windows, CentOS 7, and one Debian/Ubuntu operating system",1,"*Done in this ticket:*
- support for enclose by and escape characters added
- additional header command line parameter added to ignore the first line of the csv file if set
- csv input compatibility to RFC4180 achieved
- internal documentation in Github updated
- tests added to the regression suite
- test suite successfully executed on Win 10 and CentOS 7

*Used CSV input format definition:*
{noformat}
mcsimports CSV input file format derived from RFC4180 and cpimport

   1.  Each record is located on a separate line, delimited by a line
       break (LF or CRLF).  For example:

       aaa,bbb,ccc LF
       zzz,yyy,xxx LF
       aaa,bbb,ccc CRLF
       zzz,yyy,xxx CRLF

   2.  The last record in the file may or may not have an ending line
       break.  For example:

       aaa,bbb,ccc LF
       zzz,yyy,xxx

   3.  There maybe an optional header line appearing as the first line
       of the file with the same format as normal record lines. This
       header will contain names corresponding to the fields in the file
       and should contain the same number of fields as the records in
       the rest of the file (the presence of the header line should be
       indicated via the optional ""header"" command line argument of
	   mcsimport). If a header is specified it can be used as referrence
	   in mcsimport's mapping file. [Possible extension for the future]
	   
	   For example:

       field_name,field_name,field_name LF
       aaa,bbb,ccc LF
       zzz,yyy,xxx LF

   4.  Within the header and each record, there may be one or more
       fields, separated by a delimiter (default comma). Each line 
	   should contain the same number of fields throughout the file.
	   Spaces are considered part of a field and should not be ignored.
	   The last field in the record must not be followed by a delimiter.
	   
	   For example:

       aaa,bbb,ccc

   5.  Each field may or may not be enclosed by an enclosing character
       (default double quotes). If fields are not enclosed by an enclosing
	   character, the enclosing character may not appear inside the fields.

	   For example:

       ""aaa"",""bbb"",""ccc"" LF
       zzz,yyy,xxx
	
   6.  Fields containing line breaks (LF or CRLF), delimiters, and enclosing
       characters should be enclosed in enclosing characters.  For example:

       ""aaa"",""b LF
       bb"",""ccc"" LF
	   zzz,yyy,xxx
	
   7.  If enclosing characters are used to enclose fields, then an enclosing
       character appearing inside a field must be escaped by preceding it with
       an escaping character. (default double quotes)  For example:

       ""aaa"",""b""""bb"",""ccc""

   8.  An escaping character can escape itself when used in an enclosed field.
       For example with \ as escaping character:
	   
	   ""aaa"",""b\\bb\\"",""c\cc""
{noformat}

*For QA:*
- review added test cases and add additional test cases to the regression suite if you see fit
- execute the regression test suite on Windows, CentOS 7, and one Debian/Ubuntu operating system"
425,MCOL-1790,MCOL,Andrew Hutchings,119620,2018-11-21 11:28:06,"Pull request for server and engine.

For QA: No user feature change. Just a code change for convergence. Regression suite shouldn't change.",1,"Pull request for server and engine.

For QA: No user feature change. Just a code change for convergence. Regression suite shouldn't change."
426,MCOL-1790,MCOL,Daniel Lee,120035,2018-11-29 21:53:31,"Build verified: 1.2.2-1

Performed regression test only.",2,"Build verified: 1.2.2-1

Performed regression test only."
427,MCOL-1809,MCOL,Andrew Hutchings,119480,2018-11-19 18:54:31,This is done for 1.2.1 upgrade documentation. It still needs doing for 1.1,1,This is done for 1.2.1 upgrade documentation. It still needs doing for 1.1
428,MCOL-1809,MCOL,David Hill,119643,2018-11-21 20:53:42,This was done by Patrick for the 1.2.1 release,2,This was done by Patrick for the 1.2.1 release
429,MCOL-1809,MCOL,Andrew Hutchings,119645,2018-11-21 21:10:09,Reopening. As stated in a previous comment the 1.1 upgrade docs still need updating.,3,Reopening. As stated in a previous comment the 1.1 upgrade docs still need updating.
430,MCOL-1809,MCOL,Andrew Hutchings,119732,2018-11-23 14:20:14,Now applied to the 1.1.5->1.1.6 upgrade documentation,4,Now applied to the 1.1.5->1.1.6 upgrade documentation
431,MCOL-1816,MCOL,Andrew Hutchings,118070,2018-10-18 06:36:22,[~jens.rowekamp] Since BOOL is an alias for TINYINT(1) (and the data type will be set to TINYINT when you try BOOL) how does it not already support it?,1,[~jens.rowekamp] Since BOOL is an alias for TINYINT(1) (and the data type will be set to TINYINT when you try BOOL) how does it not already support it?
432,MCOL-1816,MCOL,Jens Röwekamp,118098,2018-10-18 19:12:27,"ah okay didn't know that it is just an alias.

C++ and Python seem to typecast boolean data types automatically to 0 and 1, but Java unfortunately doesn't.

{code:shell}
[jens@centos7 ~]$ javac -classpath "".:/usr/lib64/javamcsapi.jar"" MCOL_1816.java
MCOL_1816.java:10: error: no suitable method found for setColumn(int,boolean)
        b.setColumn(0, false);
         ^
    method ColumnStoreBulkInsert.setColumn(int,String) is not applicable
      (argument mismatch; boolean cannot be converted to String)
    method ColumnStoreBulkInsert.setColumn(int,BigInteger) is not applicable
      (argument mismatch; boolean cannot be converted to BigInteger)
    method ColumnStoreBulkInsert.setColumn(int,long) is not applicable
      (argument mismatch; boolean cannot be converted to long)
    method ColumnStoreBulkInsert.setColumn(int,int) is not applicable
      (argument mismatch; boolean cannot be converted to int)
    method ColumnStoreBulkInsert.setColumn(int,short) is not applicable
      (argument mismatch; boolean cannot be converted to short)
    method ColumnStoreBulkInsert.setColumn(int,byte) is not applicable
      (argument mismatch; boolean cannot be converted to byte)
    method ColumnStoreBulkInsert.setColumn(int,double) is not applicable
      (argument mismatch; boolean cannot be converted to double)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDateTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDateTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDecimal) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDecimal)
MCOL_1816.java:12: error: no suitable method found for setColumn(int,boolean)
        b.setColumn(0, true);
         ^
    method ColumnStoreBulkInsert.setColumn(int,String) is not applicable
      (argument mismatch; boolean cannot be converted to String)
    method ColumnStoreBulkInsert.setColumn(int,BigInteger) is not applicable
      (argument mismatch; boolean cannot be converted to BigInteger)
    method ColumnStoreBulkInsert.setColumn(int,long) is not applicable
      (argument mismatch; boolean cannot be converted to long)
    method ColumnStoreBulkInsert.setColumn(int,int) is not applicable
      (argument mismatch; boolean cannot be converted to int)
    method ColumnStoreBulkInsert.setColumn(int,short) is not applicable
      (argument mismatch; boolean cannot be converted to short)
    method ColumnStoreBulkInsert.setColumn(int,byte) is not applicable
      (argument mismatch; boolean cannot be converted to byte)
    method ColumnStoreBulkInsert.setColumn(int,double) is not applicable
      (argument mismatch; boolean cannot be converted to double)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDateTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDateTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDecimal) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDecimal)
Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output
2 errors
{code}

We could either add a boolean datatype to the C++ layer to enable Java to support boolean data types as well or I can solve it within Swig. 
I argue that it is better to fix it in the C++ layer, as we don't know if future language bindings would have the same issue and we would have to fix it in Swig for them as well.",2,"ah okay didn't know that it is just an alias.

C++ and Python seem to typecast boolean data types automatically to 0 and 1, but Java unfortunately doesn't.

{code:shell}
[jens@centos7 ~]$ javac -classpath "".:/usr/lib64/javamcsapi.jar"" MCOL_1816.java
MCOL_1816.java:10: error: no suitable method found for setColumn(int,boolean)
        b.setColumn(0, false);
         ^
    method ColumnStoreBulkInsert.setColumn(int,String) is not applicable
      (argument mismatch; boolean cannot be converted to String)
    method ColumnStoreBulkInsert.setColumn(int,BigInteger) is not applicable
      (argument mismatch; boolean cannot be converted to BigInteger)
    method ColumnStoreBulkInsert.setColumn(int,long) is not applicable
      (argument mismatch; boolean cannot be converted to long)
    method ColumnStoreBulkInsert.setColumn(int,int) is not applicable
      (argument mismatch; boolean cannot be converted to int)
    method ColumnStoreBulkInsert.setColumn(int,short) is not applicable
      (argument mismatch; boolean cannot be converted to short)
    method ColumnStoreBulkInsert.setColumn(int,byte) is not applicable
      (argument mismatch; boolean cannot be converted to byte)
    method ColumnStoreBulkInsert.setColumn(int,double) is not applicable
      (argument mismatch; boolean cannot be converted to double)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDateTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDateTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDecimal) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDecimal)
MCOL_1816.java:12: error: no suitable method found for setColumn(int,boolean)
        b.setColumn(0, true);
         ^
    method ColumnStoreBulkInsert.setColumn(int,String) is not applicable
      (argument mismatch; boolean cannot be converted to String)
    method ColumnStoreBulkInsert.setColumn(int,BigInteger) is not applicable
      (argument mismatch; boolean cannot be converted to BigInteger)
    method ColumnStoreBulkInsert.setColumn(int,long) is not applicable
      (argument mismatch; boolean cannot be converted to long)
    method ColumnStoreBulkInsert.setColumn(int,int) is not applicable
      (argument mismatch; boolean cannot be converted to int)
    method ColumnStoreBulkInsert.setColumn(int,short) is not applicable
      (argument mismatch; boolean cannot be converted to short)
    method ColumnStoreBulkInsert.setColumn(int,byte) is not applicable
      (argument mismatch; boolean cannot be converted to byte)
    method ColumnStoreBulkInsert.setColumn(int,double) is not applicable
      (argument mismatch; boolean cannot be converted to double)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDateTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDateTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreTime) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreTime)
    method ColumnStoreBulkInsert.setColumn(int,ColumnStoreDecimal) is not applicable
      (argument mismatch; boolean cannot be converted to ColumnStoreDecimal)
Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output
2 errors
{code}

We could either add a boolean datatype to the C++ layer to enable Java to support boolean data types as well or I can solve it within Swig. 
I argue that it is better to fix it in the C++ layer, as we don't know if future language bindings would have the same issue and we would have to fix it in Swig for them as well."
433,MCOL-1816,MCOL,Andrew Hutchings,118695,2018-11-05 09:58:31,Added to the C++ API. Assigned to Jens to review and add compatibility for other APIs,3,Added to the C++ API. Assigned to Jens to review and add compatibility for other APIs
434,MCOL-1816,MCOL,Zdravelina Sokolovska,118985,2018-11-09 15:29:10,"[root@cps mcsimport]# python
Python 2.7.5 (default, Jul 13 2018, 13:06:57)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pymcsapi
>>>
>>> d = pymcsapi.ColumnStoreDriver()
>>> b = d.createBulkInsert(""test"", ""MCOL_01816"", 0, 0)
>>> b.setColumn(0, False)
[<pymcsapi.ColumnStoreBulkInsert; proxy of <Swig Object of type 'mcsapi::ColumnStoreBulkInsert *' at 0x7fc118a81ae0> >, 0]
>>> b.writeRow()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/site-packages/pymcsapi.py"", line 600, in writeRow
    return _pymcsapi.ColumnStoreBulkInsert_writeRow(self)
RuntimeError: Not all the columns for this row have been filled
>>> b.setColumn(1, True)
[<pymcsapi.ColumnStoreBulkInsert; proxy of <Swig Object of type 'mcsapi::ColumnStoreBulkInsert *' at 0x7fc118a88960> >, 0]
>>> b.writeRow()
<pymcsapi.ColumnStoreBulkInsert; proxy of <Swig Object of type 'mcsapi::ColumnStoreBulkInsert *' at 0x7fc118a81ae0> >
>>> b.commit()
>>>
>>>
>>>
>>>
>>>
>>> import pymcsapi
>>>
>>> d = pymcsapi.ColumnStoreDriver()
>>> b = d.createBulkInsert(""test"", ""MCOL_01816"", 0, 0)
>>> b.setColumn(0, False)
[<pymcsapi.ColumnStoreBulkInsert; proxy of <Swig Object of type 'mcsapi::ColumnStoreBulkInsert *' at 0x7fc118a81b10> >, 0]
>>>
>>> b.setColumn(1, True)
[<pymcsapi.ColumnStoreBulkInsert; proxy of <Swig Object of type 'mcsapi::ColumnStoreBulkInsert *' at 0x7fc118a81ae0> >, 0]
>>> b.writeRow()
<pymcsapi.ColumnStoreBulkInsert; proxy of <Swig Object of type 'mcsapi::ColumnStoreBulkInsert *' at 0x7fc118a81b10> >
>>> b.commit()
>>>
>>>
>>>
KeyboardInterrupt
>>>
[4]+  Stopped                 python
[root@cps mcsimport]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 38
Server version: 10.3.10-MariaDB-log Columnstore 1.2.1-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> select * from test.MCOL_01816 ;
+------+------+
| a    | b    |
+------+------+
|    0 |    1 |
|    0 |    1 |
+------+------+
2 rows in set, 1 warning (0.045 sec)

",4,"[root@cps mcsimport]# python
Python 2.7.5 (default, Jul 13 2018, 13:06:57)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pymcsapi
>>>
>>> d = pymcsapi.ColumnStoreDriver()
>>> b = d.createBulkInsert(""test"", ""MCOL_01816"", 0, 0)
>>> b.setColumn(0, False)
[ >, 0]
>>> b.writeRow()
Traceback (most recent call last):
  File """", line 1, in 
  File ""/usr/lib/python2.7/site-packages/pymcsapi.py"", line 600, in writeRow
    return _pymcsapi.ColumnStoreBulkInsert_writeRow(self)
RuntimeError: Not all the columns for this row have been filled
>>> b.setColumn(1, True)
[ >, 0]
>>> b.writeRow()
 >
>>> b.commit()
>>>
>>>
>>>
>>>
>>>
>>> import pymcsapi
>>>
>>> d = pymcsapi.ColumnStoreDriver()
>>> b = d.createBulkInsert(""test"", ""MCOL_01816"", 0, 0)
>>> b.setColumn(0, False)
[ >, 0]
>>>
>>> b.setColumn(1, True)
[ >, 0]
>>> b.writeRow()
 >
>>> b.commit()
>>>
>>>
>>>
KeyboardInterrupt
>>>
[4]+  Stopped                 python
[root@cps mcsimport]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 38
Server version: 10.3.10-MariaDB-log Columnstore 1.2.1-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> select * from test.MCOL_01816 ;
+------+------+
| a    | b    |
+------+------+
|    0 |    1 |
|    0 |    1 |
+------+------+
2 rows in set, 1 warning (0.045 sec)

"
435,MCOL-1817,MCOL,Jens Röwekamp,118873,2018-11-07 22:15:04,"Improved the bool datatype support by switching to the native DDL bool data type and setColumn(bool).

For QA:
- execute the regression test suite
- manually test if DDL and injection of bool data works",1,"Improved the bool datatype support by switching to the native DDL bool data type and setColumn(bool).

For QA:
- execute the regression test suite
- manually test if DDL and injection of bool data works"
436,MCOL-1822,MCOL,David Hall,120881,2018-12-17 19:54:25,"While the logic is there for aggregation, it is not there for Window Function usage. This will need to be added.

We're adding this as an XML entry, so upgrade will also need to be touched.

Update: since we decided to go always on long double, there is no XML entry.",1,"While the logic is there for aggregation, it is not there for Window Function usage. This will need to be added.

We're adding this as an XML entry, so upgrade will also need to be touched.

Update: since we decided to go always on long double, there is no XML entry."
437,MCOL-1822,MCOL,David Hall,121368,2019-01-03 20:52:13,The logic found in aggregation doesn't work. This will take a bit longer than expected.,2,The logic found in aggregation doesn't work. This will take a bit longer than expected.
438,MCOL-1822,MCOL,David Hall,124149,2019-03-05 16:09:53,The solution is to use long double (80-bit float) for all SUM/AVG calculations. The code has been modified to handle long double everywhere except actual persistence in a table.,3,The solution is to use long double (80-bit float) for all SUM/AVG calculations. The code has been modified to handle long double everywhere except actual persistence in a table.
439,MCOL-1822,MCOL,David Hall,124160,2019-03-05 19:51:12,"If SUM creates a number of magnitude larger than can be held in an int, and then decimal arithmetic is done on the result, an overflow error will occur unless infinidb_double_for_decimal_math is set.

Example:
select s/d from (select b.f3, sum(a.f3) s, count(distinct a.f5) d from aggoverflowa a join aggoverflowb b on a.id=b.id group by 1) s1 order by 1;

Where f3 is INT or DECIMAL in a.

s/d tries to do decimal math. if sum(a.f3) > MAX(long int), a decimal overflow error is thrown.

Creating an automatic overflow here is difficult, since the datatype of the operation is determined in the connector, while the change to long double is determined in ExeMgr -- too late to go back.

Since long double is now enabled everywhere, infinidb_double_for_decimal_math now causes long double to be used, allowing for more accuracy, at least on Linux.",4,"If SUM creates a number of magnitude larger than can be held in an int, and then decimal arithmetic is done on the result, an overflow error will occur unless infinidb_double_for_decimal_math is set.

Example:
select s/d from (select b.f3, sum(a.f3) s, count(distinct a.f5) d from aggoverflowa a join aggoverflowb b on a.id=b.id group by 1) s1 order by 1;

Where f3 is INT or DECIMAL in a.

s/d tries to do decimal math. if sum(a.f3) > MAX(long int), a decimal overflow error is thrown.

Creating an automatic overflow here is difficult, since the datatype of the operation is determined in the connector, while the change to long double is determined in ExeMgr -- too late to go back.

Since long double is now enabled everywhere, infinidb_double_for_decimal_math now causes long double to be used, allowing for more accuracy, at least on Linux."
440,MCOL-1822,MCOL,Daniel Lee,124315,2019-03-07 19:59:28,"Build verified: 1.2.3-1 nightly

server commit:
61f32f2
engine commit:
fddd108

Also performed regression tests.",5,"Build verified: 1.2.3-1 nightly

server commit:
61f32f2
engine commit:
fddd108

Also performed regression tests."
441,MCOL-1844,MCOL,Patrick LeBlanc,119295,2018-11-14 19:00:26,"Will include that file in the set saved by the pre-uninstall script.  Then will merge the packaged version & the saved version, and continue processing the upgrade.",1,"Will include that file in the set saved by the pre-uninstall script.  Then will merge the packaged version & the saved version, and continue processing the upgrade."
442,MCOL-1844,MCOL,Patrick LeBlanc,119339,2018-11-15 15:23:44,"Tested it in develop-1.1, ported it to -1.0 and develop.  It's functional but writes a new mycnf-include-args.text file that looks like the following:
<comments from both, de-duplicated and alphabetized>
<options from both, de-duplicated and alphabetized>

If they had a comment associated with an entry, it will now be jumbled together at the top with the other comments.  If anybody cares about that I can implement it differently.",2,"Tested it in develop-1.1, ported it to -1.0 and develop.  It's functional but writes a new mycnf-include-args.text file that looks like the following:



If they had a comment associated with an entry, it will now be jumbled together at the top with the other comments.  If anybody cares about that I can implement it differently."
443,MCOL-1844,MCOL,Patrick LeBlanc,119892,2018-11-27 17:34:27,"To test...

Note, this mod includes a change to the pre-uninstall script, which older versions obviously won't have.  It's a one line change, which makes a copy of your mycnf-include.text file so it doesn't get overwritten by the new installation.  People upgrading TO this version will have to make the copy by hand.  People upgrading FROM this option don't have to do anything.

1) Add an option to your mycnf file (it can be bogus b/c this test doesn't involve running columnstore).  So BogusValue=1
2) Add the name of your value to the list at bin/myCnf-include.text
3) Copy your bin/myCnf-include.text file to bin/myCnf-include.text.rpmsave
4) Run the upgrade like usual.
5) Look at your myCnf-include.text file and myCnf.cnf file, and see that your new value is still in both.
",3,"To test...

Note, this mod includes a change to the pre-uninstall script, which older versions obviously won't have.  It's a one line change, which makes a copy of your mycnf-include.text file so it doesn't get overwritten by the new installation.  People upgrading TO this version will have to make the copy by hand.  People upgrading FROM this option don't have to do anything.

1) Add an option to your mycnf file (it can be bogus b/c this test doesn't involve running columnstore).  So BogusValue=1
2) Add the name of your value to the list at bin/myCnf-include.text
3) Copy your bin/myCnf-include.text file to bin/myCnf-include.text.rpmsave
4) Run the upgrade like usual.
5) Look at your myCnf-include.text file and myCnf.cnf file, and see that your new value is still in both.
"
444,MCOL-1844,MCOL,Daniel Lee,119939,2018-11-28 14:28:06,QA task for 1.1.7 and 1.0.16 has been moved to a new ticket MCOL-1974,4,QA task for 1.1.7 and 1.0.16 has been moved to a new ticket MCOL-1974
445,MCOL-1844,MCOL,Daniel Lee,119941,2018-11-28 14:41:37,"Build verified: 1.2.2-1

Tests performed:

1) Centos 7
2) RPM and binary
3) single server, multiple server
4) localquery and non-localquery
5) upgrade from prior releases and from 1.2.2-1 forward by ""upgrading"" to 1.2.2-1

All in all, it is working as expected.
There is also a myCnf-exclude-args.text file in the MCS bin directory, but it is not being used.  I suggest that we should have the exclude functionality implemented or have the file removed to avoid confusion.

Note for localquery setup (1UM2PM)

The user only needs to setup the myCnf-include-args.text file on UM1.  User customized values are being kept on UM1, and propagated to PMs.
",5,"Build verified: 1.2.2-1

Tests performed:

1) Centos 7
2) RPM and binary
3) single server, multiple server
4) localquery and non-localquery
5) upgrade from prior releases and from 1.2.2-1 forward by ""upgrading"" to 1.2.2-1

All in all, it is working as expected.
There is also a myCnf-exclude-args.text file in the MCS bin directory, but it is not being used.  I suggest that we should have the exclude functionality implemented or have the file removed to avoid confusion.

Note for localquery setup (1UM2PM)

The user only needs to setup the myCnf-include-args.text file on UM1.  User customized values are being kept on UM1, and propagated to PMs.
"
446,MCOL-1866,MCOL,Jens Röwekamp,119372,2018-11-16 01:11:44,"Rebranding of API, mcsimport and PDI.

- API uses the new logo in Windows installer and documentation (HTML & PDF)
- mcsimport uses the new logo in Windows installer
- PDI uses the new logo as placeholder for the plugin

----

For *QA*:
- check if I missed a spot and somewhere is still the old logo present
- check if the new logo is placed nicely (MCOL-1795 is not yet fixed)",1,"Rebranding of API, mcsimport and PDI.

- API uses the new logo in Windows installer and documentation (HTML & PDF)
- mcsimport uses the new logo in Windows installer
- PDI uses the new logo as placeholder for the plugin

----

For *QA*:
- check if I missed a spot and somewhere is still the old logo present
- check if the new logo is placed nicely (MCOL-1795 is not yet fixed)"
447,MCOL-1889,MCOL,David Hill,119292,2018-11-14 17:46:31,"This will change back

there are package name issue after all. the debian/ubuntu did change and they are different from centos
./centos-7/mariadb-columnstore-kafka-avro-adapters-1.2.1-1-x86_64-centos7.rpm
./debian-9/mariadb-columnstore-data-adapters-avro-kafka-adapter_1.2.1_amd64.deb

Looks like this was added since 1.2.0 data-adapters

data-adapters will be removed for 1.2.2",1,"This will change back

there are package name issue after all. the debian/ubuntu did change and they are different from centos
./centos-7/mariadb-columnstore-kafka-avro-adapters-1.2.1-1-x86_64-centos7.rpm
./debian-9/mariadb-columnstore-data-adapters-avro-kafka-adapter_1.2.1_amd64.deb

Looks like this was added since 1.2.0 data-adapters

data-adapters will be removed for 1.2.2"
448,MCOL-1889,MCOL,Andrew Hutchings,119559,2018-11-20 15:43:21,Need to document MCOL-1847 changes in 1.2.2,2,Need to document MCOL-1847 changes in 1.2.2
449,MCOL-1944,MCOL,Andrew Hutchings,119351,2018-11-15 18:48:01,Grepping for chmod finds some other places were we are doing bad things too that we should also fix.,1,Grepping for chmod finds some other places were we are doing bad things too that we should also fix.
450,MCOL-1944,MCOL,David Hill,119358,2018-11-15 21:04:56,"On the first round of testing where the user:group was setup to the user where MCS was running, I could only get the logs to happen when the directories were set to 777 permissions. So that is teh reason its set to 777..

I will investigate further.. I noticed on my test, when it did get logged the user:group changed to syslog. This was on a ubuntu 16 system. Will test on centos 7 and other OS also.

mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-x--- 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 755 /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-x--- 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-x--- 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 755 -R /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-xr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-xr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 775 -R /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxrwxr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxrwxr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxrwxr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxrwxr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 777 -R /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxrwxrwx 5 mysql mysql 4096 Nov 15 20:55 ./
drwxrwxrwx 3 mysql mysql 4096 Nov 15 20:55 ../
drwxrwxrwx 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxrwxrwx 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxrwxrwx 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 28
drwxrwxrwx 5 mysql  mysql  4096 Nov 15 21:01 ./
drwxrwxrwx 3 mysql  mysql  4096 Nov 15 20:55 ../
drwxrwxrwx 2 mysql  mysql  4096 Nov 15 20:55 archive/
drwxrwxrwx 2 mysql  mysql  4096 Nov 15 20:55 corefiles/
-rw-r----- 1 syslog syslog  125 Nov 15 21:01 debug.log
-rw-r----- 1 syslog syslog  125 Nov 15 21:01 info.log
drwxrwxrwx 2 mysql  mysql  4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
",2,"On the first round of testing where the user:group was setup to the user where MCS was running, I could only get the logs to happen when the directories were set to 777 permissions. So that is teh reason its set to 777..

I will investigate further.. I noticed on my test, when it did get logged the user:group changed to syslog. This was on a ubuntu 16 system. Will test on centos 7 and other OS also.

mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-x--- 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 755 /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-x--- 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-x--- 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-x--- 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 755 -R /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-xr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxr-xr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxr-xr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxr-xr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 775 -R /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxrwxr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxrwxr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxrwxr-x 5 mysql mysql 4096 Nov 15 20:55 ./
drwxrwxr-x 3 mysql mysql 4096 Nov 15 20:55 ../
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxrwxr-x 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ sudo chmod 777 -R /var/log/mariadb
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 20
drwxrwxrwx 5 mysql mysql 4096 Nov 15 20:55 ./
drwxrwxrwx 3 mysql mysql 4096 Nov 15 20:55 ../
drwxrwxrwx 2 mysql mysql 4096 Nov 15 20:55 archive/
drwxrwxrwx 2 mysql mysql 4096 Nov 15 20:55 corefiles/
drwxrwxrwx 2 mysql mysql 4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ /home/mysql/mariadb/columnstore/bin/cplogger -i 19 ""***** MariaDB Columnstore Installed *****""
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ ll
total 28
drwxrwxrwx 5 mysql  mysql  4096 Nov 15 21:01 ./
drwxrwxrwx 3 mysql  mysql  4096 Nov 15 20:55 ../
drwxrwxrwx 2 mysql  mysql  4096 Nov 15 20:55 archive/
drwxrwxrwx 2 mysql  mysql  4096 Nov 15 20:55 corefiles/
-rw-r----- 1 syslog syslog  125 Nov 15 21:01 debug.log
-rw-r----- 1 syslog syslog  125 Nov 15 21:01 info.log
drwxrwxrwx 2 mysql  mysql  4096 Nov 15 20:55 trace/
mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ 
"
451,MCOL-1944,MCOL,David Hill,119359,2018-11-15 21:08:35,"Also in the testing. I did find out why it wasnt setting user:group to non-root user passed in. 
Trying to keeping it as that user, if possible, so logs can be view from that user. When its not, you get this error when it syslog and 750

mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ cat debug.log 
cat: debug.log: Permission denied
",3,"Also in the testing. I did find out why it wasnt setting user:group to non-root user passed in. 
Trying to keeping it as that user, if possible, so logs can be view from that user. When its not, you get this error when it syslog and 750

mysql@ip-172-31-29-61:/var/log/mariadb/columnstore$ cat debug.log 
cat: debug.log: Permission denied
"
452,MCOL-1944,MCOL,David Hill,119364,2018-11-15 22:03:13,"For a 1.2.1 centos 7 non-root install work-around, do the following after the sysylogSetup.sh is running on a node:

This example assumes the non-root user is 'mysql'

 chown mysql:mysql  -R /var/log/mariadb
 chmod 750  -R /var/log/mariadb",4,"For a 1.2.1 centos 7 non-root install work-around, do the following after the sysylogSetup.sh is running on a node:

This example assumes the non-root user is 'mysql'

 chown mysql:mysql  -R /var/log/mariadb
 chmod 750  -R /var/log/mariadb"
453,MCOL-1944,MCOL,Andrew Hutchings,119367,2018-11-15 23:00:28,"I think you are missing the point. The exec bit should only been used for directories, not files.

Yes we can look into ownership but that is a different problem entirely. There are a couple of ways of fixing that.",5,"I think you are missing the point. The exec bit should only been used for directories, not files.

Yes we can look into ownership but that is a different problem entirely. There are a couple of ways of fixing that."
454,MCOL-1944,MCOL,David Hill,119564,2018-11-20 16:37:34,"I noticed in my testing that some of the Columnstore.xml* files had root user as owner after the syslogSetup.sh is run as part of the 1.2 upgrade. This is run as root user, but the owner needs to stay as the non-root user passed in as an argument.

So this was fixed by a change in syslogSetup.sh to run chown after setConfig",6,"I noticed in my testing that some of the Columnstore.xml* files had root user as owner after the syslogSetup.sh is run as part of the 1.2 upgrade. This is run as root user, but the owner needs to stay as the non-root user passed in as an argument.

So this was fixed by a change in syslogSetup.sh to run chown after setConfig"
455,MCOL-1944,MCOL,David Hill,119637,2018-11-21 17:42:13,"ok, finished coding and testing.
changes:

1. change from the chmod -R to chmod on each directory
2. moved the cplogger from post-install to syslogSetup.sh. needed to be run from here for
    non-root installs
3. Added logic around the running of cplogger to allow the most security permissions on the 
    log directories to allow logging to work. So on ubuntu I tested on, it required 777 for
    logging directories. On centos 7, 750 worked with makes it more secure.


",7,"ok, finished coding and testing.
changes:

1. change from the chmod -R to chmod on each directory
2. moved the cplogger from post-install to syslogSetup.sh. needed to be run from here for
    non-root installs
3. Added logic around the running of cplogger to allow the most security permissions on the 
    log directories to allow logging to work. So on ubuntu I tested on, it required 777 for
    logging directories. On centos 7, 750 worked with makes it more secure.


"
456,MCOL-1944,MCOL,David Hill,119638,2018-11-21 17:44:21,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/637,8,URL
457,MCOL-1944,MCOL,Ben Thompson,119751,2018-11-23 20:09:00,Reviewed/Merged,9,Reviewed/Merged
458,MCOL-1944,MCOL,David Hill,120036,2018-11-29 22:48:17,"How to test and see the difference between 1.2.1 and 1.2.2.

on centos 7 as non-root user, do the following:

install 1.2.1 centos 7 binary package as non-root user and run the following as root user.
You will see that all the directories is set to 777 permissions, which is what the BUG is about

rm -rf /var/log/mariadb/
/home/mysql/mariadb/columnstore/bin/syslogSetup.sh install --user=mysql
ll /var/log/mariadb/columnstore/
total 8
drwxrwxrwx 2 root root 6 Nov 29 22:41 archive
drwxrwxrwx 2 root root 6 Nov 29 22:41 corefiles
drwxrwxrwx 2 root root 6 Nov 29 22:41 trace

install 1.2.2 centos 7 binary package as non-root user and run the following.
The directories are now 750, which is more secure. 777 is wide open to all.
You will also notice that the debug/info logs are created with
""MariaDB Columnstore Installed"". That wasnt in 1.2.1. 

rm -rf /var/log/mariadb/  
/home/mysql/mariadb/columnstore/bin/syslogSetup.sh install --user=mysql
ll /var/log/mariadb/columnstore/
drwxr-x--- 2 root   root   6 Nov 29 22:46 archive
drwxr-x--- 2 root   root   6 Nov 29 22:46 corefiles
-rw------- 1 syslog adm  124 Nov 29 22:46 debug.log
-rw------- 1 syslog adm  124 Nov 29 22:46 info.log
drwxr-x--- 2 root   root   6 Nov 29 22:46 trace
",10,"How to test and see the difference between 1.2.1 and 1.2.2.

on centos 7 as non-root user, do the following:

install 1.2.1 centos 7 binary package as non-root user and run the following as root user.
You will see that all the directories is set to 777 permissions, which is what the BUG is about

rm -rf /var/log/mariadb/
/home/mysql/mariadb/columnstore/bin/syslogSetup.sh install --user=mysql
ll /var/log/mariadb/columnstore/
total 8
drwxrwxrwx 2 root root 6 Nov 29 22:41 archive
drwxrwxrwx 2 root root 6 Nov 29 22:41 corefiles
drwxrwxrwx 2 root root 6 Nov 29 22:41 trace

install 1.2.2 centos 7 binary package as non-root user and run the following.
The directories are now 750, which is more secure. 777 is wide open to all.
You will also notice that the debug/info logs are created with
""MariaDB Columnstore Installed"". That wasnt in 1.2.1. 

rm -rf /var/log/mariadb/  
/home/mysql/mariadb/columnstore/bin/syslogSetup.sh install --user=mysql
ll /var/log/mariadb/columnstore/
drwxr-x--- 2 root   root   6 Nov 29 22:46 archive
drwxr-x--- 2 root   root   6 Nov 29 22:46 corefiles
-rw------- 1 syslog adm  124 Nov 29 22:46 debug.log
-rw------- 1 syslog adm  124 Nov 29 22:46 info.log
drwxr-x--- 2 root   root   6 Nov 29 22:46 trace
"
459,MCOL-1944,MCOL,Daniel Lee,120072,2018-11-30 22:11:24,"Build tested: 1.2.2-1

Installed 1.2.2-1 as non-root (guest user) on centos 7.  I got errors when running the command under the root user:

[root@localhost ~]# export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
[root@localhost ~]# export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
[root@localhost ~]# 
[root@localhost ~]# /home/guest/mariadb/columnstore/bin/syslogSetup.sh install --user=guest
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 66: /usr/local/mariadb/columnstore/bin/getConfig: No such file or directory

System logging being used: rsyslog

cp: cannot stat ‘/usr/local/mariadb/columnstore/bin/columnstoreSyslog7’: No such file or directory
sed: can't read /etc/rsyslog.d/49-columnstore.conf: No such file or directory
sed: can't read /etc/rsyslog.d/49-columnstore.conf: No such file or directory
chmod: cannot access ‘/etc/rsyslog.d/49-columnstore.conf’: No such file or directory
chmod: cannot access ‘/etc/logrotate.d/columnstore’: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory



",11,"Build tested: 1.2.2-1

Installed 1.2.2-1 as non-root (guest user) on centos 7.  I got errors when running the command under the root user:

[root@localhost ~]# export COLUMNSTORE_INSTALL_DIR=/home/guest/mariadb/columnstore
[root@localhost ~]# export LD_LIBRARY_PATH=:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib:/home/guest/mariadb/columnstore/lib:/home/guest/mariadb/columnstore/mysql/lib
[root@localhost ~]# 
[root@localhost ~]# /home/guest/mariadb/columnstore/bin/syslogSetup.sh install --user=guest
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 66: /usr/local/mariadb/columnstore/bin/getConfig: No such file or directory

System logging being used: rsyslog

cp: cannot stat ‘/usr/local/mariadb/columnstore/bin/columnstoreSyslog7’: No such file or directory
sed: can't read /etc/rsyslog.d/49-columnstore.conf: No such file or directory
sed: can't read /etc/rsyslog.d/49-columnstore.conf: No such file or directory
chmod: cannot access ‘/etc/rsyslog.d/49-columnstore.conf’: No such file or directory
chmod: cannot access ‘/etc/logrotate.d/columnstore’: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory
/home/guest/mariadb/columnstore/bin/syslogSetup.sh: line 245: /usr/local/mariadb/columnstore/bin/cplogger: No such file or directory



"
460,MCOL-1944,MCOL,David Hill,120073,2018-11-30 22:16:59,"unknown why its failing. i tested as mysql user, will restest as guest user to see if i can reproduce",12,"unknown why its failing. i tested as mysql user, will restest as guest user to see if i can reproduce"
461,MCOL-1944,MCOL,David Thompson,120077,2018-12-01 00:39:01,"worked for me on ubuntu18 using the commands output by post-install:
/home/mysql/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mysql/mariadb/columnstore --user=mysql install

--installdir is required.",13,"worked for me on ubuntu18 using the commands output by post-install:
/home/mysql/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mysql/mariadb/columnstore --user=mysql install

--installdir is required."
462,MCOL-1944,MCOL,Daniel Lee,120081,2018-12-01 23:11:38,Retested,14,Retested
463,MCOL-1952,MCOL,Andrew Hutchings,119560,2018-11-20 16:13:40,Skipped review since this is a rebase. Develop is now rebased on MariaDB 10.3.11,1,Skipped review since this is a rebase. Develop is now rebased on MariaDB 10.3.11
464,MCOL-1952,MCOL,Daniel Lee,119635,2018-11-21 16:52:41,"
Build verified: GitHub source

[root@localhost ~]# cat gitInfo.log 
/root/columnstore/mariadb-columnstore-server
commit 6bd66f5df21c6a8aa638ca92051bb833f2a8479f
Author: david hill <david.hill@mariadb.com>
Date:   Thu Nov 15 09:22:34 2018 -0600

    update version
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 99f595b9dc7d5a576ef4ebb87907ad375c2d1ee4
Merge: 4107348 3bf269b
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Nov 21 15:25:30 2018 +0000

    Merge pull request #634 from mariadb-corporation/MCOL-1519_2
    
    MCOL-1519 GROUP BY handler now uses an appropriate SELECT_LEX structure.

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 11
Server version: 10.3.11-MariaDB-log Columnstore 1.2.2-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

",2,"
Build verified: GitHub source

[root@localhost ~]# cat gitInfo.log 
/root/columnstore/mariadb-columnstore-server
commit 6bd66f5df21c6a8aa638ca92051bb833f2a8479f
Author: david hill 
Date:   Thu Nov 15 09:22:34 2018 -0600

    update version
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 99f595b9dc7d5a576ef4ebb87907ad375c2d1ee4
Merge: 4107348 3bf269b
Author: Andrew Hutchings 
Date:   Wed Nov 21 15:25:30 2018 +0000

    Merge pull request #634 from mariadb-corporation/MCOL-1519_2
    
    MCOL-1519 GROUP BY handler now uses an appropriate SELECT_LEX structure.

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 11
Server version: 10.3.11-MariaDB-log Columnstore 1.2.2-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

"
465,MCOL-1961,MCOL,Jens Röwekamp,121471,2019-01-07 20:09:50,"*Done in this ticket:*
- Added the missing access functions for TableLockInfo in javamcsapi and pymcsapi through typemaps in Swig.
- Documented the changes in the Sphinx documents of javamcsapi and pymcsapi.
- Extended javamcsapi's test case to include the new getter functions.

*For QA:*
- execute mcsapi's regression test suite on CentOS 7, Windows, and one Debian/Ubuntu",1,"*Done in this ticket:*
- Added the missing access functions for TableLockInfo in javamcsapi and pymcsapi through typemaps in Swig.
- Documented the changes in the Sphinx documents of javamcsapi and pymcsapi.
- Extended javamcsapi's test case to include the new getter functions.

*For QA:*
- execute mcsapi's regression test suite on CentOS 7, Windows, and one Debian/Ubuntu"
466,MCOL-2,MCOL,David Hill,83376,2016-05-11 19:27:48,making change along with MCOL-3,1,making change along with MCOL-3
467,MCOL-2,MCOL,David Hill,83425,2016-05-13 21:06:48,"reassigning back to Hall.. Iwas going to do this at the time I was changing everything from infinidb to columnstore, including the headers, but found out we cant do that..

So best to assign back to Hall so he can change in the DB code that he knows best..

FYI- Changes in the post-mysqld-install script will also be needed at the same time..",2,"reassigning back to Hall.. Iwas going to do this at the time I was changing everything from infinidb to columnstore, including the headers, but found out we cant do that..

So best to assign back to Hall so he can change in the DB code that he knows best..

FYI- Changes in the post-mysqld-install script will also be needed at the same time.."
468,MCOL-2,MCOL,David Hall,83513,2016-05-18 22:32:58,Checked in core code. Assigning back to David Hill to finish OAM stuff.,3,Checked in core code. Assigning back to David Hill to finish OAM stuff.
469,MCOL-2,MCOL,David Hill,83518,2016-05-19 14:15:08,"The oam script hall is referring to is the post-mysql-install script checks for engine name of infinidb... Therre might be others that I need to check. BUT this is only changing the internal platform code, NOT test cases.

But those will need to change as well, but not the week of May 19, most likely",4,"The oam script hall is referring to is the post-mysql-install script checks for engine name of infinidb... Therre might be others that I need to check. BUT this is only changing the internal platform code, NOT test cases.

But those will need to change as well, but not the week of May 19, most likely"
470,MCOL-2,MCOL,David Hill,83524,2016-05-19 20:42:37,oam file change post-mysql-install - change the check for engine of infinidb to columnstore,5,oam file change post-mysql-install - change the check for engine of infinidb to columnstore
471,MCOL-2,MCOL,Daniel Lee,83861,2016-05-31 15:16:17,"Build verified:

getsoftwareinfo   Tue May 31 10:30:28 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Fri 27 May 2016 04:07:53 PM EDT
Install Date: Mon 30 May 2016 11:32:15 AM EDT      Build Host: srvbuilder

For Alpha, InfiniDB was intentionally left in there for compatibility with existing scripts.  It will be removed later.

MariaDB [mytest]> show engines;
+--------------------+---------+--------------------------------------------------------------------------------------------------+--------------+------+------------+
| Engine             | Support | Comment                                                                                          | Transactions | XA   | Savepoints |
+--------------------+---------+--------------------------------------------------------------------------------------------------+--------------+------+------------+
| Columnstore        | YES     | Columnstore storage engine                                                                       | YES          | NO   | NO         |
| CSV                | YES     | CSV storage engine                                                                               | NO           | NO   | NO         |
| MRG_MyISAM         | YES     | Collection of identical MyISAM tables                                                            | NO           | NO   | NO         |
| MyISAM             | YES     | MyISAM storage engine                                                                            | NO           | NO   | NO         |
| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables                                        | NO           | NO   | NO         |
| InfiniDB           | YES     | Columnstore storage engine (deprecated: use columnstore)                                         | YES          | NO   | NO         |
| InnoDB             | DEFAULT | Percona-XtraDB, Supports transactions, row-level locking, foreign keys and encryption for tables | YES          | YES  | YES        |
| Aria               | YES     | Crash-safe tables with MyISAM heritage                                                           | NO           | NO   | NO         |
| SEQUENCE           | YES     | Generated tables filled with sequential values                                                   | YES          | NO   | YES        |
| PERFORMANCE_SCHEMA | YES     | Performance Schema                                                                               | NO           | NO   | NO         |
+--------------------+---------+--------------------------------------------------------------------------------------------------+--------------+------+------------+
10 rows in set (0.00 sec)
",6,"Build verified:

getsoftwareinfo   Tue May 31 10:30:28 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Fri 27 May 2016 04:07:53 PM EDT
Install Date: Mon 30 May 2016 11:32:15 AM EDT      Build Host: srvbuilder

For Alpha, InfiniDB was intentionally left in there for compatibility with existing scripts.  It will be removed later.

MariaDB [mytest]> show engines;
+--------------------+---------+--------------------------------------------------------------------------------------------------+--------------+------+------------+
| Engine             | Support | Comment                                                                                          | Transactions | XA   | Savepoints |
+--------------------+---------+--------------------------------------------------------------------------------------------------+--------------+------+------------+
| Columnstore        | YES     | Columnstore storage engine                                                                       | YES          | NO   | NO         |
| CSV                | YES     | CSV storage engine                                                                               | NO           | NO   | NO         |
| MRG_MyISAM         | YES     | Collection of identical MyISAM tables                                                            | NO           | NO   | NO         |
| MyISAM             | YES     | MyISAM storage engine                                                                            | NO           | NO   | NO         |
| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables                                        | NO           | NO   | NO         |
| InfiniDB           | YES     | Columnstore storage engine (deprecated: use columnstore)                                         | YES          | NO   | NO         |
| InnoDB             | DEFAULT | Percona-XtraDB, Supports transactions, row-level locking, foreign keys and encryption for tables | YES          | YES  | YES        |
| Aria               | YES     | Crash-safe tables with MyISAM heritage                                                           | NO           | NO   | NO         |
| SEQUENCE           | YES     | Generated tables filled with sequential values                                                   | YES          | NO   | YES        |
| PERFORMANCE_SCHEMA | YES     | Performance Schema                                                                               | NO           | NO   | NO         |
+--------------------+---------+--------------------------------------------------------------------------------------------------+--------------+------+------------+
10 rows in set (0.00 sec)
"
472,MCOL-2005,MCOL,Daniel Lee,122154,2019-01-21 16:56:57,"Build verified:

server commit:
b5a7a22
engine commit:
d87b9a6

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.21-MariaDB-log Columnstore 1.1.7-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

",1,"Build verified:

server commit:
b5a7a22
engine commit:
d87b9a6

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.21-MariaDB-log Columnstore 1.1.7-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

"
473,MCOL-2013,MCOL,Jens Röwekamp,122115,2019-01-21 10:46:38,"Fixed the identified issues so that the .NET adapter can now successfully be built on Windows 10, CentOS 7, and Debian 9.

*For QA:*
- manually build the .NET adapter as documented in dotnet/README.md
- manually execute the test suite as documented in dotnet/README.md",1,"Fixed the identified issues so that the .NET adapter can now successfully be built on Windows 10, CentOS 7, and Debian 9.

*For QA:*
- manually build the .NET adapter as documented in dotnet/README.md
- manually execute the test suite as documented in dotnet/README.md"
474,MCOL-2068,MCOL,Jens Röwekamp,121908,2019-01-17 14:04:26,"Added command line parameter to postConfigure to set the Columnstore.xml values
- NumBlocksPct
- TotalUmMemory

Further created a new branch develop-1.2 in mariadb-columnstore-docker that uses the yum repository build from the Columnstore 1.2.3 (develop-1.2) branches. Added and documented the necessary environment variables and code in mariadb-columnstore-docker to set this values through the initial dbinit script.

*Note:* The develop-1.2 mariadb-columnstore-docker repository can only be used and verified once the changes to mariadb-columnstore-engine are applied and the new packages are build by buildbot and uploaded to nightly. In the meantime one could use the yum repository: 
http://34.238.186.75/repos/1.2.3-1/TestBuild-MCOL-2068/mariadb-columnstore/yum/centos/7/x86_64/

*For QA*:
- do various install tests of Columnstore, and verify that the default settings for NumBlocksPct and TotalUmMemory haven't changed
- do various install tests of Columnstore with applied -numBlocksPct and -totalUmMemory parameter, and verify that these settings are applied
- the container tests will be handled in the DBAAS-544 ticket",1,"Added command line parameter to postConfigure to set the Columnstore.xml values
- NumBlocksPct
- TotalUmMemory

Further created a new branch develop-1.2 in mariadb-columnstore-docker that uses the yum repository build from the Columnstore 1.2.3 (develop-1.2) branches. Added and documented the necessary environment variables and code in mariadb-columnstore-docker to set this values through the initial dbinit script.

*Note:* The develop-1.2 mariadb-columnstore-docker repository can only be used and verified once the changes to mariadb-columnstore-engine are applied and the new packages are build by buildbot and uploaded to nightly. In the meantime one could use the yum repository: 
URL

*For QA*:
- do various install tests of Columnstore, and verify that the default settings for NumBlocksPct and TotalUmMemory haven't changed
- do various install tests of Columnstore with applied -numBlocksPct and -totalUmMemory parameter, and verify that these settings are applied
- the container tests will be handled in the DBAAS-544 ticket"
475,MCOL-2068,MCOL,Daniel Lee,125820,2019-04-03 20:53:02,"Build verified: 1.2.4-1 nightly

server commit:
137b9a8
engine commit:
7b1f461
[dlee@master centos7]$ 


1. Verified postConfigure help text
2. Verified both -numBlocksPct and -totalUMMemory parameters and settings are applied correctly in ColumnStore.xml file
3. Verified that PrimProc observes memory settings, when postConfigure is run with or without these two parameters specified
4. Verified in both single node and milti-node installations
",2,"Build verified: 1.2.4-1 nightly

server commit:
137b9a8
engine commit:
7b1f461
[dlee@master centos7]$ 


1. Verified postConfigure help text
2. Verified both -numBlocksPct and -totalUMMemory parameters and settings are applied correctly in ColumnStore.xml file
3. Verified that PrimProc observes memory settings, when postConfigure is run with or without these two parameters specified
4. Verified in both single node and milti-node installations
"
476,MCOL-2082,MCOL,Jens Röwekamp,122488,2019-01-28 11:03:48,Added the usage documentation for mcsapi for Spark and mcsapi for PySpark,1,Added the usage documentation for mcsapi for Spark and mcsapi for PySpark
477,MCOL-2082,MCOL,Jens Röwekamp,122489,2019-01-28 11:07:18,"*For QA*:
- please check if you can replicate every step mentioned in the usage documentation to export a DataFrame
- please highlight everything that could be improved, to get the reader started as soon as possible",2,"*For QA*:
- please check if you can replicate every step mentioned in the usage documentation to export a DataFrame
- please highlight everything that could be improved, to get the reader started as soon as possible"
478,MCOL-2084,MCOL,Patrick LeBlanc,121811,2019-01-15 16:15:53,"We'll also have to subclass IDBFileSystem.  It uses system calls and boost to do things to the local filesystem.
",1,"We'll also have to subclass IDBFileSystem.  It uses system calls and boost to do things to the local filesystem.
"
479,MCOL-2084,MCOL,Patrick LeBlanc,121874,2019-01-16 21:44:03,"Looks like the fcns I thought had to be overridden in IDBPolicy are redirects to the *FileSystem class.  Should not need to subclass IDBPolicy, just IDBFileSystem and IDBDataFile.",2,"Looks like the fcns I thought had to be overridden in IDBPolicy are redirects to the *FileSystem class.  Should not need to subclass IDBPolicy, just IDBFileSystem and IDBDataFile."
480,MCOL-209,MCOL,Daniel Lee,87930,2016-10-31 21:49:02,"Build tested: 1.0.4 beta 1

For both Columnstore and InfiniDB, both queries 1 and 21 did not run for the 100gb database.
For InfiniDB, query 11 did not run for all 1, 10, 100gb databases
For  queries 2, 3, 5, 7, 8, 12, 20, and 21 returned empty set and queries 17 and 19 returned null, which is incorrect results.  These queries returned actual results and I believe this is because we fixed some bugs in 1.0.3 and 1.0.4.  The elapsed time for these queries on InfiniDB were less than they supposed to be and affected the overall timeing.  If we compare only queries with match results, performance numbers for both Columnstore and InfiniDB are virtually identical, ranging from 0.1% to 1.79%.


",1,"Build tested: 1.0.4 beta 1

For both Columnstore and InfiniDB, both queries 1 and 21 did not run for the 100gb database.
For InfiniDB, query 11 did not run for all 1, 10, 100gb databases
For  queries 2, 3, 5, 7, 8, 12, 20, and 21 returned empty set and queries 17 and 19 returned null, which is incorrect results.  These queries returned actual results and I believe this is because we fixed some bugs in 1.0.3 and 1.0.4.  The elapsed time for these queries on InfiniDB were less than they supposed to be and affected the overall timeing.  If we compare only queries with match results, performance numbers for both Columnstore and InfiniDB are virtually identical, ranging from 0.1% to 1.79%.


"
481,MCOL-209,MCOL,Daniel Lee,90242,2017-01-04 14:56:47,"Build tested: 1.0.6-1 GA

Performed the above tests on the GA release.",2,"Build tested: 1.0.6-1 GA

Performed the above tests on the GA release."
482,MCOL-2097,MCOL,Jens Röwekamp,122204,2019-01-22 11:59:35,"Adapter and (lib|java|py)mcsapi's documentation uploaded to the usual place in our Google Team Drive.

Bulk Write SDK 1.1.7 that was used to build the adapter successfully tested against CS 1.1.6 and CS 1.2.3 nightly (engine=83b2d4c server=61f32f2) through its regression test suite.

Adapter successfully tested against the same CS versions through its regression test suite.",1,"Adapter and (lib|java|py)mcsapi's documentation uploaded to the usual place in our Google Team Drive.

Bulk Write SDK 1.1.7 that was used to build the adapter successfully tested against CS 1.1.6 and CS 1.2.3 nightly (engine=83b2d4c server=61f32f2) through its regression test suite.

Adapter successfully tested against the same CS versions through its regression test suite."
483,MCOL-210,MCOL,Daniel Lee,90243,2017-01-04 15:00:27,"Build tested: 1.0.6-1 GA

Tested was done for 1g database and for single server only.

The 1g test is a good comparison between ColumnStore and InnoDB.  The 10g test would take too many hours to completed.  ",1,"Build tested: 1.0.6-1 GA

Tested was done for 1g database and for single server only.

The 1g test is a good comparison between ColumnStore and InnoDB.  The 10g test would take too many hours to completed.  "
484,MCOL-2101,MCOL,David Hall,141599,2020-01-06 21:16:19,Would not startup code need root access to create the tmp dir on some machines?,1,Would not startup code need root access to create the tmp dir on some machines?
485,MCOL-2101,MCOL,Andrew Hutchings,141608,2020-01-07 08:50:40,For RPM/DEB that would be easy during the postinst script. For non-root we would probably use /usr/local/mariadb/columnstore/tmp or something like that.,2,For RPM/DEB that would be easy during the postinst script. For non-root we would probably use /usr/local/mariadb/columnstore/tmp or something like that.
486,MCOL-2101,MCOL,Gagan Goel,143464,2020-01-31 23:09:43,"""columnstore start"" already creates the temp dir which is in the SystemTempFileDir value in the config file. This is /tmp/columnstore_tmp_files for root user and $HOME/.tmp for non-root user.",3,"""columnstore start"" already creates the temp dir which is in the SystemTempFileDir value in the config file. This is /tmp/columnstore_tmp_files for root user and $HOME/.tmp for non-root user."
487,MCOL-2101,MCOL,David Hill,143903,2020-02-07 15:47:19,"The issue isnt that the tmp directories arent being created, its that the tmp directory for root user is only writeable by root user. Customer is running some commands as non-root user, which is allowed, and the nonroot user is getting an error writing into the root user install /tmp area.  The BUG request to to setup /tmp for columnstore to allow all users to write to it.",4,"The issue isnt that the tmp directories arent being created, its that the tmp directory for root user is only writeable by root user. Customer is running some commands as non-root user, which is allowed, and the nonroot user is getting an error writing into the root user install /tmp area.  The BUG request to to setup /tmp for columnstore to allow all users to write to it."
488,MCOL-2101,MCOL,Gagan Goel,144036,2020-02-10 22:46:40,Patches submitted in the 3 respective branches.,5,Patches submitted in the 3 respective branches.
489,MCOL-2101,MCOL,Daniel Lee,146934,2020-03-17 18:20:49,"Build verified: 1.2.6-1 BB

engine commit:
d4173ef

Build verified: 1.5.0-1 BB

engine commit:
f01185f

1.4.4 is not yet available from buildbot.

/tmp/columnstore_tmp_files directory permission has been set to 777.  


",6,"Build verified: 1.2.6-1 BB

engine commit:
d4173ef

Build verified: 1.5.0-1 BB

engine commit:
f01185f

1.4.4 is not yet available from buildbot.

/tmp/columnstore_tmp_files directory permission has been set to 777.  


"
490,MCOL-2101,MCOL,Daniel Lee,147164,2020-03-19 21:03:10,"Build verified: 1.4.4.-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 86a634a0feaf7788c9bcf7cc763e500d2be97d75
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Fri Feb 28 21:55:32 2020 +0100

    Revert ""make columnstore maturity gamma""
    
    This reverts commit e4a0372cd08a53f97a62d6b6ef32114b553cacb7.


/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit ca3e2d78d6e1d06fb6711befe7bb2d618e801929
Merge: ec3630d f437152
Author: Patrick LeBlanc <43503225+pleblanc1976@users.noreply.github.com>
Date:   Thu Mar 19 11:43:55 2020 -0500

    Merge pull request #1113 from pleblanc1976/develop-1.4
    
    Bumped version num to 1.4.4-1
",7,"Build verified: 1.4.4.-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 86a634a0feaf7788c9bcf7cc763e500d2be97d75
Author: Sergei Golubchik 
Date:   Fri Feb 28 21:55:32 2020 +0100

    Revert ""make columnstore maturity gamma""
    
    This reverts commit e4a0372cd08a53f97a62d6b6ef32114b553cacb7.


/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit ca3e2d78d6e1d06fb6711befe7bb2d618e801929
Merge: ec3630d f437152
Author: Patrick LeBlanc 
Date:   Thu Mar 19 11:43:55 2020 -0500

    Merge pull request #1113 from pleblanc1976/develop-1.4
    
    Bumped version num to 1.4.4-1
"
491,MCOL-211,MCOL,David Thompson,88160,2016-11-07 21:34:49,"We have the ability to test in AWS and on hosted bare metal servers, i think this can be closed now.",1,"We have the ability to test in AWS and on hosted bare metal servers, i think this can be closed now."
492,MCOL-2110,MCOL,Andrew Hutchings,122417,2019-01-25 11:39:23,"Can you please provide a little more details with regards to the problems you face?

[~ben.thompson] I believe BuildBot is using out of source builds. Are you aware of any issues with the README.md instructions in the server tree?",1,"Can you please provide a little more details with regards to the problems you face?

[~ben.thompson] I believe BuildBot is using out of source builds. Are you aware of any issues with the README.md instructions in the server tree?"
493,MCOL-2110,MCOL,David Mott,122427,2019-01-25 13:22:05,"[pull request 681 was created|https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/681]

Prefixing SERVER_BUILD_INCLUDE_DIR and SERVER_SOURCE_ROOT_DIR with CMAKE_BINARY_DIR works in the case of the engine and server build folders being siblings to each other and relative paths supplied to cmake. If the build folders are not siblings and absolute paths are supplied to cmake it fails. Using GET_FILENAME_COMPONENT will resolve the correct path in both cases.

",2,"[pull request 681 was created|URL

Prefixing SERVER_BUILD_INCLUDE_DIR and SERVER_SOURCE_ROOT_DIR with CMAKE_BINARY_DIR works in the case of the engine and server build folders being siblings to each other and relative paths supplied to cmake. If the build folders are not siblings and absolute paths are supplied to cmake it fails. Using GET_FILENAME_COMPONENT will resolve the correct path in both cases.

"
494,MCOL-2110,MCOL,Roman,123748,2019-02-23 03:58:11,Tested the fix.,3,Tested the fix.
495,MCOL-2115,MCOL,Daniel Lee,123100,2019-02-08 17:04:54,"Build verified: 1.0.16-1 (Build from buildbot dated 02/06/2019 23:14

",1,"Build verified: 1.0.16-1 (Build from buildbot dated 02/06/2019 23:14

"
496,MCOL-2120,MCOL,Andrew Hutchings,122556,2019-01-29 09:01:02,Ben has confirmed that Debian 8 and 9 have it. The others still need to be checked.,1,Ben has confirmed that Debian 8 and 9 have it. The others still need to be checked.
497,MCOL-2120,MCOL,Ben Thompson,123016,2019-02-06 22:59:30,Buildbot has been updated nothing to checkin but need someone to confirm this is done,2,Buildbot has been updated nothing to checkin but need someone to confirm this is done
498,MCOL-2120,MCOL,Richard Stracke,123712,2019-02-22 08:05:21,"Numa for Centos is missing also for usual MariaDB 10.3.12

",3,"Numa for Centos is missing also for usual MariaDB 10.3.12

"
499,MCOL-2120,MCOL,Ben Thompson,154751,2020-05-29 19:08:01,Can you look into this now that you're doing new CI and builds,4,Can you look into this now that you're doing new CI and builds
500,MCOL-2129,MCOL,Jens Röwekamp,122797,2019-02-01 15:38:16,Added postConfigure flag -xr to reverse lookup the submitted hostnames and use the returned name instead.,1,Added postConfigure flag -xr to reverse lookup the submitted hostnames and use the returned name instead.
501,MCOL-2129,MCOL,Daniel Lee,125876,2019-04-04 16:46:26,"Build verified: 1.2.3-1 ColumnStore Docker

The fix was actually in 1.2.3-1

https://github.com/mariadb-corporation/mariadb-columnstore-docker.git

Followed instructions to do a docker setup for 2um2pm stack.

and verified the stack is working properly.  

Also verified the reverse dns lookup is working.

[dlee@master columnstore_zeppelin]$ sudo docker ps
CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                    NAMES
7f6799b2bba3        mariadb/sandboxzeppelin   ""/zeppelin/bootstrap…""   13 minutes ago      Up 13 minutes       0.0.0.0:8080->8080/tcp   columnstorezeppelin_zeppelin_1
d120b997595b        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       3306/tcp                 columnstorezeppelin_pm1_1
5d3526793194        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       0.0.0.0:3307->3306/tcp   columnstorezeppelin_um2_1
12c0f9521f24        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       3306/tcp                 columnstorezeppelin_pm2_1
8900481df985        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       0.0.0.0:3306->3306/tcp   columnstorezeppelin_um1_1
[dlee@master columnstore_zeppelin]$ sudo docker exec -it columnstorezeppelin_um1_1 /bin/bash

[root@8900481df985 /]# ls
anaconda-post.log  dev                         docker-entrypoint.sh  home  lib64  media  opt   root  sbin  sys  usr
bin                docker-entrypoint-initdb.d  etc                   lib   log    mnt    proc  run   srv   tmp  var
[root@8900481df985 /]# ping um2 -c2
PING um2 (10.5.0.3) 56(84) bytes of data.
64 bytes from columnstorezeppelin_um2_1.columnstorezeppelin_mcsnet (10.5.0.3): icmp_seq=1 ttl=64 time=0.039 ms
64 bytes from columnstorezeppelin_um2_1.columnstorezeppelin_mcsnet (10.5.0.3): icmp_seq=2 ttl=64 time=0.129 ms

--- um2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.039/0.084/0.129/0.045 ms
[root@8900481df985 /]# mcsmysql mysql
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
[root@8900481df985 /]# mcsmysql mysql -ppass
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 58
Server version: 10.3.13-MariaDB-log Columnstore 1.2.3-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mysql]> quit

[root@8900481df985 /]# nslookup 10.5.0.3
Server:		127.0.0.11
Address:	127.0.0.11#53

Non-authoritative answer:
3.0.5.10.in-addr.arpa	name = columnstorezeppelin_um2_1.columnstorezeppelin_mcsnet.",2,"Build verified: 1.2.3-1 ColumnStore Docker

The fix was actually in 1.2.3-1

URL

Followed instructions to do a docker setup for 2um2pm stack.

and verified the stack is working properly.  

Also verified the reverse dns lookup is working.

[dlee@master columnstore_zeppelin]$ sudo docker ps
CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                    NAMES
7f6799b2bba3        mariadb/sandboxzeppelin   ""/zeppelin/bootstrap…""   13 minutes ago      Up 13 minutes       0.0.0.0:8080->8080/tcp   columnstorezeppelin_zeppelin_1
d120b997595b        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       3306/tcp                 columnstorezeppelin_pm1_1
5d3526793194        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       0.0.0.0:3307->3306/tcp   columnstorezeppelin_um2_1
12c0f9521f24        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       3306/tcp                 columnstorezeppelin_pm2_1
8900481df985        mariadb/columnstore:1.2   ""docker-entrypoint.s…""   13 minutes ago      Up 13 minutes       0.0.0.0:3306->3306/tcp   columnstorezeppelin_um1_1
[dlee@master columnstore_zeppelin]$ sudo docker exec -it columnstorezeppelin_um1_1 /bin/bash

[root@8900481df985 /]# ls
anaconda-post.log  dev                         docker-entrypoint.sh  home  lib64  media  opt   root  sbin  sys  usr
bin                docker-entrypoint-initdb.d  etc                   lib   log    mnt    proc  run   srv   tmp  var
[root@8900481df985 /]# ping um2 -c2
PING um2 (10.5.0.3) 56(84) bytes of data.
64 bytes from columnstorezeppelin_um2_1.columnstorezeppelin_mcsnet (10.5.0.3): icmp_seq=1 ttl=64 time=0.039 ms
64 bytes from columnstorezeppelin_um2_1.columnstorezeppelin_mcsnet (10.5.0.3): icmp_seq=2 ttl=64 time=0.129 ms

--- um2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.039/0.084/0.129/0.045 ms
[root@8900481df985 /]# mcsmysql mysql
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)
[root@8900481df985 /]# mcsmysql mysql -ppass
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 58
Server version: 10.3.13-MariaDB-log Columnstore 1.2.3-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mysql]> quit

[root@8900481df985 /]# nslookup 10.5.0.3
Server:		127.0.0.11
Address:	127.0.0.11#53

Non-authoritative answer:
3.0.5.10.in-addr.arpa	name = columnstorezeppelin_um2_1.columnstorezeppelin_mcsnet."
502,MCOL-213,MCOL,David Thompson,88866,2016-11-29 21:27:14,[~dleeyh]  can we close this out now. I'm not sure if it's interesting for us to do any more comparison now we already reached parity on the smaller sizes?,1,[~dleeyh]  can we close this out now. I'm not sure if it's interesting for us to do any more comparison now we already reached parity on the smaller sizes?
503,MCOL-213,MCOL,Daniel Lee,92482,2017-03-02 15:03:16,Completed,2,Completed
504,MCOL-214,MCOL,David Thompson,88865,2016-11-29 21:26:34,[~dleeyh] in your testing did you use innodb tables in your column store server or use a standalone vanilla server install? We should probably test against the latter if we did the former.,1,[~dleeyh] in your testing did you use innodb tables in your column store server or use a standalone vanilla server install? We should probably test against the latter if we did the former.
505,MCOL-214,MCOL,Daniel Lee,90245,2017-01-04 15:22:03,"Sorry, I missed this msg.  I use for the former.  I will redo the test using the later.  Running LDI for 1g already took 2 hours.  I think we should do just a 1gb test for InnoDB.

",2,"Sorry, I missed this msg.  I use for the former.  I will redo the test using the later.  Running LDI for 1g already took 2 hours.  I think we should do just a 1gb test for InnoDB.

"
506,MCOL-214,MCOL,Daniel Lee,92483,2017-03-02 15:03:55,Completed,3,Completed
507,MCOL-2158,MCOL,Daniel Lee,123420,2019-02-15 03:25:24,"Build verified: 1.1.7-1

server commit:
3e8a9df
engine commit:
827d6b5
",1,"Build verified: 1.1.7-1

server commit:
3e8a9df
engine commit:
827d6b5
"
508,MCOL-2178,MCOL,Daniel Lee,134461,2019-09-19 18:21:47,"Build verified: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
975463c

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 12
Server version: 10.4.8-3-MariaDB-log Source distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

Issues found will be reported in as new bug tickets.

",1,"Build verified: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
975463c

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 12
Server version: 10.4.8-3-MariaDB-log Source distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

Issues found will be reported in as new bug tickets.

"
509,MCOL-22,MCOL,David Hill,83268,2016-05-06 21:31:06,"package name changes

currently:

infinidb-enterprise-5.0-0.x86_64.rpm              infinidb-mariadb-5.0-0-x86_64-centos6-common.rpm  infinidb-platform-5.0-0.x86_64.rpm
infinidb-libs-5.0-0.x86_64.rpm                    infinidb-mariadb-5.0-0-x86_64-centos6-server.rpm  infinidb-storage-engine-5.0-0.x86_64.rpm
infinidb-mariadb-5.0-0-x86_64-centos6-client.rpm  infinidb-mariadb-5.0-0-x86_64-centos6-shared.rpm
",1,"package name changes

currently:

infinidb-enterprise-5.0-0.x86_64.rpm              infinidb-mariadb-5.0-0-x86_64-centos6-common.rpm  infinidb-platform-5.0-0.x86_64.rpm
infinidb-libs-5.0-0.x86_64.rpm                    infinidb-mariadb-5.0-0-x86_64-centos6-server.rpm  infinidb-storage-engine-5.0-0.x86_64.rpm
infinidb-mariadb-5.0-0-x86_64-centos6-client.rpm  infinidb-mariadb-5.0-0-x86_64-centos6-shared.rpm
"
510,MCOL-22,MCOL,David Hill,83760,2016-05-29 22:52:05,"Have CentOS 6.6 build gernating the following rpms:

mariadb-columnstore-1.0-0-x86_64-centos6-client.rpm
mariadb-columnstore-1.0-0-x86_64-centos6-common.rpm
mariadb-columnstore-1.0-0-x86_64-centos6-server.rpm
mariadb-columnstore-1.0-0-x86_64-centos6-shared.rpm
mariadb-columnstore-enterprise-1.0-0.x86_64.rpm
mariadb-columnstore-libs-1.0-0.x86_64.rpm
mariadb-columnstore-platform-1.0-0.x86_64.rpm
mariadb-columnstore-storage-engine-1.0-0.x86_64.rpm",2,"Have CentOS 6.6 build gernating the following rpms:

mariadb-columnstore-1.0-0-x86_64-centos6-client.rpm
mariadb-columnstore-1.0-0-x86_64-centos6-common.rpm
mariadb-columnstore-1.0-0-x86_64-centos6-server.rpm
mariadb-columnstore-1.0-0-x86_64-centos6-shared.rpm
mariadb-columnstore-enterprise-1.0-0.x86_64.rpm
mariadb-columnstore-libs-1.0-0.x86_64.rpm
mariadb-columnstore-platform-1.0-0.x86_64.rpm
mariadb-columnstore-storage-engine-1.0-0.x86_64.rpm"
511,MCOL-22,MCOL,Daniel Lee,83884,2016-05-31 21:49:01,"Build verified:
getsoftwareinfo   Tue May 31 16:48:37 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Fri 27 May 2016 03:07:53 PM CDT
Install Date: Tue 31 May 2016 04:33:28 PM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-0.src.rpm

Tested RPM installation on Centos 6.8",3,"Build verified:
getsoftwareinfo   Tue May 31 16:48:37 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Fri 27 May 2016 03:07:53 PM CDT
Install Date: Tue 31 May 2016 04:33:28 PM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-0.src.rpm

Tested RPM installation on Centos 6.8"
512,MCOL-220,MCOL,Daniel Lee,85983,2016-08-30 16:13:49,Task has been completed,1,Task has been completed
513,MCOL-2218,MCOL,Daniel Lee,124361,2019-03-08 14:47:03,"Build verified: 1.2.3-1 nightly

[dlee@cs-tst-06 centos7]$ cat gitversionInfo.txt 
server commit:
2243073
engine commit:
26e6154

",1,"Build verified: 1.2.3-1 nightly

[dlee@cs-tst-06 centos7]$ cat gitversionInfo.txt 
server commit:
2243073
engine commit:
26e6154

"
514,MCOL-23,MCOL,David Hill,83494,2016-05-18 13:57:34,"some query fails with local errors

/root/genii/mysql/queries/working_tpch1/misc

[root@srvregtest misc]# more bug2954.sql.log
ERROR 1690 (22003) at line 1: DOUBLE value is out of range in 'cot(0)'

[root@srvregtest misc]# more bug3783.sql.log
ERROR 1815 (HY000) at line 5: Internal error: IDB-2015: Sorting length exceeded. Session varia
ble max_length_for_sort_data needs to be set higher.

[root@srvregtest misc]# more bug5267.sql.log
ERROR 1178 (42000) at line 1: The storage engine for the table doesn't support IDB-1001: Funct
ion 'xor' can only be used in the outermost select or order by clause and cannot be used in co
njunction with an aggregate function.

/root/genii/mysql/queries/working_dml/misc

pwd
/root/genii/mysql/queries/working_dml/misc
[root@srvregtest misc]# grep -i error vtabledmlcol2col.sql.log
71	65916ERROR 1815 (HY000) at line 128: Internal error: IDB-2004: Cannot connect to ExeMgr.

pwd
/root/genii/mysql/queries/working_dml/misc
[root@srvregtest misc]# grep -i error vtabledmlcol2col.sql.log
71	65916ERROR 1815 (HY000) at line 128: Internal error: IDB-2004: Cannot connect to ExeMgr.
",1,"some query fails with local errors

/root/genii/mysql/queries/working_tpch1/misc

[root@srvregtest misc]# more bug2954.sql.log
ERROR 1690 (22003) at line 1: DOUBLE value is out of range in 'cot(0)'

[root@srvregtest misc]# more bug3783.sql.log
ERROR 1815 (HY000) at line 5: Internal error: IDB-2015: Sorting length exceeded. Session varia
ble max_length_for_sort_data needs to be set higher.

[root@srvregtest misc]# more bug5267.sql.log
ERROR 1178 (42000) at line 1: The storage engine for the table doesn't support IDB-1001: Funct
ion 'xor' can only be used in the outermost select or order by clause and cannot be used in co
njunction with an aggregate function.

/root/genii/mysql/queries/working_dml/misc

pwd
/root/genii/mysql/queries/working_dml/misc
[root@srvregtest misc]# grep -i error vtabledmlcol2col.sql.log
71	65916ERROR 1815 (HY000) at line 128: Internal error: IDB-2004: Cannot connect to ExeMgr.

pwd
/root/genii/mysql/queries/working_dml/misc
[root@srvregtest misc]# grep -i error vtabledmlcol2col.sql.log
71	65916ERROR 1815 (HY000) at line 128: Internal error: IDB-2004: Cannot connect to ExeMgr.
"
515,MCOL-23,MCOL,David Hall,83519,2016-05-19 17:02:11,"working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql, line 2: fails the compare. NULLIF on date vs datetime returns as though the compare was falsem yet select with = returns true

MariaDB [tpch1]> select cidx, CDATE, CDATETIME, NULLIF(CDATE,CDATETIME), CDATE=CDATETIME from datatypetestm order by cidx;
+------+------------+---------------------+-------------------------+-----------------+
| cidx | CDATE      | CDATETIME           | NULLIF(CDATE,CDATETIME) | CDATE=CDATETIME |
+------+------------+---------------------+-------------------------+-----------------+
|    1 | 1997-01-01 | 1997-01-01 00:00:00 | 1997-01-01              |               1 |
|    2 | 1997-01-01 | 1997-01-01 00:00:01 | 1997-01-01              |               0 |
|    3 | 1997-01-02 | 1997-01-02 00:00:01 | 1997-01-02              |               0 |
|    4 | 1997-01-03 | 1997-01-03 00:00:02 | 1997-01-03              |               0 |
|    5 | 1997-01-04 | 1997-01-04 00:00:03 | 1997-01-04              |               0 |
|    6 | 2009-12-28 | 2009-12-31 23:59:56 | 2009-12-28              |               0 |
|    7 | 2009-12-29 | 2009-12-31 23:59:57 | 2009-12-29              |               0 |
|    8 | 2009-12-30 | 2009-12-31 23:59:58 | 2009-12-30              |               0 |
|    9 | 2009-12-31 | 2009-12-31 23:59:59 | 2009-12-31              |               0 |
|   10 | 2009-12-31 | 2009-12-31 23:59:59 | 2009-12-31              |               0 |
|   11 | 2009-12-31 | 2009-12-31 23:59:59 | 2009-12-31              |               0 |
+------+------------+---------------------+-------------------------+-----------------+
11 rows in set (0.06 sec)

NULLIF should return NULL if the two values compare as =. For row 1, we see in the last column that they do compare =, yet NULLIF returns the value, which it should not do.",2,"working_tpch1/qa_fe_cnxFunctions/NULLIF.DM.sql, line 2: fails the compare. NULLIF on date vs datetime returns as though the compare was falsem yet select with = returns true

MariaDB [tpch1]> select cidx, CDATE, CDATETIME, NULLIF(CDATE,CDATETIME), CDATE=CDATETIME from datatypetestm order by cidx;
+------+------------+---------------------+-------------------------+-----------------+
| cidx | CDATE      | CDATETIME           | NULLIF(CDATE,CDATETIME) | CDATE=CDATETIME |
+------+------------+---------------------+-------------------------+-----------------+
|    1 | 1997-01-01 | 1997-01-01 00:00:00 | 1997-01-01              |               1 |
|    2 | 1997-01-01 | 1997-01-01 00:00:01 | 1997-01-01              |               0 |
|    3 | 1997-01-02 | 1997-01-02 00:00:01 | 1997-01-02              |               0 |
|    4 | 1997-01-03 | 1997-01-03 00:00:02 | 1997-01-03              |               0 |
|    5 | 1997-01-04 | 1997-01-04 00:00:03 | 1997-01-04              |               0 |
|    6 | 2009-12-28 | 2009-12-31 23:59:56 | 2009-12-28              |               0 |
|    7 | 2009-12-29 | 2009-12-31 23:59:57 | 2009-12-29              |               0 |
|    8 | 2009-12-30 | 2009-12-31 23:59:58 | 2009-12-30              |               0 |
|    9 | 2009-12-31 | 2009-12-31 23:59:59 | 2009-12-31              |               0 |
|   10 | 2009-12-31 | 2009-12-31 23:59:59 | 2009-12-31              |               0 |
|   11 | 2009-12-31 | 2009-12-31 23:59:59 | 2009-12-31              |               0 |
+------+------------+---------------------+-------------------------+-----------------+
11 rows in set (0.06 sec)

NULLIF should return NULL if the two values compare as =. For row 1, we see in the last column that they do compare =, yet NULLIF returns the value, which it should not do."
516,MCOL-23,MCOL,David Hall,83523,2016-05-19 20:13:33,"Re: NULLIF(CDATE,CDATETIME) the return type requested by MariaDB is DATE, where in mysql, it requested a string object. Columnstore has specific code for the date vs datetime NULLIF compare when a string is requested. When the DATE type return is requested, it had no such special processing. 

Added the special processing for this case.",3,"Re: NULLIF(CDATE,CDATETIME) the return type requested by MariaDB is DATE, where in mysql, it requested a string object. Columnstore has specific code for the date vs datetime NULLIF compare when a string is requested. When the DATE type return is requested, it had no such special processing. 

Added the special processing for this case."
517,MCOL-23,MCOL,David Hall,83529,2016-05-19 23:11:38,"Re: working_tpch1_compareLogOnly/misc/bug4433.sql
This test is for the change made to InfiniDB to increase the maximum size of database and table names from 64 to 128. This change has not been ported to columnstore, so this test is invalid at this time. Should we decide to increase the maximum size to 120 char, I suggest we do it for all of MariaDB, as it takes a change in the Server code to effect this modification.
",4,"Re: working_tpch1_compareLogOnly/misc/bug4433.sql
This test is for the change made to InfiniDB to increase the maximum size of database and table names from 64 to 128. This change has not been ported to columnstore, so this test is invalid at this time. Should we decide to increase the maximum size to 120 char, I suggest we do it for all of MariaDB, as it takes a change in the Server code to effect this modification.
"
518,MCOL-23,MCOL,David Hall,83530,2016-05-19 23:27:53,"Re: working_tpch1_compareLogOnly/onClauseJoins/bug4031.sql query one gets a different answer than InfiniDB. It looks like another case where InfiniDB's optimizer is conflicting with MariaDB's optimizer and moving things to the inside of a join when they should stay outside. 

This was indeed the same bug that optimized when it shouldn't. The shortcut I took before wasn't good enough. A full search of the list is needed. 

{code:java}
		// MariaDB 10.1: cached_table is never available for derived tables.
		// Find the uncached object in table_list
		TABLE_LIST* tblList = ifp->context->table_list;
		while (tblList)
		{
			if (strcasecmp(tblList->alias, ifp->table_name) == 0)
			{
				if (!tblList->outer_join)
				{
					sc->derivedTable(derivedName);
					sc->derivedRefCol(cols[j].get());
				}
				break;
			}
			tblList = tblList->next_local;
		}
{code}
",5,"Re: working_tpch1_compareLogOnly/onClauseJoins/bug4031.sql query one gets a different answer than InfiniDB. It looks like another case where InfiniDB's optimizer is conflicting with MariaDB's optimizer and moving things to the inside of a join when they should stay outside. 

This was indeed the same bug that optimized when it shouldn't. The shortcut I took before wasn't good enough. A full search of the list is needed. 

{code:java}
		// MariaDB 10.1: cached_table is never available for derived tables.
		// Find the uncached object in table_list
		TABLE_LIST* tblList = ifp->context->table_list;
		while (tblList)
		{
			if (strcasecmp(tblList->alias, ifp->table_name) == 0)
			{
				if (!tblList->outer_join)
				{
					sc->derivedTable(derivedName);
					sc->derivedRefCol(cols[j].get());
				}
				break;
			}
			tblList = tblList->next_local;
		}
{code}
"
519,MCOL-23,MCOL,Dipti Joshi,83633,2016-05-24 00:06:21,"[~David.Hall] regarding your comments on working_tpch1_compareLogOnly/onClauseJoins/bug4031.sql,  it is not showing as failure in test001 test report . If this test script is outside of test001 testsuite, let us file a separate Jira item for it.",6,"[~David.Hall] regarding your comments on working_tpch1_compareLogOnly/onClauseJoins/bug4031.sql,  it is not showing as failure in test001 test report . If this test script is outside of test001 testsuite, let us file a separate Jira item for it."
520,MCOL-23,MCOL,Andrew Hutchings,86997,2016-10-03 12:07:25,"test001 as is now working, see MCOL-25 for details.

InnoDB comparison will fail, but test001 doesn't do this.",7,"test001 as is now working, see MCOL-25 for details.

InnoDB comparison will fail, but test001 doesn't do this."
521,MCOL-25,MCOL,Dipti Joshi,85968,2016-08-30 14:55:39,Is this passing now [~hill],1,Is this passing now [~hill]
522,MCOL-25,MCOL,Andrew Hutchings,86825,2016-09-26 16:42:02,"note: a majority of the ref files are bad. They look like they were generated with a different data set. Comparing with a refDB for now, but they should be regenerated after this.",2,"note: a majority of the ref files are bad. They look like they were generated with a different data set. Comparing with a refDB for now, but they should be regenerated after this."
523,MCOL-25,MCOL,Andrew Hutchings,86832,2016-09-26 19:02:03,"All my notes:

{noformat}
Compare failed - working_tpch1/aggregation/group_concat.sql - Concat of long strings appears to be truncated at 512 characters

Ref failed - working_tpch1/misc/bug2954.sql - First query in test now returns an error in MariaDB and Columnstore. Either remove or update ref
Ref failed - working_tpch1/misc/bug3481.sql - Test is columnstore only so fails RefDB
Ref failed - working_tpch1/misc/bug4827.sql - Case sensitivity error in test table name
Compare failed - working_tpch1/misc/bug2584.sql - The following query needs an order by in the test:
select x1.varchar_4, y1.char_8 from table100_char as x1, table10_char as
y1 where x1.varchar_4 = y1.char_8;
Compare failed - working_tpch1/misc/bug2892.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/misc/bug3475.sql - Precision error in MariaDB (ColumnStore actually handles this better)
Compare failed - working_tpch1/misc/bug3881.sql - ColumnStore returns results in different order. I think the queries need fixing

Compare failed - working_tpch1/postprocess/func_3.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/postprocess/order_by_func_bug.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.

Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3333.sql - Test is columnstore only so fails RefDB
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql - Precision error, I think ColumnStore is generating better results than MariaDB
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql - Bug: Random seeding behaves differently to MariaDB for negative seed values
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql - Bug: time(0) doesn't compare to '0' (in quotes). Bug2: compare with '074000' returns wrong result, minutes appear to be truncated.
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql - Bug: reverse() function puts a '+' before the 'e'
Compare failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql - Bug: cast() 0 to date / datetime returns NULL instead of 0 date/datetime
Compare failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql - Bug convert() 0 to date / datetime returns NULL instead of 0 date/datetime
Compare failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql - Precision error
Compare failed - working_tpch1/qa_fe_cnxFunctions/EXTRACT.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql - Bug: datetime to int returns junk. Also: test needs microsecond support
Compare failed - working_tpch1/qa_fe_cnxFunctions/insert.sql - Bug: INSERT() function misbehaves when 'pos' or 'len' parameters are out of range
Compare failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql - Precision error
Compare failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql - Bug: year(0) returns NULL
Compare failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql - Bug: MONTHNAME() to int should equal 0, currently equals month number
Compare failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql - Precision error
Compare failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql - Bug: subtime off by one second for select cdate, subtime(cdate, '66 12:12:12') from datatypetestm;
Compare failed - working_tpch1/qa_fe_cnxFunctions/WEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/WEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/YEARWEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/YEARWEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.

Compare failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql - Precision error
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql - Requires microsecond support
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/WEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/WEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/WEEKOFYEAR.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/YEARWEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/YEARWEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.

Compare failed - working_tpch1/view/view.sql - Bug: bad NULL match when VIEW used in subquery
{noformat}

Bugs to be filed",3,"All my notes:

{noformat}
Compare failed - working_tpch1/aggregation/group_concat.sql - Concat of long strings appears to be truncated at 512 characters

Ref failed - working_tpch1/misc/bug2954.sql - First query in test now returns an error in MariaDB and Columnstore. Either remove or update ref
Ref failed - working_tpch1/misc/bug3481.sql - Test is columnstore only so fails RefDB
Ref failed - working_tpch1/misc/bug4827.sql - Case sensitivity error in test table name
Compare failed - working_tpch1/misc/bug2584.sql - The following query needs an order by in the test:
select x1.varchar_4, y1.char_8 from table100_char as x1, table10_char as
y1 where x1.varchar_4 = y1.char_8;
Compare failed - working_tpch1/misc/bug2892.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/misc/bug3475.sql - Precision error in MariaDB (ColumnStore actually handles this better)
Compare failed - working_tpch1/misc/bug3881.sql - ColumnStore returns results in different order. I think the queries need fixing

Compare failed - working_tpch1/postprocess/func_3.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/postprocess/order_by_func_bug.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.

Ref failed - working_tpch1/qa_fe_cnxFunctions/bug3333.sql - Test is columnstore only so fails RefDB
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql - Precision error, I think ColumnStore is generating better results than MariaDB
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3506.sql - Bug: Random seeding behaves differently to MariaDB for negative seed values
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3584.sql - Bug: time(0) doesn't compare to '0' (in quotes). Bug2: compare with '074000' returns wrong result, minutes appear to be truncated.
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3788.sql - Bug: reverse() function puts a '+' before the 'e'
Compare failed - working_tpch1/qa_fe_cnxFunctions/CAST.sql - Bug: cast() 0 to date / datetime returns NULL instead of 0 date/datetime
Compare failed - working_tpch1/qa_fe_cnxFunctions/concat_ws.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_cnxFunctions/CONVERT.sql - Bug convert() 0 to date / datetime returns NULL instead of 0 date/datetime
Compare failed - working_tpch1/qa_fe_cnxFunctions/degrees.sql - Precision error
Compare failed - working_tpch1/qa_fe_cnxFunctions/EXTRACT.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/from_unixtime.sql - Bug: datetime to int returns junk. Also: test needs microsecond support
Compare failed - working_tpch1/qa_fe_cnxFunctions/insert.sql - Bug: INSERT() function misbehaves when 'pos' or 'len' parameters are out of range
Compare failed - working_tpch1/qa_fe_cnxFunctions/LOG2.NS.sql - Precision error
Compare failed - working_tpch1/qa_fe_cnxFunctions/makedate.sql - Bug: year(0) returns NULL
Compare failed - working_tpch1/qa_fe_cnxFunctions/MONTHNAME.DS.sql - Bug: MONTHNAME() to int should equal 0, currently equals month number
Compare failed - working_tpch1/qa_fe_cnxFunctions/RADIANS.NS.sql - Precision error
Compare failed - working_tpch1/qa_fe_cnxFunctions/repeat.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_cnxFunctions/SUBSTRING_INDEX.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_cnxFunctions/subtime.sql - Bug: subtime off by one second for select cdate, subtime(cdate, '66 12:12:12') from datatypetestm;
Compare failed - working_tpch1/qa_fe_cnxFunctions/WEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/WEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/YEARWEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_cnxFunctions/YEARWEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.

Compare failed - working_tpch1/qa_fe_postProcessedFunctions/CONCAT_WS.SM.sql - MCOL-271
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/LOG2.NS.sql - Precision error
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/SEC_TO_TIME.NS.sql - Requires microsecond support
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/WEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/WEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/WEEKOFYEAR.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/YEARWEEK.DM.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.
Compare failed - working_tpch1/qa_fe_postProcessedFunctions/YEARWEEK.DS.sql - InfiniDB had different week number behaviour to MariaDB. This test will always fail whilst Columnstore retains this behaviour.

Compare failed - working_tpch1/view/view.sql - Bug: bad NULL match when VIEW used in subquery
{noformat}

Bugs to be filed"
524,MCOL-25,MCOL,Andrew Hutchings,86833,2016-09-26 19:03:49,Note that I haven't tried working_tpch1_calpontonly yet as described in the bug description.,4,Note that I haven't tried working_tpch1_calpontonly yet as described in the bug description.
525,MCOL-25,MCOL,Andrew Hutchings,86834,2016-09-26 19:39:16,Update: all of working_tpch1_calpontonly passed.,5,Update: all of working_tpch1_calpontonly passed.
526,MCOL-25,MCOL,Andrew Hutchings,86995,2016-10-03 12:04:07,"Many of the regression issues have been fixed. The remaining issues are to do with the difference between InfiniDB/ColumnStore and InnoDB/Aria. I have fixed the regression suite for ref file issues.

When enabling all the working_tpch1 in test001 the only failing test now is working_tpch1_compareLogOnly/partitionOptimization/bug5848.sql which is covered by MCOL-339. Since that is being handled separately I'm closing this bug.",6,"Many of the regression issues have been fixed. The remaining issues are to do with the difference between InfiniDB/ColumnStore and InnoDB/Aria. I have fixed the regression suite for ref file issues.

When enabling all the working_tpch1 in test001 the only failing test now is working_tpch1_compareLogOnly/partitionOptimization/bug5848.sql which is covered by MCOL-339. Since that is being handled separately I'm closing this bug."
527,MCOL-25,MCOL,Andrew Hutchings,86996,2016-10-03 12:05:04,Chose PrimProc for this as most of the problems were there. See my previous comment for resolution details.,7,Chose PrimProc for this as most of the problems were there. See my previous comment for resolution details.
528,MCOL-251,MCOL,David Hill,84949,2016-07-13 20:29:39,it was agreed that the snmptrap functionality would be moved from columnstore. ,1,it was agreed that the snmptrap functionality would be moved from columnstore. 
529,MCOL-251,MCOL,David Hill,87247,2016-10-10 16:30:31,"Changes being made:

1. remove the issuing of snmptrapds
2. which will removing the luanching of snmptrapd
3. remove the snmptrap prompts in postConfigure
4. change code to issue alarms directly and not have it based on the trap being issued first
5. last but not least, remove all references to the snmp libraries and divorce Columnstore from having snmp libraries as build and install dependencies.

This is actively being worked now in MCOL-251 repo

",2,"Changes being made:

1. remove the issuing of snmptrapds
2. which will removing the luanching of snmptrapd
3. remove the snmptrap prompts in postConfigure
4. change code to issue alarms directly and not have it based on the trap being issued first
5. last but not least, remove all references to the snmp libraries and divorce Columnstore from having snmp libraries as build and install dependencies.

This is actively being worked now in MCOL-251 repo

"
530,MCOL-251,MCOL,David Hill,87265,2016-10-10 23:04:29,"code changes are completed...

1. Issue alarms without snmptraps
2. change code that referenced the term snmpmanager to alarmmanager
3. removed code for snmpmanager and add alarmmanager under oamapps
4. removed the build and install dependencies for net-snmp
5. snmptrapd not being started and prompt removed from postConfigure

ready for review in enegine mcol-251... a lot of files were changed
",3,"code changes are completed...

1. Issue alarms without snmptraps
2. change code that referenced the term snmpmanager to alarmmanager
3. removed code for snmpmanager and add alarmmanager under oamapps
4. removed the build and install dependencies for net-snmp
5. snmptrapd not being started and prompt removed from postConfigure

ready for review in enegine mcol-251... a lot of files were changed
"
531,MCOL-251,MCOL,David Hill,87266,2016-10-10 23:06:06,"How to test:

1. make sure you can build with net-snmp-devel
2. make sure you can install without net-snmp
3. run postConfigure and make sure the snmptrapd part is removed
4. check ma getsystemi and make sure the snmptrapd is no longer there
5. stopsystem and run getsystemi and check that alarms are being issue from the summary",4,"How to test:

1. make sure you can build with net-snmp-devel
2. make sure you can install without net-snmp
3. run postConfigure and make sure the snmptrapd part is removed
4. check ma getsystemi and make sure the snmptrapd is no longer there
5. stopsystem and run getsystemi and check that alarms are being issue from the summary"
532,MCOL-251,MCOL,David Hill,87267,2016-10-10 23:08:09,"Assign to review, let me know if its reviewable based on the knowledge of OAM",5,"Assign to review, let me know if its reviewable based on the knowledge of OAM"
533,MCOL-251,MCOL,Ben Thompson,87304,2016-10-11 18:48:19,"Review done, looks good, confirmed build/install. ",6,"Review done, looks good, confirmed build/install. "
534,MCOL-251,MCOL,David Hill,87785,2016-10-26 13:46:13,"tested, no snmptrap daemon is now loaded and alarms are functional",7,"tested, no snmptrap daemon is now loaded and alarms are functional"
535,MCOL-262,MCOL,Andrew Hutchings,86732,2016-09-22 23:34:44,"Great work!

Sending to Daniel for QA (although maybe David Hill should try it?)",1,"Great work!

Sending to Daniel for QA (although maybe David Hill should try it?)"
536,MCOL-262,MCOL,David Hill,87008,2016-10-03 14:01:02,"Issue with multi-node installs... needs to be reopened and assigned to me..

The platform name was changed that caused this issue

Installer looking for mariadb-columnstore-platform-1.0.4-1-x86_64-centos7.rpm  instead of mariadb-columnstore-1.0.4-1-x86_64-centos7-platform.rpm",2,"Issue with multi-node installs... needs to be reopened and assigned to me..

The platform name was changed that caused this issue

Installer looking for mariadb-columnstore-platform-1.0.4-1-x86_64-centos7.rpm  instead of mariadb-columnstore-1.0.4-1-x86_64-centos7-platform.rpm"
537,MCOL-262,MCOL,David Hill,87009,2016-10-03 14:02:21,failed multi-node installs... failed review and testing,3,failed multi-node installs... failed review and testing
538,MCOL-262,MCOL,David Hill,87081,2016-10-04 17:02:43,"change made to postConfigure and tested successfully from mcol-262 repo, will have Ben review and then will check into develop",4,"change made to postConfigure and tested successfully from mcol-262 repo, will have Ben review and then will check into develop"
539,MCOL-262,MCOL,David Hill,87082,2016-10-04 17:10:02,"please review postConfigure change in mcol-262 engine repo

[builder@ip-172-30-0-125 postConfigure]$ git diff 6d8a177bd1c9aa3ab8314084e26b7c62539d6c4f postConfigure.cpp 
diff --git a/oamapps/postConfigure/postConfigure.cpp b/oamapps/postConfigure/postConfigure.cpp
index 2c5db22..a35e852 100644
--- a/oamapps/postConfigure/postConfigure.cpp
+++ b/oamapps/postConfigure/postConfigure.cpp
@@ -2854,7 +2854,7 @@ int main(int argc, char *argv[])
                                string separator = ""-"";
                                if ( EEPackageType == ""deb"" )
                                        separator = ""_"";
-                               calpontPackage1 = ""mariadb-columnstore-platform"" + separator + version;
+                               calpontPackage1 = ""mariadb-columnstore-*"" + separator + version;
                                calpontPackage2 = ""mariadb-columnstore-libs"" + separator + version;
                                calpontPackage3 = ""mariadb-columnstore-enterprise"" + separator + version;
                                mysqlPackage = ""mariadb-columnstore-storage-engine"" + separator + version;
",5,"please review postConfigure change in mcol-262 engine repo

[builder@ip-172-30-0-125 postConfigure]$ git diff 6d8a177bd1c9aa3ab8314084e26b7c62539d6c4f postConfigure.cpp 
diff --git a/oamapps/postConfigure/postConfigure.cpp b/oamapps/postConfigure/postConfigure.cpp
index 2c5db22..a35e852 100644
--- a/oamapps/postConfigure/postConfigure.cpp
+++ b/oamapps/postConfigure/postConfigure.cpp
@@ -2854,7 +2854,7 @@ int main(int argc, char *argv[])
                                string separator = ""-"";
                                if ( EEPackageType == ""deb"" )
                                        separator = ""_"";
-                               calpontPackage1 = ""mariadb-columnstore-platform"" + separator + version;
+                               calpontPackage1 = ""mariadb-columnstore-*"" + separator + version;
                                calpontPackage2 = ""mariadb-columnstore-libs"" + separator + version;
                                calpontPackage3 = ""mariadb-columnstore-enterprise"" + separator + version;
                                mysqlPackage = ""mariadb-columnstore-storage-engine"" + separator + version;
"
540,MCOL-262,MCOL,David Hill,87084,2016-10-04 19:24:03,"merged mcol-262 to develop

how to test:

perform a multi-node install using postConfigure and make sure it locates the packages correctly in the /root directly before it runs the script to push to the other servers.

",6,"merged mcol-262 to develop

how to test:

perform a multi-node install using postConfigure and make sure it locates the packages correctly in the /root directly before it runs the script to push to the other servers.

"
541,MCOL-262,MCOL,Daniel Lee,87831,2016-10-27 17:00:31,"Build verified: 1.0.4-1 beta

Name        : mariadb-columnstore-platform
Version     : 1.0.4
Release     : 1
Architecture: x86_64
Install Date: Thu 27 Oct 2016 09:02:05 AM CDT
Group       : Applications/Databases
Size        : 9926755
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.4-1.src.rpm
Build Date  : Tue 25 Oct 2016 12:06:52 PM CDT
Build Host  : centos7


Passed both manual and automated installation tests, as well as tests on AWS.
",7,"Build verified: 1.0.4-1 beta

Name        : mariadb-columnstore-platform
Version     : 1.0.4
Release     : 1
Architecture: x86_64
Install Date: Thu 27 Oct 2016 09:02:05 AM CDT
Group       : Applications/Databases
Size        : 9926755
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.4-1.src.rpm
Build Date  : Tue 25 Oct 2016 12:06:52 PM CDT
Build Host  : centos7


Passed both manual and automated installation tests, as well as tests on AWS.
"
542,MCOL-265,MCOL,Andrew Hutchings,126820,2019-04-25 10:20:28,"External contribution, Roman to final review",1,"External contribution, Roman to final review"
543,MCOL-265,MCOL,Andrew Hutchings,127255,2019-05-02 09:53:21,Contribution may need some changes. TBD.,2,Contribution may need some changes. TBD.
544,MCOL-265,MCOL,Bharath Bokka,133941,2019-09-11 12:05:49,"TIMESTAMP data type works as expected in 1.4.0-1.

Build-
server commit:
67452bc
engine commit:
4d2a159

Ex:
MariaDB [test]> create table t3(col1 timestamp) engine=columnstore;
Query OK, 0 rows affected (0.548 sec)

MariaDB [test]>
MariaDB [test]> show create table t3;
+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                                                                          |
+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
| t3    | CREATE TABLE `t3` (
  `col1` timestamp NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp()
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> insert into t3 values();
Query OK, 1 row affected (0.215 sec)

MariaDB [test]>
MariaDB [test]> insert into t3 values('2038-01-19 03:14:08');
Query OK, 1 row affected, 1 warning (0.143 sec)

MariaDB [test]> insert into t3 values('2038-01-19 03:14:07');
Query OK, 1 row affected (0.114 sec)

MariaDB [test]> insert into t3 values('1970-01-01 00:00:01');
Query OK, 1 row affected (0.135 sec)

MariaDB [test]> insert into t3 values('1970-01-01 00:00:00');
Query OK, 1 row affected, 1 warning (0.106 sec)

MariaDB [test]> insert into t3 values(now());
Query OK, 1 row affected (0.158 sec)

MariaDB [test]> insert into t3 values(current_timestamp());
Query OK, 1 row affected (0.184 sec)

MariaDB [test]> select * from t3;
+---------------------+
| col1                |
+---------------------+
| 2019-09-09 10:57:02 |
| 0000-00-00 00:00:00 |
| 2038-01-19 03:14:07 |
| 1970-01-01 00:00:01 |
| 0000-00-00 00:00:00 |
| 2019-09-09 11:11:47 |
| 2019-09-09 11:12:02 |
+---------------------+
7 rows in set (0.104 sec)

MariaDB [test]> select * from t3 where col1 > '2019-09-09 10:57:02';
+---------------------+
| col1                |
+---------------------+
| 2038-01-19 03:14:07 |
| 2019-09-09 11:11:47 |
| 2019-09-09 11:12:02 |
+---------------------+
3 rows in set (0.013 sec)

MariaDB [test]> select * from t3 where col1 < '2019-09-09 10:57:02';
+---------------------+
| col1                |
+---------------------+
| 0000-00-00 00:00:00 |
| 1970-01-01 00:00:01 |
| 0000-00-00 00:00:00 |
+---------------------+
",3,"TIMESTAMP data type works as expected in 1.4.0-1.

Build-
server commit:
67452bc
engine commit:
4d2a159

Ex:
MariaDB [test]> create table t3(col1 timestamp) engine=columnstore;
Query OK, 0 rows affected (0.548 sec)

MariaDB [test]>
MariaDB [test]> show create table t3;
+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                                                                          |
+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
| t3    | CREATE TABLE `t3` (
  `col1` timestamp NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp()
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> insert into t3 values();
Query OK, 1 row affected (0.215 sec)

MariaDB [test]>
MariaDB [test]> insert into t3 values('2038-01-19 03:14:08');
Query OK, 1 row affected, 1 warning (0.143 sec)

MariaDB [test]> insert into t3 values('2038-01-19 03:14:07');
Query OK, 1 row affected (0.114 sec)

MariaDB [test]> insert into t3 values('1970-01-01 00:00:01');
Query OK, 1 row affected (0.135 sec)

MariaDB [test]> insert into t3 values('1970-01-01 00:00:00');
Query OK, 1 row affected, 1 warning (0.106 sec)

MariaDB [test]> insert into t3 values(now());
Query OK, 1 row affected (0.158 sec)

MariaDB [test]> insert into t3 values(current_timestamp());
Query OK, 1 row affected (0.184 sec)

MariaDB [test]> select * from t3;
+---------------------+
| col1                |
+---------------------+
| 2019-09-09 10:57:02 |
| 0000-00-00 00:00:00 |
| 2038-01-19 03:14:07 |
| 1970-01-01 00:00:01 |
| 0000-00-00 00:00:00 |
| 2019-09-09 11:11:47 |
| 2019-09-09 11:12:02 |
+---------------------+
7 rows in set (0.104 sec)

MariaDB [test]> select * from t3 where col1 > '2019-09-09 10:57:02';
+---------------------+
| col1                |
+---------------------+
| 2038-01-19 03:14:07 |
| 2019-09-09 11:11:47 |
| 2019-09-09 11:12:02 |
+---------------------+
3 rows in set (0.013 sec)

MariaDB [test]> select * from t3 where col1 < '2019-09-09 10:57:02';
+---------------------+
| col1                |
+---------------------+
| 0000-00-00 00:00:00 |
| 1970-01-01 00:00:01 |
| 0000-00-00 00:00:00 |
+---------------------+
"
545,MCOL-266,MCOL,Andrew Hutchings,117523,2018-10-08 06:48:42,Reviewing external contribution from tntnatbry,1,Reviewing external contribution from tntnatbry
546,MCOL-266,MCOL,Daniel Lee,117561,2018-10-08 21:15:28,"Build tested: 1.2 source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 6ee12248b8764569ca7e965b5cc35746b25d816c
Merge: 068b168 47fbf62
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Mon Oct 8 11:22:42 2018 +0300

    Merge pull request #586 from mariadb-corporation/MCOL-1775
    
    MCOL-1775 Fix addtime/subtime for WHERE

Found couple issues with alter table:

Add column does not support using true or false as default value.

MariaDB [mytest]> alter table t1 add column c10 boolean default true;
ERROR 1815 (HY000): Internal error: The default value is out of range for the specified data type.

Change column does not recognize boolean as data type

MariaDB [mytest]> alter table t1 change column c1 c10 boolean;
ERROR 1815 (HY000): Internal error: CAL0001: Alter table Failed:  Changing the datatype of a column is not supported  

this one worked with integer.

MariaDB [mytest]> alter table t1 change column c1 c10 integer;
Query OK, 0 rows affected (0.115 sec)              
Records: 0  Duplicates: 0  Warnings: 0



",2,"Build tested: 1.2 source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 6ee12248b8764569ca7e965b5cc35746b25d816c
Merge: 068b168 47fbf62
Author: Roman Nozdrin 
Date:   Mon Oct 8 11:22:42 2018 +0300

    Merge pull request #586 from mariadb-corporation/MCOL-1775
    
    MCOL-1775 Fix addtime/subtime for WHERE

Found couple issues with alter table:

Add column does not support using true or false as default value.

MariaDB [mytest]> alter table t1 add column c10 boolean default true;
ERROR 1815 (HY000): Internal error: The default value is out of range for the specified data type.

Change column does not recognize boolean as data type

MariaDB [mytest]> alter table t1 change column c1 c10 boolean;
ERROR 1815 (HY000): Internal error: CAL0001: Alter table Failed:  Changing the datatype of a column is not supported  

this one worked with integer.

MariaDB [mytest]> alter table t1 change column c1 c10 integer;
Query OK, 0 rows affected (0.115 sec)              
Records: 0  Duplicates: 0  Warnings: 0



"
547,MCOL-266,MCOL,Daniel Lee,117562,2018-10-08 21:31:53,"The following areas have been tested:

create table, with or without default


alter table

	add column
		when adding boolean column, it takes 0 or 1 as default value. quoted or not
***		It does not take true or false, quoted or not

MariaDB [mytest]> alter table t1 add column c10 boolean default true;
ERROR 1815 (HY000): Internal error: The default value is out of range for the specified data type.

MariaDB [mytest]> alter table t1 change column c1 c10 boolean;
ERROR 1815 (HY000): Internal error: CAL0001: Alter table Failed:  Changing the datatype of a column is not supported  


	drop column

colxml
	boolean has the same definition as tinyint

editem

dml

	insert with true , false, 1, 0
	insert positive and negative value saturation

	update with true, false, 1, 0.
	update to another numeric column
	update from an integer column
	update from an integer columns with out of range values, saturation occurred
	update with characters, failed as expected
	update from a varchar column with characters, failed as expected
	update to a varchar column
	update integer columns * (-1)
	update positive and negative value saturation

query
	where clause
		worked with true, false, 1, 0, is null, is not null, in where clause
	
	join
		column = column, worked just like a numeric column

truncate

cpimport
	It takes numeric values only, over range value saturation ok
***	TRUE, FALSE, true, false, True also saturated to 0
*** NULL handling is incorrect


",3,"The following areas have been tested:

create table, with or without default


alter table

	add column
		when adding boolean column, it takes 0 or 1 as default value. quoted or not
***		It does not take true or false, quoted or not

MariaDB [mytest]> alter table t1 add column c10 boolean default true;
ERROR 1815 (HY000): Internal error: The default value is out of range for the specified data type.

MariaDB [mytest]> alter table t1 change column c1 c10 boolean;
ERROR 1815 (HY000): Internal error: CAL0001: Alter table Failed:  Changing the datatype of a column is not supported  


	drop column

colxml
	boolean has the same definition as tinyint

editem

dml

	insert with true , false, 1, 0
	insert positive and negative value saturation

	update with true, false, 1, 0.
	update to another numeric column
	update from an integer column
	update from an integer columns with out of range values, saturation occurred
	update with characters, failed as expected
	update from a varchar column with characters, failed as expected
	update to a varchar column
	update integer columns * (-1)
	update positive and negative value saturation

query
	where clause
		worked with true, false, 1, 0, is null, is not null, in where clause
	
	join
		column = column, worked just like a numeric column

truncate

cpimport
	It takes numeric values only, over range value saturation ok
***	TRUE, FALSE, true, false, True also saturated to 0
*** NULL handling is incorrect


"
548,MCOL-266,MCOL,Andrew Hutchings,117601,2018-10-09 13:19:16,"Modified the code to allow true/false as default values.

We can only support change of matching data type and digits. So you could change tinyint(1) to Boolean and back again. But not any other int type (including tinyint's default digits of tinyint(4)).",4,"Modified the code to allow true/false as default values.

We can only support change of matching data type and digits. So you could change tinyint(1) to Boolean and back again. But not any other int type (including tinyint's default digits of tinyint(4))."
549,MCOL-266,MCOL,Daniel Lee,117768,2018-10-11 16:29:17,"Build verified: 1.2 source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 39c283281af045e5b5fb3fe3f399b21a6b1236ca
Merge: 46775f8 19c8a2b
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Wed Oct 10 20:11:12 2018 +0300

    Merge pull request #588 from mariadb-corporation/MCOL-266
    
    MCOL-266 Support true/false DDL default values

The alter table add columns with default value issue has been fixed.",5,"Build verified: 1.2 source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 39c283281af045e5b5fb3fe3f399b21a6b1236ca
Merge: 46775f8 19c8a2b
Author: Roman Nozdrin 
Date:   Wed Oct 10 20:11:12 2018 +0300

    Merge pull request #588 from mariadb-corporation/MCOL-266
    
    MCOL-266 Support true/false DDL default values

The alter table add columns with default value issue has been fixed."
550,MCOL-267,MCOL,Jose Cavieres Soto,90853,2017-01-20 13:45:24,"I wouldn't consider this a minor issue.
Not being able to use text data type leads to a reduction in the number of columns of the table to a few hundreds from the theoretical 4096 . In OLAP that's not a small problem",1,"I wouldn't consider this a minor issue.
Not being able to use text data type leads to a reduction in the number of columns of the table to a few hundreds from the theoretical 4096 . In OLAP that's not a small problem"
551,MCOL-267,MCOL,Andrew Hutchings,90855,2017-01-20 14:21:18,At the time of issue creation the priority was minor as we were prioritising critical problems for the 1.0 GA release. Now that we have started work on further GA releases we should re-evaluate this.,2,At the time of issue creation the priority was minor as we were prioritising critical problems for the 1.0 GA release. Now that we have started work on further GA releases we should re-evaluate this.
552,MCOL-267,MCOL,Andrew Hutchings,93389,2017-03-23 15:39:19,"Pull request open for feature and tests

BLOB/TEXT is implemented using a multi-block dictionary, eliminating the need for the 8KB limit. Also allows BLOB/TEXT columns to be used in cross engine joins.

Release notes:

* LONGBLOB/LONGTEXT is capped at 2,100,000,000 bytes
* LDI and INSERT...SELECT limited to 1MB row length so max 512KB blob (halved due to hex conversion) - we should open a new ticket for this to be configurable as a sysvar after this and 10.2 is merged.
* cpimport needs BLOB/TEXT to be in hex form (like VARBINARY)
* you will likely need to use '-c' to increase the buffer size with cpimport",3,"Pull request open for feature and tests

BLOB/TEXT is implemented using a multi-block dictionary, eliminating the need for the 8KB limit. Also allows BLOB/TEXT columns to be used in cross engine joins.

Release notes:

* LONGBLOB/LONGTEXT is capped at 2,100,000,000 bytes
* LDI and INSERT...SELECT limited to 1MB row length so max 512KB blob (halved due to hex conversion) - we should open a new ticket for this to be configurable as a sysvar after this and 10.2 is merged.
* cpimport needs BLOB/TEXT to be in hex form (like VARBINARY)
* you will likely need to use '-c' to increase the buffer size with cpimport"
553,MCOL-267,MCOL,Andrew Hutchings,93791,2017-04-04 14:25:31,"Additional:
* We don't support joins on BLOB/TEXT columns",4,"Additional:
* We don't support joins on BLOB/TEXT columns"
554,MCOL-267,MCOL,Daniel Lee,94817,2017-05-05 21:14:49,"Build verified: Github source
[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Thu May 4 16:06:16 2017 -0500
Merge pull request #50 from mariadb-corporation/10.2-fixes
10.2 fixes
[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 9ef603c14aa3cb8e9fe7f21c9965f6cf3f0a11d8
Merge: 19342ef 82c983e
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date: Fri May 5 13:09:34 2017 -0500
Merge pull request #169 from mariadb-corporation/MCOL-701
MCOL-701 stop join on BLOB columns

I have been testing this feature for a while now.  Since the feature have been implemented and issues found are being tracked by separate tickets, I am closing this ticket.",5,"Build verified: Github source
[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 
Date: Thu May 4 16:06:16 2017 -0500
Merge pull request #50 from mariadb-corporation/10.2-fixes
10.2 fixes
[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 9ef603c14aa3cb8e9fe7f21c9965f6cf3f0a11d8
Merge: 19342ef 82c983e
Author: dhall-InfiniDB 
Date: Fri May 5 13:09:34 2017 -0500
Merge pull request #169 from mariadb-corporation/MCOL-701
MCOL-701 stop join on BLOB columns

I have been testing this feature for a while now.  Since the feature have been implemented and issues found are being tracked by separate tickets, I am closing this ticket."
555,MCOL-270,MCOL,Andrew Hutchings,121766,2019-01-15 09:07:27,"Community contribution merged. We need tests written in the regression suite for this. [~elena.kotsinova], [~dleeyh] who should be assigned this?",1,"Community contribution merged. We need tests written in the regression suite for this. [~elena.kotsinova], [~dleeyh] who should be assigned this?"
556,MCOL-270,MCOL,Bharath Bokka,134008,2019-09-12 06:45:24,"MEDIUMINT data type is now supported on 1.4.0-1.

Git version info-
server commit:
67452bc
engine commit:
4d2a159

Following are the examples of tests on MEDIUMINT datatype:
MariaDB [mytest]> create table t2 (col1 mediumint) ENGINE=columnstore;
Query OK, 0 rows affected (0.646 sec)

MariaDB [mytest]> show create table t2 ;
+-------+----------------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                       |
+-------+----------------------------------------------------------------------------------------------------+
| t2    | CREATE TABLE `t2` (
  `col1` mediumint(9) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+----------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [mytest]>
MariaDB [mytest]> insert into t2 values (0);
Query OK, 1 row affected (0.180 sec)

MariaDB [mytest]> insert into t2 values (16777215);
Query OK, 1 row affected, 1 warning (0.091 sec)

MariaDB [test]> insert into t2 values(-1);
Query OK, 1 row affected, 1 warning (0.165 sec)

MariaDB [mytest]> insert into t2 values (-8388609);
Query OK, 1 row affected, 1 warning (0.159 sec)

MariaDB [mytest]> select * from  t2;
+----------+
| col1 |
+----------+
|        0 |
|  8388607 |
| -8388608 |
|       -1 |
+----------+
3 rows in set (0.012 sec)

MariaDB [test]> create table t(col1 mediumint unsigned) engine=columnstore;
Query OK, 0 rows affected (0.418 sec)

MariaDB [test]> show create table t;
+-------+------------------------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                               |
+-------+------------------------------------------------------------------------------------------------------------+
| t     | CREATE TABLE `t` (
  `col1` mediumint(8) unsigned DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> insert into t values();
Query OK, 1 row affected (0.216 sec)

MariaDB [test]> insert into t values('');
Query OK, 1 row affected, 1 warning (0.125 sec)

MariaDB [test]> insert into t values(16777215);
Query OK, 1 row affected (0.186 sec)

MariaDB [test]> insert into t values(16777216);
Query OK, 1 row affected, 1 warning (0.152 sec)

MariaDB [test]> insert into t values(-1);
Query OK, 1 row affected, 1 warning (0.165 sec)

MariaDB [test]> insert into t values(0);
Query OK, 1 row affected (0.170 sec)

MariaDB [test]> insert into t values(1);
Query OK, 1 row affected (0.095 sec)

MariaDB [test]> select * from t;
+----------+
| col1     |
+----------+
|     NULL |
|        0 |
| 16777215 |
| 16777215 |
|        0 |
|        0 |
|        1 |
+----------+
7 rows in set (0.108 sec)
MariaDB [test]>  select count(*) from t where col1= 0;
+----------+
| count(*) |
+----------+
|        3 |
+----------+
1 row in set (0.014 sec)

MariaDB [test]> select avg(col1) from t;
+--------------+
| avg(col1)    |
+--------------+
| 5592405.1667 |
+--------------+
1 row in set (0.031 sec)

MariaDB [test]> select * from t2 where col1<0 order by col1 desc;
+----------+
| col1     |
+----------+
|       -1 |
| -8388608 |
+----------+
2 rows in set (0.015 sec)

The datatype works as expected.",2,"MEDIUMINT data type is now supported on 1.4.0-1.

Git version info-
server commit:
67452bc
engine commit:
4d2a159

Following are the examples of tests on MEDIUMINT datatype:
MariaDB [mytest]> create table t2 (col1 mediumint) ENGINE=columnstore;
Query OK, 0 rows affected (0.646 sec)

MariaDB [mytest]> show create table t2 ;
+-------+----------------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                       |
+-------+----------------------------------------------------------------------------------------------------+
| t2    | CREATE TABLE `t2` (
  `col1` mediumint(9) DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+----------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [mytest]>
MariaDB [mytest]> insert into t2 values (0);
Query OK, 1 row affected (0.180 sec)

MariaDB [mytest]> insert into t2 values (16777215);
Query OK, 1 row affected, 1 warning (0.091 sec)

MariaDB [test]> insert into t2 values(-1);
Query OK, 1 row affected, 1 warning (0.165 sec)

MariaDB [mytest]> insert into t2 values (-8388609);
Query OK, 1 row affected, 1 warning (0.159 sec)

MariaDB [mytest]> select * from  t2;
+----------+
| col1 |
+----------+
|        0 |
|  8388607 |
| -8388608 |
|       -1 |
+----------+
3 rows in set (0.012 sec)

MariaDB [test]> create table t(col1 mediumint unsigned) engine=columnstore;
Query OK, 0 rows affected (0.418 sec)

MariaDB [test]> show create table t;
+-------+------------------------------------------------------------------------------------------------------------+
| Table | Create Table                                                                                               |
+-------+------------------------------------------------------------------------------------------------------------+
| t     | CREATE TABLE `t` (
  `col1` mediumint(8) unsigned DEFAULT NULL
) ENGINE=Columnstore DEFAULT CHARSET=latin1 |
+-------+------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> insert into t values();
Query OK, 1 row affected (0.216 sec)

MariaDB [test]> insert into t values('');
Query OK, 1 row affected, 1 warning (0.125 sec)

MariaDB [test]> insert into t values(16777215);
Query OK, 1 row affected (0.186 sec)

MariaDB [test]> insert into t values(16777216);
Query OK, 1 row affected, 1 warning (0.152 sec)

MariaDB [test]> insert into t values(-1);
Query OK, 1 row affected, 1 warning (0.165 sec)

MariaDB [test]> insert into t values(0);
Query OK, 1 row affected (0.170 sec)

MariaDB [test]> insert into t values(1);
Query OK, 1 row affected (0.095 sec)

MariaDB [test]> select * from t;
+----------+
| col1     |
+----------+
|     NULL |
|        0 |
| 16777215 |
| 16777215 |
|        0 |
|        0 |
|        1 |
+----------+
7 rows in set (0.108 sec)
MariaDB [test]>  select count(*) from t where col1= 0;
+----------+
| count(*) |
+----------+
|        3 |
+----------+
1 row in set (0.014 sec)

MariaDB [test]> select avg(col1) from t;
+--------------+
| avg(col1)    |
+--------------+
| 5592405.1667 |
+--------------+
1 row in set (0.031 sec)

MariaDB [test]> select * from t2 where col1<0 order by col1 desc;
+----------+
| col1     |
+----------+
|       -1 |
| -8388608 |
+----------+
2 rows in set (0.015 sec)

The datatype works as expected."
557,MCOL-271,MCOL,Andrew Hutchings,85686,2016-08-19 13:05:08,"Problem is in WE_DMLCommandProc::processSingleInsert:

{code:c}
origVals = columnPtr->get_DataVector();

...

tmpStr = origVals[i];
if ( tmpStr.length() == 0 )
        isNULL = true;
else
        isNULL = false;
{code}

So an empty string would be set to NULL.",1,"Problem is in WE_DMLCommandProc::processSingleInsert:

{code:c}
origVals = columnPtr->get_DataVector();

...

tmpStr = origVals[i];
if ( tmpStr.length() == 0 )
        isNULL = true;
else
        isNULL = false;
{code}

So an empty string would be set to NULL."
558,MCOL-271,MCOL,Andrew Hutchings,85699,2016-08-19 20:45:36,Written a 16KB patch so far on this basically implementing NULL support at every stage throughout a DML query. Still some work to do in the WriteEngine area and possible data reading to differentiate between empty strings and NULL data.,2,Written a 16KB patch so far on this basically implementing NULL support at every stage throughout a DML query. Still some work to do in the WriteEngine area and possible data reading to differentiate between empty strings and NULL data.
559,MCOL-271,MCOL,Justin Swanhart,85701,2016-08-19 21:05:23,"I believe cpimport also has code that treats empty string as NULL, and perhaps the bridge between cpimport and the bulk loading interface too.",3,"I believe cpimport also has code that treats empty string as NULL, and perhaps the bridge between cpimport and the bulk loading interface too."
560,MCOL-271,MCOL,Andrew Hutchings,85827,2016-08-24 19:56:55,"Progress so far in this patch: https://github.com/LinuxJedi/mariadb-columnstore-engine/commit/0434612514f851e459595924c428af30f7914118

MCOL-171 is fixed

MCOL-271 is mostly fixed. IS NULL/NOT NULL comparisons aren't yet working due to some issue in PrimProc that I haven't managed to fully diagnose yet.",4,"Progress so far in this patch: URL

MCOL-171 is fixed

MCOL-271 is mostly fixed. IS NULL/NOT NULL comparisons aren't yet working due to some issue in PrimProc that I haven't managed to fully diagnose yet."
561,MCOL-271,MCOL,Andrew Hutchings,85943,2016-08-30 09:28:55,On hold for now. I have to make heavy changes to the write engine to make this work.,5,On hold for now. I have to make heavy changes to the write engine to make this work.
562,MCOL-271,MCOL,Andrew Hutchings,88183,2016-11-08 11:37:06,I now think this should be fixed as part of MCOL-92. If we have a separate container for NULL entries in columns (and maybe other potential bitmasks?) the problem with int ranges goes away and it can be used with character type columns too.,6,I now think this should be fixed as part of MCOL-92. If we have a separate container for NULL entries in columns (and maybe other potential bitmasks?) the problem with int ranges goes away and it can be used with character type columns too.
563,MCOL-271,MCOL,Sergey Zefirov,216204,2022-03-04 17:37:56,"I think that, not having special NULL bitmap as a part of column, it is possible to use something akin to ```boost::option<string>``` (not actual boost type, but something alike) where empty value represents NULL and one has to obtain a reference to string contained by checking the non-emptiness.

Type checking can be leveraged to find out all places where this replacement has to be done.

I'll discuss that with Roman and, probably, Gagan further. Most probably, Gagan will help with advices on how he needs it to be done in his work.",7,"I think that, not having special NULL bitmap as a part of column, it is possible to use something akin to ```boost::option``` (not actual boost type, but something alike) where empty value represents NULL and one has to obtain a reference to string contained by checking the non-emptiness.

Type checking can be leveraged to find out all places where this replacement has to be done.

I'll discuss that with Roman and, probably, Gagan further. Most probably, Gagan will help with advices on how he needs it to be done in his work."
564,MCOL-271,MCOL,Sergey Zefirov,216234,2022-03-05 10:40:35,Relevant branch: https://github.com/mariadb-SergeyZefirov/mariadb-columnstore-engine/tree/MCOL-271-empty-strings-should-not-be-nulls,8,Relevant branch: URL
565,MCOL-271,MCOL,Sergey Zefirov,219515,2022-04-07 13:31:38,"[Here are some more NULL values|https://github.com/mariadb-SergeyZefirov/mariadb-columnstore-engine/blob/develop/writeengine/server/we_dmlcommandproc.cpp#L3569]:

{code:c++}
        if (((colType.colDataType == execplan::CalpontSystemCatalog::DATE) && (inData == ""0000-00-00"")) ||
            ((colType.colDataType == execplan::CalpontSystemCatalog::DATETIME) &&
             (inData == ""0000-00-00 00:00:00"")) ||
            ((colType.colDataType == execplan::CalpontSystemCatalog::TIMESTAMP) &&
             (inData == ""0000-00-00 00:00:00"")))
        {
          isNull = true;
        }
{code}

(writeengine/server/we_dmlcommandproc.cpp)

I haven't  found anything else that is also implicitly converted to NULL.",9,"[Here are some more NULL values|URL

{code:c++}
        if (((colType.colDataType == execplan::CalpontSystemCatalog::DATE) && (inData == ""0000-00-00"")) ||
            ((colType.colDataType == execplan::CalpontSystemCatalog::DATETIME) &&
             (inData == ""0000-00-00 00:00:00"")) ||
            ((colType.colDataType == execplan::CalpontSystemCatalog::TIMESTAMP) &&
             (inData == ""0000-00-00 00:00:00"")))
        {
          isNull = true;
        }
{code}

(writeengine/server/we_dmlcommandproc.cpp)

I haven't  found anything else that is also implicitly converted to NULL."
566,MCOL-271,MCOL,alexey vorovich,224950,2022-05-26 14:21:08,"[~sergey.zefirov] [~drrtuy] I want to share some related experience we had in IBI in the area. 
We also started with having '.' string treated as NULL.

 It was  major project to support the correct approach . We broke it into 2 major stages:

1. Correct the ""object orientation"" by removing direct access to control blocks and encapsulating access into functions
2. change the actual function from using  '.' to using a special bit

Of course stage 1 was 99% of the work and we kept merging into main branch without changing overall desired behavior. This allowed immediate daily testing by QA and the rest of team. 

once stage 1 was done we did an additional step of having a system-wide setting that was used in each of the encapsulated functions, that allowed to switch back/forth on the NULL implementation.

we tested with old approach  being default for  some time before changing the default to new.

This may or may not be relevant to us here . 

av


 

",10,"[~sergey.zefirov] [~drrtuy] I want to share some related experience we had in IBI in the area. 
We also started with having '.' string treated as NULL.

 It was  major project to support the correct approach . We broke it into 2 major stages:

1. Correct the ""object orientation"" by removing direct access to control blocks and encapsulating access into functions
2. change the actual function from using  '.' to using a special bit

Of course stage 1 was 99% of the work and we kept merging into main branch without changing overall desired behavior. This allowed immediate daily testing by QA and the rest of team. 

once stage 1 was done we did an additional step of having a system-wide setting that was used in each of the encapsulated functions, that allowed to switch back/forth on the NULL implementation.

we tested with old approach  being default for  some time before changing the default to new.

This may or may not be relevant to us here . 

av


 

"
567,MCOL-271,MCOL,Sergey Zefirov,225653,2022-06-02 15:59:36,"Tests that fail to load data:

# mcs103_ldi_fields_enclosed_by
# mcs102_lds_transform_csv
# mcs28_load_data_local_infile
# mcs29_load_data_local_infile_negative
# unsigned_joins

There are 23 tests that call test01_distinct procedure from ctype_cmp_create.inc. Some of them pass, but majority fail. Judging from the output, the problem is in use of GROUP_CONCAT in the test01_distinct procedure. There are 6 other tests that use GROUP_CONCAT and also fail.

I see no other ""hot spots"" here.",11,"Tests that fail to load data:

# mcs103_ldi_fields_enclosed_by
# mcs102_lds_transform_csv
# mcs28_load_data_local_infile
# mcs29_load_data_local_infile_negative
# unsigned_joins

There are 23 tests that call test01_distinct procedure from ctype_cmp_create.inc. Some of them pass, but majority fail. Judging from the output, the problem is in use of GROUP_CONCAT in the test01_distinct procedure. There are 6 other tests that use GROUP_CONCAT and also fail.

I see no other ""hot spots"" here."
568,MCOL-271,MCOL,alexey vorovich,230554,2022-07-26 14:05:41,"[~sergey.zefirov]
You say ""only new are used"" . Should we remove unused code ?",12,"[~sergey.zefirov]
You say ""only new are used"" . Should we remove unused code ?"
569,MCOL-271,MCOL,Sergey Zefirov,244967,2022-12-09 12:39:18,"No new problems observed.

The list to fix:

{noformat}
Working Folder Test scripts that failed:
Compare failed - working_ssb_compareLogOnly/ssb_dim_mixed_outer_2colgrp/ssb_dim_mixed_outer_2colgrp3.sql
Compare failed - working_tpch1/aggregation/bitop.sql
{noformat}",13,"No new problems observed.

The list to fix:

{noformat}
Working Folder Test scripts that failed:
Compare failed - working_ssb_compareLogOnly/ssb_dim_mixed_outer_2colgrp/ssb_dim_mixed_outer_2colgrp3.sql
Compare failed - working_tpch1/aggregation/bitop.sql
{noformat}"
570,MCOL-271,MCOL,Daniel Lee,257151,2023-04-25 15:57:01,"Build verified:

engine: 1e56a0b557efb677d07533d05eb02ad723955317
server: 11c83d9ae9eb249d00589cc6ab71e7f4e67ffa27
buildNo: 7534

Testing the following areas:

INSERT
UPDATE
LDI, not using cpimport
LDI, using cpimport
cpimport, with and without -n
queries, with IS NULL and IS NOT NULL
subquery, with IS NULL and IS NOT NULL
sql_mode, with and without EMPTY_STRING_IS_NULL;
NULL values and '' (empty) strings
ETC

There is a MTR performance issue for the develop branch as of 04/24 night.  According to the developer, MTR tests passed on the MCOL-271 commit.

Two tickets have been opened. 
MCOL-5480 LDI loads values incorrectly for MEDIUMINT, TIME and TIMESTAMP when cpimport is used for batch insert
The issue is not caused by this fix.  It also exists in release 23.02.2.  For completeness, we should fix this issue, as part of MCOL-271.

MCOL-5483 For UPDATE operation, row count for 'Changed: ' returned is incorrect
Not release to this fix.
",14,"Build verified:

engine: 1e56a0b557efb677d07533d05eb02ad723955317
server: 11c83d9ae9eb249d00589cc6ab71e7f4e67ffa27
buildNo: 7534

Testing the following areas:

INSERT
UPDATE
LDI, not using cpimport
LDI, using cpimport
cpimport, with and without -n
queries, with IS NULL and IS NOT NULL
subquery, with IS NULL and IS NOT NULL
sql_mode, with and without EMPTY_STRING_IS_NULL;
NULL values and '' (empty) strings
ETC

There is a MTR performance issue for the develop branch as of 04/24 night.  According to the developer, MTR tests passed on the MCOL-271 commit.

Two tickets have been opened. 
MCOL-5480 LDI loads values incorrectly for MEDIUMINT, TIME and TIMESTAMP when cpimport is used for batch insert
The issue is not caused by this fix.  It also exists in release 23.02.2.  For completeness, we should fix this issue, as part of MCOL-271.

MCOL-5483 For UPDATE operation, row count for 'Changed: ' returned is incorrect
Not release to this fix.
"
571,MCOL-272,MCOL,Andrew Hutchings,88333,2016-11-15 11:17:32,"Added what I have done so far. 'cpack_deb.cmake' is for the server tree, 'cpackEngineDEB.cmake' is for the engine tree. Don't forget to use INCLUDE() on them both in the CMakeLists.txt file.

Problems I have so far:

* OBSOLETES/CONFLICTS/etc... isn't working, don't know why
* post install and similar scripts are done very differently. There is also a bug for this in cpack (with a patch that never made the tree) which stops it working for component installs

From my point of view the second one really isn't an issue as long as we document post-install needs to be run. The first problem could be an issue.

I'm re-assigning to Ben so I can juggle other things for the next release.",1,"Added what I have done so far. 'cpack_deb.cmake' is for the server tree, 'cpackEngineDEB.cmake' is for the engine tree. Don't forget to use INCLUDE() on them both in the CMakeLists.txt file.

Problems I have so far:

* OBSOLETES/CONFLICTS/etc... isn't working, don't know why
* post install and similar scripts are done very differently. There is also a bug for this in cpack (with a patch that never made the tree) which stops it working for component installs

From my point of view the second one really isn't an issue as long as we document post-install needs to be run. The first problem could be an issue.

I'm re-assigning to Ben so I can juggle other things for the next release."
572,MCOL-272,MCOL,David Hill,88347,2016-11-15 14:45:41,this should also include a debian 8...,2,this should also include a debian 8...
573,MCOL-272,MCOL,Andrew Hutchings,88352,2016-11-15 16:32:13,the attached files work on Debian as well as Ubuntu. I suspect the final version would too without any modification.,3,the attached files work on Debian as well as Ubuntu. I suspect the final version would too without any modification.
574,MCOL-272,MCOL,Andrew Hutchings,89322,2016-12-07 17:38:52,Only minor issue is the server test package is not installable. But it also isn't required.,4,Only minor issue is the server test package is not installable. But it also isn't required.
575,MCOL-272,MCOL,Daniel Lee,89352,2016-12-08 16:10:22,"Build verified: 1.0.6-1

Package file has been created.",5,"Build verified: 1.0.6-1

Package file has been created."
576,MCOL-274,MCOL,Dipti Joshi,85838,2016-08-25 16:16:16,"[~David.Hall], [~LinuxJedi] Can you please see if this is something quick to fix ?
",1,"[~David.Hall], [~LinuxJedi] Can you please see if this is something quick to fix ?
"
577,MCOL-274,MCOL,Andrew Hutchings,85839,2016-08-25 16:33:04,"To answer [~ernstae]'s question: the date validation routine used is supplied by ""Boost"" which imposes those limits. I already have a fix I'm working on (as part of my MCOL-171 fix) that fixes this so that the limits are more in-line with MariaDB's

The reason why Boost uses year 1400 is because Gregorian calendar dates start at year 1582 and I believe year 1400 is an arbitrary date before that.",2,"To answer [~ernstae]'s question: the date validation routine used is supplied by ""Boost"" which imposes those limits. I already have a fix I'm working on (as part of my MCOL-171 fix) that fixes this so that the limits are more in-line with MariaDB's

The reason why Boost uses year 1400 is because Gregorian calendar dates start at year 1582 and I believe year 1400 is an arbitrary date before that."
578,MCOL-274,MCOL,Andrew Hutchings,85843,2016-08-25 20:38:00,complete fix is now in https://github.com/LinuxJedi/mariadb-columnstore-engine/tree/MCOL-271. If I can't finish the fix for the finial MCOL-271 issue early tomorrow I'll split out the MCOL-274 fix and put that up for review separately.,3,complete fix is now in URL If I can't finish the fix for the finial MCOL-271 issue early tomorrow I'll split out the MCOL-274 fix and put that up for review separately.
579,MCOL-274,MCOL,David Hall,85857,2016-08-26 14:33:32,"While desirable, this fix may break a number of our regression tests and could require a number of changes there. Some tests are testing the minimum date threshold.",4,"While desirable, this fix may break a number of our regression tests and could require a number of changes there. Some tests are testing the minimum date threshold."
580,MCOL-274,MCOL,Andrew Hutchings,85947,2016-08-30 10:33:41,"Pull request created. Extracted from my other patches.

https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/4",5,"Pull request created. Extracted from my other patches.

URL"
581,MCOL-274,MCOL,Andrew Hutchings,85957,2016-08-30 14:03:35,Requesting review. I think Hall is right about the test suite.,6,Requesting review. I think Hall is right about the test suite.
582,MCOL-274,MCOL,David Hall,86083,2016-09-02 16:55:23,Test and close,7,Test and close
583,MCOL-274,MCOL,Daniel Lee,86432,2016-09-13 17:14:06,"Build verified:

Name        : mariadb-columnstore-platform
Version     : 1.0.3
Release     : 1
Architecture: x86_64
Install Date: Tue 13 Sep 2016 09:26:32 AM CDT

Verified the following:
insert
update
LDI with cpimport
LDI without cpimport
cpimport
",8,"Build verified:

Name        : mariadb-columnstore-platform
Version     : 1.0.3
Release     : 1
Architecture: x86_64
Install Date: Tue 13 Sep 2016 09:26:32 AM CDT

Verified the following:
insert
update
LDI with cpimport
LDI without cpimport
cpimport
"
584,MCOL-28,MCOL,Dipti Joshi,83592,2016-05-23 07:29:16,Not failing any more so   closing this,1,Not failing any more so   closing this
585,MCOL-29,MCOL,David Hill,83538,2016-05-20 02:10:02,"mismatch in decimals

tested sql/binarysigned.sql.log against sql/binarysigned.sql.log.ref
Failed.  Check sql/binarysigned.sql.diff for differences.
tested sql/binaryunsigned.sql.log against sql/binaryunsigned.sql.log.ref
Failed.  Check sql/binaryunsigned.sql.diff for differences.

[root@srvss2 srvswdev11]# diff test011/sql/binarysigned.sql.log test011/sql/binarysigned.sql.log.ref
8,9c8,9
< 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e38	-1.7976931348623157e308
< 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e38	1.7976931348623157e308

",1,"mismatch in decimals

tested sql/binarysigned.sql.log against sql/binarysigned.sql.log.ref
Failed.  Check sql/binarysigned.sql.diff for differences.
tested sql/binaryunsigned.sql.log against sql/binaryunsigned.sql.log.ref
Failed.  Check sql/binaryunsigned.sql.diff for differences.

[root@srvss2 srvswdev11]# diff test011/sql/binarysigned.sql.log test011/sql/binarysigned.sql.log.ref
8,9c8,9
< 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e38	-1.7976931348623157e308
< 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e38	1.7976931348623157e308

"
586,MCOL-29,MCOL,Dipti Joshi,83539,2016-05-20 03:21:03,"[~hill] Output in your above comment is cutoff - so I am putting it again here for completeness

diff test011/sql/binarysigned.sql.log test011/sql/binarysigned.sql.log.ref
8,9c8,9
< 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e38	-1.7976931348623157e308
< 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e38	1.7976931348623157e308
---
> 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e+38	-1.79769313486232e+308
> 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e+38	1.79769313486232e+308

As can be seen that the difference is because MariaDB ColumnStore is using e notation  like this: 3.40282e38, where as the reference log is using 3.40383e+38 

The value in the result shown by MariaDB ColumnStore and the reference are same, but the display format notation are different - so the reference log files and binaryunsigned.sql.log.ref and binarysigned.sql.log.ref  need to be updated to use e38 notation instead of e+38 notation

After you update the reference log file - run this test script and if it passes please close the issue.",2,"[~hill] Output in your above comment is cutoff - so I am putting it again here for completeness

diff test011/sql/binarysigned.sql.log test011/sql/binarysigned.sql.log.ref
8,9c8,9
< 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e38	-1.7976931348623157e308
< 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e38	1.7976931348623157e308
---
> 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e+38	-1.79769313486232e+308
> 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e+38	1.79769313486232e+308

As can be seen that the difference is because MariaDB ColumnStore is using e notation  like this: 3.40282e38, where as the reference log is using 3.40383e+38 

The value in the result shown by MariaDB ColumnStore and the reference are same, but the display format notation are different - so the reference log files and binaryunsigned.sql.log.ref and binarysigned.sql.log.ref  need to be updated to use e38 notation instead of e+38 notation

After you update the reference log file - run this test script and if it passes please close the issue."
587,MCOL-29,MCOL,David Hill,83557,2016-05-20 18:21:29,change made to ref logs,3,change made to ref logs
588,MCOL-29,MCOL,Dipti Joshi,83558,2016-05-20 18:25:39,"[~hill], Just to confirm ref log for both binarysigned.sql.log.ref and binaryunsigned.sql.log.ref needs to be updated",4,"[~hill], Just to confirm ref log for both binarysigned.sql.log.ref and binaryunsigned.sql.log.ref needs to be updated"
589,MCOL-29,MCOL,Dipti Joshi,83593,2016-05-23 07:30:54,Not failing any more - so closing this,5,Not failing any more - so closing this
590,MCOL-294,MCOL,Andrew Hutchings,86446,2016-09-14 06:33:22,"This will be in 2 phases:

#  Make it work exactly as before, using the old jemalloc during install and LD_PRELOAD

# Use a newer jemalloc and link everything to it at compile time

Phase 1 will be in 1.0.3, Phase 2 in 1.0.4",1,"This will be in 2 phases:

#  Make it work exactly as before, using the old jemalloc during install and LD_PRELOAD

# Use a newer jemalloc and link everything to it at compile time

Phase 1 will be in 1.0.3, Phase 2 in 1.0.4"
591,MCOL-294,MCOL,Andrew Hutchings,86450,2016-09-14 07:59:03,Phase 1 is complete and in the tree. Keeping this open to work on Phase 2.,2,Phase 1 is complete and in the tree. Keeping this open to work on Phase 2.
592,MCOL-294,MCOL,Andrew Hutchings,86453,2016-09-14 11:49:53,"Phase 2 available at: https://github.com/LinuxJedi/mariadb-columnstore-engine/tree/MCOL-294-phase2

It will be turned into a pull request once the window opens for 1.0.4.

This uses the OS's jemalloc installation in the same way MariaDB does and links all the ColumnStore binaries against it. If jemalloc is not found CMake will fire a warning and continue without it.

For RHEL/CentOS we should use jemalloc from EPEL and update our build scripts and documentation accordingly.
",3,"Phase 2 available at: URL

It will be turned into a pull request once the window opens for 1.0.4.

This uses the OS's jemalloc installation in the same way MariaDB does and links all the ColumnStore binaries against it. If jemalloc is not found CMake will fire a warning and continue without it.

For RHEL/CentOS we should use jemalloc from EPEL and update our build scripts and documentation accordingly.
"
593,MCOL-294,MCOL,Andrew Hutchings,86456,2016-09-14 12:07:43,TODO: alterations to RPM spec file for phase2. These will conflict with other alterations currently going on for MCOL-292 so I'll wait until that is merged first.,4,TODO: alterations to RPM spec file for phase2. These will conflict with other alterations currently going on for MCOL-292 so I'll wait until that is merged first.
594,MCOL-294,MCOL,Andrew Hutchings,86769,2016-09-23 23:35:20,"Patch works similar to MariaDB. It will compile-in jemalloc if it is found in the OS. Otherwise it will use the standard malloc.

The patch also fixes a namespace issue with one of David Hall's patches which broke the build on my machine",5,"Patch works similar to MariaDB. It will compile-in jemalloc if it is found in the OS. Otherwise it will use the standard malloc.

The patch also fixes a namespace issue with one of David Hall's patches which broke the build on my machine"
595,MCOL-294,MCOL,Daniel Lee,87177,2016-10-07 10:14:32,"Performance in 1.0.4 has been restored back to 1.0.1.

I need someone to review the ticket from a programming practice point of view.
",6,"Performance in 1.0.4 has been restored back to 1.0.1.

I need someone to review the ticket from a programming practice point of view.
"
596,MCOL-294,MCOL,David Hall,87384,2016-10-13 18:04:00,This is all cmake stuff. Ben is a better reviewer.,7,This is all cmake stuff. Ben is a better reviewer.
597,MCOL-294,MCOL,Ben Thompson,87762,2016-10-25 17:35:09,"Review was completed when merged to develop branch, must have missed the MCOL review. ",8,"Review was completed when merged to develop branch, must have missed the MCOL review. "
598,MCOL-297,MCOL,David Hall,86501,2016-09-14 22:19:24,Removed the offending -7. I would really like to know what was going through the author's mind when s/he wrote that.,1,Removed the offending -7. I would really like to know what was going through the author's mind when s/he wrote that.
599,MCOL-297,MCOL,Daniel Lee,87120,2016-10-05 23:32:23,"Build verified: 1.0.4-1

mcsadmin> getsoft
getsoftwareinfo   Tue Oct  4 22:29:41 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.4
Release     : 1
Architecture: x86_64
Install Date: Tue 04 Oct 2016 01:38:36 PM CDT
Group       : Applications/Databases
Size        : 11506458
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.4-1.src.rpm
Build Date  : Fri 30 Sep 2016 01:02:40 PM CDT

MariaDB [mytest]> select cdate, character_length(cdate), cdatetime, character_length( cdatetime) from datatypetestm;
+------------+-------------------------+---------------------+------------------------------+
| cdate      | character_length(cdate) | cdatetime           | character_length( cdatetime) |
+------------+-------------------------+---------------------+------------------------------+
| 1997-01-01 |                      10 | 1997-01-01 00:00:00 |                           19 |
| 1997-01-01 |                      10 | 1997-01-01 00:00:01 |                           19 |
| 1997-01-02 |                      10 | 1997-01-02 00:00:01 |                           19 |
| 1997-01-03 |                      10 | 1997-01-03 00:00:02 |                           19 |
| 1997-01-04 |                      10 | 1997-01-04 00:00:03 |                           19 |
| 2009-12-28 |                      10 | 2009-12-31 23:59:56 |                           19 |
| 2009-12-29 |                      10 | 2009-12-31 23:59:57 |                           19 |
| 2009-12-30 |                      10 | 2009-12-31 23:59:58 |                           19 |
| 2009-12-31 |                      10 | 2009-12-31 23:59:59 |                           19 |
| 2009-12-31 |                      10 | 2009-12-31 23:59:59 |                           19 |
| 2009-12-31 |                      10 | 2009-12-31 23:59:59 |                           19 |
+------------+-------------------------+---------------------+------------------------------+
11 rows in set (0.01 sec)

",2,"Build verified: 1.0.4-1

mcsadmin> getsoft
getsoftwareinfo   Tue Oct  4 22:29:41 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.4
Release     : 1
Architecture: x86_64
Install Date: Tue 04 Oct 2016 01:38:36 PM CDT
Group       : Applications/Databases
Size        : 11506458
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.4-1.src.rpm
Build Date  : Fri 30 Sep 2016 01:02:40 PM CDT

MariaDB [mytest]> select cdate, character_length(cdate), cdatetime, character_length( cdatetime) from datatypetestm;
+------------+-------------------------+---------------------+------------------------------+
| cdate      | character_length(cdate) | cdatetime           | character_length( cdatetime) |
+------------+-------------------------+---------------------+------------------------------+
| 1997-01-01 |                      10 | 1997-01-01 00:00:00 |                           19 |
| 1997-01-01 |                      10 | 1997-01-01 00:00:01 |                           19 |
| 1997-01-02 |                      10 | 1997-01-02 00:00:01 |                           19 |
| 1997-01-03 |                      10 | 1997-01-03 00:00:02 |                           19 |
| 1997-01-04 |                      10 | 1997-01-04 00:00:03 |                           19 |
| 2009-12-28 |                      10 | 2009-12-31 23:59:56 |                           19 |
| 2009-12-29 |                      10 | 2009-12-31 23:59:57 |                           19 |
| 2009-12-30 |                      10 | 2009-12-31 23:59:58 |                           19 |
| 2009-12-31 |                      10 | 2009-12-31 23:59:59 |                           19 |
| 2009-12-31 |                      10 | 2009-12-31 23:59:59 |                           19 |
| 2009-12-31 |                      10 | 2009-12-31 23:59:59 |                           19 |
+------------+-------------------------+---------------------+------------------------------+
11 rows in set (0.01 sec)

"
600,MCOL-3,MCOL,David Hill,83455,2016-05-16 13:16:45,"Also changed the calpont/infinidb references in the mcsadmin, mcsmysql and postConfigure commands. These are user commands where they would see the terms.",1,"Also changed the calpont/infinidb references in the mcsadmin, mcsmysql and postConfigure commands. These are user commands where they would see the terms."
601,MCOL-3,MCOL,David Hill,83525,2016-05-19 20:51:18,"changed all directory and scripts that used Calpont or infinidb:

Directories changes:

/usr/local/MariaDB/Columnstore
/var/log/Columnstore

script changes:

service startup script infinidb to columnstore
calpontSupport to columnstoreSupport
calpontAlias (and contents) to columnStore
infinidb.def to columnstore.def
infinidb.conf to columnstore.conf
infinidbDBWrite to columnstoreDBWrite
infinidbLogrotate to columnstoreLogrotate
infinidbSyslog to columnstoreSyslog
infinidbSyslog7 to columnstoreSyslog7
infinidbSyslog-ng to columnstoreSyslog-ng
master-rep-infinidb.sh to master-rep-columnstore.sh
slave-rep-infinidb.sh to slave-rep-columnstore.sh
upgrade-infinidb.sh to upgrade-columnstore.sh

Change all references in these 3 user commands

mcsmysql
mcsadmin
postConfigure",2,"changed all directory and scripts that used Calpont or infinidb:

Directories changes:

/usr/local/MariaDB/Columnstore
/var/log/Columnstore

script changes:

service startup script infinidb to columnstore
calpontSupport to columnstoreSupport
calpontAlias (and contents) to columnStore
infinidb.def to columnstore.def
infinidb.conf to columnstore.conf
infinidbDBWrite to columnstoreDBWrite
infinidbLogrotate to columnstoreLogrotate
infinidbSyslog to columnstoreSyslog
infinidbSyslog7 to columnstoreSyslog7
infinidbSyslog-ng to columnstoreSyslog-ng
master-rep-infinidb.sh to master-rep-columnstore.sh
slave-rep-infinidb.sh to slave-rep-columnstore.sh
upgrade-infinidb.sh to upgrade-columnstore.sh

Change all references in these 3 user commands

mcsmysql
mcsadmin
postConfigure"
602,MCOL-3,MCOL,David Hill,83527,2016-05-19 21:28:37,"also package name changes

mariadb-columnstore-1.0-1-x86_64-centos6-client.rpm                                                                                                 
mariadb-columnstore-1.0-1-x86_64-centos6-common.rpm                                                                                                 
mariadb-columnstore-1.0-1-x86_64-centos6-server.rpm                                                                                                 
mariadb-columnstore-1.0-1-x86_64-centos6-shared.rpm                                                                                                 
mariadb-columnstore-enterprise-1.0-1.x86_64.rpm                                                                                                     
mariadb-columnstore-libs-1.0-1.x86_64.rpm                                                                                                           
mariadb-columnstore-platform-1.0-1.x86_64.rpm                                                                                                       
mariadb-columnstore-storage-engine-1.0-1.x86_64.rpm                   ",3,"also package name changes

mariadb-columnstore-1.0-1-x86_64-centos6-client.rpm                                                                                                 
mariadb-columnstore-1.0-1-x86_64-centos6-common.rpm                                                                                                 
mariadb-columnstore-1.0-1-x86_64-centos6-server.rpm                                                                                                 
mariadb-columnstore-1.0-1-x86_64-centos6-shared.rpm                                                                                                 
mariadb-columnstore-enterprise-1.0-1.x86_64.rpm                                                                                                     
mariadb-columnstore-libs-1.0-1.x86_64.rpm                                                                                                           
mariadb-columnstore-platform-1.0-1.x86_64.rpm                                                                                                       
mariadb-columnstore-storage-engine-1.0-1.x86_64.rpm                   "
603,MCOL-3,MCOL,Dipti Joshi,83855,2016-05-31 14:43:33,"[~hill]Just noticed that the changes that you made are installing in /usr/local/MariaDB/Columnstore - with M, D, B and C capitalized , as you notice in the description I used all lower case. Can you please change so that we are consistent ?",4,"[~hill]Just noticed that the changes that you made are installing in /usr/local/MariaDB/Columnstore - with M, D, B and C capitalized , as you notice in the description I used all lower case. Can you please change so that we are consistent ?"
604,MCOL-3,MCOL,David Hill,83914,2016-06-01 13:54:26,"Name changes done

/usr/local/MariaDB/Columnstore to
/usr/local/mariadb/columnstore

/var/log/Columnstore to
/var/log/mariadb/columnstore",5,"Name changes done

/usr/local/MariaDB/Columnstore to
/usr/local/mariadb/columnstore

/var/log/Columnstore to
/var/log/mariadb/columnstore"
605,MCOL-3,MCOL,Daniel Lee,83949,2016-06-02 15:19:38,"Build verified:

mscadmin> getsoft
getsoftwareinfo   Thu Jun  2 10:17:09 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Tue 31 May 2016 07:25:57 PM CDT
Install Date: Thu 02 Jun 2016 10:09:03 AM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-0.src.rpm

",6,"Build verified:

mscadmin> getsoft
getsoftwareinfo   Thu Jun  2 10:17:09 2016

Name        : mariadb-columnstore-platform  Relocations: (not relocatable)
Version     : 1.0                               Vendor: MariaDB Corporation Ab
Release     : 0                             Build Date: Tue 31 May 2016 07:25:57 PM CDT
Install Date: Thu 02 Jun 2016 10:09:03 AM CDT      Build Host: srvbuilder
Group       : Applications                  Source RPM: mariadb-columnstore-1.0-0.src.rpm

"
606,MCOL-30,MCOL,David Hill,83532,2016-05-20 02:04:43,"failing because not supported

[root@srvss2 srvswdev11]# cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log

diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
---
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by Infi",1,"failing because not supported

[root@srvss2 srvswdev11]# cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log

diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
---
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by Infi"
607,MCOL-30,MCOL,Dipti Joshi,83540,2016-05-20 04:05:12,"[~hill] The script not failing because it is not supported - The script is failing because the reference log is using MySQL error number 138, where as MariaDB ColumnStore uses error number from MariaDB Server   1178 - other wise essentially the error text is the same message. 

Please update the reference log file to use ""ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB."" instead of  :ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB""

Once reference log file is updated rerun the test and if it passes close this Jira item.",2,"[~hill] The script not failing because it is not supported - The script is failing because the reference log is using MySQL error number 138, where as MariaDB ColumnStore uses error number from MariaDB Server   1178 - other wise essentially the error text is the same message. 

Please update the reference log file to use ""ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB."" instead of  :ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB""

Once reference log file is updated rerun the test and if it passes close this Jira item."
608,MCOL-30,MCOL,David Hill,83559,2016-05-20 18:30:54,ref logs changed,3,ref logs changed
609,MCOL-30,MCOL,Dipti Joshi,83594,2016-05-23 07:32:06,Not failing any more - so closing,4,Not failing any more - so closing
610,MCOL-304,MCOL,David Thompson,89419,2016-12-09 21:16:44,When we push the builds to the download site can you make sure we use a directory structure like maxscale as this will help subsequent creation of the repo files.,1,When we push the builds to the download site can you make sure we use a directory structure like maxscale as this will help subsequent creation of the repo files.
611,MCOL-304,MCOL,David Hill,89537,2016-12-14 00:52:33,repor directories setup with DT review,2,repor directories setup with DT review
612,MCOL-304,MCOL,David Thompson,89538,2016-12-14 01:10:04,Re-opening as we still need to do the actual package repo work.,3,Re-opening as we still need to do the actual package repo work.
613,MCOL-304,MCOL,David Thompson,89557,2016-12-14 17:20:31,removing fix version so can close the release though this is associated with 1.0.6.,4,removing fix version so can close the release though this is associated with 1.0.6.
614,MCOL-304,MCOL,David Hill,94103,2017-04-19 15:23:24,"Progress - checking into this error

I have the repo setup on the Buildbot Master with rpms.

I'm trying to download to a centos 7 amazon instance and I'm getting this error. SO trying to figure this one out.

yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: priorities, update-motd, upgrade-helper
amzn-main/latest                                            | 2.1 kB     00:00     
amzn-main/latest/group                                      |  35 kB     00:00     
amzn-main/latest/primary_db                                 | 3.6 MB     00:00     
amzn-updates/latest                                         | 2.3 kB     00:00     
amzn-updates/latest/group                                   |  35 kB     00:00     
amzn-updates/latest/updateinfo                              | 376 kB     00:00     
amzn-updates/latest/primary_db                              |  15 kB     00:00     
http://x.x.x.x/repos/centos/7/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - ""The requested URL returned error: 404 Not Found""
Trying other mirror.

this the repo file on that server using the Elastic IP of the Master Instance. Can ping and I have http access setupo there.

[mariadb-columnstore]
name = MariaDB ColumnStore
baseurl = http://x.x.x.x/repos/centos/7/x86_64/
gpgkey=http://x.x.x.x/repos/RPM-GPG-KEY-MariaDB-ColumnStore
gpgcheck=1

file does exist on the master:

ll /repos/centos/7/x86_64/repodata/repomd.xml
-rw-r--r--. 1 root root 3700 Apr 19 09:11 /repos/centos/7/x86_64/repodata/repomd.xml
",5,"Progress - checking into this error

I have the repo setup on the Buildbot Master with rpms.

I'm trying to download to a centos 7 amazon instance and I'm getting this error. SO trying to figure this one out.

yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: priorities, update-motd, upgrade-helper
amzn-main/latest                                            | 2.1 kB     00:00     
amzn-main/latest/group                                      |  35 kB     00:00     
amzn-main/latest/primary_db                                 | 3.6 MB     00:00     
amzn-updates/latest                                         | 2.3 kB     00:00     
amzn-updates/latest/group                                   |  35 kB     00:00     
amzn-updates/latest/updateinfo                              | 376 kB     00:00     
amzn-updates/latest/primary_db                              |  15 kB     00:00     
URL [Errno 14] PYCURL ERROR 22 - ""The requested URL returned error: 404 Not Found""
Trying other mirror.

this the repo file on that server using the Elastic IP of the Master Instance. Can ping and I have http access setupo there.

[mariadb-columnstore]
name = MariaDB ColumnStore
baseurl = URL
gpgkey=URL
gpgcheck=1

file does exist on the master:

ll /repos/centos/7/x86_64/repodata/repomd.xml
-rw-r--r--. 1 root root 3700 Apr 19 09:11 /repos/centos/7/x86_64/repodata/repomd.xml
"
615,MCOL-304,MCOL,David Hill,94118,2017-04-19 19:45:39,"UPDATE: resolved the file permissions issue...

Now the groupinstall is working...

yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: priorities, update-motd, upgrade-helper
amzn-main/latest                                           | 2.1 kB     00:00     
amzn-updates/latest                                        | 2.3 kB     00:00     
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-common.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-libs.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-platform.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-shared.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-storage-engine.x86_64 0:1.0.8-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==================================================================================
 Package                      Arch   Version            Repository           Size
==================================================================================
Installing for group install ""MariaDB ColumnStore"":
 mariadb-columnstore-client   x86_64 1.0.8-1.el7.centos mariadb-columnstore  12 M
 mariadb-columnstore-common   x86_64 1.0.8-1.el7.centos mariadb-columnstore 122 k
 mariadb-columnstore-gssapi-client
                              x86_64 1.0.8-1.el7.centos mariadb-columnstore  19 k
 mariadb-columnstore-gssapi-server
                              x86_64 1.0.8-1.el7.centos mariadb-columnstore  46 k
 mariadb-columnstore-libs     x86_64 1.0.8-1            mariadb-columnstore 3.5 M
 mariadb-columnstore-platform x86_64 1.0.8-1            mariadb-columnstore 2.1 M
 mariadb-columnstore-server   x86_64 1.0.8-1.el7.centos mariadb-columnstore  70 M
 mariadb-columnstore-shared   x86_64 1.0.8-1.el7.centos mariadb-columnstore 1.2 M
 mariadb-columnstore-storage-engine
                              x86_64 1.0.8-1            mariadb-columnstore 424 k

Transaction Summary
==================================================================================
Install  9 Packages

Total size: 89 M
Installed size: 403 M
Is this ok [y/d/N]: N
Exiting on user command
Your transaction was saved, rerun it with:
 yum load-transaction /tmp/yum_save_tx.2017-04-19.19-43.K2otPO.yumtx
[root@ip-172-30-0-119 ~]# yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: priorities, update-motd, upgrade-helper
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-common.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-libs.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-platform.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-shared.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-storage-engine.x86_64 0:1.0.8-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================
 Package                                        Arch               Version                          Repository                       Size
==========================================================================================================================================
Installing for group install ""MariaDB ColumnStore"":
 mariadb-columnstore-client                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore              12 M
 mariadb-columnstore-common                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore             122 k
 mariadb-columnstore-gssapi-client              x86_64             1.0.8-1.el7.centos               mariadb-columnstore              19 k
 mariadb-columnstore-gssapi-server              x86_64             1.0.8-1.el7.centos               mariadb-columnstore              46 k
 mariadb-columnstore-libs                       x86_64             1.0.8-1                          mariadb-columnstore             3.5 M
 mariadb-columnstore-platform                   x86_64             1.0.8-1                          mariadb-columnstore             2.1 M
 mariadb-columnstore-server                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore              70 M
 mariadb-columnstore-shared                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore             1.2 M
 mariadb-columnstore-storage-engine             x86_64             1.0.8-1                          mariadb-columnstore             424 k

Transaction Summary
==========================================================================================================================================
Install  9 Packages

Total size: 89 M
Installed size: 403 M
Is this ok [y/d/N]: y
Downloading packages:
warning: /var/cache/yum/x86_64/latest/mariadb-columnstore/packages/mariadb-columnstore-1.0.8-1-x86_64-centos7-client.rpm: Header V4 RSA/SHA1 Signature, key ID c12ebbf9: NOKEY
Retrieving key from http://54.172.106.13/repos/RPM-GPG-KEY-MariaDB-ColumnStore
Importing GPG key 0xC12EBBF9:
 Userid     : ""David Hill (Columnstore Key) <support@mariadb.com>""
 Fingerprint: 2561 235d 532e 3ce9 511b 8085 91d0 be3b c12e bbf9
 From       : http://54.172.106.13/repos/RPM-GPG-KEY-MariaDB-ColumnStore
Is this ok [y/N]: y
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : mariadb-columnstore-common-1.0.8-1.el7.centos.x86_64                                                                   1/9 
  Installing : mariadb-columnstore-libs-1.0.8-1.x86_64                                                                                2/9 
MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-client-1.0.8-1.el7.centos.x86_64                                                                   3/9 
  Installing : mariadb-columnstore-server-1.0.8-1.el7.centos.x86_64                                                                   4/9 
  Installing : mariadb-columnstore-gssapi-server-1.0.8-1.el7.centos.x86_64                                                            5/9 
  Installing : mariadb-columnstore-storage-engine-1.0.8-1.x86_64                                                                      6/9 
MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-platform-1.0.8-1.x86_64                                                                            7/9 
The next step is:

/usr/local/mariadb/columnstore/bin/postConfigure

MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-shared-1.0.8-1.el7.centos.x86_64                                                                   8/9 
  Installing : mariadb-columnstore-gssapi-client-1.0.8-1.el7.centos.x86_64                                                            9/9 
  Verifying  : mariadb-columnstore-client-1.0.8-1.el7.centos.x86_64                                                                   1/9 
  Verifying  : mariadb-columnstore-shared-1.0.8-1.el7.centos.x86_64                                                                   2/9 
  Verifying  : mariadb-columnstore-common-1.0.8-1.el7.centos.x86_64                                                                   3/9 
  Verifying  : mariadb-columnstore-storage-engine-1.0.8-1.x86_64                                                                      4/9 
  Verifying  : mariadb-columnstore-gssapi-server-1.0.8-1.el7.centos.x86_64                                                            5/9 
  Verifying  : mariadb-columnstore-platform-1.0.8-1.x86_64                                                                            6/9 
  Verifying  : mariadb-columnstore-server-1.0.8-1.el7.centos.x86_64                                                                   7/9 
  Verifying  : mariadb-columnstore-gssapi-client-1.0.8-1.el7.centos.x86_64                                                            8/9 
  Verifying  : mariadb-columnstore-libs-1.0.8-1.x86_64                                                                                9/9 

Installed:
  mariadb-columnstore-client.x86_64 0:1.0.8-1.el7.centos              mariadb-columnstore-common.x86_64 0:1.0.8-1.el7.centos             
  mariadb-columnstore-gssapi-client.x86_64 0:1.0.8-1.el7.centos       mariadb-columnstore-gssapi-server.x86_64 0:1.0.8-1.el7.centos      
  mariadb-columnstore-libs.x86_64 0:1.0.8-1                           mariadb-columnstore-platform.x86_64 0:1.0.8-1                      
  mariadb-columnstore-server.x86_64 0:1.0.8-1.el7.centos              mariadb-columnstore-shared.x86_64 0:1.0.8-1.el7.centos             
  mariadb-columnstore-storage-engine.x86_64 0:1.0.8-1                

Complete!
[root@ip-172-30-0-119 ~]# 
",6,"UPDATE: resolved the file permissions issue...

Now the groupinstall is working...

yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: priorities, update-motd, upgrade-helper
amzn-main/latest                                           | 2.1 kB     00:00     
amzn-updates/latest                                        | 2.3 kB     00:00     
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-common.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-libs.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-platform.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-shared.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-storage-engine.x86_64 0:1.0.8-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==================================================================================
 Package                      Arch   Version            Repository           Size
==================================================================================
Installing for group install ""MariaDB ColumnStore"":
 mariadb-columnstore-client   x86_64 1.0.8-1.el7.centos mariadb-columnstore  12 M
 mariadb-columnstore-common   x86_64 1.0.8-1.el7.centos mariadb-columnstore 122 k
 mariadb-columnstore-gssapi-client
                              x86_64 1.0.8-1.el7.centos mariadb-columnstore  19 k
 mariadb-columnstore-gssapi-server
                              x86_64 1.0.8-1.el7.centos mariadb-columnstore  46 k
 mariadb-columnstore-libs     x86_64 1.0.8-1            mariadb-columnstore 3.5 M
 mariadb-columnstore-platform x86_64 1.0.8-1            mariadb-columnstore 2.1 M
 mariadb-columnstore-server   x86_64 1.0.8-1.el7.centos mariadb-columnstore  70 M
 mariadb-columnstore-shared   x86_64 1.0.8-1.el7.centos mariadb-columnstore 1.2 M
 mariadb-columnstore-storage-engine
                              x86_64 1.0.8-1            mariadb-columnstore 424 k

Transaction Summary
==================================================================================
Install  9 Packages

Total size: 89 M
Installed size: 403 M
Is this ok [y/d/N]: N
Exiting on user command
Your transaction was saved, rerun it with:
 yum load-transaction /tmp/yum_save_tx.2017-04-19.19-43.K2otPO.yumtx
[root@ip-172-30-0-119 ~]# yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: priorities, update-motd, upgrade-helper
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-common.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-client.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-gssapi-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-libs.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-platform.x86_64 0:1.0.8-1 will be installed
---> Package mariadb-columnstore-server.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-shared.x86_64 0:1.0.8-1.el7.centos will be installed
---> Package mariadb-columnstore-storage-engine.x86_64 0:1.0.8-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================
 Package                                        Arch               Version                          Repository                       Size
==========================================================================================================================================
Installing for group install ""MariaDB ColumnStore"":
 mariadb-columnstore-client                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore              12 M
 mariadb-columnstore-common                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore             122 k
 mariadb-columnstore-gssapi-client              x86_64             1.0.8-1.el7.centos               mariadb-columnstore              19 k
 mariadb-columnstore-gssapi-server              x86_64             1.0.8-1.el7.centos               mariadb-columnstore              46 k
 mariadb-columnstore-libs                       x86_64             1.0.8-1                          mariadb-columnstore             3.5 M
 mariadb-columnstore-platform                   x86_64             1.0.8-1                          mariadb-columnstore             2.1 M
 mariadb-columnstore-server                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore              70 M
 mariadb-columnstore-shared                     x86_64             1.0.8-1.el7.centos               mariadb-columnstore             1.2 M
 mariadb-columnstore-storage-engine             x86_64             1.0.8-1                          mariadb-columnstore             424 k

Transaction Summary
==========================================================================================================================================
Install  9 Packages

Total size: 89 M
Installed size: 403 M
Is this ok [y/d/N]: y
Downloading packages:
warning: /var/cache/yum/x86_64/latest/mariadb-columnstore/packages/mariadb-columnstore-1.0.8-1-x86_64-centos7-client.rpm: Header V4 RSA/SHA1 Signature, key ID c12ebbf9: NOKEY
Retrieving key from URL
Importing GPG key 0xC12EBBF9:
 Userid     : ""David Hill (Columnstore Key) ""
 Fingerprint: 2561 235d 532e 3ce9 511b 8085 91d0 be3b c12e bbf9
 From       : URL
Is this ok [y/N]: y
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : mariadb-columnstore-common-1.0.8-1.el7.centos.x86_64                                                                   1/9 
  Installing : mariadb-columnstore-libs-1.0.8-1.x86_64                                                                                2/9 
MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-client-1.0.8-1.el7.centos.x86_64                                                                   3/9 
  Installing : mariadb-columnstore-server-1.0.8-1.el7.centos.x86_64                                                                   4/9 
  Installing : mariadb-columnstore-gssapi-server-1.0.8-1.el7.centos.x86_64                                                            5/9 
  Installing : mariadb-columnstore-storage-engine-1.0.8-1.x86_64                                                                      6/9 
MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-platform-1.0.8-1.x86_64                                                                            7/9 
The next step is:

/usr/local/mariadb/columnstore/bin/postConfigure

MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-shared-1.0.8-1.el7.centos.x86_64                                                                   8/9 
  Installing : mariadb-columnstore-gssapi-client-1.0.8-1.el7.centos.x86_64                                                            9/9 
  Verifying  : mariadb-columnstore-client-1.0.8-1.el7.centos.x86_64                                                                   1/9 
  Verifying  : mariadb-columnstore-shared-1.0.8-1.el7.centos.x86_64                                                                   2/9 
  Verifying  : mariadb-columnstore-common-1.0.8-1.el7.centos.x86_64                                                                   3/9 
  Verifying  : mariadb-columnstore-storage-engine-1.0.8-1.x86_64                                                                      4/9 
  Verifying  : mariadb-columnstore-gssapi-server-1.0.8-1.el7.centos.x86_64                                                            5/9 
  Verifying  : mariadb-columnstore-platform-1.0.8-1.x86_64                                                                            6/9 
  Verifying  : mariadb-columnstore-server-1.0.8-1.el7.centos.x86_64                                                                   7/9 
  Verifying  : mariadb-columnstore-gssapi-client-1.0.8-1.el7.centos.x86_64                                                            8/9 
  Verifying  : mariadb-columnstore-libs-1.0.8-1.x86_64                                                                                9/9 

Installed:
  mariadb-columnstore-client.x86_64 0:1.0.8-1.el7.centos              mariadb-columnstore-common.x86_64 0:1.0.8-1.el7.centos             
  mariadb-columnstore-gssapi-client.x86_64 0:1.0.8-1.el7.centos       mariadb-columnstore-gssapi-server.x86_64 0:1.0.8-1.el7.centos      
  mariadb-columnstore-libs.x86_64 0:1.0.8-1                           mariadb-columnstore-platform.x86_64 0:1.0.8-1                      
  mariadb-columnstore-server.x86_64 0:1.0.8-1.el7.centos              mariadb-columnstore-shared.x86_64 0:1.0.8-1.el7.centos             
  mariadb-columnstore-storage-engine.x86_64 0:1.0.8-1                

Complete!
[root@ip-172-30-0-119 ~]# 
"
616,MCOL-304,MCOL,Daniel Bartholomew,103217,2017-11-15 17:38:17,"For debian and ubuntu packages, the tool we use for MariaDB Server is reprepro.

# To use it you first start with creating the repository directory, then a conf subdirectory. 
# Inside the conf subdir create a distributions file
** This file contains entries for all of the distributions your repository supports
** You can use the one on MariaDB.com to get you started: https://downloads.mariadb.com/MariaDB/mariadb-10.2/repo/ubuntu/conf/distributions
** The SignWith line should be set to the ID of the GPG key you're using to sign your packages, the other lines should be fairly self-explanatory (ping me if you have questions)
# once the distributions file is in place, from the repository directory, run reprepro to import the .deb files
** you can either import each .deb file individually with, e.g.:
*** {{reprepro --basedir=. includedeb xenial /path/to/file.deb}}
** or you can import all files listed in a .changes file (if your packaging scripts are creating one of those) with, e.g.: 
*** {{reprepro --basedir=. include xenial /path/to/file.changes}}
# When reprepro is finished you'll have a directory that can be copied wherever and used as a repository

",7,"For debian and ubuntu packages, the tool we use for MariaDB Server is reprepro.

# To use it you first start with creating the repository directory, then a conf subdirectory. 
# Inside the conf subdir create a distributions file
** This file contains entries for all of the distributions your repository supports
** You can use the one on MariaDB.com to get you started: URL
** The SignWith line should be set to the ID of the GPG key you're using to sign your packages, the other lines should be fairly self-explanatory (ping me if you have questions)
# once the distributions file is in place, from the repository directory, run reprepro to import the .deb files
** you can either import each .deb file individually with, e.g.:
*** {{reprepro --basedir=. includedeb xenial /path/to/file.deb}}
** or you can import all files listed in a .changes file (if your packaging scripts are creating one of those) with, e.g.: 
*** {{reprepro --basedir=. include xenial /path/to/file.changes}}
# When reprepro is finished you'll have a directory that can be copied wherever and used as a repository

"
617,MCOL-304,MCOL,David Hill,103929,2017-11-30 21:37:16,"Cool.... reprepro was available for centos 7 download and I got it working manually with setting up a ubuntu 16 repo on the centos 7 buildmaster node and then was able to install using apt-ge on another ubuntu 16 server...

So it does work, will need to automate it next... Did get a library error on install that doesnt happen with dpkg install, so will need to look into that future... BUT WE HAVE UBUNTU REPO PROGRESS.. WOO HOO

 apt-get install mariadb-columnstore*
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Note, selecting 'mariadb-columnstore-storage-engine' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-shared' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-rocksdb-engine' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-platform' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-common' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-libs' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-devel' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-tokudb-engine' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-gssapi-server' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-gssapi-client' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-server' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-client' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-test' for glob 'mariadb-columnstore*'
mariadb-columnstore-gssapi-server is already the newest version (1.0.10).
mariadb-columnstore-gssapi-client is already the newest version (1.0.10).
The following packages were automatically installed and are no longer required:
  linux-aws-headers-4.4.0-1013 linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
  linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws linux-image-4.4.0-1016-aws snap-confine
Use 'sudo apt autoremove' to remove them.
The following NEW packages will be installed:
  mariadb-columnstore-devel mariadb-columnstore-rocksdb-engine mariadb-columnstore-test
  mariadb-columnstore-tokudb-engine
The following packages will be upgraded:
  mariadb-columnstore-client mariadb-columnstore-common mariadb-columnstore-libs
  mariadb-columnstore-platform mariadb-columnstore-server mariadb-columnstore-shared
  mariadb-columnstore-storage-engine
7 upgraded, 4 newly installed, 0 to remove and 120 not upgraded.
Need to get 410 MB of archives.
After this operation, 608 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-common amd64 1.1.3 [166 kB]
Get:2 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-client amd64 1.1.3 [18.4 MB]
Get:3 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-devel amd64 1.1.3 [1,813 kB]
Get:4 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-libs amd64 1.1.3 [96.6 MB]
Get:5 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-platform amd64 1.1.3 [44.4 MB]   
Get:6 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-rocksdb-engine amd64 1.1.3 [89.4 MB]
Get:7 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-server amd64 1.1.3 [98.3 MB]     
Get:8 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-shared amd64 1.1.3 [459 kB]      
Get:9 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-storage-engine amd64 1.1.3 [10.0 MB]                                  
Get:10 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-test amd64 1.1.3 [45.4 MB]                                                      
Get:11 http://54.172.106.13/repos xenial/main amd64 mariadb-columnstore-tokudb-engine amd64 1.1.3 [5,184 kB]                                            
Fetched 410 MB in 56s (7,292 kB/s)                                                                                                                      
(Reading database ... 148981 files and directories currently installed.)
Preparing to unpack .../mariadb-columnstore-common_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-common (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-client_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-client (1.1.3) over (1.0.10) ...
Selecting previously unselected package mariadb-columnstore-devel.
Preparing to unpack .../mariadb-columnstore-devel_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-devel (1.1.3) ...
Preparing to unpack .../mariadb-columnstore-libs_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-libs (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-platform_1.1.3_amd64.deb ...
/usr/local/mariadb/columnstore/bin/getConfig: error while loading shared libraries: libmysqlcl_idb.so.1: cannot open shared object file: No such file or directory
/usr/local/mariadb/columnstore/bin/pre-uninstall: line 65: [: =: unary operator expected
/usr/local/mariadb/columnstore/bin/pre-uninstall: line 65: [: =: unary operator expected
 
Mariab Columnstore uninstall completed
Unpacking mariadb-columnstore-platform (1.1.3) over (1.0.10) ...
Selecting previously unselected package mariadb-columnstore-rocksdb-engine.
Preparing to unpack .../mariadb-columnstore-rocksdb-engine_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-rocksdb-engine (1.1.3) ...
Preparing to unpack .../mariadb-columnstore-server_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-server (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-shared_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-shared (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-storage-engine_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-storage-engine (1.1.3) over (1.0.10) ...
Selecting previously unselected package mariadb-columnstore-test.
Preparing to unpack .../mariadb-columnstore-test_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-test (1.1.3) ...
Selecting previously unselected package mariadb-columnstore-tokudb-engine.
Preparing to unpack .../mariadb-columnstore-tokudb-engine_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-tokudb-engine (1.1.3) ...
Processing triggers for libc-bin (2.23-0ubuntu9) ...
Setting up mariadb-columnstore-common (1.1.3) ...
Setting up mariadb-columnstore-client (1.1.3) ...
Setting up mariadb-columnstore-devel (1.1.3) ...
Setting up mariadb-columnstore-libs (1.1.3) ...
MariaDB ColumnStore install completed
Setting up mariadb-columnstore-platform (1.1.3) ...
The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distributed option:

/usr/local/mariadb/columnstore/bin/columnstore start


MariaDB ColumnStore install completed
Setting up mariadb-columnstore-rocksdb-engine (1.1.3) ...
Setting up mariadb-columnstore-server (1.1.3) ...
Setting up mariadb-columnstore-shared (1.1.3) ...
Setting up mariadb-columnstore-storage-engine (1.1.3) ...
MariaDB ColumnStore install completed
Setting up mariadb-columnstore-test (1.1.3) ...
Setting up mariadb-columnstore-tokudb-engine (1.1.3) ...
Processing triggers for libc-bin (2.23-0ubuntu9) ...",8,"Cool.... reprepro was available for centos 7 download and I got it working manually with setting up a ubuntu 16 repo on the centos 7 buildmaster node and then was able to install using apt-ge on another ubuntu 16 server...

So it does work, will need to automate it next... Did get a library error on install that doesnt happen with dpkg install, so will need to look into that future... BUT WE HAVE UBUNTU REPO PROGRESS.. WOO HOO

 apt-get install mariadb-columnstore*
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Note, selecting 'mariadb-columnstore-storage-engine' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-shared' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-rocksdb-engine' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-platform' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-common' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-libs' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-devel' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-tokudb-engine' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-gssapi-server' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-gssapi-client' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-server' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-client' for glob 'mariadb-columnstore*'
Note, selecting 'mariadb-columnstore-test' for glob 'mariadb-columnstore*'
mariadb-columnstore-gssapi-server is already the newest version (1.0.10).
mariadb-columnstore-gssapi-client is already the newest version (1.0.10).
The following packages were automatically installed and are no longer required:
  linux-aws-headers-4.4.0-1013 linux-aws-headers-4.4.0-1016 linux-headers-4.4.0-1013-aws
  linux-headers-4.4.0-1016-aws linux-image-4.4.0-1013-aws linux-image-4.4.0-1016-aws snap-confine
Use 'sudo apt autoremove' to remove them.
The following NEW packages will be installed:
  mariadb-columnstore-devel mariadb-columnstore-rocksdb-engine mariadb-columnstore-test
  mariadb-columnstore-tokudb-engine
The following packages will be upgraded:
  mariadb-columnstore-client mariadb-columnstore-common mariadb-columnstore-libs
  mariadb-columnstore-platform mariadb-columnstore-server mariadb-columnstore-shared
  mariadb-columnstore-storage-engine
7 upgraded, 4 newly installed, 0 to remove and 120 not upgraded.
Need to get 410 MB of archives.
After this operation, 608 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 URL xenial/main amd64 mariadb-columnstore-common amd64 1.1.3 [166 kB]
Get:2 URL xenial/main amd64 mariadb-columnstore-client amd64 1.1.3 [18.4 MB]
Get:3 URL xenial/main amd64 mariadb-columnstore-devel amd64 1.1.3 [1,813 kB]
Get:4 URL xenial/main amd64 mariadb-columnstore-libs amd64 1.1.3 [96.6 MB]
Get:5 URL xenial/main amd64 mariadb-columnstore-platform amd64 1.1.3 [44.4 MB]   
Get:6 URL xenial/main amd64 mariadb-columnstore-rocksdb-engine amd64 1.1.3 [89.4 MB]
Get:7 URL xenial/main amd64 mariadb-columnstore-server amd64 1.1.3 [98.3 MB]     
Get:8 URL xenial/main amd64 mariadb-columnstore-shared amd64 1.1.3 [459 kB]      
Get:9 URL xenial/main amd64 mariadb-columnstore-storage-engine amd64 1.1.3 [10.0 MB]                                  
Get:10 URL xenial/main amd64 mariadb-columnstore-test amd64 1.1.3 [45.4 MB]                                                      
Get:11 URL xenial/main amd64 mariadb-columnstore-tokudb-engine amd64 1.1.3 [5,184 kB]                                            
Fetched 410 MB in 56s (7,292 kB/s)                                                                                                                      
(Reading database ... 148981 files and directories currently installed.)
Preparing to unpack .../mariadb-columnstore-common_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-common (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-client_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-client (1.1.3) over (1.0.10) ...
Selecting previously unselected package mariadb-columnstore-devel.
Preparing to unpack .../mariadb-columnstore-devel_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-devel (1.1.3) ...
Preparing to unpack .../mariadb-columnstore-libs_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-libs (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-platform_1.1.3_amd64.deb ...
/usr/local/mariadb/columnstore/bin/getConfig: error while loading shared libraries: libmysqlcl_idb.so.1: cannot open shared object file: No such file or directory
/usr/local/mariadb/columnstore/bin/pre-uninstall: line 65: [: =: unary operator expected
/usr/local/mariadb/columnstore/bin/pre-uninstall: line 65: [: =: unary operator expected
 
Mariab Columnstore uninstall completed
Unpacking mariadb-columnstore-platform (1.1.3) over (1.0.10) ...
Selecting previously unselected package mariadb-columnstore-rocksdb-engine.
Preparing to unpack .../mariadb-columnstore-rocksdb-engine_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-rocksdb-engine (1.1.3) ...
Preparing to unpack .../mariadb-columnstore-server_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-server (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-shared_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-shared (1.1.3) over (1.0.10) ...
Preparing to unpack .../mariadb-columnstore-storage-engine_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-storage-engine (1.1.3) over (1.0.10) ...
Selecting previously unselected package mariadb-columnstore-test.
Preparing to unpack .../mariadb-columnstore-test_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-test (1.1.3) ...
Selecting previously unselected package mariadb-columnstore-tokudb-engine.
Preparing to unpack .../mariadb-columnstore-tokudb-engine_1.1.3_amd64.deb ...
Unpacking mariadb-columnstore-tokudb-engine (1.1.3) ...
Processing triggers for libc-bin (2.23-0ubuntu9) ...
Setting up mariadb-columnstore-common (1.1.3) ...
Setting up mariadb-columnstore-client (1.1.3) ...
Setting up mariadb-columnstore-devel (1.1.3) ...
Setting up mariadb-columnstore-libs (1.1.3) ...
MariaDB ColumnStore install completed
Setting up mariadb-columnstore-platform (1.1.3) ...
The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distributed option:

/usr/local/mariadb/columnstore/bin/columnstore start


MariaDB ColumnStore install completed
Setting up mariadb-columnstore-rocksdb-engine (1.1.3) ...
Setting up mariadb-columnstore-server (1.1.3) ...
Setting up mariadb-columnstore-shared (1.1.3) ...
Setting up mariadb-columnstore-storage-engine (1.1.3) ...
MariaDB ColumnStore install completed
Setting up mariadb-columnstore-test (1.1.3) ...
Setting up mariadb-columnstore-tokudb-engine (1.1.3) ...
Processing triggers for libc-bin (2.23-0ubuntu9) ..."
618,MCOL-304,MCOL,David Hill,104052,2017-12-04 23:43:42,"to get 1.1.3 repo packages off of buildbot master

ubuntu 16

apt-get install apt-transport-https
vi /etc/apt/sources.list.d/mariab-columnstore.list 
  deb http://54.172.106.13/repos/1.1.3/repo/ubuntu xenial main
  deb-src http://54.172.106.13/repos/1.1.3/repo/ubuntu xenial main

wget -qO - http://54.172.106.13/repos/1.1.3/repo/MariaDB-ColumnStore.gpg.key | sudo apt-key add -

apt-get update
apt-get install mariadb-columnstore*

debian 8

apt-get install apt-transport-https
vi /etc/apt/sources.list.d/mariab-columnstore.list 
  deb http://54.172.106.13/repos/1.1.3/repo/debian jessie main
  deb-src http://54.172.106.13/repos/1.1.3/repo/debian jessie main

wget -qO - http://54.172.106.13/repos/1.1.3/repo/MariaDB-ColumnStore.gpg.key | sudo apt-key add -

apt-get update
apt-get install mariadb-columnstore*


centos6

vi /etc/yum.repos.d/mariadb-columnstore.repo
[mariadb-columnstore]
name=MariaDB ColumnStore
baseurl=http://54.172.106.13/repos/1.1.3/yum/centos/6/x86_64
gpgkey=http://54.172.106.13/repos/1.1.3/repo/MariaDB-ColumnStore.gpg.key
gpgcheck=1
 
# and try install
yum --enablerepo=mariadb-columnstore clean metadata
yum groupinstall ""MariaDB ColumnStore""

centos7

vi /etc/yum.repos.d/mariadb-columnstore.repo
[mariadb-columnstore]
name=MariaDB ColumnStore
baseurl=http://54.172.106.13/repos/1.1.3/yum/centos/7/x86_64
gpgkey=http://54.172.106.13/repos/1.1.3/repo/MariaDB-ColumnStore.gpg.key
gpgcheck=1
 
# and try install
yum --enablerepo=mariadb-columnstore clean metadata
yum groupinstall ""MariaDB ColumnStore""


suse12

zypper addrepo -n mariadb-columnstore -t rpm-md http://54.172.106.13/repos/1.1.3/yum/sles/12/x86_64 mariadb-columnstore

rpm --import http://54.172.106.13/repos/1.1.3/repo/MariaDB-ColumnStore.gpg.key

zypper refresh
zypper install mariadb-columnstore*


",9,"to get 1.1.3 repo packages off of buildbot master

ubuntu 16

apt-get install apt-transport-URL
vi /etc/apt/sources.list.d/mariab-columnstore.list 
  deb URL xenial main
  deb-src URL xenial main

wget -qO - URL | sudo apt-key add -

apt-get update
apt-get install mariadb-columnstore*

debian 8

apt-get install apt-transport-URL
vi /etc/apt/sources.list.d/mariab-columnstore.list 
  deb URL jessie main
  deb-src URL jessie main

wget -qO - URL | sudo apt-key add -

apt-get update
apt-get install mariadb-columnstore*


centos6

vi /etc/yum.repos.d/mariadb-columnstore.repo
[mariadb-columnstore]
name=MariaDB ColumnStore
baseurl=URL
gpgkey=URL
gpgcheck=1
 
# and try install
yum --enablerepo=mariadb-columnstore clean metadata
yum groupinstall ""MariaDB ColumnStore""

centos7

vi /etc/yum.repos.d/mariadb-columnstore.repo
[mariadb-columnstore]
name=MariaDB ColumnStore
baseurl=URL
gpgkey=URL
gpgcheck=1
 
# and try install
yum --enablerepo=mariadb-columnstore clean metadata
yum groupinstall ""MariaDB ColumnStore""


suse12

zypper addrepo -n mariadb-columnstore -t rpm-md URL mariadb-columnstore

rpm --import URL

zypper refresh
zypper install mariadb-columnstore*


"
619,MCOL-304,MCOL,David Hill,104053,2017-12-04 23:48:19,"work to be done...

debian - 9 buildbot worker

try to get api build on all versions with repo
try to ge data-adaptors building on all versions with repo",10,"work to be done...

debian - 9 buildbot worker

try to get api build on all versions with repo
try to ge data-adaptors building on all versions with repo"
620,MCOL-304,MCOL,David Hill,104728,2017-12-19 23:27:09,"got debian 8 repo working, but debian 9 using the same keys repos the following error on the client server... keys again. what a pain


W: GPG error: http://54.172.106.13/repos/1.1.3/repo/debian9 stretch InRelease: The following signatures were invalid: 2561235D532E3CE9511B808591D0BE3BC12EBBF9
W: The repository 'http://54.172.106.13/repos/1.1.3/repo/debian9 stretch InRelease' is not signed.
N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration details.
root@debian-9-pm1:/var/lib/apt# 

I run this on the server

vi /etc/apt/sources.list.d/mariab-columnstore.list 
  deb http://54.172.106.13/repos/1.1.3/repo/debian stretch main
  deb-src http://54.172.106.13/repos/1.1.3/repo/debian stretch main

wget -qO - http://54.172.106.13/repos/1.1.3/repo/MariaDB-ColumnStore.gpg.key | sudo apt-key add -

apt-get update

and the key here matches the key in SignWith key...

:(",11,"got debian 8 repo working, but debian 9 using the same keys repos the following error on the client server... keys again. what a pain


W: GPG error: URL stretch InRelease: The following signatures were invalid: 2561235D532E3CE9511B808591D0BE3BC12EBBF9
W: The repository 'URL stretch InRelease' is not signed.
N: Data from such a repository can't be authenticated and is therefore potentially dangerous to use.
N: See apt-secure(8) manpage for repository creation and user configuration details.
root@debian-9-pm1:/var/lib/apt# 

I run this on the server

vi /etc/apt/sources.list.d/mariab-columnstore.list 
  deb URL stretch main
  deb-src URL stretch main

wget -qO - URL | sudo apt-key add -

apt-get update

and the key here matches the key in SignWith key...

:("
621,MCOL-304,MCOL,David Hill,104730,2017-12-19 23:48:43,"[root@ip-10-0-0-227 debian9]# gpg --list-keys --finger
/root/.gnupg/pubring.gpg
------------------------
pub   2048R/C12EBBF9 2017-04-19
      Key fingerprint = 2561 235D 532E 3CE9 511B  8085 91D0 BE3B C12E BBF9
uid                  David Hill (Columnstore Key) <support@mariadb.com>
sub   2048R/BA114A21 2017-04-19

pub   4096R/97254D1E 2017-04-27
      Key fingerprint = 0278 4156 F165 68FF C19E  2662 4084 AA18 9725 4D1E
uid                  david hill <support@mariadb.com>

[root@ip-10-0-0-227 debian9]# cat conf/distributions
Origin: MariaDB
Label: MariaDB Columnstore
Codename: stretch
Architectures: amd64 x86_64 source
Components: main
Description: MariaDB Columnstore Repository
SignWith: C12EBBF9

",12,"[root@ip-10-0-0-227 debian9]# gpg --list-keys --finger
/root/.gnupg/pubring.gpg
------------------------
pub   2048R/C12EBBF9 2017-04-19
      Key fingerprint = 2561 235D 532E 3CE9 511B  8085 91D0 BE3B C12E BBF9
uid                  David Hill (Columnstore Key) 
sub   2048R/BA114A21 2017-04-19

pub   4096R/97254D1E 2017-04-27
      Key fingerprint = 0278 4156 F165 68FF C19E  2662 4084 AA18 9725 4D1E
uid                  david hill 

[root@ip-10-0-0-227 debian9]# cat conf/distributions
Origin: MariaDB
Label: MariaDB Columnstore
Codename: stretch
Architectures: amd64 x86_64 source
Components: main
Description: MariaDB Columnstore Repository
SignWith: C12EBBF9

"
622,MCOL-304,MCOL,David Hill,105175,2018-01-02 23:37:12,"still a few issues

centos 6 install

MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-platform-1.1.3-1.x86_64                                                     7/8 
/usr/local/mariadb/columnstore/bin/cplogger: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory
The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distributed option:

/usr/local/mariadb/columnstore/bin/columnstore start


centos 7

groupinstall now not showing all the packages

sudo yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirror.rackspace.com
 * epel: mirror.steadfast.net
 * extras: bay.uchicago.edu
 * updates: mirrors.umflint.edu
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-tokudb-engine.x86_64 0:1.1.2-1.el7.centos will be installed
--> Processing Dependency: MariaDB = 1.1.2-1.el7.centos for package: mariadb-columnstore-tokudb-engine-1.1.2-1.el7.centos.x86_64
--> Running transaction check
---> Package mariadb-columnstore-server.x86_64 0:1.1.2-1.el7.centos will be installed
--> Processing Dependency: mariadb-columnstore-common for package: mariadb-columnstore-server-1.1.2-1.el7.centos.x86_64
--> Processing Dependency: mariadb-columnstore-client for package: mariadb-columnstore-server-1.1.2-1.el7.centos.x86_64
--> Running transaction check
---> Package mariadb-columnstore-client.x86_64 0:1.1.2-1.el7.centos will be installed
---> Package mariadb-columnstore-common.x86_64 0:1.1.2-1.el7.centos will be installed
--> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================
 Package                                  Arch          Version                   Repository                  Size
===================================================================================================================
Installing for group upgrade ""MariaDB ColumnStore"":
 mariadb-columnstore-tokudb-engine        x86_64        1.1.2-1.el7.centos        mariadb-columnstore        5.0 M
Installing for dependencies:
 mariadb-columnstore-client               x86_64        1.1.2-1.el7.centos        mariadb-columnstore         12 M
 mariadb-columnstore-common               x86_64        1.1.2-1.el7.centos        mariadb-columnstore        154 k
 mariadb-columnstore-server               x86_64        1.1.2-1.el7.centos        mariadb-columnstore         69 M

Transaction Summary
===================================================================================================================
Install  1 Package (+3 Dependent packages)

Total download size: 86 M
Installed size: 386 M



",13,"still a few issues

centos 6 install

MariaDB ColumnStore RPM install completed
  Installing : mariadb-columnstore-platform-1.1.3-1.x86_64                                                     7/8 
/usr/local/mariadb/columnstore/bin/cplogger: error while loading shared libraries: libmariadb.so.3: cannot open shared object file: No such file or directory
The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distributed option:

/usr/local/mariadb/columnstore/bin/columnstore start


centos 7

groupinstall now not showing all the packages

sudo yum groupinstall ""MariaDB ColumnStore""
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirror.rackspace.com
 * epel: mirror.steadfast.net
 * extras: bay.uchicago.edu
 * updates: mirrors.umflint.edu
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-tokudb-engine.x86_64 0:1.1.2-1.el7.centos will be installed
--> Processing Dependency: MariaDB = 1.1.2-1.el7.centos for package: mariadb-columnstore-tokudb-engine-1.1.2-1.el7.centos.x86_64
--> Running transaction check
---> Package mariadb-columnstore-server.x86_64 0:1.1.2-1.el7.centos will be installed
--> Processing Dependency: mariadb-columnstore-common for package: mariadb-columnstore-server-1.1.2-1.el7.centos.x86_64
--> Processing Dependency: mariadb-columnstore-client for package: mariadb-columnstore-server-1.1.2-1.el7.centos.x86_64
--> Running transaction check
---> Package mariadb-columnstore-client.x86_64 0:1.1.2-1.el7.centos will be installed
---> Package mariadb-columnstore-common.x86_64 0:1.1.2-1.el7.centos will be installed
--> Finished Dependency Resolution

Dependencies Resolved

===================================================================================================================
 Package                                  Arch          Version                   Repository                  Size
===================================================================================================================
Installing for group upgrade ""MariaDB ColumnStore"":
 mariadb-columnstore-tokudb-engine        x86_64        1.1.2-1.el7.centos        mariadb-columnstore        5.0 M
Installing for dependencies:
 mariadb-columnstore-client               x86_64        1.1.2-1.el7.centos        mariadb-columnstore         12 M
 mariadb-columnstore-common               x86_64        1.1.2-1.el7.centos        mariadb-columnstore        154 k
 mariadb-columnstore-server               x86_64        1.1.2-1.el7.centos        mariadb-columnstore         69 M

Transaction Summary
===================================================================================================================
Install  1 Package (+3 Dependent packages)

Total download size: 86 M
Installed size: 386 M



"
623,MCOL-304,MCOL,Sasha V,105211,2018-01-03 21:40:49,"Note that the Ubuntu 16 URL for 1.1.3 repo packages for /etc/apt/sources.list.d/mariab-columnstore.list 
{noformat}
deb http://54.172.106.13/repos/1.1.3/repo/ubuntu xenial main
{noformat}
is currently at
{noformat}
deb http://54.172.106.13/repos/1.1.3/repo/ubuntu16 xenial main
{noformat}",14,"Note that the Ubuntu 16 URL for 1.1.3 repo packages for /etc/apt/sources.list.d/mariab-columnstore.list 
{noformat}
deb URL xenial main
{noformat}
is currently at
{noformat}
deb URL xenial main
{noformat}"
624,MCOL-304,MCOL,David Hill,105245,2018-01-04 16:33:33,"resolved the centos 7 licence key issue...

Looking into the deb packages issue. during the repo script execution, the actual name of the packages are being changed... Need to investigate which package name we need for the finial repo releasing.

name from our build is:

   mariadb-columnstore-1.1.2-1-x86_64-stretch-client.deb

name after going though the repo scripts:

   mariadb-columnstore-client_1.1.2_amd64.deb",15,"resolved the centos 7 licence key issue...

Looking into the deb packages issue. during the repo script execution, the actual name of the packages are being changed... Need to investigate which package name we need for the finial repo releasing.

name from our build is:

   mariadb-columnstore-1.1.2-1-x86_64-stretch-client.deb

name after going though the repo scripts:

   mariadb-columnstore-client_1.1.2_amd64.deb"
625,MCOL-304,MCOL,Sasha V,105267,2018-01-04 23:55:17,"I tried multi-server install with Ubuntu 16 URL for 1.1.3 repo packages. The install step was successfull, with packages downloaded like
/var/cache/apt/archives/mariadb-columnstore-client_1.1.3_amd64.deb
The postConfigure step got affected by few issues. First, it looks for:
{noformat}
Performing a MariaDB ColumnStore System install using using DEB packages located in the /root directory.
ls: cannot access '/root/mariadb-columnstore-1.1.3-1*.deb': No such file or directory

 Error: can't locate /root/mariadb-columnstore-1.1.3-1*.deb Package in directory /root
{noformat}
Second, with packages renamed as expected and placed in the /root directory, the scp to other nodes failed because package_installer.sh has
{noformat}
send ""scp -v $HOME/mariadb-columnstore*$VERSION*$PKGTYPE $USERNAME@$SERVER:.\n""
{noformat}
https://github.com/mariadb-corporation/mariadb-columnstore-engine/blob/1.1-merge-up/oam/install_scripts/package_installer.sh#L144
and in my environment, $HOME has not been set to /root
",16,"I tried multi-server install with Ubuntu 16 URL for 1.1.3 repo packages. The install step was successfull, with packages downloaded like
/var/cache/apt/archives/mariadb-columnstore-client_1.1.3_amd64.deb
The postConfigure step got affected by few issues. First, it looks for:
{noformat}
Performing a MariaDB ColumnStore System install using using DEB packages located in the /root directory.
ls: cannot access '/root/mariadb-columnstore-1.1.3-1*.deb': No such file or directory

 Error: can't locate /root/mariadb-columnstore-1.1.3-1*.deb Package in directory /root
{noformat}
Second, with packages renamed as expected and placed in the /root directory, the scp to other nodes failed because package_installer.sh has
{noformat}
send ""scp -v $HOME/mariadb-columnstore*$VERSION*$PKGTYPE $USERNAME@$SERVER:.\n""
{noformat}
URL
and in my environment, $HOME has not been set to /root
"
626,MCOL-304,MCOL,David Hill,105290,2018-01-05 21:07:25,"centos 7 api install working

yum install mariadb-columnstore-api
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirror.steadfast.net
 * epel: mirror.steadfast.net
 * extras: mirror.tzulo.com
 * updates: repos.forethought.net
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-api.x86_64 0:1.1.2-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

======================================================================================================================
 Package                             Arch               Version             Repository                           Size
======================================================================================================================
Installing:
 mariadb-columnstore-api             x86_64             1.1.2-1             mariadb-columnstore-api             184 k

Transaction Summary
======================================================================================================================
Install  1 Package

Total download size: 184 k
Installed size: 900 k
Is this ok [y/d/N]: y
Downloading packages:
mariadb-columnstore-api-1.1.2-1-x86_64-centos7.rpm                                             | 184 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : mariadb-columnstore-api-1.1.2-1.x86_64                                                             1/1 
  Verifying  : mariadb-columnstore-api-1.1.2-1.x86_64                                                             1/1 

Installed:
  mariadb-columnstore-api.x86_64 0:1.1.2-1  ",17,"centos 7 api install working

yum install mariadb-columnstore-api
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirror.steadfast.net
 * epel: mirror.steadfast.net
 * extras: mirror.tzulo.com
 * updates: repos.forethought.net
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-api.x86_64 0:1.1.2-1 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

======================================================================================================================
 Package                             Arch               Version             Repository                           Size
======================================================================================================================
Installing:
 mariadb-columnstore-api             x86_64             1.1.2-1             mariadb-columnstore-api             184 k

Transaction Summary
======================================================================================================================
Install  1 Package

Total download size: 184 k
Installed size: 900 k
Is this ok [y/d/N]: y
Downloading packages:
mariadb-columnstore-api-1.1.2-1-x86_64-centos7.rpm                                             | 184 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : mariadb-columnstore-api-1.1.2-1.x86_64                                                             1/1 
  Verifying  : mariadb-columnstore-api-1.1.2-1.x86_64                                                             1/1 

Installed:
  mariadb-columnstore-api.x86_64 0:1.1.2-1  "
627,MCOL-304,MCOL,David Hill,105291,2018-01-05 21:16:36,"Error with centos 7 data adapters package installed... libcdc_connentor is installed, so not sure why the error

[root@centos-7-yum-pm1 ~]# yum groupinstall mariadb-columnstore-data-adapters
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirror.steadfast.net
 * epel: mirror.steadfast.net
 * extras: mirror.tzulo.com
 * updates: repos.forethought.net
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-kafka-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: librdkafka++.so.1()(64bit) for package: mariadb-columnstore-kafka-adapters-1.1.2-1.x86_64
---> Package mariadb-columnstore-maxscale-cdc-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: libcdc_connector.so.1.0.0()(64bit) for package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64
--> Running transaction check
---> Package librdkafka.x86_64 0:0.11.1-1.el7 will be installed
--> Processing Dependency: liblz4.so.1()(64bit) for package: librdkafka-0.11.1-1.el7.x86_64
---> Package mariadb-columnstore-maxscale-cdc-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: libcdc_connector.so.1.0.0()(64bit) for package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64
--> Running transaction check
---> Package lz4.x86_64 0:1.7.3-1.el7 will be installed
---> Package mariadb-columnstore-maxscale-cdc-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: libcdc_connector.so.1.0.0()(64bit) for package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64
--> Finished Dependency Resolution
Error: Package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64 (mariadb-columnstore-data-adapters)
           Requires: libcdc_connector.so.1.0.0()(64bit)
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
[root@centos-7-yum-pm1 ~]# find // -name libcdc_connector.so.1.0.0
//root/maxscale-cdc-connector/build/libcdc_connector.so.1.0.0
//usr/lib64/libcdc_connector.so.1.0.0
[root@centos-7-yum-pm1 ~]# ",18,"Error with centos 7 data adapters package installed... libcdc_connentor is installed, so not sure why the error

[root@centos-7-yum-pm1 ~]# yum groupinstall mariadb-columnstore-data-adapters
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirror.steadfast.net
 * epel: mirror.steadfast.net
 * extras: mirror.tzulo.com
 * updates: repos.forethought.net
Resolving Dependencies
--> Running transaction check
---> Package mariadb-columnstore-kafka-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: librdkafka++.so.1()(64bit) for package: mariadb-columnstore-kafka-adapters-1.1.2-1.x86_64
---> Package mariadb-columnstore-maxscale-cdc-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: libcdc_connector.so.1.0.0()(64bit) for package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64
--> Running transaction check
---> Package librdkafka.x86_64 0:0.11.1-1.el7 will be installed
--> Processing Dependency: liblz4.so.1()(64bit) for package: librdkafka-0.11.1-1.el7.x86_64
---> Package mariadb-columnstore-maxscale-cdc-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: libcdc_connector.so.1.0.0()(64bit) for package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64
--> Running transaction check
---> Package lz4.x86_64 0:1.7.3-1.el7 will be installed
---> Package mariadb-columnstore-maxscale-cdc-adapters.x86_64 0:1.1.2-1 will be installed
--> Processing Dependency: libcdc_connector.so.1.0.0()(64bit) for package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64
--> Finished Dependency Resolution
Error: Package: mariadb-columnstore-maxscale-cdc-adapters-1.1.2-1.x86_64 (mariadb-columnstore-data-adapters)
           Requires: libcdc_connector.so.1.0.0()(64bit)
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
[root@centos-7-yum-pm1 ~]# find // -name libcdc_connector.so.1.0.0
//root/maxscale-cdc-connector/build/libcdc_connector.so.1.0.0
//usr/lib64/libcdc_connector.so.1.0.0
[root@centos-7-yum-pm1 ~]# "
628,MCOL-304,MCOL,David Hill,105292,2018-01-05 21:31:20,Will need a yum and apt-get repo for libcdc_connector to get the data adapters to install,19,Will need a yum and apt-get repo for libcdc_connector to get the data adapters to install
629,MCOL-304,MCOL,David Hill,105502,2018-01-10 20:37:11,"api and data-adapters repos are done for 1.1.2.. Document in same doc with columnstore

https://mariadb.com/kb/en/library/installing-mariadb-columnstore-from-the-package-repositories/

",20,"api and data-adapters repos are done for 1.1.2.. Document in same doc with columnstore

URL

"
630,MCOL-304,MCOL,David Hill,107897,2018-03-05 17:26:03,completed with the 1.1.3 yum/apt-get repos becoming available at GA,21,completed with the 1.1.3 yum/apt-get repos becoming available at GA
631,MCOL-305,MCOL,Andrew Hutchings,86711,2016-09-22 13:36:31,Did a test merge with the current 10.1 tree and it appears to be a clean merge,1,Did a test merge with the current 10.1 tree and it appears to be a clean merge
632,MCOL-305,MCOL,Andrew Hutchings,86986,2016-10-02 15:34:39,Doesn't appear to have broken anything in my tests,2,Doesn't appear to have broken anything in my tests
633,MCOL-305,MCOL,David Hall,87003,2016-10-03 13:18:18,Any breakage should show up in normal regression tests. ,3,Any breakage should show up in normal regression tests. 
634,MCOL-307,MCOL,David Thompson,87769,2016-10-25 20:38:01,"This should be implemented as a set of mcsadmin commands to support:
- starting (asynchronously)
- retrieving status
- stopping

This should disable writes while running. Not sure if it's realistic or not to consider concurrent queries?",1,"This should be implemented as a set of mcsadmin commands to support:
- starting (asynchronously)
- retrieving status
- stopping

This should disable writes while running. Not sure if it's realistic or not to consider concurrent queries?"
635,MCOL-307,MCOL,David Hall,88314,2016-11-14 20:24:14,Added command 4 - Redistribute to the mcsadmin code,2,Added command 4 - Redistribute to the mcsadmin code
636,MCOL-307,MCOL,David Hall,88315,2016-11-14 20:25:04,"I couldn't get this thing into review mode, so it's in test for now. DHill, please review.",3,"I couldn't get this thing into review mode, so it's in test for now. DHill, please review."
637,MCOL-307,MCOL,David Hill,89068,2016-12-02 15:40:33,"reviewed completed...

Needs some test cases from David Hall....

2 of them should be: dbroot assigned to pm1, and dbroot assigned to another pm..

",4,"reviewed completed...

Needs some test cases from David Hall....

2 of them should be: dbroot assigned to pm1, and dbroot assigned to another pm..

"
638,MCOL-307,MCOL,David Hill,89253,2016-12-06 20:23:41,"Did recommend changing the command name from redistribute to redistributeData

and here is a link to the document, which will be publish in 1.0.6 release

https://mariadb.com/kb/en/mariadb/columnstore-redistribute-data/",5,"Did recommend changing the command name from redistribute to redistributeData

and here is a link to the document, which will be publish in 1.0.6 release

URL"
639,MCOL-307,MCOL,David Hall,89267,2016-12-06 23:20:25,"Name changed to redistributeData.

Because this command works at partition granularity, it won't move any data for smaller tables (See documentation).

There are three tests that make sense. Start a multi PM system. 

1) Add large amounts of data using cpimport mode 2 or 3 to create an unbalanced table (or more). Run redistributeData and watch the result.

For example using lineitem, run this query before and after to see how many rows moved. The counts should be about the same (+/- 64000000).
select count(*), idbdbroot(l_orderkey) from lineitem group by idbdbroot(l_orderkey);

2) Start with a multi PM system. Have an extra PM ready to add (or at least an extra dbroot or two). Add a bunch of data, then add the new PM in and run redistributeData. See what happens.

3). After test (2), run redistributeData again, but with ""remove"" and the newly added dbroots. It should empty those new roots, moving the data to the old dbroots.",6,"Name changed to redistributeData.

Because this command works at partition granularity, it won't move any data for smaller tables (See documentation).

There are three tests that make sense. Start a multi PM system. 

1) Add large amounts of data using cpimport mode 2 or 3 to create an unbalanced table (or more). Run redistributeData and watch the result.

For example using lineitem, run this query before and after to see how many rows moved. The counts should be about the same (+/- 64000000).
select count(*), idbdbroot(l_orderkey) from lineitem group by idbdbroot(l_orderkey);

2) Start with a multi PM system. Have an extra PM ready to add (or at least an extra dbroot or two). Add a bunch of data, then add the new PM in and run redistributeData. See what happens.

3). After test (2), run redistributeData again, but with ""remove"" and the newly added dbroots. It should empty those new roots, moving the data to the old dbroots."
640,MCOL-307,MCOL,Daniel Lee,89326,2016-12-07 18:31:09,"Per discussion on the Slack channel today, redistributeData does not check for database write suspension for now.  It will run as the use requests it.  That set's the testing scope for QA for testing this feature for this release.",7,"Per discussion on the Slack channel today, redistributeData does not check for database write suspension for now.  It will run as the use requests it.  That set's the testing scope for QA for testing this feature for this release."
641,MCOL-307,MCOL,Daniel Lee,89334,2016-12-08 00:19:09,"Build tested: 1.0.6-1

mcsadmin> getsoft
getsoftwareinfo   Wed Dec  7 19:15:31 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.6
Release     : 1
Architecture: x86_64
Install Date: Wed 07 Dec 2016 03:17:06 PM EST
Group       : Applications/Databases
Size        : 10017001
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.6-1.src.rpm
Build Date  : Wed 07 Dec 2016 02:08:09 PM EST

""START REMOVE"" does move any data.  It finished immediately

[6:10]  
mcsadmin> redistributedata start remove 3
redistributedata   Wed Dec  7 19:07:42 2016
redistributeData START     Removing dbroots: 3
Source dbroots: 1 2 3 4
Destination dbroots: 1 2 4

WriteEngineServer returned status 1: Cleared.
WriteEngineServer returned status 2: Redistribute is started.
mcsadmin> redistributedata status
redistributedata   Wed Dec  7 19:07:47 2016
WriteEngineServer returned status 3: Redistribute is finished.
0 success, 0 skipped, 0 failed.
Total time: 0 seconds.

[6:10]  
+-----------+-----------------------+
| count(*)  | idbdbroot(l_orderkey) |
+-----------+-----------------------+
| 332115454 |                     2 |
| 319282160 |                     4 |
| 336056627 |                     1 |
| 338592834 |                     3 |
+-----------+-----------------------+
4 rows in set (8.80 sec)

In the KB article, this statement ""Even TRUNCATE doesn't really get rid of the partition, though the space may eventually get re-used. DROP PARTITION will affect the balancing of Redistribute."" is not correct.  TRUNCATE drops all files for the columns and pre allocate each column with an abbreviated extent file.



",8,"Build tested: 1.0.6-1

mcsadmin> getsoft
getsoftwareinfo   Wed Dec  7 19:15:31 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.6
Release     : 1
Architecture: x86_64
Install Date: Wed 07 Dec 2016 03:17:06 PM EST
Group       : Applications/Databases
Size        : 10017001
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.6-1.src.rpm
Build Date  : Wed 07 Dec 2016 02:08:09 PM EST

""START REMOVE"" does move any data.  It finished immediately

[6:10]  
mcsadmin> redistributedata start remove 3
redistributedata   Wed Dec  7 19:07:42 2016
redistributeData START     Removing dbroots: 3
Source dbroots: 1 2 3 4
Destination dbroots: 1 2 4

WriteEngineServer returned status 1: Cleared.
WriteEngineServer returned status 2: Redistribute is started.
mcsadmin> redistributedata status
redistributedata   Wed Dec  7 19:07:47 2016
WriteEngineServer returned status 3: Redistribute is finished.
0 success, 0 skipped, 0 failed.
Total time: 0 seconds.

[6:10]  
+-----------+-----------------------+
| count(*)  | idbdbroot(l_orderkey) |
+-----------+-----------------------+
| 332115454 |                     2 |
| 319282160 |                     4 |
| 336056627 |                     1 |
| 338592834 |                     3 |
+-----------+-----------------------+
4 rows in set (8.80 sec)

In the KB article, this statement ""Even TRUNCATE doesn't really get rid of the partition, though the space may eventually get re-used. DROP PARTITION will affect the balancing of Redistribute."" is not correct.  TRUNCATE drops all files for the columns and pre allocate each column with an abbreviated extent file.



"
642,MCOL-307,MCOL,Daniel Lee,89335,2016-12-08 00:20:56,reopen per my last comment.,9,reopen per my last comment.
643,MCOL-307,MCOL,David Hall,89357,2016-12-08 16:29:42,"The KB has been changed to remove the comment about TRUNCATE.

All references to start remove have been removed from the help file and KB.
MCOL-455 has been opened to address the start remove issue.

Closing this.",10,"The KB has been changed to remove the comment about TRUNCATE.

All references to start remove have been removed from the help file and KB.
MCOL-455 has been opened to address the start remove issue.

Closing this."
644,MCOL-307,MCOL,David Hall,89360,2016-12-08 17:28:22,Needs testing,11,Needs testing
645,MCOL-307,MCOL,David Hall,89361,2016-12-08 17:30:45,Fixed the problem with the help file,12,Fixed the problem with the help file
646,MCOL-307,MCOL,Daniel Lee,89364,2016-12-08 18:37:51,"Build verified: 1.0.6-1 Github source

[root@localhost mariadb-columnstore-server]# git show
commit 4c075221724fb8dfc0c1d51e223343a0b3f2ee64
Author: david hill <david.hill@mariadb.com>
Date:   Thu Dec 8 09:42:04 2016 -0600

    Update README.md

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 9152d920eba16e84674376ef4049f313c15e3fdc
Merge: d6717cc f5532d5
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Thu Dec 8 11:30:02 2016 -0600

    Merge pull request #76 from mariadb-corporation/MCOL-307
    
    MCOL-307 ConsoleCmds.xml needs to have sequential arg numbers

mcsadmin> redistributedata
redistributedata   Thu Dec  8 18:30:24 2016
redistributeData must have one of START, STOP or STATUS
mcsadmin> help redistributedata
help   Thu Dec  8 18:30:30 2016

   Command:     redistributeData

   Description: Redistribute table data accross all dbroots to balance disk usage

   Arguments:   START to begin a redistribution
                STOP to stop redistribution before completion
                STATUS to to view statistics and progress


Also verified KB article regarding TRUNCATE.

""Deleted records still take up space, so deleting a bunch of rows won't affect Redistribute.""

The ""START REMOVE"" issue is not being tracked by ticket MCOL-455.  Closing this ticket.
",13,"Build verified: 1.0.6-1 Github source

[root@localhost mariadb-columnstore-server]# git show
commit 4c075221724fb8dfc0c1d51e223343a0b3f2ee64
Author: david hill 
Date:   Thu Dec 8 09:42:04 2016 -0600

    Update README.md

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 9152d920eba16e84674376ef4049f313c15e3fdc
Merge: d6717cc f5532d5
Author: dhall-InfiniDB 
Date:   Thu Dec 8 11:30:02 2016 -0600

    Merge pull request #76 from mariadb-corporation/MCOL-307
    
    MCOL-307 ConsoleCmds.xml needs to have sequential arg numbers

mcsadmin> redistributedata
redistributedata   Thu Dec  8 18:30:24 2016
redistributeData must have one of START, STOP or STATUS
mcsadmin> help redistributedata
help   Thu Dec  8 18:30:30 2016

   Command:     redistributeData

   Description: Redistribute table data accross all dbroots to balance disk usage

   Arguments:   START to begin a redistribution
                STOP to stop redistribution before completion
                STATUS to to view statistics and progress


Also verified KB article regarding TRUNCATE.

""Deleted records still take up space, so deleting a bunch of rows won't affect Redistribute.""

The ""START REMOVE"" issue is not being tracked by ticket MCOL-455.  Closing this ticket.
"
647,MCOL-309,MCOL,David Thompson,87768,2016-10-25 20:36:16,"Should look at info_schema as maybe the best approach but if not implement as an mcsadmin function e.g. runSizeReport. If the latter i think simple argument capability of (both optional):
- Database name (including wildcard)
- Table name (including wildcard)
would suffice. The output should be something that is easily parsable and cut/pastable in case someone wants to run further shell analytics and / or import to a spreadsheet with no manual editing.

",1,"Should look at info_schema as maybe the best approach but if not implement as an mcsadmin function e.g. runSizeReport. If the latter i think simple argument capability of (both optional):
- Database name (including wildcard)
- Table name (including wildcard)
would suffice. The output should be something that is easily parsable and cut/pastable in case someone wants to run further shell analytics and / or import to a spreadsheet with no manual editing.

"
648,MCOL-309,MCOL,Andrew Hutchings,88146,2016-11-07 14:43:08,"I first believed ""calpontsys"" might be the best place for this. Looking at historical information this database is used as a replacement for information_schema specifically for InfiniDB/ColumnStore tables. It is implemented in an InfiniDB/ColumnStore specific way and already contains some of what we need for this feature.

Digging deeper I find the calponsys is not fully implemented and the name doesn't fit the general direction of ColumnStore. I'll implement as information_schema and at some point we should retire calpontsys.",2,"I first believed ""calpontsys"" might be the best place for this. Looking at historical information this database is used as a replacement for information_schema specifically for InfiniDB/ColumnStore tables. It is implemented in an InfiniDB/ColumnStore specific way and already contains some of what we need for this feature.

Digging deeper I find the calponsys is not fully implemented and the name doesn't fit the general direction of ColumnStore. I'll implement as information_schema and at some point we should retire calpontsys."
649,MCOL-309,MCOL,David Thompson,88153,2016-11-07 18:49:24,If we can make info schema work without too much contortions i think that's the best choice.,3,If we can make info schema work without too much contortions i think that's the best choice.
650,MCOL-309,MCOL,Andrew Hutchings,88306,2016-11-14 15:47:26,"Requesting review from Hall, but [~dthompson] might want to take a peek too.

Suggest it comes back to me before closing so that I can add documentation.",4,"Requesting review from Hall, but [~dthompson] might want to take a peek too.

Suggest it comes back to me before closing so that I can add documentation."
651,MCOL-309,MCOL,David Thompson,88802,2016-11-29 00:22:12,Closing this portion of the work for 1.0.5.,5,Closing this portion of the work for 1.0.5.
652,MCOL-311,MCOL,David Thompson,87767,2016-10-25 20:32:56,"This should be implemented as an mcsadmin function, e.g named findObjectFile. It should take the following set of arguments (either / or):
- OID
- Database Name, Table Name (performing an intermediate OID lookup).

",1,"This should be implemented as an mcsadmin function, e.g named findObjectFile. It should take the following set of arguments (either / or):
- OID
- Database Name, Table Name (performing an intermediate OID lookup).

"
653,MCOL-311,MCOL,David Hall,88324,2016-11-14 23:56:56,For review,2,For review
654,MCOL-311,MCOL,David Thompson,88645,2016-11-24 03:33:39,Pull request was never merged..,3,Pull request was never merged..
655,MCOL-311,MCOL,David Hill,88668,2016-11-24 14:54:09,"reviewed, all looks good",4,"reviewed, all looks good"
656,MCOL-311,MCOL,David Hill,88963,2016-11-30 21:58:59,needs to be merged,5,needs to be merged
657,MCOL-311,MCOL,Daniel Lee,89025,2016-12-01 22:46:55,"Build tested: nightly build targeted for 1.0.6

[root@localhost mariadb-columnstore-server]# git status
# On branch develop
nothing to commit, working directory clean
[root@localhost mariadb-columnstore-server]# dit show
-bash: dit: command not found
[root@localhost mariadb-columnstore-server]# git show
commit 3795bd4cf42d59b792c473101703911fb53e9297
Merge: 570184c 84714c9
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Wed Nov 30 11:42:14 2016 -0600

    Merge pull request #18 from mariadb-corporation/MCOL-424
    
    MCOL-424 Disable indexes for cross-engine

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 691c52ca3364d968d2fa378284e45aca694785ec
Merge: d4d9be6 fbb02f5
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Thu Dec 1 11:57:45 2016 -0600

    Merge pull request #64 from mariadb-corporation/MCOL-307
    
    MCOL-307 Need path to we_redistributedef.h. The path isn't in everyon…

----------
Found few issues as the following:

1) incorrect command name of ""getDatafileName"" is returned

Current:

mcsadmin> findobjectfile
findobjectfile   Thu Dec  1 21:49:17 2016

getDatafileName requires one of
a) oid of column for which file name is to be retrieved
b) schema, table and column for which file name is to be retrieved


2) When supplied with an table object ID, which is not associated with any data file, it still outputs an directory.  ""OID directory not found"" is too alarming, suggesting that a directory is missing.  Also ""file name"" should be ""Path"" or 'Directory name"".

Current:

mcsadmin> findobjectfile 3004
findobjectfile   Thu Dec  1 21:50:10 2016
file name for oid 3004:
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/011.dir/188.dir (OID directory not found)

Suggestion:

OID=3004 Object=[schemaName.tableName]
This command returns the directory for a column only.

3) When supplied with an invalid OID, it should indicate as such.

Current:

mcsadmin> findobjectfile 30050
findobjectfile   Thu Dec  1 22:04:13 2016
file name for oid 30050:
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/117.dir/098.dir (OID directory not found)

Suggestion:
OID=30050 Object=n/a
The supplied OID is invalid.

4) When column OID is supplied, we should return the object name as a confirmation for the user, since the OID itself is meaningless.

Current: 

findobjectfile   Thu Dec  1 21:51:18 2016
file name for oid 3005:
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/011.dir/189.dir

Suggestion:
OID=3005 Object=[schemaName.tableName.columnName]
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/011.dir/189.dir

",6,"Build tested: nightly build targeted for 1.0.6

[root@localhost mariadb-columnstore-server]# git status
# On branch develop
nothing to commit, working directory clean
[root@localhost mariadb-columnstore-server]# dit show
-bash: dit: command not found
[root@localhost mariadb-columnstore-server]# git show
commit 3795bd4cf42d59b792c473101703911fb53e9297
Merge: 570184c 84714c9
Author: dhall-InfiniDB 
Date:   Wed Nov 30 11:42:14 2016 -0600

    Merge pull request #18 from mariadb-corporation/MCOL-424
    
    MCOL-424 Disable indexes for cross-engine

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 691c52ca3364d968d2fa378284e45aca694785ec
Merge: d4d9be6 fbb02f5
Author: dhall-InfiniDB 
Date:   Thu Dec 1 11:57:45 2016 -0600

    Merge pull request #64 from mariadb-corporation/MCOL-307
    
    MCOL-307 Need path to we_redistributedef.h. The path isn't in everyon…

----------
Found few issues as the following:

1) incorrect command name of ""getDatafileName"" is returned

Current:

mcsadmin> findobjectfile
findobjectfile   Thu Dec  1 21:49:17 2016

getDatafileName requires one of
a) oid of column for which file name is to be retrieved
b) schema, table and column for which file name is to be retrieved


2) When supplied with an table object ID, which is not associated with any data file, it still outputs an directory.  ""OID directory not found"" is too alarming, suggesting that a directory is missing.  Also ""file name"" should be ""Path"" or 'Directory name"".

Current:

mcsadmin> findobjectfile 3004
findobjectfile   Thu Dec  1 21:50:10 2016
file name for oid 3004:
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/011.dir/188.dir (OID directory not found)

Suggestion:

OID=3004 Object=[schemaName.tableName]
This command returns the directory for a column only.

3) When supplied with an invalid OID, it should indicate as such.

Current:

mcsadmin> findobjectfile 30050
findobjectfile   Thu Dec  1 22:04:13 2016
file name for oid 30050:
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/117.dir/098.dir (OID directory not found)

Suggestion:
OID=30050 Object=n/a
The supplied OID is invalid.

4) When column OID is supplied, we should return the object name as a confirmation for the user, since the OID itself is meaningless.

Current: 

findobjectfile   Thu Dec  1 21:51:18 2016
file name for oid 3005:
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/011.dir/189.dir

Suggestion:
OID=3005 Object=[schemaName.tableName.columnName]
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/011.dir/189.dir

"
658,MCOL-311,MCOL,David Hall,89268,2016-12-06 23:40:02,Made the changes as requested by QA. There's a new pull request.,7,Made the changes as requested by QA. There's a new pull request.
659,MCOL-311,MCOL,David Hill,89269,2016-12-07 00:48:13,"review looks good based on Daniels request...

Testing, I asked this before so need to make sure what happens in the test phase.

Does this command on a separate system when run on a um and a pm asking for the same info on a OID...",8,"review looks good based on Daniels request...

Testing, I asked this before so need to make sure what happens in the test phase.

Does this command on a separate system when run on a um and a pm asking for the same info on a OID..."
660,MCOL-311,MCOL,Daniel Lee,89323,2016-12-07 17:53:21,"Build verified: Github source
[root@localhost mariadb-columnstore-server]# git show
commit 8592d353c5477940f9600566639302de9fa994c7
Merge: 3795bd4 7af4e57
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date: Tue Dec 6 09:49:03 2016 -0600
Merge pull request #20 from mariadb-corporation/MCOL-441
MCOL-441 Fix segfault on SP error
[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 7a8322dc28471b830aca243698cd7fce5bc4401c
Merge: 5c0ced8 9b6beb4
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date: Wed Dec 7 10:10:22 2016 -0600
Merge pull request #73 from mariadb-corporation/MCOL-435
Mcol 435

Still having couple issues.

1) table object returned the first column of the table.

mcsadmin> findobjectfile 3147
findobjectfile   Wed Dec  7 17:50:34 2016
for table OID 3147:
column OID 3148 tpch1c.nation.n_nationkey
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/012.dir/076.dir

2) OID for dictionary column treated as invalid OID (3153 is the OID for n_comment dictionary)
mcsadmin> findobjectfile 3153
findobjectfile   Wed Dec  7 17:41:43 2016
OID 3153 does not represent a table or column in columnstore
",9,"Build verified: Github source
[root@localhost mariadb-columnstore-server]# git show
commit 8592d353c5477940f9600566639302de9fa994c7
Merge: 3795bd4 7af4e57
Author: dhall-InfiniDB 
Date: Tue Dec 6 09:49:03 2016 -0600
Merge pull request #20 from mariadb-corporation/MCOL-441
MCOL-441 Fix segfault on SP error
[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 7a8322dc28471b830aca243698cd7fce5bc4401c
Merge: 5c0ced8 9b6beb4
Author: dhall-InfiniDB 
Date: Wed Dec 7 10:10:22 2016 -0600
Merge pull request #73 from mariadb-corporation/MCOL-435
Mcol 435

Still having couple issues.

1) table object returned the first column of the table.

mcsadmin> findobjectfile 3147
findobjectfile   Wed Dec  7 17:50:34 2016
for table OID 3147:
column OID 3148 tpch1c.nation.n_nationkey
/usr/local/mariadb/columnstore/data1/000.dir/000.dir/012.dir/076.dir

2) OID for dictionary column treated as invalid OID (3153 is the OID for n_comment dictionary)
mcsadmin> findobjectfile 3153
findobjectfile   Wed Dec  7 17:41:43 2016
OID 3153 does not represent a table or column in columnstore
"
661,MCOL-311,MCOL,Daniel Lee,89324,2016-12-07 17:54:15,"Per my last comment, couple issues identified.",10,"Per my last comment, couple issues identified."
662,MCOL-311,MCOL,David Hall,89327,2016-12-07 18:56:02,"I opened MCOL-451 to address the dictionary oid issue because it most likely won't be in 1.0.6.

The complaint about table oids returning the first column file name. This is intentional (though not documented). What behavior should it be for a table oid? There are no data files associated with a table, only with columns.",11,"I opened MCOL-451 to address the dictionary oid issue because it most likely won't be in 1.0.6.

The complaint about table oids returning the first column file name. This is intentional (though not documented). What behavior should it be for a table oid? There are no data files associated with a table, only with columns."
663,MCOL-311,MCOL,David Hall,89353,2016-12-08 16:15:26,Added the concept of reporting for all columns when table name or table oid are entered to MCOL-451.,12,Added the concept of reporting for all columns when table name or table oid are entered to MCOL-451.
664,MCOL-311,MCOL,David Hall,89354,2016-12-08 16:16:49,The concerns of QA are addressed in MCOL-451. Closing this.,13,The concerns of QA are addressed in MCOL-451. Closing this.
665,MCOL-311,MCOL,Daniel Lee,89356,2016-12-08 16:19:36,"I suggest to output a message stating that the supplied OID is for the table object and not to output a path since there is none.

It is not good to return something else when the requested info is not found.  That makes the result unpredictable and hard for user to script to account for that.
",14,"I suggest to output a message stating that the supplied OID is for the table object and not to output a path since there is none.

It is not good to return something else when the requested info is not found.  That makes the result unpredictable and hard for user to script to account for that.
"
666,MCOL-312,MCOL,Roman,168535,2020-10-11 18:29:40,I believe we need to extend the task and create an utility to load data into a single column. This will be helpfull for backup/restore purposes.,1,I believe we need to extend the task and create an utility to load data into a single column. This will be helpfull for backup/restore purposes.
667,MCOL-312,MCOL,Gregory Dorman,179219,2021-02-05 20:41:52,"[~tntnatbry] - Roman says that the utility already exists, except that it does not work for compressed data, and as such needs finishing. He will give you all the pointers needed.",2,"[~tntnatbry] - Roman says that the utility already exists, except that it does not work for compressed data, and as such needs finishing. He will give you all the pointers needed."
668,MCOL-312,MCOL,David Hall,196117,2021-08-05 18:25:05,"The current mcsRebuildEM says ""Requirement: all DBRoots must be on this node"". Not sure how to do this with a 3 node system. Probably need to look into that.",3,"The current mcsRebuildEM says ""Requirement: all DBRoots must be on this node"". Not sure how to do this with a 3 node system. Probably need to look into that."
669,MCOL-312,MCOL,David Hall,196119,2021-08-05 19:01:29,"It also has a line:
""Only internal DBRootStorageType is supported, provided: ""
We need this to work on external and s3 files and for multi-node systems. It appears mutli-node is the most likely to need it.",4,"It also has a line:
""Only internal DBRootStorageType is supported, provided: ""
We need this to work on external and s3 files and for multi-node systems. It appears mutli-node is the most likely to need it."
670,MCOL-312,MCOL,David Hall,196121,2021-08-05 19:13:47,"This utility should give us the ability to restore any system, regardless of number of nodes or type of backing file system, to:
1) restore from BRM_saves if possible.
2) rebuild from data sources if possible.
3) option to override (1) in favor of (2). Sometimes the BRM_saves that are available are out of date and data would be lost.

Data retention is more important than speed, so if we need to chug for a bit to accomplish this, so be it.",5,"This utility should give us the ability to restore any system, regardless of number of nodes or type of backing file system, to:
1) restore from BRM_saves if possible.
2) rebuild from data sources if possible.
3) option to override (1) in favor of (2). Sometimes the BRM_saves that are available are out of date and data would be lost.

Data retention is more important than speed, so if we need to chug for a bit to accomplish this, so be it."
671,MCOL-312,MCOL,David Hall,196122,2021-08-05 19:17:44,"This effort should also contain either a separate utility, or as part of the same process, the ability to rebuild the calponsystemcatalog from the .frm files and the data file/extent map. While there are plans to modify the way systemcatalog operates, we need to get this now.

The code should be modular enough (or whatever) to handle serious changes in the extent map and system catalog formats and storage. Future plans include such.",6,"This effort should also contain either a separate utility, or as part of the same process, the ability to rebuild the calponsystemcatalog from the .frm files and the data file/extent map. While there are plans to modify the way systemcatalog operates, we need to get this now.

The code should be modular enough (or whatever) to handle serious changes in the extent map and system catalog formats and storage. Future plans include such."
672,MCOL-312,MCOL,Gregory Dorman,203711,2021-10-26 16:25:56,"After 4566 was done, this utility is now as complete as it should be. 

Let's doc it.",7,"After 4566 was done, this utility is now as complete as it should be. 

Let's doc it."
673,MCOL-3242,MCOL,Patrick LeBlanc,129337,2019-06-14 17:17:11,"Made the PR: https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/785

Assign to whoever deserves it most.  :D",1,"Made the PR: URL

Assign to whoever deserves it most.  :D"
674,MCOL-3242,MCOL,Patrick LeBlanc,129338,2019-06-14 17:19:42,"Just realized I didn't prevent it from writing a snapshot after every commit.  Will resubmit in a bit.
",2,"Just realized I didn't prevent it from writing a snapshot after every commit.  Will resubmit in a bit.
"
675,MCOL-3242,MCOL,Patrick LeBlanc,129339,2019-06-14 17:34:26,OK that's in,3,OK that's in
676,MCOL-3242,MCOL,Andrew Hutchings,129422,2019-06-17 13:30:56,"[~dleeyh] with Patrick's patch we should see transactional BRM snapshots happening faster, I suspect when there are many columns and many tables in particular.  I'm guessing you will see this most in DML.",4,"[~dleeyh] with Patrick's patch we should see transactional BRM snapshots happening faster, I suspect when there are many columns and many tables in particular.  I'm guessing you will see this most in DML."
677,MCOL-3242,MCOL,Patrick LeBlanc,131189,2019-07-17 12:58:49,It's been working for about a month it seems.  Just merged the final cleanup.,5,It's been working for about a month it seems.  Just merged the final cleanup.
678,MCOL-3242,MCOL,Daniel Lee,134512,2019-09-20 14:17:35,"Build verified: 1.4.0-1

engine commit:
975463c

Have been running autopilot on different builds.  Verified by regression.

",6,"Build verified: 1.4.0-1

engine commit:
975463c

Have been running autopilot on different builds.  Verified by regression.

"
679,MCOL-325,MCOL,Andrew Hutchings,86855,2016-09-27 13:36:16,Commit to fix the flag handling of the WEEK() and YEARWEEK() functions,1,Commit to fix the flag handling of the WEEK() and YEARWEEK() functions
680,MCOL-325,MCOL,David Hill,87139,2016-10-06 15:58:01,passing regression testing on centos 7 OS,2,passing regression testing on centos 7 OS
681,MCOL-3267,MCOL,Roman,126620,2019-04-22 18:28:40,"create table t1 (a bigint, b bigint)engine=columnstore;
create table t2 (a bigint, b bigint)engine=columnstore;
insert into t1 values (5,5),(4,4),(3,3);
insert into t1 values (5,5),(4,4),(3,3);

SELECT * FROM( 
 SELECT * FROM (
  select * from (SELECT a, b FROM t1 ORDER BY b LIMIT 100) sq1 
  UNION ALL 
  select * from (SELECT a, b FROM t2 ORDER BY b LIMIT 100) sq2) sq3 
 order by sq3.a limit 999) sq4;

This query produces expected results. LimitedOrderBy got called three times as expected. And if I add ORDER BY at the top level then filesort comes into play.
I will get back to the original data set and problem description.",1,"create table t1 (a bigint, b bigint)engine=columnstore;
create table t2 (a bigint, b bigint)engine=columnstore;
insert into t1 values (5,5),(4,4),(3,3);
insert into t1 values (5,5),(4,4),(3,3);

SELECT * FROM( 
 SELECT * FROM (
  select * from (SELECT a, b FROM t1 ORDER BY b LIMIT 100) sq1 
  UNION ALL 
  select * from (SELECT a, b FROM t2 ORDER BY b LIMIT 100) sq2) sq3 
 order by sq3.a limit 999) sq4;

This query produces expected results. LimitedOrderBy got called three times as expected. And if I add ORDER BY at the top level then filesort comes into play.
I will get back to the original data set and problem description."
682,MCOL-3267,MCOL,Roman,126703,2019-04-23 19:46:14,"Please review the change.
",2,"Please review the change.
"
683,MCOL-3267,MCOL,Roman,126704,2019-04-23 19:47:37,"For QA: Here are the steps to reproduce the issue.

create table t1 (a bigint, b bigint)engine=columnstore;
create table t2 (a bigint, b bigint)engine=columnstore;
insert into t1 values (5,5),(4,4),(3,3);
insert into t1 values (5,5),(4,4),(3,3);

SELECT * FROM
((SELECT a, b FROM t1 ORDER BY b LIMIT 2) sq1
UNION ALL
(SELECT a, b FROM t2 ORDER BY b LIMIT 2) sq2)
ORDER BY 1,2;",3,"For QA: Here are the steps to reproduce the issue.

create table t1 (a bigint, b bigint)engine=columnstore;
create table t2 (a bigint, b bigint)engine=columnstore;
insert into t1 values (5,5),(4,4),(3,3);
insert into t1 values (5,5),(4,4),(3,3);

SELECT * FROM
((SELECT a, b FROM t1 ORDER BY b LIMIT 2) sq1
UNION ALL
(SELECT a, b FROM t2 ORDER BY b LIMIT 2) sq2)
ORDER BY 1,2;"
684,MCOL-3267,MCOL,Daniel Lee,126876,2019-04-25 21:37:44,"Reproduced the issue on 1.2.3-1

It is not yet fixed in last nights 1.2.4-1 nightly.  It looks like it was merged after the build was made.

The SELECT statement in the last comment has an extra comma in the 2nd subquery.

",4,"Reproduced the issue on 1.2.3-1

It is not yet fixed in last nights 1.2.4-1 nightly.  It looks like it was merged after the build was made.

The SELECT statement in the last comment has an extra comma in the 2nd subquery.

"
685,MCOL-3267,MCOL,Daniel Lee,126916,2019-04-26 15:52:11,"Build verified: 1.2.4-1 nightly

server commit:
137b9a8
engine commit:
b8de456

Reproduced incorrect result issue in 1.2.3-1:

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.3.13-MariaDB-log Columnstore 1.2.3-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> SELECT * FROM ((SELECT a, b FROM t1 ORDER BY b LIMIT 2) UNION ALL (SELECT a, b FROM t2 ORDER BY b LIMIT 2)) as sq3 ORDER BY 1,2;
+------+------+
| a    | b    |
+------+------+
|    4 |    4 |
|    5 |    5 |
+------+------+
2 rows in set (0.110 sec)


1.2.4-1

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 14
Server version: 10.3.13-MariaDB-log Columnstore 1.2.4-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> SELECT * FROM ((SELECT a, b FROM t1 ORDER BY b LIMIT 2) UNION ALL (SELECT a, b FROM t2 ORDER BY b LIMIT 2)) as sq3 ORDER BY 1,2;
+------+------+
| a    | b    |
+------+------+
|    3 |    3 |
|    3 |    3 |
+------+------+
2 rows in set (0.033 sec)


",5,"Build verified: 1.2.4-1 nightly

server commit:
137b9a8
engine commit:
b8de456

Reproduced incorrect result issue in 1.2.3-1:

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.3.13-MariaDB-log Columnstore 1.2.3-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> SELECT * FROM ((SELECT a, b FROM t1 ORDER BY b LIMIT 2) UNION ALL (SELECT a, b FROM t2 ORDER BY b LIMIT 2)) as sq3 ORDER BY 1,2;
+------+------+
| a    | b    |
+------+------+
|    4 |    4 |
|    5 |    5 |
+------+------+
2 rows in set (0.110 sec)


1.2.4-1

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 14
Server version: 10.3.13-MariaDB-log Columnstore 1.2.4-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> SELECT * FROM ((SELECT a, b FROM t1 ORDER BY b LIMIT 2) UNION ALL (SELECT a, b FROM t2 ORDER BY b LIMIT 2)) as sq3 ORDER BY 1,2;
+------+------+
| a    | b    |
+------+------+
|    3 |    3 |
|    3 |    3 |
+------+------+
2 rows in set (0.033 sec)


"
686,MCOL-3270,MCOL,Roman,126551,2019-04-18 19:00:04,For QA: you could test the patch comparing ingestion speed of text or varchars without and with the patch using at least 10 000 000 records. There must be a reasonable difference in timings.,1,For QA: you could test the patch comparing ingestion speed of text or varchars without and with the patch using at least 10 000 000 records. There must be a reasonable difference in timings.
687,MCOL-3270,MCOL,Daniel Lee,126577,2019-04-19 17:20:08,"Build verified: 1.2.4-1 nightly

[dlee@master centos7]$ cat gitversionInfo.txt 
server commit:
137b9a8
engine commit:
b3a7559


Dataset tested, 10 gb dbt3 

orders table has 15,000,000 rows
lineitem table has 59,986,052 rows
plus 6 smaller tables.

Performed cpimport timing test on both 1.2.2-1 and 1.2.4-1. 1.2.4-1 is about 2.5 times faster.  Disk space utilization remained the same.

Also with 1.2.4-1, loaded two 1gb dbt3 databases. columnstore database loaded using cpimport and innnodb database loaded using LDI.  Verified all varchar columns in the orders table to be identical between the two databases using cross-engine join.

1.2.2-1

[root@localhost columnstore]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	8m53.788s
user	10m56.330s
sys	0m38.518s

[root@localhost columnstore]# du -sh data1
9.2G	data1


1.2.4-1

[root@localhost columnstore]# du -sh data1
739M	data1
[root@localhost columnstore]# cd

[root@localhost ~]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	3m27.598s
user	3m44.575s
sys	0m36.012s

[root@localhost columnstore]# du -sh data1
9.2G	data1
",2,"Build verified: 1.2.4-1 nightly

[dlee@master centos7]$ cat gitversionInfo.txt 
server commit:
137b9a8
engine commit:
b3a7559


Dataset tested, 10 gb dbt3 

orders table has 15,000,000 rows
lineitem table has 59,986,052 rows
plus 6 smaller tables.

Performed cpimport timing test on both 1.2.2-1 and 1.2.4-1. 1.2.4-1 is about 2.5 times faster.  Disk space utilization remained the same.

Also with 1.2.4-1, loaded two 1gb dbt3 databases. columnstore database loaded using cpimport and innnodb database loaded using LDI.  Verified all varchar columns in the orders table to be identical between the two databases using cross-engine join.

1.2.2-1

[root@localhost columnstore]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	8m53.788s
user	10m56.330s
sys	0m38.518s

[root@localhost columnstore]# du -sh data1
9.2G	data1


1.2.4-1

[root@localhost columnstore]# du -sh data1
739M	data1
[root@localhost columnstore]# cd

[root@localhost ~]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	3m27.598s
user	3m44.575s
sys	0m36.012s

[root@localhost columnstore]# du -sh data1
9.2G	data1
"
688,MCOL-33,MCOL,David Hill,83536,2016-05-20 02:08:45,"different errors in memory settings and mysql server crash

root@srvss2 srvswdev11]# more test200/diff.txt
3,4c3
< count2
< 130000000
---
> ERROR 1815 (HY000) at line 6: Internal error: InetStreamSocket::readToMagic: Remote is closed
6,49c5,31
< 0
< ERROR 122 (HY000) at line 10: IDB-2003: Aggregation/Distinct memory limit is exceeded.
< sleep(30)
< 0
< count4
< 100761020
< count5
< 100761020
< sleep(30)
< 0
< ERROR 122 (HY000) at line 19: IDB-2001: Join or subselect exceeds memory limit.
< sleep(30)
< 0
< count7
< 88907548
< count8
< 88907548
< sleep(30)
< 0
< ERROR 122 (HY000) at line 28: IDB-2001: Join or subselect exceeds memory limit.
< sleep(30)
< 0
< count10
< 136000000
< count11
< 136000000
< sleep(30)
< 0
< ERROR 122 (HY000) at line 37: IDB-2002: Union memory limit exceeded.
< sleep(60)
< 0
< count13
< 68000000
< F	count14
< 1	68000000
< sleep(30)
< 0
< ERROR 122 (HY000) at line 47: IDB-2003: Aggregation/Distinct memory limit is exceeded.
< sleep(30)
< 0
< ERROR 122 (HY000) at line 55: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.
< count16
< 51372630
< ERROR 122 (HY000) at line 61: IDB-2001: Join or subselect exceeds memory limit.
---
> 1
> ERROR 2006 (HY000) at line 10: MySQL server has gone away
> ERROR 2006 (HY000) at line 11: MySQL server has gone away
> ERROR 2006 (HY000) at line 14: MySQL server has gone away
> ERROR 2006 (HY000) at line 15: MySQL server has gone away
> ERROR 2006 (HY000) at line 16: MySQL server has gone away
> ERROR 2006 (HY000) at line 19: MySQL server has gone away
> ERROR 2006 (HY000) at line 20: MySQL server has gone away
> ERROR 2006 (HY000) at line 23: MySQL server has gone away
> ERROR 2006 (HY000) at line 24: MySQL server has gone away
> ERROR 2006 (HY000) at line 25: MySQL server has gone away
> ERROR 2006 (HY000) at line 28: MySQL server has gone away
> ERROR 2006 (HY000) at line 29: MySQL server has gone away
> ERROR 2006 (HY000) at line 32: MySQL server has gone away
> ERROR 2006 (HY000) at line 33: MySQL server has gone away
> ERROR 2006 (HY000) at line 34: MySQL server has gone away
> ERROR 2006 (HY000) at line 37: MySQL server has gone away
> ERROR 2006 (HY000) at line 38: MySQL server has gone away
> ERROR 2006 (HY000) at line 42: MySQL server has gone away
> ERROR 2006 (HY000) at line 43: MySQL server has gone away
> ERROR 2006 (HY000) at line 44: MySQL server has gone away
> ERROR 2006 (HY000) at line 47: MySQL server has gone away
> ERROR 2006 (HY000) at line 48: MySQL server has gone away
> ERROR 2006 (HY000) at line 52: MySQL server has gone away
> ERROR 2006 (HY000) at line 55: MySQL server has gone away
> ERROR 2006 (HY000) at line 58: MySQL server has gone away
> ERROR 2006 (HY000) at line 61: MySQL server has gone away



",1,"different errors in memory settings and mysql server crash

root@srvss2 srvswdev11]# more test200/diff.txt
3,4c3
< count2
< 130000000
---
> ERROR 1815 (HY000) at line 6: Internal error: InetStreamSocket::readToMagic: Remote is closed
6,49c5,31
< 0
< ERROR 122 (HY000) at line 10: IDB-2003: Aggregation/Distinct memory limit is exceeded.
< sleep(30)
< 0
< count4
< 100761020
< count5
< 100761020
< sleep(30)
< 0
< ERROR 122 (HY000) at line 19: IDB-2001: Join or subselect exceeds memory limit.
< sleep(30)
< 0
< count7
< 88907548
< count8
< 88907548
< sleep(30)
< 0
< ERROR 122 (HY000) at line 28: IDB-2001: Join or subselect exceeds memory limit.
< sleep(30)
< 0
< count10
< 136000000
< count11
< 136000000
< sleep(30)
< 0
< ERROR 122 (HY000) at line 37: IDB-2002: Union memory limit exceeded.
< sleep(60)
< 0
< count13
< 68000000
< F	count14
< 1	68000000
< sleep(30)
< 0
< ERROR 122 (HY000) at line 47: IDB-2003: Aggregation/Distinct memory limit is exceeded.
< sleep(30)
< 0
< ERROR 122 (HY000) at line 55: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.
< count16
< 51372630
< ERROR 122 (HY000) at line 61: IDB-2001: Join or subselect exceeds memory limit.
---
> 1
> ERROR 2006 (HY000) at line 10: MySQL server has gone away
> ERROR 2006 (HY000) at line 11: MySQL server has gone away
> ERROR 2006 (HY000) at line 14: MySQL server has gone away
> ERROR 2006 (HY000) at line 15: MySQL server has gone away
> ERROR 2006 (HY000) at line 16: MySQL server has gone away
> ERROR 2006 (HY000) at line 19: MySQL server has gone away
> ERROR 2006 (HY000) at line 20: MySQL server has gone away
> ERROR 2006 (HY000) at line 23: MySQL server has gone away
> ERROR 2006 (HY000) at line 24: MySQL server has gone away
> ERROR 2006 (HY000) at line 25: MySQL server has gone away
> ERROR 2006 (HY000) at line 28: MySQL server has gone away
> ERROR 2006 (HY000) at line 29: MySQL server has gone away
> ERROR 2006 (HY000) at line 32: MySQL server has gone away
> ERROR 2006 (HY000) at line 33: MySQL server has gone away
> ERROR 2006 (HY000) at line 34: MySQL server has gone away
> ERROR 2006 (HY000) at line 37: MySQL server has gone away
> ERROR 2006 (HY000) at line 38: MySQL server has gone away
> ERROR 2006 (HY000) at line 42: MySQL server has gone away
> ERROR 2006 (HY000) at line 43: MySQL server has gone away
> ERROR 2006 (HY000) at line 44: MySQL server has gone away
> ERROR 2006 (HY000) at line 47: MySQL server has gone away
> ERROR 2006 (HY000) at line 48: MySQL server has gone away
> ERROR 2006 (HY000) at line 52: MySQL server has gone away
> ERROR 2006 (HY000) at line 55: MySQL server has gone away
> ERROR 2006 (HY000) at line 58: MySQL server has gone away
> ERROR 2006 (HY000) at line 61: MySQL server has gone away



"
689,MCOL-33,MCOL,Dipti Joshi,83574,2016-05-22 04:24:46,"Latest nightly run log has now improved - though not completely clear

[~hill] Here is the analysis of this log: cat test200/diff.txt
7c7
< ERROR 122 (HY000) at line 10: IDB-2003: Aggregation/Distinct memory limit is exceeded.
---
> ERROR 1815 (HY000) at line 10: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.
16c16
< ERROR 122 (HY000) at line 19: IDB-2001: Join or subselect exceeds memory limit.
---
> ERROR 1815 (HY000) at line 19: Internal error: IDB-2001: Join or subselect exceeds memory limit.
20c20
< 88907548
---
> 18907548
22c22
< 88907548
---
> 18907548
25c25,26
< ERROR 122 (HY000) at line 28: IDB-2001: Join or subselect exceeds memory limit.
---
> count9
> 18907551
34c35
< ERROR 122 (HY000) at line 37: IDB-2002: Union memory limit exceeded.
---
> ERROR 1815 (HY000) at line 37: Internal error: IDB-2002: Union memory limit exceeded.
43c44
< ERROR 122 (HY000) at line 47: IDB-2003: Aggregation/Distinct memory limit is exceeded.
---
> ERROR 1815 (HY000) at line 47: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.
46c47
< ERROR 122 (HY000) at line 55: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.
---
> ERROR 1815 (HY000) at line 55: Internal error: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.
49c50
< ERROR 122 (HY000) at line 61: IDB-2001: Join or subselect exceeds memory limit.
---
> ERROR 1815 (HY000) at line 61: Internal error: IDB-2001: Join or subselect exceeds memory limit.

Where ever the mis-match is due to ERROR 122 vs ERROR 1815 - is not true failure - It is due the fact that MariaDB ColumnStore that is based on MariaDB Server uses error number 1815, where as the reference log is showing old error number of 122 - Other wise the text of the error is essentially the same. 

Hence test200/memLimits.sql.ref.log needs to be updated as following
(1) replace line 7 with ""ERROR 1815 (HY000) at line 10: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.""
(2) replace line 16 with ""ERROR 1815 (HY000) at line 19: Internal error: IDB-2001: Join or subselect exceeds memory limit.""
(3) replace line 35 with ""ERROR 1815 (HY000) at line 37: Internal error: IDB-2002: Union memory limit exceeded.""
(4) replace line 43 with ""ERROR 1815 (HY000) at line 47: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.""
(5) replace line 46 with ""ERROR 1815 (HY000) at line 55: Internal error: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.""
(6) replace line 49 with ""ERROR 1815 (HY000) at line 61: Internal error: IDB-2001: Join or subselect exceeds memory limit.""

Above changes will clear out these differences - however mismatch at line 20, 22 and 25 will still remain.
[~hill] After you make this update,  on Monday morning let us manually run this test to gather, I will be able to help figure out the mismatch in 20,22 and 25 as well when we do that - as I need to see the actual data in the test table test200 to diagnose it.

",2,"Latest nightly run log has now improved - though not completely clear

[~hill] Here is the analysis of this log: cat test200/diff.txt
7c7
< ERROR 122 (HY000) at line 10: IDB-2003: Aggregation/Distinct memory limit is exceeded.
---
> ERROR 1815 (HY000) at line 10: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.
16c16
< ERROR 122 (HY000) at line 19: IDB-2001: Join or subselect exceeds memory limit.
---
> ERROR 1815 (HY000) at line 19: Internal error: IDB-2001: Join or subselect exceeds memory limit.
20c20
< 88907548
---
> 18907548
22c22
< 88907548
---
> 18907548
25c25,26
< ERROR 122 (HY000) at line 28: IDB-2001: Join or subselect exceeds memory limit.
---
> count9
> 18907551
34c35
< ERROR 122 (HY000) at line 37: IDB-2002: Union memory limit exceeded.
---
> ERROR 1815 (HY000) at line 37: Internal error: IDB-2002: Union memory limit exceeded.
43c44
< ERROR 122 (HY000) at line 47: IDB-2003: Aggregation/Distinct memory limit is exceeded.
---
> ERROR 1815 (HY000) at line 47: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.
46c47
< ERROR 122 (HY000) at line 55: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.
---
> ERROR 1815 (HY000) at line 55: Internal error: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.
49c50
< ERROR 122 (HY000) at line 61: IDB-2001: Join or subselect exceeds memory limit.
---
> ERROR 1815 (HY000) at line 61: Internal error: IDB-2001: Join or subselect exceeds memory limit.

Where ever the mis-match is due to ERROR 122 vs ERROR 1815 - is not true failure - It is due the fact that MariaDB ColumnStore that is based on MariaDB Server uses error number 1815, where as the reference log is showing old error number of 122 - Other wise the text of the error is essentially the same. 

Hence test200/memLimits.sql.ref.log needs to be updated as following
(1) replace line 7 with ""ERROR 1815 (HY000) at line 10: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.""
(2) replace line 16 with ""ERROR 1815 (HY000) at line 19: Internal error: IDB-2001: Join or subselect exceeds memory limit.""
(3) replace line 35 with ""ERROR 1815 (HY000) at line 37: Internal error: IDB-2002: Union memory limit exceeded.""
(4) replace line 43 with ""ERROR 1815 (HY000) at line 47: Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.""
(5) replace line 46 with ""ERROR 1815 (HY000) at line 55: Internal error: CAL0002: Update Failed:  IDB-2001: Join or subselect exceeds memory limit.""
(6) replace line 49 with ""ERROR 1815 (HY000) at line 61: Internal error: IDB-2001: Join or subselect exceeds memory limit.""

Above changes will clear out these differences - however mismatch at line 20, 22 and 25 will still remain.
[~hill] After you make this update,  on Monday morning let us manually run this test to gather, I will be able to help figure out the mismatch in 20,22 and 25 as well when we do that - as I need to see the actual data in the test table test200 to diagnose it.

"
690,MCOL-33,MCOL,David Hill,83581,2016-05-22 20:33:21,made the changes to the ref logs,3,made the changes to the ref logs
691,MCOL-33,MCOL,Dipti Joshi,83595,2016-05-23 07:34:02,Not failing any more - so closing,4,Not failing any more - so closing
692,MCOL-3315,MCOL,Andrew Hutchings,127902,2019-05-16 20:14:06,develop-1.2 rebased.,1,develop-1.2 rebased.
693,MCOL-3315,MCOL,Daniel Lee,127907,2019-05-16 21:22:48,"Build tested: Created build from GitHub source

/root/columnstore/mariadb-columnstore-server
commit e3d99393916f0231db02564dd5e316e803bdbbe9
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Mon Jan 14 16:20:01 2019 +0000

    Disable Travis triggering on pull requests


/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 92d662491ac08095e5998b69a9c730e9d208e8f4
Merge: 0dd33c6 45df72a
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Thu May 16 21:50:05 2019 +0300

    Merge pull request #761 from mariadb-corporation/develop-1.2-merge-up-20190514
    
    Merge develop-1.1 into develop-1.2



[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.3.15-MariaDB-debug-log Columnstore 1.2.4-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.


",2,"Build tested: Created build from GitHub source

/root/columnstore/mariadb-columnstore-server
commit e3d99393916f0231db02564dd5e316e803bdbbe9
Author: Andrew Hutchings 
Date:   Mon Jan 14 16:20:01 2019 +0000

    Disable Travis triggering on pull requests


/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 92d662491ac08095e5998b69a9c730e9d208e8f4
Merge: 0dd33c6 45df72a
Author: Roman Nozdrin 
Date:   Thu May 16 21:50:05 2019 +0300

    Merge pull request #761 from mariadb-corporation/develop-1.2-merge-up-20190514
    
    Merge develop-1.1 into develop-1.2



[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.3.15-MariaDB-debug-log Columnstore 1.2.4-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.


"
694,MCOL-3321,MCOL,David Hall,128343,2019-05-24 15:35:36,"RegressionTest PR #115
Since this is a change only to the regression  test, no need to send to QA. The nightly runs will suffice.",1,"RegressionTest PR #115
Since this is a change only to the regression  test, no need to send to QA. The nightly runs will suffice."
695,MCOL-3321,MCOL,Daniel Lee,130632,2019-07-05 19:43:13,Close ticket per developer's comment,2,Close ticket per developer's comment
696,MCOL-3331,MCOL,Jens Röwekamp,128345,2019-05-24 16:55:04,"Just checked that building pymcsapi with Python 3.6 instead of Python 3.4 is enough to change pymcsapi's Python 3 dependency in CentOS 7.
The test pipelines passed from the 1.1.8, 1.2.5, and 1.4.0 branches against ColumnStore 1.2.3 on my build environment.
I updated the built instructions in GitHub's README.md in the regarding branches.

It is mainly using python36-devel instead of python34-devel.
Further to note is, that all pip3 test dependencies need to be reinstalled in order to get the test pipeline working.
I further validated in my test environment that the built rpms state the correct Python 3.6 dependency and are install and usable.

In order to make this change effective, we have to change our buildbot build pipeline for CentOS 7 to use Python 3.6 (once CS 1.2.4 is released)

I'll therefore forward the ticket to [~ben.thompson] to make the regarding changes in buildbot.
Once he is done, he can issue the pull requests for the linked branches _MCOL-3331_, _MCOL-3331-upmerge-into-1.2_, and _MCOL-3331-upmerge-into-1.4_.

-----

*For QA:*
Once Ben made his changes and buildbot ran, verify:
- that all mcsapi related tests in our buildbot test pipeline succeed
- that the build CentOS 7 pymcsapi3 and pyspark3 packages state Python 3.6 as dependency
- that Python 3.6 is resolved automatically as dependency when pymcsapi3 or pyspark3 is installed through our package repositories on CentOS 7",1,"Just checked that building pymcsapi with Python 3.6 instead of Python 3.4 is enough to change pymcsapi's Python 3 dependency in CentOS 7.
The test pipelines passed from the 1.1.8, 1.2.5, and 1.4.0 branches against ColumnStore 1.2.3 on my build environment.
I updated the built instructions in GitHub's README.md in the regarding branches.

It is mainly using python36-devel instead of python34-devel.
Further to note is, that all pip3 test dependencies need to be reinstalled in order to get the test pipeline working.
I further validated in my test environment that the built rpms state the correct Python 3.6 dependency and are install and usable.

In order to make this change effective, we have to change our buildbot build pipeline for CentOS 7 to use Python 3.6 (once CS 1.2.4 is released)

I'll therefore forward the ticket to [~ben.thompson] to make the regarding changes in buildbot.
Once he is done, he can issue the pull requests for the linked branches _MCOL-3331_, _MCOL-3331-upmerge-into-1.2_, and _MCOL-3331-upmerge-into-1.4_.

-----

*For QA:*
Once Ben made his changes and buildbot ran, verify:
- that all mcsapi related tests in our buildbot test pipeline succeed
- that the build CentOS 7 pymcsapi3 and pyspark3 packages state Python 3.6 as dependency
- that Python 3.6 is resolved automatically as dependency when pymcsapi3 or pyspark3 is installed through our package repositories on CentOS 7"
697,MCOL-3331,MCOL,Ben Thompson,135445,2019-10-07 21:07:05,I think this ticket got left in limbo and has been resolved for a while.  centos7 builds are using python 3.6,2,I think this ticket got left in limbo and has been resolved for a while.  centos7 builds are using python 3.6
698,MCOL-3331,MCOL,Daniel Lee,137319,2019-11-07 18:29:37,"Build verified: 1.4.1-1 source

[root@localhost api]# git show
commit 10c5ea531a20c54f4fffe45b9558fa86f70b6b4b
Merge: 262a726 d7c70a5
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Oct 16 09:27:00 2019 +0100

    Merge pull request #164 from mariadb-corporation/fix-testing
    
    Make C++ regression suite work


Verified that it is now using python 3.6",3,"Build verified: 1.4.1-1 source

[root@localhost api]# git show
commit 10c5ea531a20c54f4fffe45b9558fa86f70b6b4b
Merge: 262a726 d7c70a5
Author: Andrew Hutchings 
Date:   Wed Oct 16 09:27:00 2019 +0100

    Merge pull request #164 from mariadb-corporation/fix-testing
    
    Make C++ regression suite work


Verified that it is now using python 3.6"
699,MCOL-3343,MCOL,David Hall,128702,2019-05-31 18:43:58,"It appears that if one side of an arithmetic operator (or function like DIV) is an aggregate and the other is a Window Function, then somehow it gets the operator twice as a sub function of each side. When each is executed, the other side isn't available. Whichever gets done last is the final result.

ExeMgr spins up threads for each, so in the following query:

select count(*) / sum(d1) over () from tb1 group by d1;

The count(*) will get done in one thread and perform the divide of count(*) with unknown in the denominator.
The sum(d1) will get done in another thread and perform the divide with unknown in the numerator and sum(d1) in the denominator.
On my machine, the second result is returned.

Somehow, the setup code is acting as if the '/' is inside of each function, rather than the root of the tree.

This does not happen if we do aggregate <op> aggregate. Only if one side is a Window Function.",1,"It appears that if one side of an arithmetic operator (or function like DIV) is an aggregate and the other is a Window Function, then somehow it gets the operator twice as a sub function of each side. When each is executed, the other side isn't available. Whichever gets done last is the final result.

ExeMgr spins up threads for each, so in the following query:

select count(*) / sum(d1) over () from tb1 group by d1;

The count(*) will get done in one thread and perform the divide of count(*) with unknown in the denominator.
The sum(d1) will get done in another thread and perform the divide with unknown in the numerator and sum(d1) in the denominator.
On my machine, the second result is returned.

Somehow, the setup code is acting as if the '/' is inside of each function, rather than the root of the tree.

This does not happen if we do aggregate  aggregate. Only if one side is a Window Function."
700,MCOL-3343,MCOL,David Hall,129344,2019-06-14 18:45:38,"The title of this MCOL says incorrect in a specific case. That is incorrect. It's a much more general case involving more than one cause.

After fixing the original compaint of agg/win, I thought to play around with more complex arithmetic formula involving window and aggregate functions and got a crash. Simplifying it down, it turns out something simple doesn't work:
MariaDB [dhall]> select sum(d1) over() / c1  from tb1;
ERROR 1815 (HY000): Internal error: IDB-9024: 'c1' is not in tuple.
Whereas aggregate:
MariaDB [dhall]> select sum(d1) / c1  from tb1 group by c1;
+--------------------+
| sum(d1) / c1       |
+--------------------+
| 0.2714285714285714 |
|  2.267515923566879 |
| 0.7087378640776698 |
+--------------------+
The c1 is not in tuple error implies that the setup logic doesn't realize that c1 has to be in the projection list. Figuring out just where to make a small change isn't always easy.
Interestingly.
MariaDB [dhall]> select c1,sum(d1) over() / c1  from tb1;
+------+---------------------+
| c1   | sum(d1) over() / c1 |
+------+---------------------+
| 3.14 |   4.095541401273885 |
| 5.15 |  2.4970873786407766 |
|  7.7 |    1.67012987012987 |
+------+---------------------+
This works because c1 is already in the projection list because we specifically asked for it.

However, fixing this little problem may or may not cause this to work:
MariaDB [dhall]> select c1*sum(d1) over() / count(*)  from tb1 group by c1, d1;
+--------------------------------+
| c1*sum(d1) over() / count(*)   |
+--------------------------------+
| 0.0000000000007071900395043137 |
|                           NULL |
|                           NULL |
+--------------------------------+
As you can see, this returns garbage.",2,"The title of this MCOL says incorrect in a specific case. That is incorrect. It's a much more general case involving more than one cause.

After fixing the original compaint of agg/win, I thought to play around with more complex arithmetic formula involving window and aggregate functions and got a crash. Simplifying it down, it turns out something simple doesn't work:
MariaDB [dhall]> select sum(d1) over() / c1  from tb1;
ERROR 1815 (HY000): Internal error: IDB-9024: 'c1' is not in tuple.
Whereas aggregate:
MariaDB [dhall]> select sum(d1) / c1  from tb1 group by c1;
+--------------------+
| sum(d1) / c1       |
+--------------------+
| 0.2714285714285714 |
|  2.267515923566879 |
| 0.7087378640776698 |
+--------------------+
The c1 is not in tuple error implies that the setup logic doesn't realize that c1 has to be in the projection list. Figuring out just where to make a small change isn't always easy.
Interestingly.
MariaDB [dhall]> select c1,sum(d1) over() / c1  from tb1;
+------+---------------------+
| c1   | sum(d1) over() / c1 |
+------+---------------------+
| 3.14 |   4.095541401273885 |
| 5.15 |  2.4970873786407766 |
|  7.7 |    1.67012987012987 |
+------+---------------------+
This works because c1 is already in the projection list because we specifically asked for it.

However, fixing this little problem may or may not cause this to work:
MariaDB [dhall]> select c1*sum(d1) over() / count(*)  from tb1 group by c1, d1;
+--------------------------------+
| c1*sum(d1) over() / count(*)   |
+--------------------------------+
| 0.0000000000007071900395043137 |
|                           NULL |
|                           NULL |
+--------------------------------+
As you can see, this returns garbage."
701,MCOL-3343,MCOL,David Hall,129701,2019-06-20 21:07:00,"Test case PR #121

For QA: The bug is manifest any time a Window Function is used with another function or arithmetic operator. See the regression tests in working_tpch1_compareLogOnly/windowFunctions/MCOL-3343.sql for examples. You can then try any combination of Window Functions, aggregates and simple columns to try to break it. Use any database for a ref if you want. Notice that MariaDB Server may allow some combinations without the proper GROUP BY clause, but CS does not allow such things.",3,"Test case PR #121

For QA: The bug is manifest any time a Window Function is used with another function or arithmetic operator. See the regression tests in working_tpch1_compareLogOnly/windowFunctions/MCOL-3343.sql for examples. You can then try any combination of Window Functions, aggregates and simple columns to try to break it. Use any database for a ref if you want. Notice that MariaDB Server may allow some combinations without the proper GROUP BY clause, but CS does not allow such things."
702,MCOL-3343,MCOL,David Hall,129702,2019-06-20 21:08:34,"Once this bug is closed, then MCOL-3338 should be tested and closed",4,"Once this bug is closed, then MCOL-3338 should be tested and closed"
703,MCOL-3343,MCOL,Daniel Lee,130633,2019-07-05 19:48:29,"Build verified: 1.2.5-1 nightly

erver commit:
f44f7d9
engine commit:
4e477ab

Verified test case in the ticket.


",5,"Build verified: 1.2.5-1 nightly

erver commit:
f44f7d9
engine commit:
4e477ab

Verified test case in the ticket.


"
704,MCOL-3349,MCOL,Andrew Hutchings,142554,2020-01-21 18:38:56,Committed along with fixes for MCOL-128,1,Committed along with fixes for MCOL-128
705,MCOL-3349,MCOL,susil.behera,144118,2020-02-11 22:03:34,"Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 5efa6a4dc52129be2de49fdfc23e44020401b86b

Test cases:
1.
DROP DATABASE IF EXISTS db1;
CREATE DATABASE db1;
CREATE TABLE db1.t1 (c1 int, c2 int, PRIMARY KEY (c1)) ;
CREATE TABLE  db1.t4  ENGINE=columnstore SELECT * FROM db1.t1;
CREATE TABLE  db1.t5  ENGINE=""columnstore"" SELECT * FROM db1.t1;
CREATE TABLE  db1.t3  ENGINE=columnstore AS (SELECT * FROM db1.t1);
CREATE TABLE  db1.t6  ENGINE=columnstore AS SELECT c1 AS c11 FROM db1.t1;
CREATE TABLE  db1.t7  ENGINE=columnstore AS (SELECT c1 AS c11 FROM db1.t1);
CREATE TABLE  db1.t8  ENGINE=columnstore SELECT c1 AS c11 FROM db1.t1;

2.
DROP DATABASE IF EXISTS db1;
CREATE DATABASE db1;
USE db1;
CREATE TABLE src (c0 int, c1 int);
INSERT INTO src VALUES (1,1),(1,1),(1,1),(2,2),(2,2),(2,2),(3,3),(3,3),(3,3);

CREATE TABLE tgt2 engine = columnstore AS SELECT c0, c1 FROM src GROUP BY c0 HAVING COUNT(*) > 1;
CREATE TABLE tgt3 SELECT c0, c1 FROM src order by c0 limit 2 offset 1;
CREATE TABLE tgt4 SELECT distinct c0 FROM src;
CREATE TABLE tgt5 as (SELECT c0 FROM src where c0 > 0);
CREATE TABLE tgt6 (c2 bigint primary key) SELECT distinct c0 as c2 FROM src;

3.
DROP DATABASE IF EXISTS db1;
CREATE DATABASE db1;
USE db1;
CREATE TABLE src (c0 char(20));
INSERT INTO src VALUES ('android'),('iphone');
CREATE TABLE tgt2 (field1 int) engine=columnstore SELECT c0 FROM src;
CREATE TABLE tgt3 (field1 varchar(20), c0 char(20)) engine=columnstore SELECT * FROM src;
CREATE TABLE tgt5 engine = columnstore SELECT c0 FROM src WHERE c0='iphone';

There are some remote cases where the behavior is different from InnoDB. I'll raise tickets separately for those.
",2,"Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 5efa6a4dc52129be2de49fdfc23e44020401b86b

Test cases:
1.
DROP DATABASE IF EXISTS db1;
CREATE DATABASE db1;
CREATE TABLE db1.t1 (c1 int, c2 int, PRIMARY KEY (c1)) ;
CREATE TABLE  db1.t4  ENGINE=columnstore SELECT * FROM db1.t1;
CREATE TABLE  db1.t5  ENGINE=""columnstore"" SELECT * FROM db1.t1;
CREATE TABLE  db1.t3  ENGINE=columnstore AS (SELECT * FROM db1.t1);
CREATE TABLE  db1.t6  ENGINE=columnstore AS SELECT c1 AS c11 FROM db1.t1;
CREATE TABLE  db1.t7  ENGINE=columnstore AS (SELECT c1 AS c11 FROM db1.t1);
CREATE TABLE  db1.t8  ENGINE=columnstore SELECT c1 AS c11 FROM db1.t1;

2.
DROP DATABASE IF EXISTS db1;
CREATE DATABASE db1;
USE db1;
CREATE TABLE src (c0 int, c1 int);
INSERT INTO src VALUES (1,1),(1,1),(1,1),(2,2),(2,2),(2,2),(3,3),(3,3),(3,3);

CREATE TABLE tgt2 engine = columnstore AS SELECT c0, c1 FROM src GROUP BY c0 HAVING COUNT(*) > 1;
CREATE TABLE tgt3 SELECT c0, c1 FROM src order by c0 limit 2 offset 1;
CREATE TABLE tgt4 SELECT distinct c0 FROM src;
CREATE TABLE tgt5 as (SELECT c0 FROM src where c0 > 0);
CREATE TABLE tgt6 (c2 bigint primary key) SELECT distinct c0 as c2 FROM src;

3.
DROP DATABASE IF EXISTS db1;
CREATE DATABASE db1;
USE db1;
CREATE TABLE src (c0 char(20));
INSERT INTO src VALUES ('android'),('iphone');
CREATE TABLE tgt2 (field1 int) engine=columnstore SELECT c0 FROM src;
CREATE TABLE tgt3 (field1 varchar(20), c0 char(20)) engine=columnstore SELECT * FROM src;
CREATE TABLE tgt5 engine = columnstore SELECT c0 FROM src WHERE c0='iphone';

There are some remote cases where the behavior is different from InnoDB. I'll raise tickets separately for those.
"
706,MCOL-3388,MCOL,Andrew Hutchings,129670,2019-06-20 12:54:39,"For QA: See related issues for what this includes. MCOL-2243 is still open for now but there is a potential fix in the code, hence why that is included here.",1,"For QA: See related issues for what this includes. MCOL-2243 is still open for now but there is a potential fix in the code, hence why that is included here."
707,MCOL-3388,MCOL,Andrew Hutchings,129724,2019-06-21 13:21:58,Note: PR in engine and regression test,2,Note: PR in engine and regression test
708,MCOL-3388,MCOL,Daniel Lee,130733,2019-07-08 16:33:34,"Build verified: 1.2.5-1 nightly

server commit:
f44f7d9
engine commit:
4e477ab

 Verified MCOL-1968 and MCOL-1989",3,"Build verified: 1.2.5-1 nightly

server commit:
f44f7d9
engine commit:
4e477ab

 Verified MCOL-1968 and MCOL-1989"
709,MCOL-3389,MCOL,Daniel Lee,131415,2019-07-22 22:17:05,"Build verified: 1.1.8-1 nightly

[root@localhost centos7]# cat gitversionInfo.txt 
server commit:
09faec8
engine commit:
7d86e1c

Memory utilization is about the same as in MCOL-1495.",1,"Build verified: 1.1.8-1 nightly

[root@localhost centos7]# cat gitversionInfo.txt 
server commit:
09faec8
engine commit:
7d86e1c

Memory utilization is about the same as in MCOL-1495."
710,MCOL-3398,MCOL,Andrew Hutchings,130532,2019-07-04 13:12:57,Develop-1.2 has been rebased,1,Develop-1.2 has been rebased
711,MCOL-3398,MCOL,Daniel Lee,130618,2019-07-05 15:30:36,"Build verified: 1.2.5-1 nightly

server commit:
f44f7d9
engine commit:
4e477ab
",2,"Build verified: 1.2.5-1 nightly

server commit:
f44f7d9
engine commit:
4e477ab
"
712,MCOL-3416,MCOL,Roman,132483,2019-08-15 09:50:42,Please review.,1,Please review.
713,MCOL-3416,MCOL,Roman,134976,2019-09-27 19:40:02,"Greetings,

This issue should be closed b/c COND_EQUAL is internal representation that
MDB uses and shouldn't be used in CS.

Regards,
Roman


",2,"Greetings,

This issue should be closed b/c COND_EQUAL is internal representation that
MDB uses and shouldn't be used in CS.

Regards,
Roman


"
714,MCOL-3416,MCOL,Daniel Lee,135144,2019-10-01 18:11:14,Closed per development's comment.,3,Closed per development's comment.
715,MCOL-3419,MCOL,David Hall,132234,2019-08-08 21:24:36,Tested with 1.2.2. It did not break. Searching....,1,Tested with 1.2.2. It did not break. Searching....
716,MCOL-3419,MCOL,David Hall,132356,2019-08-12 13:36:53,"It appears that the bug was introduced by MCOL-1559, which was merged after 1.2.5 was released, so no released version should have this problem.

For MCOL-1559 we added std::locale::global() to idb_setlocale() in utils_utf8.h. This is supposed to set the C++ locale, which is a different setting than the C locale set by setlocale(). For reasons not yet know, this is messing up something.",2,"It appears that the bug was introduced by MCOL-1559, which was merged after 1.2.5 was released, so no released version should have this problem.

For MCOL-1559 we added std::locale::global() to idb_setlocale() in utils_utf8.h. This is supposed to set the C++ locale, which is a different setting than the C locale set by setlocale(). For reasons not yet know, this is messing up something."
717,MCOL-3419,MCOL,David Hall,132432,2019-08-13 20:57:48,Can't figure out why std::locale::global() breaks things. We can work without it.,3,Can't figure out why std::locale::global() breaks things. We can work without it.
718,MCOL-3419,MCOL,David Hall,139760,2019-12-10 21:53:42,Found a fix without using  std::locale::global().,4,Found a fix without using  std::locale::global().
719,MCOL-3419,MCOL,Daniel Lee,139861,2019-12-11 20:19:30,"Build verified: 1.2.6-1

[root@localhost ~]# cat gitversionInfo.txt 
engine commit:
d4173ef

The ""Affected version"" is set to 1.2.5, so I tried to reproduce the issue in 1.2.5-1.  I failed to reproduced after trying for couple hours.  It turned out the issue was introduced after 1.2.5-1 was released.  Anyway, I verified the issue did not happen in 1.2.6-1.
",5,"Build verified: 1.2.6-1

[root@localhost ~]# cat gitversionInfo.txt 
engine commit:
d4173ef

The ""Affected version"" is set to 1.2.5, so I tried to reproduce the issue in 1.2.5-1.  I failed to reproduced after trying for couple hours.  It turned out the issue was introduced after 1.2.5-1 was released.  Anyway, I verified the issue did not happen in 1.2.6-1.
"
720,MCOL-3420,MCOL,David Hall,145893,2020-03-05 23:46:38,"QA: To test, set systemlang in the XML to something not accepted. I used ""Bogus""
Try to start the system.
Broken = never start. Possible continuous restart of ProcMgr.

Fixed: Should start up, raise an alarm and log ""Failed to set locale  Bogus : Setting to 'C'. Critical alarm generated""",1,"QA: To test, set systemlang in the XML to something not accepted. I used ""Bogus""
Try to start the system.
Broken = never start. Possible continuous restart of ProcMgr.

Fixed: Should start up, raise an alarm and log ""Failed to set locale  Bogus : Setting to 'C'. Critical alarm generated"""
721,MCOL-3420,MCOL,Daniel Lee,146935,2020-03-17 18:31:34,"Build verified: 1.5.0-1 BB

engine commit:
f01185f

1.4.4-1 is not yet available in buildbot.

crit.log

Mar 17 18:26:57 localhost ProcessManager[12140]: 57.944436 |0|0|0| E 17 CAL0001: Failed to set locale  aloha : Setting to 'C'. Critical alarm generated
Mar 17 18:27:00 localhost ProcessManager[12201]: 00.024550 |0|0|0| E 17 CAL0001: Failed to set locale  aloha : Setting to 'C'. Critical alarm generated
Mar 17 18:27:04 localhost ProcessManager[12234]: 04.075806 |0|0|0| E 17 CAL0001: Failed to set locale  aloha : Setting to 'C'. Critical alarm generated

Critical Active Alarms:

AlarmID           = 36
Brief Description = INVALID_LOCALE
Alarm Severity    = CRITICAL
Time Issued       = Tue Mar 17 18:26:57 2020
Reporting Module  = pm1
Reporting Process = DBRMControllerNode
Reported Device   = system

",2,"Build verified: 1.5.0-1 BB

engine commit:
f01185f

1.4.4-1 is not yet available in buildbot.

crit.log

Mar 17 18:26:57 localhost ProcessManager[12140]: 57.944436 |0|0|0| E 17 CAL0001: Failed to set locale  aloha : Setting to 'C'. Critical alarm generated
Mar 17 18:27:00 localhost ProcessManager[12201]: 00.024550 |0|0|0| E 17 CAL0001: Failed to set locale  aloha : Setting to 'C'. Critical alarm generated
Mar 17 18:27:04 localhost ProcessManager[12234]: 04.075806 |0|0|0| E 17 CAL0001: Failed to set locale  aloha : Setting to 'C'. Critical alarm generated

Critical Active Alarms:

AlarmID           = 36
Brief Description = INVALID_LOCALE
Alarm Severity    = CRITICAL
Time Issued       = Tue Mar 17 18:26:57 2020
Reporting Module  = pm1
Reporting Process = DBRMControllerNode
Reported Device   = system

"
722,MCOL-3420,MCOL,Daniel Lee,147165,2020-03-19 21:12:21,"Build verified: 1.4.4.-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 86a634a0feaf7788c9bcf7cc763e500d2be97d75
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Fri Feb 28 21:55:32 2020 +0100

    Revert ""make columnstore maturity gamma""
    
    This reverts commit e4a0372cd08a53f97a62d6b6ef32114b553cacb7.


/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit ca3e2d78d6e1d06fb6711befe7bb2d618e801929
Merge: ec3630d f437152
Author: Patrick LeBlanc <43503225+pleblanc1976@users.noreply.github.com>
Date:   Thu Mar 19 11:43:55 2020 -0500

    Merge pull request #1113 from pleblanc1976/develop-1.4
    
    Bumped version num to 1.4.4-1
",3,"Build verified: 1.4.4.-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 86a634a0feaf7788c9bcf7cc763e500d2be97d75
Author: Sergei Golubchik 
Date:   Fri Feb 28 21:55:32 2020 +0100

    Revert ""make columnstore maturity gamma""
    
    This reverts commit e4a0372cd08a53f97a62d6b6ef32114b553cacb7.


/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit ca3e2d78d6e1d06fb6711befe7bb2d618e801929
Merge: ec3630d f437152
Author: Patrick LeBlanc 
Date:   Thu Mar 19 11:43:55 2020 -0500

    Merge pull request #1113 from pleblanc1976/develop-1.4
    
    Bumped version num to 1.4.4-1
"
723,MCOL-3483,MCOL,Patrick LeBlanc,133771,2019-09-09 17:20:35,found and fixed mcol-3488,1,found and fixed mcol-3488
724,MCOL-3483,MCOL,Patrick LeBlanc,133874,2019-09-10 18:06:01,"OK it's ready.  I pushed it to the DBS-release branch in my fork of the engine repo, and also to the corp's engine repo.  Remember to build it with the columnstore-1.2.5 commit of the mariadb-columnstore-server repo.",2,"OK it's ready.  I pushed it to the DBS-release branch in my fork of the engine repo, and also to the corp's engine repo.  Remember to build it with the columnstore-1.2.5 commit of the mariadb-columnstore-server repo."
725,MCOL-3483,MCOL,Patrick LeBlanc,133986,2019-09-11 19:43:10,"For the purposes of leaving a paper trail, these are the notes I took while doing this.  Includes lists of commits related to each component touched by the cloud mods.

Copy S3-only directories into 1.2.5  - done
Cherry-pick relevant commits from the OAM / BRM / idbdatafile dirs

OAM-related commits
42b79bd5e77a76e7dc9b73e9b4d24348e8711ebd
98c9cd7b22109c8c139dc205e46ec00643866d0e
f34f785f8a5eaa6ec0f95e18c4b2a6dc27cc098b
0ae0761e1a9659ec52a6a2cbd36099d217e21095
366cd406bad29e5b20d17fb89b6dba61e9a20206
753139a933f758c0439e0f4e92989795f764f124
303d182d5c9a3fdd8ba199f2bc4afb743d2fe9c4
8ae5e948f4ea3bd4104e8ae8d6f4758743ce84f6
fc8f68e39115f19981e258d71028089a6ac650db

checkpointed to origin

BRM-related changes (reverse order)
62c7ab648028d15c888ee88db60ed0d9e6b62424
8190e44d1fa8a90b72594dfb284356ebf1b6f3e0
a84e3a408fa14cd499d749b672c28db72c14f461
9ced374c819465f1130cdae82cbfaf92b73c0dd7
16dc887f2088bcc44a7be59e091ae7f60753c22e
7dca5365b6757b09b5897ce877f813aafed4e435
157d1ef667d52f22d438c71f838079c3fde96823
bcdd76adc02b1e10493a465499f394e031bc2ba2
029b4a61a851d67c923c639242e43d6f4566bb08
603cbc931401a0af10eb7d3940f459e013a72fb5
c0f93d904a80e75c2020c7fb685597b286e58f6e
6e6cfdb59b0cab82abf180a3b66e8c0196ecc34b

checkpointed to origin

IDBdatafile changes (reverse order)
00158c86ecacec6977552d50d1401917185b7b03
9e42ddf72c2851c3a204dfa73583eb2c7236cfa8
eead61f238fe76afc7e6e64ba7a9367847d27b9a
15c256b01195cbb433ffdc67f79e13ec548444e5
dfc29fa833613c2e4ccbe34033e6c0c2226c0282
6e6cfdb59b0cab82abf180a3b66e8c0196ecc34b  -- dup
92f609f718ae505eb7b2fd756968f181b28116f5
d97a570b866a5ad3c0d6a74d8178318e030101ec
acb464618c38dba689e6f86854da62836f1d10c8
f063f78242d3dc429fa6b6177aa7456bbf33f3af
d53471fc759ae3948cf9156b21ba8868b76b60fd

checkpointed to origin

procmgr changes (reverse order)
f6b464f49097fcc6db5da54b33875f18fe34971c
094e5b554ac2b7658ee03109999b477c6d5a0808
45d4a7536be3f500e66c7beffce300a48ba115ba
1f09c533d522d1945353771a67aecc0d9cb021e6
ee81f96f4a869eff1b88cd6c77b85cda5d38beef
9f938447728d15eb1d9a2c602d06befc2c2be750
e8dd6443d62a4c5bab2f7e70bcab45fc91a99b5f
9daa44efa143d3307d4d182671518a541f4f7f2c
52bd0ee967d9fcfae5746135781c28fb4c717f21
be214a6fda6e0fa82da3ff69460b790ebdbc48ff
1a7e7330169094826f890322dae5f9eced20bd2e
d0a576483e2eb7ed56aceb10cb7d25992ae88dbb
7d4f4068886f1d77b4ff23dda47a6e765f190a66
753139a933f758c0439e0f4e92989795f764f124  - dup
366cd406bad29e5b20d17fb89b6dba61e9a20206  - dup
d454f84b82972210aa40db482c9995f0486701cb
15c256b01195cbb433ffdc67f79e13ec548444e5  - dup
98c9cd7b22109c8c139dc205e46ec00643866d0e  - dup

checkpointed to origin

procmon changes (reverse order)
fa1ad8faca94f03cee4f0ae2e5323e608abe633b
094e5b554ac2b7658ee03109999b477c6d5a0808  - dup
d0a576483e2eb7ed56aceb10cb7d25992ae88dbb  - dup
43f7a71bad539fecbbeb62d7aa2d17568c4a30b5
4ec3cdc8b95633fbb7df6def9a00cf4804048e26
3f532eb0596e174f93af13b13f8a10048aa8a20e
eead61f238fe76afc7e6e64ba7a9367847d27b9a  - dup
a79b3131b5e66eacc809819ffdf1e971d1f09206
42b67765981533496b32130fe0e8492eba2d96b1
753139a933f758c0439e0f4e92989795f764f124  - dup
366cd406bad29e5b20d17fb89b6dba61e9a20206  - dup
4ae09352d2613bfd41cfb2a0e5d5870a2c9583ce
d454f84b82972210aa40db482c9995f0486701cb  - dup
15c256b01195cbb433ffdc67f79e13ec548444e5  - dup
98c9cd7b22109c8c139dc205e46ec00643866d0e  - dup

checkpointed to origin

oamapps changes (reverse order)
040f34979813372bfac89c74897f487330d178b0
4ae09352d2613bfd41cfb2a0e5d5870a2c9583ce  - dup
6193d2cda2f2ee261c8366396fdf6c976fda876a

checkpointed to origin

checking the diff vs develop
missed commits
46163c129   - brought it in
51bb9f305  - covered with a patch made by hand
2e5d6db0c73 - brought it in",3,"For the purposes of leaving a paper trail, these are the notes I took while doing this.  Includes lists of commits related to each component touched by the cloud mods.

Copy S3-only directories into 1.2.5  - done
Cherry-pick relevant commits from the OAM / BRM / idbdatafile dirs

OAM-related commits
42b79bd5e77a76e7dc9b73e9b4d24348e8711ebd
98c9cd7b22109c8c139dc205e46ec00643866d0e
f34f785f8a5eaa6ec0f95e18c4b2a6dc27cc098b
0ae0761e1a9659ec52a6a2cbd36099d217e21095
366cd406bad29e5b20d17fb89b6dba61e9a20206
753139a933f758c0439e0f4e92989795f764f124
303d182d5c9a3fdd8ba199f2bc4afb743d2fe9c4
8ae5e948f4ea3bd4104e8ae8d6f4758743ce84f6
fc8f68e39115f19981e258d71028089a6ac650db

checkpointed to origin

BRM-related changes (reverse order)
62c7ab648028d15c888ee88db60ed0d9e6b62424
8190e44d1fa8a90b72594dfb284356ebf1b6f3e0
a84e3a408fa14cd499d749b672c28db72c14f461
9ced374c819465f1130cdae82cbfaf92b73c0dd7
16dc887f2088bcc44a7be59e091ae7f60753c22e
7dca5365b6757b09b5897ce877f813aafed4e435
157d1ef667d52f22d438c71f838079c3fde96823
bcdd76adc02b1e10493a465499f394e031bc2ba2
029b4a61a851d67c923c639242e43d6f4566bb08
603cbc931401a0af10eb7d3940f459e013a72fb5
c0f93d904a80e75c2020c7fb685597b286e58f6e
6e6cfdb59b0cab82abf180a3b66e8c0196ecc34b

checkpointed to origin

IDBdatafile changes (reverse order)
00158c86ecacec6977552d50d1401917185b7b03
9e42ddf72c2851c3a204dfa73583eb2c7236cfa8
eead61f238fe76afc7e6e64ba7a9367847d27b9a
15c256b01195cbb433ffdc67f79e13ec548444e5
dfc29fa833613c2e4ccbe34033e6c0c2226c0282
6e6cfdb59b0cab82abf180a3b66e8c0196ecc34b  -- dup
92f609f718ae505eb7b2fd756968f181b28116f5
d97a570b866a5ad3c0d6a74d8178318e030101ec
acb464618c38dba689e6f86854da62836f1d10c8
f063f78242d3dc429fa6b6177aa7456bbf33f3af
d53471fc759ae3948cf9156b21ba8868b76b60fd

checkpointed to origin

procmgr changes (reverse order)
f6b464f49097fcc6db5da54b33875f18fe34971c
094e5b554ac2b7658ee03109999b477c6d5a0808
45d4a7536be3f500e66c7beffce300a48ba115ba
1f09c533d522d1945353771a67aecc0d9cb021e6
ee81f96f4a869eff1b88cd6c77b85cda5d38beef
9f938447728d15eb1d9a2c602d06befc2c2be750
e8dd6443d62a4c5bab2f7e70bcab45fc91a99b5f
9daa44efa143d3307d4d182671518a541f4f7f2c
52bd0ee967d9fcfae5746135781c28fb4c717f21
be214a6fda6e0fa82da3ff69460b790ebdbc48ff
1a7e7330169094826f890322dae5f9eced20bd2e
d0a576483e2eb7ed56aceb10cb7d25992ae88dbb
7d4f4068886f1d77b4ff23dda47a6e765f190a66
753139a933f758c0439e0f4e92989795f764f124  - dup
366cd406bad29e5b20d17fb89b6dba61e9a20206  - dup
d454f84b82972210aa40db482c9995f0486701cb
15c256b01195cbb433ffdc67f79e13ec548444e5  - dup
98c9cd7b22109c8c139dc205e46ec00643866d0e  - dup

checkpointed to origin

procmon changes (reverse order)
fa1ad8faca94f03cee4f0ae2e5323e608abe633b
094e5b554ac2b7658ee03109999b477c6d5a0808  - dup
d0a576483e2eb7ed56aceb10cb7d25992ae88dbb  - dup
43f7a71bad539fecbbeb62d7aa2d17568c4a30b5
4ec3cdc8b95633fbb7df6def9a00cf4804048e26
3f532eb0596e174f93af13b13f8a10048aa8a20e
eead61f238fe76afc7e6e64ba7a9367847d27b9a  - dup
a79b3131b5e66eacc809819ffdf1e971d1f09206
42b67765981533496b32130fe0e8492eba2d96b1
753139a933f758c0439e0f4e92989795f764f124  - dup
366cd406bad29e5b20d17fb89b6dba61e9a20206  - dup
4ae09352d2613bfd41cfb2a0e5d5870a2c9583ce
d454f84b82972210aa40db482c9995f0486701cb  - dup
15c256b01195cbb433ffdc67f79e13ec548444e5  - dup
98c9cd7b22109c8c139dc205e46ec00643866d0e  - dup

checkpointed to origin

oamapps changes (reverse order)
040f34979813372bfac89c74897f487330d178b0
4ae09352d2613bfd41cfb2a0e5d5870a2c9583ce  - dup
6193d2cda2f2ee261c8366396fdf6c976fda876a

checkpointed to origin

checking the diff vs develop
missed commits
46163c129   - brought it in
51bb9f305  - covered with a patch made by hand
2e5d6db0c73 - brought it in"
726,MCOL-35,MCOL,David Hill,83535,2016-05-20 02:07:35,"mismatch results with decimals

tested sql/binarysigned.sql.log against sql/binarysigned.sql.log.ref
Failed.  Check sql/binarysigned.sql.diff for differences.
tested sql/binaryunsigned.sql.log against sql/binaryunsigned.sql.log.ref
Failed.  Check sql/binaryunsigned.sql.diff for differences.

[root@srvss2 srvswdev11]# diff test011/sql/binarysigned.sql.log test011/sql/binarysigned.sql.log.ref
8,9c8,9
< 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e38	-1.7976931348623157e308
< 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e38	1.7976931348623157e308

",1,"mismatch results with decimals

tested sql/binarysigned.sql.log against sql/binarysigned.sql.log.ref
Failed.  Check sql/binarysigned.sql.diff for differences.
tested sql/binaryunsigned.sql.log against sql/binaryunsigned.sql.log.ref
Failed.  Check sql/binaryunsigned.sql.diff for differences.

[root@srvss2 srvswdev11]# diff test011/sql/binarysigned.sql.log test011/sql/binarysigned.sql.log.ref
8,9c8,9
< 0	1007	-126	-32766	-2147483646	-9223372036854775806	-9.9	-99.99	-99999.99	-9999999999.99	-3.40282e38	-1.7976931348623157e308
< 0	1008	127	32767	2147483647	9223372036854775807	9.9	99.99	99999.99	9999999999.99	3.40282e38	1.7976931348623157e308

"
727,MCOL-35,MCOL,Dipti Joshi,83542,2016-05-20 04:36:46,"[~hill] This does not seem to be correct log - this log you have here is for MCOL-29(see your comments on it) - accidentally you seems to be have copied the same comment here too.

This is a concurrency test and does not involve binarysigned or binaryunigned sql test. It runs multiple threads trying to do parallel connections to UM. The reason for this test to fail is that at some point, MySQL process is going away and connections are failing on some the threads 
Here is the status result
cat status.txt
(seconds=1800, thr=15, rowFactor=10000, batches=0, rows=0, bad count=0, errors=75, hung threads=0)

Indicating total there are 75 errors across threads.

Here are the error messages across all the threads's log

grep -i error *.log
thread.10.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.11.log:ERROR 1178 (42000) at line 1: The storage engine for the table doesn't support The syntax or the data type(s) is not supported by InfiniDB. Please check the InfiniDB syntax guide for supported syntax or data types.
thread.11.log:Unable to set default JobID; Error getting OID for table dmlc.test211_11: IDB-2006: 'dmlc.test211_11' does not exist in InfiniDB.
thread.11.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.11.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.11.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.13.log:ERROR 1178 (42000) at line 1: The storage engine for the table doesn't support The syntax or the data type(s) is not supported by InfiniDB. Please check the InfiniDB syntax guide for supported syntax or data types.
thread.13.log:Unable to set default JobID; Error getting OID for table dmlc.test211_13: IDB-2006: 'dmlc.test211_13' does not exist in InfiniDB.
thread.13.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.13.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.13.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.15.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.15.log:Unable to set default JobID; Error getting OID for table dmlc.test211_15: IDB-2006: 'dmlc.test211_15' does not exist in InfiniDB.
thread.15.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.15.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.15.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.1.log:ERROR 1815 (HY000) at line 1: Internal error: CAL0009: Drop table failed due to  IDB-2006: 'dmlc.test211_1' does not exist in InfiniDB.  
thread.1.log:Unable to set default JobID; Error getting OID for table dmlc.test211_1: IDB-2006: 'dmlc.test211_1' does not exist in InfiniDB.
thread.1.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.1.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.1.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.3.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.3.log:Unable to set default JobID; Error getting OID for table dmlc.test211_3: IDB-2006: 'dmlc.test211_3' does not exist in InfiniDB.
thread.3.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.3.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.3.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.5.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.5.log:Unable to set default JobID; Error getting OID for table dmlc.test211_5: IDB-2006: 'dmlc.test211_5' does not exist in InfiniDB.
thread.5.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.5.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.5.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.7.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.7.log:Unable to set default JobID; Error getting OID for table dmlc.test211_7: IDB-2006: 'dmlc.test211_7' does not exist in InfiniDB.
thread.7.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.7.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.7.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 1815 (HY000) at line 1: Internal error: CAL0009: Drop table failed due to  IDB-2006: 'dmlc.test211_8' does not exist in InfiniDB.  
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.9.log:ERROR 1815 (HY000) at line 1: Internal error: CAL0009: Drop table failed due to  IDB-2006: 'dmlc.test211_9' does not exist in InfiniDB.  
thread.9.log:Unable to set default JobID; Error getting OID for table dmlc.test211_9: IDB-2006: 'dmlc.test211_9' does not exist in InfiniDB.
thread.9.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.9.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.9.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
",2,"[~hill] This does not seem to be correct log - this log you have here is for MCOL-29(see your comments on it) - accidentally you seems to be have copied the same comment here too.

This is a concurrency test and does not involve binarysigned or binaryunigned sql test. It runs multiple threads trying to do parallel connections to UM. The reason for this test to fail is that at some point, MySQL process is going away and connections are failing on some the threads 
Here is the status result
cat status.txt
(seconds=1800, thr=15, rowFactor=10000, batches=0, rows=0, bad count=0, errors=75, hung threads=0)

Indicating total there are 75 errors across threads.

Here are the error messages across all the threads's log

grep -i error *.log
thread.10.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.10.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.11.log:ERROR 1178 (42000) at line 1: The storage engine for the table doesn't support The syntax or the data type(s) is not supported by InfiniDB. Please check the InfiniDB syntax guide for supported syntax or data types.
thread.11.log:Unable to set default JobID; Error getting OID for table dmlc.test211_11: IDB-2006: 'dmlc.test211_11' does not exist in InfiniDB.
thread.11.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.11.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.11.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.12.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.13.log:ERROR 1178 (42000) at line 1: The storage engine for the table doesn't support The syntax or the data type(s) is not supported by InfiniDB. Please check the InfiniDB syntax guide for supported syntax or data types.
thread.13.log:Unable to set default JobID; Error getting OID for table dmlc.test211_13: IDB-2006: 'dmlc.test211_13' does not exist in InfiniDB.
thread.13.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.13.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.13.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.14.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.15.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.15.log:Unable to set default JobID; Error getting OID for table dmlc.test211_15: IDB-2006: 'dmlc.test211_15' does not exist in InfiniDB.
thread.15.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.15.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.15.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.1.log:ERROR 1815 (HY000) at line 1: Internal error: CAL0009: Drop table failed due to  IDB-2006: 'dmlc.test211_1' does not exist in InfiniDB.  
thread.1.log:Unable to set default JobID; Error getting OID for table dmlc.test211_1: IDB-2006: 'dmlc.test211_1' does not exist in InfiniDB.
thread.1.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.1.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.1.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.2.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.3.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.3.log:Unable to set default JobID; Error getting OID for table dmlc.test211_3: IDB-2006: 'dmlc.test211_3' does not exist in InfiniDB.
thread.3.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.3.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.3.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.4.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.5.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.5.log:Unable to set default JobID; Error getting OID for table dmlc.test211_5: IDB-2006: 'dmlc.test211_5' does not exist in InfiniDB.
thread.5.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.5.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.5.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.6.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.7.log:ERROR 2013 (HY000) at line 1: Lost connection to MySQL server during query
thread.7.log:Unable to set default JobID; Error getting OID for table dmlc.test211_7: IDB-2006: 'dmlc.test211_7' does not exist in InfiniDB.
thread.7.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.7.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.7.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 1815 (HY000) at line 1: Internal error: CAL0009: Drop table failed due to  IDB-2006: 'dmlc.test211_8' does not exist in InfiniDB.  
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.8.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.9.log:ERROR 1815 (HY000) at line 1: Internal error: CAL0009: Drop table failed due to  IDB-2006: 'dmlc.test211_9' does not exist in InfiniDB.  
thread.9.log:Unable to set default JobID; Error getting OID for table dmlc.test211_9: IDB-2006: 'dmlc.test211_9' does not exist in InfiniDB.
thread.9.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.9.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
thread.9.log:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/usr/local/Calpont/mysql/lib/mysql/mysql.sock' (111 ""Connection refused"")
"
728,MCOL-35,MCOL,Dipti Joshi,83630,2016-05-23 20:39:00,"This may be related to MCOL-66, MCOL-64, MCOL-14 and MCOL-20",3,"This may be related to MCOL-66, MCOL-64, MCOL-14 and MCOL-20"
729,MCOL-35,MCOL,David Hill,83717,2016-05-26 19:40:51,"Looksl ike DDLProc crash in latest runs, possible related to DROP TABLE issue

HALL - DMLPROC when running test211 today... dont know if this is new or previous

[9:58] 
May 26 09:50:32 srvregtest joblist[3466]: 32.769031 |0|0|0| C 05 CAL0000: distributedenginecomm.cpp @ 381 DEC: lost connection to 192.168.1.125      
May 26 09:50:32 srvregtest joblist[3295]: 32.772833 |0|0|0| C 05 CAL0000: distributedenginecomm.cpp @ 381 DEC: lost connection to 192.168.1.125      
May 26 09:50:32 srvregtest joblist[3400]: 32.774381 |0|0|0| C 05 CAL0000: distributedenginecomm.cpp @ 381 DEC: lost connection to 192.168.1.125      
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.077963 |2147483676|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.078466 |2147483677|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.078664 |2147483678|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.078949 |2147483679|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.084472 |2147483680|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:03 srvregtest ProcessMonitor[5947]: 03.863565 |0|0|0| C 18 CAL0000: *****Calpont Process Restarting: DDLProc, old PID = 6826

david.hill [10:08 AM] 
looksl ike still having issues in drop table

[10:08] 
Thu May 26 09:50:57 CDT 2016 - Running drop table if exists test211_15; create table test211_15(batch int, c1 int, c2 int)engine=infinidb;
Error detected, touch stop.txt
Thu May 26 09:50:57 CDT 2016 - Importing rows:  batch=1; table=test211_9; rows=90000.
Error detected, touch stop.txt
Error detected, touch stop.txt
Error detected, touch stop.txt
Error detected, touch stop.txt",4,"Looksl ike DDLProc crash in latest runs, possible related to DROP TABLE issue

HALL - DMLPROC when running test211 today... dont know if this is new or previous

[9:58] 
May 26 09:50:32 srvregtest joblist[3466]: 32.769031 |0|0|0| C 05 CAL0000: distributedenginecomm.cpp @ 381 DEC: lost connection to 192.168.1.125      
May 26 09:50:32 srvregtest joblist[3295]: 32.772833 |0|0|0| C 05 CAL0000: distributedenginecomm.cpp @ 381 DEC: lost connection to 192.168.1.125      
May 26 09:50:32 srvregtest joblist[3400]: 32.774381 |0|0|0| C 05 CAL0000: distributedenginecomm.cpp @ 381 DEC: lost connection to 192.168.1.125      
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.077963 |2147483676|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.078466 |2147483677|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.078664 |2147483678|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.078949 |2147483679|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:00 srvregtest ExeMgr[6702]: 00.084472 |2147483680|0|0| C 16 CAL0055: ERROR: ExeMgr has caught an exception. InetStreamSocket::readToMagic(): I/O error2.1: err = -1 e = 104: Connection reset by peer
May 26 09:51:03 srvregtest ProcessMonitor[5947]: 03.863565 |0|0|0| C 18 CAL0000: *****Calpont Process Restarting: DDLProc, old PID = 6826

david.hill [10:08 AM] 
looksl ike still having issues in drop table

[10:08] 
Thu May 26 09:50:57 CDT 2016 - Running drop table if exists test211_15; create table test211_15(batch int, c1 int, c2 int)engine=infinidb;
Error detected, touch stop.txt
Thu May 26 09:50:57 CDT 2016 - Importing rows:  batch=1; table=test211_9; rows=90000.
Error detected, touch stop.txt
Error detected, touch stop.txt
Error detected, touch stop.txt
Error detected, touch stop.txt"
730,MCOL-35,MCOL,Dipti Joshi,85179,2016-07-26 19:15:26,"As MCOL-66 fix has been developed, the regression test in development environment shows that this test suite is now passing.",5,"As MCOL-66 fix has been developed, the regression test in development environment shows that this test suite is now passing."
731,MCOL-35,MCOL,Dipti Joshi,85345,2016-08-02 19:25:59,Fixed as part of MCOL-66,6,Fixed as part of MCOL-66
732,MCOL-3503,MCOL,David Hall,134814,2019-09-25 20:00:11,"The Columnstore UDAF uses a template class to differentiate the types of input and allows for smallest type in the hash table.

The Server UDF uses the return type to dictate the datatype of the hash table. This does not allow for the smallest possible hash table. The Return Types are INT_RESULT, REAL_RESULT, DECIMAL_RESULT and STRING_RESULT, with DECIMAL_RESULT expected to return a c string. So we have int64_t, double, char*, char*. That's our options. There doesn't appear to be any way to determine the characteristics of the incoming column. For decimal, I chose to store as long double, as that seemed more efficient than trying to store the incoming decimal strings.",1,"The Columnstore UDAF uses a template class to differentiate the types of input and allows for smallest type in the hash table.

The Server UDF uses the return type to dictate the datatype of the hash table. This does not allow for the smallest possible hash table. The Return Types are INT_RESULT, REAL_RESULT, DECIMAL_RESULT and STRING_RESULT, with DECIMAL_RESULT expected to return a c string. So we have int64_t, double, char*, char*. That's our options. There doesn't appear to be any way to determine the characteristics of the incoming column. For decimal, I chose to store as long double, as that seemed more efficient than trying to store the incoming decimal strings."
733,MCOL-3503,MCOL,David Hall,135058,2019-09-30 18:16:19,There's a regression test pull request as well,2,There's a regression test pull request as well
734,MCOL-3503,MCOL,Daniel Lee,135440,2019-10-07 19:45:52,"Build tested: 1.4.1-1 source

Engine code:

[root@localhost engine]# git show
commit fe5d815a458e41f7e0dd53b1fc2f5245281da5b8
Merge: 51129d6 986e541
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Oct 7 08:34:59 2019 -0500
   Merge pull request #891 from LinuxJedi/fix-ubuntu18
   Fix abs() usage for Ubuntu 18.04

The moda() should return 2.

I worked with Mr. Hall and there seems to be an issue here:

MariaDB [(none)]> select * from mysql.func where name = 'moda';
+------+-----+------------------+-----------+
| name | ret | dl               | type      |
+------+-----+------------------+-----------+
| moda |   1 | libregr_mysql.so | aggregate |
+------+-----+------------------+-----------+

After I changed the value for ret to 4 and restarted the system, the same query would return the correct result.
",3,"Build tested: 1.4.1-1 source

Engine code:

[root@localhost engine]# git show
commit fe5d815a458e41f7e0dd53b1fc2f5245281da5b8
Merge: 51129d6 986e541
Author: David.Hall 
Date:   Mon Oct 7 08:34:59 2019 -0500
   Merge pull request #891 from LinuxJedi/fix-ubuntu18
   Fix abs() usage for Ubuntu 18.04

The moda() should return 2.

I worked with Mr. Hall and there seems to be an issue here:

MariaDB [(none)]> select * from mysql.func where name = 'moda';
+------+-----+------------------+-----------+
| name | ret | dl               | type      |
+------+-----+------------------+-----------+
| moda |   1 | libregr_mysql.so | aggregate |
+------+-----+------------------+-----------+

After I changed the value for ret to 4 and restarted the system, the same query would return the correct result.
"
735,MCOL-3503,MCOL,David Hall,135442,2019-10-07 20:03:46,Changed return type to 4 (DECIMAL_RESULT) in install_calpont_mysql.sh,4,Changed return type to 4 (DECIMAL_RESULT) in install_calpont_mysql.sh
736,MCOL-3503,MCOL,Daniel Lee,135511,2019-10-08 19:21:38,"Build tested: 1.4.1-1 source

[root@localhost engine]# git show
commit 1af0248ae5946efad46e6bce6af761b75da014f7
Merge: ccb36e9 49d63de
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue Oct 8 08:08:41 2019 +0100

    Merge pull request #896 from mariadb-corporation/updateGitIgnore
    
    update .gitignore to reflect generated scripts

For unsigned integer and unsigned bigint, incorrect result is returned:


--------------
CREATE TABLE mcol3503 (C1 INTEGER UNSIGNED) ENGINE=COLUMNSTORE
--------------

Query OK, 0 rows affected (0.168 sec)

--------------
INSERT mcol3503 VALUES (1),(2),(2),(2),(3),(4),(8),(8),(8)
--------------

Query OK, 9 rows affected (0.580 sec)
Records: 9  Duplicates: 0  Warnings: 0

--------------
SELECT AVG(C1) FROM mcol3503
--------------

+---------+
| AVG(C1) |
+---------+
|  4.2222 |
+---------+
1 row in set (0.134 sec)

--------------
SELECT MODA(C1) FROM mcol3503
--------------

+----------+
| MODA(C1) |
+----------+
|        8 |
+----------+
1 row in set (0.016 sec)





--------------
CREATE TABLE mcol3503 (C1 BIGINT UNSIGNED) ENGINE=COLUMNSTORE
--------------

Query OK, 0 rows affected (0.172 sec)

--------------
INSERT mcol3503 VALUES (1),(2),(2),(2),(3),(4),(8),(8),(8)
--------------

Query OK, 9 rows affected (0.890 sec)
Records: 9  Duplicates: 0  Warnings: 0

--------------
SELECT AVG(C1) FROM mcol3503
--------------

+---------+
| AVG(C1) |
+---------+
|  4.2222 |
+---------+
1 row in set (0.163 sec)

--------------
SELECT MODA(C1) FROM mcol3503
--------------

+----------+
| MODA(C1) |
+----------+
|        8 |
+----------+
1 row in set (0.016 sec)


Both queries should return 2 instead of 8.

Also verify moda() with subquery, functions, and having clause.

",5,"Build tested: 1.4.1-1 source

[root@localhost engine]# git show
commit 1af0248ae5946efad46e6bce6af761b75da014f7
Merge: ccb36e9 49d63de
Author: Andrew Hutchings 
Date:   Tue Oct 8 08:08:41 2019 +0100

    Merge pull request #896 from mariadb-corporation/updateGitIgnore
    
    update .gitignore to reflect generated scripts

For unsigned integer and unsigned bigint, incorrect result is returned:


--------------
CREATE TABLE mcol3503 (C1 INTEGER UNSIGNED) ENGINE=COLUMNSTORE
--------------

Query OK, 0 rows affected (0.168 sec)

--------------
INSERT mcol3503 VALUES (1),(2),(2),(2),(3),(4),(8),(8),(8)
--------------

Query OK, 9 rows affected (0.580 sec)
Records: 9  Duplicates: 0  Warnings: 0

--------------
SELECT AVG(C1) FROM mcol3503
--------------

+---------+
| AVG(C1) |
+---------+
|  4.2222 |
+---------+
1 row in set (0.134 sec)

--------------
SELECT MODA(C1) FROM mcol3503
--------------

+----------+
| MODA(C1) |
+----------+
|        8 |
+----------+
1 row in set (0.016 sec)





--------------
CREATE TABLE mcol3503 (C1 BIGINT UNSIGNED) ENGINE=COLUMNSTORE
--------------

Query OK, 0 rows affected (0.172 sec)

--------------
INSERT mcol3503 VALUES (1),(2),(2),(2),(3),(4),(8),(8),(8)
--------------

Query OK, 9 rows affected (0.890 sec)
Records: 9  Duplicates: 0  Warnings: 0

--------------
SELECT AVG(C1) FROM mcol3503
--------------

+---------+
| AVG(C1) |
+---------+
|  4.2222 |
+---------+
1 row in set (0.163 sec)

--------------
SELECT MODA(C1) FROM mcol3503
--------------

+----------+
| MODA(C1) |
+----------+
|        8 |
+----------+
1 row in set (0.016 sec)


Both queries should return 2 instead of 8.

Also verify moda() with subquery, functions, and having clause.

"
737,MCOL-3503,MCOL,Daniel Lee,135630,2019-10-09 14:52:31,"The following test case also failed, for both SIGN and UNSIGNED data types:
TINYINT, SMALLINT, INTEGER, BIGINT, DECIMAL(4), DECIMAL(18)
but DECIMAL(4,2) did return 6.00 as expected.

Does it have something to do with the failed data types being integers?

Example for TINYINT. A return value of 6 is expected
--------------
TRUNCATE TABLE mcol3503
--------------

Query OK, 0 rows affected (0.247 sec)

--------------
INSERT mcol3503 VALUES (1),(2),(3),(4),(5),(6),(7),(8),(15)
--------------

Query OK, 9 rows affected (0.633 sec)
Records: 9  Duplicates: 0  Warnings: 0

--------------
SELECT AVG(C1) FROM mcol3503
--------------

+---------+
| AVG(C1) |
+---------+
|  5.6667 |
+---------+
1 row in set (0.140 sec)

--------------
SELECT 'TINYINT', MODA(C1), IF(MODA(C1)=6,'Passed', 'Failed') AS STATUS FROM mcol3503
--------------

+---------+----------+--------+
| TINYINT | MODA(C1) | STATUS |
+---------+----------+--------+
| TINYINT |        5 | Failed |
+---------+----------+--------+
1 row in set (0.018 sec)

",6,"The following test case also failed, for both SIGN and UNSIGNED data types:
TINYINT, SMALLINT, INTEGER, BIGINT, DECIMAL(4), DECIMAL(18)
but DECIMAL(4,2) did return 6.00 as expected.

Does it have something to do with the failed data types being integers?

Example for TINYINT. A return value of 6 is expected
--------------
TRUNCATE TABLE mcol3503
--------------

Query OK, 0 rows affected (0.247 sec)

--------------
INSERT mcol3503 VALUES (1),(2),(3),(4),(5),(6),(7),(8),(15)
--------------

Query OK, 9 rows affected (0.633 sec)
Records: 9  Duplicates: 0  Warnings: 0

--------------
SELECT AVG(C1) FROM mcol3503
--------------

+---------+
| AVG(C1) |
+---------+
|  5.6667 |
+---------+
1 row in set (0.140 sec)

--------------
SELECT 'TINYINT', MODA(C1), IF(MODA(C1)=6,'Passed', 'Failed') AS STATUS FROM mcol3503
--------------

+---------+----------+--------+
| TINYINT | MODA(C1) | STATUS |
+---------+----------+--------+
| TINYINT |        5 | Failed |
+---------+----------+--------+
1 row in set (0.018 sec)

"
738,MCOL-3503,MCOL,David Hall,135847,2019-10-11 16:26:52,"For unsigned, the issue is this:
avg = 4 (truncated from 4.22 because of integer arithmatic)
value = 2. Calculating distance: the calculation is abs(value - avg) . Because it's unsigned and value < avg, we get value-avg = 4294967294.

The second issue with the return value being 5 rather than 6 is indeed due to INTEGER arithmetic. AVG is being truncated before the compare, so the AVG 5.6667 is turned into 5.",7,"For unsigned, the issue is this:
avg = 4 (truncated from 4.22 because of integer arithmatic)
value = 2. Calculating distance: the calculation is abs(value - avg) . Because it's unsigned and value < avg, we get value-avg = 4294967294.

The second issue with the return value being 5 rather than 6 is indeed due to INTEGER arithmetic. AVG is being truncated before the compare, so the AVG 5.6667 is turned into 5."
739,MCOL-3503,MCOL,David Hall,135849,2019-10-11 17:07:01,"The fix is to use long double arithmetic for tie breaking regardless of the column data type. Long double gives us the full unsigned int precision for integers (on most platforms), while double would have more rounding error for large values.",8,"The fix is to use long double arithmetic for tie breaking regardless of the column data type. Long double gives us the full unsigned int precision for integers (on most platforms), while double would have more rounding error for large values."
740,MCOL-3503,MCOL,Daniel Lee,136692,2019-10-30 22:00:46,"Build tested: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
3e7a964

Encountered the following issue. Investigated on the issue but have not figured out the cause.

MariaDB [mytest]> select moda(c1) from mcol3505;
ERROR 1146 (42S02): Table 'mytest.mcol3505' doesn't exist
MariaDB [mytest]> select moda(c1) from mcol3503;
ERROR 1305 (42000): FUNCTION mytest.moda does not exist
MariaDB [mytest]> CREATE AGGREGATE FUNCTION moda returns DECIMAL soname 'libregr_mysql.so';
ERROR 1125 (HY000): Function 'moda' already exists
",9,"Build tested: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
3e7a964

Encountered the following issue. Investigated on the issue but have not figured out the cause.

MariaDB [mytest]> select moda(c1) from mcol3505;
ERROR 1146 (42S02): Table 'mytest.mcol3505' doesn't exist
MariaDB [mytest]> select moda(c1) from mcol3503;
ERROR 1305 (42000): FUNCTION mytest.moda does not exist
MariaDB [mytest]> CREATE AGGREGATE FUNCTION moda returns DECIMAL soname 'libregr_mysql.so';
ERROR 1125 (HY000): Function 'moda' already exists
"
741,MCOL-3503,MCOL,Daniel Lee,138082,2019-11-18 20:35:37,"Build verfied: 1.4.1-1 github source

Server
commit 77a245fe5658b8d6d937620586ecd802b3432a78
Author: Marko Mäkelä <marko.makela@mariadb.com>
Date:   Sun Nov 17 20:04:11 2019 +0200

Engine
commit 7c6a086cfb54b8bbd500efb41f34c9fa1ed03ca1
Merge: f291d88 1a94d53
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Mon Nov 18 12:11:30 2019 +0300

Make a build from source and verified test case
",10,"Build verfied: 1.4.1-1 github source

Server
commit 77a245fe5658b8d6d937620586ecd802b3432a78
Author: Marko Mäkelä 
Date:   Sun Nov 17 20:04:11 2019 +0200

Engine
commit 7c6a086cfb54b8bbd500efb41f34c9fa1ed03ca1
Merge: f291d88 1a94d53
Author: Roman Nozdrin 
Date:   Mon Nov 18 12:11:30 2019 +0300

Make a build from source and verified test case
"
742,MCOL-3514,MCOL,Andrew Hutchings,134632,2019-09-24 09:54:59,"Implementation details...

New options for cpimport:

{code}
        -y      S3 Authentication Key (for S3 imports)
        -K      S3 Authentication Secret (for S3 imports)
        -t      S3 Bucket (for S3 imports)
        -H      S3 Hostname (for S3 imports, Amazon's S3 default)
        -g      S3 Region (for S3 imports)
{code}

The hostname only needs to be supplied if the S3 server is not Amazon's.

It will then use the path/filename to retrieve the file from the S3 bucket into memory and apply it. You will need enough RAM spare to take the entire CSV file.",1,"Implementation details...

New options for cpimport:

{code}
        -y      S3 Authentication Key (for S3 imports)
        -K      S3 Authentication Secret (for S3 imports)
        -t      S3 Bucket (for S3 imports)
        -H      S3 Hostname (for S3 imports, Amazon's S3 default)
        -g      S3 Region (for S3 imports)
{code}

The hostname only needs to be supplied if the S3 server is not Amazon's.

It will then use the path/filename to retrieve the file from the S3 bucket into memory and apply it. You will need enough RAM spare to take the entire CSV file."
743,MCOL-3514,MCOL,Daniel Lee,134973,2019-09-27 18:55:26,"Build tested: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
1f47534

Running test on multi-node (1um2pm) returned an error

/usr/local/mariadb/columnstore/bin/cpimport mytest lineitem lineitem.tbl -y [mykey] -K [mysecret] -t dleeqatest -g us-west-2 

2019-09-27 18:25:08 (9124) ERR  : Could not open Input file lineitem.tbl


It worked in single node stack:

/usr/local/mariadb/columnstore/bin/cpimport mytest lineitem lineitem.tbl -y [mykey] -K [mysecret] -t dleeqatest -g us-west-2 
Locale is : C

Using table OID 3017 as the default JOB ID
Input file will be read from S3 Bucket : dleeqatest, file/object : /usr/local/mariadb/columnstore/data/bulk/tmpjob/3017_D20190927_T185039_S235758_Job_3017.xml
Job description file : /usr/local/mariadb/columnstore/data/bulk/tmpjob/3017_D20190927_T185039_S235758_Job_3017.xml
Log file for this job: /usr/local/mariadb/columnstore/data/bulk/log/Job_3017.log
2019-09-27 18:50:39 (16701) INFO : successfully loaded job file /usr/local/mariadb/columnstore/data/bulk/tmpjob/3017_D20190927_T185039_S235758_Job_3017.xml
2019-09-27 18:50:39 (16701) INFO : Job file loaded, run time for this step : 0.21343 seconds
2019-09-27 18:50:39 (16701) INFO : PreProcessing check starts
2019-09-27 18:50:55 (16701) INFO : PreProcessing check completed
2019-09-27 18:50:55 (16701) INFO : preProcess completed, run time for this step : 15.8133 seconds
2019-09-27 18:50:55 (16701) INFO : No of Read Threads Spawned = 1
2019-09-27 18:50:55 (16701) INFO : No of Parse Threads Spawned = 3
2019-09-27 18:50:56 (16701) INFO : For table mytest.lineitem: 6005 rows processed and 6005 rows inserted.
2019-09-27 18:50:57 (16701) INFO : Bulk load completed, total run time : 18.0366 seconds

[root@localhost ~]# mcsmysql mytest
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.4.8-3-MariaDB-log Source distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> select count(*) from lineitem;
+----------+
| count(*) |
+----------+
|     6005 |
+----------+
1 row in set (0.119 sec)

",2,"Build tested: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
1f47534

Running test on multi-node (1um2pm) returned an error

/usr/local/mariadb/columnstore/bin/cpimport mytest lineitem lineitem.tbl -y [mykey] -K [mysecret] -t dleeqatest -g us-west-2 

2019-09-27 18:25:08 (9124) ERR  : Could not open Input file lineitem.tbl


It worked in single node stack:

/usr/local/mariadb/columnstore/bin/cpimport mytest lineitem lineitem.tbl -y [mykey] -K [mysecret] -t dleeqatest -g us-west-2 
Locale is : C

Using table OID 3017 as the default JOB ID
Input file will be read from S3 Bucket : dleeqatest, file/object : /usr/local/mariadb/columnstore/data/bulk/tmpjob/3017_D20190927_T185039_S235758_Job_3017.xml
Job description file : /usr/local/mariadb/columnstore/data/bulk/tmpjob/3017_D20190927_T185039_S235758_Job_3017.xml
Log file for this job: /usr/local/mariadb/columnstore/data/bulk/log/Job_3017.log
2019-09-27 18:50:39 (16701) INFO : successfully loaded job file /usr/local/mariadb/columnstore/data/bulk/tmpjob/3017_D20190927_T185039_S235758_Job_3017.xml
2019-09-27 18:50:39 (16701) INFO : Job file loaded, run time for this step : 0.21343 seconds
2019-09-27 18:50:39 (16701) INFO : PreProcessing check starts
2019-09-27 18:50:55 (16701) INFO : PreProcessing check completed
2019-09-27 18:50:55 (16701) INFO : preProcess completed, run time for this step : 15.8133 seconds
2019-09-27 18:50:55 (16701) INFO : No of Read Threads Spawned = 1
2019-09-27 18:50:55 (16701) INFO : No of Parse Threads Spawned = 3
2019-09-27 18:50:56 (16701) INFO : For table mytest.lineitem: 6005 rows processed and 6005 rows inserted.
2019-09-27 18:50:57 (16701) INFO : Bulk load completed, total run time : 18.0366 seconds

[root@localhost ~]# mcsmysql mytest
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.4.8-3-MariaDB-log Source distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> select count(*) from lineitem;
+----------+
| count(*) |
+----------+
|     6005 |
+----------+
1 row in set (0.119 sec)

"
744,MCOL-3514,MCOL,Daniel Lee,144110,2020-02-11 20:08:22,Verified sub-tasks.  Closing this ticket now.,3,Verified sub-tasks.  Closing this ticket now.
745,MCOL-3515,MCOL,Bharath Bokka,135019,2019-09-30 13:31:52,"Build verified: 1.4.0-1

gitversionEngine-
1f47534

/usr/local/mariadb/columnstore/bin/postConfigure -d and /usr/local/mariadb/columnstore/bin/postConfigure -n doesn't work on 1.4.0-1

Ex-
[root@localhost ~]# /usr/local/mariadb/columnstore/bin/postConfigure -d
   ERROR: Invalid Argument = -d
   Usage: postConfigure [-h][-c][-u][-p][-qs][-qm][-qa][-port][-i][-n][-d][-sn][-pm-ip-addrs][-um-ip-addrs][-pm-count][-um-count][-x][-xr][-numBlocksPct][-totalUmMemory]

[root@localhost ~]# /usr/local/mariadb/columnstore/bin/postConfigure -n
   ERROR: Invalid Argument = -n
   Usage: postConfigure [-h][-c][-u][-p][-qs][-qm][-qa][-port][-i][-n][-d][-sn][-pm-ip-addrs][-um-ip-addrs][-pm-count][-um-count][-x][-xr][-numBlocksPct][-totalUmMemory]

postConfigure Help command doesn't list -d and -n options.

Installation works with-
/usr/local/mariadb/columnstore/bin/postConfigure

This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1
.
.
.
.
===== Setup System Server Type Configuration =====

There are 2 options when configuring the System Server Type: single and multi

  'single'  - Single-Server install is used when there will only be 1 server configured
              on the system. It can also be used for production systems, if the plan is
              to stay single-server.

  'multi'   - Multi-Server install is used when you want to configure multiple servers now or
              in the future. With Multi-Server install, you can still configure just 1 server
              now and add on addition servers/modules in the future.

Select the type of System Server install [1=single, 2=multi] (2) >

1.4.0-1 installation works as expected with only install mechanism as non-distribute.
",1,"Build verified: 1.4.0-1

gitversionEngine-
1f47534

/usr/local/mariadb/columnstore/bin/postConfigure -d and /usr/local/mariadb/columnstore/bin/postConfigure -n doesn't work on 1.4.0-1

Ex-
[root@localhost ~]# /usr/local/mariadb/columnstore/bin/postConfigure -d
   ERROR: Invalid Argument = -d
   Usage: postConfigure [-h][-c][-u][-p][-qs][-qm][-qa][-port][-i][-n][-d][-sn][-pm-ip-addrs][-um-ip-addrs][-pm-count][-um-count][-x][-xr][-numBlocksPct][-totalUmMemory]

[root@localhost ~]# /usr/local/mariadb/columnstore/bin/postConfigure -n
   ERROR: Invalid Argument = -n
   Usage: postConfigure [-h][-c][-u][-p][-qs][-qm][-qa][-port][-i][-n][-d][-sn][-pm-ip-addrs][-um-ip-addrs][-pm-count][-um-count][-x][-xr][-numBlocksPct][-totalUmMemory]

postConfigure Help command doesn't list -d and -n options.

Installation works with-
/usr/local/mariadb/columnstore/bin/postConfigure

This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System and will perform a Package
Installation of all of the Servers within the System that is being configured.

IMPORTANT: This tool requires to run on the Performance Module #1
.
.
.
.
===== Setup System Server Type Configuration =====

There are 2 options when configuring the System Server Type: single and multi

  'single'  - Single-Server install is used when there will only be 1 server configured
              on the system. It can also be used for production systems, if the plan is
              to stay single-server.

  'multi'   - Multi-Server install is used when you want to configure multiple servers now or
              in the future. With Multi-Server install, you can still configure just 1 server
              now and add on addition servers/modules in the future.

Select the type of System Server install [1=single, 2=multi] (2) >

1.4.0-1 installation works as expected with only install mechanism as non-distribute.
"
746,MCOL-3520,MCOL,Andrew Hutchings,134972,2019-09-27 18:43:49,"For reference, error in multi-PM:

2019-09-27 18:25:08 (9124) ERR  : Could not open Input file lineitem.tbl",1,"For reference, error in multi-PM:

2019-09-27 18:25:08 (9124) ERR  : Could not open Input file lineitem.tbl"
747,MCOL-3520,MCOL,Daniel Lee,144107,2020-02-11 20:06:14,"removed fixed version 1.5. That is being track by MCOL-3784 now.


",2,"removed fixed version 1.5. That is being track by MCOL-3784 now.


"
748,MCOL-3520,MCOL,Daniel Lee,144109,2020-02-11 20:07:16,"Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 3eed0aaae40a28fd4a16d382e7b9e27621012cc2

Verified with 1um2pm


",3,"Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 3eed0aaae40a28fd4a16d382e7b9e27621012cc2

Verified with 1um2pm


"
749,MCOL-3542,MCOL,patrice,135274,2019-10-03 20:43:22,"the problem is not that the SSL cert can't be verified, it is the url construction that makes it not verifiable. constructing the url in the same way it is when using an IP would make it work. for reference : http://www.wryway.com/blog/aws-s3-url-styles/",1,"the problem is not that the SSL cert can't be verified, it is the url construction that makes it not verifiable. constructing the url in the same way it is when using an IP would make it work. for reference : URL"
750,MCOL-3542,MCOL,Ben Thompson,184134,2021-03-29 20:31:38,"Change should be made to storagemanager.cnf to support setting these option in libmarias3 via StorageManager
SM_USE_HTTP (default disabled -- current default is https) 
SM_SSL_VERIFY (default enabled)",2,"Change should be made to storagemanager.cnf to support setting these option in libmarias3 via StorageManager
SM_USE_HTTP (default disabled -- current default is URL 
SM_SSL_VERIFY (default enabled)"
751,MCOL-3542,MCOL,Ben Thompson,184687,2021-04-02 17:34:42,"cnf file options added:
# Setting use_http to 'enabled' for host to use http instead of https
# The default is use_http = disabled (https)
# use_http = enabled

# Setting ssl_verify to 'disabled' for how to not use SSL verification
# Default is ssl_verify = enabled
# ssl_verify = disabled
",3,"cnf file options added:
# Setting use_http to 'enabled' for host to use http instead of URL
# The default is use_http = disabled (URL
# use_http = enabled

# Setting ssl_verify to 'disabled' for how to not use SSL verification
# Default is ssl_verify = enabled
# ssl_verify = disabled
"
752,MCOL-3551,MCOL,Andrew Hutchings,135798,2019-10-11 10:27:08,[~ben.thompson] for this to work with buildbot you'll need to remove all the path related build flags. I was using just cmake -DRPM=centos7 -DWITH_WSREP=OFF,1,[~ben.thompson] for this to work with buildbot you'll need to remove all the path related build flags. I was using just cmake -DRPM=centos7 -DWITH_WSREP=OFF
753,MCOL-3551,MCOL,Daniel Lee,136613,2019-10-29 21:49:42,"Build verified: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
3e7a964

server is now installed in the following paths

[root@localhost bin]# ps -ef |grep mysql
root     14155     1  0 21:27 ?        00:00:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --pid-file=/var/lib/mysql/localhost.localdomain.pid
mysql    14314 14155  1 21:27 ?        00:00:15 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --user=mysql --log-error=/var/lib/mysql/localhost.localdomain.err --pid-file=/var/lib/mysql/localhost.localdomain.pid

",2,"Build verified: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
3e7a964

server is now installed in the following paths

[root@localhost bin]# ps -ef |grep mysql
root     14155     1  0 21:27 ?        00:00:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --pid-file=/var/lib/mysql/localhost.localdomain.pid
mysql    14314 14155  1 21:27 ?        00:00:15 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --user=mysql --log-error=/var/lib/mysql/localhost.localdomain.err --pid-file=/var/lib/mysql/localhost.localdomain.pid

"
754,MCOL-3552,MCOL,Daniel Lee,136616,2019-10-29 21:58:40,"Build verified: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
3e7a964

columnstore.cnf is now in /etc/my.cnf.d
",1,"Build verified: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
3e7a964

columnstore.cnf is now in /etc/my.cnf.d
"
755,MCOL-3554,MCOL,Jens Röwekamp,135981,2019-10-15 11:05:01,"- mcsapi uses the ports as defined in Columnstore.xml
- added an optional client side Columnstore.xml flag
{noformat}
 <SkySQL>SkipVersionCheck>Y</SkipVersionCheck></SkySQL>
{noformat}
 to skip the ProcMon version check
- (py|java)mcsapi test suite successfully executed against ColumnStore 1.2.2 on Debian 9
- manual pymcsapi connection test with minimal Columnstore.xml (SkipVersionCheck flag and no ProcMon entry) successful on Debian 9",1,"- mcsapi uses the ports as defined in Columnstore.xml
- added an optional client side Columnstore.xml flag
{noformat}
 SkipVersionCheck>Y
{noformat}
 to skip the ProcMon version check
- (py|java)mcsapi test suite successfully executed against ColumnStore 1.2.2 on Debian 9
- manual pymcsapi connection test with minimal Columnstore.xml (SkipVersionCheck flag and no ProcMon entry) successful on Debian 9"
756,MCOL-3554,MCOL,Jens Röwekamp,135983,2019-10-15 11:06:43,"{noformat}
jens@debian9:~$ python3
Python 3.5.3 (default, Sep 27 2018, 17:25:39)
[GCC 6.3.0 20170516] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pymcsapi
>>> d = pymcsapi.ColumnStoreDriver(""/home/jens/ColumnStore-singlenode-client-minimal.xml"")
>>> b = d.createBulkInsert(""test"", ""ti"", 0, 0)
>>> d.listTableLocks()
(<pymcsapi.TableLockInfo; proxy of <Swig Object of type 'mcsapi::TableLockInfo *' at 0x7fd3a90a3b40> >,)
>>> b.rollback()
>>> exit()
{noformat}",2,"{noformat}
jens@debian9:~$ python3
Python 3.5.3 (default, Sep 27 2018, 17:25:39)
[GCC 6.3.0 20170516] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pymcsapi
>>> d = pymcsapi.ColumnStoreDriver(""/home/jens/ColumnStore-singlenode-client-minimal.xml"")
>>> b = d.createBulkInsert(""test"", ""ti"", 0, 0)
>>> d.listTableLocks()
( >,)
>>> b.rollback()
>>> exit()
{noformat}"
757,MCOL-3554,MCOL,Daniel Lee,139153,2019-12-03 15:15:40,Patch has been verified and used by the SkySQL team,3,Patch has been verified and used by the SkySQL team
758,MCOL-3556,MCOL,Andrew Hutchings,136200,2019-10-18 14:12:49,For QA: To enable ColumnStore to replicate events from another engine you need to set columnstore_replication_slave=on in one of the my.cnf files (such as columnstore.cnf) instead of setting the XML setting as per 1.2. The XML setting is now ignored.,1,For QA: To enable ColumnStore to replicate events from another engine you need to set columnstore_replication_slave=on in one of the my.cnf files (such as columnstore.cnf) instead of setting the XML setting as per 1.2. The XML setting is now ignored.
759,MCOL-3556,MCOL,Daniel Lee,136967,2019-11-04 22:05:31,"Build tested: 1.4.1-1

engine commit:
3e7a964

Test replication between two columnstore installations.  On the slave, columnstore_replication_slave can be inserted in the server.cnf or columnstore.cnf.  Columnstore engine and Innodb engine tables are replicated as it.  Is this the expected behavior? or is the innodb table should also be replicated as columnstore table?
",2,"Build tested: 1.4.1-1

engine commit:
3e7a964

Test replication between two columnstore installations.  On the slave, columnstore_replication_slave can be inserted in the server.cnf or columnstore.cnf.  Columnstore engine and Innodb engine tables are replicated as it.  Is this the expected behavior? or is the innodb table should also be replicated as columnstore table?
"
760,MCOL-3556,MCOL,Andrew Hutchings,137190,2019-11-06 16:36:31,"That is expected behaviour, we haven't done any magic to MariaDB server to change table type on the fly (although there are ways of doing that)",3,"That is expected behaviour, we haven't done any magic to MariaDB server to change table type on the fly (although there are ways of doing that)"
761,MCOL-3559,MCOL,Andrew Hutchings,138037,2019-11-18 13:55:43,"If mysqld is already running postConfigure will now give the error:

MariaDB Server is currently running on PID 47204. Cannot run postConfigure whilst this is running. Exiting..",1,"If mysqld is already running postConfigure will now give the error:

MariaDB Server is currently running on PID 47204. Cannot run postConfigure whilst this is running. Exiting.."
762,MCOL-3559,MCOL,Daniel Lee,138640,2019-11-25 20:27:20,"Build verified: 1.4.1-1 gitub source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit f9ceb0a67ffb20631c936a7e8e8776c000d677ac
Author: Marko Mäkelä <marko.makela@mariadb.com>

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit e1435a34ac4dcf49c5c86984d83839f91434c13b
Merge: 6b91667 a76ceb5
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>

",2,"Build verified: 1.4.1-1 gitub source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit f9ceb0a67ffb20631c936a7e8e8776c000d677ac
Author: Marko Mäkelä 

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit e1435a34ac4dcf49c5c86984d83839f91434c13b
Merge: 6b91667 a76ceb5
Author: Andrew Hutchings 

"
763,MCOL-3563,MCOL,Ben Thompson,136769,2019-10-31 20:34:53,Fixed effected classes to handle errors correctly. Also worth noting the warning will not appear in centos7 with default compiler installed. Testing verified with regression and storagemanager unit tests. Only would've seen this if posix::read failed and other things would have failed if that were happening.,1,Fixed effected classes to handle errors correctly. Also worth noting the warning will not appear in centos7 with default compiler installed. Testing verified with regression and storagemanager unit tests. Only would've seen this if posix::read failed and other things would have failed if that were happening.
764,MCOL-3563,MCOL,Daniel Lee,139648,2019-12-09 16:46:31,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable

Engine

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit a9717ad49daded3a037313556f68bcad7826fa39
Merge: 3f5e1fd c29c41e
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Mon Dec 9 11:55:08 2019 +0200

    Merge pull request #962 from jmrojas2332/MCOL-3474
    
    MCOL 3474 Fix TIMEDIFF w/ non-temporal data types

Made the latest build with debug option on and capture all terminal output to a file.  Verified that such messages are no longer outputted.

",2,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable

Engine

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit a9717ad49daded3a037313556f68bcad7826fa39
Merge: 3f5e1fd c29c41e
Author: Roman Nozdrin 
Date:   Mon Dec 9 11:55:08 2019 +0200

    Merge pull request #962 from jmrojas2332/MCOL-3474
    
    MCOL 3474 Fix TIMEDIFF w/ non-temporal data types

Made the latest build with debug option on and capture all terminal output to a file.  Verified that such messages are no longer outputted.

"
765,MCOL-3577,MCOL,Patrick LeBlanc,138489,2019-11-22 19:51:41,Merged it,1,Merged it
766,MCOL-3581,MCOL,Andrew Hutchings,136750,2019-10-31 16:28:02,[~drrtuy] has implemented this in 1.2 and will port it to 1.4 which will be tracked in MCOL-894.,1,[~drrtuy] has implemented this in 1.2 and will port it to 1.4 which will be tracked in MCOL-894.
767,MCOL-3581,MCOL,Daniel Lee,137412,2019-11-08 20:52:42,"Build tested: 1.2.6-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
5db61b9

I did not observe and performance change when changing infinidb_orderby_threads from 1 to 8.
The same test in 1.4.1-1 has a noticeable performance gain.
It did not make a different when I set infinidb_ordered_only to on or off

1.2.5-1

[root@localhost ~]# mcsmysql tpch10
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 17
Server version: 10.3.16-MariaDB-log Columnstore 1.2.5-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [tpch10]>  select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (1 min 20.821 sec)

MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+-------------+
| l_orderkey | l_comment   |
+------------+-------------+
|   28832352 |  about the  |
|    7868261 |  about the  |
|    7873062 |  about the  |
|    7865349 |  about the  |
|    7864577 |  about the  |
|    7882560 |  about the  |
|   24644000 |  about the  |
|    7865091 |  about the  |
|   20456834 |  about the  |
|   16260771 |  about the  |
+------------+-------------+
10 rows in set (1 min 45.069 sec)


1.2.6-1

infinidb_orderby_threads 

MariaDB [tpch10]> set infinidb_orderby_threads=1;
Query OK, 0 rows affected (0.000 sec)

MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (1 min 22.179 sec)


MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+-------------+
| l_orderkey | l_comment   |
+------------+-------------+
|    2099971 |  about the  |
|     519554 |  about the  |
|     525381 |  about the  |
|    4709378 |  about the  |
|    4727808 |  about the  |
|    4738112 |  about the  |
|    2624033 |  about the  |
|     516389 |  about the  |
|    1568610 |  about the  |
|    1597216 |  about the  |
+------------+-------------+
10 rows in set (1 min 46.981 sec)

MariaDB [tpch10]> set infinidb_orderby_threads=8;
Query OK, 0 rows affected (0.000 sec)


MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (1 min 23.097 sec)


MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+-------------+
| l_orderkey | l_comment   |
+------------+-------------+
|     519554 |  about the  |
|     516389 |  about the  |
|    1568610 |  about the  |
|    3705219 |  about the  |
|    3669733 |  about the  |
|     620550 |  about the  |
|     525381 |  about the  |
|    2624033 |  about the  |
|    3707879 |  about the  |
|    3703911 |  about the  |
+------------+-------------+
10 rows in set (1 min 46.995 sec)

",2,"Build tested: 1.2.6-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
5db61b9

I did not observe and performance change when changing infinidb_orderby_threads from 1 to 8.
The same test in 1.4.1-1 has a noticeable performance gain.
It did not make a different when I set infinidb_ordered_only to on or off

1.2.5-1

[root@localhost ~]# mcsmysql tpch10
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 17
Server version: 10.3.16-MariaDB-log Columnstore 1.2.5-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [tpch10]>  select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (1 min 20.821 sec)

MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+-------------+
| l_orderkey | l_comment   |
+------------+-------------+
|   28832352 |  about the  |
|    7868261 |  about the  |
|    7873062 |  about the  |
|    7865349 |  about the  |
|    7864577 |  about the  |
|    7882560 |  about the  |
|   24644000 |  about the  |
|    7865091 |  about the  |
|   20456834 |  about the  |
|   16260771 |  about the  |
+------------+-------------+
10 rows in set (1 min 45.069 sec)


1.2.6-1

infinidb_orderby_threads 

MariaDB [tpch10]> set infinidb_orderby_threads=1;
Query OK, 0 rows affected (0.000 sec)

MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (1 min 22.179 sec)


MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+-------------+
| l_orderkey | l_comment   |
+------------+-------------+
|    2099971 |  about the  |
|     519554 |  about the  |
|     525381 |  about the  |
|    4709378 |  about the  |
|    4727808 |  about the  |
|    4738112 |  about the  |
|    2624033 |  about the  |
|     516389 |  about the  |
|    1568610 |  about the  |
|    1597216 |  about the  |
+------------+-------------+
10 rows in set (1 min 46.981 sec)

MariaDB [tpch10]> set infinidb_orderby_threads=8;
Query OK, 0 rows affected (0.000 sec)


MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (1 min 23.097 sec)


MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+-------------+
| l_orderkey | l_comment   |
+------------+-------------+
|     519554 |  about the  |
|     516389 |  about the  |
|    1568610 |  about the  |
|    3705219 |  about the  |
|    3669733 |  about the  |
|     620550 |  about the  |
|     525381 |  about the  |
|    2624033 |  about the  |
|    3707879 |  about the  |
|    3703911 |  about the  |
+------------+-------------+
10 rows in set (1 min 46.995 sec)

"
768,MCOL-3581,MCOL,Daniel Lee,137416,2019-11-08 21:08:46,"It turned out the performance would realize when subquery is used.  Modified test queries to run as subqueries and performance is virtually the same as 1.4.1-1.

MariaDB [tpch10]> set infinidb_orderby_threads=1;
Query OK, 0 rows affected (0.000 sec)

MariaDB [tpch10]> select * from ( select l_orderkey from lineitem order by l_orderkey limit 10) a;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (4.360 sec)

MariaDB [tpch10]> select * from (select l_orderkey, l_comment from lineitem order by l_comment limit 10) a;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|    8907489 |  Tiresias  |
|   17668260 |  Tiresias  |
|   42581319 |  Tiresias  |
|   17615556 |  Tiresias  |
|   17323302 |  Tiresias  |
|   21883973 |  Tiresias  |
|   56633829 |  Tiresias  |
|   42593603 |  Tiresias  |
|    9386147 |  Tiresias  |
|     753413 |  Tiresias  |
+------------+------------+
10 rows in set (35.330 sec)

MariaDB [tpch10]> set infinidb_orderby_threads=8;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select * from ( select l_orderkey from lineitem order by l_orderkey limit 10) a;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (2.670 sec)

MariaDB [tpch10]> select * from (select l_orderkey, l_comment from lineitem order by l_comment limit 10) a;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|     753413 |  Tiresias  |
|   35694661 |  Tiresias  |
|   33292934 |  Tiresias  |
|    6527073 |  Tiresias  |
|   52480608 |  Tiresias  |
|   58061957 |  Tiresias  |
|   53788672 |  Tiresias  |
|   44041447 |  Tiresias  |
|    8907489 |  Tiresias  |
|    3948419 |  Tiresias  |
+------------+------------+
10 rows in set (16.507 sec)
",3,"It turned out the performance would realize when subquery is used.  Modified test queries to run as subqueries and performance is virtually the same as 1.4.1-1.

MariaDB [tpch10]> set infinidb_orderby_threads=1;
Query OK, 0 rows affected (0.000 sec)

MariaDB [tpch10]> select * from ( select l_orderkey from lineitem order by l_orderkey limit 10) a;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (4.360 sec)

MariaDB [tpch10]> select * from (select l_orderkey, l_comment from lineitem order by l_comment limit 10) a;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|    8907489 |  Tiresias  |
|   17668260 |  Tiresias  |
|   42581319 |  Tiresias  |
|   17615556 |  Tiresias  |
|   17323302 |  Tiresias  |
|   21883973 |  Tiresias  |
|   56633829 |  Tiresias  |
|   42593603 |  Tiresias  |
|    9386147 |  Tiresias  |
|     753413 |  Tiresias  |
+------------+------------+
10 rows in set (35.330 sec)

MariaDB [tpch10]> set infinidb_orderby_threads=8;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select * from ( select l_orderkey from lineitem order by l_orderkey limit 10) a;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (2.670 sec)

MariaDB [tpch10]> select * from (select l_orderkey, l_comment from lineitem order by l_comment limit 10) a;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|     753413 |  Tiresias  |
|   35694661 |  Tiresias  |
|   33292934 |  Tiresias  |
|    6527073 |  Tiresias  |
|   52480608 |  Tiresias  |
|   58061957 |  Tiresias  |
|   53788672 |  Tiresias  |
|   44041447 |  Tiresias  |
|    8907489 |  Tiresias  |
|    3948419 |  Tiresias  |
+------------+------------+
10 rows in set (16.507 sec)
"
769,MCOL-3585,MCOL,Jose Rojas,138347,2019-11-21 06:21:54,"Updated PmMaxMemorySmall, FifoSizeLargeSize, and FifoSize settings from config file.

RowAggrThreads and RowAggrBuckets are already being automated. I updated automation. Now automating MaxOutstandingRequests as well.

Decreasing FifoSize and FifoSizeLargeSide did not cause any performance hit.
",1,"Updated PmMaxMemorySmall, FifoSizeLargeSize, and FifoSize settings from config file.

RowAggrThreads and RowAggrBuckets are already being automated. I updated automation. Now automating MaxOutstandingRequests as well.

Decreasing FifoSize and FifoSizeLargeSide did not cause any performance hit.
"
770,MCOL-3585,MCOL,Patrick LeBlanc,138490,2019-11-22 19:52:13,Merged it,2,Merged it
771,MCOL-3597,MCOL,Andrew Hutchings,137366,2019-11-08 11:18:37,"This is likely due to MCOL-3474, the test may need updating.",1,"This is likely due to MCOL-3474, the test may need updating."
772,MCOL-3597,MCOL,Daniel Lee,138321,2019-11-20 18:51:20,"Build verified: 1.4.1-1

Server
/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 6cedb671e99038f1a10e0d8504f835aaabed9780
Author: Marko Mäkelä <marko.makela@mariadb.com>

Engine
commit 0f86a3ab14a530183c4fc30b752f8c54c89f13d2
Merge: 7c6a086 2275f4f
Author: benthompson15 <ben.thompson.015@gmail.com>
Date:   Tue Nov 19 23:07:31 2019 +0100

MariaDB [tpch1]> select cidx, CDATETIME, TIMEDIFF(CDATETIME,'2007-02-28 22:23:0') from datatypetestm;
+------+---------------------+------------------------------------------+
| cidx | CDATETIME           | TIMEDIFF(CDATETIME,'2007-02-28 22:23:0') |
+------+---------------------+------------------------------------------+
|    1 | 1997-01-01 00:00:00 | -838:59:59                               |
|    2 | 1997-01-01 00:00:01 | -838:59:59                               |
|    3 | 1997-01-02 00:00:01 | -838:59:59                               |
|    4 | 1997-01-03 00:00:02 | -838:59:59                               |
|    5 | 1997-01-04 00:00:03 | -838:59:59                               |
|    6 | 2009-12-31 23:59:56 | 838:59:59                                |
|    7 | 2009-12-31 23:59:57 | 838:59:59                                |
|    8 | 2009-12-31 23:59:58 | 838:59:59                                |
|    9 | 2009-12-31 23:59:59 | 838:59:59                                |
|   10 | 2009-12-31 23:59:59 | 838:59:59                                |
|   11 | 2009-12-31 23:59:59 | 838:59:59                                |
+------+---------------------+------------------------------------------+
11 rows in set (0.011 sec)




",2,"Build verified: 1.4.1-1

Server
/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 6cedb671e99038f1a10e0d8504f835aaabed9780
Author: Marko Mäkelä 

Engine
commit 0f86a3ab14a530183c4fc30b752f8c54c89f13d2
Merge: 7c6a086 2275f4f
Author: benthompson15 
Date:   Tue Nov 19 23:07:31 2019 +0100

MariaDB [tpch1]> select cidx, CDATETIME, TIMEDIFF(CDATETIME,'2007-02-28 22:23:0') from datatypetestm;
+------+---------------------+------------------------------------------+
| cidx | CDATETIME           | TIMEDIFF(CDATETIME,'2007-02-28 22:23:0') |
+------+---------------------+------------------------------------------+
|    1 | 1997-01-01 00:00:00 | -838:59:59                               |
|    2 | 1997-01-01 00:00:01 | -838:59:59                               |
|    3 | 1997-01-02 00:00:01 | -838:59:59                               |
|    4 | 1997-01-03 00:00:02 | -838:59:59                               |
|    5 | 1997-01-04 00:00:03 | -838:59:59                               |
|    6 | 2009-12-31 23:59:56 | 838:59:59                                |
|    7 | 2009-12-31 23:59:57 | 838:59:59                                |
|    8 | 2009-12-31 23:59:58 | 838:59:59                                |
|    9 | 2009-12-31 23:59:59 | 838:59:59                                |
|   10 | 2009-12-31 23:59:59 | 838:59:59                                |
|   11 | 2009-12-31 23:59:59 | 838:59:59                                |
+------+---------------------+------------------------------------------+
11 rows in set (0.011 sec)




"
773,MCOL-3598,MCOL,Gagan Goel,143446,2020-01-31 18:48:29,This was fixed in [PR 969|https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/969].,1,This was fixed in [PR 969|URL
774,MCOL-3598,MCOL,Daniel Lee,144766,2020-02-20 22:27:34,"Build verified: 1.4.3-1 Azure build (20200220)

Verified test case in the ticket.
",2,"Build verified: 1.4.3-1 Azure build (20200220)

Verified test case in the ticket.
"
775,MCOL-3599,MCOL,Andrew Hutchings,137367,2019-11-08 11:25:25,They are created and installed. Unfortunately the library is in the wrong place so they don't load.,1,They are created and installed. Unfortunately the library is in the wrong place so they don't load.
776,MCOL-3599,MCOL,Daniel Lee,138083,2019-11-18 20:37:21,"Build verfied: 1.4.1-1 github source

Server
commit 77a245fe5658b8d6d937620586ecd802b3432a78
Author: Marko Mäkelä <marko.makela@mariadb.com>
Date:   Sun Nov 17 20:04:11 2019 +0200

Engine
commit 7c6a086cfb54b8bbd500efb41f34c9fa1ed03ca1
Merge: f291d88 1a94d53
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Mon Nov 18 12:11:30 2019 +0300

",2,"Build verfied: 1.4.1-1 github source

Server
commit 77a245fe5658b8d6d937620586ecd802b3432a78
Author: Marko Mäkelä 
Date:   Sun Nov 17 20:04:11 2019 +0200

Engine
commit 7c6a086cfb54b8bbd500efb41f34c9fa1ed03ca1
Merge: f291d88 1a94d53
Author: Roman Nozdrin 
Date:   Mon Nov 18 12:11:30 2019 +0300

"
777,MCOL-3601,MCOL,Daniel Lee,138697,2019-11-26 15:28:50,"Build verified: 1.4.1-1
engine commit:
57724e5

",1,"Build verified: 1.4.1-1
engine commit:
57724e5

"
778,MCOL-3602,MCOL,Daniel Lee,138700,2019-11-26 15:40:34,"Build verified: 1.4.1-1
engine commit:
57724e5

",1,"Build verified: 1.4.1-1
engine commit:
57724e5

"
779,MCOL-3603,MCOL,Daniel Lee,138702,2019-11-26 15:46:11,"Build verified: 1.4.1-1
engine commit:
57724e5
",1,"Build verified: 1.4.1-1
engine commit:
57724e5
"
780,MCOL-3606,MCOL,Daniel Lee,138182,2019-11-19 20:38:41,"Build verified: 1.4.1-1 github source

Server
commit 589a1235b64866c7bbe85da2a6f6bf19ee8282fe
Merge: 77a245f 39d8652
Author: Marko Mäkelä <marko.makela@mariadb.com>
Date:   Tue Nov 19 01:32:50 2019 +0200

Engine
commit 7c6a086cfb54b8bbd500efb41f34c9fa1ed03ca1
Merge: f291d88 1a94d53
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Mon Nov 18 12:11:30 2019 +0300

",1,"Build verified: 1.4.1-1 github source

Server
commit 589a1235b64866c7bbe85da2a6f6bf19ee8282fe
Merge: 77a245f 39d8652
Author: Marko Mäkelä 
Date:   Tue Nov 19 01:32:50 2019 +0200

Engine
commit 7c6a086cfb54b8bbd500efb41f34c9fa1ed03ca1
Merge: f291d88 1a94d53
Author: Roman Nozdrin 
Date:   Mon Nov 18 12:11:30 2019 +0300

"
781,MCOL-3607,MCOL,Richard Stracke,137543,2019-11-12 12:51:48,"workarround:

select log10(CAST(sum(a) as decimal)) from log10test;

",1,"workarround:

select log10(CAST(sum(a) as decimal)) from log10test;

"
782,MCOL-3607,MCOL,David Hall,137599,2019-11-12 23:20:30,Added long double support to math functions,2,Added long double support to math functions
783,MCOL-3607,MCOL,David Hall,137600,2019-11-12 23:30:29,"For QA, the following math functions are involved:
acos, asin, atan, cos, cot, log, log2, log10, sin, sqrt, tan, radians, degrees

Special cases of atan and log take an optional second parameter.

The test is to use sum(v1) or avg(v1) as the argument ",3,"For QA, the following math functions are involved:
acos, asin, atan, cos, cot, log, log2, log10, sin, sqrt, tan, radians, degrees

Special cases of atan and log take an optional second parameter.

The test is to use sum(v1) or avg(v1) as the argument "
784,MCOL-3607,MCOL,Daniel Lee,138181,2019-11-19 20:36:57,"Build verified: 1.2.6-1

MariaDB [mytest]> select log10(sum(a)) from log10test;
+-------------------+
| log10(sum(a))     |
+-------------------+
| 2.089905111439398 |
+-------------------+
1 row in set (0.065 sec)

Also verified other functions in the last comment.

Fixed has not yet been up merged to 1.4.1-1
",4,"Build verified: 1.2.6-1

MariaDB [mytest]> select log10(sum(a)) from log10test;
+-------------------+
| log10(sum(a))     |
+-------------------+
| 2.089905111439398 |
+-------------------+
1 row in set (0.065 sec)

Also verified other functions in the last comment.

Fixed has not yet been up merged to 1.4.1-1
"
785,MCOL-3607,MCOL,Daniel Lee,138645,2019-11-25 21:52:35,The fix for 1.4.1-1 will be tracked in a different ticket.  Closing this one.,5,The fix for 1.4.1-1 will be tracked in a different ticket.  Closing this one.
786,MCOL-3609,MCOL,Gagan Goel,161831,2020-08-03 15:29:02,"As part of this task, also update dbcon/execplan/calpontsystemcatalog.h::isUnsigned()/isSignedInteger() to include UDECIMAL/DECIMAL. When this change is done, any code calling execplan::isUnsigned()/execplan::isSignedInteger() would also need to be reviewed.",1,"As part of this task, also update dbcon/execplan/calpontsystemcatalog.h::isUnsigned()/isSignedInteger() to include UDECIMAL/DECIMAL. When this change is done, any code calling execplan::isUnsigned()/execplan::isSignedInteger() would also need to be reviewed."
787,MCOL-3609,MCOL,Roman,161834,2020-08-03 15:46:38,This won't be done under this issue. This will be a separate project.,2,This won't be done under this issue. This will be a separate project.
788,MCOL-3612,MCOL,Andrew Hutchings,137743,2019-11-13 21:38:35,"PRs in engine, regression-suite and data-adapters",1,"PRs in engine, regression-suite and data-adapters"
789,MCOL-3612,MCOL,Daniel Lee,138299,2019-11-20 16:54:25,"Build verified: 1.4.1-1

Server
/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 6cedb671e99038f1a10e0d8504f835aaabed9780
Author: Marko Mäkelä <marko.makela@mariadb.com>

Engine
commit 0f86a3ab14a530183c4fc30b752f8c54c89f13d2
Merge: 7c6a086 2275f4f
Author: benthompson15 <ben.thompson.015@gmail.com>
Date:   Tue Nov 19 23:07:31 2019 +0100

Verified related tickets.",2,"Build verified: 1.4.1-1

Server
/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 6cedb671e99038f1a10e0d8504f835aaabed9780
Author: Marko Mäkelä 

Engine
commit 0f86a3ab14a530183c4fc30b752f8c54c89f13d2
Merge: 7c6a086 2275f4f
Author: benthompson15 
Date:   Tue Nov 19 23:07:31 2019 +0100

Verified related tickets."
790,MCOL-3624,MCOL,Daniel Lee,139543,2019-12-06 20:39:09,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
Regression test only.
",1,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings 
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
Regression test only.
"
791,MCOL-3624,MCOL,Patrick LeBlanc,141512,2020-01-03 15:34:26,"For MCOL-3702, added a bit of logic to figure out the right name for the jemalloc lib.  Affects this as well.",2,"For MCOL-3702, added a bit of logic to figure out the right name for the jemalloc lib.  Affects this as well."
792,MCOL-3625,MCOL,Daniel Lee,139549,2019-12-06 21:27:36,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
When building Server and Engine separately.  I got:

MariaDB-1.4.2-1-x86_64-centos7-columnstore-engine.rpm
MariaDB-1.4.2-1-x86_64-centos7-columnstore-libs.rpm
MariaDB-1.4.2-1-x86_64-centos7-columnstore-platform.rpm
MariaDB-backup-10.4.10_4-1.el7.x86_64.rpm
MariaDB-backup-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-client-10.4.10_4-1.el7.x86_64.rpm
MariaDB-client-debuginfo-10.4.10_4-1.el7.x86_64.rpm
mariadb-columnstore-1.4.2-1-centos7.x86_64.rpm.tar.gz
MariaDB-common-10.4.10_4-1.el7.x86_64.rpm
MariaDB-common-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-connect-engine-10.4.10_4-1.el7.x86_64.rpm
MariaDB-connect-engine-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-devel-10.4.10_4-1.el7.x86_64.rpm
MariaDB-devel-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-gssapi-server-10.4.10_4-1.el7.x86_64.rpm
MariaDB-gssapi-server-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-rocksdb-engine-10.4.10_4-1.el7.x86_64.rpm
MariaDB-rocksdb-engine-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-server-10.4.10_4-1.el7.x86_64.rpm
MariaDB-server-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-shared-10.4.10_4-1.el7.x86_64.rpm
MariaDB-shared-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-test-10.4.10_4-1.el7.x86_64.rpm
MariaDB-test-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-tokudb-engine-10.4.10_4-1.el7.x86_64.rpm
MariaDB-tokudb-engine-debuginfo-10.4.10_4-1.el7.x86_64.rpm

When build by the server in Azura, we got engine files in the follow format

MariaDB-columnstore-[component]-[release number]*, as in

MariaDB-columnstore-engine-10.4.10_4-1.el7.x86_64.rpm




",1,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings 
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
When building Server and Engine separately.  I got:

MariaDB-1.4.2-1-x86_64-centos7-columnstore-engine.rpm
MariaDB-1.4.2-1-x86_64-centos7-columnstore-libs.rpm
MariaDB-1.4.2-1-x86_64-centos7-columnstore-platform.rpm
MariaDB-backup-10.4.10_4-1.el7.x86_64.rpm
MariaDB-backup-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-client-10.4.10_4-1.el7.x86_64.rpm
MariaDB-client-debuginfo-10.4.10_4-1.el7.x86_64.rpm
mariadb-columnstore-1.4.2-1-centos7.x86_64.rpm.tar.gz
MariaDB-common-10.4.10_4-1.el7.x86_64.rpm
MariaDB-common-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-connect-engine-10.4.10_4-1.el7.x86_64.rpm
MariaDB-connect-engine-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-devel-10.4.10_4-1.el7.x86_64.rpm
MariaDB-devel-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-gssapi-server-10.4.10_4-1.el7.x86_64.rpm
MariaDB-gssapi-server-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-rocksdb-engine-10.4.10_4-1.el7.x86_64.rpm
MariaDB-rocksdb-engine-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-server-10.4.10_4-1.el7.x86_64.rpm
MariaDB-server-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-shared-10.4.10_4-1.el7.x86_64.rpm
MariaDB-shared-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-test-10.4.10_4-1.el7.x86_64.rpm
MariaDB-test-debuginfo-10.4.10_4-1.el7.x86_64.rpm
MariaDB-tokudb-engine-10.4.10_4-1.el7.x86_64.rpm
MariaDB-tokudb-engine-debuginfo-10.4.10_4-1.el7.x86_64.rpm

When build by the server in Azura, we got engine files in the follow format

MariaDB-columnstore-[component]-[release number]*, as in

MariaDB-columnstore-engine-10.4.10_4-1.el7.x86_64.rpm




"
793,MCOL-3626,MCOL,Daniel Lee,139542,2019-12-06 20:34:28,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
",1,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings 
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
"
794,MCOL-3627,MCOL,Daniel Lee,139550,2019-12-06 21:30:18,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
[100%] Linking CXX shared library ../../lib/ha_columnstore.so
[100%] Built target ha_columnstore
[ 90%] Built target ha_columnstore
",1,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings 
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson15/update-gitignore
    
[100%] Linking CXX shared library ../../lib/ha_columnstore.so
[100%] Built target ha_columnstore
[ 90%] Built target ha_columnstore
"
795,MCOL-3628,MCOL,Daniel Lee,139669,2019-12-09 21:00:22,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable

Engine

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit a9717ad49daded3a037313556f68bcad7826fa39
Merge: 3f5e1fd c29c41e
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Mon Dec 9 11:55:08 2019 +0200

    Merge pull request #962 from jmrojas2332/MCOL-3474
    
    MCOL 3474 Fix TIMEDIFF w/ non-temporal data types

All ColumnStore related tickets in MENT-124 has been verified and closed.  Also performance Autopilot regression tests suite.

",1,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable

Engine

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit a9717ad49daded3a037313556f68bcad7826fa39
Merge: 3f5e1fd c29c41e
Author: Roman Nozdrin 
Date:   Mon Dec 9 11:55:08 2019 +0200

    Merge pull request #962 from jmrojas2332/MCOL-3474
    
    MCOL 3474 Fix TIMEDIFF w/ non-temporal data types

All ColumnStore related tickets in MENT-124 has been verified and closed.  Also performance Autopilot regression tests suite.

"
796,MCOL-3630,MCOL,Andrew Hutchings,139455,2019-12-05 16:52:10,"Removed from ""columnstore-post-install""",1,"Removed from ""columnstore-post-install"""
797,MCOL-3630,MCOL,Daniel Lee,139548,2019-12-06 20:59:16,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson1",2,"Build verified: 1.4.2-1 source

Server

commit 6e1a53f89ee8d4eb9e66257b01898995aa7691de
Author: Sergei Golubchik 
Date:   Mon Dec 2 18:07:11 2019 +0100

    MENT-240 change plugin-maturity default to stable


Engine

commit 1d2ea3ad1f2a3a4eed3b220da1ca6082b2cff28f
Merge: 1d6abbe 83ef73a
Author: Andrew Hutchings 
Date:   Fri Dec 6 17:34:56 2019 +0000

    Merge pull request #972 from benthompson1"
798,MCOL-3631,MCOL,David Hall,139187,2019-12-03 23:13:25,"Internals were converted to using long double. This may be the cause of longer decimal precision. If so, then the golden files need to be updated.",1,"Internals were converted to using long double. This may be the cause of longer decimal precision. If so, then the golden files need to be updated."
799,MCOL-3631,MCOL,Roman,139196,2019-12-04 07:10:05,"Thank you for the explanation.


",2,"Thank you for the explanation.


"
800,MCOL-3631,MCOL,David Hall,146258,2020-03-10 20:14:19,"QA: Not sure what you need to do here. If you have any tests for the regr_*** suite, you probably need to update your golden answers.",3,"QA: Not sure what you need to do here. If you have any tests for the regr_*** suite, you probably need to update your golden answers."
801,MCOL-3631,MCOL,Daniel Lee,147777,2020-03-25 21:19:49,"Build verified: 1.4.4-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit aaf2a53452447b0223866db16e88d52448986ea3
Author: Monty <monty@mariadb.org>
Date:   Thu Aug 8 23:04:05 2019 +0300

    MENT-401: Include Aria and S3 index length limit increase in ES 10.4
    
    Cherry-picked from:
    
    98ea611940fd492fc5f883625f2afcbbab312795
    
    MDEV-20279 Increase Aria index length limit

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 0574127148e4c0a1cd9a341c7b9e8ffc982d0d24
Merge: ca3e2d7 cd7372f
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Mar 23 13:43:27 2020 -0500

    Merge pull request #1099 from pleblanc1976/mcol-2022-1.4
    
    Mcol 2022 1.4

I performed Autopilot's window function (precision of 8 decimal places) test suite on both 1.2.6-1 and 1.4.4-1 and test results matched.  My reference test result is from Oracle and there is no need to update it.

There is no actually ColumnStore code change for this ticket.  It is for update reference test results if needed.

",4,"Build verified: 1.4.4-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit aaf2a53452447b0223866db16e88d52448986ea3
Author: Monty 
Date:   Thu Aug 8 23:04:05 2019 +0300

    MENT-401: Include Aria and S3 index length limit increase in ES 10.4
    
    Cherry-picked from:
    
    98ea611940fd492fc5f883625f2afcbbab312795
    
    MDEV-20279 Increase Aria index length limit

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 0574127148e4c0a1cd9a341c7b9e8ffc982d0d24
Merge: ca3e2d7 cd7372f
Author: David.Hall 
Date:   Mon Mar 23 13:43:27 2020 -0500

    Merge pull request #1099 from pleblanc1976/mcol-2022-1.4
    
    Mcol 2022 1.4

I performed Autopilot's window function (precision of 8 decimal places) test suite on both 1.2.6-1 and 1.4.4-1 and test results matched.  My reference test result is from Oracle and there is no need to update it.

There is no actually ColumnStore code change for this ticket.  It is for update reference test results if needed.

"
802,MCOL-3632,MCOL,Gagan Goel,139742,2019-12-10 18:04:48,working_tpch1_compareLogOnly/windowFunctions/q0035.sql needs further investigation.,1,working_tpch1_compareLogOnly/windowFunctions/q0035.sql needs further investigation.
803,MCOL-3632,MCOL,David Hall,142776,2020-01-23 20:26:11,"q0035 is a negative test. That is, it's supposed to generate an error. Unfortunately, code changes catch the error so soon, that they don't know what's going on and produce a not so useful message.

This last reported a useful error message in 1.1",2,"q0035 is a negative test. That is, it's supposed to generate an error. Unfortunately, code changes catch the error so soon, that they don't know what's going on and produce a not so useful message.

This last reported a useful error message in 1.1"
804,MCOL-3632,MCOL,David Hall,142781,2020-01-23 21:48:43,I have submitted a pull request to correct the error message.,3,I have submitted a pull request to correct the error message.
805,MCOL-3632,MCOL,David Hall,143085,2020-01-28 17:29:13,Resubmitted PR ,4,Resubmitted PR 
806,MCOL-3632,MCOL,Daniel Lee,147732,2020-03-25 18:07:10,"Build verified: 1.4.4-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 86a634a0feaf7788c9bcf7cc763e500d2be97d75
Author: Sergei Golubchik <serg@mariadb.org>
Date:   Fri Feb 28 21:55:32 2020 +0100

    Revert ""make columnstore maturity gamma""
    
    This reverts commit e4a0372cd08a53f97a62d6b6ef32114b553cacb7.

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit ca3e2d78d6e1d06fb6711befe7bb2d618e801929
Merge: ec3630d f437152
Author: Patrick LeBlanc <43503225+pleblanc1976@users.noreply.github.com>
Date:   Thu Mar 19 11:43:55 2020 -0500

    Merge pull request #1113 from pleblanc1976/develop-1.4
    
    Bumped version num to 1.4.4-1

The query now returns the same error as 1.2.6-1.  Autopilot's window functions test suite also return the same result as 1.2.6-1.

MariaDB [tpch1]> SELECT lead(ten, (SELECT two FROM tenk1 WHERE s.unique2 = unique2)) OVER (PARTITION BY four ORDER BY ten) FROM tenk1 s WHERE unique2 < 10;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-3016: Function or Operator with sub query on the SELECT clause is currently not supported.
",5,"Build verified: 1.4.4-1 source

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 86a634a0feaf7788c9bcf7cc763e500d2be97d75
Author: Sergei Golubchik 
Date:   Fri Feb 28 21:55:32 2020 +0100

    Revert ""make columnstore maturity gamma""
    
    This reverts commit e4a0372cd08a53f97a62d6b6ef32114b553cacb7.

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit ca3e2d78d6e1d06fb6711befe7bb2d618e801929
Merge: ec3630d f437152
Author: Patrick LeBlanc 
Date:   Thu Mar 19 11:43:55 2020 -0500

    Merge pull request #1113 from pleblanc1976/develop-1.4
    
    Bumped version num to 1.4.4-1

The query now returns the same error as 1.2.6-1.  Autopilot's window functions test suite also return the same result as 1.2.6-1.

MariaDB [tpch1]> SELECT lead(ten, (SELECT two FROM tenk1 WHERE s.unique2 = unique2)) OVER (PARTITION BY four ORDER BY ten) FROM tenk1 s WHERE unique2 < 10;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-3016: Function or Operator with sub query on the SELECT clause is currently not supported.
"
807,MCOL-3647,MCOL,Andrew Hutchings,139267,2019-12-04 15:19:36,PRs in Engine and API trees.,1,PRs in Engine and API trees.
808,MCOL-3647,MCOL,Daniel Lee,140422,2019-12-16 17:04:20,"Build verified: 1.4.2-1 source

Server

commit 809261c65c2aef686bd1d3ad16580870fdf786c2
Author: Alexey Bychko <abychko@gmail.com>
Date:   Thu Dec 12 18:58:18 2019 +0700

    MENT-558 move Galera MTR tests to the separate pipeline or disable for long MTR run
    
    added one more stage to run MTR tests 2*3 in parallel

Engine

commit 451284aeee3de57c03e6e74e904e3c0da52b3b1b
Merge: ff96140 114c5be
Author: benthompson15 <ben.thompson.015@gmail.com>
Date:   Fri Dec 13 22:07:37 2019 +0100

    Merge pull request #982 from LinuxJedi/MCOL-3669a
    
    MCOL-3669 Add real versioning to plugin


Verified tickets 1.4.2-1
Also check buildbot log
",2,"Build verified: 1.4.2-1 source

Server

commit 809261c65c2aef686bd1d3ad16580870fdf786c2
Author: Alexey Bychko 
Date:   Thu Dec 12 18:58:18 2019 +0700

    MENT-558 move Galera MTR tests to the separate pipeline or disable for long MTR run
    
    added one more stage to run MTR tests 2*3 in parallel

Engine

commit 451284aeee3de57c03e6e74e904e3c0da52b3b1b
Merge: ff96140 114c5be
Author: benthompson15 
Date:   Fri Dec 13 22:07:37 2019 +0100

    Merge pull request #982 from LinuxJedi/MCOL-3669a
    
    MCOL-3669 Add real versioning to plugin


Verified tickets 1.4.2-1
Also check buildbot log
"
809,MCOL-366,MCOL,David Thompson,87506,2016-10-17 22:25:14,also ideally automate setup for cross engine joins as some tests rely on this or minimally document this in README.,1,also ideally automate setup for cross engine joins as some tests rely on this or minimally document this in README.
810,MCOL-366,MCOL,David Thompson,87607,2016-10-19 03:52:52,I fixed these as part of my non root work. Will still need you to make nightly changes once its merged.,2,I fixed these as part of my non root work. Will still need you to make nightly changes once its merged.
811,MCOL-366,MCOL,David Hill,87783,2016-10-26 13:43:48,"nightlly builds as root user is working, just need to update the systemReference after each new pull.",3,"nightlly builds as root user is working, just need to update the systemReference after each new pull."
812,MCOL-366,MCOL,David Hill,87784,2016-10-26 13:44:33,"reviewed and tested, all is good",4,"reviewed and tested, all is good"
813,MCOL-3686,MCOL,Andrew Hutchings,141518,2020-01-03 16:59:57,Created sub-tasks for the things that can be dealt with in 1.4. To some extent systemd is already used (columnstore-post-install installs the service currently and after postConfigure it can be used). The rest is part of a bigger project for 1.5 which will be linked to this when the tasks are created.,1,Created sub-tasks for the things that can be dealt with in 1.4. To some extent systemd is already used (columnstore-post-install installs the service currently and after postConfigure it can be used). The rest is part of a bigger project for 1.5 which will be linked to this when the tasks are created.
814,MCOL-3686,MCOL,Ralf Gebhardt,141843,2020-01-10 09:10:55,The list in this task has been defined in Frankfurt to be the minimum needed to change the maturity of the storage engine to stable,2,The list in this task has been defined in Frankfurt to be the minimum needed to change the maturity of the storage engine to stable
815,MCOL-3686,MCOL,Ralf Gebhardt,145606,2020-03-03 11:36:59,"Hi [~julien.fritsch], 

can this task really be closed? The requirements from Frankfurt are covered, but the task is listing other topics [~serg] wants to get addressed at some point.

 /Ralf",3,"Hi [~julien.fritsch], 

can this task really be closed? The requirements from Frankfurt are covered, but the task is listing other topics [~serg] wants to get addressed at some point.

 /Ralf"
816,MCOL-3686,MCOL,David Hall,152413,2020-05-08 14:55:47,All subtasks are done,4,All subtasks are done
817,MCOL-3707,MCOL,Andrew Hutchings,142561,2020-01-21 19:40:01,Engine side of this is up for review combined with MCOL-3708. There is a server counterpart which can't have a PR until this is merged.,1,Engine side of this is up for review combined with MCOL-3708. There is a server counterpart which can't have a PR until this is merged.
818,MCOL-3707,MCOL,Daniel Lee,144744,2020-02-20 17:32:24,"Build verified: 1.4.3-1 Azure build (20200220)

Verified both RPM and DEB packages.  colummstore-post-install does not need to be executed manually.

",2,"Build verified: 1.4.3-1 Azure build (20200220)

Verified both RPM and DEB packages.  colummstore-post-install does not need to be executed manually.

"
819,MCOL-3708,MCOL,Andrew Hutchings,142562,2020-01-21 19:40:21,In the PR with MCOL-3707,1,In the PR with MCOL-3707
820,MCOL-3708,MCOL,Daniel Lee,144033,2020-02-10 21:44:33,"Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 5efa6a4dc52129be2de49fdfc23e44020401b86b

Verified RPM packages only.  DEB packages are not yet available.

[root@localhost ~]# rpm -e $(rpm -qa 'MariaDB*')
shutdownsystem   Mon Feb 10 21:41:51 2020

This command stops the processing of applications on all Modules within the MariaDB ColumnStore System

   Checking for active transactions

   Stopping System...
   Successful stop of System 

   Shutting Down System...
   Successful shutdown of System 

 
Mariab Columnstore uninstall completed
",2,"Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 5efa6a4dc52129be2de49fdfc23e44020401b86b

Verified RPM packages only.  DEB packages are not yet available.

[root@localhost ~]# rpm -e $(rpm -qa 'MariaDB*')
shutdownsystem   Mon Feb 10 21:41:51 2020

This command stops the processing of applications on all Modules within the MariaDB ColumnStore System

   Checking for active transactions

   Stopping System...
   Successful stop of System 

   Shutting Down System...
   Successful shutdown of System 

 
Mariab Columnstore uninstall completed
"
821,MCOL-3708,MCOL,Daniel Lee,144205,2020-02-12 22:04:54,"Build tested: 1.4.3-1 BB nightly

Engine commit: 8588678

Centos 7 RPM worked as specified, but Debian 8 DEB did not run ""mcsadmin shutdown"".

I used ""dpkg -r"" and ""dpkg -P"" to remove the packages.  Is there a different command I should use?
root@jessie:~# dpkg -r $(dpkg -l |grep Maria|awk -F"" "" '{print $2}')
(Reading database ... 72839 files and directories currently installed.)
Removing libmariadb3-compat (1:10.4.12.6+maria~jessie) ...
Removing mariadb-columnstore-engine (1.4.3-1) ...
Removing mariadb-columnstore-libs (1.4.3-1) ...
Removing mariadb-columnstore-platform (1.4.3-1) ...
dpkg: warning: while removing mariadb-columnstore-platform, directory '/var/lib/columnstore/local' not empty so not removed
dpkg: warning: while removing mariadb-columnstore-platform, directory '/etc/columnstore' not empty so not removed
Removing mariadb-server-10.4 (1:10.4.12.6+maria~jessie) ...
Warning: Unit file of mysql.service changed on disk, 'systemctl daemon-reload' recommended.
Removing mariadb-server-core-10.4 (1:10.4.12.6+maria~jessie) ...
dpkg: warning: while removing mariadb-server-core-10.4, directory '/usr/share/mysql' not empty so not removed
Removing mariadb-client-10.4 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-client-core-10.4 (1:10.4.12.6+maria~jessie) ...
Removing libmariadb3:amd64 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-common (1:10.4.12.6+maria~jessie) ...
Removing mysql-common (1:10.4.12.6+maria~jessie) ...
Processing triggers for man-db (2.7.0.2-5) ...
Processing triggers for libc-bin (2.19-18+deb8u6) ...
root@jessie:~# ma
-bash: /usr/bin/mcsadmin: No such file or directory
root@jessie:~# ps -ef |grep ExeMgr
root      4441  3297  0 21:02 ?        00:00:00 [ExeMgr]
root     17328 15864  0 21:40 pts/0    00:00:00 grep ExeMgr

root@jessie:~# dpkg -P $(dpkg -l |grep Maria|awk -F"" "" '{print $2}')
(Reading database ... 72361 files and directories currently installed.)
Removing libmariadb3:amd64 (1:10.4.12.6+maria~jessie) ...
Purging configuration files for libmariadb3:amd64 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-client-10.4 (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mariadb-client-10.4 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-common (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mariadb-common (1:10.4.12.6+maria~jessie) ...
Removing mariadb-server-10.4 (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mariadb-server-10.4 (1:10.4.12.6+maria~jessie) ...
userdel: user mysql is currently used by process 3870
Removing mysql-common (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mysql-common (1:10.4.12.6+maria~jessie) ...
root@jessie:~# ps -ef |grep ExeMgr
root      4441  3297  0 21:02 ?        00:00:00 [ExeMgr]
root     18454 17849  0 21:57 pts/0    00:00:00 grep ExeMgr




",3,"Build tested: 1.4.3-1 BB nightly

Engine commit: 8588678

Centos 7 RPM worked as specified, but Debian 8 DEB did not run ""mcsadmin shutdown"".

I used ""dpkg -r"" and ""dpkg -P"" to remove the packages.  Is there a different command I should use?
root@jessie:~# dpkg -r $(dpkg -l |grep Maria|awk -F"" "" '{print $2}')
(Reading database ... 72839 files and directories currently installed.)
Removing libmariadb3-compat (1:10.4.12.6+maria~jessie) ...
Removing mariadb-columnstore-engine (1.4.3-1) ...
Removing mariadb-columnstore-libs (1.4.3-1) ...
Removing mariadb-columnstore-platform (1.4.3-1) ...
dpkg: warning: while removing mariadb-columnstore-platform, directory '/var/lib/columnstore/local' not empty so not removed
dpkg: warning: while removing mariadb-columnstore-platform, directory '/etc/columnstore' not empty so not removed
Removing mariadb-server-10.4 (1:10.4.12.6+maria~jessie) ...
Warning: Unit file of mysql.service changed on disk, 'systemctl daemon-reload' recommended.
Removing mariadb-server-core-10.4 (1:10.4.12.6+maria~jessie) ...
dpkg: warning: while removing mariadb-server-core-10.4, directory '/usr/share/mysql' not empty so not removed
Removing mariadb-client-10.4 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-client-core-10.4 (1:10.4.12.6+maria~jessie) ...
Removing libmariadb3:amd64 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-common (1:10.4.12.6+maria~jessie) ...
Removing mysql-common (1:10.4.12.6+maria~jessie) ...
Processing triggers for man-db (2.7.0.2-5) ...
Processing triggers for libc-bin (2.19-18+deb8u6) ...
root@jessie:~# ma
-bash: /usr/bin/mcsadmin: No such file or directory
root@jessie:~# ps -ef |grep ExeMgr
root      4441  3297  0 21:02 ?        00:00:00 [ExeMgr]
root     17328 15864  0 21:40 pts/0    00:00:00 grep ExeMgr

root@jessie:~# dpkg -P $(dpkg -l |grep Maria|awk -F"" "" '{print $2}')
(Reading database ... 72361 files and directories currently installed.)
Removing libmariadb3:amd64 (1:10.4.12.6+maria~jessie) ...
Purging configuration files for libmariadb3:amd64 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-client-10.4 (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mariadb-client-10.4 (1:10.4.12.6+maria~jessie) ...
Removing mariadb-common (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mariadb-common (1:10.4.12.6+maria~jessie) ...
Removing mariadb-server-10.4 (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mariadb-server-10.4 (1:10.4.12.6+maria~jessie) ...
userdel: user mysql is currently used by process 3870
Removing mysql-common (1:10.4.12.6+maria~jessie) ...
Purging configuration files for mysql-common (1:10.4.12.6+maria~jessie) ...
root@jessie:~# ps -ef |grep ExeMgr
root      4441  3297  0 21:02 ?        00:00:00 [ExeMgr]
root     18454 17849  0 21:57 pts/0    00:00:00 grep ExeMgr




"
822,MCOL-3708,MCOL,Daniel Lee,144741,2020-02-20 17:09:45,"Build verified: 1.4.3-1 Azure build (20200220)

Verified both RPM and DEB packages.
",4,"Build verified: 1.4.3-1 Azure build (20200220)

Verified both RPM and DEB packages.
"
823,MCOL-3718,MCOL,David Hall,144124,2020-02-11 23:13:40,Ben asked a question in the PR. I don't understand what he's asking,1,Ben asked a question in the PR. I don't understand what he's asking
824,MCOL-3718,MCOL,Andrew Hutchings,144145,2020-02-12 10:01:32,"Race condition could have caused ""mcsadmin restart"" to hang. This is now fixed in the PR",2,"Race condition could have caused ""mcsadmin restart"" to hang. This is now fixed in the PR"
825,MCOL-3718,MCOL,Daniel Lee,144359,2020-02-14 17:33:12,Caused the issue in MCOL-3793,3,Caused the issue in MCOL-3793
826,MCOL-3718,MCOL,Daniel Lee,144593,2020-02-18 22:59:16,"Build verified: 1.4.3-1 BB nightly

engine commit:
1a65d34

1.4.2-1
[root@localhost columnstore_tmp_files]# cat actionMysqlCalpont.log
Starting MySQL. SUCCESS! 

1.4.3-1

[root@localhost columnstore_tmp_files]# cat actionMysqlCalpont.log 
● mariadb.service - MariaDB 10.4.12-6 database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─migrated-from-my.cnf-settings.conf
   Active: inactive (dead)
     Docs: man:mysqld(8)
           https://mariadb.com/kb/en/library/systemd/

Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] Event Scheduler: Purging the queue. 0 events
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: FTS optimize thread exiting.
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 server_audit: STOPPED
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: Starting shutdown...
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: Dumping buffer pool(s) to /var/lib/mysql/ib_buffer_pool
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: Buffer pool(s) dump completed at 200218 16:55:36
Feb 18 16:55:38 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:38 0 [Note] InnoDB: Shutdown completed; log sequence number 77050; transaction id 50
Feb 18 16:55:38 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:38 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
Feb 18 16:55:38 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:38 0 [Note] /usr/sbin/mysqld: Shutdown complete
Feb 18 16:55:38 localhost.localdomain systemd[1]: Stopped MariaDB 10.4.12-6 database server.


",4,"Build verified: 1.4.3-1 BB nightly

engine commit:
1a65d34

1.4.2-1
[root@localhost columnstore_tmp_files]# cat actionMysqlCalpont.log
Starting MySQL. SUCCESS! 

1.4.3-1

[root@localhost columnstore_tmp_files]# cat actionMysqlCalpont.log 
● mariadb.service - MariaDB 10.4.12-6 database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─migrated-from-my.cnf-settings.conf
   Active: inactive (dead)
     Docs: man:mysqld(8)
           URL

Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] Event Scheduler: Purging the queue. 0 events
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: FTS optimize thread exiting.
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 server_audit: STOPPED
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: Starting shutdown...
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: Dumping buffer pool(s) to /var/lib/mysql/ib_buffer_pool
Feb 18 16:55:36 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:36 0 [Note] InnoDB: Buffer pool(s) dump completed at 200218 16:55:36
Feb 18 16:55:38 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:38 0 [Note] InnoDB: Shutdown completed; log sequence number 77050; transaction id 50
Feb 18 16:55:38 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:38 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
Feb 18 16:55:38 localhost.localdomain mysqld[14913]: 2020-02-18 16:55:38 0 [Note] /usr/sbin/mysqld: Shutdown complete
Feb 18 16:55:38 localhost.localdomain systemd[1]: Stopped MariaDB 10.4.12-6 database server.


"
827,MCOL-3721,MCOL,Andrew Hutchings,143477,2020-02-01 15:04:55,We don't have any code to give warnings on other DDL issues so cannot generate warnings there. But we can generate a warning on an ORDER BY.,1,We don't have any code to give warnings on other DDL issues so cannot generate warnings there. But we can generate a warning on an ORDER BY.
828,MCOL-3721,MCOL,David Hall,144184,2020-02-12 16:47:43,"QA:

{code:java}
--Test COLLATE in ORDER BY
DROP TABLE IF EXISTS test_collate;
CREATE TABLE test_collate (a INT, b INT) ENGINE=columnstore;
INSERT INTO test_collate VALUES (1,2), (2,4);
SELECT a, b FROM test_collate ORDER BY a COLLATE latin1_german2_ci;
SHOW WARNINGS;
SELECT a, b FROM test_collate ORDER BY a COLLATE latin1_german2_ci DESC;
SHOW WARNINGS;
DROP TABLE IF EXISTS test_collate;

--Test COLLATE in table definition and column definition
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (col1 CHAR(10)) CHARSET latin1 COLLATE latin1_bin ENGINE=columnstore;
INSERT INTO t1 VALUES ('a'), ('1'), ('-1');
SELECT col1 FROM t1;
DESCRIBE t1;
DROP TABLE IF EXISTS t1;

DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (col1 CHAR(10) CHARACTER SET utf8 COLLATE utf8_unicode_ci) ENGINE=columnstore;
INSERT INTO t1 VALUES ('a'), ('1'), ('-1');
SELECT col1 FROM t1;
DESCRIBE t1;
DROP TABLE IF EXISTS t1;
{code}
Should result in warning message
Note	1618	COLLATE is ignored in ColumnStore",2,"QA:

{code:java}
--Test COLLATE in ORDER BY
DROP TABLE IF EXISTS test_collate;
CREATE TABLE test_collate (a INT, b INT) ENGINE=columnstore;
INSERT INTO test_collate VALUES (1,2), (2,4);
SELECT a, b FROM test_collate ORDER BY a COLLATE latin1_german2_ci;
SHOW WARNINGS;
SELECT a, b FROM test_collate ORDER BY a COLLATE latin1_german2_ci DESC;
SHOW WARNINGS;
DROP TABLE IF EXISTS test_collate;

--Test COLLATE in table definition and column definition
DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (col1 CHAR(10)) CHARSET latin1 COLLATE latin1_bin ENGINE=columnstore;
INSERT INTO t1 VALUES ('a'), ('1'), ('-1');
SELECT col1 FROM t1;
DESCRIBE t1;
DROP TABLE IF EXISTS t1;

DROP TABLE IF EXISTS t1;
CREATE TABLE t1 (col1 CHAR(10) CHARACTER SET utf8 COLLATE utf8_unicode_ci) ENGINE=columnstore;
INSERT INTO t1 VALUES ('a'), ('1'), ('-1');
SELECT col1 FROM t1;
DESCRIBE t1;
DROP TABLE IF EXISTS t1;
{code}
Should result in warning message
Note	1618	COLLATE is ignored in ColumnStore"
829,MCOL-3721,MCOL,Daniel Lee,144207,2020-02-12 22:38:11,"Build tested: 1.4.3-1 BB nightly
Engine commit: 8588678

MariaDB [mytest]> show warnings;
+-------+------+-----------------------------------+
| Level | Code | Message                           |
+-------+------+-----------------------------------+
| Note  | 1618 | COLLATE is ignored in ColumnStore |
+-------+------+-----------------------------------+
1 row in set (0.000 sec)

",3,"Build tested: 1.4.3-1 BB nightly
Engine commit: 8588678

MariaDB [mytest]> show warnings;
+-------+------+-----------------------------------+
| Level | Code | Message                           |
+-------+------+-----------------------------------+
| Note  | 1618 | COLLATE is ignored in ColumnStore |
+-------+------+-----------------------------------+
1 row in set (0.000 sec)

"
830,MCOL-3731,MCOL,Ben Thompson,143378,2020-01-30 22:48:14,Not sure if this falls in your QA bucket but handing off anyway.,1,Not sure if this falls in your QA bucket but handing off anyway.
831,MCOL-3731,MCOL,Ben Thompson,143379,2020-01-30 22:48:48,image available on dockerhub,2,image available on dockerhub
832,MCOL-3731,MCOL,Daniel Lee,143855,2020-02-06 22:11:13,Verified that the docker image is in docker hub,3,Verified that the docker image is in docker hub
833,MCOL-3732,MCOL,Ben Thompson,143372,2020-01-30 22:35:51,All other dockerhub images have been removed that are unofficial columnstore images,1,All other dockerhub images have been removed that are unofficial columnstore images
834,MCOL-3733,MCOL,Ben Thompson,143380,2020-01-30 22:50:51,Image available on dockerhub,1,Image available on dockerhub
835,MCOL-3733,MCOL,Ben Thompson,143381,2020-01-30 22:51:28,This and MCOL-3731 are basically the same ticket.,2,This and MCOL-3731 are basically the same ticket.
836,MCOL-3733,MCOL,Daniel Lee,143905,2020-02-07 15:51:14,verified,3,verified
837,MCOL-3741,MCOL,David Hall,196444,2021-08-11 15:11:49,There are pull requests for regression as well,1,There are pull requests for regression as well
838,MCOL-3741,MCOL,Daniel Lee,206726,2021-11-22 16:59:18,"Build tests 6.2.2 (b3395)

The changes are already in 6.2.2 release candidate.
I also verified the MTR tests.  I also made changes to Autopilot tests to work with these changes.
",2,"Build tests 6.2.2 (b3395)

The changes are already in 6.2.2 release candidate.
I also verified the MTR tests.  I also made changes to Autopilot tests to work with these changes.
"
839,MCOL-3743,MCOL,Patrick LeBlanc,146851,2020-03-16 21:01:10,"I'll add a couple more for things they're likely to want to change.
 - cache size.  The default is intentionally really small for safety.
 - location to store files.  The default is currently $HOME/storagemanager (will probably change for MCOL-3889).",1,"I'll add a couple more for things they're likely to want to change.
 - cache size.  The default is intentionally really small for safety.
 - location to store files.  The default is currently $HOME/storagemanager (will probably change for MCOL-3889)."
840,MCOL-3743,MCOL,Patrick LeBlanc,146939,2020-03-17 18:43:45,Some of the sol'n is subjective.  Let me know if you have any stylistic feedback.,2,Some of the sol'n is subjective.  Let me know if you have any stylistic feedback.
841,MCOL-3743,MCOL,Daniel Lee,151733,2020-04-30 21:06:43,"1.4.4-1

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 00abe03ad1da3719e06f7112000a331ee2b6786a
Author: Patrick LeBlanc <43503225+pleblanc1976@users.noreply.github.com>
Date:   Wed Apr 29 10:00:54 2020 -0500

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 2b67ac7f3537bd4b4d132c8a6c3a53e4cc63f4a1
Merge: beaac49 23d65dc
Author: benthompson15 <ben.thompson.015@gmail.com>
Date:   Tue Apr 28 15:40:45 2020 -0500

",3,"1.4.4-1

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 00abe03ad1da3719e06f7112000a331ee2b6786a
Author: Patrick LeBlanc 
Date:   Wed Apr 29 10:00:54 2020 -0500

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 2b67ac7f3537bd4b4d132c8a6c3a53e4cc63f4a1
Merge: beaac49 23d65dc
Author: benthompson15 
Date:   Tue Apr 28 15:40:45 2020 -0500

"
842,MCOL-3745,MCOL,Patrick LeBlanc,143430,2020-01-31 15:55:17,"Yes it's contention on a few hot files that get changed on many ops like in this sequence.  Access to the BRM files, and the syscat files is blocked while they are being sync'd to cloud storage.  On home networks with limited upstream that could be quite a while.  For now I'll add documentation to relevant parameters in the config file to reduce the observed latency.  I'll add a ticket to try to avoid locking files during network xfers.",1,"Yes it's contention on a few hot files that get changed on many ops like in this sequence.  Access to the BRM files, and the syscat files is blocked while they are being sync'd to cloud storage.  On home networks with limited upstream that could be quite a while.  For now I'll add documentation to relevant parameters in the config file to reduce the observed latency.  I'll add a ticket to try to avoid locking files during network xfers."
843,MCOL-3745,MCOL,Patrick LeBlanc,143580,2020-02-03 15:09:19,The intermediate fix for this is documentation in the config file.,2,The intermediate fix for this is documentation in the config file.
844,MCOL-3745,MCOL,Patrick LeBlanc,145365,2020-02-28 14:52:02,It was merged some time ago.,3,It was merged some time ago.
845,MCOL-3747,MCOL,David Hall,143214,2020-01-29 15:53:15,"This same bug most probably is responsible for failure in q.9.1.3.sql:
select * from (
         select lo_orderkey, count(*) from lineorder
         where lo_orderkey in ( select * from
                ( select lo_orderkey
                  from lineorder join dateinfo on lo_orderdate = d_datekey
                  group by lo_orderkey
--                order by sum(lo_ordtotalprice) desc
                  order by sum(lo_ordtotalprice) desc, lo_orderkey
                  limit 5 ) alias1 )
--       group by 1 order by 2 asc limit 4 ) alias2
         group by 1 order by 2 asc, 1 limit 4 ) alias2
-- order by 2 desc;
order by 2 desc, 1;
",1,"This same bug most probably is responsible for failure in q.9.1.3.sql:
select * from (
         select lo_orderkey, count(*) from lineorder
         where lo_orderkey in ( select * from
                ( select lo_orderkey
                  from lineorder join dateinfo on lo_orderdate = d_datekey
                  group by lo_orderkey
--                order by sum(lo_ordtotalprice) desc
                  order by sum(lo_ordtotalprice) desc, lo_orderkey
                  limit 5 ) alias1 )
--       group by 1 order by 2 asc limit 4 ) alias2
         group by 1 order by 2 asc, 1 limit 4 ) alias2
-- order by 2 desc;
order by 2 desc, 1;
"
846,MCOL-3747,MCOL,Gregory Dorman,143463,2020-01-31 23:06:00,"The story here is that in the presence of outermost wrapper, the ""order by LIMIT 5"" inside alias1 is simply ignored. The result is the same as if you remove it altogether.

What seems to be related also is the ""where lo_orderkey in"" clause in alias2. Below is what works also, this time with the outer query as well as with inner only. Notice that JOIN is redundant and does not change anything, so I removed it for simplicity of the repro. Also, DESC inside alias1 is not contributing since it is placed on the second column (unique too), so I removed it as well.

select * from 
    (select lo_orderkey, count(*) 
         from (select lo_orderkey from lineorder 
                where lo_orderkey <= 900
               group by lo_orderkey 
               order by sum(lo_ordtotalprice), lo_orderkey 
               limit 5
     ) alias1 
     group by lo_orderkey 
     order by 1, 2 ASC 
     limit 4 
     ) alias2
;

",2,"The story here is that in the presence of outermost wrapper, the ""order by LIMIT 5"" inside alias1 is simply ignored. The result is the same as if you remove it altogether.

What seems to be related also is the ""where lo_orderkey in"" clause in alias2. Below is what works also, this time with the outer query as well as with inner only. Notice that JOIN is redundant and does not change anything, so I removed it for simplicity of the repro. Also, DESC inside alias1 is not contributing since it is placed on the second column (unique too), so I removed it as well.

select * from 
    (select lo_orderkey, count(*) 
         from (select lo_orderkey from lineorder 
                where lo_orderkey <= 900
               group by lo_orderkey 
               order by sum(lo_ordtotalprice), lo_orderkey 
               limit 5
     ) alias1 
     group by lo_orderkey 
     order by 1, 2 ASC 
     limit 4 
     ) alias2
;

"
847,MCOL-3747,MCOL,Gregory Dorman,143482,2020-02-01 16:56:04,"And then - my hunch is that it has something to do with:

MariaDB [ssb]> select lo_orderkey, count(*)
    ->       FROM lineorder WHERE lo_orderkey IN
    ->                (select lo_orderkey from lineorder
    ->                 where lo_orderkey <= 900
    ->                group by lo_orderkey
    ->                order by sum(lo_ordtotalprice), lo_orderkey desc
    ->                limit 5
    ->                ) alias1
    ->      group by lo_orderkey
    ->      order by 1, 2 ASC
    ->      limit 4 ;
ERROR 1235 (42000): This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'


If it doesn't support it, why it allows to proceed if you pile up enough select * from over it?",3,"And then - my hunch is that it has something to do with:

MariaDB [ssb]> select lo_orderkey, count(*)
    ->       FROM lineorder WHERE lo_orderkey IN
    ->                (select lo_orderkey from lineorder
    ->                 where lo_orderkey <= 900
    ->                group by lo_orderkey
    ->                order by sum(lo_ordtotalprice), lo_orderkey desc
    ->                limit 5
    ->                ) alias1
    ->      group by lo_orderkey
    ->      order by 1, 2 ASC
    ->      limit 4 ;
ERROR 1235 (42000): This version of MariaDB doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'


If it doesn't support it, why it allows to proceed if you pile up enough select * from over it?"
848,MCOL-3747,MCOL,Roman,143638,2020-02-04 09:44:31,B/c we don't run IN-INTO-EXISTS if IN is deeper then two levels down.,4,B/c we don't run IN-INTO-EXISTS if IN is deeper then two levels down.
849,MCOL-3747,MCOL,Roman,143639,2020-02-04 09:45:42,"4QA
Here is the simple test case to reproduce the issue:
{noformat}
create table cs1(key_ bigint) engine=columnstore;
insert into cs1 values (42),(43),(45),(666),(777),(333);

select key_, count(*) from cs1 where key_ in (select * from (select key_ from cs1 group by key_ order by key_ limit 2) a1) group by key_;

select * from (select key_, count(*) from cs1 where key_ in (select * from (select key_ from cs1 group by key_ order by key_ limit 2) a1) group by key_) a2;
{noformat}
The first query works as expected the second one returns onordered full set from cs1.",5,"4QA
Here is the simple test case to reproduce the issue:
{noformat}
create table cs1(key_ bigint) engine=columnstore;
insert into cs1 values (42),(43),(45),(666),(777),(333);

select key_, count(*) from cs1 where key_ in (select * from (select key_ from cs1 group by key_ order by key_ limit 2) a1) group by key_;

select * from (select key_, count(*) from cs1 where key_ in (select * from (select key_ from cs1 group by key_ order by key_ limit 2) a1) group by key_) a2;
{noformat}
The first query works as expected the second one returns onordered full set from cs1."
850,MCOL-3747,MCOL,Roman,143641,2020-02-04 09:51:09,"Here is the explanation. 
If you take a look into create_columnstore_select_handler() in dbcon/mysql/ha_mcs_pushdown.cpp there will be a call to select_lex->optimize_unflattened_subqueries(). This method must be run recursively for all derived tables. SELECT_LEX::optimize_unflattened_subqueries() finishes IN-INTO-EXISTS rewrite adding equi-JOIN predicate insert into subquery a2.",6,"Here is the explanation. 
If you take a look into create_columnstore_select_handler() in dbcon/mysql/ha_mcs_pushdown.cpp there will be a call to select_lex->optimize_unflattened_subqueries(). This method must be run recursively for all derived tables. SELECT_LEX::optimize_unflattened_subqueries() finishes IN-INTO-EXISTS rewrite adding equi-JOIN predicate insert into subquery a2."
851,MCOL-3747,MCOL,Daniel Lee,144040,2020-02-10 23:51:09,"Build verified: 1.4.3-1

Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 5efa6a4dc52129be2de49fdfc23e44020401b86b

Build tested: 1.5.0-1 

server
commit 57950ded281731263f6aa358d43c7b9d51f3dbfb
engine
commit 46f30be5561f65eec488d61e011b727ca358720b

The fixed is not in 1.5.0-1
",7,"Build verified: 1.4.3-1

Build verified: 1.4.3-1 source
server
commit 9bd5e14f4de1402c6cd4a3f81564887c1213c9e1
engine
commit 5efa6a4dc52129be2de49fdfc23e44020401b86b

Build tested: 1.5.0-1 

server
commit 57950ded281731263f6aa358d43c7b9d51f3dbfb
engine
commit 46f30be5561f65eec488d61e011b727ca358720b

The fixed is not in 1.5.0-1
"
852,MCOL-3747,MCOL,Daniel Lee,144085,2020-02-11 15:39:30,"Close ticket for 1.4.3-1

Fixed for 1.5 is being tracked on MCOL-3780",8,"Close ticket for 1.4.3-1

Fixed for 1.5 is being tracked on MCOL-3780"
853,MCOL-3756,MCOL,Roman,149244,2020-04-07 07:46:39,"4QA. There was a relevant test added: working_tpch1/qa_fe_cnxFunctions/truth_functions.sql
",1,"4QA. There was a relevant test added: working_tpch1/qa_fe_cnxFunctions/truth_functions.sql
"
854,MCOL-3756,MCOL,Daniel Lee,152255,2020-05-06 22:25:56,"Build verified: 1.4.4-1, 1.5.0-1 source

1.4.4-1

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit d8ff39957275afc7a870487631cd3b3be5eb8818
Author: Rasmus Johansson <razze@iki.fi>
Date:   Mon May 4 10:10:07 2020 +0000

    MDEV-22273 jUnit patch: xml test result differs from MTR output in case if retry


/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 4c275557633edb60905faa500990d55a6834951d
Merge: aa054a9 47f2933
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue May 5 12:34:03 2020 -0500

    Merge pull request #1179 from pleblanc1976/update-libs3-ref
    
    Updated the s3 lib.


1.5.0-1

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 43b7e5d29a5480214cee8317a4625b749ccffbaf
Author: Rasmus Johansson <razze@iki.fi>
Date:   Mon May 4 10:10:07 2020 +0000

    MDEV-22273 jUnit patch: xml test result differs from MTR output in case if retry

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 368c4fac059d5cf4f1596e56ea7b1e729e29ec49
Merge: 15a9efa 4bc408c
Author: David.Hall <david.hall@mariadb.com>

Date:   Tue May 5 12:35:14 2020 -0500

    Merge pull request #1178 from pleblanc1976/update-libs3-ref-1.5
    
    Updated s3 lib ref

Verified scripts against innodb tables.

",2,"Build verified: 1.4.4-1, 1.5.0-1 source

1.4.4-1

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit d8ff39957275afc7a870487631cd3b3be5eb8818
Author: Rasmus Johansson 
Date:   Mon May 4 10:10:07 2020 +0000

    MDEV-22273 jUnit patch: xml test result differs from MTR output in case if retry


/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 4c275557633edb60905faa500990d55a6834951d
Merge: aa054a9 47f2933
Author: David.Hall 
Date:   Tue May 5 12:34:03 2020 -0500

    Merge pull request #1179 from pleblanc1976/update-libs3-ref
    
    Updated the s3 lib.


1.5.0-1

/root/ColumnStore/buildColumnstoreFromGithubSource/server
commit 43b7e5d29a5480214cee8317a4625b749ccffbaf
Author: Rasmus Johansson 
Date:   Mon May 4 10:10:07 2020 +0000

    MDEV-22273 jUnit patch: xml test result differs from MTR output in case if retry

/root/ColumnStore/buildColumnstoreFromGithubSource/server/engine
commit 368c4fac059d5cf4f1596e56ea7b1e729e29ec49
Merge: 15a9efa 4bc408c
Author: David.Hall 

Date:   Tue May 5 12:35:14 2020 -0500

    Merge pull request #1178 from pleblanc1976/update-libs3-ref-1.5
    
    Updated s3 lib ref

Verified scripts against innodb tables.

"
855,MCOL-3757,MCOL,David Hall,145316,2020-02-27 22:36:49,"In 1.2, we had

{code:java}
        if (csep->limitNum() != (uint64_t) - 1 &&
                gwi.subQuery && !gwi.correlatedTbNameVec.empty())
{code}
In 1.4, we have

{code:java}
        if (gwi.subQuery && !gwi.correlatedTbNameVec.empty() && csep->hasOrderBy())
{code}
This was changed in MCOL-2178 'Fix for handlers fallback mechanism patch'
However, csep->hasOrderBy() is never set for subquery, so the error is never detected.

I commented out the error detection code in 1.2 and the query succeeded, so it appears the system has supported at least this one example of a limit in a subquery for a while.",1,"In 1.2, we had

{code:java}
        if (csep->limitNum() != (uint64_t) - 1 &&
                gwi.subQuery && !gwi.correlatedTbNameVec.empty())
{code}
In 1.4, we have

{code:java}
        if (gwi.subQuery && !gwi.correlatedTbNameVec.empty() && csep->hasOrderBy())
{code}
This was changed in MCOL-2178 'Fix for handlers fallback mechanism patch'
However, csep->hasOrderBy() is never set for subquery, so the error is never detected.

I commented out the error detection code in 1.2 and the query succeeded, so it appears the system has supported at least this one example of a limit in a subquery for a while."
856,MCOL-3757,MCOL,Roman,145428,2020-03-01 17:12:27,Does it really returns expected results?,2,Does it really returns expected results?
857,MCOL-3757,MCOL,David Hall,145895,2020-03-05 23:56:24,The result set returned is incorrect. We must re-enable the error message.,3,The result set returned is incorrect. We must re-enable the error message.
858,MCOL-3757,MCOL,David Hall,150595,2020-04-20 22:29:58,"I had submitted, as part of MCOL-3594, changes to support the new behavior. Since the new behavior is incorrect and has been reverted, I also reverted the changes in the Pull Request for MCOL-3594",4,"I had submitted, as part of MCOL-3594, changes to support the new behavior. Since the new behavior is incorrect and has been reverted, I also reverted the changes in the Pull Request for MCOL-3594"
859,MCOL-3757,MCOL,David Hall,152329,2020-05-07 18:28:13,"DDL and DML for example:

CREATE TABLE t1 (lid int, name char(10)) engine=columnstore;
INSERT INTO t1 (lid, name) VALUES (1, 'YES'), (2, 'NO');

CREATE TABLE t2 (  id int, gid int, lid int, dt date) engine=columnstore;
INSERT INTO t2 (id, gid, lid, dt) VALUES
 (1, 1, 1, '2007-01-01'),(2, 1, 2, '2007-01-02'),
 (3, 2, 2, '2007-02-01'),(4, 2, 1, '2007-02-02');

",5,"DDL and DML for example:

CREATE TABLE t1 (lid int, name char(10)) engine=columnstore;
INSERT INTO t1 (lid, name) VALUES (1, 'YES'), (2, 'NO');

CREATE TABLE t2 (  id int, gid int, lid int, dt date) engine=columnstore;
INSERT INTO t2 (id, gid, lid, dt) VALUES
 (1, 1, 1, '2007-01-01'),(2, 1, 2, '2007-01-02'),
 (3, 2, 2, '2007-02-01'),(4, 2, 1, '2007-02-02');

"
860,MCOL-3757,MCOL,Daniel Lee,152331,2020-05-07 18:35:09,"Build verified: 1.4.4-1, Jenkins-20200506

The fix is to restore the error checking and return a messages for non-support syntax

Server version: 10.4.12-6-MariaDB-enterprise MariaDB Enterprise Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> CREATE TABLE t1 (lid int, name char(10)) engine=columnstore;
Query OK, 0 rows affected (0.219 sec)

MariaDB [mytest]> INSERT INTO t1 (lid, name) VALUES (1, 'YES'), (2, 'NO');
Query OK, 2 rows affected (1.334 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [mytest]> CREATE TABLE t2 ( id int, gid int, lid int, dt date) engine=columnstore;
Query OK, 0 rows affected (0.181 sec)

MariaDB [mytest]> INSERT INTO t2 (id, gid, lid, dt) VALUES
    -> (1, 1, 1, '2007-01-01'),(2, 1, 2, '2007-01-02'),
    -> (3, 2, 2, '2007-02-01'),(4, 2, 1, '2007-02-02');
Query OK, 4 rows affected (0.456 sec)
Records: 4  Duplicates: 0  Warnings: 0

MariaDB [mytest]> SELECT DISTINCT t2.gid AS lgid,
    ->                 (SELECT t1.name FROM t1, t2
    ->                    WHERE t1.lid  = t2.lid AND t2.gid = lgid
    ->                      ORDER BY t2.dt DESC LIMIT 1
    ->                 ) as clid
    ->   FROM t2;
ERROR 1815 (HY000): Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
MariaDB [mytest]> CREATE VIEW v1 AS
    -> SELECT DISTINCT t2.gid AS lgid,
    ->                 (SELECT t1.name FROM t1, t2
    ->                    WHERE t1.lid  = t2.lid AND t2.gid = lgid
    ->                      ORDER BY t2.dt DESC LIMIT 1
    ->                 ) as clid
    ->   FROM t2;
Query OK, 0 rows affected (0.001 sec)

MariaDB [mytest]> SELECT * FROM v1;
ERROR 1815 (HY000): Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
",6,"Build verified: 1.4.4-1, Jenkins-20200506

The fix is to restore the error checking and return a messages for non-support syntax

Server version: 10.4.12-6-MariaDB-enterprise MariaDB Enterprise Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> CREATE TABLE t1 (lid int, name char(10)) engine=columnstore;
Query OK, 0 rows affected (0.219 sec)

MariaDB [mytest]> INSERT INTO t1 (lid, name) VALUES (1, 'YES'), (2, 'NO');
Query OK, 2 rows affected (1.334 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [mytest]> CREATE TABLE t2 ( id int, gid int, lid int, dt date) engine=columnstore;
Query OK, 0 rows affected (0.181 sec)

MariaDB [mytest]> INSERT INTO t2 (id, gid, lid, dt) VALUES
    -> (1, 1, 1, '2007-01-01'),(2, 1, 2, '2007-01-02'),
    -> (3, 2, 2, '2007-02-01'),(4, 2, 1, '2007-02-02');
Query OK, 4 rows affected (0.456 sec)
Records: 4  Duplicates: 0  Warnings: 0

MariaDB [mytest]> SELECT DISTINCT t2.gid AS lgid,
    ->                 (SELECT t1.name FROM t1, t2
    ->                    WHERE t1.lid  = t2.lid AND t2.gid = lgid
    ->                      ORDER BY t2.dt DESC LIMIT 1
    ->                 ) as clid
    ->   FROM t2;
ERROR 1815 (HY000): Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
MariaDB [mytest]> CREATE VIEW v1 AS
    -> SELECT DISTINCT t2.gid AS lgid,
    ->                 (SELECT t1.name FROM t1, t2
    ->                    WHERE t1.lid  = t2.lid AND t2.gid = lgid
    ->                      ORDER BY t2.dt DESC LIMIT 1
    ->                 ) as clid
    ->   FROM t2;
Query OK, 0 rows affected (0.001 sec)

MariaDB [mytest]> SELECT * FROM v1;
ERROR 1815 (HY000): Internal error: IDB-3019: Limit within a correlated subquery is currently not supported.
"
861,MCOL-3821,MCOL,Daniel Lee,146833,2020-03-16 19:33:57,"Build verified: 1.2.6-1 BB

engine commit:
d4173ef

MariaDB [tpch1]> SELECT lead(ten, (SELECT two FROM tenk1 WHERE s.unique2 = unique2)) OVER (PARTITION BY four ORDER BY ten) FROM tenk1 s WHERE unique2 < 10;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-3016: Function or Operator with sub query on the SELECT clause is currently not supported.
MariaDB [tpch1]> 
",1,"Build verified: 1.2.6-1 BB

engine commit:
d4173ef

MariaDB [tpch1]> SELECT lead(ten, (SELECT two FROM tenk1 WHERE s.unique2 = unique2)) OVER (PARTITION BY four ORDER BY ten) FROM tenk1 s WHERE unique2 < 10;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-3016: Function or Operator with sub query on the SELECT clause is currently not supported.
MariaDB [tpch1]> 
"
862,MCOL-3836,MCOL,Roman,212877,2022-02-01 19:50:34,The feature had been implemented.,1,The feature had been implemented.
863,MCOL-385,MCOL,Andrew Hutchings,88159,2016-11-07 21:13:22,Fix in engine for a crash observed as well as the branch in the server tree.,1,Fix in engine for a crash observed as well as the branch in the server tree.
864,MCOL-385,MCOL,David Thompson,88615,2016-11-22 23:32:54,Assigning to Ben to fix mysql version label.,2,Assigning to Ben to fix mysql version label.
865,MCOL-385,MCOL,David Hill,88790,2016-11-28 17:02:59,"fixed, new packages now show updated MariaDB version",3,"fixed, new packages now show updated MariaDB version"
866,MCOL-3852,MCOL,Patrick LeBlanc,145498,2020-03-02 16:43:42,cloned from MCOL-128 for 1.5,1,cloned from MCOL-128 for 1.5
867,MCOL-3852,MCOL,Daniel Lee,146583,2020-03-13 20:09:26,"Build verified: 1.5.0-1 BB

engine commit:
f01185f
",2,"Build verified: 1.5.0-1 BB

engine commit:
f01185f
"
868,MCOL-3854,MCOL,Patrick LeBlanc,145500,2020-03-02 16:48:12,cloned from MCOL-3707 for 1.5,1,cloned from MCOL-3707 for 1.5
869,MCOL-3854,MCOL,Daniel Lee,155270,2020-06-03 15:21:07,"Build verified: 1.5.0-1 (drone 20200602 engine commit: 5440977f)

Note:  For this build, OAM has been removed, after packages have been installed, a single-node ColumnStore installation will be setup.",2,"Build verified: 1.5.0-1 (drone 20200602 engine commit: 5440977f)

Note:  For this build, OAM has been removed, after packages have been installed, a single-node ColumnStore installation will be setup."
870,MCOL-3856,MCOL,Patrick LeBlanc,145502,2020-03-02 16:50:48,Cloned from MCOL-3632 for 1.5,1,Cloned from MCOL-3632 for 1.5
871,MCOL-3856,MCOL,Daniel Lee,146836,2020-03-16 19:46:04,"Build verified: 1.5.0-1 BB

engine commit:
f01185f

MariaDB [mytest]> SELECT lead(ten, (SELECT two FROM tenk1 WHERE s.unique2 = unique2)) OVER (PARTITION BY four ORDER BY ten) FROM tenk1 s WHERE unique2 < 10;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-3016: Function or Operator with sub query on the SELECT clause is currently not supported.",2,"Build verified: 1.5.0-1 BB

engine commit:
f01185f

MariaDB [mytest]> SELECT lead(ten, (SELECT two FROM tenk1 WHERE s.unique2 = unique2)) OVER (PARTITION BY four ORDER BY ten) FROM tenk1 s WHERE unique2 < 10;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-3016: Function or Operator with sub query on the SELECT clause is currently not supported."
872,MCOL-386,MCOL,Andrew Hutchings,88356,2016-11-15 17:31:53,"Testing should probably be:

We don't spit out anything strange and unexpected during postConfigure.",1,"Testing should probably be:

We don't spit out anything strange and unexpected during postConfigure."
873,MCOL-386,MCOL,Daniel Lee,88610,2016-11-22 21:45:34,"Build verified: 1.0.5-1
mcsadmin> getsoft
getsoftwareinfo   Tue Nov 22 10:13:49 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.5
Release     : 1
Architecture: x86_64
Install Date: Tue 22 Nov 2016 10:10:12 AM CST
Group       : Applications/Databases
Size        : 9990112
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.5-1.src.rpm
Build Date  : Mon 21 Nov 2016 07:29:37 PM CST

Verified by regression of postConfigure.
",2,"Build verified: 1.0.5-1
mcsadmin> getsoft
getsoftwareinfo   Tue Nov 22 10:13:49 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.5
Release     : 1
Architecture: x86_64
Install Date: Tue 22 Nov 2016 10:10:12 AM CST
Group       : Applications/Databases
Size        : 9990112
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.5-1.src.rpm
Build Date  : Mon 21 Nov 2016 07:29:37 PM CST

Verified by regression of postConfigure.
"
874,MCOL-3861,MCOL,Patrick LeBlanc,145569,2020-03-02 18:13:35,Cloned from MCOL-3520 for 1.5,1,Cloned from MCOL-3520 for 1.5
875,MCOL-3861,MCOL,Daniel Lee,146569,2020-03-13 17:35:25,"Build verified: 1.5.0-1 BB

engine commit:
f01185f
",2,"Build verified: 1.5.0-1 BB

engine commit:
f01185f
"
876,MCOL-3862,MCOL,Patrick LeBlanc,145570,2020-03-02 18:14:41,Cloned from MCOL-3721 for 1.5,1,Cloned from MCOL-3721 for 1.5
877,MCOL-3862,MCOL,Daniel Lee,146587,2020-03-13 20:20:37,CLONE - EXPLAIN gives error every other try,2,CLONE - EXPLAIN gives error every other try
878,MCOL-3910,MCOL,Patrick LeBlanc,153688,2020-05-19 18:22:39,"I finished the initial cut last week, forgot to update the ticket
",1,"I finished the initial cut last week, forgot to update the ticket
"
879,MCOL-392,MCOL,Andrew Hutchings,109923,2018-04-19 17:55:49,WIP branch https://github.com/mariadb-corporation/mariadb-columnstore-engine/tree/MCOL-392 - microsecond for DATETIME support added.,1,WIP branch URL - microsecond for DATETIME support added.
880,MCOL-392,MCOL,Andrew Hutchings,110368,2018-05-01 09:02:33,"Three pull requests for this, one in engine, one in API and one in regression suite.

This adds support for:
* DATETIME microseconds, for example DATETIME(6)
* TIME with and without microseconds, for example TIME or TIME(6)

It also fixes things such as saturation handling of TIME values and several DATETIME/TIME related functions which broke on various edge cases.

[~David.Hall] You may need [~jens.rowekamp] to help you review the API portion. We may also need a spin-off of this ticket to apply the API changes to the other API languages.

For QA: The regression suite has test014 added to cover the basics. The API has a new set of tests when running ""make test"".",2,"Three pull requests for this, one in engine, one in API and one in regression suite.

This adds support for:
* DATETIME microseconds, for example DATETIME(6)
* TIME with and without microseconds, for example TIME or TIME(6)

It also fixes things such as saturation handling of TIME values and several DATETIME/TIME related functions which broke on various edge cases.

[~David.Hall] You may need [~jens.rowekamp] to help you review the API portion. We may also need a spin-off of this ticket to apply the API changes to the other API languages.

For QA: The regression suite has test014 added to cover the basics. The API has a new set of tests when running ""make test""."
881,MCOL-392,MCOL,Andrew Hutchings,110372,2018-05-01 09:30:07,"Other things to note:

* DATETIME is fully backwards/forwards compatible (TIME is new so doesn't matter)
* This also fixes MCOL-807, MCOL-320 and probably a few others too (they should go in testing at the same time as this one)",3,"Other things to note:

* DATETIME is fully backwards/forwards compatible (TIME is new so doesn't matter)
* This also fixes MCOL-807, MCOL-320 and probably a few others too (they should go in testing at the same time as this one)"
882,MCOL-392,MCOL,David Hall,110399,2018-05-01 20:30:20,"Jens, Im sending this over to you to review the API portion -- pull request  https://github.com/mariadb-corporation/mariadb-columnstore-api/pull/72",4,"Jens, Im sending this over to you to review the API portion -- pull request  URL"
883,MCOL-392,MCOL,Jens Röwekamp,110568,2018-05-04 22:40:42,"Added tests for Java and Python to the regression suite.
PySpark and Scala spark don't have native time only data types, therefore I wasn't able to write from ColumnStoreExporter to the new time field.
Quickly compiled the PDI adapter with the new API, also worked. Opened MCOL-1392 to add the time field in the adapter as well.

[~LinuxJedi]'s code looks good. Approved it.

Could someone else quickly check out my additions and then merge, if they look okay.",5,"Added tests for Java and Python to the regression suite.
PySpark and Scala spark don't have native time only data types, therefore I wasn't able to write from ColumnStoreExporter to the new time field.
Quickly compiled the PDI adapter with the new API, also worked. Opened MCOL-1392 to add the time field in the adapter as well.

[~LinuxJedi]'s code looks good. Approved it.

Could someone else quickly check out my additions and then merge, if they look okay."
884,MCOL-392,MCOL,Jens Röwekamp,110569,2018-05-04 23:12:00,"Also created a branch and pull request MCOL-1344 which has the same fixes for MCOL-1344 pushed down from develop to develop-1.1.
It would be nice if that could be merged too, if it looks alright.",6,"Also created a branch and pull request MCOL-1344 which has the same fixes for MCOL-1344 pushed down from develop to develop-1.1.
It would be nice if that could be merged too, if it looks alright."
885,MCOL-392,MCOL,Daniel Lee,117924,2018-10-15 18:08:27,All sub-tasks have been verified.,7,All sub-tasks have been verified.
886,MCOL-3964,MCOL,David Hall,151272,2020-04-27 16:19:05,This sub-task is not QA testable by itself,1,This sub-task is not QA testable by itself
887,MCOL-3976,MCOL,David Hill,156197,2020-06-10 20:19:32,"Amazon offers 2 ways to configure for
configuration updates, using the access
keys or using Roles where they can control
the level of permissions and commands
a user can do. This customer has always
used Role instead of the Access key that is
currently supported via the SM config file.

So they need the use of Role configuration 
supported or will not be able to use the
1.4 + releases.",1,"Amazon offers 2 ways to configure for
configuration updates, using the access
keys or using Roles where they can control
the level of permissions and commands
a user can do. This customer has always
used Role instead of the Access key that is
currently supported via the SM config file.

So they need the use of Role configuration 
supported or will not be able to use the
1.4 + releases."
888,MCOL-3976,MCOL,David Hill,156198,2020-06-10 20:31:56,"more info that might help

https://mariadb.com/kb/en/installing-and-configuring-a-columnstore-system-using-the-amazon-ami/#amazon-iam-role",2,"more info that might help

URL"
889,MCOL-3976,MCOL,Daniel Lee,168505,2020-10-10 01:05:38,"Build tested: Drone builds. ColumnStore: 907, cmapi: 283

Tested local cpimport and S3 source cpimport
IAM role used: S3-test
STS Region: us-west-2
STS Endpoint: sts.us-west-2.amazonaws.com

With STS region and endpoint specified


Installation: PASSED
Sanity test (LDI using local cpimport, 1gb lineitem), S3 cpimport 1gb orders.  PASSED

AWS S3 storage is used, bucket = dleeqadbroot1, objects = 295, total size = 493284230 (470 MB)

1st 1g lineitem local cpimport test successful
2nd lineitem local cpimport test FAILED

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-09 23:16:50 (7098) INFO : Running distributed import (mode 1) on all PMs...
2020-10-09 23:25:24 (7098) INFO : For table mytest.lineitem: 6001215 rows processed and 6001215 rows inserted.
2020-10-09 23:25:24 (7098) INFO : Bulk load completed, total run time : 514.093 seconds

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-10 00:21:21 (8190) INFO : Running distributed import (mode 1) on all PMs...
2020-10-10 00:22:44 (8190) ERR  : Received a Cpimport Failure from PM1
2020-10-10 00:22:44 (8190) INFO : Please verify error log files in PM1
2020-10-10 00:22:44 (8190) INFO : Canceling outstanding cpimports

[centos8:root~]# cat err.log 
Oct 10 00:21:55 centos-8 StorageManager[5845]: S3Storage::getConnection(): ERROR: ms3_init_assume_role. Verify iam_role_name = S3-test, aws_access_key_id, aws_secret_access_key values. Also check sts_region and sts_endpoint if configured.
Oct 10 00:21:55 centos-8 StorageManager[5845]: S3Storage::getConnection(): ms3_error: server says 'Couldn't resolve host name'  role name = S3-test
Oct 10 00:21:57 centos-8 configcpp[8218]: 57.292114 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:21:58 centos-8 configcpp[8218]: 58.296044 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:21:59 centos-8 configcpp[8218]: 59.296465 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
.
.
.
.
Oct 10 00:22:41 centos-8 configcpp[8218]: 41.525869 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:22:42 centos-8 configcpp[8218]: 42.527265 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:22:42 centos-8 cpimport.bin[8218]: 42.527779 |0|0|0| E 34 CAL0087: BulkLoad Error: Backup error writing backup for column OID-3032; DBRoot-1; partition-0; segment-0;  Unable to rename compressed chunk bulk backup file. ; Success
Oct 10 00:22:42 centos-8 cpimport.bin[8218]: 42.528037 |0|0|0| E 34 CAL0087: BulkLoad Error: Error in pre-processing the job file for table mytest.lineitem
Oct 10 00:22:42 centos-8 writeengineserver[6210]: 42.558702 |0|0|0| E 32 CAL0000: pushing data : PIPE error .........Broken pipe
Oct 10 00:22:44 centos-8 writeengineserver[6210]: 44.670002 |0|0|0| E 32 CAL0000: 9765 : cpimport exit on failure (signal -1)
Oct 10 00:22:44 centos-8 writeenginesplit[8190]: 44.672718 |0|0|0| E 33 CAL0000: #033[0;31mReceived a Cpimport Failure from PM1#033[0m
Oct 10 00:22:44 centos-8 writeenginesplit[8190]: 44.673207 |0|0|0| E 33 CAL0087: BulkLoad Error: #033[0;31mReceived a Cpimport Failure from PM1#033[0m
Oct 10 00:25:49 centos-8 configcpp[6033]: 49.396311 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:25:50 centos-8 configcpp[6033]: 50.413124 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
.
.
.
Oct 10 00:48:56 centos-8 configcpp[6149]: 56.574334 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:48:57 centos-8 configcpp[6149]: 57.576610 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:48:58 centos-8 configcpp[6149]: 58.578203 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused


On PM1, from ‘top’ command

   4447 root      20   0 1929636  63044  20600 S   2.0   0.3   2:08.18 python3                                                       
   8190 root      20   0 1653100  41312  23824 S   0.7   0.2   0:05.11 cpimport                                                      
    572 root      20   0   94188  15360  13980 S   0.3   0.1   0:02.22 systemd-journal                                               
   7041 root      20   0       0      0      0 I   0.3   0.0   0:02.82 kworker/0:2-events                                            
   8584 root      20   0  153256   6128   4764 S   0.3   0.0   0:00.01 sshd                                                          
   8614 root      20   0   64516   4496   3828 R   0.3   0.0   0:00.09 top                                                           

There is no much activity going on PM1 and the failed cpimport  job returned to the system prompt after 26 minutes with a core dumped error:

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-10 00:21:21 (8190) INFO : Running distributed import (mode 1) on all PMs...
2020-10-10 00:22:44 (8190) ERR  : Received a Cpimport Failure from PM1
2020-10-10 00:22:44 (8190) INFO : Please verify error log files in PM1
2020-10-10 00:22:44 (8190) INFO : Canceling outstanding cpimports

caught an exception: Table lock save file failure
terminate called after throwing an instance of 'std::runtime_error'
  what():  Table lock save file failure
Aborted (core dumped)
",3,"Build tested: Drone builds. ColumnStore: 907, cmapi: 283

Tested local cpimport and S3 source cpimport
IAM role used: S3-test
STS Region: us-west-2
STS Endpoint: sts.us-west-2.amazonaws.com

With STS region and endpoint specified


Installation: PASSED
Sanity test (LDI using local cpimport, 1gb lineitem), S3 cpimport 1gb orders.  PASSED

AWS S3 storage is used, bucket = dleeqadbroot1, objects = 295, total size = 493284230 (470 MB)

1st 1g lineitem local cpimport test successful
2nd lineitem local cpimport test FAILED

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-09 23:16:50 (7098) INFO : Running distributed import (mode 1) on all PMs...
2020-10-09 23:25:24 (7098) INFO : For table mytest.lineitem: 6001215 rows processed and 6001215 rows inserted.
2020-10-09 23:25:24 (7098) INFO : Bulk load completed, total run time : 514.093 seconds

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-10 00:21:21 (8190) INFO : Running distributed import (mode 1) on all PMs...
2020-10-10 00:22:44 (8190) ERR  : Received a Cpimport Failure from PM1
2020-10-10 00:22:44 (8190) INFO : Please verify error log files in PM1
2020-10-10 00:22:44 (8190) INFO : Canceling outstanding cpimports

[centos8:root~]# cat err.log 
Oct 10 00:21:55 centos-8 StorageManager[5845]: S3Storage::getConnection(): ERROR: ms3_init_assume_role. Verify iam_role_name = S3-test, aws_access_key_id, aws_secret_access_key values. Also check sts_region and sts_endpoint if configured.
Oct 10 00:21:55 centos-8 StorageManager[5845]: S3Storage::getConnection(): ms3_error: server says 'Couldn't resolve host name'  role name = S3-test
Oct 10 00:21:57 centos-8 configcpp[8218]: 57.292114 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:21:58 centos-8 configcpp[8218]: 58.296044 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:21:59 centos-8 configcpp[8218]: 59.296465 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
.
.
.
.
Oct 10 00:22:41 centos-8 configcpp[8218]: 41.525869 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:22:42 centos-8 configcpp[8218]: 42.527265 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:22:42 centos-8 cpimport.bin[8218]: 42.527779 |0|0|0| E 34 CAL0087: BulkLoad Error: Backup error writing backup for column OID-3032; DBRoot-1; partition-0; segment-0;  Unable to rename compressed chunk bulk backup file. ; Success
Oct 10 00:22:42 centos-8 cpimport.bin[8218]: 42.528037 |0|0|0| E 34 CAL0087: BulkLoad Error: Error in pre-processing the job file for table mytest.lineitem
Oct 10 00:22:42 centos-8 writeengineserver[6210]: 42.558702 |0|0|0| E 32 CAL0000: pushing data : PIPE error .........Broken pipe
Oct 10 00:22:44 centos-8 writeengineserver[6210]: 44.670002 |0|0|0| E 32 CAL0000: 9765 : cpimport exit on failure (signal -1)
Oct 10 00:22:44 centos-8 writeenginesplit[8190]: 44.672718 |0|0|0| E 33 CAL0000: #033[0;31mReceived a Cpimport Failure from PM1#033[0m
Oct 10 00:22:44 centos-8 writeenginesplit[8190]: 44.673207 |0|0|0| E 33 CAL0087: BulkLoad Error: #033[0;31mReceived a Cpimport Failure from PM1#033[0m
Oct 10 00:25:49 centos-8 configcpp[6033]: 49.396311 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:25:50 centos-8 configcpp[6033]: 50.413124 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
.
.
.
Oct 10 00:48:56 centos-8 configcpp[6149]: 56.574334 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:48:57 centos-8 configcpp[6149]: 57.576610 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 00:48:58 centos-8 configcpp[6149]: 58.578203 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused


On PM1, from ‘top’ command

   4447 root      20   0 1929636  63044  20600 S   2.0   0.3   2:08.18 python3                                                       
   8190 root      20   0 1653100  41312  23824 S   0.7   0.2   0:05.11 cpimport                                                      
    572 root      20   0   94188  15360  13980 S   0.3   0.1   0:02.22 systemd-journal                                               
   7041 root      20   0       0      0      0 I   0.3   0.0   0:02.82 kworker/0:2-events                                            
   8584 root      20   0  153256   6128   4764 S   0.3   0.0   0:00.01 sshd                                                          
   8614 root      20   0   64516   4496   3828 R   0.3   0.0   0:00.09 top                                                           

There is no much activity going on PM1 and the failed cpimport  job returned to the system prompt after 26 minutes with a core dumped error:

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-10 00:21:21 (8190) INFO : Running distributed import (mode 1) on all PMs...
2020-10-10 00:22:44 (8190) ERR  : Received a Cpimport Failure from PM1
2020-10-10 00:22:44 (8190) INFO : Please verify error log files in PM1
2020-10-10 00:22:44 (8190) INFO : Canceling outstanding cpimports

caught an exception: Table lock save file failure
terminate called after throwing an instance of 'std::runtime_error'
  what():  Table lock save file failure
Aborted (core dumped)
"
890,MCOL-3976,MCOL,Daniel Lee,168509,2020-10-10 01:56:55,"Tried another test run and got a different error on the first cpimport

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-10 01:43:24 (7409) INFO : Running distributed import (mode 1) on all PMs...
caught an exception: Table lock save file failure
terminate called after throwing an instance of 'std::runtime_error'
  what():  Table lock save file failure
Aborted (core dumped)

It did not have that IAM error in the err.log file

[centos8:root~]# cat err.log 
Oct 10 01:45:08 centos-8 configcpp[6042]: 08.335435 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:45:09 centos-8 configcpp[6042]: 09.337178 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:45:10 centos-8 configcpp[6042]: 10.343533 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
.
.
.
Oct 10 01:50:01 centos-8 configcpp[6042]: 01.794403 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:50:01 centos-8 writeengine[6179]: 01.929063 |0|0|0| E 19 CAL0001: SplitterReadThread::operator: Broken Pipe
Oct 10 01:50:02 centos-8 configcpp[6042]: 02.820562 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:50:03 centos-8 configcpp[6042]: 03.826773 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'


",4,"Tried another test run and got a different error on the first cpimport

[centos8:root~]# /usr/bin/cpimport mytest lineitem /data/qa/source/dbt3/1g/lineitem.tbl 
2020-10-10 01:43:24 (7409) INFO : Running distributed import (mode 1) on all PMs...
caught an exception: Table lock save file failure
terminate called after throwing an instance of 'std::runtime_error'
  what():  Table lock save file failure
Aborted (core dumped)

It did not have that IAM error in the err.log file

[centos8:root~]# cat err.log 
Oct 10 01:45:08 centos-8 configcpp[6042]: 08.335435 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:45:09 centos-8 configcpp[6042]: 09.337178 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:45:10 centos-8 configcpp[6042]: 10.343533 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
.
.
.
Oct 10 01:50:01 centos-8 configcpp[6042]: 01.794403 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:50:01 centos-8 writeengine[6179]: 01.929063 |0|0|0| E 19 CAL0001: SplitterReadThread::operator: Broken Pipe
Oct 10 01:50:02 centos-8 configcpp[6042]: 02.820562 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'
Oct 10 01:50:03 centos-8 configcpp[6042]: 03.826773 |0|0|0| E 12 SocketPool::getSocket() failed to connect; got 'Connection refused'


"
891,MCOL-3976,MCOL,Patrick LeBlanc,168667,2020-10-12 22:36:38,"To me it looks like SM is down.  That's really the only reason SocketPool will return a 'connection refused' error.

I tried for 3 hours to reproduce this problem with the specified engine/cmapi builds (engine 907, cmapi 283) using a tarballed playbook from Jose.  I'm told this is how to try to reproduce it.  Anyway, I couldn't get far enough in the playbook to be able to reproduce this.  There were just too many other problems in those two builds.

I did successfully run everything using the current builds (engine 922 & cmapi 315) twice.  I believe I reproduced the problem by causing ms3_init_assume_role() to fail, and logged it as MCOL-4347.  I saw SM crash with an assertion failure.  The only missing bit of info in the logs above is the output of journalctl for the SM unit.  That would include the above, plus info about the assertion failure.

In general, this feature seems to be working.  It's not great that a failure to assume a role (via misconfiguration in my case or because of a DNS failure in Daniel's case) causes an SM crash, but I thought about it, and the user's experience will be the same as if it didn't crash.  The user gets a mess of errors both ways.  A little investigation (like getting the journal output or running testS3Connection) tells them exactly what the problem was.  The main difference is that the assertion failure can cause a core-dump, which may fill up the disk if SM keeps getting restarted.

IMO this isn't a show-stopper under the circumstances, confirmed it with Todd.  We'll follow up on it via the ticket I logged above.",5,"To me it looks like SM is down.  That's really the only reason SocketPool will return a 'connection refused' error.

I tried for 3 hours to reproduce this problem with the specified engine/cmapi builds (engine 907, cmapi 283) using a tarballed playbook from Jose.  I'm told this is how to try to reproduce it.  Anyway, I couldn't get far enough in the playbook to be able to reproduce this.  There were just too many other problems in those two builds.

I did successfully run everything using the current builds (engine 922 & cmapi 315) twice.  I believe I reproduced the problem by causing ms3_init_assume_role() to fail, and logged it as MCOL-4347.  I saw SM crash with an assertion failure.  The only missing bit of info in the logs above is the output of journalctl for the SM unit.  That would include the above, plus info about the assertion failure.

In general, this feature seems to be working.  It's not great that a failure to assume a role (via misconfiguration in my case or because of a DNS failure in Daniel's case) causes an SM crash, but I thought about it, and the user's experience will be the same as if it didn't crash.  The user gets a mess of errors both ways.  A little investigation (like getting the journal output or running testS3Connection) tells them exactly what the problem was.  The main difference is that the assertion failure can cause a core-dump, which may fill up the disk if SM keeps getting restarted.

IMO this isn't a show-stopper under the circumstances, confirmed it with Todd.  We'll follow up on it via the ticket I logged above."
892,MCOL-4,MCOL,Dipti Joshi,84652,2016-06-28 19:10:30,Is this going to use  systemd  ?,1,Is this going to use  systemd  ?
893,MCOL-4,MCOL,David Hill,84653,2016-06-28 19:23:47,"first will get to where I can build...

Then will included the systemd changes. Still research what that will be and the amount of changes needed.",2,"first will get to where I can build...

Then will included the systemd changes. Still research what that will be and the amount of changes needed."
894,MCOL-4,MCOL,Dipti Joshi,84654,2016-06-28 19:28:29,MCOL-70 is the task on systemd,3,MCOL-70 is the task on systemd
895,MCOL-4,MCOL,David Hill,84678,2016-06-29 19:08:38,"can successfully build and generate centos-7 rpms on a centos 7 Os machine..

Next is to setup and run regression test on the centos 7 machine with the centos 7 build..",4,"can successfully build and generate centos-7 rpms on a centos 7 Os machine..

Next is to setup and run regression test on the centos 7 machine with the centos 7 build.."
896,MCOL-4,MCOL,David Hill,84717,2016-07-01 20:44:29,generating centos 7 system rpms and running nightly regression test,5,generating centos 7 system rpms and running nightly regression test
897,MCOL-4000,MCOL,Gagan Goel,152887,2020-05-13 00:06:31,"Following values can be used for columnstore_use_import_for_batchinsert:

  OFF (0): Do not use cpimport for LDI or INSERT..SELECT
  ON (1, default): Use cpimport for LDI or INSERT..SELECT only for non-transactional queries
  ALWAYS (2): Use cpimport for LDI or INSERT..SELECT from within a transaction as well as for non-transactional queries

The session variable can be changed for either the current session using the 'set' command,
or for all future sessions using the 'set global' command.

For QA:
  Create a table with a few columns and a large data file (e.g. 100k or 200k records) to be imported using LDI.

1.   First test the current default behaviour, i.e. columnstore_use_import_for_batchinsert=ON:

*  Perform LDI inside a transaction (e.g. by using the START TRANSACTION command, or SET AUTOCOMMIT=0): this should use the regular DML operation. Doing a rollback will undo the load from the table.
*  Perform LDI outside the transaction: this will use cpimport for the load. To check the operation uses cpimport, use the linux top command. This operation will also run magnitudes of order faster than the previous one from within a transaction that used DML. Performing a rollback with not undo the load.



2.   Test the new behaviour by setting columnstore_use_import_for_batchinsert=ALWAYS:

*   Perform LDI inside a transaction: this time, it should use cpimport for the operation. Doing a rollback will not undo the load from the table.
*   Perform LDI outside the transaction: this should again use cpimport for the load. Performing a rollback with not undo the load.

  Same tests can be repeated for INSERT ... SELECT.",1,"Following values can be used for columnstore_use_import_for_batchinsert:

  OFF (0): Do not use cpimport for LDI or INSERT..SELECT
  ON (1, default): Use cpimport for LDI or INSERT..SELECT only for non-transactional queries
  ALWAYS (2): Use cpimport for LDI or INSERT..SELECT from within a transaction as well as for non-transactional queries

The session variable can be changed for either the current session using the 'set' command,
or for all future sessions using the 'set global' command.

For QA:
  Create a table with a few columns and a large data file (e.g. 100k or 200k records) to be imported using LDI.

1.   First test the current default behaviour, i.e. columnstore_use_import_for_batchinsert=ON:

*  Perform LDI inside a transaction (e.g. by using the START TRANSACTION command, or SET AUTOCOMMIT=0): this should use the regular DML operation. Doing a rollback will undo the load from the table.
*  Perform LDI outside the transaction: this will use cpimport for the load. To check the operation uses cpimport, use the linux top command. This operation will also run magnitudes of order faster than the previous one from within a transaction that used DML. Performing a rollback with not undo the load.



2.   Test the new behaviour by setting columnstore_use_import_for_batchinsert=ALWAYS:

*   Perform LDI inside a transaction: this time, it should use cpimport for the operation. Doing a rollback will not undo the load from the table.
*   Perform LDI outside the transaction: this should again use cpimport for the load. Performing a rollback with not undo the load.

  Same tests can be repeated for INSERT ... SELECT."
898,MCOL-4000,MCOL,Gagan Goel,153328,2020-05-15 17:53:39,"For QA: I have added instructions above on how to test this feature. In addition, we have added test cases to the regression suite for this: mysql/queries/working_dml/misc/MCOL-4000.sql",2,"For QA: I have added instructions above on how to test this feature. In addition, we have added test cases to the regression suite for this: mysql/queries/working_dml/misc/MCOL-4000.sql"
899,MCOL-4000,MCOL,Daniel Lee,153572,2020-05-19 01:37:25,Created MCOL-4007 for tracking this issue in 1.5.  Removing fixed version of 1.5 in this ticket.,3,Created MCOL-4007 for tracking this issue in 1.5.  Removing fixed version of 1.5 in this ticket.
900,MCOL-4000,MCOL,Daniel Lee,153573,2020-05-19 01:42:32,"Build verified: 1.4.4-1 (Jenkins 20200518)

Tested all these modes (On, Off, Always) for the columnstore_use_import_for_batchinsert variable, with and without transaction.

Tested for commit, auto commit, and rollback, for both LDI and insert..select.

",4,"Build verified: 1.4.4-1 (Jenkins 20200518)

Tested all these modes (On, Off, Always) for the columnstore_use_import_for_batchinsert variable, with and without transaction.

Tested for commit, auto commit, and rollback, for both LDI and insert..select.

"
901,MCOL-405,MCOL,David Hill,97925,2017-07-24 16:23:47,document pubished,1,document pubished
902,MCOL-406,MCOL,Andrew Hutchings,88350,2016-11-15 16:22:07,"My initial query was wrong for a number of reasons. Most of these down to one join's results skewing the other join. These are the three queries:

Extent file sizes:
{noformat}
select c.table_name, sum(e1.file_size) extent_file_size from columnstore_columns c join columnstore_extents e1 on c.object_id = e1.object_id and e1.block_offset=0 group by c.table_name;
{noformat}

Dict file sizes:
{noformat}
select c.table_name, sum(e1.fsize) dict_file_size from columnstore_columns c join (select object_id, sum(file_size) fsize from columnstore_extents where block_offset=0 group by object_id) e1 on c.dictionary_object_id = e1.object_id group by c.table_name;
{noformat}

Total file sizes:
{noformat}
select sum(file_size) from columnstore_extents where block_offset=0;
{noformat}",1,"My initial query was wrong for a number of reasons. Most of these down to one join's results skewing the other join. These are the three queries:

Extent file sizes:
{noformat}
select c.table_name, sum(e1.file_size) extent_file_size from columnstore_columns c join columnstore_extents e1 on c.object_id = e1.object_id and e1.block_offset=0 group by c.table_name;
{noformat}

Dict file sizes:
{noformat}
select c.table_name, sum(e1.fsize) dict_file_size from columnstore_columns c join (select object_id, sum(file_size) fsize from columnstore_extents where block_offset=0 group by object_id) e1 on c.dictionary_object_id = e1.object_id group by c.table_name;
{noformat}

Total file sizes:
{noformat}
select sum(file_size) from columnstore_extents where block_offset=0;
{noformat}"
903,MCOL-406,MCOL,Andrew Hutchings,88351,2016-11-15 16:25:12,"Things to note:

* It will be slightly different to 'du' because 'du' includes things like versionbuffer and the file size of the directory entries.
* Extent+Dict != total. This is because there are small hidden tables that cannot be exposed to columnstore_columns",2,"Things to note:

* It will be slightly different to 'du' because 'du' includes things like versionbuffer and the file size of the directory entries.
* Extent+Dict != total. This is because there are small hidden tables that cannot be exposed to columnstore_columns"
904,MCOL-406,MCOL,Andrew Hutchings,88357,2016-11-15 17:56:23,"Total data size:
{noformat}
select sum(dsize) from (select object_id, max(block_offset), sum(data_size) as dsize from columnstore_extents group by object_id) ce;
{noformat}

Need to use this to work out estimated compression ratio",3,"Total data size:
{noformat}
select sum(dsize) from (select object_id, max(block_offset), sum(data_size) as dsize from columnstore_extents group by object_id) ce;
{noformat}

Need to use this to work out estimated compression ratio"
905,MCOL-406,MCOL,Andrew Hutchings,88358,2016-11-15 18:15:56,also requested for this feature: a shell wrapper script,4,also requested for this feature: a shell wrapper script
906,MCOL-406,MCOL,Andrew Hutchings,88567,2016-11-21 21:34:49,"Compression ratio (for compressed tables):

{noformat}
select sum(data_size) / sum(compressed_data_size) * 100 from columnstore_extents where block_offset=0 and uncompressed_file_size is not null;
{noformat}

Not quite right, need mac(block_offset)",5,"Compression ratio (for compressed tables):

{noformat}
select sum(data_size) / sum(compressed_data_size) * 100 from columnstore_extents where block_offset=0 and uncompressed_file_size is not null;
{noformat}

Not quite right, need mac(block_offset)"
907,MCOL-406,MCOL,Andrew Hutchings,88643,2016-11-23 22:20:08,"New patch does the following:

* Fix a few minor bugs
* Move the filename/size to a new table called information_schema.columnstore_files
* Adds a new schema called columnstore_info
* Add three new stored procedures in columnstore_info:
** total_usage() - gives total disk/data usage for all columnstore tables (excluding system tables)
** table_usage() - gives total disk/data usage for a given table in single quotes or NULL to get usage for all tables
** compression_ration() - gives the overall compression ratio for the compressed columns in ColumnStore

Done everything apart from a script to access it. Getting a script to work if a MySQL password is changed could be difficult and it should be easy to access the stored procedures.

Moving to stalled until tree is unfrozen",6,"New patch does the following:

* Fix a few minor bugs
* Move the filename/size to a new table called information_schema.columnstore_files
* Adds a new schema called columnstore_info
* Add three new stored procedures in columnstore_info:
** total_usage() - gives total disk/data usage for all columnstore tables (excluding system tables)
** table_usage() - gives total disk/data usage for a given table in single quotes or NULL to get usage for all tables
** compression_ration() - gives the overall compression ratio for the compressed columns in ColumnStore

Done everything apart from a script to access it. Getting a script to work if a MySQL password is changed could be difficult and it should be easy to access the stored procedures.

Moving to stalled until tree is unfrozen"
908,MCOL-406,MCOL,Andrew Hutchings,88669,2016-11-24 15:41:02,Documentation updates to follow after merge,7,Documentation updates to follow after merge
909,MCOL-406,MCOL,Andrew Hutchings,88794,2016-11-28 19:55:36,Documentation draft available at: https://mariadb.com/kb/en/mariadb/columnstore-information-schema-tables/+r/50150/,8,Documentation draft available at: URL
910,MCOL-406,MCOL,Daniel Lee,89092,2016-12-02 23:18:38,"Build tested:  Source from Github.

[root@localhost mariadb-columnstore-server]# git show
commit 3795bd4cf42d59b792c473101703911fb53e9297
Merge: 570184c 84714c9
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Wed Nov 30 11:42:14 2016 -0600

    Merge pull request #18 from mariadb-corporation/MCOL-424
    
    MCOL-424 Disable indexes for cross-engine

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 63266938716bd933b80eb03fc8aa4ef10d25eab6
Merge: 691c52c 4efd58d
Author: david hill <david.hill@mariadb.com>
Date:   Fri Dec 2 10:03:11 2016 -0600

    merge mcol-421 branch

It is nicely implemented and the information that it provides is very useful.  The following are few issues that I found:

1) 
MariaDB [columnstore_info]> call columnstore_info.table_usage();
ERROR 1318 (42000): Incorrect number of arguments for PROCEDURE columnstore_info.table_usage; expected 1, got 0

The documentation says this:

The table_usage() procedure gives a the total data disk usage, dictionary disk usage and grand total disk usage per-table. It can be called one of two ways, the first gives a total for each table:

> call columnstore_info.table_usage();

Or for a specific table, my_table in this example:

> call columnstore_info.table_usage('my_table');


2)
MariaDB [tpch1c]> call columnstore_info.table_usage('lineitem');
+------------+-----------------+-----------------+-------------+
| TABLE_NAME | DATA_DISK_USAGE | DICT_DISK_USAGE | TOTAL_USAGE |
+------------+-----------------+-----------------+-------------+
| lineitem   | 5.34 GB         | 4.50 GB         | 9.84 GB     |
+------------+-----------------+-----------------+-------------+
1 row in set (0.09 sec)


The lineitem table exist in different schemas.  How do I specify a schema to identify a specific table.  Is this usage for all lineitem tables?


3) mysqld crashed when calling the table_usage() procedure, but I could not reproduce it after.


MariaDB [tpch1c]> call columnstore_info.table_usage('tpch10c.lineitem');
ERROR 2013 (HY000): Lost connection to MySQL server during query
MariaDB [tpch1c]> call columnstore_info.table_usage('tpch10c.lineitem');
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    3
Current database: tpch1c

Info from the /var/log/mariadb/mariadb.log file

161202 22:14:34 [ERROR] mysqld got signal 11 ;
This could be because you hit a bug. It is also possible that this binary
or one of the libraries it was linked against is corrupt, improperly built,
or misconfigured. This error can also be caused by malfunctioning hardware.

To report this bug, see https://mariadb.com/kb/en/reporting-bugs

We will try our best to scrape up some info that will hopefully help
diagnose the problem, but since we have already crashed, 
something is definitely wrong and this may fail.

Server version: 10.1.19-MariaDB
key_buffer_size=536870912
read_buffer_size=4194304
max_used_connections=1
max_threads=153
thread_count=1
It is possible that mysqld could use up to 
key_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 1780837 K  bytes of memory
Hope that's ok; if not, decrease some variables in the equation.

Thread pointer: 0x0x7f1de1a90908
Attempting backtrace. You can use the following information to find out
where mysqld died. If you see no messages after this, something went
terribly wrong...
stack_bottom = 0x7f1db0720d98 thread_stack 0x80000
/usr/local/mariadb/columnstore/mysql//bin/mysqld(my_print_stacktrace+0x29)[0x7f1ddff7d639]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(handle_fatal_signal+0x2dd)[0x7f1ddfb3a74d]
/lib64/libpthread.so.0(+0xf100)[0x7f1ddf168100]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(+0x4686b5)[0x7f1ddfa016b5]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN4JOIN14optimize_innerEv+0xed4)[0x7f1ddfa1a294]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN4JOIN8optimizeEv+0x2f)[0x7f1ddfa1c35f]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z12mysql_selectP3THDPPP4ItemP10TABLE_LISTjR4ListIS1_ES2_jP8st_orderSB_S2_SB_yP13select_resultP18st_select_lex_unitP13st_select_lex+0x1bb)[0x7f1ddfa1c5cb]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z13handle_selectP3THDP3LEXP13select_resultm+0x234)[0x7f1ddfa1d184]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(+0x428e09)[0x7f1ddf9c1e09]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z21mysql_execute_commandP3THD+0x5e52)[0x7f1ddf9cdca2]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN13sp_instr_stmt9exec_coreEP3THDPj+0x35)[0x7f1ddfc37d75]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN13sp_lex_keeper23reset_lex_and_exec_coreEP3THDPjbP8sp_instr+0x7d)[0x7f1ddfc3e2bd]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN13sp_instr_stmt7executeEP3THDPj+0x204)[0x7f1ddfc3e864]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN7sp_head7executeEP3THDb+0x767)[0x7f1ddfc3aa97]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN7sp_head17execute_procedureEP3THDP4ListI4ItemE+0x5a7)[0x7f1ddfc3bf47]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(+0x428afe)[0x7f1ddf9c1afe]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z21mysql_execute_commandP3THD+0x159c)[0x7f1ddf9c93ec]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z11mysql_parseP3THDPcjP12Parser_state+0x33b)[0x7f1ddf9d110b]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z18idb_vtable_processP3THDyP9Statement+0x10c8)[0x7f1ddf9d3318]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z16dispatch_command19enum_server_commandP3THDPcj+0x1f02)[0x7f1ddf9d7bd2]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z10do_commandP3THD+0x161)[0x7f1ddf9d8a41]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z24do_handle_one_connectionP3THD+0x194)[0x7f1ddfa8f0a4]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(handle_one_connection+0x37)[0x7f1ddfa8f277]
/lib64/libpthread.so.0(+0x7dc5)[0x7f1ddf160dc5]
/lib64/libc.so.6(clone+0x6d)[0x7f1ddd7a2ced]

Trying to get some variables.
Some pointers may be invalid and cause the dump to abort.
Query (0x7f1d70135e88): is an invalid pointer
Connection ID (thread ID): 12
Status: NOT_KILLED

Optimizer switch: index_merge=off,index_merge_union=off,index_merge_sort_union=off,index_merge_intersection=off,index_merge_sort_intersection=off,engine_condition_pushdown=off,index_condition_pushdown=off,derived_merge=off,derived_with_keys=off,firstmatch=off,loosescan=off,materialization=off,in_to_exists=on,semijoin=off,partial_match_rowid_merge=off,partial_match_table_scan=off,subquery_cache=off,mrr=off,mrr_cost_based=off,mrr_sort_keys=off,outer_join_with_cache=off,semijoin_with_cache=off,join_cache_incremental=off,join_cache_hashed=off,join_cache_bka=off,optimize_join_buffer_size=off,table_elimination=off,extended_keys=off,exists_to_in=on,orderby_uses_equalities=off

The manual page at http://dev.mysql.com/doc/mysql/en/crashing.html contains
information that should help you find out what is causing the crash.

We think the query pointer is invalid, but we will try to print it anyway. 
Query: SELECT TABLE_NAME, format_filesize(sum(cf.file_size)) DATA_DISK_USAGE, format_filesize(sum(IFNULL(ccf.file_size, 0))) DICT_DISK_USAGE, format_filesize(sum(cf.file_size) + sum(IFNULL(ccf.file_size, 0))) TOTAL_USAGE FROM INFORMATION_SCHEMA.COLUMNSTORE_COLUMNS cc
JOIN INFORMATION_SCHEMA.COLUMNSTORE_FILES cf ON cc.object_id = cf.object_id
LEFT JOIN INFORMATION_SCHEMA.COLUMNSTORE_FILES ccf ON cc.dictionary_object_id = ccf.object_id
WHERE table_name =  NAME_CONST('t_name',_latin1'tpch10c.lineitem' COLLATE 'latin1_swedish_ci') GROUP BY table_name

161202 22:14:35 mysqld_safe Number of processes running now: 0
161202 22:14:35 mysqld_safe mysqld restarted
",9,"Build tested:  Source from Github.

[root@localhost mariadb-columnstore-server]# git show
commit 3795bd4cf42d59b792c473101703911fb53e9297
Merge: 570184c 84714c9
Author: dhall-InfiniDB 
Date:   Wed Nov 30 11:42:14 2016 -0600

    Merge pull request #18 from mariadb-corporation/MCOL-424
    
    MCOL-424 Disable indexes for cross-engine

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 63266938716bd933b80eb03fc8aa4ef10d25eab6
Merge: 691c52c 4efd58d
Author: david hill 
Date:   Fri Dec 2 10:03:11 2016 -0600

    merge mcol-421 branch

It is nicely implemented and the information that it provides is very useful.  The following are few issues that I found:

1) 
MariaDB [columnstore_info]> call columnstore_info.table_usage();
ERROR 1318 (42000): Incorrect number of arguments for PROCEDURE columnstore_info.table_usage; expected 1, got 0

The documentation says this:

The table_usage() procedure gives a the total data disk usage, dictionary disk usage and grand total disk usage per-table. It can be called one of two ways, the first gives a total for each table:

> call columnstore_info.table_usage();

Or for a specific table, my_table in this example:

> call columnstore_info.table_usage('my_table');


2)
MariaDB [tpch1c]> call columnstore_info.table_usage('lineitem');
+------------+-----------------+-----------------+-------------+
| TABLE_NAME | DATA_DISK_USAGE | DICT_DISK_USAGE | TOTAL_USAGE |
+------------+-----------------+-----------------+-------------+
| lineitem   | 5.34 GB         | 4.50 GB         | 9.84 GB     |
+------------+-----------------+-----------------+-------------+
1 row in set (0.09 sec)


The lineitem table exist in different schemas.  How do I specify a schema to identify a specific table.  Is this usage for all lineitem tables?


3) mysqld crashed when calling the table_usage() procedure, but I could not reproduce it after.


MariaDB [tpch1c]> call columnstore_info.table_usage('tpch10c.lineitem');
ERROR 2013 (HY000): Lost connection to MySQL server during query
MariaDB [tpch1c]> call columnstore_info.table_usage('tpch10c.lineitem');
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    3
Current database: tpch1c

Info from the /var/log/mariadb/mariadb.log file

161202 22:14:34 [ERROR] mysqld got signal 11 ;
This could be because you hit a bug. It is also possible that this binary
or one of the libraries it was linked against is corrupt, improperly built,
or misconfigured. This error can also be caused by malfunctioning hardware.

To report this bug, see URL

We will try our best to scrape up some info that will hopefully help
diagnose the problem, but since we have already crashed, 
something is definitely wrong and this may fail.

Server version: 10.1.19-MariaDB
key_buffer_size=536870912
read_buffer_size=4194304
max_used_connections=1
max_threads=153
thread_count=1
It is possible that mysqld could use up to 
key_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 1780837 K  bytes of memory
Hope that's ok; if not, decrease some variables in the equation.

Thread pointer: 0x0x7f1de1a90908
Attempting backtrace. You can use the following information to find out
where mysqld died. If you see no messages after this, something went
terribly wrong...
stack_bottom = 0x7f1db0720d98 thread_stack 0x80000
/usr/local/mariadb/columnstore/mysql//bin/mysqld(my_print_stacktrace+0x29)[0x7f1ddff7d639]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(handle_fatal_signal+0x2dd)[0x7f1ddfb3a74d]
/lib64/libpthread.so.0(+0xf100)[0x7f1ddf168100]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(+0x4686b5)[0x7f1ddfa016b5]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN4JOIN14optimize_innerEv+0xed4)[0x7f1ddfa1a294]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN4JOIN8optimizeEv+0x2f)[0x7f1ddfa1c35f]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z12mysql_selectP3THDPPP4ItemP10TABLE_LISTjR4ListIS1_ES2_jP8st_orderSB_S2_SB_yP13select_resultP18st_select_lex_unitP13st_select_lex+0x1bb)[0x7f1ddfa1c5cb]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z13handle_selectP3THDP3LEXP13select_resultm+0x234)[0x7f1ddfa1d184]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(+0x428e09)[0x7f1ddf9c1e09]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z21mysql_execute_commandP3THD+0x5e52)[0x7f1ddf9cdca2]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN13sp_instr_stmt9exec_coreEP3THDPj+0x35)[0x7f1ddfc37d75]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN13sp_lex_keeper23reset_lex_and_exec_coreEP3THDPjbP8sp_instr+0x7d)[0x7f1ddfc3e2bd]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN13sp_instr_stmt7executeEP3THDPj+0x204)[0x7f1ddfc3e864]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN7sp_head7executeEP3THDb+0x767)[0x7f1ddfc3aa97]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_ZN7sp_head17execute_procedureEP3THDP4ListI4ItemE+0x5a7)[0x7f1ddfc3bf47]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(+0x428afe)[0x7f1ddf9c1afe]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z21mysql_execute_commandP3THD+0x159c)[0x7f1ddf9c93ec]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z11mysql_parseP3THDPcjP12Parser_state+0x33b)[0x7f1ddf9d110b]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z18idb_vtable_processP3THDyP9Statement+0x10c8)[0x7f1ddf9d3318]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z16dispatch_command19enum_server_commandP3THDPcj+0x1f02)[0x7f1ddf9d7bd2]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z10do_commandP3THD+0x161)[0x7f1ddf9d8a41]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(_Z24do_handle_one_connectionP3THD+0x194)[0x7f1ddfa8f0a4]
/usr/local/mariadb/columnstore/mysql//bin/mysqld(handle_one_connection+0x37)[0x7f1ddfa8f277]
/lib64/libpthread.so.0(+0x7dc5)[0x7f1ddf160dc5]
/lib64/libc.so.6(clone+0x6d)[0x7f1ddd7a2ced]

Trying to get some variables.
Some pointers may be invalid and cause the dump to abort.
Query (0x7f1d70135e88): is an invalid pointer
Connection ID (thread ID): 12
Status: NOT_KILLED

Optimizer switch: index_merge=off,index_merge_union=off,index_merge_sort_union=off,index_merge_intersection=off,index_merge_sort_intersection=off,engine_condition_pushdown=off,index_condition_pushdown=off,derived_merge=off,derived_with_keys=off,firstmatch=off,loosescan=off,materialization=off,in_to_exists=on,semijoin=off,partial_match_rowid_merge=off,partial_match_table_scan=off,subquery_cache=off,mrr=off,mrr_cost_based=off,mrr_sort_keys=off,outer_join_with_cache=off,semijoin_with_cache=off,join_cache_incremental=off,join_cache_hashed=off,join_cache_bka=off,optimize_join_buffer_size=off,table_elimination=off,extended_keys=off,exists_to_in=on,orderby_uses_equalities=off

The manual page at URL contains
information that should help you find out what is causing the crash.

We think the query pointer is invalid, but we will try to print it anyway. 
Query: SELECT TABLE_NAME, format_filesize(sum(cf.file_size)) DATA_DISK_USAGE, format_filesize(sum(IFNULL(ccf.file_size, 0))) DICT_DISK_USAGE, format_filesize(sum(cf.file_size) + sum(IFNULL(ccf.file_size, 0))) TOTAL_USAGE FROM INFORMATION_SCHEMA.COLUMNSTORE_COLUMNS cc
JOIN INFORMATION_SCHEMA.COLUMNSTORE_FILES cf ON cc.object_id = cf.object_id
LEFT JOIN INFORMATION_SCHEMA.COLUMNSTORE_FILES ccf ON cc.dictionary_object_id = ccf.object_id
WHERE table_name =  NAME_CONST('t_name',_latin1'tpch10c.lineitem' COLLATE 'latin1_swedish_ci') GROUP BY table_name

161202 22:14:35 mysqld_safe Number of processes running now: 0
161202 22:14:35 mysqld_safe mysqld restarted
"
911,MCOL-406,MCOL,Andrew Hutchings,89106,2016-12-03 10:37:04,"For point 1, it is supposed to be ""call columnstore_info.table_usage(NULL);"".

For the rest, I'll look into this early next week.",10,"For point 1, it is supposed to be ""call columnstore_info.table_usage(NULL);"".

For the rest, I'll look into this early next week."
912,MCOL-406,MCOL,Andrew Hutchings,89198,2016-12-05 20:55:19,Segfault was very likely to be MCOL-441,11,Segfault was very likely to be MCOL-441
913,MCOL-406,MCOL,Andrew Hutchings,89223,2016-12-06 13:05:17,"Review for fixes based on Daniel's comments.

To answer the specific problems:

1) A 'NULL' needs to be provided for the parameter since optional parameters are not possible. I will make sure to update the documentation appropriately

2) Which you can only use the table name in the query (schema.table format won't work) with the latest pull request it will give the results broken down by schema and table.

3) I'm pretty sure MCOL-441 will fix this.",12,"Review for fixes based on Daniel's comments.

To answer the specific problems:

1) A 'NULL' needs to be provided for the parameter since optional parameters are not possible. I will make sure to update the documentation appropriately

2) Which you can only use the table name in the query (schema.table format won't work) with the latest pull request it will give the results broken down by schema and table.

3) I'm pretty sure MCOL-441 will fix this."
914,MCOL-406,MCOL,David Thompson,89248,2016-12-06 19:28:03,"Agreed, lets go forward with justins suggestion for 1.0.6.",13,"Agreed, lets go forward with justins suggestion for 1.0.6."
915,MCOL-406,MCOL,Andrew Hutchings,89259,2016-12-06 21:33:59,Another pull request implemented the discussed change to table_usage(),14,Another pull request implemented the discussed change to table_usage()
916,MCOL-406,MCOL,Andrew Hutchings,89286,2016-12-07 09:17:43,"Another new pull request, fixes the table_usage('schema', NULL) use case and the hang [~dleeyh] experienced.",15,"Another new pull request, fixes the table_usage('schema', NULL) use case and the hang [~dleeyh] experienced."
917,MCOL-406,MCOL,Daniel Lee,89320,2016-12-07 17:03:51,"Build verified: Github source

[root@localhost mariadb-columnstore-server]# git show
commit 8592d353c5477940f9600566639302de9fa994c7
Merge: 3795bd4 7af4e57
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Tue Dec 6 09:49:03 2016 -0600

    Merge pull request #20 from mariadb-corporation/MCOL-441
    
    MCOL-441 Fix segfault on SP error

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 7a8322dc28471b830aca243698cd7fce5bc4401c
Merge: 5c0ced8 9b6beb4
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Wed Dec 7 10:10:22 2016 -0600

    Merge pull request #73 from mariadb-corporation/MCOL-435
    
    Mcol 435

Verified the two fixes.

",16,"Build verified: Github source

[root@localhost mariadb-columnstore-server]# git show
commit 8592d353c5477940f9600566639302de9fa994c7
Merge: 3795bd4 7af4e57
Author: dhall-InfiniDB 
Date:   Tue Dec 6 09:49:03 2016 -0600

    Merge pull request #20 from mariadb-corporation/MCOL-441
    
    MCOL-441 Fix segfault on SP error

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 7a8322dc28471b830aca243698cd7fce5bc4401c
Merge: 5c0ced8 9b6beb4
Author: dhall-InfiniDB 
Date:   Wed Dec 7 10:10:22 2016 -0600

    Merge pull request #73 from mariadb-corporation/MCOL-435
    
    Mcol 435

Verified the two fixes.

"
918,MCOL-4155,MCOL,David Hall,163350,2020-08-18 19:51:24,"The enterprise build should have a x-columnstore.cnf (not sure about the hyphen). Community build should have columnstore.cnf. Neither should have both.

Note that the name change is made during installation of packages.",1,"The enterprise build should have a x-columnstore.cnf (not sure about the hyphen). Community build should have columnstore.cnf. Neither should have both.

Note that the name change is made during installation of packages."
919,MCOL-4155,MCOL,Ben Thompson,163583,2020-08-19 21:05:02,Fix needs to be redone to work with building columnstore as submodule of server,2,Fix needs to be redone to work with building columnstore as submodule of server
920,MCOL-4155,MCOL,David Hall,165941,2020-09-14 17:07:42,We need the Jenkins build from Server to test.,3,We need the Jenkins build from Server to test.
921,MCOL-4167,MCOL,Gagan Goel,159811,2020-07-13 22:23:05,"Support to FuncExp framework is added, along with functional tests for both SIGNED and UNSIGNED wide decimal types.

Some bug scenarios are also identified for some specific edge cases/functions, the source of which is either ColumnStore or Server. These include: MCOL-4112, MCOL-4121, MCOL-4161, MDEV-23032, MDEV-23118.",1,"Support to FuncExp framework is added, along with functional tests for both SIGNED and UNSIGNED wide decimal types.

Some bug scenarios are also identified for some specific edge cases/functions, the source of which is either ColumnStore or Server. These include: MCOL-4112, MCOL-4121, MCOL-4161, MDEV-23032, MDEV-23118."
922,MCOL-4171,MCOL,David Hall,161728,2020-07-31 20:10:47,Also regression test #235 which has picked up all the changes to develop since the inception of MCOL-641,1,Also regression test #235 which has picked up all the changes to develop since the inception of MCOL-641
923,MCOL-4172,MCOL,David Hall,161445,2020-07-29 17:14:39,"Something to look into:
InnoDB:
MariaDB [dhall]> SELECT ""avg(38)_test2"", avg(d1),avg(d2),avg(d3) FROM cs2;
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | avg(d1)                     | avg(d2)                              | avg(d3)                                  |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | 2255892235599326597959.1818 | 224466889113357793635.80022446687273 | 0.10101010010101010037373725644743325455 |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
1 row in set (0.010 sec)

Columnstore:
MariaDB [tpch1]> SELECT ""avg(38)_test2"", avg(d1),avg(d2),avg(d3) FROM cs2;
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | avg(d1)                     | avg(d2)                              | avg(d3)                                  |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | 2255892235599326597959.1818 | 224466889113357793635.8002244668727{color:red}2{color} | 0.1010101001010101003737372564474332545{color:red}4{color} |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+

This can happen signed or unsigned
",1,"Something to look into:
InnoDB:
MariaDB [dhall]> SELECT ""avg(38)_test2"", avg(d1),avg(d2),avg(d3) FROM cs2;
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | avg(d1)                     | avg(d2)                              | avg(d3)                                  |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | 2255892235599326597959.1818 | 224466889113357793635.80022446687273 | 0.10101010010101010037373725644743325455 |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
1 row in set (0.010 sec)

Columnstore:
MariaDB [tpch1]> SELECT ""avg(38)_test2"", avg(d1),avg(d2),avg(d3) FROM cs2;
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | avg(d1)                     | avg(d2)                              | avg(d3)                                  |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+
| avg(38)_test2 | 2255892235599326597959.1818 | 224466889113357793635.8002244668727{color:red}2{color} | 0.1010101001010101003737372564474332545{color:red}4{color} |
+---------------+-----------------------------+--------------------------------------+------------------------------------------+

This can happen signed or unsigned
"
924,MCOL-4172,MCOL,Roman,165120,2020-09-03 11:22:07,[~David.Hall] Thanks for putting a note. It is a known and still open issue.,2,[~David.Hall] Thanks for putting a note. It is a known and still open issue.
925,MCOL-4172,MCOL,Roman,166182,2020-09-17 12:58:32,Plz review.,3,Plz review.
926,MCOL-4174,MCOL,Gagan Goel,172681,2020-11-19 19:46:22,"Looks like we broke the connector UDF code with this patch (i.e. the changes to dbcon/mysql/ha_mcs_partition.cpp):

{code:sql}
MariaDB [test]> create table cs1 (a int)engine=columnstore;
Query OK, 0 rows affected (0.650 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                    |
+--------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     Enabled |
+--------------------------------------------------------------------------------------------------+
1 row in set (0.048 sec)

MariaDB [test]> insert into cs1 values (123), (234);
Query OK, 2 rows affected (0.407 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     N/A                           N/A                           Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> select * from cs1;
+------+
| a    |
+------+
|  123 |
|  234 |
+------+
2 rows in set (0.097 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                    |
+--------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     Enabled |
+--------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)
{code}

The expected output is the following:
{code:sql}
MariaDB [test]> create table cs1 (a int)engine=columnstore;
Query OK, 0 rows affected (0.472 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     Empty/Null                    Empty/Null                    Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.048 sec)

MariaDB [test]> insert into cs1 values (123), (234);
Query OK, 2 rows affected (0.357 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     N/A                           N/A                           Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> select * from cs1;
+------+
| a    |
+------+
|  123 |
|  234 |
+------+
2 rows in set (0.080 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     123                           234                           Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)
{code}

Other UDFs might also have issues, I only tried calshowpartitions().",1,"Looks like we broke the connector UDF code with this patch (i.e. the changes to dbcon/mysql/ha_mcs_partition.cpp):

{code:sql}
MariaDB [test]> create table cs1 (a int)engine=columnstore;
Query OK, 0 rows affected (0.650 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                    |
+--------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     Enabled |
+--------------------------------------------------------------------------------------------------+
1 row in set (0.048 sec)

MariaDB [test]> insert into cs1 values (123), (234);
Query OK, 2 rows affected (0.407 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     N/A                           N/A                           Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> select * from cs1;
+------+
| a    |
+------+
|  123 |
|  234 |
+------+
2 rows in set (0.097 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                    |
+--------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     Enabled |
+--------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)
{code}

The expected output is the following:
{code:sql}
MariaDB [test]> create table cs1 (a int)engine=columnstore;
Query OK, 0 rows affected (0.472 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     Empty/Null                    Empty/Null                    Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.048 sec)

MariaDB [test]> insert into cs1 values (123), (234);
Query OK, 2 rows affected (0.357 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     N/A                           N/A                           Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)

MariaDB [test]> select * from cs1;
+------+
| a    |
+------+
|  123 |
|  234 |
+------+
2 rows in set (0.080 sec)

MariaDB [test]> select calshowpartitions('cs1', 'a');
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| calshowpartitions('cs1', 'a')                                                                                                                                |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Part#     Min                           Max                           Status
  0.0.1     123                           234                           Enabled |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.001 sec)
{code}

Other UDFs might also have issues, I only tried calshowpartitions()."
927,MCOL-4174,MCOL,Alexander Barkov,172713,2020-11-20 03:07:09,"The bug seems to be in this piece of the code in ha_mcs_partition.cpp:
{code:cpp}
        for (partIt = partMap.begin(); partIt != partMap.end(); ++partIt)
        {
            ostringstream oss;
            oss << partIt->first;        for (partIt = partMap.begin(); partIt != partMap.end(); ++partIt)
        {
            ostringstream oss;
            oss << partIt->first;
            output << ""\n  "" << setw(10) << oss.str();

            if (partIt->second.is_invalid())
            {
                output << setw(valueCharLength) << ""N/A""
                       << setw(valueCharLength) << ""N/A"";
            }
            else
            {
                const datatypes::TypeHandler *h= ct.typeHandler();
                oss << h->formatPartitionInfo(ct, partIt->second);
{code}
The last line should be:
{code:cpp}
                output << h->formatPartitionInfo(ct, partIt->second);
{code}
",2,"The bug seems to be in this piece of the code in ha_mcs_partition.cpp:
{code:cpp}
        for (partIt = partMap.begin(); partIt != partMap.end(); ++partIt)
        {
            ostringstream oss;
            oss first;        for (partIt = partMap.begin(); partIt != partMap.end(); ++partIt)
        {
            ostringstream oss;
            oss first;
            output << ""\n  "" << setw(10) << oss.str();

            if (partIt->second.is_invalid())
            {
                output << setw(valueCharLength) << ""N/A""
                       << setw(valueCharLength) << ""N/A"";
            }
            else
            {
                const datatypes::TypeHandler *h= ct.typeHandler();
                oss formatPartitionInfo(ct, partIt->second);
{code}
The last line should be:
{code:cpp}
                output formatPartitionInfo(ct, partIt->second);
{code}
"
928,MCOL-4175,MCOL,Roman,161027,2020-07-24 12:32:13,Plz review.,1,Plz review.
929,MCOL-4177,MCOL,Gagan Goel,161700,2020-07-31 14:59:38,This task should also add support for INSERT .. SELECT and LDI.,1,This task should also add support for INSERT .. SELECT and LDI.
930,MCOL-4178,MCOL,Gagan Goel,161475,2020-07-29 22:22:14,"At a few places in the code base, we are streaming extent map info to stdout or stringstream, as an example: ExtentMap::printEM(const EMEntry& em). We have currently excluded the streaming of bigLoVal/bigHiVal values from such places.",1,"At a few places in the code base, we are streaming extent map info to stdout or stringstream, as an example: ExtentMap::printEM(const EMEntry& em). We have currently excluded the streaming of bigLoVal/bigHiVal values from such places."
931,MCOL-4178,MCOL,Gagan Goel,161596,2020-07-31 00:03:23," [^casual_partitioning.sql]  [^casual_partitioning.sql.log] 

Attached is a very simple test to demonstrate casual partitioning on wide decimals. These tests, however, have not been added to the regression suite.",2," [^casual_partitioning.sql]  [^casual_partitioning.sql.log] 

Attached is a very simple test to demonstrate casual partitioning on wide decimals. These tests, however, have not been added to the regression suite."
932,MCOL-4180,MCOL,Roman,177809,2021-01-20 16:24:13,The main purpose for this issue is to take another look at the code changes made for wide decimal in dbcon/execplan.,1,The main purpose for this issue is to take another look at the code changes made for wide decimal in dbcon/execplan.
933,MCOL-4180,MCOL,Roman,177810,2021-01-20 16:24:32,Plz review.,2,Plz review.
934,MCOL-4188,MCOL,Roman,178287,2021-01-26 08:51:55,"Here is the list of the failed tests on my machine that needs to be researched:
{noformat}
working_tpch1_compareLogOnly/sub/bug3780.sql
working_tpch1_compareLogOnly/regr/regr_r2.sql
Compare failed - working_tpch1_compareLogOnly/misc/3a-xlou.sql
Compare failed - working_tpch1_compareLogOnly/misc/3b-xlou.sql
Compare failed - working_tpch1_compareLogOnly/misc/bug2891_negative.sql
Compare failed - working_tpch1_compareLogOnly/misc/bug3691.sql
Compare failed - working_tpch1_compareLogOnly/misc/bug5715.sql
Compare failed - working_tpch1_compareLogOnly/misc/sumavgoverflow.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_overflow_error.negative.sql
{noformat}
Othere failed tests are known and described(floor/ceil, bit ops, timezone dependant functions).
The list produced by CI is bigger. There are number of issues not listed here.",1,"Here is the list of the failed tests on my machine that needs to be researched:
{noformat}
working_tpch1_compareLogOnly/sub/bug3780.sql
working_tpch1_compareLogOnly/regr/regr_r2.sql
Compare failed - working_tpch1_compareLogOnly/misc/3a-xlou.sql
Compare failed - working_tpch1_compareLogOnly/misc/3b-xlou.sql
Compare failed - working_tpch1_compareLogOnly/misc/bug2891_negative.sql
Compare failed - working_tpch1_compareLogOnly/misc/bug3691.sql
Compare failed - working_tpch1_compareLogOnly/misc/bug5715.sql
Compare failed - working_tpch1_compareLogOnly/misc/sumavgoverflow.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_ceil.sql
Compare failed - working_tpch1/qa_fe_cnxFunctions/bug3334_overflow_error.negative.sql
{noformat}
Othere failed tests are known and described(floor/ceil, bit ops, timezone dependant functions).
The list produced by CI is bigger. There are number of issues not listed here."
935,MCOL-4188,MCOL,Roman,178439,2021-01-27 15:19:30,"I used the solution from [this commit|https://github.com/mariadb-corporation/mariadb-columnstore-engine/commit/9c18771b61] to fix working_tpch1_compareLogOnly/sub/bug3780.sql.
",2,"I used the solution from [this commit|URL to fix working_tpch1_compareLogOnly/sub/bug3780.sql.
"
936,MCOL-4188,MCOL,Roman,178457,2021-01-27 19:26:52,"There is a floating error that occasionally happens in unsigned tests:
Compare failed - working_tpch1_compareLogOnly/unsigned/unsigned_greatest.sql
Compare failed - working_tpch1_compareLogOnly/unsigned/unsigned_joins.sql
Compare failed - working_tpch1_compareLogOnly/unsigned/unsigned_least.sql
For details see MCOL-4519",3,"There is a floating error that occasionally happens in unsigned tests:
Compare failed - working_tpch1_compareLogOnly/unsigned/unsigned_greatest.sql
Compare failed - working_tpch1_compareLogOnly/unsigned/unsigned_joins.sql
Compare failed - working_tpch1_compareLogOnly/unsigned/unsigned_least.sql
For details see MCOL-4519"
937,MCOL-420,MCOL,Andrew Hutchings,88674,2016-11-24 19:42:01,added alias. Requested review from Hill as he knows OAM.,1,added alias. Requested review from Hill as he knows OAM.
938,MCOL-420,MCOL,Daniel Lee,88962,2016-11-30 21:33:36,"Build verified:

concept guides. See 'git help <command>' or 'git help <concept>'
to read about a specific subcommand or concept.
[root@localhost mariadb-columnstore-server]# git show
commit 3795bd4cf42d59b792c473101703911fb53e9297
Merge: 570184c 84714c9
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Wed Nov 30 11:42:14 2016 -0600

    Merge pull request #18 from mariadb-corporation/MCOL-424
    
    MCOL-424 Disable indexes for cross-engine

[root@localhost mariadb-columnstore-engine]# git show
commit 8cabbfbc1607667bcb3c25f4489c5c81f6b36b4b
Merge: 54b8643 8382a73
Author: david hill <david.hill@mariadb.com>
Date:   Tue Nov 29 14:19:09 2016 -0600

    Merge branch 'develop' of https://github.com/mariadb-corporation/mariadb-columnstore-engine into develop

Verified cpimport alias exist in ~/.bashrc and columnstoreAlias



",2,"Build verified:

concept guides. See 'git help ' or 'git help '
to read about a specific subcommand or concept.
[root@localhost mariadb-columnstore-server]# git show
commit 3795bd4cf42d59b792c473101703911fb53e9297
Merge: 570184c 84714c9
Author: dhall-InfiniDB 
Date:   Wed Nov 30 11:42:14 2016 -0600

    Merge pull request #18 from mariadb-corporation/MCOL-424
    
    MCOL-424 Disable indexes for cross-engine

[root@localhost mariadb-columnstore-engine]# git show
commit 8cabbfbc1607667bcb3c25f4489c5c81f6b36b4b
Merge: 54b8643 8382a73
Author: david hill 
Date:   Tue Nov 29 14:19:09 2016 -0600

    Merge branch 'develop' of URL into develop

Verified cpimport alias exist in ~/.bashrc and columnstoreAlias



"
939,MCOL-4285,MCOL,Gagan Goel,164724,2020-08-31 17:19:06,"To test the ColumnStore insert cache for replication, perform the steps as would be required for a normal replication set up between an InnoDB master and a ColumnStore slave (refer to https://mariadb.com/kb/en/setting-up-replication/). But this time, enable the cache on the slave node.

To enable the cache, turn on the read-only system variable 'columnstore_cache_inserts'. This can be set either in the .cnf config file for the slave node ('columnstore_cache_inserts=ON'), or as a command line option '--columnstore_cache_inserts' at server start up. Ensure that the cache is enabled by executing ""show variables like 'columnstore_cache_inserts';"" command. The cache is disabled by default.

There is an additional session variable that is added: 'columnstore_cache_flush_threshold'. This controls the number of records after which the cache will be flushed to the ColumnStore table. Default value is set to 500000, but the user can increase or decrease this setting based on the workload (it shouldn't be set to too low else the cache will be flushed too often. On the other hand, it shouldn't be set to too high, else a single cache flush might take too long).

Current limitations of the cache:

* Rollbacks are not supported. This is not an issue for the replication use case since the binlog discards transactions with rollbacks, and the events are not played on the slave.

Things to keep in mind:

* A ColumnStore table created with the cache disabled (default behaviour) cannot participate in DQL or DML queries if the server is restarted with the cache enabled. This behaviour is expected since the actual cache was not created in the first place when the original ColumnStore table was created. *UPDATE:* This is fixed in MCOL-4769.
* Cache only improves performance of INSERTs, including batch inserts as well as LDI. INSERT..SELECT (as well as any other type of DML) is not improved, however.",1,"To test the ColumnStore insert cache for replication, perform the steps as would be required for a normal replication set up between an InnoDB master and a ColumnStore slave (refer to URL But this time, enable the cache on the slave node.

To enable the cache, turn on the read-only system variable 'columnstore_cache_inserts'. This can be set either in the .cnf config file for the slave node ('columnstore_cache_inserts=ON'), or as a command line option '--columnstore_cache_inserts' at server start up. Ensure that the cache is enabled by executing ""show variables like 'columnstore_cache_inserts';"" command. The cache is disabled by default.

There is an additional session variable that is added: 'columnstore_cache_flush_threshold'. This controls the number of records after which the cache will be flushed to the ColumnStore table. Default value is set to 500000, but the user can increase or decrease this setting based on the workload (it shouldn't be set to too low else the cache will be flushed too often. On the other hand, it shouldn't be set to too high, else a single cache flush might take too long).

Current limitations of the cache:

* Rollbacks are not supported. This is not an issue for the replication use case since the binlog discards transactions with rollbacks, and the events are not played on the slave.

Things to keep in mind:

* A ColumnStore table created with the cache disabled (default behaviour) cannot participate in DQL or DML queries if the server is restarted with the cache enabled. This behaviour is expected since the actual cache was not created in the first place when the original ColumnStore table was created. *UPDATE:* This is fixed in MCOL-4769.
* Cache only improves performance of INSERTs, including batch inserts as well as LDI. INSERT..SELECT (as well as any other type of DML) is not improved, however."
940,MCOL-4285,MCOL,Gagan Goel,164725,2020-08-31 17:19:53,For QA: Refer to my previous comment for instructions on testing the cache. ,2,For QA: Refer to my previous comment for instructions on testing the cache. 
941,MCOL-4285,MCOL,Daniel Lee,164873,2020-09-01 18:49:51,"Build tested: 1.5.4-1 (drone #496)

Tested in two configurations

1. Single node, ColumnStore tables
   insert, successful
   insert..select, successful
   LDI, successful

2. Single node, InnoDB tables, replicating to ColumnStore table in another server
   insert, successful
   insert..select, successful
   LDI, failed.

LDI on master caused MariaDB on slave crashed and not recoverable

On master:

load data infile '/data/qa/shares/mcol-4285/lineitem.100.tbl' into table lineitem fields terminated by '|';

Slave

/var/log/messages file:

Sep  1 18:33:12 localhost mariadbd: 200901 18:33:12 [ERROR] mysqld got signal 11 ;
Sep  1 18:33:12 localhost mariadbd: This could be because you hit a bug. It is also possible that this binary
Sep  1 18:33:12 localhost mariadbd: or one of the libraries it was linked against is corrupt, improperly built,
Sep  1 18:33:12 localhost mariadbd: or misconfigured. This error can also be caused by malfunctioning hardware.
Sep  1 18:33:12 localhost mariadbd: To report this bug, see https://mariadb.com/kb/en/reporting-bugs
Sep  1 18:33:12 localhost mariadbd: We will try our best to scrape up some info that will hopefully help
Sep  1 18:33:12 localhost mariadbd: diagnose the problem, but since we have already crashed,
Sep  1 18:33:12 localhost mariadbd: something is definitely wrong and this may fail.
Sep  1 18:33:12 localhost mariadbd: Server version: 10.6.0-MariaDB-log
Sep  1 18:33:12 localhost mariadbd: key_buffer_size=134217728
Sep  1 18:33:12 localhost mariadbd: read_buffer_size=131072
Sep  1 18:33:12 localhost mariadbd: max_used_connections=1
Sep  1 18:33:12 localhost mariadbd: max_threads=153
Sep  1 18:33:12 localhost mariadbd: thread_count=5
Sep  1 18:33:12 localhost mariadbd: It is possible that mysqld could use up to
Sep  1 18:33:12 localhost mariadbd: key_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 467576 K  bytes of memory
Sep  1 18:33:12 localhost mariadbd: Hope that's ok; if not, decrease some variables in the equation.
Sep  1 18:33:12 localhost mariadbd: Thread pointer: 0x7f32840087a8
Sep  1 18:33:12 localhost mariadbd: Attempting backtrace. You can use the following information to find out
Sep  1 18:33:12 localhost mariadbd: where mysqld died. If you see no messages after this, something went
Sep  1 18:33:12 localhost mariadbd: terribly wrong...
Sep  1 18:33:12 localhost mariadbd: stack_bottom = 0x7f32a8b963f0 thread_stack 0x49000
Sep  1 18:33:12 localhost mariadbd: ??:0(my_print_stacktrace)[0x55babc0c91ee]
Sep  1 18:33:12 localhost mariadbd: ??:0(handle_fatal_signal)[0x55babbace567]
Sep  1 18:33:12 localhost mariadbd: sigaction.c:0(__restore_rt)[0x7f32c4e3f630]
Sep  1 18:33:12 localhost mariadbd: :0(__strlen_sse2_pminub)[0x7f32c3249691]
Sep  1 18:33:12 localhost mariadbd: ??:0(std::string::assign(char const*))[0x7f32c3868344]
Sep  1 18:33:12 localhost mariadbd: ??:0(ha_mcs_impl_start_bulk_insert(unsigned long long, TABLE*, bool))[0x7f32c028a6e5]
Sep  1 18:33:12 localhost mariadbd: ??:0(ha_mcs::start_bulk_insert_from_cache(unsigned long long, unsigned int))[0x7f32c0283cdd]
Sep  1 18:33:12 localhost mariadbd: ??:0(Rows_log_event::write_row(rpl_group_info*, bool))[0x55babbbe187b]
Sep  1 18:33:12 localhost mariadbd: ??:0(Write_rows_log_event::do_exec_row(rpl_group_info*))[0x55babbbe1cd3]
Sep  1 18:33:12 localhost mariadbd: ??:0(Rows_log_event::do_apply_event(rpl_group_info*))[0x55babbbd750c]
Sep  1 18:33:12 localhost mariadbd: ??:0(non-virtual thunk to Item_string_sys::~Item_string_sys())[0x55babb8222a7]
Sep  1 18:33:12 localhost mariadbd: ??:0(handle_slave_sql)[0x55babb82c47c]
Sep  1 18:33:12 localhost mariadbd: ??:0(MyCTX_nopad::finish(unsigned char*, unsigned int*))[0x55babbd3e03d]
Sep  1 18:33:12 localhost mariadbd: pthread_create.c:0(start_thread)[0x7f32c4e37ea5]
Sep  1 18:33:12 localhost mariadbd: ??:0(__clone)[0x7f32c31d88dd]
Sep  1 18:33:12 localhost mariadbd: Trying to get some variables.
Sep  1 18:33:12 localhost mariadbd: Some pointers may be invalid and cause the dump to abort.
Sep  1 18:33:12 localhost mariadbd: Query (0x0):
Sep  1 18:33:12 localhost mariadbd: Connection ID (thread ID): 6
Sep  1 18:33:12 localhost mariadbd: Status: NOT_KILLED
Sep  1 18:33:12 localhost mariadbd: Optimizer switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,index_merge_sort_intersection=off,engine_condition_pushdown=off,index_condition_pushdown=on,derived_merge=on,derived_with_keys=on,firstmatch=on,loosescan=on,materialization=on,in_to_exists=on,semijoin=on,partial_match_rowid_merge=on,partial_match_table_scan=on,subquery_cache=on,mrr=off,mrr_cost_based=off,mrr_sort_keys=off,outer_join_with_cache=on,semijoin_with_cache=on,join_cache_incremental=on,join_cache_hashed=on,join_cache_bka=on,optimize_join_buffer_size=on,table_elimination=on,extended_keys=on,exists_to_in=on,orderby_uses_equalities=on,condition_pushdown_for_derived=on,split_materialized=on,condition_pushdown_for_subquery=on,rowid_filter=on,condition_pushdown_from_having=on,not_null_range_scan=off
Sep  1 18:33:12 localhost mariadbd: The manual page at https://mariadb.com/kb/en/how-to-produce-a-full-stack-trace-for-mysqld/ contains
Sep  1 18:33:12 localhost mariadbd: information that should help you find out what is causing the crash.
Sep  1 18:33:12 localhost mariadbd: Writing a core file...
Sep  1 18:33:12 localhost mariadbd: Working directory at /var/lib/mysql
Sep  1 18:33:12 localhost mariadbd: Resource Limits:
Sep  1 18:33:12 localhost mariadbd: Limit                     Soft Limit           Hard Limit           Units
Sep  1 18:33:12 localhost mariadbd: Max cpu time              unlimited            unlimited            seconds
Sep  1 18:33:12 localhost mariadbd: Max file size             unlimited            unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max data size             unlimited            unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max stack size            8388608              unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max core file size        0                    unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max resident set          unlimited            unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max processes             23129                23129                processes
Sep  1 18:33:12 localhost mariadbd: Max open files            16384                16384                files
Sep  1 18:33:12 localhost mariadbd: Max locked memory         65536                65536                bytes
Sep  1 18:33:12 localhost mariadbd: Max address space         unlimited            unlimited            bytes
Sep  1 18:33:12 localhost kernel: mariadbd[13932]: segfault at 0 ip 00007f32c3249691 sp 00007f32a8b95b38 error 4 in libc-2.17.so[7f32c30da000+1c3000]
Sep  1 18:33:12 localhost mariadbd: Max file locks            unlimited            unlimited            locks
Sep  1 18:33:12 localhost mariadbd: Max pending signals       23129                23129                signals
Sep  1 18:33:12 localhost mariadbd: Max msgqueue size         819200               819200               bytes
Sep  1 18:33:12 localhost mariadbd: Max nice priority         0                    0
Sep  1 18:33:12 localhost mariadbd: Max realtime priority     0                    0
Sep  1 18:33:12 localhost mariadbd: Max realtime timeout      unlimited            unlimited            us
Sep  1 18:33:12 localhost mariadbd: Core pattern: core
Sep  1 18:33:12 localhost systemd: mariadb.service: main process exited, code=killed, status=11/SEGV
Sep  1 18:33:12 localhost systemd: Unit mariadb.service entered failed state.
Sep  1 18:33:12 localhost systemd: mariadb.service failed.
Sep  1 18:33:17 localhost systemd: mariadb.service holdoff time over, scheduling restart.
Sep  1 18:33:17 localhost systemd: Stopped MariaDB 10.6.0 database server.
Sep  1 18:33:17 localhost systemd: Starting MariaDB 10.6.0 database server...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] /usr/sbin/mariadbd (mysqld 10.6.0-MariaDB-log) starting as process 14304 ...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Warning] Could not increase number of max_open_files to more than 16384 (request: 32186)
Sep  1 18:33:17 localhost mariadbd: 200901 18:33:17 Columnstore: Started; Version: 1.5.4-1
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Using Linux native AIO
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Uses event mutexes
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Compressed tables use zlib 1.2.7
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Number of pools: 1
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Using SSE4.2 crc32 instructions
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] mariadbd: O_TMPFILE is not supported on /tmp (disabling future attempts)
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Initializing buffer pool, total size = 134217728, chunk size = 134217728
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Completed initialization of buffer pool
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Starting crash recovery from checkpoint LSN=97979
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: 128 rollback segments are active.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Creating shared tablespace for temporary tables
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: 10.6.0 started; log sequence number 97991; transaction id 272
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Plugin 'FEEDBACK' is disabled.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Buffer pool(s) load completed at 200901 18:33:17
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Recovering after a crash using master1-bin
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Starting crash recovery...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Crash recovery finished.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Server socket created on IP: '::'.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Reading of all Master_info entries succeeded
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Added new Master_info '' to hash table
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 5 [Note] Slave I/O thread: Start asynchronous replication to master 'replication_user@s1pm1:3306' in log 'master1-bin.000006' at position 30654622
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] /usr/sbin/mariadbd: ready for connections.
Sep  1 18:33:17 localhost mariadbd: Version: '10.6.0-MariaDB-log'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MariaDB Server
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 6 [Note] Slave SQL thread initialized, starting replication in log 'master1-bin.000006' at position 30643828, relay log './master1-relay-bin.000002' position: 31405
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 5 [Note] Slave I/O thread: connected to master 'replication_user@s1pm1:3306',replication started in log 'master1-bin.000006' at position 30654622


Performance timing:

Single server without replication, ColumnStore table

Insert only, end-to-end timing
columnstore_cache_flush_threshold=500000

rowcnt		Disabled(s)		Enabled(s)
100				60				0
1000			600				1
10000							2
100000							14


Insert with replication, Innodb-to-Columnstore
columnstore_cache_flush_threshold=500000

Replication time is from start of insert to end of replication

				Disabled						Enabled
rowcnt		insert  	replication			insert  	replication
100				0			79				   0			2
1000			0			739				   1	        1
10000										   5            13	
100000										   52		    71
1000000										   514          698


columnstore_cache_flush_threshold=1000
100000										   50		    74


negative tests
	a. restart of master
	   restarted master MariaDB server in the middle of 100000 inserts, both master and slave ended with 19773 rows.  matched
	b. restart of slave
	   restarted slave MariaDB server, replication continued and finished correctly

",3,"Build tested: 1.5.4-1 (drone #496)

Tested in two configurations

1. Single node, ColumnStore tables
   insert, successful
   insert..select, successful
   LDI, successful

2. Single node, InnoDB tables, replicating to ColumnStore table in another server
   insert, successful
   insert..select, successful
   LDI, failed.

LDI on master caused MariaDB on slave crashed and not recoverable

On master:

load data infile '/data/qa/shares/mcol-4285/lineitem.100.tbl' into table lineitem fields terminated by '|';

Slave

/var/log/messages file:

Sep  1 18:33:12 localhost mariadbd: 200901 18:33:12 [ERROR] mysqld got signal 11 ;
Sep  1 18:33:12 localhost mariadbd: This could be because you hit a bug. It is also possible that this binary
Sep  1 18:33:12 localhost mariadbd: or one of the libraries it was linked against is corrupt, improperly built,
Sep  1 18:33:12 localhost mariadbd: or misconfigured. This error can also be caused by malfunctioning hardware.
Sep  1 18:33:12 localhost mariadbd: To report this bug, see URL
Sep  1 18:33:12 localhost mariadbd: We will try our best to scrape up some info that will hopefully help
Sep  1 18:33:12 localhost mariadbd: diagnose the problem, but since we have already crashed,
Sep  1 18:33:12 localhost mariadbd: something is definitely wrong and this may fail.
Sep  1 18:33:12 localhost mariadbd: Server version: 10.6.0-MariaDB-log
Sep  1 18:33:12 localhost mariadbd: key_buffer_size=134217728
Sep  1 18:33:12 localhost mariadbd: read_buffer_size=131072
Sep  1 18:33:12 localhost mariadbd: max_used_connections=1
Sep  1 18:33:12 localhost mariadbd: max_threads=153
Sep  1 18:33:12 localhost mariadbd: thread_count=5
Sep  1 18:33:12 localhost mariadbd: It is possible that mysqld could use up to
Sep  1 18:33:12 localhost mariadbd: key_buffer_size + (read_buffer_size + sort_buffer_size)*max_threads = 467576 K  bytes of memory
Sep  1 18:33:12 localhost mariadbd: Hope that's ok; if not, decrease some variables in the equation.
Sep  1 18:33:12 localhost mariadbd: Thread pointer: 0x7f32840087a8
Sep  1 18:33:12 localhost mariadbd: Attempting backtrace. You can use the following information to find out
Sep  1 18:33:12 localhost mariadbd: where mysqld died. If you see no messages after this, something went
Sep  1 18:33:12 localhost mariadbd: terribly wrong...
Sep  1 18:33:12 localhost mariadbd: stack_bottom = 0x7f32a8b963f0 thread_stack 0x49000
Sep  1 18:33:12 localhost mariadbd: ??:0(my_print_stacktrace)[0x55babc0c91ee]
Sep  1 18:33:12 localhost mariadbd: ??:0(handle_fatal_signal)[0x55babbace567]
Sep  1 18:33:12 localhost mariadbd: sigaction.c:0(__restore_rt)[0x7f32c4e3f630]
Sep  1 18:33:12 localhost mariadbd: :0(__strlen_sse2_pminub)[0x7f32c3249691]
Sep  1 18:33:12 localhost mariadbd: ??:0(std::string::assign(char const*))[0x7f32c3868344]
Sep  1 18:33:12 localhost mariadbd: ??:0(ha_mcs_impl_start_bulk_insert(unsigned long long, TABLE*, bool))[0x7f32c028a6e5]
Sep  1 18:33:12 localhost mariadbd: ??:0(ha_mcs::start_bulk_insert_from_cache(unsigned long long, unsigned int))[0x7f32c0283cdd]
Sep  1 18:33:12 localhost mariadbd: ??:0(Rows_log_event::write_row(rpl_group_info*, bool))[0x55babbbe187b]
Sep  1 18:33:12 localhost mariadbd: ??:0(Write_rows_log_event::do_exec_row(rpl_group_info*))[0x55babbbe1cd3]
Sep  1 18:33:12 localhost mariadbd: ??:0(Rows_log_event::do_apply_event(rpl_group_info*))[0x55babbbd750c]
Sep  1 18:33:12 localhost mariadbd: ??:0(non-virtual thunk to Item_string_sys::~Item_string_sys())[0x55babb8222a7]
Sep  1 18:33:12 localhost mariadbd: ??:0(handle_slave_sql)[0x55babb82c47c]
Sep  1 18:33:12 localhost mariadbd: ??:0(MyCTX_nopad::finish(unsigned char*, unsigned int*))[0x55babbd3e03d]
Sep  1 18:33:12 localhost mariadbd: pthread_create.c:0(start_thread)[0x7f32c4e37ea5]
Sep  1 18:33:12 localhost mariadbd: ??:0(__clone)[0x7f32c31d88dd]
Sep  1 18:33:12 localhost mariadbd: Trying to get some variables.
Sep  1 18:33:12 localhost mariadbd: Some pointers may be invalid and cause the dump to abort.
Sep  1 18:33:12 localhost mariadbd: Query (0x0):
Sep  1 18:33:12 localhost mariadbd: Connection ID (thread ID): 6
Sep  1 18:33:12 localhost mariadbd: Status: NOT_KILLED
Sep  1 18:33:12 localhost mariadbd: Optimizer switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_intersection=on,index_merge_sort_intersection=off,engine_condition_pushdown=off,index_condition_pushdown=on,derived_merge=on,derived_with_keys=on,firstmatch=on,loosescan=on,materialization=on,in_to_exists=on,semijoin=on,partial_match_rowid_merge=on,partial_match_table_scan=on,subquery_cache=on,mrr=off,mrr_cost_based=off,mrr_sort_keys=off,outer_join_with_cache=on,semijoin_with_cache=on,join_cache_incremental=on,join_cache_hashed=on,join_cache_bka=on,optimize_join_buffer_size=on,table_elimination=on,extended_keys=on,exists_to_in=on,orderby_uses_equalities=on,condition_pushdown_for_derived=on,split_materialized=on,condition_pushdown_for_subquery=on,rowid_filter=on,condition_pushdown_from_having=on,not_null_range_scan=off
Sep  1 18:33:12 localhost mariadbd: The manual page at URL contains
Sep  1 18:33:12 localhost mariadbd: information that should help you find out what is causing the crash.
Sep  1 18:33:12 localhost mariadbd: Writing a core file...
Sep  1 18:33:12 localhost mariadbd: Working directory at /var/lib/mysql
Sep  1 18:33:12 localhost mariadbd: Resource Limits:
Sep  1 18:33:12 localhost mariadbd: Limit                     Soft Limit           Hard Limit           Units
Sep  1 18:33:12 localhost mariadbd: Max cpu time              unlimited            unlimited            seconds
Sep  1 18:33:12 localhost mariadbd: Max file size             unlimited            unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max data size             unlimited            unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max stack size            8388608              unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max core file size        0                    unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max resident set          unlimited            unlimited            bytes
Sep  1 18:33:12 localhost mariadbd: Max processes             23129                23129                processes
Sep  1 18:33:12 localhost mariadbd: Max open files            16384                16384                files
Sep  1 18:33:12 localhost mariadbd: Max locked memory         65536                65536                bytes
Sep  1 18:33:12 localhost mariadbd: Max address space         unlimited            unlimited            bytes
Sep  1 18:33:12 localhost kernel: mariadbd[13932]: segfault at 0 ip 00007f32c3249691 sp 00007f32a8b95b38 error 4 in libc-2.17.so[7f32c30da000+1c3000]
Sep  1 18:33:12 localhost mariadbd: Max file locks            unlimited            unlimited            locks
Sep  1 18:33:12 localhost mariadbd: Max pending signals       23129                23129                signals
Sep  1 18:33:12 localhost mariadbd: Max msgqueue size         819200               819200               bytes
Sep  1 18:33:12 localhost mariadbd: Max nice priority         0                    0
Sep  1 18:33:12 localhost mariadbd: Max realtime priority     0                    0
Sep  1 18:33:12 localhost mariadbd: Max realtime timeout      unlimited            unlimited            us
Sep  1 18:33:12 localhost mariadbd: Core pattern: core
Sep  1 18:33:12 localhost systemd: mariadb.service: main process exited, code=killed, status=11/SEGV
Sep  1 18:33:12 localhost systemd: Unit mariadb.service entered failed state.
Sep  1 18:33:12 localhost systemd: mariadb.service failed.
Sep  1 18:33:17 localhost systemd: mariadb.service holdoff time over, scheduling restart.
Sep  1 18:33:17 localhost systemd: Stopped MariaDB 10.6.0 database server.
Sep  1 18:33:17 localhost systemd: Starting MariaDB 10.6.0 database server...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] /usr/sbin/mariadbd (mysqld 10.6.0-MariaDB-log) starting as process 14304 ...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Warning] Could not increase number of max_open_files to more than 16384 (request: 32186)
Sep  1 18:33:17 localhost mariadbd: 200901 18:33:17 Columnstore: Started; Version: 1.5.4-1
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Using Linux native AIO
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Uses event mutexes
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Compressed tables use zlib 1.2.7
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Number of pools: 1
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Using SSE4.2 crc32 instructions
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] mariadbd: O_TMPFILE is not supported on /tmp (disabling future attempts)
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Initializing buffer pool, total size = 134217728, chunk size = 134217728
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Completed initialization of buffer pool
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Starting crash recovery from checkpoint LSN=97979
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: 128 rollback segments are active.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Creating shared tablespace for temporary tables
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: 10.6.0 started; log sequence number 97991; transaction id 272
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Plugin 'FEEDBACK' is disabled.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] InnoDB: Buffer pool(s) load completed at 200901 18:33:17
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Recovering after a crash using master1-bin
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Starting crash recovery...
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Crash recovery finished.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Server socket created on IP: '::'.
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Reading of all Master_info entries succeeded
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] Added new Master_info '' to hash table
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 5 [Note] Slave I/O thread: Start asynchronous replication to master 'replication_user@s1pm1:3306' in log 'master1-bin.000006' at position 30654622
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 0 [Note] /usr/sbin/mariadbd: ready for connections.
Sep  1 18:33:17 localhost mariadbd: Version: '10.6.0-MariaDB-log'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MariaDB Server
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 6 [Note] Slave SQL thread initialized, starting replication in log 'master1-bin.000006' at position 30643828, relay log './master1-relay-bin.000002' position: 31405
Sep  1 18:33:17 localhost mariadbd: 2020-09-01 18:33:17 5 [Note] Slave I/O thread: connected to master 'replication_user@s1pm1:3306',replication started in log 'master1-bin.000006' at position 30654622


Performance timing:

Single server without replication, ColumnStore table

Insert only, end-to-end timing
columnstore_cache_flush_threshold=500000

rowcnt		Disabled(s)		Enabled(s)
100				60				0
1000			600				1
10000							2
100000							14


Insert with replication, Innodb-to-Columnstore
columnstore_cache_flush_threshold=500000

Replication time is from start of insert to end of replication

				Disabled						Enabled
rowcnt		insert  	replication			insert  	replication
100				0			79				   0			2
1000			0			739				   1	        1
10000										   5            13	
100000										   52		    71
1000000										   514          698


columnstore_cache_flush_threshold=1000
100000										   50		    74


negative tests
	a. restart of master
	   restarted master MariaDB server in the middle of 100000 inserts, both master and slave ended with 19773 rows.  matched
	b. restart of slave
	   restarted slave MariaDB server, replication continued and finished correctly

"
942,MCOL-4313,MCOL,Roman,169312,2020-10-19 14:55:13,Plz review.,1,Plz review.
943,MCOL-4313,MCOL,Roman,170023,2020-10-26 14:53:10,gcc >= 7 has an internal optimization that converts ::memcpy into a pair of SIMD instructions so we are safe to leverage the optimization.,2,gcc >= 7 has an internal optimization that converts ::memcpy into a pair of SIMD instructions so we are safe to leverage the optimization.
944,MCOL-4319,MCOL,Daniel Lee,176485,2021-01-05 16:55:38,"Build tested: 5.6.1-1 (drone #1437)

The following sections have been added.

/var/lib/columnstore/data/bulk/job/*.xml {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.err {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.log {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
",1,"Build tested: 5.6.1-1 (drone #1437)

The following sections have been added.

/var/lib/columnstore/data/bulk/job/*.xml {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.err {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.log {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
"
945,MCOL-4319,MCOL,Daniel Lee,178262,2021-01-25 21:53:56,"Build verified: 5.5.2 (b1552), develop b1546

The fixed also checked into the develop branch

centos8:root~]# cat /etc/logrotate.d/columnstore 
# 
# MariaDB ColumnStore Log Rotate file that gets installed in /etc/logrotate.d
# as part of the package installation
# 

/var/log/mariadb/columnstore/*.log {
    missingok
    rotate 7
    daily
    dateext
    copytruncate
    olddir /var/log/mariadb/columnstore/archive
    su root root 
}
/etc/columnstore/Columnstore.xml {
    daily
    dateext
    copy
    olddir /etc/columnstore
}
/var/lib/columnstore/data/bulk/job/*.xml {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.err {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.log {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}


",2,"Build verified: 5.5.2 (b1552), develop b1546

The fixed also checked into the develop branch

centos8:root~]# cat /etc/logrotate.d/columnstore 
# 
# MariaDB ColumnStore Log Rotate file that gets installed in /etc/logrotate.d
# as part of the package installation
# 

/var/log/mariadb/columnstore/*.log {
    missingok
    rotate 7
    daily
    dateext
    copytruncate
    olddir /var/log/mariadb/columnstore/archive
    su root root 
}
/etc/columnstore/Columnstore.xml {
    daily
    dateext
    copy
    olddir /etc/columnstore
}
/var/lib/columnstore/data/bulk/job/*.xml {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.err {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}
/var/lib/columnstore/data/bulk/log/*.log {
    missingok
    daily
    rotate 1
    compress
    maxage 30 
    nocreate
}


"
946,MCOL-4337,MCOL,Roman,168405,2020-10-09 09:16:29,Plz review.,1,Plz review.
947,MCOL-4337,MCOL,Roman,168949,2020-10-15 08:10:01,"4QA should be tested with 5.5 release.
To test the feature one should manually start mcs-loadbrm, then controllernode and then with some delay workernode on another console. controlernode should delay its startup waiting for controllernode to come up. There is a new XML setting that tells controllernode how much should it wait for workernodes on startup, namely DBRM_Controller.WorkerConnectionTimeout. Second is the unit. Its default value is 30 seconds.",2,"4QA should be tested with 5.5 release.
To test the feature one should manually start mcs-loadbrm, then controllernode and then with some delay workernode on another console. controlernode should delay its startup waiting for controllernode to come up. There is a new XML setting that tells controllernode how much should it wait for workernodes on startup, namely DBRM_Controller.WorkerConnectionTimeout. Second is the unit. Its default value is 30 seconds."
948,MCOL-4337,MCOL,Daniel Lee,170637,2020-10-30 17:31:41,"Builds tested: 5.5.1-1
Drone #1013, branch develop-1.5
Drone #1017, branch develop

Tested on both centos 8 and ubuntu18.04

1. I stopped mcs-controllernode and Macs-workernode@1 services
2. started mcs-loadbrm service
3. started mcs-controllernode service.  It did not wait for workernode@1 service to start.  mcs-controllernode service started in about 7 seconds.

In stead of using systemctl to start services, I did another round of tests by running load_brm and controller node in /usr/bin directly.  controllernode started immediately.

I noticed there is no DBRM_Controller:WorkerConnectionTiimeout entry in the Columnstore.xml file.  I added the entry with a value of 30 and did another round of tests, controllernode still did not wait for workernode to start.
 ",3,"Builds tested: 5.5.1-1
Drone #1013, branch develop-1.5
Drone #1017, branch develop

Tested on both centos 8 and ubuntu18.04

1. I stopped mcs-controllernode and Macs-workernode@1 services
2. started mcs-loadbrm service
3. started mcs-controllernode service.  It did not wait for workernode@1 service to start.  mcs-controllernode service started in about 7 seconds.

In stead of using systemctl to start services, I did another round of tests by running load_brm and controller node in /usr/bin directly.  controllernode started immediately.

I noticed there is no DBRM_Controller:WorkerConnectionTiimeout entry in the Columnstore.xml file.  I added the entry with a value of 30 and did another round of tests, controllernode still did not wait for workernode to start.
 "
949,MCOL-4337,MCOL,Roman,171650,2020-11-10 08:19:43,[~toddstoffel] I would like to suggest to change the mark to stability or something similar.,4,[~toddstoffel] I would like to suggest to change the mark to stability or something similar.
950,MCOL-4337,MCOL,Roman,171652,2020-11-10 08:24:12,"[~dleeyh] I need to confess that my test scenario is totally misguiding.
The patch itself enables controllernode to establish connections with its workernodes on startup. Before the patch controllernode did lazy connections establishing them when a new request pops up at controllernode.
So to test the patch you should follow the scenario to took:
1. Stop mcs-controllernode and Macs-workernode@1 services
2. Start mcs-loadbrm service
3. Start mcs-controllernode service. At this point it should complain once into /var/log/mariadb/columnstore/error.log about the workernode it can't connect with. 
4. Start mcs-workernode service. At this point controllernode should log about the established connection.

JFYI The test you tried will work after MCOL-4170 that is on its last stages.",5,"[~dleeyh] I need to confess that my test scenario is totally misguiding.
The patch itself enables controllernode to establish connections with its workernodes on startup. Before the patch controllernode did lazy connections establishing them when a new request pops up at controllernode.
So to test the patch you should follow the scenario to took:
1. Stop mcs-controllernode and Macs-workernode@1 services
2. Start mcs-loadbrm service
3. Start mcs-controllernode service. At this point it should complain once into /var/log/mariadb/columnstore/error.log about the workernode it can't connect with. 
4. Start mcs-workernode service. At this point controllernode should log about the established connection.

JFYI The test you tried will work after MCOL-4170 that is on its last stages."
951,MCOL-4337,MCOL,Daniel Lee,171729,2020-11-10 22:12:14,"Build tested: 5.5.1-1 (Drone)

engine: 1ffca618dfba15d1edda4b21a2d9f9713d4f7262
server: 10b2d5726fa21675362596ff4f52f2eca748bdc9
buildNo: 1097

Few issues

1. The error msg is in the warning.log file, not err.log file

Nov 10 22:00:15 centos-8 controllernode[4810]: 15.905223 |0|0|0| D 29 CAL0000: DBRM Controller: Connected to DBRM_Worker1

2. After both controller and worker started, the cluster is in a system-not-ready state

I logged into the MySQL client after both services were started

MariaDB [mytest]> select count(*) from lineitem;
ERROR 1815 (HY000): Internal error: The system is not yet ready to accept queries

[centos8:root~]# systemctl status mariadb
● mariadb.service - MariaDB 10.5.8 database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─migrated-from-my.cnf-settings.conf
   Active: active (running) since Tue 2020-11-10 21:53:40 UTC; 7min ago
     Docs: man:mariadbd(8)
           https://mariadb.com/kb/en/library/systemd/
 Main PID: 3920 (mariadbd)
   Status: ""Taking your SQL requests now...""
    Tasks: 12 (limit: 50823)
   Memory: 799.2M
   CGroup: /system.slice/mariadb.service
           └─3920 /usr/sbin/mariadbd

Nov 10 21:53:40 centos-8 mariadbd[3920]: 2020-11-10 21:53:40 0 [Note] Added new Master_info '' to hash table
Nov 10 21:53:40 centos-8 mariadbd[3920]: 2020-11-10 21:53:40 0 [Note] /usr/sbin/mariadbd: ready for connections.
Nov 10 21:53:40 centos-8 mariadbd[3920]: Version: '10.5.8-MariaDB'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MariaDB Server
Nov 10 21:53:40 centos-8 systemd[1]: Started MariaDB 10.5.8 database server.
Nov 10 21:54:40 centos-8 dbcon[3920]: 40.145388 |4|0|0| D 24 CAL0001: Start SQL statement: load data infile ""/data/qa/source/dbt3/1g/lineitem.tbl"" in>
Nov 10 21:55:58 centos-8 writeenginesplit[4425]: 58.648139 |0|0|0| I 33 CAL0000: Send EOD message to All PMs
Nov 10 21:55:59 centos-8 writeenginesplit[4425]: 59.023209 |0|0|0| I 33 CAL0098: Received a Cpimport Pass from PM1.
Nov 10 21:55:59 centos-8 writeenginesplit[4425]: 59.024888 |0|0|0| I 33 CAL0000: Released Table Lock
Nov 10 21:55:59 centos-8 dbcon[3920]: 59.935069 |4|0|0| D 24 CAL0001: End SQL statement
Nov 10 22:01:16 centos-8 mariadbd[3920]: DBRM::send_recv: controller node closed the connection

restarting the mariadb service did not resolve the issue, but restarting mariadb-columnstore did.
",6,"Build tested: 5.5.1-1 (Drone)

engine: 1ffca618dfba15d1edda4b21a2d9f9713d4f7262
server: 10b2d5726fa21675362596ff4f52f2eca748bdc9
buildNo: 1097

Few issues

1. The error msg is in the warning.log file, not err.log file

Nov 10 22:00:15 centos-8 controllernode[4810]: 15.905223 |0|0|0| D 29 CAL0000: DBRM Controller: Connected to DBRM_Worker1

2. After both controller and worker started, the cluster is in a system-not-ready state

I logged into the MySQL client after both services were started

MariaDB [mytest]> select count(*) from lineitem;
ERROR 1815 (HY000): Internal error: The system is not yet ready to accept queries

[centos8:root~]# systemctl status mariadb
● mariadb.service - MariaDB 10.5.8 database server
   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/mariadb.service.d
           └─migrated-from-my.cnf-settings.conf
   Active: active (running) since Tue 2020-11-10 21:53:40 UTC; 7min ago
     Docs: man:mariadbd(8)
           URL
 Main PID: 3920 (mariadbd)
   Status: ""Taking your SQL requests now...""
    Tasks: 12 (limit: 50823)
   Memory: 799.2M
   CGroup: /system.slice/mariadb.service
           └─3920 /usr/sbin/mariadbd

Nov 10 21:53:40 centos-8 mariadbd[3920]: 2020-11-10 21:53:40 0 [Note] Added new Master_info '' to hash table
Nov 10 21:53:40 centos-8 mariadbd[3920]: 2020-11-10 21:53:40 0 [Note] /usr/sbin/mariadbd: ready for connections.
Nov 10 21:53:40 centos-8 mariadbd[3920]: Version: '10.5.8-MariaDB'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MariaDB Server
Nov 10 21:53:40 centos-8 systemd[1]: Started MariaDB 10.5.8 database server.
Nov 10 21:54:40 centos-8 dbcon[3920]: 40.145388 |4|0|0| D 24 CAL0001: Start SQL statement: load data infile ""/data/qa/source/dbt3/1g/lineitem.tbl"" in>
Nov 10 21:55:58 centos-8 writeenginesplit[4425]: 58.648139 |0|0|0| I 33 CAL0000: Send EOD message to All PMs
Nov 10 21:55:59 centos-8 writeenginesplit[4425]: 59.023209 |0|0|0| I 33 CAL0098: Received a Cpimport Pass from PM1.
Nov 10 21:55:59 centos-8 writeenginesplit[4425]: 59.024888 |0|0|0| I 33 CAL0000: Released Table Lock
Nov 10 21:55:59 centos-8 dbcon[3920]: 59.935069 |4|0|0| D 24 CAL0001: End SQL statement
Nov 10 22:01:16 centos-8 mariadbd[3920]: DBRM::send_recv: controller node closed the connection

restarting the mariadb service did not resolve the issue, but restarting mariadb-columnstore did.
"
952,MCOL-4337,MCOL,Roman,171812,2020-11-11 15:42:15,[~dleeyh] I need the exact steps you took b/c my list of actions didn't include starting or testing of the whole cluster so I'm unaware of what you are exactly doing.,7,[~dleeyh] I need the exact steps you took b/c my list of actions didn't include starting or testing of the whole cluster so I'm unaware of what you are exactly doing.
953,MCOL-4337,MCOL,Daniel Lee,171814,2020-11-11 15:47:22,"I did the steps you specified and tried to run a query.  The cluster was in ""not ready"" state.

Your change maybe doing what you want to do specifically, but you need to ensure the cluster is not broken.
",8,"I did the steps you specified and tried to run a query.  The cluster was in ""not ready"" state.

Your change maybe doing what you want to do specifically, but you need to ensure the cluster is not broken.
"
954,MCOL-4337,MCOL,Roman,171817,2020-11-11 15:55:01,"How did you start the cluster?


",9,"How did you start the cluster?


"
955,MCOL-4337,MCOL,Daniel Lee,171819,2020-11-11 16:00:11,"The ""not ready"" issue occurred right after I performed the steps you specified.  No start or restart was performed.

I did restart the cluster later, attempting to recover the cluster
It was a single node

systemctl restart mariadb
systemctl restart mariadb-columnstore
",10,"The ""not ready"" issue occurred right after I performed the steps you specified.  No start or restart was performed.

I did restart the cluster later, attempting to recover the cluster
It was a single node

systemctl restart mariadb
systemctl restart mariadb-columnstore
"
956,MCOL-4363,MCOL,Jose Rojas,185011,2021-04-06 22:01:32,"PR not linking to JIRA automatically.
https://github.com/mariadb-corporation/mariadb-columnstore-cmapi/pull/115",1,"PR not linking to JIRA automatically.
URL"
957,MCOL-4363,MCOL,Roman,189264,2021-05-12 16:35:20,4QA. As of now one doesn't need to manually copy /etc/columnstore/storagemanager.cnf to nodes added to the cluster b/c CMAPI now copies the configuration file adding a node. This doesn't work for the first node added though.,2,4QA. As of now one doesn't need to manually copy /etc/columnstore/storagemanager.cnf to nodes added to the cluster b/c CMAPI now copies the configuration file adding a node. This doesn't work for the first node added though.
958,MCOL-4363,MCOL,Daniel Lee,189293,2021-05-12 22:25:26,"Build tested: 
Engine: 5.6.1 ( Drone #2359 )
CMPAI: 0.5.0 ( Drone #476 )

storagemanager.cnf still not being pushed.  There is no PR info the in this Jira ticket. 

Is it a git and Jira integration issue (as it happens sometimes)? 
Is it merged yet? 
Is it a bug?

",3,"Build tested: 
Engine: 5.6.1 ( Drone #2359 )
CMPAI: 0.5.0 ( Drone #476 )

storagemanager.cnf still not being pushed.  There is no PR info the in this Jira ticket. 

Is it a git and Jira integration issue (as it happens sometimes)? 
Is it merged yet? 
Is it a bug?

"
959,MCOL-4363,MCOL,Daniel Lee,189452,2021-05-14 15:54:47,Build verified: cmapi 0.5.0 ( Drone #477 ),4,Build verified: cmapi 0.5.0 ( Drone #477 )
960,MCOL-4387,MCOL,Roman,172025,2020-11-13 13:01:17,Plz review.,1,Plz review.
961,MCOL-4394,MCOL,Roman,172146,2020-11-16 12:40:48,Plz review.,1,Plz review.
962,MCOL-4398,MCOL,Roman,176838,2021-01-11 07:59:13,The tests are in future/mcol641-create.test,1,The tests are in future/mcol641-create.test
963,MCOL-4406,MCOL,Roman Navrotskiy,200239,2021-09-22 18:07:22,fixed via https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/2059,1,fixed via URL
964,MCOL-4409,MCOL,Roman,173202,2020-11-25 17:03:13,Plz review.,1,Plz review.
965,MCOL-444,MCOL,David Hill,89242,2016-12-06 18:26:12,InfiniDB Issue #10684,1,InfiniDB Issue #10684
966,MCOL-444,MCOL,Andrew Hutchings,103487,2017-11-21 16:42:16,"Can you please let me know what parts are this bug? Also the ""show create table"" for flights?

With a suitable schema the only issue I can see at the moment is char_length() and substr() don't appear to work for UTF8.

For the cpimport truncation problem, I'm assuming this is due to a schema that doesn't have a column wide enough? If so this is non-trivial to fix. We need to modify the system catalog to support UTF8 and then add full UTF8 support to ColumnStore. This becomes a major feature (a very large piece of work) and should probably be separate.",2,"Can you please let me know what parts are this bug? Also the ""show create table"" for flights?

With a suitable schema the only issue I can see at the moment is char_length() and substr() don't appear to work for UTF8.

For the cpimport truncation problem, I'm assuming this is due to a schema that doesn't have a column wide enough? If so this is non-trivial to fix. We need to modify the system catalog to support UTF8 and then add full UTF8 support to ColumnStore. This becomes a major feature (a very large piece of work) and should probably be separate."
967,MCOL-444,MCOL,Andrew Hutchings,103717,2017-11-28 12:25:42,"So, at first I thought this was a problem with char_length() and substr() internally since I couldn't get it to work when the column size was in fact wide enough. It turns out this was due to me not setting SystemLang in Columnstore.xml.

The problem is two-fold:

1. cpimport truncates at the byte level instead of the character level which means for one multi-byte character only part of the character is stored
and
2. we use mbstowcs() internally to do the multibyte character counting in char_length() and substr() which will return a zero length if any part of the data is invalid (such as a truncation in problem 1)

The first problem is solvable by having cpimport use the multibyte string length functions in funcexp to calculate where to truncate. The second problem requires a new multibyte handling library.

For this bug I will attempt to solve #1 but #2 is a much bigger feature that is already covered by other tickets such as MCOL-337.",3,"So, at first I thought this was a problem with char_length() and substr() internally since I couldn't get it to work when the column size was in fact wide enough. It turns out this was due to me not setting SystemLang in Columnstore.xml.

The problem is two-fold:

1. cpimport truncates at the byte level instead of the character level which means for one multi-byte character only part of the character is stored
and
2. we use mbstowcs() internally to do the multibyte character counting in char_length() and substr() which will return a zero length if any part of the data is invalid (such as a truncation in problem 1)

The first problem is solvable by having cpimport use the multibyte string length functions in funcexp to calculate where to truncate. The second problem requires a new multibyte handling library.

For this bug I will attempt to solve #1 but #2 is a much bigger feature that is already covered by other tickets such as MCOL-337."
968,MCOL-444,MCOL,Andrew Hutchings,103775,2017-11-29 10:53:49,"Patch makes cpimport find the correct place to truncate UTF8 data when required. This is a short to medium term hotfix until we can support the full set of MariaDB charsets.

Patch is for 1.0, will merge up through 1.1 later.

For QA:
Difficult to explain the test as Jira doesn't handle UTF8 data well. First you need to enable UTF8 (as described in our docs, the Columnstore.xml change is the most important). Then you need to create a table with a varchar(100) column and cpimport the UTF8 data represented by this hex:
5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E2809320E2809C6F6365616EE2809D2C20E2809C7761746572E2809D2C20E2809C76696577E2809D206F7220E2809C6573706C616E61646522

You can convert this to UTF8 text to copy/paste into a file using:
https://sites.google.com/site/nathanlexwww/tools/utf8-convert

You then need to run char_length() on it and you should see a length value rather than 0 before the patch.",4,"Patch makes cpimport find the correct place to truncate UTF8 data when required. This is a short to medium term hotfix until we can support the full set of MariaDB charsets.

Patch is for 1.0, will merge up through 1.1 later.

For QA:
Difficult to explain the test as Jira doesn't handle UTF8 data well. First you need to enable UTF8 (as described in our docs, the Columnstore.xml change is the most important). Then you need to create a table with a varchar(100) column and cpimport the UTF8 data represented by this hex:
5265746172676574696E67202D20496E7465726E616C2052756E206F662053697465202D205365617263686573207468617420636F6E7461696E206174206C65617374206F6E65206F662074686520666F6C6C6F77696E67206B6579776F72647320E2809320E2809C6F6365616EE2809D2C20E2809C7761746572E2809D2C20E2809C76696577E2809D206F7220E2809C6573706C616E61646522

You can convert this to UTF8 text to copy/paste into a file using:
URL

You then need to run char_length() on it and you should see a length value rather than 0 before the patch."
969,MCOL-444,MCOL,Daniel Lee,104173,2017-12-06 17:56:58,"Builds verified: GitHub source

1.0.12-1

[root@localhost ~]# cat mariadb-columnstore-1.0.12-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 25e9d054cd3d05683fade1b974e1730316d256ed
Merge: 89b2ea1 7c52a83
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Nov 21 10:49:11 2017 -0600

    Merge pull request #79 from mariadb-corporation/MCOL-954-1.0
    
    MCOL-954 Init vtable state

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b112e826a2793228f5f3c1312fec5291fc1d8bf5
Merge: 7c2640f b657938
Author: David.Hall <david.hall@mariadb.com>
Date:   Fri Dec 1 16:17:28 2017 -0600

    Merge pull request #338 from mariadb-corporation/MCOL-1068
    
    MCOL-1068 Improve compression_ratio() procedure


1.1.3-1

/root/columnstore/mariadb-columnstore-server
commit 632e265687674fb66bd1d704bc18032b00dd6b17
Merge: 5e9fe52 200f5be
Author: david hill <david.hill@mariadb.com>
Date:   Tue Nov 21 15:22:06 2017 -0600

    Merge branch 'develop-1.1' of https://github.com/mariadb-corporation/mariadb-columnstore-server into develop-1.1

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 4d8026618cfb5377c9a200170848092ce5660f10
Author: david hill <david.hill@mariadb.com>
Date:   Wed Nov 29 09:36:24 2017 -0600

    change how the os_detect is run on remote nodes

Verified mentioned test case.  Character string was truncated at 98, instead of 100.
",5,"Builds verified: GitHub source

1.0.12-1

[root@localhost ~]# cat mariadb-columnstore-1.0.12-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 25e9d054cd3d05683fade1b974e1730316d256ed
Merge: 89b2ea1 7c52a83
Author: David.Hall 
Date:   Tue Nov 21 10:49:11 2017 -0600

    Merge pull request #79 from mariadb-corporation/MCOL-954-1.0
    
    MCOL-954 Init vtable state

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b112e826a2793228f5f3c1312fec5291fc1d8bf5
Merge: 7c2640f b657938
Author: David.Hall 
Date:   Fri Dec 1 16:17:28 2017 -0600

    Merge pull request #338 from mariadb-corporation/MCOL-1068
    
    MCOL-1068 Improve compression_ratio() procedure


1.1.3-1

/root/columnstore/mariadb-columnstore-server
commit 632e265687674fb66bd1d704bc18032b00dd6b17
Merge: 5e9fe52 200f5be
Author: david hill 
Date:   Tue Nov 21 15:22:06 2017 -0600

    Merge branch 'develop-1.1' of URL into develop-1.1

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 4d8026618cfb5377c9a200170848092ce5660f10
Author: david hill 
Date:   Wed Nov 29 09:36:24 2017 -0600

    change how the os_detect is run on remote nodes

Verified mentioned test case.  Character string was truncated at 98, instead of 100.
"
970,MCOL-445,MCOL,David Hill,89243,2016-12-06 18:29:08,InfiniDB Issue #10420,1,InfiniDB Issue #10420
971,MCOL-445,MCOL,David Thompson,103676,2017-11-27 17:50:17,[~ben.thompson] can you evaluate this for effort?,2,[~ben.thompson] can you evaluate this for effort?
972,MCOL-445,MCOL,Ben Thompson,104121,2017-12-05 23:15:31,getConfig and setConfig should be case insensitive on variable names and section names,3,getConfig and setConfig should be case insensitive on variable names and section names
973,MCOL-445,MCOL,Daniel Lee,104474,2017-12-13 23:15:00,"Build verified: 1.0.12-1

Verify the getConfig and setConfig commands.  Section and parameter names no longer needs to be case sensitive.  Values being assign/changed will be as the user provided.
",4,"Build verified: 1.0.12-1

Verify the getConfig and setConfig commands.  Section and parameter names no longer needs to be case sensitive.  Values being assign/changed will be as the user provided.
"
974,MCOL-4452,MCOL,Roman,175940,2020-12-22 13:13:37,The problem is caused by the fact that RowAggregationUMP2::doUDAF() uses fRGContext and not udafContextsColl[funcColsIdx].,1,The problem is caused by the fact that RowAggregationUMP2::doUDAF() uses fRGContext and not udafContextsColl[funcColsIdx].
975,MCOL-446,MCOL,David Hill,89244,2016-12-06 18:31:47,For InfiniDB Issue #10267,1,For InfiniDB Issue #10267
976,MCOL-446,MCOL,David Hill,104117,2017-12-05 20:09:47,"Actually Ben just needs to see this this functionality works... This is a file that was intended for the customer to update where they can define what variables gets applied in my.cnf during an upgrade process. This was done for InfiniDB, just need to make sure the functionality still works and then we will need to get this documented.",2,"Actually Ben just needs to see this this functionality works... This is a file that was intended for the customer to update where they can define what variables gets applied in my.cnf during an upgrade process. This was done for InfiniDB, just need to make sure the functionality still works and then we will need to get this documented."
977,MCOL-446,MCOL,Ben Thompson,104122,2017-12-05 23:19:42,"Should only find strings in myCnf-include-args.text if it is preceeded by nothing but 'spaces' or '#' and followed by 'spaces' or '=' and will still update if a different setting is commented out earlier in the file.

example: 
\# tmpdir = /tmp
tmpdir = /different/tmp

the value will still be preserved in upgrade.",3,"Should only find strings in myCnf-include-args.text if it is preceeded by nothing but 'spaces' or '#' and followed by 'spaces' or '=' and will still update if a different setting is commented out earlier in the file.

example: 
\# tmpdir = /tmp
tmpdir = /different/tmp

the value will still be preserved in upgrade."
978,MCOL-446,MCOL,Daniel Lee,104184,2017-12-07 00:00:45,"Builds verified: GitHub source

1.0.12-1

/root/columnstore/mariadb-columnstore-server
commit 7ec285d104a8e68320cbf14b44ee8509693fbda2
Merge: 25e9d05 f546eaf
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Dec 6 10:04:56 2017 -0600

    Merge pull request #81 from mariadb-corporation/MCOL-1082-1.0
    
    MCOL-1082 Preserve row_count through vtable

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b295c8fc0f827546e81b4e50135eb8dceb2a3fb9
Merge: 22cb352 26f7344
Author: david hill <david.hill@mariadb.com>
Date:   Wed Dec 6 11:02:56 2017 -0600

    Merge pull request #340 from mariadb-corporation/MCOL-445
    
    MCOL-445: Modify getConfig and setConfig to be case insensitive on va…



1.1.3-1

/root/columnstore/mariadb-columnstore-server
commit 0b3b26032aa60d2937cd06535946d7d8575cd4fd
Merge: 632e265 101ea14
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Dec 6 10:05:32 2017 -0600

    Merge pull request #80 from mariadb-corporation/MCOL-1082
    
    MCOL-1082 Preserve row_count through vtable

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 71a901e8b38bca584ece8d7b070ad263c4b65c3f
Merge: 4d80266 addd719
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Dec 6 11:17:49 2017 -0600

    Merge pull request #341 from mariadb-corporation/MCOL-1083
    
    MCOL-1083 Fix NULL row init for TEXT/BLOB

Verified that postConfigure handled the myCnf-include-args.text file during upgrade.
",4,"Builds verified: GitHub source

1.0.12-1

/root/columnstore/mariadb-columnstore-server
commit 7ec285d104a8e68320cbf14b44ee8509693fbda2
Merge: 25e9d05 f546eaf
Author: David.Hall 
Date:   Wed Dec 6 10:04:56 2017 -0600

    Merge pull request #81 from mariadb-corporation/MCOL-1082-1.0
    
    MCOL-1082 Preserve row_count through vtable

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b295c8fc0f827546e81b4e50135eb8dceb2a3fb9
Merge: 22cb352 26f7344
Author: david hill 
Date:   Wed Dec 6 11:02:56 2017 -0600

    Merge pull request #340 from mariadb-corporation/MCOL-445
    
    MCOL-445: Modify getConfig and setConfig to be case insensitive on va…



1.1.3-1

/root/columnstore/mariadb-columnstore-server
commit 0b3b26032aa60d2937cd06535946d7d8575cd4fd
Merge: 632e265 101ea14
Author: David.Hall 
Date:   Wed Dec 6 10:05:32 2017 -0600

    Merge pull request #80 from mariadb-corporation/MCOL-1082
    
    MCOL-1082 Preserve row_count through vtable

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 71a901e8b38bca584ece8d7b070ad263c4b65c3f
Merge: 4d80266 addd719
Author: David.Hall 
Date:   Wed Dec 6 11:17:49 2017 -0600

    Merge pull request #341 from mariadb-corporation/MCOL-1083
    
    MCOL-1083 Fix NULL row init for TEXT/BLOB

Verified that postConfigure handled the myCnf-include-args.text file during upgrade.
"
979,MCOL-4478,MCOL,Roman,176199,2020-12-30 10:22:53,Plz review.,1,Plz review.
980,MCOL-4479,MCOL,Roman,176204,2020-12-30 12:38:38,Plz review.,1,Plz review.
981,MCOL-4513,MCOL,Denis Khalikov,185789,2021-04-13 15:43:53,"Currently CS use following steps to apply join for tables:
1. Staring from the largest table (largest by rows number);
2. join:
2.1. inner join:
`joinToLargeTable` walk down starting from the root (largest table) using basic DFS and applying join from the bottom up to ""parent"" node. So this is why CS has check for spanning tree.
2..2 left, right. outer join
`joinTablesInOrder` sorts joins in order `join order` then apply join.

How to add support of circular joins.
1. Walk on graph, find all cycles.
2. Transform edges which makes cycle to filter (need a rule to decide which edge to transform to filter which not), probably we can use something like
` addJoinFilte` to add filter, but not sure currently, need more expertise in this part.


",1,"Currently CS use following steps to apply join for tables:
1. Staring from the largest table (largest by rows number);
2. join:
2.1. inner join:
`joinToLargeTable` walk down starting from the root (largest table) using basic DFS and applying join from the bottom up to ""parent"" node. So this is why CS has check for spanning tree.
2..2 left, right. outer join
`joinTablesInOrder` sorts joins in order `join order` then apply join.

How to add support of circular joins.
1. Walk on graph, find all cycles.
2. Transform edges which makes cycle to filter (need a rule to decide which edge to transform to filter which not), probably we can use something like
` addJoinFilte` to add filter, but not sure currently, need more expertise in this part.


"
982,MCOL-4520,MCOL,Roman Navrotskiy,178775,2021-02-01 11:48:16,fixed via https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1741,1,fixed via URL
983,MCOL-4525,MCOL,Roman,181489,2021-03-04 04:10:27,"Te linked issue MCOL-4574 is the mandatory prerequisite.
As of now MCS can't fallback to table API execution b/c it changes SELECT LEX applying some rewrites mentioned in the linked issue.
When the issue is done MCS won't change SELECT LEX so it will be possible to alter the way MCS reacts on select handler runtime errors thus allows MCS to switch to table API if Select handler execution fails.
",1,"Te linked issue MCOL-4574 is the mandatory prerequisite.
As of now MCS can't fallback to table API execution b/c it changes SELECT LEX applying some rewrites mentioned in the linked issue.
When the issue is done MCS won't change SELECT LEX so it will be possible to alter the way MCS reacts on select handler runtime errors thus allows MCS to switch to table API if Select handler execution fails.
"
984,MCOL-4525,MCOL,Gagan Goel,191829,2021-06-14 08:56:44,"For QA:

This feature adds a new option to the {{columnstore_select_handler}} session variable: AUTO. Default value of this variable is ON. If the variable is set to AUTO, ColumnStore first attempts to execute the query, if the query execution inside ColumnStore fails, we fallback and re-execution of the query is performed by the server using the handler API. I share below a scenario where a query execution in CS would fail due to an unsupported feature (cartesian joins). For testing this feature:
  1. Please come up with additional test case scenarios that are know to fail in ColumnStore with {{columnstore_select_handler=ON}}.
  2. Re-execute the test case from 1. again but this time setting {{columnstore_select_handler=AUTO}}.

Here is an example:

{code:sql}
MariaDB [test]> set default_storage_engine=columnstore;
Query OK, 0 rows affected (0.000 sec)

MariaDB [test]> create table t1 (a int);
Query OK, 0 rows affected (0.376 sec)

MariaDB [test]> create table t2 (a int);
Query OK, 0 rows affected (0.372 sec)

MariaDB [test]> insert into t1 values (1), (2);
Query OK, 2 rows affected (0.239 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> insert into t2 values (1), (2);
Query OK, 2 rows affected (0.200 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> set columnstore_select_handler=ON;
Query OK, 0 rows affected (0.000 sec)

MariaDB [test]> select * from t1,t2;
ERROR 1815 (HY000): Internal error: IDB-1000: 't1' and 't2' are not joined.
MariaDB [test]> set columnstore_select_handler=AUTO;
Query OK, 0 rows affected (0.000 sec)

MariaDB [test]> select * from t1,t2;
+------+------+
| a    | a    |
+------+------+
|    1 |    1 |
|    2 |    1 |
|    1 |    2 |
|    2 |    2 |
+------+------+
4 rows in set, 1 warning (0.064 sec)

MariaDB [test]> show warnings;
+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------+
| Level   | Code | Message                                                                                                                                    |
+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------+
| Warning | 9999 | MCS select_handler execution failed due to: 1815: Internal error: IDB-1000: 't1' and 't2' are not joined. Falling back to server execution |
+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.000 sec)
{code}",2,"For QA:

This feature adds a new option to the {{columnstore_select_handler}} session variable: AUTO. Default value of this variable is ON. If the variable is set to AUTO, ColumnStore first attempts to execute the query, if the query execution inside ColumnStore fails, we fallback and re-execution of the query is performed by the server using the handler API. I share below a scenario where a query execution in CS would fail due to an unsupported feature (cartesian joins). For testing this feature:
  1. Please come up with additional test case scenarios that are know to fail in ColumnStore with {{columnstore_select_handler=ON}}.
  2. Re-execute the test case from 1. again but this time setting {{columnstore_select_handler=AUTO}}.

Here is an example:

{code:sql}
MariaDB [test]> set default_storage_engine=columnstore;
Query OK, 0 rows affected (0.000 sec)

MariaDB [test]> create table t1 (a int);
Query OK, 0 rows affected (0.376 sec)

MariaDB [test]> create table t2 (a int);
Query OK, 0 rows affected (0.372 sec)

MariaDB [test]> insert into t1 values (1), (2);
Query OK, 2 rows affected (0.239 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> insert into t2 values (1), (2);
Query OK, 2 rows affected (0.200 sec)
Records: 2  Duplicates: 0  Warnings: 0

MariaDB [test]> set columnstore_select_handler=ON;
Query OK, 0 rows affected (0.000 sec)

MariaDB [test]> select * from t1,t2;
ERROR 1815 (HY000): Internal error: IDB-1000: 't1' and 't2' are not joined.
MariaDB [test]> set columnstore_select_handler=AUTO;
Query OK, 0 rows affected (0.000 sec)

MariaDB [test]> select * from t1,t2;
+------+------+
| a    | a    |
+------+------+
|    1 |    1 |
|    2 |    1 |
|    1 |    2 |
|    2 |    2 |
+------+------+
4 rows in set, 1 warning (0.064 sec)

MariaDB [test]> show warnings;
+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------+
| Level   | Code | Message                                                                                                                                    |
+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------+
| Warning | 9999 | MCS select_handler execution failed due to: 1815: Internal error: IDB-1000: 't1' and 't2' are not joined. Falling back to server execution |
+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.000 sec)
{code}"
985,MCOL-4525,MCOL,Daniel Lee,192006,2021-06-15 20:42:24,"Build verified: 6.1.1 ( Drone #2586)
",3,"Build verified: 6.1.1 ( Drone #2586)
"
986,MCOL-4528,MCOL,David Hall,179328,2021-02-08 22:17:45,The analysis can be found here: https://docs.google.com/spreadsheets/d/1RtqIovT1kmU2G5iEmYBYF6O4CQRraeN_j0g5lrQBAIM/edit#gid=0,1,The analysis can be found here: URL
987,MCOL-4529,MCOL,Sergey Zefirov,179120,2021-02-04 15:48:38,"Roman proposed to split extent map by column width and I would like to piggy back that idea.

I would like to propose to keep ranges in maps indexed by the the length.

Right now the structure as follows:

{code:c++}
struct EMCasualPartition_struct
{
    int32_t sequenceNum;
    char isValid; //CP_INVALID - No min/max and no DML in progress. CP_UPDATING - Update in progress. CP_VALID- min/max is valid
    union
    {
        int128_t bigLoVal; // These need to be reinterpreted as unsigned for uint64_t/uint128_t column types.
        int64_t loVal;
    };
    union
    {
        int128_t bigHiVal;
        int64_t hiVal;
    };
    // some constructors.
};
{code}

What I propose is the following:

{code:c++}
dstruct EMCasualPartition_struct
{
    int32_t sequenceNum;
    char isValid; //CP_INVALID - No min/max and no DML in progress. CP_UPDATING - Update in progress. CP_VALID- min/max is valid
    uint8_t keyLen; // up to 256 bytes
    uint8_t minMax[]; // memcmp-friendly (big-endian for integers) encoding of min and max prefixes.
                                        // The array is twice the keyLen field long, first goes min, then max.

    // here we need methods to compute record size, get the min/max keys, etc.
};
{code}

And extentmap will now contains several ""extentarrays"", one for each key length.

It is easy to define maximum and minimum values for integers. Floating point numbers also can be handled, possible with one exception of negative zero.

For strings of arbitrary length, the minimum and maximum can be defined as follows:
# if string is not longer than key length, we extend the string with zeroes up to key length and this extended string forms minimum and maximum value to be added to a range. For string ""ab"" and key length 4 we will have minimum and maximum both equal to ""ab\0\0"".
# if string is longer than key length we cut it up to to key length. For string ""abcde"" and key length 4 the minimum and maximum are ""abcd"".

Please note that we consider minimum prefix to be extended to the right with zeroes infinitely and maximum to be extended to the right with the 0xff infinitely. That makes comparison of key length prefixes of strings with maximums and minimums valid: minimum is equal or less than any string in the range and maximum is equal or greater to any string in the range.

The prefix of the string preserves utf8 encoding, except for some last symbol. That last symbol can be thrown off and several first symbols can be used with collation for better extent use or elimination.",1,"Roman proposed to split extent map by column width and I would like to piggy back that idea.

I would like to propose to keep ranges in maps indexed by the the length.

Right now the structure as follows:

{code:c++}
struct EMCasualPartition_struct
{
    int32_t sequenceNum;
    char isValid; //CP_INVALID - No min/max and no DML in progress. CP_UPDATING - Update in progress. CP_VALID- min/max is valid
    union
    {
        int128_t bigLoVal; // These need to be reinterpreted as unsigned for uint64_t/uint128_t column types.
        int64_t loVal;
    };
    union
    {
        int128_t bigHiVal;
        int64_t hiVal;
    };
    // some constructors.
};
{code}

What I propose is the following:

{code:c++}
dstruct EMCasualPartition_struct
{
    int32_t sequenceNum;
    char isValid; //CP_INVALID - No min/max and no DML in progress. CP_UPDATING - Update in progress. CP_VALID- min/max is valid
    uint8_t keyLen; // up to 256 bytes
    uint8_t minMax[]; // memcmp-friendly (big-endian for integers) encoding of min and max prefixes.
                                        // The array is twice the keyLen field long, first goes min, then max.

    // here we need methods to compute record size, get the min/max keys, etc.
};
{code}

And extentmap will now contains several ""extentarrays"", one for each key length.

It is easy to define maximum and minimum values for integers. Floating point numbers also can be handled, possible with one exception of negative zero.

For strings of arbitrary length, the minimum and maximum can be defined as follows:
# if string is not longer than key length, we extend the string with zeroes up to key length and this extended string forms minimum and maximum value to be added to a range. For string ""ab"" and key length 4 we will have minimum and maximum both equal to ""ab\0\0"".
# if string is longer than key length we cut it up to to key length. For string ""abcde"" and key length 4 the minimum and maximum are ""abcd"".

Please note that we consider minimum prefix to be extended to the right with zeroes infinitely and maximum to be extended to the right with the 0xff infinitely. That makes comparison of key length prefixes of strings with maximums and minimums valid: minimum is equal or less than any string in the range and maximum is equal or greater to any string in the range.

The prefix of the string preserves utf8 encoding, except for some last symbol. That last symbol can be thrown off and several first symbols can be used with collation for better extent use or elimination."
988,MCOL-4529,MCOL,Sergey Zefirov,179126,2021-02-04 15:57:41,"Extentmap internally is defined as preallocated array due to the way changes are handled: these changes are recorded as n-byte values starting at some memory address(es). When something is about to be changed, its value (n bytes of it) first recorded in the rollback record and then it is changed. The ExtentMap inherits from Undoable class and uses default implementation.

Thus, we should keep ""array of records"" design if we do not want to change in-memory rollback mechanism.

We can do that by having several arrays instead of one. And, possibly, even save some memory, I'll expand that soon.

Atomic update of several ranges with different key lengths is also possible: we can acquire locks for each individual key length sequentially, by using some order on key length (ascending or any other). The same ordering of lock acquisition between several mutator processes prevents dead lock.",2,"Extentmap internally is defined as preallocated array due to the way changes are handled: these changes are recorded as n-byte values starting at some memory address(es). When something is about to be changed, its value (n bytes of it) first recorded in the rollback record and then it is changed. The ExtentMap inherits from Undoable class and uses default implementation.

Thus, we should keep ""array of records"" design if we do not want to change in-memory rollback mechanism.

We can do that by having several arrays instead of one. And, possibly, even save some memory, I'll expand that soon.

Atomic update of several ranges with different key lengths is also possible: we can acquire locks for each individual key length sequentially, by using some order on key length (ascending or any other). The same ordering of lock acquisition between several mutator processes prevents dead lock."
989,MCOL-4529,MCOL,Sergey Zefirov,179128,2021-02-04 16:03:33,"We can use ""array of arrays"" storage to keep allocation smaller. The idea is described [here in Compact Dynamic Arrays section|https://concatenative.org/wiki/view/Exotic%20Data%20Structures]. Instead of having dynamic arrays, we can allocate needed parts of static array, having sqrt(N) array for pointers to several sqrt(N) subarrays.

N above is maximum number of extent records kept.

This way we will have same address for our implementation of Undoable and not more than O(sqrt(N)) overhead over maximum number of extents actually used.",3,"We can use ""array of arrays"" storage to keep allocation smaller. The idea is described [here in Compact Dynamic Arrays section|URL Instead of having dynamic arrays, we can allocate needed parts of static array, having sqrt(N) array for pointers to several sqrt(N) subarrays.

N above is maximum number of extent records kept.

This way we will have same address for our implementation of Undoable and not more than O(sqrt(N)) overhead over maximum number of extents actually used."
990,MCOL-4529,MCOL,Roman,179170,2021-02-05 09:53:51,[~sergey.zefirov] There will be no uint128 based dtypes so we don't need this integral type.,4,[~sergey.zefirov] There will be no uint128 based dtypes so we don't need this integral type.
991,MCOL-4529,MCOL,Sergey Zefirov,179306,2021-02-08 16:01:20,"The locking in ExtentMap is done through a call to grabEMEntryTable(OPS op), where operation op can be READ or WRITE. It does so for the whole table at once.

For more efficient split access we need to perform locking on multiple tables and for that we need to establish locking order for tables. Without lock ordering we can have deadlocks.

The easiest ordering for extent map operation is by length of a key in an extent. We have fixed number of key lengths (1, 2, 4, 8 and 16 bytes) and set of key lengths for locking can be provided with the single integer values with some bits set.

Thus, the grabEMEntryTable will change to grabEMEntryTables(OPS op, smallintset keyLenSet). The operation is still READ or WRITE, the key length set is a set of small integers i (i=0..5) for key lengths 2^i^, represented as a structure with single field.

Corresponding release operation should receive same parameters and unlock tables in reverse locking order. The introduction of smallintset structure helps with threading values with types: we can provide singleton, union and enumeration operations, as well as operations for collections of LBID-associated ranges.",5,"The locking in ExtentMap is done through a call to grabEMEntryTable(OPS op), where operation op can be READ or WRITE. It does so for the whole table at once.

For more efficient split access we need to perform locking on multiple tables and for that we need to establish locking order for tables. Without lock ordering we can have deadlocks.

The easiest ordering for extent map operation is by length of a key in an extent. We have fixed number of key lengths (1, 2, 4, 8 and 16 bytes) and set of key lengths for locking can be provided with the single integer values with some bits set.

Thus, the grabEMEntryTable will change to grabEMEntryTables(OPS op, smallintset keyLenSet). The operation is still READ or WRITE, the key length set is a set of small integers i (i=0..5) for key lengths 2^i^, represented as a structure with single field.

Corresponding release operation should receive same parameters and unlock tables in reverse locking order. The introduction of smallintset structure helps with threading values with types: we can provide singleton, union and enumeration operations, as well as operations for collections of LBID-associated ranges."
992,MCOL-4529,MCOL,Sergey Zefirov,179485,2021-02-10 09:32:47,"I definitely should record what Roman suggested: we may check extents with textual information with simple comparison of prefixes:

extent_range_min <= SUBSTR(string, 0, LENGTH(extent_range_min)) && SUBSTR(string, 0, LENGTH(extent_range_max)) <= extent_range_max

The ""less or equal"" comparison is collation-aware one.

We keep ranges for TOKEN column extents in 16-byte (128-bit) keys. These TOKEN columns provide information about strings layout in dictionary file. Please note that TOKEN columns are 8-bytes wide and this can be a source of confusion.

The string prefixes in ranges are binary - no collation is applied at the time of writing the strings into dictionary files. Collation, if any, is applied at the extent elimination phase.",6,"I definitely should record what Roman suggested: we may check extents with textual information with simple comparison of prefixes:

extent_range_min <= SUBSTR(string, 0, LENGTH(extent_range_min)) && SUBSTR(string, 0, LENGTH(extent_range_max)) <= extent_range_max

The ""less or equal"" comparison is collation-aware one.

We keep ranges for TOKEN column extents in 16-byte (128-bit) keys. These TOKEN columns provide information about strings layout in dictionary file. Please note that TOKEN columns are 8-bytes wide and this can be a source of confusion.

The string prefixes in ranges are binary - no collation is applied at the time of writing the strings into dictionary files. Collation, if any, is applied at the extent elimination phase."
993,MCOL-4529,MCOL,Sergey Zefirov,179544,2021-02-10 16:11:53,"Elena Starikova from server team shared this interesting bug from MySQL: https://bugs.mysql.com/bug.php?id=20247

Binary collation for strings ""B"", ""D"" and ""Č"" (accented C) will result in range ""B""..""Č"" - both excessively large and, when used with proper collation, will not include ""D"" at all. The use of proper collation would result in ""B""..""D"" but will preclude the use of any other collations.",7,"Elena Starikova from server team shared this interesting bug from MySQL: URL

Binary collation for strings ""B"", ""D"" and ""Č"" (accented C) will result in range ""B""..""Č"" - both excessively large and, when used with proper collation, will not include ""D"" at all. The use of proper collation would result in ""B""..""D"" but will preclude the use of any other collations."
994,MCOL-4529,MCOL,Sergey Zefirov,179566,2021-02-10 18:53:49,"Alexander Barkov explained that MariaDB does not optimize ""some_column COLLATE collation"" if collation specified differ from the collation specified for column.

But still, we need to store binary (utf-8) representation of strings for different collations to be applied.",8,"Alexander Barkov explained that MariaDB does not optimize ""some_column COLLATE collation"" if collation specified differ from the collation specified for column.

But still, we need to store binary (utf-8) representation of strings for different collations to be applied."
995,MCOL-4529,MCOL,Sergey Zefirov,179609,2021-02-11 09:24:13,"Started a document on this, shared it with Greg and Roman.",9,"Started a document on this, shared it with Greg and Roman."
996,MCOL-4529,MCOL,Gregory Dorman,182130,2021-03-11 14:47:52,"This task was to produce the design. It was accomplished. It is now MCOL-4580, implementation time.",10,"This task was to produce the design. It was accomplished. It is now MCOL-4580, implementation time."
997,MCOL-453,MCOL,David Hill,89373,2016-12-08 21:09:00,"Changed mcsadmin to remove/add/alterSystem commands to only allow to run on active oam parent

commit 10b0199e81eb8e445abd240f1a6b45e32f02df6e
Author: David Hill <david.hill@mariadb.com>
Date:   Thu Dec 8 13:15:55 2016 -0600

    MCOL-452

 oam/install_scripts/performance_installer.sh |  4 ++--
 oam/install_scripts/user_installer.sh        |  4 ++--
 oamapps/mcsadmin/mcsadmin.cpp                | 13 -------------
 3 files changed, 4 insertions(+), 17 deletions(-)
",1,"Changed mcsadmin to remove/add/alterSystem commands to only allow to run on active oam parent

commit 10b0199e81eb8e445abd240f1a6b45e32f02df6e
Author: David Hill 
Date:   Thu Dec 8 13:15:55 2016 -0600

    MCOL-452

 oam/install_scripts/performance_installer.sh |  4 ++--
 oam/install_scripts/user_installer.sh        |  4 ++--
 oamapps/mcsadmin/mcsadmin.cpp                | 13 -------------
 3 files changed, 4 insertions(+), 17 deletions(-)
"
998,MCOL-453,MCOL,David Hill,89374,2016-12-08 21:09:28,please review,2,please review
999,MCOL-453,MCOL,Ben Thompson,89375,2016-12-08 21:20:55,"Reviewed, looks good to me",3,"Reviewed, looks good to me"
1000,MCOL-453,MCOL,Daniel Lee,89477,2016-12-12 17:53:46,"Build tested: 1.0.6-1

getsoftwareinfo   Mon Dec 12 17:50:23 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.6
Release     : 1
Architecture: x86_64
Install Date: Mon 12 Dec 2016 05:30:41 PM UTC
Group       : Applications/Databases
Size        : 10016657
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.6-1.src.rpm
Build Date  : Fri 09 Dec 2016 11:25:12 PM UTC

The removeModule command still can be executed from UM1 and PM2, which are non-active PM module.",4,"Build tested: 1.0.6-1

getsoftwareinfo   Mon Dec 12 17:50:23 2016

Name        : mariadb-columnstore-platform
Version     : 1.0.6
Release     : 1
Architecture: x86_64
Install Date: Mon 12 Dec 2016 05:30:41 PM UTC
Group       : Applications/Databases
Size        : 10016657
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.6-1.src.rpm
Build Date  : Fri 09 Dec 2016 11:25:12 PM UTC

The removeModule command still can be executed from UM1 and PM2, which are non-active PM module."
1001,MCOL-453,MCOL,David Hill,89564,2016-12-14 19:52:35,"Previously checked in one fix for removeModule.. Then checked in other to correct a cutand paste issue. change comment of addModule to removeModule.

commit d260bb4bfb64d55f481d070c8a5c8ed81e3cd3c9
Author: David Hill <david.hill@mariadb.com>
Date:   Wed Dec 14 13:47:39 2016 -0600

    MCOL-453

 oamapps/mcsadmin/mcsadmin.cpp | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)


commit fae09df1b7c5b62092189eec7747169e4708464b
Author: David Hill <david.hill@mariadb.com>
Date:   Mon Dec 12 11:59:51 2016 -0600

    MCOL-453 - added check to not allow removemodule to run on non-parent

",5,"Previously checked in one fix for removeModule.. Then checked in other to correct a cutand paste issue. change comment of addModule to removeModule.

commit d260bb4bfb64d55f481d070c8a5c8ed81e3cd3c9
Author: David Hill 
Date:   Wed Dec 14 13:47:39 2016 -0600

    MCOL-453

 oamapps/mcsadmin/mcsadmin.cpp | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)


commit fae09df1b7c5b62092189eec7747169e4708464b
Author: David Hill 
Date:   Mon Dec 12 11:59:51 2016 -0600

    MCOL-453 - added check to not allow removemodule to run on non-parent

"
1002,MCOL-453,MCOL,David Hill,89565,2016-12-14 19:53:26,"I originally didnt have this check for removeModule, so it has been added, please check and review",6,"I originally didnt have this check for removeModule, so it has been added, please check and review"
1003,MCOL-453,MCOL,Ben Thompson,89616,2016-12-15 22:12:11,"Reviewed, looks good.",7,"Reviewed, looks good."
1004,MCOL-453,MCOL,David Hill,90525,2017-01-11 15:53:02,to be reviewed,8,to be reviewed
1005,MCOL-453,MCOL,Ben Thompson,90534,2017-01-11 18:02:50,Reviewed and merged,9,Reviewed and merged
1006,MCOL-453,MCOL,Daniel Lee,90877,2017-01-20 23:06:38,"Build verified: 1.0.7-1

mcsadmin> getsoft
getsoftwareinfo   Fri Jan 20 23:05:47 2017

Name        : mariadb-columnstore-platform
Version     : 1.0.7
Release     : 1
Architecture: x86_64
Install Date: Fri 20 Jan 2017 10:46:14 PM UTC
Group       : Applications/Databases
Size        : 10001348
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.7-1.src.rpm

Verified removeModule check has been added.
",10,"Build verified: 1.0.7-1

mcsadmin> getsoft
getsoftwareinfo   Fri Jan 20 23:05:47 2017

Name        : mariadb-columnstore-platform
Version     : 1.0.7
Release     : 1
Architecture: x86_64
Install Date: Fri 20 Jan 2017 10:46:14 PM UTC
Group       : Applications/Databases
Size        : 10001348
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.7-1.src.rpm

Verified removeModule check has been added.
"
1007,MCOL-4560,MCOL,David Hall,197041,2021-08-18 16:15:01,Walked the Columnstore.xml and resourcemanager. Removed all unused entries in Columnstore.xml and all unreferenced code in resourcemanager. I also removed dead code in joblist. This code has been dead since InfiniDB 3.0 where the introduction of tupblebps deprecated it.,1,Walked the Columnstore.xml and resourcemanager. Removed all unused entries in Columnstore.xml and all unreferenced code in resourcemanager. I also removed dead code in joblist. This code has been dead since InfiniDB 3.0 where the introduction of tupblebps deprecated it.
1008,MCOL-4560,MCOL,David Hall,216183,2022-03-04 14:49:41,I've created a new PR to be compatible with the new formatting of develop-6. I hope to have addressed your concerns with the previous PR.,2,I've created a new PR to be compatible with the new formatting of develop-6. I hope to have addressed your concerns with the previous PR.
1009,MCOL-4560,MCOL,David Hall,216185,2022-03-04 14:55:06,new PR https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/2302,3,new PR URL
1010,MCOL-4560,MCOL,alexey vorovich,219418,2022-04-06 20:18:40,[~tntnatbry] please new PR,4,[~tntnatbry] please new PR
1011,MCOL-4566,MCOL,Denis Khalikov,181256,2021-03-02 09:44:53,"Regarding to this task the file header should contain new information:
1. Column width.
2. Column data type.
Also we need a function which maps full filename to oid to be able to create columnExtent.",1,"Regarding to this task the file header should contain new information:
1. Column width.
2. Column data type.
Also we need a function which maps full filename to oid to be able to create columnExtent."
1012,MCOL-4566,MCOL,Denis Khalikov,181584,2021-03-04 20:45:48,file2Oid added for review https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1794,2,file2Oid added for review URL
1013,MCOL-4566,MCOL,Denis Khalikov,181699,2021-03-05 19:22:24,patch which adds 2 new fields to `CompressedDBFileHeader` added for review https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1795,3,patch which adds 2 new fields to `CompressedDBFileHeader` added for review URL
1014,MCOL-4566,MCOL,Denis Khalikov,182067,2021-03-10 21:14:17,add patch with rebuildEM tool for review https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1808,4,add patch with rebuildEM tool for review URL
1015,MCOL-4566,MCOL,Denis Khalikov,182302,2021-03-12 21:22:39,"Currently this tool works with compressed segment files created by engine, but it's not enough to rebuild full extent map.
The problem: system segment files does not have compression, that means we cannot simply restore needed data (column type and width) to create a column extent. Those extents are needed inside extent map for proper work of the database.
Some ideas:
1. Keep the initial state of extent map inside the rebuildEM tool and restore it, before going inside dbroot.
a. Keep as binary blob, a global inside rebuildEM tool, the initial size is about 4k, but looking to the hexdum it seems like a sparse matrix and the restore it using ExtentMap.load()
b. Keep as (oid, partition, segment, width, col data type, and may be other other needed data) and try to restore it by calling createColumnExtent.
Will experiment on it starting from the next week.
Problems:
Currently I'm not sure about structure of the system files, the main question could the tables increase in size over time and create another segment files to keep data. In this case the initial state will be invalid.
2. Another way to keep separate file with system extent map.
Problems:
it seems like it's the same as keeping full extent map in file, as it works now.
",5,"Currently this tool works with compressed segment files created by engine, but it's not enough to rebuild full extent map.
The problem: system segment files does not have compression, that means we cannot simply restore needed data (column type and width) to create a column extent. Those extents are needed inside extent map for proper work of the database.
Some ideas:
1. Keep the initial state of extent map inside the rebuildEM tool and restore it, before going inside dbroot.
a. Keep as binary blob, a global inside rebuildEM tool, the initial size is about 4k, but looking to the hexdum it seems like a sparse matrix and the restore it using ExtentMap.load()
b. Keep as (oid, partition, segment, width, col data type, and may be other other needed data) and try to restore it by calling createColumnExtent.
Will experiment on it starting from the next week.
Problems:
Currently I'm not sure about structure of the system files, the main question could the tables increase in size over time and create another segment files to keep data. In this case the initial state will be invalid.
2. Another way to keep separate file with system extent map.
Problems:
it seems like it's the same as keeping full extent map in file, as it works now.
"
1016,MCOL-4566,MCOL,Denis Khalikov,182444,2021-03-15 21:06:18,"After initializing extents for the system table i got to errors:
1. Was related to extent status, if we use `createColumnExtent` it by default is 'unavailable` so we need to mark it available by setLocalHWM.
2. Error related to tables with 'varchar' and `char` (all columns which has additional dictionary segment file) the current approach uses a `greedy` strategy to allocate LBID from the freelist, so after running the rebuildEM we got extents with different `range.start` compared with original if we start from oid different than it was in original pass, so I got different errors when trying to select from 'varchar` column, it could be `null` values or it could be a values from other tables.
Some solutions could be: 
1. extend file header and add `range.start` field,  walk on all segment files in dbroot, save all needed data and create extent map.
2. walk on dbroot and try to sort extents by oid, than create extent map. 

Currently I'm able to rebuildEM and get `partially` working database. The database works with all columns except 'varchar'. ",6,"After initializing extents for the system table i got to errors:
1. Was related to extent status, if we use `createColumnExtent` it by default is 'unavailable` so we need to mark it available by setLocalHWM.
2. Error related to tables with 'varchar' and `char` (all columns which has additional dictionary segment file) the current approach uses a `greedy` strategy to allocate LBID from the freelist, so after running the rebuildEM we got extents with different `range.start` compared with original if we start from oid different than it was in original pass, so I got different errors when trying to select from 'varchar` column, it could be `null` values or it could be a values from other tables.
Some solutions could be: 
1. extend file header and add `range.start` field,  walk on all segment files in dbroot, save all needed data and create extent map.
2. walk on dbroot and try to sort extents by oid, than create extent map. 

Currently I'm able to rebuildEM and get `partially` working database. The database works with all columns except 'varchar'. "
1017,MCOL-4566,MCOL,Denis Khalikov,182484,2021-03-16 10:06:24,"Unfortunately we have to keep the start address of freeList in the segment files.
Following example:
create table t1 (a varchar (255)) engine = columnstore;
insert into t1 values(""a"");
create table t2 (a varchar (255), b varchar (255), c varchar (255)) engine=columnstore;
insert into t2 values(""a"", ""b"", ""c"");

will create for us following extents:

range.start|range.size|fileId|blockOffset|HWM|partition|segment|dbroot|width|status|hiVal|loVal|seqNum|isValid|
234496|8|3001|0|0|0|0|1|8|0|0|-1|4|0|
242688|8|3002|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|2|0|
250880|8|3004|0|0|0|0|1|8|0|0|-1|2|0|
259072|8|3007|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|1|0|
267264|8|3005|0|0|0|0|1|8|0|0|-1|2|0|
275456|8|3008|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|1|0|
283648|8|3006|0|0|0|0|1|8|0|0|-1|2|0|
291840|8|3009|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|1|0|

To rebuild extent map in the right way (to be able to access columns with char data through dictionary files)  we have to rebuild `range.start` as it was originally, but currently we do not have enough information. I was thinking that we can walk inside dbroot, collect needed data, and keep it sorted by oid, than rebuild em, but the example above shows that it was a wrong.
",7,"Unfortunately we have to keep the start address of freeList in the segment files.
Following example:
create table t1 (a varchar (255)) engine = columnstore;
insert into t1 values(""a"");
create table t2 (a varchar (255), b varchar (255), c varchar (255)) engine=columnstore;
insert into t2 values(""a"", ""b"", ""c"");

will create for us following extents:

range.start|range.size|fileId|blockOffset|HWM|partition|segment|dbroot|width|status|hiVal|loVal|seqNum|isValid|
234496|8|3001|0|0|0|0|1|8|0|0|-1|4|0|
242688|8|3002|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|2|0|
250880|8|3004|0|0|0|0|1|8|0|0|-1|2|0|
259072|8|3007|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|1|0|
267264|8|3005|0|0|0|0|1|8|0|0|-1|2|0|
275456|8|3008|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|1|0|
283648|8|3006|0|0|0|0|1|8|0|0|-1|2|0|
291840|8|3009|0|0|0|0|1|0|0|-9223372036854775808|9223372036854775807|1|0|

To rebuild extent map in the right way (to be able to access columns with char data through dictionary files)  we have to rebuild `range.start` as it was originally, but currently we do not have enough information. I was thinking that we can walk inside dbroot, collect needed data, and keep it sorted by oid, than rebuild em, but the example above shows that it was a wrong.
"
1018,MCOL-4566,MCOL,Denis Khalikov,182561,2021-03-16 18:55:31,"Updated https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1808
Currently it works in following way:
1. Initialize the extents for the system tables from the initial binary blob.
2. Walks  dbroot and collects (oid, part, segment, coltype, colwidth, isDict) for segment files. (Keeps it sorted via map<FIleId, OidComparator>)
3. Rebuilds extent maps from the collected data starting from the lowest OID.

Current version successfully rebuild initial tables works with columns without (varchar types).

This should be updated to keep sorted not by oid, but by offset in freeList like (map<FileId, start.rangeComparator>), after this we can build extent map in greedy way.

",8,"Updated URL
Currently it works in following way:
1. Initialize the extents for the system tables from the initial binary blob.
2. Walks  dbroot and collects (oid, part, segment, coltype, colwidth, isDict) for segment files. (Keeps it sorted via map)
3. Rebuilds extent maps from the collected data starting from the lowest OID.

Current version successfully rebuild initial tables works with columns without (varchar types).

This should be updated to keep sorted not by oid, but by offset in freeList like (map), after this we can build extent map in greedy way.

"
1019,MCOL-4566,MCOL,Denis Khalikov,182931,2021-03-18 20:38:16,"Updated patch on review. Current solution create extent in order it was created originally. 
Tested solution on different tables, seems like working solution.
Need more testing - like adding table with million rows.",9,"Updated patch on review. Current solution create extent in order it was created originally. 
Tested solution on different tables, seems like working solution.
Need more testing - like adding table with million rows."
1020,MCOL-4566,MCOL,Denis Khalikov,182943,2021-03-18 23:06:47,"Run some test on rows > 8kk, and found that we also need to support multiple extents per one segment file. I think we can hard-code it into 2 as in config file and check the config, if we got more. Just do not run the tool.
Also other question is how to set HWM properly?",10,"Run some test on rows > 8kk, and found that we also need to support multiple extents per one segment file. I think we can hard-code it into 2 as in config file and check the config, if we got more. Just do not run the tool.
Also other question is how to set HWM properly?"
1021,MCOL-4566,MCOL,Denis Khalikov,182989,2021-03-19 09:57:40,"Currently it does not work after inserting data with `cpimport` for example inserting 17kk rows:
create table t1(a int) engine=columnstore;
`$cpimport temp t1 content.tbl`.
 it will create a 3 segment files and 3 extents, but `lbid` is not changing from default values.
running: 
`
$rebuilEM -v
FileId is collected [OID: 3001, partition: 0, segment: 1, col width: 4, lbid:-1, isDict: 0]
Processing file: /var/lib/columnstore/data1/000.dir/000.dir/011.dir/185.dir/000.dir/FILE000.cdf  [OID: 3001, partition: 0, segment: 0] 
FileId is collected [OID: 3001, partition: 0, segment: 0, col width: 4, lbid:-1, isDict: 0]
Processing file: /var/lib/columnstore/data1/000.dir/000.dir/011.dir/185.dir/000.dir/FILE002.cdf  [OID: 3001, partition: 0, segment: 2] 
FileId is collected [OID: 3001, partition: 0, segment: 2, col width: 4, lbid:-1, isDict: 0]
Build extent map with size 1
Extent is created, allocated size 4096 actual LBID 234496 for [OID: 3001, partition: 0, segment: 1, col width: 4, lbid:-1, isDict: 0]
`
Another thing I dont understand currently. It seems like after the size of extent is exceed 8m rows, engine should create a new extent for the same segment file. The number of extents per segment file is defined inside config, but using `cpimport` it creates a new segment file for new extent.
`
range.start|range.size|fileId|blockOffset|HWM|partition|segment|dbroot|width|status|hiVal|loVal|seqNum|isValid|
234496|4|3001|0|4095|0|0|1|4|0|999999|0|9|2|
238592|4|3001|0|4095|0|1|1|4|0|999999|0|9|2|
242688|4|3001|0|121|0|2|1|4|0|999999|751616|1|2|
`
`
./000.dir/011.dir/185.dir/000.dir/FILE001.cdf
./000.dir/011.dir/185.dir/000.dir/FILE000.cdf
./000.dir/011.dir/185.dir/000.dir/FILE002.cdf
`",11,"Currently it does not work after inserting data with `cpimport` for example inserting 17kk rows:
create table t1(a int) engine=columnstore;
`$cpimport temp t1 content.tbl`.
 it will create a 3 segment files and 3 extents, but `lbid` is not changing from default values.
running: 
`
$rebuilEM -v
FileId is collected [OID: 3001, partition: 0, segment: 1, col width: 4, lbid:-1, isDict: 0]
Processing file: /var/lib/columnstore/data1/000.dir/000.dir/011.dir/185.dir/000.dir/FILE000.cdf  [OID: 3001, partition: 0, segment: 0] 
FileId is collected [OID: 3001, partition: 0, segment: 0, col width: 4, lbid:-1, isDict: 0]
Processing file: /var/lib/columnstore/data1/000.dir/000.dir/011.dir/185.dir/000.dir/FILE002.cdf  [OID: 3001, partition: 0, segment: 2] 
FileId is collected [OID: 3001, partition: 0, segment: 2, col width: 4, lbid:-1, isDict: 0]
Build extent map with size 1
Extent is created, allocated size 4096 actual LBID 234496 for [OID: 3001, partition: 0, segment: 1, col width: 4, lbid:-1, isDict: 0]
`
Another thing I dont understand currently. It seems like after the size of extent is exceed 8m rows, engine should create a new extent for the same segment file. The number of extents per segment file is defined inside config, but using `cpimport` it creates a new segment file for new extent.
`
range.start|range.size|fileId|blockOffset|HWM|partition|segment|dbroot|width|status|hiVal|loVal|seqNum|isValid|
234496|4|3001|0|4095|0|0|1|4|0|999999|0|9|2|
238592|4|3001|0|4095|0|1|1|4|0|999999|0|9|2|
242688|4|3001|0|121|0|2|1|4|0|999999|751616|1|2|
`
`
./000.dir/011.dir/185.dir/000.dir/FILE001.cdf
./000.dir/011.dir/185.dir/000.dir/FILE000.cdf
./000.dir/011.dir/185.dir/000.dir/FILE002.cdf
`"
1022,MCOL-4566,MCOL,Denis Khalikov,183005,2021-03-19 12:18:25,"Needed to add HWM calculation such as (decompressed file size - header size * 2 ) / block  size.

Updated this will not work. File size does not reflect to HWM. Currently HWM is incremented by one after next block is needed inside extent.

Will try to calculate hwm by searching an first block with empty value in file.
`
200                 for (j = 0, curVal = buf; j < totalRowPerBlock; j++, curVal += column.colWidth)
 201                 {
 202                     if (isEmptyRow((uint64_t*)curVal, emptyVal, column.colWidth))
 203                     {
`",12,"Needed to add HWM calculation such as (decompressed file size - header size * 2 ) / block  size.

Updated this will not work. File size does not reflect to HWM. Currently HWM is incremented by one after next block is needed inside extent.

Will try to calculate hwm by searching an first block with empty value in file.
`
200                 for (j = 0, curVal = buf; j < totalRowPerBlock; j++, curVal += column.colWidth)
 201                 {
 202                     if (isEmptyRow((uint64_t*)curVal, emptyVal, column.colWidth))
 203                     {
`"
1023,MCOL-4566,MCOL,Denis Khalikov,183439,2021-03-23 20:36:45,"Last patch https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1808
adds:
Proper HWM recovery from segment file.
Added support for bulk insertion via cpimport.
Current limitations - it does not work with multiple extents per segment file.
We can simple detect those kind of files in case we recover hwm >= (columnWidth * numExtentRows) / blockSizeInBytes but there is no straight way to create an extent in the same order as it was created originally, because starting lbid is not known for each extent.
Test for different tables schemes, for example:
create table t1 (a int, b varchar (255), c int, d varchar(255), e int, d varchar(255)) engine=columnstore;
and insert 20M rows into table via cpimport.
In this case bulk will create 3 segment file for each int column and 6 for each varchar column (1 segment file with tokens and 1 dictionary file).",13,"Last patch URL
adds:
Proper HWM recovery from segment file.
Added support for bulk insertion via cpimport.
Current limitations - it does not work with multiple extents per segment file.
We can simple detect those kind of files in case we recover hwm >= (columnWidth * numExtentRows) / blockSizeInBytes but there is no straight way to create an extent in the same order as it was created originally, because starting lbid is not known for each extent.
Test for different tables schemes, for example:
create table t1 (a int, b varchar (255), c int, d varchar(255), e int, d varchar(255)) engine=columnstore;
and insert 20M rows into table via cpimport.
In this case bulk will create 3 segment file for each int column and 6 for each varchar column (1 segment file with tokens and 1 dictionary file)."
1024,MCOL-4566,MCOL,Denis Khalikov,183604,2021-03-24 19:29:12,Found bug related to HWM calculation for dictionary files.,14,Found bug related to HWM calculation for dictionary files.
1025,MCOL-4566,MCOL,Denis Khalikov,183718,2021-03-25 16:30:19,"Final version is on review.
Currently it supports 2 extents per segment file, it could be updated if needed, but will require to add one more field in compressed header.",15,"Final version is on review.
Currently it supports 2 extents per segment file, it could be updated if needed, but will require to add one more field in compressed header."
1026,MCOL-4566,MCOL,Roman,184671,2021-04-02 14:53:52,4QA. Plz msg me or [~denis0x0D] when you are ready to test the tool so I can explain how does it work.,16,4QA. Plz msg me or [~denis0x0D] when you are ready to test the tool so I can explain how does it work.
1027,MCOL-4566,MCOL,Roman Navrotskiy,185750,2021-04-13 12:19:29,"[~dleeyh]
You can try any latest build from develop branch for testing rpm-based platforms. If you want to test others, you can use this one:

https://cspkg.s3.amazonaws.com/index.html?prefix=develop/pull_request/2141/amd64/

Also it will be available from regular cron builds since tomorrow I suppose.",17,"[~dleeyh]
You can try any latest build from develop branch for testing rpm-based platforms. If you want to test others, you can use this one:

URL

Also it will be available from regular cron builds since tomorrow I suppose."
1028,MCOL-4566,MCOL,Daniel Lee,185819,2021-04-13 17:09:23,Reopened pending for requirement discussion by management.,18,Reopened pending for requirement discussion by management.
1029,MCOL-4566,MCOL,Roman,186634,2021-04-20 08:18:37,"ExtentsPerSegmentFile controls the number of extents per segment file so it doesn't affect Dictionary files. We should just remove the setting ExtentsPerSegmentFile from the default Columnstore.xml shiped with the package.
BTW All this activity crosses the scope of this project and must be done outside of this issue.",19,"ExtentsPerSegmentFile controls the number of extents per segment file so it doesn't affect Dictionary files. We should just remove the setting ExtentsPerSegmentFile from the default Columnstore.xml shiped with the package.
BTW All this activity crosses the scope of this project and must be done outside of this issue."
1030,MCOL-4566,MCOL,Roman,186665,2021-04-20 11:38:26,[~gdorman] After discussion with Denis I will answer the 4th question. The effort is minimal since we just need to remove the option from the default config file shiped.,20,[~gdorman] After discussion with Denis I will answer the 4th question. The effort is minimal since we just need to remove the option from the default config file shiped.
1031,MCOL-4566,MCOL,Roman,186668,2021-04-20 11:45:02,"The next commentary is a developer note of how to overcome the limitation described by Denis previously.
There are two fixed extents descriptors that are added to every Segment and Dictionary file. The descriptor tells about initial LBID and number of blocks in the extent and its purpose is to map Segment files with Tokens and Dictionaries. The number of extents in a Dictionary is dynamic though. We extend the compressed Dictionary with a dynamic sized section. After the mandatory two extent descriptors there will be a number of the following descriptors and a set of these additional descriptors.",21,"The next commentary is a developer note of how to overcome the limitation described by Denis previously.
There are two fixed extents descriptors that are added to every Segment and Dictionary file. The descriptor tells about initial LBID and number of blocks in the extent and its purpose is to map Segment files with Tokens and Dictionaries. The number of extents in a Dictionary is dynamic though. We extend the compressed Dictionary with a dynamic sized section. After the mandatory two extent descriptors there will be a number of the following descriptors and a set of these additional descriptors."
1032,MCOL-4566,MCOL,Gregory Dorman,187162,2021-04-23 11:40:01,"Then don’t. Keep for 6.1.

",22,"Then don’t. Keep for 6.1.

"
1033,MCOL-4566,MCOL,Daniel Lee,192090,2021-06-16 22:15:11,"Build tested: 6.1.1 ( Drone #2576 )

[centos8:root~]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 

Build tested: 6.1.1 ( Drone #2576 )

Performed a test on a 3-node cluster with local storage and did not received any error/warning indicating that such configuration is not supported. I noticed there is a PR (#1884) was declined.

Test #1, without -v option
{noformat}
[centos8:root~]# mcsRebuildEM
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 
Y
{noformat}
I have few concerns for the maturity of the tool.
1. The message ""Note: that the launch can break the cluster."" is quite alarming.  Breaking a cluster is too serious that user or support team would not want to continue.  The tool should be mature enough for the user to move on with confidence.
2. ""Requirement: all DBRoots must be on this node. "".  The user should not need to find out if all DBroots are on this node.  The tool should determine where the data is.  If it does not meet the requirement, it should exit with an appropriate message.
3. ""Do you want to continue Y/N? "".  The user's reply should be on the same line
4. After answering with an ""Y"", the tool simply ended.  It must return a message indicating if the run was successful or not.  If successful, it should print out the BRM file that was generated, with directory path.
5. For large database, the tool may take a while to run.  For each dbroot, the tool should print out few steps of the BRM building process, prefixed with system time stamp.  Such information will be helpful for support engineers should the run failed.
6. The tool should check if the ColumnStore cluster is active.  If yes, then exit out immediately.
{noformat}
[centos8:root~]# mcsRebuildEM
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 
Y
/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em file exists. 
Please note: this tool is only suitable in situations where there is no `BRM_saves_em` file. 
If `BRM_saves_em` exists extent map will be restored from it. 
{noformat}
7. The tool should check for the existence of the BRM_saves_em file at the beginning of the run.  If exists, it should exit immediately without user interaction.
BRM_saved_em only when the run is successful.

Test #2, with -v option
{noformat}
[centos8:root~]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 
Y
Initialize system extents from the initial state
Collect extents for the DBRoot /var/lib/columnstore/data1
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/019.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/013.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/016.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/028.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/025.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/022.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from 
{noformat}
The above messages are always outputted.  Engineer explained that there are system catalog files and they are not compressed.  Since system catalog files are not compressed by design, this is an expected behavior.  The tool should not output these messages.  These messages should only apply to user data files that are not compressed.  Please do not simply suppress such messages, only the ones for the system catalog files.",23,"Build tested: 6.1.1 ( Drone #2576 )

[centos8:root~]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 

Build tested: 6.1.1 ( Drone #2576 )

Performed a test on a 3-node cluster with local storage and did not received any error/warning indicating that such configuration is not supported. I noticed there is a PR (#1884) was declined.

Test #1, without -v option
{noformat}
[centos8:root~]# mcsRebuildEM
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 
Y
{noformat}
I have few concerns for the maturity of the tool.
1. The message ""Note: that the launch can break the cluster."" is quite alarming.  Breaking a cluster is too serious that user or support team would not want to continue.  The tool should be mature enough for the user to move on with confidence.
2. ""Requirement: all DBRoots must be on this node. "".  The user should not need to find out if all DBroots are on this node.  The tool should determine where the data is.  If it does not meet the requirement, it should exit with an appropriate message.
3. ""Do you want to continue Y/N? "".  The user's reply should be on the same line
4. After answering with an ""Y"", the tool simply ended.  It must return a message indicating if the run was successful or not.  If successful, it should print out the BRM file that was generated, with directory path.
5. For large database, the tool may take a while to run.  For each dbroot, the tool should print out few steps of the BRM building process, prefixed with system time stamp.  Such information will be helpful for support engineers should the run failed.
6. The tool should check if the ColumnStore cluster is active.  If yes, then exit out immediately.
{noformat}
[centos8:root~]# mcsRebuildEM
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 
Y
/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves_em file exists. 
Please note: this tool is only suitable in situations where there is no `BRM_saves_em` file. 
If `BRM_saves_em` exists extent map will be restored from it. 
{noformat}
7. The tool should check for the existence of the BRM_saves_em file at the beginning of the run.  If exists, it should exit immediately without user interaction.
BRM_saved_em only when the run is successful.

Test #2, with -v option
{noformat}
[centos8:root~]# mcsRebuildEM -v
The launch of mcsRebuildEM tool must be sanctioned by MariaDB support. 
Requirement: all DBRoots must be on this node. 
Note: that the launch can break the cluster.
Do you want to continue Y/N? 
Y
Initialize system extents from the initial state
Collect extents for the DBRoot /var/lib/columnstore/data1
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/019.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/013.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/016.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/028.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/025.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from the file /var/lib/columnstore/data1/000.dir/000.dir/008.dir/022.dir/000.dir/FILE000.cdf, probably this file was created without compression. 
Cannot read file header from 
{noformat}
The above messages are always outputted.  Engineer explained that there are system catalog files and they are not compressed.  Since system catalog files are not compressed by design, this is an expected behavior.  The tool should not output these messages.  These messages should only apply to user data files that are not compressed.  Please do not simply suppress such messages, only the ones for the system catalog files."
1034,MCOL-4566,MCOL,Roman,203710,2021-10-26 16:23:15,This is a certain step towards a generic MCOL-312.,24,This is a certain step towards a generic MCOL-312.
1035,MCOL-4585,MCOL,Daniel Lee,181829,2021-03-08 17:03:01,"Build verified: 5.5.2-1 (Drone #1825)

Reproduced the issue in 5.5.1 and 5.5.2 Jenkins RC build.

New result:

{noformat}
MariaDB [tpch1]> select l_orderkey, group_concat(l_partkey, ""TF"" order by l_partkey) partkeys, regr_avgx(l_tax, l_extendedprice) avg_price from lineitem where l_orderkey < 10 group by l_orderkey order by l_orderkey;
+------------+--------------------------------------------------------------+--------------+
| l_orderkey | partkeys                                                     | avg_price    |
+------------+--------------------------------------------------------------+--------------+
|          1 | 2132TF,15635TF,24027TF,63700TF,67310TF,155190TF              | 30310.211667 |
|          2 | 106170TF                                                     | 44694.460000 |
|          3 | 4297TF,19036TF,29380TF,62143TF,128449TF,183095TF             | 34180.720000 |
|          4 | 88035TF                                                      | 30690.900000 |
|          5 | 37531TF,108570TF,123927TF                                    | 49276.323333 |
|          6 | 139636TF                                                     | 61998.310000 |
|          7 | 79251TF,94780TF,145243TF,151894TF,157238TF,163073TF,182052TF | 37447.331429 |
+------------+--------------------------------------------------------------+--------------+
7 rows in set (0.215 sec)
{noformat}",1,"Build verified: 5.5.2-1 (Drone #1825)

Reproduced the issue in 5.5.1 and 5.5.2 Jenkins RC build.

New result:

{noformat}
MariaDB [tpch1]> select l_orderkey, group_concat(l_partkey, ""TF"" order by l_partkey) partkeys, regr_avgx(l_tax, l_extendedprice) avg_price from lineitem where l_orderkey < 10 group by l_orderkey order by l_orderkey;
+------------+--------------------------------------------------------------+--------------+
| l_orderkey | partkeys                                                     | avg_price    |
+------------+--------------------------------------------------------------+--------------+
|          1 | 2132TF,15635TF,24027TF,63700TF,67310TF,155190TF              | 30310.211667 |
|          2 | 106170TF                                                     | 44694.460000 |
|          3 | 4297TF,19036TF,29380TF,62143TF,128449TF,183095TF             | 34180.720000 |
|          4 | 88035TF                                                      | 30690.900000 |
|          5 | 37531TF,108570TF,123927TF                                    | 49276.323333 |
|          6 | 139636TF                                                     | 61998.310000 |
|          7 | 79251TF,94780TF,145243TF,151894TF,157238TF,163073TF,182052TF | 37447.331429 |
+------------+--------------------------------------------------------------+--------------+
7 rows in set (0.215 sec)
{noformat}"
1036,MCOL-4589,MCOL,Daniel Lee,186722,2021-04-20 14:47:01,"Build verified: 5.6.1 ( Drone #2207 )

Tested on a 1gb dbt3 database

5.5.2-1
{noformat}
MariaDB [tpch1]> SELECT count(l_orderkey) FROM (SELECT * FROM lineitem UNION ALL SELECT * FROM lineitem) q;
+-------------------+
| count(l_orderkey) |
+-------------------+
|          12002430 |
+-------------------+
1 row in set (7.106 sec)
{noformat}

5.6.1
{noformat}
MariaDB [tpch1]> SELECT count(l_orderkey) FROM (SELECT * FROM lineitem UNION ALL SELECT * FROM lineitem) q;
+-------------------+
| count(l_orderkey) |
+-------------------+
|          12002430 |
+-------------------+
1 row in set (1.731 sec)
{noformat}
",1,"Build verified: 5.6.1 ( Drone #2207 )

Tested on a 1gb dbt3 database

5.5.2-1
{noformat}
MariaDB [tpch1]> SELECT count(l_orderkey) FROM (SELECT * FROM lineitem UNION ALL SELECT * FROM lineitem) q;
+-------------------+
| count(l_orderkey) |
+-------------------+
|          12002430 |
+-------------------+
1 row in set (7.106 sec)
{noformat}

5.6.1
{noformat}
MariaDB [tpch1]> SELECT count(l_orderkey) FROM (SELECT * FROM lineitem UNION ALL SELECT * FROM lineitem) q;
+-------------------+
| count(l_orderkey) |
+-------------------+
|          12002430 |
+-------------------+
1 row in set (1.731 sec)
{noformat}
"
1037,MCOL-4590,MCOL,Jigao Luo,226328,2022-06-09 12:27:19,"h1. The Preimage: Before The Performance Improvement

h2. Enviorment

Instance type: c5.4xlarge && Debian 11 && 30GiB EBS SSD

h2. Codebase

server codebase: 10.8 branch
columnstore codebase: https://github.com/cakebytheoceanLuo/mariadb-columnstore-engine/commit/94ba91c687ec5b293bc5b71de6e2225c2cc379ff

h2. Dataset

https://github.com/mariadb-corporation/mariadb-columnstore-samples/

{code:java}
MariaDB [bts]> describe flights;
+---------------------+-------------+------+-----+---------+-------+
| Field               | Type        | Null | Key | Default | Extra |
+---------------------+-------------+------+-----+---------+-------+
| year                | smallint(6) | YES  |     | NULL    |       |
| month               | tinyint(4)  | YES  |     | NULL    |       |
| day                 | tinyint(4)  | YES  |     | NULL    |       |
| day_of_week         | tinyint(4)  | YES  |     | NULL    |       |
| fl_date             | date        | YES  |     | NULL    |       |
| carrier             | varchar(2)  | YES  |     | NULL    |       |
| tail_num            | varchar(6)  | YES  |     | NULL    |       |
| fl_num              | smallint(6) | YES  |     | NULL    |       |
| origin              | varchar(5)  | YES  |     | NULL    |       |
| dest                | varchar(5)  | YES  |     | NULL    |       |
| crs_dep_time        | varchar(4)  | YES  |     | NULL    |       |
| dep_time            | varchar(4)  | YES  |     | NULL    |       |
| dep_delay           | smallint(6) | YES  |     | NULL    |       |
| taxi_out            | smallint(6) | YES  |     | NULL    |       |
| wheels_off          | varchar(4)  | YES  |     | NULL    |       |
| wheels_on           | varchar(4)  | YES  |     | NULL    |       |
| taxi_in             | smallint(6) | YES  |     | NULL    |       |
| crs_arr_time        | varchar(4)  | YES  |     | NULL    |       |
| arr_time            | varchar(4)  | YES  |     | NULL    |       |
| arr_delay           | smallint(6) | YES  |     | NULL    |       |
| cancelled           | smallint(6) | YES  |     | NULL    |       |
| cancellation_code   | smallint(6) | YES  |     | NULL    |       |
| diverted            | smallint(6) | YES  |     | NULL    |       |
| crs_elapsed_time    | smallint(6) | YES  |     | NULL    |       |
| actual_elapsed_time | smallint(6) | YES  |     | NULL    |       |
| air_time            | smallint(6) | YES  |     | NULL    |       |
| distance            | smallint(6) | YES  |     | NULL    |       |
| carrier_delay       | smallint(6) | YES  |     | NULL    |       |
| weather_delay       | smallint(6) | YES  |     | NULL    |       |
| nas_delay           | smallint(6) | YES  |     | NULL    |       |
| security_delay      | smallint(6) | YES  |     | NULL    |       |
| late_aircraft_delay | smallint(6) | YES  |     | NULL    |       |
+---------------------+-------------+------+-----+---------+-------+
32 rows in set (0.000 sec)

MariaDB [bts]> select count(*) from INFORMATION_SCHEMA.COLUMNS where table_name = ""flights"";
+----------+
| count(*) |
+----------+
|       32 |
+----------+
1 row in set (0.003 sec)
MariaDB [bts]> select count(*) from flights;
+----------+
| count(*) |
+----------+
| 38083735 |
+----------+
1 row in set (0.166 sec)
{code}

In this work, the table _flights_ is focused, which has 32 columns and over 38M tuples.

h2. Query With UNION ALL: Q1

{code:java}
MariaDB [bts]> select count(*) from (select * from flights union all select * from flights) as Q1;
+----------+
| count(*) |
+----------+
| 76167470 |
+----------+
1 row in set (3.011 sec)
{code}
I run this SQL Query Q1 100 times in the release build: the average runtime is *3.49s*.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    3151.619 |*****                                    2
    3208.883 |*******                                  3
    3267.187 |************************                 10
    3326.551 |*********************                    9
    3386.993 |****************                         7
    3448.533 |*********************************        14
    3511.192 |*********************************        14
    3574.989 |**************************************** 17
    3639.945 |************************                 10
    3706.081 |**************                           6
    3773.420 |*******                                  3
    3841.981 |*******                                  3
    3911.789 |*****                                    2
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.29 per sec.)
    queries:                             100    (0.29 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          349.7696s
    total number of events:              100

Latency (ms):
         min:                                 3135.96
         avg:                                 3497.67
         max:                                 3928.22
         95th percentile:                     3773.42
         sum:                               349767.47

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   349.7675/0.00
{code}

h2. Query Without UNION ALL: Q2

{code:java}
MariaDB [bts]> select count(*) from (select * from flights) as Q2;
+----------+
| count(*) |
+----------+
| 38083735 |
+----------+
1 row in set (1.049 sec)
{code}

I run this SQL Query Q2 10 times in the release build: the average runtime is *0.65s*.

h2. The Comparison Between Q1 and Q2 From Flight Dataset

Q1 AVG Runtime: 3.49s
Q2 AVG Runtime: 0.65s
The Runtime Slowdown Ratio: 4.92x
But the size of Q1 is doubled from the size of Q2.

The conclusion is there exists a runtime overhead due to the computation of the UNION ALL operator inside of the columnstore engine.
",1,"h1. The Preimage: Before The Performance Improvement

h2. Enviorment

Instance type: c5.4xlarge && Debian 11 && 30GiB EBS SSD

h2. Codebase

server codebase: 10.8 branch
columnstore codebase: URL

h2. Dataset

URL

{code:java}
MariaDB [bts]> describe flights;
+---------------------+-------------+------+-----+---------+-------+
| Field               | Type        | Null | Key | Default | Extra |
+---------------------+-------------+------+-----+---------+-------+
| year                | smallint(6) | YES  |     | NULL    |       |
| month               | tinyint(4)  | YES  |     | NULL    |       |
| day                 | tinyint(4)  | YES  |     | NULL    |       |
| day_of_week         | tinyint(4)  | YES  |     | NULL    |       |
| fl_date             | date        | YES  |     | NULL    |       |
| carrier             | varchar(2)  | YES  |     | NULL    |       |
| tail_num            | varchar(6)  | YES  |     | NULL    |       |
| fl_num              | smallint(6) | YES  |     | NULL    |       |
| origin              | varchar(5)  | YES  |     | NULL    |       |
| dest                | varchar(5)  | YES  |     | NULL    |       |
| crs_dep_time        | varchar(4)  | YES  |     | NULL    |       |
| dep_time            | varchar(4)  | YES  |     | NULL    |       |
| dep_delay           | smallint(6) | YES  |     | NULL    |       |
| taxi_out            | smallint(6) | YES  |     | NULL    |       |
| wheels_off          | varchar(4)  | YES  |     | NULL    |       |
| wheels_on           | varchar(4)  | YES  |     | NULL    |       |
| taxi_in             | smallint(6) | YES  |     | NULL    |       |
| crs_arr_time        | varchar(4)  | YES  |     | NULL    |       |
| arr_time            | varchar(4)  | YES  |     | NULL    |       |
| arr_delay           | smallint(6) | YES  |     | NULL    |       |
| cancelled           | smallint(6) | YES  |     | NULL    |       |
| cancellation_code   | smallint(6) | YES  |     | NULL    |       |
| diverted            | smallint(6) | YES  |     | NULL    |       |
| crs_elapsed_time    | smallint(6) | YES  |     | NULL    |       |
| actual_elapsed_time | smallint(6) | YES  |     | NULL    |       |
| air_time            | smallint(6) | YES  |     | NULL    |       |
| distance            | smallint(6) | YES  |     | NULL    |       |
| carrier_delay       | smallint(6) | YES  |     | NULL    |       |
| weather_delay       | smallint(6) | YES  |     | NULL    |       |
| nas_delay           | smallint(6) | YES  |     | NULL    |       |
| security_delay      | smallint(6) | YES  |     | NULL    |       |
| late_aircraft_delay | smallint(6) | YES  |     | NULL    |       |
+---------------------+-------------+------+-----+---------+-------+
32 rows in set (0.000 sec)

MariaDB [bts]> select count(*) from INFORMATION_SCHEMA.COLUMNS where table_name = ""flights"";
+----------+
| count(*) |
+----------+
|       32 |
+----------+
1 row in set (0.003 sec)
MariaDB [bts]> select count(*) from flights;
+----------+
| count(*) |
+----------+
| 38083735 |
+----------+
1 row in set (0.166 sec)
{code}

In this work, the table _flights_ is focused, which has 32 columns and over 38M tuples.

h2. Query With UNION ALL: Q1

{code:java}
MariaDB [bts]> select count(*) from (select * from flights union all select * from flights) as Q1;
+----------+
| count(*) |
+----------+
| 76167470 |
+----------+
1 row in set (3.011 sec)
{code}
I run this SQL Query Q1 100 times in the release build: the average runtime is *3.49s*.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    3151.619 |*****                                    2
    3208.883 |*******                                  3
    3267.187 |************************                 10
    3326.551 |*********************                    9
    3386.993 |****************                         7
    3448.533 |*********************************        14
    3511.192 |*********************************        14
    3574.989 |**************************************** 17
    3639.945 |************************                 10
    3706.081 |**************                           6
    3773.420 |*******                                  3
    3841.981 |*******                                  3
    3911.789 |*****                                    2
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.29 per sec.)
    queries:                             100    (0.29 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          349.7696s
    total number of events:              100

Latency (ms):
         min:                                 3135.96
         avg:                                 3497.67
         max:                                 3928.22
         95th percentile:                     3773.42
         sum:                               349767.47

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   349.7675/0.00
{code}

h2. Query Without UNION ALL: Q2

{code:java}
MariaDB [bts]> select count(*) from (select * from flights) as Q2;
+----------+
| count(*) |
+----------+
| 38083735 |
+----------+
1 row in set (1.049 sec)
{code}

I run this SQL Query Q2 10 times in the release build: the average runtime is *0.65s*.

h2. The Comparison Between Q1 and Q2 From Flight Dataset

Q1 AVG Runtime: 3.49s
Q2 AVG Runtime: 0.65s
The Runtime Slowdown Ratio: 4.92x
But the size of Q1 is doubled from the size of Q2.

The conclusion is there exists a runtime overhead due to the computation of the UNION ALL operator inside of the columnstore engine.
"
1038,MCOL-4590,MCOL,Jigao Luo,226465,2022-06-10 13:32:05,"h1. Simple Approach on switch AND change on addToOutput

In this commit: 

The flight table is specially fixed with switch cases: https://github.com/mariadb-corporation/mariadb-columnstore-engine/commit/17f9ffe11a1398f8a32b5dfb7980e3666c4a1fb9

h2. Query With UNION ALL: Q1

{code:java}
MariaDB [bts]> 
select count(*) from (select * from flights union all select * from flights) as Q1;
+----------+
| count(*) |
+----------+
| 76167470 |
+----------+
1 row in set (3.945 sec)

{code}


I run this SQL Query Q1 100 times in the release build: the average runtime is *3.44s*.
It is basically a minor performance improvement over the baseline.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1304.208 |***                                      2
    1327.905 |**************                           9
    1352.033 |***********************                  15
    1376.599 |********************************         21
    1401.611 |**************************************** 26
    1427.078 |******************                       12
    1453.007 |*********                                6
    1479.408 |******                                   4
    1506.288 |***                                      2
    1533.657 |**                                       1
    1589.895 |**                                       1
    1618.783 |**                                       1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.72 per sec.)
    queries:                             100    (0.72 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          139.6992s
    total number of events:              100

Latency (ms):
         min:                                 1306.24
         avg:                                 1396.97
         max:                                 1609.73
         95th percentile:                     1479.41
         sum:                               139697.21

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   139.6972/0.00
{code}",2,"h1. Simple Approach on switch AND change on addToOutput

In this commit: 

The flight table is specially fixed with switch cases: URL

h2. Query With UNION ALL: Q1

{code:java}
MariaDB [bts]> 
select count(*) from (select * from flights union all select * from flights) as Q1;
+----------+
| count(*) |
+----------+
| 76167470 |
+----------+
1 row in set (3.945 sec)

{code}


I run this SQL Query Q1 100 times in the release build: the average runtime is *3.44s*.
It is basically a minor performance improvement over the baseline.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1304.208 |***                                      2
    1327.905 |**************                           9
    1352.033 |***********************                  15
    1376.599 |********************************         21
    1401.611 |**************************************** 26
    1427.078 |******************                       12
    1453.007 |*********                                6
    1479.408 |******                                   4
    1506.288 |***                                      2
    1533.657 |**                                       1
    1589.895 |**                                       1
    1618.783 |**                                       1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.72 per sec.)
    queries:                             100    (0.72 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          139.6992s
    total number of events:              100

Latency (ms):
         min:                                 1306.24
         avg:                                 1396.97
         max:                                 1609.73
         95th percentile:                     1479.41
         sum:                               139697.21

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   139.6972/0.00
{code}"
1039,MCOL-4590,MCOL,Jigao Luo,229024,2022-07-09 05:41:00,"h1. Simple Approach WITH change on addToOutput

In this commit: https://github.com/mariadb-corporation/mariadb-columnstore-engine/commit/75987029c277aba42c9920508e23cb1c6480bf2c

h2. Query With UNION ALL: Q1

`admin@ip-172-31-28-183:~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1376.599 |*                                        1
    1401.611 |*********                                7
    1427.078 |********************                     16
    1453.007 |**************************************** 32
    1479.408 |******************************           24
    1506.288 |***************                          12
    1533.657 |******                                   5
    1561.523 |***                                      2
    1618.783 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.68 per sec.)
    queries:                             100    (0.68 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          146.5432s
    total number of events:              100

Latency (ms):
         min:                                 1388.30
         avg:                                 1465.41
         max:                                 1612.34
         95th percentile:                     1533.66
         sum:                               146541.11

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   146.5411/0.00
{code}",3,"h1. Simple Approach WITH change on addToOutput

In this commit: URL

h2. Query With UNION ALL: Q1

`admin@ip-172-31-28-183:~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1376.599 |*                                        1
    1401.611 |*********                                7
    1427.078 |********************                     16
    1453.007 |**************************************** 32
    1479.408 |******************************           24
    1506.288 |***************                          12
    1533.657 |******                                   5
    1561.523 |***                                      2
    1618.783 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.68 per sec.)
    queries:                             100    (0.68 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          146.5432s
    total number of events:              100

Latency (ms):
         min:                                 1388.30
         avg:                                 1465.41
         max:                                 1612.34
         95th percentile:                     1533.66
         sum:                               146541.11

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   146.5411/0.00
{code}"
1040,MCOL-4590,MCOL,Jigao Luo,229025,2022-07-09 06:53:40,"h1. Baseline WITH change on addToOutput

In this commit: https://github.com/cakebytheoceanLuo/mariadb-columnstore-engine/commits/MCOL-4590-baseline

h2. Query With UNION ALL: Q1

`admin@ip-172-31-28-183:~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1304.208 |********                                 5
    1327.905 |*********************                    13
    1352.033 |**************************************** 25
    1376.599 |******************************           19
    1401.611 |***************************              17
    1427.078 |****************                         10
    1453.007 |**                                       1
    1479.408 |***********                              7
    1506.288 |***                                      2
    1533.657 |**                                       1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.72 per sec.)
    queries:                             100    (0.72 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          138.1056s
    total number of events:              100

Latency (ms):
         min:                                 1299.39
         avg:                                 1381.04
         max:                                 1521.33
         95th percentile:                     1479.41
         sum:                               138103.60

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   138.1036/0.00
{code}",4,"h1. Baseline WITH change on addToOutput

In this commit: URL

h2. Query With UNION ALL: Q1

`admin@ip-172-31-28-183:~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1304.208 |********                                 5
    1327.905 |*********************                    13
    1352.033 |**************************************** 25
    1376.599 |******************************           19
    1401.611 |***************************              17
    1427.078 |****************                         10
    1453.007 |**                                       1
    1479.408 |***********                              7
    1506.288 |***                                      2
    1533.657 |**                                       1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.72 per sec.)
    queries:                             100    (0.72 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          138.1056s
    total number of events:              100

Latency (ms):
         min:                                 1299.39
         avg:                                 1381.04
         max:                                 1521.33
         95th percentile:                     1479.41
         sum:                               138103.60

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   138.1036/0.00
{code}"
1041,MCOL-4590,MCOL,Jigao Luo,230905,2022-07-28 13:40:53,"h1. Midterm

In this commit: https://github.com/cakebytheoceanLuo/mariadb-columnstore-engine/commit/f92831d990ed4c18768ec1add3db970bad411ac2

h2. Query With UNION ALL: Q1

`admin@ip-172-31-28-183:~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1304.208 |*                                        1
    1327.905 |*********                                6
    1352.033 |************                             8
    1376.599 |**************************************** 27
    1401.611 |***************************              18
    1427.078 |*******************************          21
    1453.007 |*************                            9
    1479.408 |******                                   4
    1506.288 |***                                      2
    1533.657 |*                                        1
    1561.523 |*                                        1
    1589.895 |*                                        1
    1678.143 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.71 per sec.)
    queries:                             100    (0.71 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          140.8248s
    total number of events:              100

Latency (ms):
         min:                                 1305.74
         avg:                                 1408.23
         max:                                 1677.00
         95th percentile:                     1506.29
         sum:                               140822.79

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   140.8228/0.00
{code}",5,"h1. Midterm

In this commit: URL

h2. Query With UNION ALL: Q1

`admin@ip-172-31-28-183:~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1304.208 |*                                        1
    1327.905 |*********                                6
    1352.033 |************                             8
    1376.599 |**************************************** 27
    1401.611 |***************************              18
    1427.078 |*******************************          21
    1453.007 |*************                            9
    1479.408 |******                                   4
    1506.288 |***                                      2
    1533.657 |*                                        1
    1561.523 |*                                        1
    1589.895 |*                                        1
    1678.143 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.71 per sec.)
    queries:                             100    (0.71 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          140.8248s
    total number of events:              100

Latency (ms):
         min:                                 1305.74
         avg:                                 1408.23
         max:                                 1677.00
         95th percentile:                     1506.29
         sum:                               140822.79

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   140.8228/0.00
{code}"
1042,MCOL-4590,MCOL,Jigao Luo,235085,2022-09-15 13:06:10,"h1.  Performance Testing of The PR Fixing This Issue

You can find the same report following at this GitHub PR page: https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/2528#issuecomment-1242706499

h2.  Experiment Environment

The experiments are run on the following hardware configuration:
- AWS Instance type: c5.4xlarge
- Debian 11
- 30GiB EBS SSD

h2.  Dataset

The benchmark dataset is provided by the community: https://github.com/mariadb-corporation/mariadb-columnstore-samples/
In this work, the table *flights* is focused, which has 32 columns and over 38M tuples.

h3.  Schema

There are details of the table *flights*:
{code:java}
MariaDB [bts]> describe flights;
--
+---------------------+-------------+------+-----+---------+-------+
\| Field               \| Type        \| Null \| Key \| Default \| Extra \|
+---------------------+-------------+------+-----+---------+-------+
\| year                \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| month               \| tinyint(4)  \| YES  \|     \| NULL    \|       \|
\| day                 \| tinyint(4)  \| YES  \|     \| NULL    \|       \|
\| day_of_week         \| tinyint(4)  \| YES  \|     \| NULL    \|       \|
\| fl_date             \| date        \| YES  \|     \| NULL    \|       \|
\| carrier             \| varchar(2)  \| YES  \|     \| NULL    \|       \|
\| tail_num            \| varchar(6)  \| YES  \|     \| NULL    \|       \|
\| fl_num              \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| origin              \| varchar(5)  \| YES  \|     \| NULL    \|       \|
\| dest                \| varchar(5)  \| YES  \|     \| NULL    \|       \|
\| crs_dep_time        \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| dep_time            \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| dep_delay           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| taxi_out            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| wheels_off          \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| wheels_on           \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| taxi_in             \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| crs_arr_time        \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| arr_time            \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| arr_delay           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| cancelled           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| cancellation_code   \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| diverted            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| crs_elapsed_time    \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| actual_elapsed_time \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| air_time            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| distance            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| carrier_delay       \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| weather_delay       \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| nas_delay           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| security_delay      \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| late_aircraft_delay \| smallint(6) \| YES  \|     \| NULL    \|       \|
+---------------------+-------------+------+-----+---------+-------+
32 rows in set (0.000 sec)
 
MariaDB [bts]> select count(*) from INFORMATION_SCHEMA.COLUMNS where table_name = ""flights"";
+----------+
\| count(*) \|
+----------+
\|       32 \|
+----------+
1 row in set (0.003 sec)
MariaDB [bts]> select count(*) from flights;
+----------+
\| count(*) \|
+----------+
\| 38083735 \|
+----------+
1 row in set (0.166 sec)
{code}

h3.  Benchmark Query Q1

The following Query Q1 is the benchmark query in our experiments and the query to be optimized.
{code:java}
MariaDB [bts]> select count(*) from (select * from flights union all select * from flights) as Q1;
--
+----------+
\| count(*) \|
+----------+
\| 76167470 \|
+----------+
1 row in set (3.011 sec)
{code}

h3.  Benchmark Query Q2

The following Query Q2 has no `UNION` statement.  Ideally, Q1 should be close to 2x runtime of Q2.

{code:java}
MariaDB [bts]> select count(*) from (select * from flights) as Q2;
--
+----------+
\| count(*) \|
+----------+
\| 38083735 \|
+----------+
1 row in set (1.049 sec)
{code}

h2.  Benchmark Tool

The benchmark tool & script are provided by the community: https://github.com/drrtuy/cs-docker-tools

Here is how I run the Q1: `~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`. The `slapunion.lua bts` is loaded only with Q1.
The way to benchmark Q2 is similar.

h2.  Q2 Performance

The average runtime of Q2 is **632.42ms**.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
     623.335 |**************************************** 63
     634.661 |*************                            21
     646.192 |******                                   10
     657.933 |**                                       3
     694.452 |**                                       3
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (1.58 per sec.)
    queries:                             100    (1.58 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          63.2444s
    total number of events:              100

Latency (ms):
         min:                                  623.54
         avg:                                  632.42
         max:                                  698.28
         95th percentile:                      657.93
         sum:                                63242.44

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   63.2424/0.00
{code}

h2.  Q1 Performance Without This PR

I benchmark with this commit https://github.com/mariadb-corporation/mariadb-columnstore-engine/commits/develop, which is the last commit and this PR is based on.

The average runtime of Q1 without the optimization of this PR is **3229.35ms**.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    2828.869 |***                                      1
    2880.269 |******                                   2
    2932.602 |***********                              4
    2985.887 |**************                           5
    3040.139 |*******************************          11
    3095.377 |***********************                  8
    3151.619 |***********************                  8
    3208.883 |*******************************          11
    3267.187 |*************************************    13
    3326.551 |**************************************** 14
    3386.993 |**************************               9
    3448.533 |*****************                        6
    3511.192 |*********                                3
    3574.989 |***                                      1
    3639.945 |*********                                3
    3706.081 |***                                      1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.31 per sec.)
    queries:                             100    (0.31 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          322.9365s
    total number of events:              100

Latency (ms):
         min:                                 2831.68
         avg:                                 3229.35
         max:                                 3707.17
         95th percentile:                     3511.19
         sum:                               322934.52

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   322.9345/0.00
{code}

h2.  Q1 Performance With This PR 

The average runtime of Q1 with the optimization of this PR is **1312.31ms**.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1280.934 |*************************                30
    1304.208 |**************************************** 48
    1327.905 |*********                                11
    1352.033 |****                                     5
    1376.599 |**                                       2
    1401.611 |*                                        1
    1427.078 |*                                        1
    1453.007 |*                                        1
    1771.289 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.76 per sec.)
    queries:                             100    (0.76 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          131.2332s
    total number of events:              100

Latency (ms):
         min:                                 1285.18
         avg:                                 1312.31
         max:                                 1768.78
         95th percentile:                     1376.60
         sum:                               131231.23

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   131.2312/0.00
{code}

h2.  Summary

Q2 AVG Runtime: 0.63s
Q1 AVG Runtime: 3.23s
Optimized Q1 AVG Runtime: 1.31s

The *Runtime Slowdown Ratio* of Q1 and Q2 is ~5x which means the Q1 has more than 5 times the runtime of Q2.  Ideally, the Runtime Slowdown Ratio should be close to 2. The current `develop` branch has a very inefficient UNION processing logic with overhead.

Applying this patch, the runtime of Q1 is optimized to 1.31s, resulting in the Runtime Slowdown Ratio of 2.07. This ratio is very close to the ideal ratio. Moreover, the theoretical minimum is 2, which makes it impossible to optimize this ratio under 2.

In summary, I have optimized the UNION processing in ColumnStore. The performance improvement is satisfying and close to a theoretical limit.",6,"h1.  Performance Testing of The PR Fixing This Issue

You can find the same report following at this GitHub PR page: URL

h2.  Experiment Environment

The experiments are run on the following hardware configuration:
- AWS Instance type: c5.4xlarge
- Debian 11
- 30GiB EBS SSD

h2.  Dataset

The benchmark dataset is provided by the community: URL
In this work, the table *flights* is focused, which has 32 columns and over 38M tuples.

h3.  Schema

There are details of the table *flights*:
{code:java}
MariaDB [bts]> describe flights;
--
+---------------------+-------------+------+-----+---------+-------+
\| Field               \| Type        \| Null \| Key \| Default \| Extra \|
+---------------------+-------------+------+-----+---------+-------+
\| year                \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| month               \| tinyint(4)  \| YES  \|     \| NULL    \|       \|
\| day                 \| tinyint(4)  \| YES  \|     \| NULL    \|       \|
\| day_of_week         \| tinyint(4)  \| YES  \|     \| NULL    \|       \|
\| fl_date             \| date        \| YES  \|     \| NULL    \|       \|
\| carrier             \| varchar(2)  \| YES  \|     \| NULL    \|       \|
\| tail_num            \| varchar(6)  \| YES  \|     \| NULL    \|       \|
\| fl_num              \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| origin              \| varchar(5)  \| YES  \|     \| NULL    \|       \|
\| dest                \| varchar(5)  \| YES  \|     \| NULL    \|       \|
\| crs_dep_time        \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| dep_time            \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| dep_delay           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| taxi_out            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| wheels_off          \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| wheels_on           \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| taxi_in             \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| crs_arr_time        \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| arr_time            \| varchar(4)  \| YES  \|     \| NULL    \|       \|
\| arr_delay           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| cancelled           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| cancellation_code   \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| diverted            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| crs_elapsed_time    \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| actual_elapsed_time \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| air_time            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| distance            \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| carrier_delay       \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| weather_delay       \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| nas_delay           \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| security_delay      \| smallint(6) \| YES  \|     \| NULL    \|       \|
\| late_aircraft_delay \| smallint(6) \| YES  \|     \| NULL    \|       \|
+---------------------+-------------+------+-----+---------+-------+
32 rows in set (0.000 sec)
 
MariaDB [bts]> select count(*) from INFORMATION_SCHEMA.COLUMNS where table_name = ""flights"";
+----------+
\| count(*) \|
+----------+
\|       32 \|
+----------+
1 row in set (0.003 sec)
MariaDB [bts]> select count(*) from flights;
+----------+
\| count(*) \|
+----------+
\| 38083735 \|
+----------+
1 row in set (0.166 sec)
{code}

h3.  Benchmark Query Q1

The following Query Q1 is the benchmark query in our experiments and the query to be optimized.
{code:java}
MariaDB [bts]> select count(*) from (select * from flights union all select * from flights) as Q1;
--
+----------+
\| count(*) \|
+----------+
\| 76167470 \|
+----------+
1 row in set (3.011 sec)
{code}

h3.  Benchmark Query Q2

The following Query Q2 has no `UNION` statement.  Ideally, Q1 should be close to 2x runtime of Q2.

{code:java}
MariaDB [bts]> select count(*) from (select * from flights) as Q2;
--
+----------+
\| count(*) \|
+----------+
\| 38083735 \|
+----------+
1 row in set (1.049 sec)
{code}

h2.  Benchmark Tool

The benchmark tool & script are provided by the community: URL

Here is how I run the Q1: `~/cs-docker-tools/slapit$ sudo ./sysbench_wrapper.sh 1 100 slapunion.lua bts`. The `slapunion.lua bts` is loaded only with Q1.
The way to benchmark Q2 is similar.

h2.  Q2 Performance

The average runtime of Q2 is **632.42ms**.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
     623.335 |**************************************** 63
     634.661 |*************                            21
     646.192 |******                                   10
     657.933 |**                                       3
     694.452 |**                                       3
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (1.58 per sec.)
    queries:                             100    (1.58 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          63.2444s
    total number of events:              100

Latency (ms):
         min:                                  623.54
         avg:                                  632.42
         max:                                  698.28
         95th percentile:                      657.93
         sum:                                63242.44

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   63.2424/0.00
{code}

h2.  Q1 Performance Without This PR

I benchmark with this commit URL which is the last commit and this PR is based on.

The average runtime of Q1 without the optimization of this PR is **3229.35ms**.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    2828.869 |***                                      1
    2880.269 |******                                   2
    2932.602 |***********                              4
    2985.887 |**************                           5
    3040.139 |*******************************          11
    3095.377 |***********************                  8
    3151.619 |***********************                  8
    3208.883 |*******************************          11
    3267.187 |*************************************    13
    3326.551 |**************************************** 14
    3386.993 |**************************               9
    3448.533 |*****************                        6
    3511.192 |*********                                3
    3574.989 |***                                      1
    3639.945 |*********                                3
    3706.081 |***                                      1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.31 per sec.)
    queries:                             100    (0.31 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          322.9365s
    total number of events:              100

Latency (ms):
         min:                                 2831.68
         avg:                                 3229.35
         max:                                 3707.17
         95th percentile:                     3511.19
         sum:                               322934.52

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   322.9345/0.00
{code}

h2.  Q1 Performance With This PR 

The average runtime of Q1 with the optimization of this PR is **1312.31ms**.

{code:java}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    1280.934 |*************************                30
    1304.208 |**************************************** 48
    1327.905 |*********                                11
    1352.033 |****                                     5
    1376.599 |**                                       2
    1401.611 |*                                        1
    1427.078 |*                                        1
    1453.007 |*                                        1
    1771.289 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            100
        write:                           0
        other:                           0
        total:                           100
    transactions:                        100    (0.76 per sec.)
    queries:                             100    (0.76 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          131.2332s
    total number of events:              100

Latency (ms):
         min:                                 1285.18
         avg:                                 1312.31
         max:                                 1768.78
         95th percentile:                     1376.60
         sum:                               131231.23

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   131.2312/0.00
{code}

h2.  Summary

Q2 AVG Runtime: 0.63s
Q1 AVG Runtime: 3.23s
Optimized Q1 AVG Runtime: 1.31s

The *Runtime Slowdown Ratio* of Q1 and Q2 is ~5x which means the Q1 has more than 5 times the runtime of Q2.  Ideally, the Runtime Slowdown Ratio should be close to 2. The current `develop` branch has a very inefficient UNION processing logic with overhead.

Applying this patch, the runtime of Q1 is optimized to 1.31s, resulting in the Runtime Slowdown Ratio of 2.07. This ratio is very close to the ideal ratio. Moreover, the theoretical minimum is 2, which makes it impossible to optimize this ratio under 2.

In summary, I have optimized the UNION processing in ColumnStore. The performance improvement is satisfying and close to a theoretical limit."
1043,MCOL-4603,MCOL,Roman,193972,2021-07-08 19:16:48,4QA This feature replaces long double as the internal data type for sum() and avg() on all integer and decimal types. The expected outcome is that nothing had been broken. It would be nice if you measure the performance impact. ,1,4QA This feature replaces long double as the internal data type for sum() and avg() on all integer and decimal types. The expected outcome is that nothing had been broken. It would be nice if you measure the performance impact. 
1044,MCOL-4603,MCOL,Gagan Goel,194016,2021-07-09 10:15:41,"This patch broke the following tests:
  in test001: working_tpch1_compareLogOnly/misc/sumavgoverflow.sql
  in MTR: columnstore/future.mcol641-aggregate

If they can't be fixed soon enough, we should back out the change from 6.1.1.",2,"This patch broke the following tests:
  in test001: working_tpch1_compareLogOnly/misc/sumavgoverflow.sql
  in MTR: columnstore/future.mcol641-aggregate

If they can't be fixed soon enough, we should back out the change from 6.1.1."
1045,MCOL-4603,MCOL,Roman,194078,2021-07-10 07:10:10,The previous comment is irrelevant now.,3,The previous comment is irrelevant now.
1046,MCOL-4603,MCOL,Daniel Lee,194214,2021-07-12 19:19:02,Build verified: 6.1.1 ( #2769),4,Build verified: 6.1.1 ( #2769)
1047,MCOL-461,MCOL,David Hill,89813,2016-12-21 16:25:54,"fixed in repo 1.0.6.1

commit b6a22feb6e519601c51df61b82c058f4b1d50336
Author: David Hill <david.hill@mariadb.com>
Date:   Wed Dec 21 10:25:14 2016 -0600

    MCOL-461 - remove -mp option

 oamapps/postConfigure/postConfigure.cpp | 19 ++-----------------
",1,"fixed in repo 1.0.6.1

commit b6a22feb6e519601c51df61b82c058f4b1d50336
Author: David Hill 
Date:   Wed Dec 21 10:25:14 2016 -0600

    MCOL-461 - remove -mp option

 oamapps/postConfigure/postConfigure.cpp | 19 ++-----------------
"
1048,MCOL-461,MCOL,David Hill,90433,2017-01-09 20:15:20,pull request done,2,pull request done
1049,MCOL-461,MCOL,David Hill,90655,2017-01-16 15:02:22,just check that the -mp option is removed in postConfigure,3,just check that the -mp option is removed in postConfigure
1050,MCOL-461,MCOL,Daniel Lee,90774,2017-01-18 15:09:27,"Build verified: Github source build

[root@localhost mariadb-columnstore-server]# git show
commit 83b0e5c54a644bc31461752cf73f0e1140586d39
Merge: b975814 93c1c7e
Author: david hill <david.hill@mariadb.com>
Date:   Thu Jan 12 09:27:28 2017 -0600

    Merge pull request #26 from mariadb-corporation/MCOL-500
    
    Update README.md

[root@localhost mariadb-columnstore-engine]# git show
commit c6799df6408c0e86ccba9ee63929a6b9ad4294bf
Merge: fa0fde9 2f3937a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Jan 13 21:03:35 2017 -0600

    Merge pull request #96 from mariadb-corporation/MCOL-505
    
    MCOL-505 Performance improvements to ExeMgr

[root@localhost bin]# ./postConfigure -h

This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System based on Operator inputs and
will perform a Package Installation of all of the Modules within the
System that is being configured.

IMPORTANT: This tool should only be run on a Performance Module Server,
           preferably Module #1

Instructions:

	Press 'enter' to accept a value in (), if available or
	Enter one of the options within [], if available, or
	Enter a new value


Usage: postConfigure [-h][-c][-u][-p][-s][-port][-i]
   -h  Help
   -c  Config File to use to extract configuration data, default is Columnstore.xml.rpmsave
   -u  Upgrade, Install using the Config File from -c, default to Columnstore.xml.rpmsave
	    If ssh-keys aren't setup, you should provide passwords as command line arguments
   -p  Unix Password, used with no-prompting option
   -s  Single Threaded Remote Install
   -port MariaDB ColumnStore Port Address
   -i Non-root Install directory, Only use for non-root installs
[root@localhost bin]# 
[root@localhost bin]# 
[root@localhost bin]# ./postConfigure -mp
   ERROR: Invalid Argument = -mp
   Usage: postConfigure [-h][-c][-u][-p][-s][-port][-i]
",4,"Build verified: Github source build

[root@localhost mariadb-columnstore-server]# git show
commit 83b0e5c54a644bc31461752cf73f0e1140586d39
Merge: b975814 93c1c7e
Author: david hill 
Date:   Thu Jan 12 09:27:28 2017 -0600

    Merge pull request #26 from mariadb-corporation/MCOL-500
    
    Update README.md

[root@localhost mariadb-columnstore-engine]# git show
commit c6799df6408c0e86ccba9ee63929a6b9ad4294bf
Merge: fa0fde9 2f3937a
Author: Andrew Hutchings 
Date:   Fri Jan 13 21:03:35 2017 -0600

    Merge pull request #96 from mariadb-corporation/MCOL-505
    
    MCOL-505 Performance improvements to ExeMgr

[root@localhost bin]# ./postConfigure -h

This is the MariaDB ColumnStore System Configuration and Installation tool.
It will Configure the MariaDB ColumnStore System based on Operator inputs and
will perform a Package Installation of all of the Modules within the
System that is being configured.

IMPORTANT: This tool should only be run on a Performance Module Server,
           preferably Module #1

Instructions:

	Press 'enter' to accept a value in (), if available or
	Enter one of the options within [], if available, or
	Enter a new value


Usage: postConfigure [-h][-c][-u][-p][-s][-port][-i]
   -h  Help
   -c  Config File to use to extract configuration data, default is Columnstore.xml.rpmsave
   -u  Upgrade, Install using the Config File from -c, default to Columnstore.xml.rpmsave
	    If ssh-keys aren't setup, you should provide passwords as command line arguments
   -p  Unix Password, used with no-prompting option
   -s  Single Threaded Remote Install
   -port MariaDB ColumnStore Port Address
   -i Non-root Install directory, Only use for non-root installs
[root@localhost bin]# 
[root@localhost bin]# 
[root@localhost bin]# ./postConfigure -mp
   ERROR: Invalid Argument = -mp
   Usage: postConfigure [-h][-c][-u][-p][-s][-port][-i]
"
1051,MCOL-462,MCOL,David Hill,89569,2016-12-14 21:46:11,"change code to use the ENV variables instead of reading the access/secret keys from a file.
These can be passed from an IAM role that is provide when the instance is launched or setup in the instances themselves.

AWS_ACCESS_KEY environment variable
AWS_SECRET_KEY environment variable",1,"change code to use the ENV variables instead of reading the access/secret keys from a file.
These can be passed from an IAM role that is provide when the instance is launched or setup in the instances themselves.

AWS_ACCESS_KEY environment variable
AWS_SECRET_KEY environment variable"
1052,MCOL-462,MCOL,David Hill,89877,2016-12-23 21:42:43,"change to use the latest amazon APIs called aws ec2 cli commands. Previous ones being used was brought over from InfiniDB, but they were obsolete.",2,"change to use the latest amazon APIs called aws ec2 cli commands. Previous ones being used was brought over from InfiniDB, but they were obsolete."
1053,MCOL-462,MCOL,David Hill,90366,2017-01-06 21:53:05,great progress - I got a beta version of the AMI done which uses IAM certificates. Plan is to have the customer test it out to make sure it works for there IAM user/role setup and it meets their needs. And they can provide is feedback before making it public in 1.0.7..,3,great progress - I got a beta version of the AMI done which uses IAM certificates. Plan is to have the customer test it out to make sure it works for there IAM user/role setup and it meets their needs. And they can provide is feedback before making it public in 1.0.7..
1054,MCOL-462,MCOL,David Hill,90434,2017-01-09 20:18:49,pull request done,4,pull request done
1055,MCOL-462,MCOL,David Hill,90656,2017-01-16 15:06:07,"Positive test cases for multi-node installs:

1. use the IAM rol oe 'mcs' when launching, second page of new instance startup
2. dont provide any IAM role, but enter the access/secret keys in the .aws/certificate file, rename from certificate.template

Negitive test case

1. Don't provide a IAm role or certificate file and do a multi-node install and select 'y' to use the AWS CLI APIs. You should get an error",5,"Positive test cases for multi-node installs:

1. use the IAM rol oe 'mcs' when launching, second page of new instance startup
2. dont provide any IAM role, but enter the access/secret keys in the .aws/certificate file, rename from certificate.template

Negitive test case

1. Don't provide a IAm role or certificate file and do a multi-node install and select 'y' to use the AWS CLI APIs. You should get an error"
1056,MCOL-462,MCOL,David Hill,90963,2017-01-23 17:23:36,"Passed, can now launch AMI using IAM roles",6,"Passed, can now launch AMI using IAM roles"
1057,MCOL-462,MCOL,Abhinav santi,93540,2017-03-28 01:02:12,"Do we need to provide both IAM role and Access keys while installing multi server system? I faced this issue while installing Maria column store 1.7 using AMI in AWS US-EAST-1 region. Is this expected? If yes, can you please provide me the rationale. 
Thank you in advance.",7,"Do we need to provide both IAM role and Access keys while installing multi server system? I faced this issue while installing Maria column store 1.7 using AMI in AWS US-EAST-1 region. Is this expected? If yes, can you please provide me the rationale. 
Thank you in advance."
1058,MCOL-462,MCOL,David Hill,93555,2017-03-28 14:02:26,"Let me try to clarify. 

The user has the option of either setting up IAM users and roles where the Access and Secret keys are automatically created and assigned. Then our code will read the keys from the meta-data at Process start on the instances.
Or the user create a set of Access and Secret keys and can add them to a local file in the AMI Instance. Then our code will read the keys from that file, if no keys are provided in the meta-data.

It is explained here in this Amazon AMI installation document:

https://mariadb.com/kb/en/mariadb/installing-and-configuring-a-columnstore-system-using-the-amazon-ami/

Also just an fyi - We have recently released a 1.0.8 version of the AMI.

https://mariadb.com/kb/en/mariadb/mariadb-columnstore-108-ga-release-notes/
",8,"Let me try to clarify. 

The user has the option of either setting up IAM users and roles where the Access and Secret keys are automatically created and assigned. Then our code will read the keys from the meta-data at Process start on the instances.
Or the user create a set of Access and Secret keys and can add them to a local file in the AMI Instance. Then our code will read the keys from that file, if no keys are provided in the meta-data.

It is explained here in this Amazon AMI installation document:

URL

Also just an fyi - We have recently released a 1.0.8 version of the AMI.

URL
"
1059,MCOL-4624,MCOL,Denis Khalikov,183315,2021-03-23 12:42:14,"This is implemented, algorithm is simple, starting from the last block in segment file, and searching for the first non empty row in block.
We need to  start from the last block, because bulk inserts to file starting from the new block, so it could be a empty block.",1,"This is implemented, algorithm is simple, starting from the last block in segment file, and searching for the first non empty row in block.
We need to  start from the last block, because bulk inserts to file starting from the new block, so it could be a empty block."
1060,MCOL-4635,MCOL,Denis Khalikov,183716,2021-03-25 16:15:27,"Add support for `bulk` and 2 extent per one segment file.
Actually bulk can create even more than 2 extents per segment file, it could be updated, if is needed, to support more.",1,"Add support for `bulk` and 2 extent per one segment file.
Actually bulk can create even more than 2 extents per segment file, it could be updated, if is needed, to support more."
1061,MCOL-4654,MCOL,Denis Khalikov,184825,2021-04-05 17:04:09,Closed because of `subtask` requirements.,1,Closed because of `subtask` requirements.
1062,MCOL-4679,MCOL,Roman,186304,2021-04-16 09:12:50,Plz review.,1,Plz review.
1063,MCOL-4679,MCOL,Roman,186541,2021-04-19 15:30:02,Now. We need to address CMAPI part.,2,Now. We need to address CMAPI part.
1064,MCOL-4679,MCOL,Roman,190375,2021-05-26 14:54:19,CMAPI when adds a node creates additional PMSX sections in Columnstore.xml where X = number of nodes * PrimitiveServers.ConnectionsPerPrimProc parameter value from Columnstore.xml. [~leonid.fedorov] plz remove the multiplication so that there will be only N PMSX sections in Columnstore.xml where N = number of nodes. ,3,CMAPI when adds a node creates additional PMSX sections in Columnstore.xml where X = number of nodes * PrimitiveServers.ConnectionsPerPrimProc parameter value from Columnstore.xml. [~leonid.fedorov] plz remove the multiplication so that there will be only N PMSX sections in Columnstore.xml where N = number of nodes. 
1065,MCOL-4679,MCOL,Roman,192277,2021-06-18 14:53:37,[~gdorman] Yes it is.,4,[~gdorman] Yes it is.
1066,MCOL-4679,MCOL,Roman,192289,2021-06-18 17:03:32,Plz review.,5,Plz review.
1067,MCOL-4679,MCOL,Roman,192831,2021-06-25 19:19:51,"4QA One needs to have a single PMS section in Columnstore.xml per PM and set PrimitiveServers.ConnectionsPerPrimProc to whatever number of connections needed. EM automatically create PrimitiveServers.ConnectionsPerPrimProc connections in its startup.
Start MCS count the output lines for ss -tenp | grep PrimProc. Set PrimitiveServers.ConnectionsPerPrimProc to 10, restart MCS and count lines for ss -tenp | grep PrimProc again.",6,"4QA One needs to have a single PMS section in Columnstore.xml per PM and set PrimitiveServers.ConnectionsPerPrimProc to whatever number of connections needed. EM automatically create PrimitiveServers.ConnectionsPerPrimProc connections in its startup.
Start MCS count the output lines for ss -tenp | grep PrimProc. Set PrimitiveServers.ConnectionsPerPrimProc to 10, restart MCS and count lines for ss -tenp | grep PrimProc again."
1068,MCOL-4679,MCOL,Daniel Lee,192983,2021-06-28 19:29:41,"Build tested: 6.1.1 ( Drone #2640), CMAPI ( Drone #502 )

CMAPI DELETE is not update PMS<x> in Columnstore.xml correctly

after installation
{noformat}
	<PMS1>
        <IPAddr>s1pm1</IPAddr>
        <Port>8620</Port>
    </PMS1>
    <PMS2>
        <IPAddr>s1pm2</IPAddr>
        <Port>8620</Port>
    </PMS2>
    <PMS3>
        <IPAddr>s1pm3</IPAddr>
        <Port>8620</Port>
    </PMS3>
{noformat}
After deleting node s1pm3
There should be 1 entry per PM, a total of 2
But there are 2 entries per PM, a total of 4
{noformat}
    <PMS1>
        <IPAddr>s1pm1</IPAddr>
        <Port>8620</Port>
    </PMS1>
    <PMS2>
        <IPAddr>s1pm2</IPAddr>
        <Port>8620</Port>
    </PMS2>
    <PMS3>
        <IPAddr>s1pm1</IPAddr>
        <Port>8620</Port>
    </PMS3>
    <PMS4>
        <IPAddr>s1pm2</IPAddr>
        <Port>8620</Port>
    </PMS4>
{noformat}
    After putting back node s1pm3
{noformat}
    <PMS1>
        <IPAddr>s1pm1</IPAddr>
        <Port>8620</Port>
    </PMS1>
    <PMS2>
        <IPAddr>s1pm2</IPAddr>
        <Port>8620</Port>
    </PMS2>
    <PMS3>
        <IPAddr>s1pm3</IPAddr>
        <Port>8620</Port>
    </PMS3>
{noformat}",7,"Build tested: 6.1.1 ( Drone #2640), CMAPI ( Drone #502 )

CMAPI DELETE is not update PMS in Columnstore.xml correctly

after installation
{noformat}
	
        s1pm1
        8620
    
    
        s1pm2
        8620
    
    
        s1pm3
        8620
    
{noformat}
After deleting node s1pm3
There should be 1 entry per PM, a total of 2
But there are 2 entries per PM, a total of 4
{noformat}
    
        s1pm1
        8620
    
    
        s1pm2
        8620
    
    
        s1pm1
        8620
    
    
        s1pm2
        8620
    
{noformat}
    After putting back node s1pm3
{noformat}
    
        s1pm1
        8620
    
    
        s1pm2
        8620
    
    
        s1pm3
        8620
    
{noformat}"
1069,MCOL-4679,MCOL,Daniel Lee,192986,2021-06-28 20:05:23,"Build tested: 6.1.1 ( Drone #2640), CMAPI ( Drone #502 )

Number of connections don't seem to be correspond to what's configured.
Used ""ss -tenp |grep PrimProc""

After installation
ConnectionsPerPrimProc=2
pm1=7
pm2=10
pm3=7

After change
ConnectionsPerPrimProc=5
pm1=7
pm2=21
pm3=7
",8,"Build tested: 6.1.1 ( Drone #2640), CMAPI ( Drone #502 )

Number of connections don't seem to be correspond to what's configured.
Used ""ss -tenp |grep PrimProc""

After installation
ConnectionsPerPrimProc=2
pm1=7
pm2=10
pm3=7

After change
ConnectionsPerPrimProc=5
pm1=7
pm2=21
pm3=7
"
1070,MCOL-4679,MCOL,Roman,193043,2021-06-29 11:25:41,[~dleeyh] The command 'ss -tenp |grep PrimProc' returns all connections that has a line PrimProc so there are additional connections at PP2. If you checked its output right after adding a node it is clear that some of EM to PP connections were not established and you need to run any query that re-establish all connections that EM couldn't create on its startup.,9,[~dleeyh] The command 'ss -tenp |grep PrimProc' returns all connections that has a line PrimProc so there are additional connections at PP2. If you checked its output right after adding a node it is clear that some of EM to PP connections were not established and you need to run any query that re-establish all connections that EM couldn't create on its startup.
1071,MCOL-4679,MCOL,Roman,193456,2021-07-02 11:33:50,The issue with a number of PMS sections on node removal had been fixed. The connections number factor had been explained earlier.,10,The issue with a number of PMS sections on node removal had been fixed. The connections number factor had been explained earlier.
1072,MCOL-4679,MCOL,Daniel Lee,193478,2021-07-02 17:20:12,"Build verified: 6.1.1 ( Drone #2687), CMAPI ( Drone #505 )

",11,"Build verified: 6.1.1 ( Drone #2687), CMAPI ( Drone #505 )

"
1073,MCOL-4681,MCOL,Alexander Barkov,186362,2021-04-16 16:06:33,"It appeared that ColumnStore used to have CREATE FUNCTION statements instead of direct INSERTs.

CREATE FUNCTION statements were replaced to INSERTs in this commit:

{noformat}
MCOL-3432 Fix plugin install for non-root

MariaDB 10.4 only allows the system root user to log in as the root DB
user. This means the SQL commands to install the plugins won't run on a
non-root install. This patch skips the grant tables for plugin install
and does some workarounds to get the plugins in.
{noformat}

This sound strange. MariaDB requires the same privileges to do CREATE (UDF) FUNCTION and to INSERT into mysql.proc:

https://mariadb.com/kb/en/create-function-udf/ says:

{quote}To create a function, you must have the INSERT privilege for the mysql database. This is necessary because CREATE FUNCTION adds a row to the mysql.func system table that records the function's name, type, and shared library name.
{quote}
MCOL-3432 should not make the process of a non-root installation to require less privileges.

",1,"It appeared that ColumnStore used to have CREATE FUNCTION statements instead of direct INSERTs.

CREATE FUNCTION statements were replaced to INSERTs in this commit:

{noformat}
MCOL-3432 Fix plugin install for non-root

MariaDB 10.4 only allows the system root user to log in as the root DB
user. This means the SQL commands to install the plugins won't run on a
non-root install. This patch skips the grant tables for plugin install
and does some workarounds to get the plugins in.
{noformat}

This sound strange. MariaDB requires the same privileges to do CREATE (UDF) FUNCTION and to INSERT into mysql.proc:

URL says:

{quote}To create a function, you must have the INSERT privilege for the mysql database. This is necessary because CREATE FUNCTION adds a row to the mysql.func system table that records the function's name, type, and shared library name.
{quote}
MCOL-3432 should not make the process of a non-root installation to require less privileges.

"
1074,MCOL-4681,MCOL,David Hall,192727,2021-06-24 14:13:43,"QA: To test, create a fresh install and attempt a number of functions as listed above. If they get CREATEd, the functions should work.",2,"QA: To test, create a fresh install and attempt a number of functions as listed above. If they get CREATEd, the functions should work."
1075,MCOL-4681,MCOL,Daniel Lee,192814,2021-06-25 16:06:06,Build verified: 6.1.1 ( Drone #2640 ),3,Build verified: 6.1.1 ( Drone #2640 )
1076,MCOL-4685,MCOL,Denis Khalikov,191257,2021-06-07 10:20:44,"Changes:

1. Removes the option to declare uncompressed columns (set columnstore_compression_type = 0).
2. Ignores [COMMENT '[compression=0] option at table or column level (no error messages, just disregard).
3. Removes the option to set more than 2 extents per file (ExtentsPreSegmentFile).
4. Updates rebuildEM tool to support up to 10 dictionary extent per dictionary segment file.
5. Adds check for `DBRootStorageType` for rebuildEM tool.
6. Renamed rebuildEM to mcsRebuildEM.

Testing instructions ([~denis0x0D], please verify and complete):
1. try set columnstore_compression_type = 0. Should return no messages, but set compression to SNAPPY regardless.
2. set compreesion=0 in COMMENT, but check the resulting data, should be compressed still.
3. see what happens if you try to set ExtentsPreSegmentFile to 1 or something bigger than 2. Pay attntion to upgrade in addition to fresh install.
4. [~denis0x0D] - please define the test
5. Try the tool for both disk-resident and S3. See what happens on each.
6. check the name.",1,"Changes:

1. Removes the option to declare uncompressed columns (set columnstore_compression_type = 0).
2. Ignores [COMMENT '[compression=0] option at table or column level (no error messages, just disregard).
3. Removes the option to set more than 2 extents per file (ExtentsPreSegmentFile).
4. Updates rebuildEM tool to support up to 10 dictionary extent per dictionary segment file.
5. Adds check for `DBRootStorageType` for rebuildEM tool.
6. Renamed rebuildEM to mcsRebuildEM.

Testing instructions ([~denis0x0D], please verify and complete):
1. try set columnstore_compression_type = 0. Should return no messages, but set compression to SNAPPY regardless.
2. set compreesion=0 in COMMENT, but check the resulting data, should be compressed still.
3. see what happens if you try to set ExtentsPreSegmentFile to 1 or something bigger than 2. Pay attntion to upgrade in addition to fresh install.
4. [~denis0x0D] - please define the test
5. Try the tool for both disk-resident and S3. See what happens on each.
6. check the name."
1077,MCOL-4685,MCOL,Daniel Lee,191988,2021-06-15 16:24:00,"Is the test case for in the last comment meant for MCOL-4566?
",2,"Is the test case for in the last comment meant for MCOL-4566?
"
1078,MCOL-4685,MCOL,Daniel Lee,191993,2021-06-15 17:06:36,"Build tested: 6.1.1 ( Drone #2586 )

So far, tested columnstore_compression_type.

When setting columnstore_compression_type in MariaDB session or server.cnf, this is the new behavior:

6.1.1
set to 0, ""show variables"" shows UNUSED, table created using compression 2
set to 1, ""show variables"" shows SNAPPY, table created using compression 2
set to 2, ""show variables"" shows SNAPPY, table created using compression 2

Why do we introduced the new value UNUSED?
What does UNUSED mean?

This new value of UNUSED is ambiguous and creates confusion.  It should be set to SNAPPY instead when columnstore_compression_type is set to 0.


5.5.2 behavior
set to 0, ""show variables"" shows NO_COMPRESSION, table created using compression 0
set to 1, ""show variables"" shows SNAPPY, table created using compression 2
set to 2, ""show variables"" shows SNAPPY, table created using compression 2


",3,"Build tested: 6.1.1 ( Drone #2586 )

So far, tested columnstore_compression_type.

When setting columnstore_compression_type in MariaDB session or server.cnf, this is the new behavior:

6.1.1
set to 0, ""show variables"" shows UNUSED, table created using compression 2
set to 1, ""show variables"" shows SNAPPY, table created using compression 2
set to 2, ""show variables"" shows SNAPPY, table created using compression 2

Why do we introduced the new value UNUSED?
What does UNUSED mean?

This new value of UNUSED is ambiguous and creates confusion.  It should be set to SNAPPY instead when columnstore_compression_type is set to 0.


5.5.2 behavior
set to 0, ""show variables"" shows NO_COMPRESSION, table created using compression 0
set to 1, ""show variables"" shows SNAPPY, table created using compression 2
set to 2, ""show variables"" shows SNAPPY, table created using compression 2


"
1079,MCOL-4685,MCOL,Daniel Lee,191995,2021-06-15 17:17:44,"I did not get an error when setting compression to 0

{noformat}
MariaDB [mytest]> set columnstore_compression_type=0;
Query OK, 0 rows affected (0.000 sec)
MariaDB [mytest]> show variables like ""%compression%"";
+------------------------------------------+------------------+
| Variable_name                            | Value            |
+------------------------------------------+------------------+
| column_compression_threshold             | 100              |
| column_compression_zlib_level            | 6                |
| column_compression_zlib_strategy         | DEFAULT_STRATEGY |
| column_compression_zlib_wrap             | OFF              |
| columnstore_compression_type             | UNUSED           |
| innodb_compression_algorithm             | zlib             |
| innodb_compression_default               | OFF              |
| innodb_compression_failure_threshold_pct | 5                |
| innodb_compression_level                 | 6                |
| innodb_compression_pad_pct_max           | 50               |
+------------------------------------------+------------------+
10 rows in set (0.003 sec)

{noformat}


",4,"I did not get an error when setting compression to 0

{noformat}
MariaDB [mytest]> set columnstore_compression_type=0;
Query OK, 0 rows affected (0.000 sec)
MariaDB [mytest]> show variables like ""%compression%"";
+------------------------------------------+------------------+
| Variable_name                            | Value            |
+------------------------------------------+------------------+
| column_compression_threshold             | 100              |
| column_compression_zlib_level            | 6                |
| column_compression_zlib_strategy         | DEFAULT_STRATEGY |
| column_compression_zlib_wrap             | OFF              |
| columnstore_compression_type             | UNUSED           |
| innodb_compression_algorithm             | zlib             |
| innodb_compression_default               | OFF              |
| innodb_compression_failure_threshold_pct | 5                |
| innodb_compression_level                 | 6                |
| innodb_compression_pad_pct_max           | 50               |
+------------------------------------------+------------------+
10 rows in set (0.003 sec)

{noformat}


"
1080,MCOL-4685,MCOL,Denis Khalikov,191998,2021-06-15 17:37:27,"[~dleeyh] right, currently no error for compression_type=0, it sets just default compression.
[~gdorman] ok, I'll change it to SNAPPY",5,"[~dleeyh] right, currently no error for compression_type=0, it sets just default compression.
[~gdorman] ok, I'll change it to SNAPPY"
1081,MCOL-4685,MCOL,Daniel Lee,192004,2021-06-15 19:31:04,"Build tested: 6.1.1 ( Drone #2586 )

Verified:
1. When columnstore_compression_type is set to 0, type 2 (SNAPPY) is used to create compressed tables
2. Setting compression to 0 at both table and column level also ended type 2 SNAPPY compression
3. Uncompressed tables created in 5.5.2 are readable in 6.1.1

4. ExtentsPerSegmentFile has been removed from Columnstore.xml file
5. Adding ExtentsPerSegmentFile to Columnstore.xml and setting it to 4 has no effect on data files allocation
",6,"Build tested: 6.1.1 ( Drone #2586 )

Verified:
1. When columnstore_compression_type is set to 0, type 2 (SNAPPY) is used to create compressed tables
2. Setting compression to 0 at both table and column level also ended type 2 SNAPPY compression
3. Uncompressed tables created in 5.5.2 are readable in 6.1.1

4. ExtentsPerSegmentFile has been removed from Columnstore.xml file
5. Adding ExtentsPerSegmentFile to Columnstore.xml and setting it to 4 has no effect on data files allocation
"
1082,MCOL-4685,MCOL,Daniel Lee,192070,2021-06-16 15:11:55,"Build verified: 6.1.1 ( Drone #2599 )

columnstore_compression_type shows SNAPPY when setting to 0.
",7,"Build verified: 6.1.1 ( Drone #2599 )

columnstore_compression_type shows SNAPPY when setting to 0.
"
1083,MCOL-4692,MCOL,Roman,187265,2021-04-23 18:36:58,Closing it since there is nothing to test really. There must be no regression after the patch.,1,Closing it since there is nothing to test really. There must be no regression after the patch.
1084,MCOL-4694,MCOL,Roman,189254,2021-05-12 15:53:25,Waiting for MCOL-563 to be merged into develop.,1,Waiting for MCOL-563 to be merged into develop.
1085,MCOL-4694,MCOL,Roman,192276,2021-06-18 14:52:52,"4QA there is no way to properly test the distribution now however MCS must work with non-default value for PrimitiveServers.ConnectionsPerPrimProc in Columnstore.xml.
",2,"4QA there is no way to properly test the distribution now however MCS must work with non-default value for PrimitiveServers.ConnectionsPerPrimProc in Columnstore.xml.
"
1086,MCOL-4694,MCOL,Daniel Lee,192430,2021-06-21 19:11:01,"Build verified: 6.1.1 ( Drone #2587 )

Tested ConnectionsPerPrimProc with value set to 4.  Ran regression test suite.
",3,"Build verified: 6.1.1 ( Drone #2587 )

Tested ConnectionsPerPrimProc with value set to 4.  Ran regression test suite.
"
1087,MCOL-4699,MCOL,Kyle Joiner,218713,2022-03-31 14:09:34,I have requested the queries and sample data from Appian.,1,I have requested the queries and sample data from Appian.
1088,MCOL-4699,MCOL,alexey vorovich,222048,2022-04-27 20:11:56,[~kjoiner] do we have an example from Appian ,2,[~kjoiner] do we have an example from Appian 
1089,MCOL-4699,MCOL,Denis Khalikov,229656,2022-07-15 13:05:47,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/2444,3,URL
1090,MCOL-4699,MCOL,Roman,229982,2022-07-20 11:29:42,"4QA I would suggest to take a look at MTR tests to get some ""inspiration"" on how to test this feature. In short MCS now can handle multiple cycles in a JOIN graph.",4,"4QA I would suggest to take a look at MTR tests to get some ""inspiration"" on how to test this feature. In short MCS now can handle multiple cycles in a JOIN graph."
1091,MCOL-4699,MCOL,Daniel Lee,230964,2022-07-28 19:04:14,"Build verified: 22.08-1 (#5108)
Reproduced the issue in 6.4.2-1 and verified that it works in 22.08-1. Also verified query result matched the same test using innodb tables.
",5,"Build verified: 22.08-1 (#5108)
Reproduced the issue in 6.4.2-1 and verified that it works in 22.08-1. Also verified query result matched the same test using innodb tables.
"
1092,MCOL-470,MCOL,Andrew Hutchings,89668,2016-12-16 23:40:00,Passes regression suite for me.,1,Passes regression suite for me.
1093,MCOL-470,MCOL,David Hall,90042,2016-12-28 20:43:32,Passes for me on CentOS 6.5,2,Passes for me on CentOS 6.5
1094,MCOL-470,MCOL,David Hill,90882,2017-01-20 23:51:08,"closing, passing regression test",3,"closing, passing regression test"
1095,MCOL-4704,MCOL,Daniel Lee,190183,2021-05-24 15:32:59,"{noformat}
MariaDB [monty]> INSERT INTO bar SELECT * FROM view_foo;
Query OK, 1 row affected (1.623 sec)
Records: 1  Duplicates: 0  Warnings: 0

MariaDB [monty]> select * from view_foo;
+------+-------+
| id   | fname |
+------+-------+
|    1 | Greg  |
+------+-------+
1 row in set (0.001 sec)

MariaDB [monty]> select * from bar;
+------+-------+
| id   | fname |
+------+-------+
|    1 | Greg  |
+------+-------+
1 row in set (0.067 sec)

MariaDB [monty]> INSERT INTO bar SELECT * FROM (SELECT * FROM view_foo) t;
Query OK, 1 row affected (1.284 sec)
Records: 1  Duplicates: 0  Warnings: 0

MariaDB [monty]> select * from bar;
+------+-------+
| id   | fname |
+------+-------+
|    1 | Greg  |
|    1 | Greg  |
+------+-------+
2 rows in set (0.018 sec)

{noformat}",1,"{noformat}
MariaDB [monty]> INSERT INTO bar SELECT * FROM view_foo;
Query OK, 1 row affected (1.623 sec)
Records: 1  Duplicates: 0  Warnings: 0

MariaDB [monty]> select * from view_foo;
+------+-------+
| id   | fname |
+------+-------+
|    1 | Greg  |
+------+-------+
1 row in set (0.001 sec)

MariaDB [monty]> select * from bar;
+------+-------+
| id   | fname |
+------+-------+
|    1 | Greg  |
+------+-------+
1 row in set (0.067 sec)

MariaDB [monty]> INSERT INTO bar SELECT * FROM (SELECT * FROM view_foo) t;
Query OK, 1 row affected (1.284 sec)
Records: 1  Duplicates: 0  Warnings: 0

MariaDB [monty]> select * from bar;
+------+-------+
| id   | fname |
+------+-------+
|    1 | Greg  |
|    1 | Greg  |
+------+-------+
2 rows in set (0.018 sec)

{noformat}"
1096,MCOL-4705,MCOL,Daniel Lee,190184,2021-05-24 15:37:03,"Build verified: 6.1.1 (Drone #2394)

Result now matches with Innodb.

{noformat}
MariaDB [monty]> DROP TABLE IF EXISTS t1;
Query OK, 0 rows affected, 1 warning (0.120 sec)

MariaDB [monty]> CREATE TABLE t1 (a DECIMAL(17,1), b BIGINT) ENGINE=ColumnStore;
Query OK, 0 rows affected (0.626 sec)

MariaDB [monty]> INSERT INTO t1 VALUES (9999999999999999.9, 999999999999999999);
Query OK, 1 row affected (0.276 sec)

MariaDB [monty]> SELECT * FROM (SELECT a FROM t1 UNION SELECT b FROM t1) tu;
+----------------------+
| a                    |
+----------------------+
| 999999999999999999.0 |
|   9999999999999999.9 |
+----------------------+
2 rows in set (0.123 sec)

{noformat}",1,"Build verified: 6.1.1 (Drone #2394)

Result now matches with Innodb.

{noformat}
MariaDB [monty]> DROP TABLE IF EXISTS t1;
Query OK, 0 rows affected, 1 warning (0.120 sec)

MariaDB [monty]> CREATE TABLE t1 (a DECIMAL(17,1), b BIGINT) ENGINE=ColumnStore;
Query OK, 0 rows affected (0.626 sec)

MariaDB [monty]> INSERT INTO t1 VALUES (9999999999999999.9, 999999999999999999);
Query OK, 1 row affected (0.276 sec)

MariaDB [monty]> SELECT * FROM (SELECT a FROM t1 UNION SELECT b FROM t1) tu;
+----------------------+
| a                    |
+----------------------+
| 999999999999999999.0 |
|   9999999999999999.9 |
+----------------------+
2 rows in set (0.123 sec)

{noformat}"
1097,MCOL-4706,MCOL,Gagan Goel,188245,2021-05-03 06:55:31,For QA: The precision loss issue reported in MCOL-4613 in 5.6.1 should now be fixed in 6.1.1 under MCOL-4612. Please verify.,1,For QA: The precision loss issue reported in MCOL-4613 in 5.6.1 should now be fixed in 6.1.1 under MCOL-4612. Please verify.
1098,MCOL-4709,MCOL,Daniel Lee,192221,2021-06-17 20:48:23,"Build verified: 6.1.1 ( Drone #2488 )

Repeated the test in MCOL-563.
",1,"Build verified: 6.1.1 ( Drone #2488 )

Repeated the test in MCOL-563.
"
1099,MCOL-4713,MCOL,Denis Khalikov,190259,2021-05-25 12:43:47,"I would like to logically split this task into 3 sub tasks:
1. *MCS Plugin part.*
1.1. Implement the ""Analyze Table execplan"" in the MCS plugin part. (Handle analyze table command).
2. *ExeManager.*
2.1. Implement ""Analyze Table Job Step"", ""Analyze Table Command"".
2.2. Parse ""Analyze Table execplan"", lower it to ""Analyze Column Job Steps"" and initialize TupleBPS with it.
2.3. Transfer ""Analyze Column Job Step"" to Primitive Server as ""Analyze Column Command "". 
2.4. Initialize Batch primitive processor.
2.5. Run Batch Primitive processor.
3. *Batch Primitive processor.*
3.1. Implement statistic collection (""Analyze Column Command"" execution) by Batch Primitive processor.
3.2. Collect statistics for column.
3.3. Send it back to ExeManager.
 !Screenshot from 2021-05-25 00-59-29.png|thumbnail! ",1,"I would like to logically split this task into 3 sub tasks:
1. *MCS Plugin part.*
1.1. Implement the ""Analyze Table execplan"" in the MCS plugin part. (Handle analyze table command).
2. *ExeManager.*
2.1. Implement ""Analyze Table Job Step"", ""Analyze Table Command"".
2.2. Parse ""Analyze Table execplan"", lower it to ""Analyze Column Job Steps"" and initialize TupleBPS with it.
2.3. Transfer ""Analyze Column Job Step"" to Primitive Server as ""Analyze Column Command "". 
2.4. Initialize Batch primitive processor.
2.5. Run Batch Primitive processor.
3. *Batch Primitive processor.*
3.1. Implement statistic collection (""Analyze Column Command"" execution) by Batch Primitive processor.
3.2. Collect statistics for column.
3.3. Send it back to ExeManager.
 !Screenshot from 2021-05-25 00-59-29.png|thumbnail! "
1100,MCOL-4713,MCOL,Daniel Lee,193674,2021-07-06 16:12:50,"Build verified: 6.1.1 ( #2727)

MariaDB [tpch10]> analyze table customer, orders, lineitem, supplier, nation, region;
+-----------------+---------+----------+----------+
| Table           | Op      | Msg_type | Msg_text |
+-----------------+---------+----------+----------+
| tpch10.customer | analyze | status   | OK       |
| tpch10.orders   | analyze | status   | OK       |
| tpch10.lineitem | analyze | status   | OK       |
| tpch10.supplier | analyze | status   | OK       |
| tpch10.nation   | analyze | status   | OK       |
| tpch10.region   | analyze | status   | OK       |
+-----------------+---------+----------+----------+
6 rows in set (0.295 sec)

The analyze command did help dbt3's query #5 to work. Please see MCOL-1205.
",2,"Build verified: 6.1.1 ( #2727)

MariaDB [tpch10]> analyze table customer, orders, lineitem, supplier, nation, region;
+-----------------+---------+----------+----------+
| Table           | Op      | Msg_type | Msg_text |
+-----------------+---------+----------+----------+
| tpch10.customer | analyze | status   | OK       |
| tpch10.orders   | analyze | status   | OK       |
| tpch10.lineitem | analyze | status   | OK       |
| tpch10.supplier | analyze | status   | OK       |
| tpch10.nation   | analyze | status   | OK       |
| tpch10.region   | analyze | status   | OK       |
+-----------------+---------+----------+----------+
6 rows in set (0.295 sec)

The analyze command did help dbt3's query #5 to work. Please see MCOL-1205.
"
1101,MCOL-473,MCOL,Ben Thompson,89771,2016-12-20 16:48:27,no longer required to put cmake version and release info at the command line updated engineering documents to reflect change.,1,no longer required to put cmake version and release info at the command line updated engineering documents to reflect change.
1102,MCOL-473,MCOL,David Hill,90880,2017-01-20 23:47:45,builds correcly,2,builds correcly
1103,MCOL-4739,MCOL,Daniel Lee,191141,2021-06-05 05:56:07,"Build tested: 5.6.1 ( Drone #2537)

Performed new installation for the requested tests, except failover.
Also performed 3-node Glusterfs tests

Pending for failover and upgrade tests.
",1,"Build tested: 5.6.1 ( Drone #2537)

Performed new installation for the requested tests, except failover.
Also performed 3-node Glusterfs tests

Pending for failover and upgrade tests.
"
1104,MCOL-4739,MCOL,Daniel Lee,191427,2021-06-08 15:52:50,All tasks completed,2,All tasks completed
1105,MCOL-4742,MCOL,alexey vorovich,218084,2022-03-24 18:30:50,[~alan.mologorsky] is that a quick fix?,1,[~alan.mologorsky] is that a quick fix?
1106,MCOL-4742,MCOL,Roman,228354,2022-06-30 17:39:04,The implementation shares the dbroot1 prefix path for all other dbroots. ,2,The implementation shares the dbroot1 prefix path for all other dbroots. 
1107,MCOL-4742,MCOL,Daniel Lee,228637,2022-07-05 15:16:19,"Build tested: cmapi-6.4.1 (b681)

Moving /var/lib/columnstore/data1 to another location and updating /etc/columnstore/Columnstore.xml accordingly would not work.

{noformat}
.
.
.
<DBRootCount>1</DBRootCount>
<DBRoot1>/etc/columnstore/data1</DBRoot1>
<DBRMRoot>/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves</DBRMRoot>
<TableLockSaveFile>/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks</TableLockSaveFile>
.
.
.
{noformat}

That is because there are other entries in Columnstore.xml still referencing the original location.

{noformat}
<DBRMRoot>/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves</DBRMRoot>
<TableLockSaveFile>/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks</TableLockSaveFile>
<TxnIDFile>/var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID</TxnIDFile>
<OIDBitmapFile>/var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap</OIDBitmapFile>
<BulkRollbackDir>/var/lib/columnstore/data1/systemFiles/bulkRollback</BulkRollbackDir>
{noformat}

The only way to get it to work is to make a new ""data1"" directory in the desired location, then move /var/lib/columnstore/data1/000.dir to under the new 'data1' directory.

I checked the cmapi source code.  It seems that only <dbrootn> (where n is the dbroot number).  Other references are still hard coded.  My opinion is that we need a better and complete solution to this hard-coded path issue.

If the purpose of the ticket is indeed to fix the hardcoded dbroot issue only, then we still have other issues.  I checked the source coded and that there are still other areas that still refence to ""/var/lib/columnstore/dbrootn"". 

{noformat}
[dlee@aloha cmapi]$ grep -r '/var/lib' *
cmapi_server/test/CS-config-test.xml:        <DBRoot1>/var/lib/columnstore/data1</DBRoot1>
cmapi_server/test/CS-config-test.xml:        <DBRMRoot>/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves</DBRMRoot>
cmapi_server/test/CS-config-test.xml:        <TableLockSaveFile>/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks</TableLockSaveFile>
cmapi_server/test/CS-config-test.xml:        <TxnIDFile>/var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID</TxnIDFile>
cmapi_server/test/CS-config-test.xml:        <OIDBitmapFile>/var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap</OIDBitmapFile>
cmapi_server/test/CS-config-test.xml:        <BulkRoot>/var/lib/columnstore/data/bulk</BulkRoot>
cmapi_server/test/CS-config-test.xml:        <BulkRollbackDir>/var/lib/columnstore/data1/systemFiles/bulkRollback</BulkRollbackDir>
cmapi_server/test/Columnstore_apply_config.xml:        <DBRoot1>/var/lib/columnstore/data1</DBRoot1>
cmapi_server/test/Columnstore_apply_config.xml:        <DBRMRoot>/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves</DBRMRoot>
cmapi_server/test/Columnstore_apply_config.xml:        <TableLockSaveFile>/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks</TableLockSaveFile>
cmapi_server/test/Columnstore_apply_config.xml:		<DBRoot2>/var/lib/columnstore/data2</DBRoot2>
cmapi_server/test/Columnstore_apply_config.xml:        <TxnIDFile>/var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID</TxnIDFile>
cmapi_server/test/Columnstore_apply_config.xml:        <OIDBitmapFile>/var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap</OIDBitmapFile>
cmapi_server/test/Columnstore_apply_config.xml:        <BulkRoot>/var/lib/columnstore/data/bulk</BulkRoot>
cmapi_server/test/Columnstore_apply_config.xml:        <BulkRollbackDir>/var/lib/columnstore/data1/systemFiles/bulkRollback</BulkRollbackDir>
cmapi_server/SingleNode.xml:        <DBRoot1>/var/lib/columnstore/data1</DBRoot1>
cmapi_server/SingleNode.xml:        <DBRMRoot>/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves</DBRMRoot>
cmapi_server/SingleNode.xml:        <TableLockSaveFile>/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks</TableLockSaveFile>
cmapi_server/SingleNode.xml:        <TxnIDFile>/var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID</TxnIDFile>
cmapi_server/SingleNode.xml:        <OIDBitmapFile>/var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap</OIDBitmapFile>
cmapi_server/SingleNode.xml:        <BulkRoot>/var/lib/columnstore/data/bulk</BulkRoot>
cmapi_server/SingleNode.xml:        <BulkRollbackDir>/var/lib/columnstore/data1/systemFiles/bulkRollback</BulkRollbackDir>
cmapi_server/constants.py:MCS_DATA_PATH = '/var/lib/columnstore'
mcs_node_control/test/Columnstore_old.xml:        <DBRoot1>/var/lib/columnstore/data1</DBRoot1>
mcs_node_control/test/Columnstore_old.xml:        <DBRMRoot>/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves</DBRMRoot>
mcs_node_control/test/Columnstore_old.xml:        <TableLockSaveFile>/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks</TableLockSaveFile>
mcs_node_control/test/Columnstore_old.xml:        <TxnIDFile>/var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID</TxnIDFile>
mcs_node_control/test/Columnstore_old.xml:        <OIDBitmapFile>/var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap</OIDBitmapFile>
mcs_node_control/test/Columnstore_old.xml:        <BulkRoot>/var/lib/columnstore/data/bulk</BulkRoot>
mcs_node_control/test/Columnstore_old.xml:        <BulkRollbackDir>/var/lib/columnstore/data1/systemFiles/bulkRollback</BulkRollbackDir>
mcs_node_control/custom_dispatchers/container.sh:    MCS_INSTALL_PATH=/var/lib/columnstore
mcs_node_control/custom_dispatchers/container.sh:    MCS_INSTALL_PATH=/var/lib/columnstore
mcs_node_control/custom_dispatchers/container.sh:    MCS_INSTALL_PATH=/var/lib/columnstore
{noformat}




 ",3,"Build tested: cmapi-6.4.1 (b681)

Moving /var/lib/columnstore/data1 to another location and updating /etc/columnstore/Columnstore.xml accordingly would not work.

{noformat}
.
.
.
1
/etc/columnstore/data1
/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves
/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks
.
.
.
{noformat}

That is because there are other entries in Columnstore.xml still referencing the original location.

{noformat}
/var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves
/var/lib/columnstore/data1/systemFiles/dbrm/tablelocks
/var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID
/var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap
/var/lib/columnstore/data1/systemFiles/bulkRollback
{noformat}

The only way to get it to work is to make a new ""data1"" directory in the desired location, then move /var/lib/columnstore/data1/000.dir to under the new 'data1' directory.

I checked the cmapi source code.  It seems that only  (where n is the dbroot number).  Other references are still hard coded.  My opinion is that we need a better and complete solution to this hard-coded path issue.

If the purpose of the ticket is indeed to fix the hardcoded dbroot issue only, then we still have other issues.  I checked the source coded and that there are still other areas that still refence to ""/var/lib/columnstore/dbrootn"". 

{noformat}
[dlee@aloha cmapi]$ grep -r '/var/lib' *
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data1
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/tablelocks
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data/bulk
cmapi_server/test/CS-config-test.xml:        /var/lib/columnstore/data1/systemFiles/bulkRollback
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data1
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/tablelocks
cmapi_server/test/Columnstore_apply_config.xml:		/var/lib/columnstore/data2
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data/bulk
cmapi_server/test/Columnstore_apply_config.xml:        /var/lib/columnstore/data1/systemFiles/bulkRollback
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data1
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/tablelocks
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data/bulk
cmapi_server/SingleNode.xml:        /var/lib/columnstore/data1/systemFiles/bulkRollback
cmapi_server/constants.py:MCS_DATA_PATH = '/var/lib/columnstore'
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data1
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/BRM_saves
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/tablelocks
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/SMTxnID
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data1/systemFiles/dbrm/oidbitmap
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data/bulk
mcs_node_control/test/Columnstore_old.xml:        /var/lib/columnstore/data1/systemFiles/bulkRollback
mcs_node_control/custom_dispatchers/container.sh:    MCS_INSTALL_PATH=/var/lib/columnstore
mcs_node_control/custom_dispatchers/container.sh:    MCS_INSTALL_PATH=/var/lib/columnstore
mcs_node_control/custom_dispatchers/container.sh:    MCS_INSTALL_PATH=/var/lib/columnstore
{noformat}




 "
1108,MCOL-4761,MCOL,Roman Navrotskiy,192369,2021-06-21 10:01:59,what about systemd unit name? all lowercase?,1,what about systemd unit name? all lowercase?
1109,MCOL-4761,MCOL,Roman Navrotskiy,193475,2021-07-02 16:44:23,packages for testing https://cspkg.s3.amazonaws.com/index.html?prefix=cmapi/pr/513/,2,packages for testing URL
1110,MCOL-4769,MCOL,Gagan Goel,204698,2021-11-04 21:37:30,"For QA:

To test Point 1:
  Start mariadb with {{columnstore_cache_inserts}} set to OFF. Then create a table, perform some INSERTs. After this, restart the server with {{columnstore_cache_inserts}} set to ON. Make sure the table created when the cache inserts were disabled is now accessible.

To test Point 3:
  Launch a 3-node MCS cluster as normal, but make sure the cache is enabled on the primary and the replica nodes using the conf file. Create a table and perform some inserts into the primary. Then perform SELECTs on this table on the primary as well as the replica nodes. You should see the correct number of records in the table. Before the fix, if a single record is inserted, it will be duplicated on the replica nodes as well, and when the cache is flushed on the replicas, the record will be inserted into the CS table. So in the end, you would see 3 copies of the same record (provided a SELECT is executed on the primary as well as both the replicas) instead of 1. This is now fixed.

Point 4 is already tested and closed in MCOL-4790.
  ",1,"For QA:

To test Point 1:
  Start mariadb with {{columnstore_cache_inserts}} set to OFF. Then create a table, perform some INSERTs. After this, restart the server with {{columnstore_cache_inserts}} set to ON. Make sure the table created when the cache inserts were disabled is now accessible.

To test Point 3:
  Launch a 3-node MCS cluster as normal, but make sure the cache is enabled on the primary and the replica nodes using the conf file. Create a table and perform some inserts into the primary. Then perform SELECTs on this table on the primary as well as the replica nodes. You should see the correct number of records in the table. Before the fix, if a single record is inserted, it will be duplicated on the replica nodes as well, and when the cache is flushed on the replicas, the record will be inserted into the CS table. So in the end, you would see 3 copies of the same record (provided a SELECT is executed on the primary as well as both the replicas) instead of 1. This is now fixed.

Point 4 is already tested and closed in MCOL-4790.
  "
1111,MCOL-4769,MCOL,Daniel Lee,205789,2021-11-15 17:59:14,"Build tests: 6.2.1-1 (#3379)

The following issues has been fixed.

1. Once the feature is activated in configs, all previous columnstore tables can no longer be accessed. Once de-activated, they come back.",2,"Build tests: 6.2.1-1 (#3379)

The following issues has been fixed.

1. Once the feature is activated in configs, all previous columnstore tables can no longer be accessed. Once de-activated, they come back."
1112,MCOL-4769,MCOL,Daniel Lee,205815,2021-11-15 23:37:20,"Build tests: 6.2.1-1 (#3379)

Timing tests

Insert cache ON vs OFF (1,000 rows)
{noformat}
1. With insert cache on, batch insert and LDI are over 200x and 4x faster, respectively
{noformat}
Insert tests with insert cache ON (400,000)
{noformat}
1. When columnstore_cache_use_import=on
      insert is about 3% faster
      LDI is about 33% faster

2. Single session vs 10 concurrent sesions, 10 concurrent sessions is faster
      insert is about 10% faster
      LDI is about 30% faster

3. While insert test is being executed, executing a ""select count(*)""query,
   which would cause the insert cache to be flushed, from another session 
   every second would have a serious performance impact.

   Single session
      insert is about 152% slower
      LDI is about 33% slower
   
   10 sessionss
      insert is about 147% slower
      LDI is aout 19% slower

{noformat}
LDI vs cpimport
{noformat}
When Use_cpimport_for_batch_insert=on, LDI is 24% slower when insert cache is on
When Use_cpimport_for_batch_insert=off, LDI is 350% faster when insert cache is on

When insert cache is on
columnstore_use_import_for_batchinsert=on would increase LDI performance by 227%
Having Use_cpimport_for_batch_insert also on would increase LDI performance by 234%

Whether insert cache is on or off, cpimport is still many times faster than LDI for immporting data.
{noformat}
Detail timing information can be found in the following link
https://docs.google.com/spreadsheets/d/1V475H96YJ0pcZ4BoGT1JWLpLfcH5a2g8XP-bqpY9byI/edit?usp=sharing

Others
{noformat}
It has been verified that transaction rollback is not supported.

With autocommit=off, while inserting 400,000 rows in a single session,
a ""select count(*)"" query from another session showed intermediate 
inserted rows.  It seems that rows are committed as soon as they are inserted,
just like autocommit is always on.  Is that the reason why transaction
rollback is not supported?  If yes, we need to note that in the release note.
{noformat}
",3,"Build tests: 6.2.1-1 (#3379)

Timing tests

Insert cache ON vs OFF (1,000 rows)
{noformat}
1. With insert cache on, batch insert and LDI are over 200x and 4x faster, respectively
{noformat}
Insert tests with insert cache ON (400,000)
{noformat}
1. When columnstore_cache_use_import=on
      insert is about 3% faster
      LDI is about 33% faster

2. Single session vs 10 concurrent sesions, 10 concurrent sessions is faster
      insert is about 10% faster
      LDI is about 30% faster

3. While insert test is being executed, executing a ""select count(*)""query,
   which would cause the insert cache to be flushed, from another session 
   every second would have a serious performance impact.

   Single session
      insert is about 152% slower
      LDI is about 33% slower
   
   10 sessionss
      insert is about 147% slower
      LDI is aout 19% slower

{noformat}
LDI vs cpimport
{noformat}
When Use_cpimport_for_batch_insert=on, LDI is 24% slower when insert cache is on
When Use_cpimport_for_batch_insert=off, LDI is 350% faster when insert cache is on

When insert cache is on
columnstore_use_import_for_batchinsert=on would increase LDI performance by 227%
Having Use_cpimport_for_batch_insert also on would increase LDI performance by 234%

Whether insert cache is on or off, cpimport is still many times faster than LDI for immporting data.
{noformat}
Detail timing information can be found in the following link
URL

Others
{noformat}
It has been verified that transaction rollback is not supported.

With autocommit=off, while inserting 400,000 rows in a single session,
a ""select count(*)"" query from another session showed intermediate 
inserted rows.  It seems that rows are committed as soon as they are inserted,
just like autocommit is always on.  Is that the reason why transaction
rollback is not supported?  If yes, we need to note that in the release note.
{noformat}
"
1113,MCOL-4771,MCOL,David Hall,196179,2021-08-06 21:15:47,"The code inserted to handle multiple rand() calls is unneeded and was not thread safe, causing the crash. The code added to make certain function objects use fDynamicFunctor handles the multiple rand() problem.",1,"The code inserted to handle multiple rand() calls is unneeded and was not thread safe, causing the crash. The code added to make certain function objects use fDynamicFunctor handles the multiple rand() problem."
1114,MCOL-4771,MCOL,Daniel Lee,196349,2021-08-10 14:42:53,"Build verified: 6.2.1 (#2947)

Reproduced the issue in 5.6.1 using 1gb dbt3 database.  The same statement may fail at different times.

{noformat}
MariaDB [mytest]> select rand() from orders limit 1;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
MariaDB [mytest]> create table test as select *, md5(rand(1)) from orders;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
MariaDB [mytest]> select rand() from orders limit 2;
+---------------------+
| rand()              |
+---------------------+
| 0.13503894408702752 |
| 0.11773851617960121 |
+---------------------+
2 rows in set (0.128 sec)
MariaDB [mytest]> select rand() from orders limit 2;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
{noformat}

Verified the fix in 6.2.1.  The issue is no longer occurring.
",2,"Build verified: 6.2.1 (#2947)

Reproduced the issue in 5.6.1 using 1gb dbt3 database.  The same statement may fail at different times.

{noformat}
MariaDB [mytest]> select rand() from orders limit 1;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
MariaDB [mytest]> create table test as select *, md5(rand(1)) from orders;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
MariaDB [mytest]> select rand() from orders limit 2;
+---------------------+
| rand()              |
+---------------------+
| 0.13503894408702752 |
| 0.11773851617960121 |
+---------------------+
2 rows in set (0.128 sec)
MariaDB [mytest]> select rand() from orders limit 2;
ERROR 1815 (HY000): Internal error: IDB-2045: At least one PrimProc closed the connection unexpectedly.
{noformat}

Verified the fix in 6.2.1.  The issue is no longer occurring.
"
1115,MCOL-4809,MCOL,Roman,203560,2021-10-25 18:49:22,"Here is the microbench run to compare vectorized code against legacy. JFYI vectorized and templated tests actually follow the same path.
{noformat}
root@f6c7f6d6c651:/git/mdb-server/storage/columnstore/columnstore# ./bin/primitives_scan_bench 
2021-10-25 18:30:56
Running ./bin/primitives_scan_bench
Run on (12 X 5000 MHz CPU s)
CPU Caches:
  L1 Data 32K (x6)
  L1 Instruction 32K (x6)
  L2 Unified 256K (x6)
  L3 Unified 12288K (x1)
Load Average: 0.53, 0.64, 0.92
-----------------------------------------------------------------------------------------------------
Benchmark                                                           Time             CPU   Iterations
-----------------------------------------------------------------------------------------------------
FilterBenchFixture/BM_ColumnScan1ByteLegacyCode                151645 ns       151542 ns         4827
FilterBenchFixture/BM_ColumnScan1Byte1FilterLegacyCode         150197 ns       150135 ns         4687
FilterBenchFixture/BM_ColumnScan1ByteTemplatedCode             117699 ns       117634 ns         5948
FilterBenchFixture/BM_ColumnScan1Byte1FilterTemplatedCode      116762 ns       116678 ns         5991
FilterBenchFixture/BM_ColumnScan1ByteVectorizedCode             12960 ns        12954 ns        53812
FilterBenchFixture/BM_ColumnScan1Byte1FilterVectorizedCode      12992 ns        12985 ns        53878
FilterBenchFixture/BM_ColumnScan2ByteLegacyCode                 30773 ns        30763 ns        22778
FilterBenchFixture/BM_ColumnScan2Byte1FilterLegacyCode          30927 ns        30911 ns        22648
FilterBenchFixture/BM_ColumnScan2ByteTemplatedCode               7596 ns         7592 ns        91764
FilterBenchFixture/BM_ColumnScan2Byte1FilterTemplatedCode        7025 ns         7021 ns        99417
FilterBenchFixture/BM_ColumnScan2Byte1FilterVectorizedCode       7034 ns         7029 ns        99361
FilterBenchFixture/BM_ColumnScan4ByteLegacyCode                 17081 ns        17093 ns        40779
FilterBenchFixture/BM_ColumnScan4Byte1FilterLegacyCode          17055 ns        17063 ns        40836
FilterBenchFixture/BM_ColumnScan4ByteTemplatedCode               4757 ns         4757 ns       147177
FilterBenchFixture/BM_ColumnScan4ByteVectorizedCode              4733 ns         4733 ns       147717
FilterBenchFixture/BM_ColumnScan8ByteLegacyCode                  8225 ns         8236 ns        84080
FilterBenchFixture/BM_ColumnScan8Byte1FilterLegacyCode           8204 ns         8216 ns        84243
FilterBenchFixture/BM_ColumnScan8ByteTemplatedCode               3535 ns         3535 ns       196507
FilterBenchFixture/BM_ColumnScan8Byte1FilterTemplatedCode        3552 ns         3552 ns       196760
FilterBenchFixture/BM_ColumnScan8ByteVectorizedCode              3575 ns         3575 ns       196497
FilterBenchFixture/BM_ColumnScan8Byte1FilterVectorizedCode       3531 ns         3530 ns       198363
{noformat}",1,"Here is the microbench run to compare vectorized code against legacy. JFYI vectorized and templated tests actually follow the same path.
{noformat}
root@f6c7f6d6c651:/git/mdb-server/storage/columnstore/columnstore# ./bin/primitives_scan_bench 
2021-10-25 18:30:56
Running ./bin/primitives_scan_bench
Run on (12 X 5000 MHz CPU s)
CPU Caches:
  L1 Data 32K (x6)
  L1 Instruction 32K (x6)
  L2 Unified 256K (x6)
  L3 Unified 12288K (x1)
Load Average: 0.53, 0.64, 0.92
-----------------------------------------------------------------------------------------------------
Benchmark                                                           Time             CPU   Iterations
-----------------------------------------------------------------------------------------------------
FilterBenchFixture/BM_ColumnScan1ByteLegacyCode                151645 ns       151542 ns         4827
FilterBenchFixture/BM_ColumnScan1Byte1FilterLegacyCode         150197 ns       150135 ns         4687
FilterBenchFixture/BM_ColumnScan1ByteTemplatedCode             117699 ns       117634 ns         5948
FilterBenchFixture/BM_ColumnScan1Byte1FilterTemplatedCode      116762 ns       116678 ns         5991
FilterBenchFixture/BM_ColumnScan1ByteVectorizedCode             12960 ns        12954 ns        53812
FilterBenchFixture/BM_ColumnScan1Byte1FilterVectorizedCode      12992 ns        12985 ns        53878
FilterBenchFixture/BM_ColumnScan2ByteLegacyCode                 30773 ns        30763 ns        22778
FilterBenchFixture/BM_ColumnScan2Byte1FilterLegacyCode          30927 ns        30911 ns        22648
FilterBenchFixture/BM_ColumnScan2ByteTemplatedCode               7596 ns         7592 ns        91764
FilterBenchFixture/BM_ColumnScan2Byte1FilterTemplatedCode        7025 ns         7021 ns        99417
FilterBenchFixture/BM_ColumnScan2Byte1FilterVectorizedCode       7034 ns         7029 ns        99361
FilterBenchFixture/BM_ColumnScan4ByteLegacyCode                 17081 ns        17093 ns        40779
FilterBenchFixture/BM_ColumnScan4Byte1FilterLegacyCode          17055 ns        17063 ns        40836
FilterBenchFixture/BM_ColumnScan4ByteTemplatedCode               4757 ns         4757 ns       147177
FilterBenchFixture/BM_ColumnScan4ByteVectorizedCode              4733 ns         4733 ns       147717
FilterBenchFixture/BM_ColumnScan8ByteLegacyCode                  8225 ns         8236 ns        84080
FilterBenchFixture/BM_ColumnScan8Byte1FilterLegacyCode           8204 ns         8216 ns        84243
FilterBenchFixture/BM_ColumnScan8ByteTemplatedCode               3535 ns         3535 ns       196507
FilterBenchFixture/BM_ColumnScan8Byte1FilterTemplatedCode        3552 ns         3552 ns       196760
FilterBenchFixture/BM_ColumnScan8ByteVectorizedCode              3575 ns         3575 ns       196497
FilterBenchFixture/BM_ColumnScan8Byte1FilterVectorizedCode       3531 ns         3530 ns       198363
{noformat}"
1116,MCOL-4809,MCOL,David Hall,206181,2021-11-17 22:01:38,"Some work may not be finished for 6.2.2. We may release with partial implementation as this is only a performance gain at each phase, so they can be implemented separately.",2,"Some work may not be finished for 6.2.2. We may release with partial implementation as this is only a performance gain at each phase, so they can be implemented separately."
1117,MCOL-4809,MCOL,Roman,212876,2022-02-01 19:49:00,Plz review.,3,Plz review.
1118,MCOL-4810,MCOL,Roman,197776,2021-08-27 11:08:45,"4QA We changed the way MCS serializes long strings sending it over the cluster. The main check should be: there is no errors, RAM consumption for queries with long strings is the same or less in PP and EM, performance is the same or better.",1,"4QA We changed the way MCS serializes long strings sending it over the cluster. The main check should be: there is no errors, RAM consumption for queries with long strings is the same or less in PP and EM, performance is the same or better."
1119,MCOL-4814,MCOL,Sergei Golubchik,194541,2021-07-15 18:47:00,"Ideally, it should be something like
{code:cmake}
SET(WITH_COLUMNSTORE_LZ4 AUTO CACHE STRING
  ""Build with lz4. Possible values are 'ON', 'OFF', 'AUTO' and default is 'AUTO'"")
{code}
and with {{OFF}} it won't link with lz4, with {{ON}} it'll fail if lz4 is not found (like now). WIth {{AUTO}} it'll build with lz4 if it's found, otherwise without.

Doing it without any option at all, just always auto, is fine too.",1,"Ideally, it should be something like
{code:cmake}
SET(WITH_COLUMNSTORE_LZ4 AUTO CACHE STRING
  ""Build with lz4. Possible values are 'ON', 'OFF', 'AUTO' and default is 'AUTO'"")
{code}
and with {{OFF}} it won't link with lz4, with {{ON}} it'll fail if lz4 is not found (like now). WIth {{AUTO}} it'll build with lz4 if it's found, otherwise without.

Doing it without any option at all, just always auto, is fine too."
1120,MCOL-4814,MCOL,Denis Khalikov,194558,2021-07-15 23:20:34,"Updated, based on comments.
https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/2051",2,"Updated, based on comments.
URL"
1121,MCOL-4814,MCOL,Denis Khalikov,194651,2021-07-16 17:25:02,"[~dleeyh] Since we have a policy to test every PR, I moved it to you, I think it's ok to just test that CS has LZ4 support after this patch, since all our build systems have lz4 installed and we running under ""AUTO"" option.",3,"[~dleeyh] Since we have a policy to test every PR, I moved it to you, I think it's ok to just test that CS has LZ4 support after this patch, since all our build systems have lz4 installed and we running under ""AUTO"" option."
1122,MCOL-4849,MCOL,Daniel Lee,206185,2021-11-17 22:37:45,"Build tested: 6.2.2 (#3334, #3335)

Build performance compared to release 6.1.1
{noformat}
Build #3334
   DBT3 performance
      Disk-run   is 10.05% faster
      Cached-run is 10.84% faster

   CPImport is       5.71% faster
   LDI is            1.03% faster
   insertSelect is   2.42% faster

Build #3335
   DBT3 performance
      Disk-run   is 10.69% faster
      Cached-run is 12.07% faster

   CPImport is       1.43% faster
   LDI is            1.20% faster
   insertSelect is   2.76% faster

{noformat}

Detailed performance test result can be found here:

https://docs.google.com/spreadsheets/d/1tznQqmpKfkbnn4HjfjYIIeowJlQpNlHj8PVI6QM-3mc/edit?usp=sharing",1,"Build tested: 6.2.2 (#3334, #3335)

Build performance compared to release 6.1.1
{noformat}
Build #3334
   DBT3 performance
      Disk-run   is 10.05% faster
      Cached-run is 10.84% faster

   CPImport is       5.71% faster
   LDI is            1.03% faster
   insertSelect is   2.42% faster

Build #3335
   DBT3 performance
      Disk-run   is 10.69% faster
      Cached-run is 12.07% faster

   CPImport is       1.43% faster
   LDI is            1.20% faster
   insertSelect is   2.76% faster

{noformat}

Detailed performance test result can be found here:

URL"
1123,MCOL-4876,MCOL,Roman,201400,2021-10-03 18:46:54,Plz review.,1,Plz review.
1124,MCOL-488,MCOL,David Hill,90575,2017-01-12 17:35:43,"I just checked into develop-1.0, 2 small changes..

commit 02a2fbeae6bc9de7f009044815a22b2cd8fe120e
Author: David Hill <david.hill@mariadb.com>
Date:   Thu Jan 12 11:33:27 2017 -0600

    MCOL-488 - added back of Alarm Config File

 oam/install_scripts/post-install  | 3 +++
 oam/install_scripts/pre-uninstall | 1 +
",1,"I just checked into develop-1.0, 2 small changes..

commit 02a2fbeae6bc9de7f009044815a22b2cd8fe120e
Author: David Hill 
Date:   Thu Jan 12 11:33:27 2017 -0600

    MCOL-488 - added back of Alarm Config File

 oam/install_scripts/post-install  | 3 +++
 oam/install_scripts/pre-uninstall | 1 +
"
1125,MCOL-488,MCOL,Ben Thompson,90578,2017-01-12 19:42:25,"Reviewed, already merged.",2,"Reviewed, already merged."
1126,MCOL-488,MCOL,David Hill,90662,2017-01-16 15:13:47,check for the file AlarmConfig.xml.installSave after an install is completed... it will remain until columnstore is uninstalled,3,check for the file AlarmConfig.xml.installSave after an install is completed... it will remain until columnstore is uninstalled
1127,MCOL-488,MCOL,Daniel Lee,90746,2017-01-18 00:19:12,"Build verified: Github source build.

[root@localhost mariadb-columnstore-server]# git show
commit 9c73cdaa843fe475c0362fb478dae9fff2d0d355
Merge: 9ddd6d0 4c07522
Author: David Hill <david.hill@mariadb.com>
Date:   Tue Dec 13 12:02:46 2016 -0600

    merge develop branch
[root@localhost columnstore]# cd etc
[root@localhost etc]# ls
AlarmConfig.xml              Columnstore.xml.columnstoreSave  ConsoleCmds.xml   ProcessConfig.xml
AlarmConfig.xml.installSave  Columnstore.xml.installSave      ErrorMessage.txt  ProcessConfig.xml.columnstoreSave
Columnstore.xml              Columnstore.xml.singleserver     MessageFile.txt   ProcessConfig.xml.singleserver
[root@localhost etc]# cd
",4,"Build verified: Github source build.

[root@localhost mariadb-columnstore-server]# git show
commit 9c73cdaa843fe475c0362fb478dae9fff2d0d355
Merge: 9ddd6d0 4c07522
Author: David Hill 
Date:   Tue Dec 13 12:02:46 2016 -0600

    merge develop branch
[root@localhost columnstore]# cd etc
[root@localhost etc]# ls
AlarmConfig.xml              Columnstore.xml.columnstoreSave  ConsoleCmds.xml   ProcessConfig.xml
AlarmConfig.xml.installSave  Columnstore.xml.installSave      ErrorMessage.txt  ProcessConfig.xml.columnstoreSave
Columnstore.xml              Columnstore.xml.singleserver     MessageFile.txt   ProcessConfig.xml.singleserver
[root@localhost etc]# cd
"
1128,MCOL-488,MCOL,Daniel Lee,90752,2017-01-18 01:04:39,"One more time, retested:

Build verified: Github source build

[root@localhost mariadb-columnstore-server]# git show
commit 83b0e5c54a644bc31461752cf73f0e1140586d39
Merge: b975814 93c1c7e
Author: david hill <david.hill@mariadb.com>
Date:   Thu Jan 12 09:27:28 2017 -0600

    Merge pull request #26 from mariadb-corporation/MCOL-500
    
    Update README.md

[root@localhost mariadb-columnstore-engine]# git show
commit c6799df6408c0e86ccba9ee63929a6b9ad4294bf
Merge: fa0fde9 2f3937a
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Jan 13 21:03:35 2017 -0600

    Merge pull request #96 from mariadb-corporation/MCOL-505
    
    MCOL-505 Performance improvements to ExeMgr",5,"One more time, retested:

Build verified: Github source build

[root@localhost mariadb-columnstore-server]# git show
commit 83b0e5c54a644bc31461752cf73f0e1140586d39
Merge: b975814 93c1c7e
Author: david hill 
Date:   Thu Jan 12 09:27:28 2017 -0600

    Merge pull request #26 from mariadb-corporation/MCOL-500
    
    Update README.md

[root@localhost mariadb-columnstore-engine]# git show
commit c6799df6408c0e86ccba9ee63929a6b9ad4294bf
Merge: fa0fde9 2f3937a
Author: Andrew Hutchings 
Date:   Fri Jan 13 21:03:35 2017 -0600

    Merge pull request #96 from mariadb-corporation/MCOL-505
    
    MCOL-505 Performance improvements to ExeMgr"
1129,MCOL-4898,MCOL,Daniel Lee,213380,2022-02-07 20:12:54,"Build tested: 6.2.3-1 CMAPI-1.6 (Build 600)

Verified that, with the fix, ""is_primary"" value is returned correct.

Issue reproduction
------------------
With MCS 6.2.2 and CMAPI 1.5, I did not see the error messages in the cmapi_server.log.  But the returned values was indeed incorrect.

With port 12345, return value was null.  A value of true of false should be returned
{noformat}
[centos8:root~]# curl https://127.0.0.1:8640/cmapi/0.4.0/node/new_primary --header 'Content-Type:application/json' --header 'x-api-key:somekey123' -k | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    20  100    20    0     0    869      0 --:--:-- --:--:-- --:--:--   869
{
  ""is_primary"": null
}
{noformat}
cmapi_server.log entry
{noformat}
[07/Feb/2022 19:50:47] root  get_module_net_address Module 1 network address 127.0.0.1
[07/Feb/2022 19:50:47] cmapi_server get_new_primary returns {'is_primary': None}
[07/Feb/2022 19:50:47] cherrypy.access.139915931324304 127.0.0.1 - - [07/Feb/2022:19:50:47] ""GET /cmapi/0.4.0/node/new_primary HTTP/1.1"" 200 20 """" ""curl/7.61.1""
{noformat}



",1,"Build tested: 6.2.3-1 CMAPI-1.6 (Build 600)

Verified that, with the fix, ""is_primary"" value is returned correct.

Issue reproduction
------------------
With MCS 6.2.2 and CMAPI 1.5, I did not see the error messages in the cmapi_server.log.  But the returned values was indeed incorrect.

With port 12345, return value was null.  A value of true of false should be returned
{noformat}
[centos8:root~]# curl URL --header 'Content-Type:application/json' --header 'x-api-key:somekey123' -k | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    20  100    20    0     0    869      0 --:--:-- --:--:-- --:--:--   869
{
  ""is_primary"": null
}
{noformat}
cmapi_server.log entry
{noformat}
[07/Feb/2022 19:50:47] root  get_module_net_address Module 1 network address 127.0.0.1
[07/Feb/2022 19:50:47] cmapi_server get_new_primary returns {'is_primary': None}
[07/Feb/2022 19:50:47] cherrypy.access.139915931324304 127.0.0.1 - - [07/Feb/2022:19:50:47] ""GET /cmapi/0.4.0/node/new_primary HTTP/1.1"" 200 20 """" ""curl/7.61.1""
{noformat}



"
1130,MCOL-4917,MCOL,Roman,223876,2022-05-16 11:19:50,All future work about merging MCOL-4912 and MCOL-4917 belongs to MCOL-5089.,1,All future work about merging MCOL-4912 and MCOL-4917 belongs to MCOL-5089.
1131,MCOL-4923,MCOL,Sergey Zefirov,206680,2021-11-22 12:47:58,"Okay, first results:

{noformat}
------------------------------------------------------------------------------------------
Benchmark                                                Time             CPU   Iterations
------------------------------------------------------------------------------------------
WideUTF8CollationProcessingBenchmarkNotFactored    4475399 ns      4492188 ns          160
{noformat}

It is on my notebook which is not the speedest machine out there. 2.5GHz or so, i7, Linux subsystem for Windows VM.

I compare two strings of the same length and same content, 452 bytes of mostly 3-byte wide utf-8 code points.

One such comparison takes about 28 microseconds.",1,"Okay, first results:

{noformat}
------------------------------------------------------------------------------------------
Benchmark                                                Time             CPU   Iterations
------------------------------------------------------------------------------------------
WideUTF8CollationProcessingBenchmarkNotFactored    4475399 ns      4492188 ns          160
{noformat}

It is on my notebook which is not the speedest machine out there. 2.5GHz or so, i7, Linux subsystem for Windows VM.

I compare two strings of the same length and same content, 452 bytes of mostly 3-byte wide utf-8 code points.

One such comparison takes about 28 microseconds."
1132,MCOL-4923,MCOL,Sergey Zefirov,206734,2021-11-22 18:15:04,"Regarding these results:
{noformat}
------------------------------------------------------------------------------------------
Benchmark                                                Time             CPU   Iterations
------------------------------------------------------------------------------------------
WideUTF8CollationProcessingBenchmarkNotFactored    4475399 ns      4492188 ns          160
{noformat}

4.48 milliseconds reported above is an average time for a single iteration. Each such iteration performs 1024 comparisons which result in equality. So each comparison takes thousand times less time - 4.4 microseconds for 452 bytes or about 150 mutibyte symbols.",2,"Regarding these results:
{noformat}
------------------------------------------------------------------------------------------
Benchmark                                                Time             CPU   Iterations
------------------------------------------------------------------------------------------
WideUTF8CollationProcessingBenchmarkNotFactored    4475399 ns      4492188 ns          160
{noformat}

4.48 milliseconds reported above is an average time for a single iteration. Each such iteration performs 1024 comparisons which result in equality. So each comparison takes thousand times less time - 4.4 microseconds for 452 bytes or about 150 mutibyte symbols."
1133,MCOL-4923,MCOL,Sergey Zefirov,206794,2021-11-23 12:53:07,"I've employed ""-O3 -DNDEBUG"" options to better optimize the program and got these results:

{noformat}
2021-11-23 15:50:14
Running ./cfmb
Run on (16 X 2304 MHz CPU s)
Load Average: 0.52, 0.58, 0.59
------------------------------------------------------------------------------------------
Benchmark                                                Time             CPU   Iterations
------------------------------------------------------------------------------------------
WideUTF8CollationProcessingBenchmarkNotFactored     828603 ns       837054 ns          896
WideUTF8CollationProcessingBenchmarkFactored        484965 ns       474330 ns         1120
{noformat}

Thus, factored out weight computation saves 44% of run time in the best case.",3,"I've employed ""-O3 -DNDEBUG"" options to better optimize the program and got these results:

{noformat}
2021-11-23 15:50:14
Running ./cfmb
Run on (16 X 2304 MHz CPU s)
Load Average: 0.52, 0.58, 0.59
------------------------------------------------------------------------------------------
Benchmark                                                Time             CPU   Iterations
------------------------------------------------------------------------------------------
WideUTF8CollationProcessingBenchmarkNotFactored     828603 ns       837054 ns          896
WideUTF8CollationProcessingBenchmarkFactored        484965 ns       474330 ns         1120
{noformat}

Thus, factored out weight computation saves 44% of run time in the best case."
1134,MCOL-4923,MCOL,Sergey Zefirov,206823,2021-11-23 17:01:31,"The code [lives here|https://github.com/mariadb-SergeyZefirov/mariadb-columnstore-engine/tree/MCOL-4923-benchmarking-factoring-out-constant-string-collation/tests/benchmarks/micro/collation-factoring] for a while.

Interesting bits are in [main.cpp|https://github.com/mariadb-SergeyZefirov/mariadb-columnstore-engine/blob/MCOL-4923-benchmarking-factoring-out-constant-string-collation/tests/benchmarks/micro/collation-factoring/main.cpp].

Here are two benchmarks:

{code:cpp}
static void WideUTF8CollationProcessingBenchmarkNotFactored(benchmark::State& state)
{
    int neqs = 0;
    SetupWideChars();
    CHARSET_INFO *cs= &my_charset_utf8mb3_general_ci;
    for (auto _ : state)
    {
        int i;
	size_t len = sizeof(latin_chars) - 1;
	for (i=0;i<NUM_SAMPLES;i++)
	{
            if (cs->coll->strnncollsp(cs, pretend_block[i], len, latin_chars, len) != 0)
	    {
                neqs ++;
	    }
	}
    }
    if (neqs)
    {
        printf(""there are %d inequalities found, something wrong!\n"", neqs);
    }
}

static void WideUTF8CollationProcessingBenchmarkFactored(benchmark::State& state)
{
    int neqs = 0;
    SetupWideChars();
    CHARSET_INFO *cs= &my_charset_utf8mb3_general_ci;
    for (auto _ : state)
    {
        int i;
        size_t len = sizeof(latin_chars) - 1;
        size_t buf_len_bytes = cs->coll->strnncollsp_right_buf_size(cs, latin_chars, len);
        void* right_buf = malloc(buf_len_bytes);
        cs->coll->strnncollsp_right_precompute(cs, right_buf, latin_chars, len);
	for (i=0;i<NUM_SAMPLES;i++)
	{
            if (cs->coll->strnncollsp_right(cs, pretend_block[i], len, right_buf) != 0)
	    {
                neqs ++;
	    }
	}
	free(right_buf);
    }
    if (neqs)
    {
        printf(""there are %d inequalities found, something wrong!\n"", neqs);
    }
}
{code}

We obtain the size of buffer to store weights, allocate it and then precompute weights there. Then, for every string in the ""batch"", we perform strnncollsp function, but only one string gets scanned.",4,"The code [lives here|URL for a while.

Interesting bits are in [main.cpp|URL

Here are two benchmarks:

{code:cpp}
static void WideUTF8CollationProcessingBenchmarkNotFactored(benchmark::State& state)
{
    int neqs = 0;
    SetupWideChars();
    CHARSET_INFO *cs= &my_charset_utf8mb3_general_ci;
    for (auto _ : state)
    {
        int i;
	size_t len = sizeof(latin_chars) - 1;
	for (i=0;i<NUM_SAMPLES;i++)
	{
            if (cs->coll->strnncollsp(cs, pretend_block[i], len, latin_chars, len) != 0)
	    {
                neqs ++;
	    }
	}
    }
    if (neqs)
    {
        printf(""there are %d inequalities found, something wrong!\n"", neqs);
    }
}

static void WideUTF8CollationProcessingBenchmarkFactored(benchmark::State& state)
{
    int neqs = 0;
    SetupWideChars();
    CHARSET_INFO *cs= &my_charset_utf8mb3_general_ci;
    for (auto _ : state)
    {
        int i;
        size_t len = sizeof(latin_chars) - 1;
        size_t buf_len_bytes = cs->coll->strnncollsp_right_buf_size(cs, latin_chars, len);
        void* right_buf = malloc(buf_len_bytes);
        cs->coll->strnncollsp_right_precompute(cs, right_buf, latin_chars, len);
	for (i=0;i<NUM_SAMPLES;i++)
	{
            if (cs->coll->strnncollsp_right(cs, pretend_block[i], len, right_buf) != 0)
	    {
                neqs ++;
	    }
	}
	free(right_buf);
    }
    if (neqs)
    {
        printf(""there are %d inequalities found, something wrong!\n"", neqs);
    }
}
{code}

We obtain the size of buffer to store weights, allocate it and then precompute weights there. Then, for every string in the ""batch"", we perform strnncollsp function, but only one string gets scanned."
1135,MCOL-4923,MCOL,Sergey Zefirov,206825,2021-11-23 17:10:13,"The proposed addition in server/include/m_char.h:

{code:cpp}
struct my_collation_handler_st
{
  my_bool (*init)(struct charset_info_st *, MY_CHARSET_LOADER *);
  /* Collation routines */
  int     (*strnncoll)(CHARSET_INFO *,
                       const uchar *, size_t, const uchar *, size_t, my_bool);
  int     (*strnncollsp)(CHARSET_INFO *,
                         const uchar *, size_t, const uchar *, size_t);
  // this function computes buffer size needed to preprocess weights for ""right"" argument of strnncollsp.
  // it returns number of bytes to allocate which includes all weights even terminal zero.
  // weight size can vary, for example, the latin1-based collations can use single unsigned char while
  // utf-8 collations should use 32-bit integers.
  size_t     (*strnncollsp_right_buf_size)(CHARSET_INFO*, const uchar*, size_t);
  // this functions preprocess right argument for a strnncollsp, so that only fetch of weight is needed.
  // it uses externally allocated buffer of size indicated by strnncollsp_right_buf_size.
  void       (*strnncollsp_right_precompute)(CHARSET_INFO*, void*, const uchar*, size_t);
  // and, finally, the strnncollsp where right argument has been preprocessed.
  // ass the right string buffer is zero-terminated, we do not pass it's size.
  int        (*strnncollsp_right)(CHARSET_INFO*, const uchar*, size_t, void*);
{code}

First lines are for context. The last three ""methods"" can be added for right string factoring.

I guess some other things needs to be done in other parts of the server code - as we can factor only right string, we need to transform operations like ""constant_string OP field_value"" to ""field_value OP' constant_string"" and OP' can be different from OP for operations like "">="" or ""<"".",5,"The proposed addition in server/include/m_char.h:

{code:cpp}
struct my_collation_handler_st
{
  my_bool (*init)(struct charset_info_st *, MY_CHARSET_LOADER *);
  /* Collation routines */
  int     (*strnncoll)(CHARSET_INFO *,
                       const uchar *, size_t, const uchar *, size_t, my_bool);
  int     (*strnncollsp)(CHARSET_INFO *,
                         const uchar *, size_t, const uchar *, size_t);
  // this function computes buffer size needed to preprocess weights for ""right"" argument of strnncollsp.
  // it returns number of bytes to allocate which includes all weights even terminal zero.
  // weight size can vary, for example, the latin1-based collations can use single unsigned char while
  // utf-8 collations should use 32-bit integers.
  size_t     (*strnncollsp_right_buf_size)(CHARSET_INFO*, const uchar*, size_t);
  // this functions preprocess right argument for a strnncollsp, so that only fetch of weight is needed.
  // it uses externally allocated buffer of size indicated by strnncollsp_right_buf_size.
  void       (*strnncollsp_right_precompute)(CHARSET_INFO*, void*, const uchar*, size_t);
  // and, finally, the strnncollsp where right argument has been preprocessed.
  // ass the right string buffer is zero-terminated, we do not pass it's size.
  int        (*strnncollsp_right)(CHARSET_INFO*, const uchar*, size_t, void*);
{code}

First lines are for context. The last three ""methods"" can be added for right string factoring.

I guess some other things needs to be done in other parts of the server code - as we can factor only right string, we need to transform operations like ""constant_string OP field_value"" to ""field_value OP' constant_string"" and OP' can be different from OP for operations like "">="" or ""<""."
1136,MCOL-4923,MCOL,Sergey Zefirov,207225,2021-11-30 09:05:28,"We have proven that factoring out processing of the query-constant string can be beneficial. In a larger scheme of things the speed up will not be as dramatic but it may be noticeable nonetheless.

We also provided a change needed for a charset interface for this optimization to work. The change required is in server code and warrants different task.

Thus, this task can be closed as there's nothing to do here.",6,"We have proven that factoring out processing of the query-constant string can be beneficial. In a larger scheme of things the speed up will not be as dramatic but it may be noticeable nonetheless.

We also provided a change needed for a charset interface for this optimization to work. The change required is in server code and warrants different task.

Thus, this task can be closed as there's nothing to do here."
1137,MCOL-4936,MCOL,Gagan Goel,209751,2022-01-04 21:07:28,"For QA: Set up an MCS cluster as usual and test the queries mentioned in the issue description:
# Test to make sure entries are not written to the binlog for the LDI statement.
# Test to make sure entries are not written to the binlog for other DML statements (INSERT/UPDATE/DELETE/INSERT..SELECT).
# Test that DDLs are replicated as usual to the replica nodes (binlog entries are written to the primary node for DDL statements).

Also test that the HTAP topology (InnoDB primary replicating to a replica MCS cluster) is not affected, i.e. the DMLs performed on the InnoDB table are replicated successfully on the replica MCS cluster. For the DMLs to be replicated in the MCS cluster, the system variable {{columnstore_replication_slave}} should be set to ON in the master node in the MCS cluster.",1,"For QA: Set up an MCS cluster as usual and test the queries mentioned in the issue description:
# Test to make sure entries are not written to the binlog for the LDI statement.
# Test to make sure entries are not written to the binlog for other DML statements (INSERT/UPDATE/DELETE/INSERT..SELECT).
# Test that DDLs are replicated as usual to the replica nodes (binlog entries are written to the primary node for DDL statements).

Also test that the HTAP topology (InnoDB primary replicating to a replica MCS cluster) is not affected, i.e. the DMLs performed on the InnoDB table are replicated successfully on the replica MCS cluster. For the DMLs to be replicated in the MCS cluster, the system variable {{columnstore_replication_slave}} should be set to ON in the master node in the MCS cluster."
1138,MCOL-4936,MCOL,Gagan Goel,210342,2022-01-10 19:54:57,"*NOTE* With the fix for this issue, not writing to primary node's binlog in a ColumnStore cluster means DML replication from a ColumnStore cluster to another ColumnStore cluster or to another foreign engine will not work.",2,"*NOTE* With the fix for this issue, not writing to primary node's binlog in a ColumnStore cluster means DML replication from a ColumnStore cluster to another ColumnStore cluster or to another foreign engine will not work."
1139,MCOL-4936,MCOL,Daniel Lee,210355,2022-01-10 21:23:48,"Build tested: 6.2.3-1 (#3651)

Reproduced the behavior the binlog growing behavior for LDI in 6.2.1
Verified that LDI is no longer log to binlog after this fix.
Verified replication continues to work between clusters (innodb-to-innodb and innodb-to-columnstore)

DML comands INSERT, UPDATE, DELETE are not being logged to the binlog, but 'INSERT..SELECT' and 'INSERT INTO..SELECT' are being logged.  Hence, reopening the ticket.",3,"Build tested: 6.2.3-1 (#3651)

Reproduced the behavior the binlog growing behavior for LDI in 6.2.1
Verified that LDI is no longer log to binlog after this fix.
Verified replication continues to work between clusters (innodb-to-innodb and innodb-to-columnstore)

DML comands INSERT, UPDATE, DELETE are not being logged to the binlog, but 'INSERT..SELECT' and 'INSERT INTO..SELECT' are being logged.  Hence, reopening the ticket."
1140,MCOL-4936,MCOL,Roman,210413,2022-01-11 08:58:41,"Good point about RD [~allen.herrera]. I don't think binlog replication via MDB would be a scalable solution but it can be a first cut of the feature.
We will discuss how to enable/disable binlog replication using MDB session variable, e.g. columnstore_binglog.   ",4,"Good point about RD [~allen.herrera]. I don't think binlog replication via MDB would be a scalable solution but it can be a first cut of the feature.
We will discuss how to enable/disable binlog replication using MDB session variable, e.g. columnstore_binglog.   "
1141,MCOL-4936,MCOL,Gagan Goel,210575,2022-01-12 00:41:13,[~allen.herrera] and [~drrtuy] I have created MCOL-4960 to allow a user to enable back binlog for DMLs  statements for cross-cluster replication.,5,[~allen.herrera] and [~drrtuy] I have created MCOL-4960 to allow a user to enable back binlog for DMLs  statements for cross-cluster replication.
1142,MCOL-4936,MCOL,Daniel Lee,210582,2022-01-12 02:59:30,"It would be more natural and intuitive for this MCOL-4936 to be configurable instead, instead of disabling by default and have another variable to enable it back.
",6,"It would be more natural and intuitive for this MCOL-4936 to be configurable instead, instead of disabling by default and have another variable to enable it back.
"
1143,MCOL-4936,MCOL,Gagan Goel,214377,2022-02-15 21:40:01,"[~dleeyh] I have confirmed that none of the DML statements, including INSERT .. SELECT and INSERT INTO SELECT (in addition to LDI/INSERT/UPDATE/DELETE) are getting logged to the primary node's binlog. I am assigning the ticket back to you to retest.",7,"[~dleeyh] I have confirmed that none of the DML statements, including INSERT .. SELECT and INSERT INTO SELECT (in addition to LDI/INSERT/UPDATE/DELETE) are getting logged to the primary node's binlog. I am assigning the ticket back to you to retest."
1144,MCOL-4936,MCOL,Daniel Lee,214756,2022-02-18 17:53:19,"Build verified: 6.3.1 (b3885)
",8,"Build verified: 6.3.1 (b3885)
"
1145,MCOL-4938,MCOL,Roman Navrotskiy,207535,2021-12-06 22:22:58,"I can offer mask/unmask systemd unit:
https://www.freedesktop.org/software/systemd/man/systemctl.html#mask%20UNIT…

https://github.com/mariadb-corporation/mariadb-columnstore-cmapi/pull/132",1,"I can offer mask/unmask systemd unit:
URL

URL"
1146,MCOL-4938,MCOL,Daniel Lee,207954,2021-12-09 16:09:09,"Build tested: 6.2.2-1 (#3464), cmapi 1.5 (#567)
Distributions tested: Debian 10, Ubuntu 18.04 and 20.04

For a single node installation, if the cmapi package is installed, one must setup a one-node cluster using the cmapi commands and start it.  Otherwise, the stack would end up in DBRM read-only state.  One way to avoid this is not to install the cmapi package.
",2,"Build tested: 6.2.2-1 (#3464), cmapi 1.5 (#567)
Distributions tested: Debian 10, Ubuntu 18.04 and 20.04

For a single node installation, if the cmapi package is installed, one must setup a one-node cluster using the cmapi commands and start it.  Otherwise, the stack would end up in DBRM read-only state.  One way to avoid this is not to install the cmapi package.
"
1147,MCOL-4939,MCOL,Roman,208310,2021-12-14 11:25:23,4QA Previously there was no way to disable failover for clusters with >= 3 nodes. It affects clusters with non-shared storages a lot.,1,4QA Previously there was no way to disable failover for clusters with >= 3 nodes. It affects clusters with non-shared storages a lot.
1148,MCOL-4939,MCOL,Alan Mologorsky,208755,2021-12-17 16:38:29,"[~dleeyh] 
Changes has been made:
* add application section with auto_failover = False parameter to default cmapi_server.conf
* failover now is turned off by default even if there are no ""application"" section or no auto_failover parameter exist in cmapi_server.conf
* failover has now three different logical states:
** turned off - no failover thread started. To turn it on set auto_failover=True in application section of cmapi_server.conf file of each node and restart cmapi.
** turned on and inactive - there are failover thread but it doesn't work. It becomes active automatically if nodes count >= 3
** turned on and active - there are an active failover thread and it is activated. Can be deactivated automatically if nodes_count < 3",2,"[~dleeyh] 
Changes has been made:
* add application section with auto_failover = False parameter to default cmapi_server.conf
* failover now is turned off by default even if there are no ""application"" section or no auto_failover parameter exist in cmapi_server.conf
* failover has now three different logical states:
** turned off - no failover thread started. To turn it on set auto_failover=True in application section of cmapi_server.conf file of each node and restart cmapi.
** turned on and inactive - there are failover thread but it doesn't work. It becomes active automatically if nodes count >= 3
** turned on and active - there are an active failover thread and it is activated. Can be deactivated automatically if nodes_count < 3"
1149,MCOL-4939,MCOL,Daniel Lee,208801,2021-12-17 20:45:08,"Build tested: ColumnStore Engine (build 3561)
                       CMAPI (585)

Test cluster: 3-node cluster

Reproduced the reported issue in 6.2.2-1, with CMAPI 1.6 as released.

Non-shared storage (local dbroot)
With auto_failover=False, failover did not occur when PM1 was suspended.

With auto_failover=True, this is a misconfiguration since non-shared storage is used.  When PM1 was suspended, I expected failover to occur and the cluster would end up in a non-operational state, but it did not occur.  Was it because CMAPI detected non-shared storage and did not kick off the failover process? or was it failover simply did not occur?

Glusterfs
With auto_failover=False, failover did not occur when PM1 was suspended.
With auto_failover=True, I expected failover to occur, having PM2 taking over as the master node.  It did not happen.
The same test did worked in 6.1.1 and 6.2.2.







",3,"Build tested: ColumnStore Engine (build 3561)
                       CMAPI (585)

Test cluster: 3-node cluster

Reproduced the reported issue in 6.2.2-1, with CMAPI 1.6 as released.

Non-shared storage (local dbroot)
With auto_failover=False, failover did not occur when PM1 was suspended.

With auto_failover=True, this is a misconfiguration since non-shared storage is used.  When PM1 was suspended, I expected failover to occur and the cluster would end up in a non-operational state, but it did not occur.  Was it because CMAPI detected non-shared storage and did not kick off the failover process? or was it failover simply did not occur?

Glusterfs
With auto_failover=False, failover did not occur when PM1 was suspended.
With auto_failover=True, I expected failover to occur, having PM2 taking over as the master node.  It did not happen.
The same test did worked in 6.1.1 and 6.2.2.







"
1150,MCOL-4939,MCOL,alexey vorovich,216543,2022-03-09 18:58:13,"[~David.Hall] I moved this to testing . was it incorrect .

Is the action item for [~alan.mologorsky] instead",4,"[~David.Hall] I moved this to testing . was it incorrect .

Is the action item for [~alan.mologorsky] instead"
1151,MCOL-4939,MCOL,alexey vorovich,217016,2022-03-15 16:23:25,"[~alan.mologorsky] i moved to testing by mistake

please post the status ",5,"[~alan.mologorsky] i moved to testing by mistake

please post the status "
1152,MCOL-4939,MCOL,alexey vorovich,217848,2022-03-23 14:36:53,"[~dleeyh] Question : all 3 scenarios worked in the previous version ? which one ?
[~alan.mologorsky] please see Daniel's request for steps. 

Note that we are discussing the possible regression in the overall failover functionally in this release - not the actual change that Alan did on this ticket. 
[~toddstoffel] should  maxscale be involved in this ?
Eventually we will need to review this in Sky as well  [~petko.vasilev]",6,"[~dleeyh] Question : all 3 scenarios worked in the previous version ? which one ?
[~alan.mologorsky] please see Daniel's request for steps. 

Note that we are discussing the possible regression in the overall failover functionally in this release - not the actual change that Alan did on this ticket. 
[~toddstoffel] should  maxscale be involved in this ?
Eventually we will need to review this in Sky as well  [~petko.vasilev]"
1153,MCOL-4939,MCOL,Daniel Lee,217871,2022-03-23 16:43:04,"Build tested: 6.3.1-1 (#4101), cmapi 1.6 (#580), 3-node glusterfs 

[~alexey.vorovich] Yes, it was tested before and it worked fine.  Just in case, I retested the same build of ColumnStore 6.3.1-1 using an older build of CMAPI.

1.  When restarting ColumnStore (mcsShutdown and mcsStart, not a failover situation), PM1 remained as the master node.  ColumnStore continued function properly as expected.

2. Failover scenario, PM1 also came back as master node. ColumnStore continued function properly as expected.
{noformat}
MariaDB [mytest]> select count(*) from lineitem;
+----------+
| count(*) |
+----------+
|  6001215 |
+----------+
1 row in set (0.186 sec)

MariaDB [mytest]> create table t1 (c1 int) engine=columnstore;
Query OK, 0 rows affected (1.619 sec)

use near 'table t1 values (1)' at line 1
MariaDB [mytest]> insert t1 values (1);
Query OK, 1 row affected (0.159 sec)

MariaDB [mytest]> insert t1 values (2);
Query OK, 1 row affected (0.074 sec)

MariaDB [mytest]> select * from t1;
+------+
| c1   |
+------+
|    1 |
|    2 |
+------+
2 rows in set (0.521 sec)
{noformat}

",7,"Build tested: 6.3.1-1 (#4101), cmapi 1.6 (#580), 3-node glusterfs 

[~alexey.vorovich] Yes, it was tested before and it worked fine.  Just in case, I retested the same build of ColumnStore 6.3.1-1 using an older build of CMAPI.

1.  When restarting ColumnStore (mcsShutdown and mcsStart, not a failover situation), PM1 remained as the master node.  ColumnStore continued function properly as expected.

2. Failover scenario, PM1 also came back as master node. ColumnStore continued function properly as expected.
{noformat}
MariaDB [mytest]> select count(*) from lineitem;
+----------+
| count(*) |
+----------+
|  6001215 |
+----------+
1 row in set (0.186 sec)

MariaDB [mytest]> create table t1 (c1 int) engine=columnstore;
Query OK, 0 rows affected (1.619 sec)

use near 'table t1 values (1)' at line 1
MariaDB [mytest]> insert t1 values (1);
Query OK, 1 row affected (0.159 sec)

MariaDB [mytest]> insert t1 values (2);
Query OK, 1 row affected (0.074 sec)

MariaDB [mytest]> select * from t1;
+------+
| c1   |
+------+
|    1 |
|    2 |
+------+
2 rows in set (0.521 sec)
{noformat}

"
1154,MCOL-4939,MCOL,alexey vorovich,217901,2022-03-23 18:19:53,"Guys, i would suggest this .

Start with something that works for at least one of you  "" ColumnStore 6.3.1-1 using an older build of CMAPI.""

Indeed , Daniel, please create an annotated script in your repo to do the actions for test1,2,3  to minimize the chance of miscommunication.

Then Alan can try  to repeat them on the version he believes cannot work and we go from there. 

",8,"Guys, i would suggest this .

Start with something that works for at least one of you  "" ColumnStore 6.3.1-1 using an older build of CMAPI.""

Indeed , Daniel, please create an annotated script in your repo to do the actions for test1,2,3  to minimize the chance of miscommunication.

Then Alan can try  to repeat them on the version he believes cannot work and we go from there. 

"
1155,MCOL-4939,MCOL,alexey vorovich,217902,2022-03-23 18:21:52,"and please always use build numbers  in notes instead of ""latest"" and ""older build"" ",9,"and please always use build numbers  in notes instead of ""latest"" and ""older build"" "
1156,MCOL-4939,MCOL,Daniel Lee,217903,2022-03-23 18:28:46,"I always start my comments for test with a line like the following:

Build tested: 6.3.1-1 (#4101), cmapi 1.6 (#580)

When closing a ticket, I do

Build verified: 6.3.1-1 (#4101), cmapi 1.6 (#580)

The number in () is the build number from Drone.

For example.  The CMAPI build that I had issues with was ""cmapi 1.6.2 (#612)"" and the older one that I retested was ""cmapi 1.6 (#580)""
",10,"I always start my comments for test with a line like the following:

Build tested: 6.3.1-1 (#4101), cmapi 1.6 (#580)

When closing a ticket, I do

Build verified: 6.3.1-1 (#4101), cmapi 1.6 (#580)

The number in () is the build number from Drone.

For example.  The CMAPI build that I had issues with was ""cmapi 1.6.2 (#612)"" and the older one that I retested was ""cmapi 1.6 (#580)""
"
1157,MCOL-4939,MCOL,alexey vorovich,217911,2022-03-23 19:50:13,"[~alan.mologorsky] 
Yes, in the future we will share QA scripts (and they will include k8s  commands as well) .

For today,  test1 has these 4 steps  that you  can run one after the other .  Please try these with ""cmapi 1.6 (#580)"" and see if you can confirm what Daniel is seeing (he sees that working and you believe it cannot work)  . Lets start from there.

1. Set auto_failover to True in /etc/columnstore/cmapi_serfer.conf in all nodes
2. ""systemctl restart mariadb-columnstore-cmapi"" in all nodes
3. mcsShutdown
4. mcsStart

*As a side note :*  putting  commands that are supposed to be run on separate multiples  host  could be done via a loop of SSH or via Kubectl . We will need to decide how to do that in the future.  There is also SkyTf framework for these created by [~georgi.harizanov]
Eventually we will integrate multi-node tests with that framework . For now -  just heads up


",11,"[~alan.mologorsky] 
Yes, in the future we will share QA scripts (and they will include k8s  commands as well) .

For today,  test1 has these 4 steps  that you  can run one after the other .  Please try these with ""cmapi 1.6 (#580)"" and see if you can confirm what Daniel is seeing (he sees that working and you believe it cannot work)  . Lets start from there.

1. Set auto_failover to True in /etc/columnstore/cmapi_serfer.conf in all nodes
2. ""systemctl restart mariadb-columnstore-cmapi"" in all nodes
3. mcsShutdown
4. mcsStart

*As a side note :*  putting  commands that are supposed to be run on separate multiples  host  could be done via a loop of SSH or via Kubectl . We will need to decide how to do that in the future.  There is also SkyTf framework for these created by [~georgi.harizanov]
Eventually we will integrate multi-node tests with that framework . For now -  just heads up


"
1158,MCOL-4939,MCOL,alexey vorovich,217917,2022-03-23 20:44:25,"i kind of doubt that rocky and types of data loaded are important. Could be.

What is important is to have the same common  scripts to *install* the system to begin with .

If Daniel has these scripts that Alan should use them. I will try this install script as well.

after these scripts are used  I would start with the the older build that works for both and then move to newer builds",12,"i kind of doubt that rocky and types of data loaded are important. Could be.

What is important is to have the same common  scripts to *install* the system to begin with .

If Daniel has these scripts that Alan should use them. I will try this install script as well.

after these scripts are used  I would start with the the older build that works for both and then move to newer builds"
1159,MCOL-4939,MCOL,alexey vorovich,217919,2022-03-23 20:54:57,"well , If Alan can reproduce the problem using a separate setup then good. 

However , i would definitely  invest into a common installation script",13,"well , If Alan can reproduce the problem using a separate setup then good. 

However , i would definitely  invest into a common installation script"
1160,MCOL-4939,MCOL,alexey vorovich,217927,2022-03-23 22:18:40,"We have 3 candidates for common install script for multi-node 

# Daniel's  QA setup . Needs work as per Daniel to make it really standalone 
# Direct MOE  that brings up cluster with pods ... Pending this week  , i hope and pray
# Docker compose from [~toddstoffel]. If this supports shared disk then we could use that to start/test /validate  and share identical setup between different people  short term and maybe long term as well

Todd, do u agree ?",14,"We have 3 candidates for common install script for multi-node 

# Daniel's  QA setup . Needs work as per Daniel to make it really standalone 
# Direct MOE  that brings up cluster with pods ... Pending this week  , i hope and pray
# Docker compose from [~toddstoffel]. If this supports shared disk then we could use that to start/test /validate  and share identical setup between different people  short term and maybe long term as well

Todd, do u agree ?"
1161,MCOL-4939,MCOL,alexey vorovich,217937,2022-03-23 23:43:51,"The link leads to step 2 of  9 step procedure. Many of these steps require the user to  execute commands on each host. Much time is needed and many possibilities of errors exist.

On the contrary the Direct MOE will allow  something along these lines 

moe -topology CS -replicas 3 -nodetype verylarge

this will create all the nodes, s3, nfs, config files etc etc

I suspect that docker compose approach is simple to use . Todd will clarify

My concern is actually debugging. How one can do symbolic debugging in the docker. There are tools for that as well (at least for Python)

https://code.visualstudio.com/docs/containers/debug-common





",15,"The link leads to step 2 of  9 step procedure. Many of these steps require the user to  execute commands on each host. Much time is needed and many possibilities of errors exist.

On the contrary the Direct MOE will allow  something along these lines 

moe -topology CS -replicas 3 -nodetype verylarge

this will create all the nodes, s3, nfs, config files etc etc

I suspect that docker compose approach is simple to use . Todd will clarify

My concern is actually debugging. How one can do symbolic debugging in the docker. There are tools for that as well (at least for Python)

URL





"
1162,MCOL-4939,MCOL,Georgi Harizanov,217967,2022-03-24 06:16:02,[~alan.mologorsky] can you point me to a repo where your scripts are so I can have a look?,16,[~alan.mologorsky] can you point me to a repo where your scripts are so I can have a look?
1163,MCOL-4939,MCOL,alexey vorovich,218734,2022-03-31 16:06:38,"[~alan.mologorsky] did I summarize the meeting correctly ? If so, please do the reversal of default and pass to [~dleeyh] so that we can move on
 
[~toddstoffel] Here is a suggestion /question from [~gdorman] What if we ALWAYS require maxscale to be present to enable HA. This is currently the case for Sky.

this would reduce the various options . Before we discuss this in dev , what is our take from PM point of view",17,"[~alan.mologorsky] did I summarize the meeting correctly ? If so, please do the reversal of default and pass to [~dleeyh] so that we can move on
 
[~toddstoffel] Here is a suggestion /question from [~gdorman] What if we ALWAYS require maxscale to be present to enable HA. This is currently the case for Sky.

this would reduce the various options . Before we discuss this in dev , what is our take from PM point of view"
1164,MCOL-4939,MCOL,Roman,218795,2022-04-01 06:45:05,[~alexey.vorovich] As the result we decided that I will ask [~toddstoffel] offline regarding default behavior and wait for his decision.,18,[~alexey.vorovich] As the result we decided that I will ask [~toddstoffel] offline regarding default behavior and wait for his decision.
1165,MCOL-4939,MCOL,Roman,219224,2022-04-06 06:06:32,4QA Plz use the [latest|https://cspkg.s3.amazonaws.com/index.html?prefix=cmapi/latest/] CMAPI build. ,19,4QA Plz use the [latest|URL CMAPI build. 
1166,MCOL-4939,MCOL,Daniel Lee,219832,2022-04-08 23:53:27,"Build tested: 6.3.1-1 (#4234), CMAPI-1.6.3 (619)

Cluster: 3-node

Test #1, PASSED
Default auto_failover value

In /etc/columnstore/cmapi_server.conf, auto_failover is set to True by default.

------

Test #2, PASSED
Setup: no shared storage
auto_failover: False

This is the use case which the user does not have shared storage setup and failover is not desired

Failover did not occur

------

Test #3, PASSED
Setup: gluster
auto_failover: False

This is the use case which the user has glusterfs setup and failover IS NOT desired

Failover did not occur

------

Test #4, FAILED
Setup: gluster
auto_failover: True

This is the use case which the user has glusterfs setup and failover IS desired

Observation:

When PM1, which is the master node, taken offline
mcsStatus on PM2 showed PM2 as the master, PM3 as slave (2-node cluster)
MaxScale showed PM2 as the master, PM1 was down
So far, this is expected

When PM1 was put back online
mcsStatus showed PM1 eventually bacame the master node again
but MaxScale showed PM2 should be the master node

It was expected that ColumnStore would set the master according to what MaxScale selected, but this did not happen.  Now ColumnStore and MaxScale are out of sync.

------

At the time of this writing, fixVersion of the ticket has been set for cmapi-1.6.3, but the package as been named for 1.6.2, such as MariaDB-columnstore-cmapi-1.6.2.x86_64.rpm.  The package name shoudl be corrected.
",20,"Build tested: 6.3.1-1 (#4234), CMAPI-1.6.3 (619)

Cluster: 3-node

Test #1, PASSED
Default auto_failover value

In /etc/columnstore/cmapi_server.conf, auto_failover is set to True by default.

------

Test #2, PASSED
Setup: no shared storage
auto_failover: False

This is the use case which the user does not have shared storage setup and failover is not desired

Failover did not occur

------

Test #3, PASSED
Setup: gluster
auto_failover: False

This is the use case which the user has glusterfs setup and failover IS NOT desired

Failover did not occur

------

Test #4, FAILED
Setup: gluster
auto_failover: True

This is the use case which the user has glusterfs setup and failover IS desired

Observation:

When PM1, which is the master node, taken offline
mcsStatus on PM2 showed PM2 as the master, PM3 as slave (2-node cluster)
MaxScale showed PM2 as the master, PM1 was down
So far, this is expected

When PM1 was put back online
mcsStatus showed PM1 eventually bacame the master node again
but MaxScale showed PM2 should be the master node

It was expected that ColumnStore would set the master according to what MaxScale selected, but this did not happen.  Now ColumnStore and MaxScale are out of sync.

------

At the time of this writing, fixVersion of the ticket has been set for cmapi-1.6.3, but the package as been named for 1.6.2, such as MariaDB-columnstore-cmapi-1.6.2.x86_64.rpm.  The package name shoudl be corrected.
"
1167,MCOL-4939,MCOL,alexey vorovich,220231,2022-04-12 14:28:35,"Guys, 

1.  I tend to agree that default file section created at new install should be empty
2. Let's go back to Test4 failed in  https://jira.mariadb.org/browse/MCOL-4939?focusedCommentId=219832&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-219832

I am trying to repro myself, but inconclusive so far..

[~dleeyh] and [~toddstoffel]

Besides the discrepancy between MCS  and MXS in respect to master node choice , what issues with DDL/DML update do we observe ?
Please list what has been found.  My understanding is that Maxscale will direct updates  to PM2.

Also Daniel , for whatever symptoms we see , please confirm in which old release we did not see them 

[~alan.mologorsky] [~drrtuy] [~gdorman]  FYI

",21,"Guys, 

1.  I tend to agree that default file section created at new install should be empty
2. Let's go back to Test4 failed in  URL

I am trying to repro myself, but inconclusive so far..

[~dleeyh] and [~toddstoffel]

Besides the discrepancy between MCS  and MXS in respect to master node choice , what issues with DDL/DML update do we observe ?
Please list what has been found.  My understanding is that Maxscale will direct updates  to PM2.

Also Daniel , for whatever symptoms we see , please confirm in which old release we did not see them 

[~alan.mologorsky] [~drrtuy] [~gdorman]  FYI

"
1168,MCOL-4939,MCOL,alexey vorovich,220449,2022-04-13 17:24:25,"[~alan.mologorsky] [~dleeyh] I opened a new https://jira.mariadb.org/browse/MCOL-5052 for that mismatch discussion 

The only remaining item here is for Alan and is described above ",22,"[~alan.mologorsky] [~dleeyh] I opened a new URL for that mismatch discussion 

The only remaining item here is for Alan and is described above "
1169,MCOL-4939,MCOL,Daniel Lee,220482,2022-04-14 02:01:04,"Build tested: 6.3.1-1 (#4234), CMAPI-1.6.3-1 (#623)

Preliminary test results for failover behavior. More functional tests will be done.

3-node cluster, with gluster, schema replication, MaxScale

For each of the follow tests, a newly installed 3-node cluster is used

Test #1
Default installation, auto_failover parameter has been removed from /etc/columnstore/cmapi_server.conf , default behavior is auto failover enabled.

Failover now works the same way as it used to be.  When putting PM1 back online, PM2 remained as the master node, in sync with MaxScale.

Test #2
On each node, added the following to /etc/columnstore/cmapi_server.conf 
{noformat}
[application]
auto_failover = False
{noformat}
and restarted cmapi
{noformat}
systemctl restart mariadb-columnstore-cmapi
{noformat}

mcsStatus on all three (3) nodes showed there is only one (1) node (pm1) in the cluster.  pm2 and pm3 are no longer part of the cluster.  Output is like the following:
{noformat}
[rocky8:root~]# mcsStatus
{
  ""timestamp"": ""2022-04-14 00:43:02.932548"",
  ""s1pm1"": {
    ""timestamp"": ""2022-04-14 00:43:02.938951"",
    ""uptime"": 1149,
    ""dbrm_mode"": ""master"",
    ""cluster_mode"": ""readwrite"",
    ""dbroots"": [],
    ""module_id"": 1,
    ""services"": [
      {
        ""name"": ""workernode"",
        ""pid"": 9290
      },
      {
        ""name"": ""controllernode"",
        ""pid"": 9301
      },
      {
        ""name"": ""PrimProc"",
        ""pid"": 9317
      },
      {
        ""name"": ""ExeMgr"",
        ""pid"": 9365
      },
      {
        ""name"": ""WriteEngine"",
        ""pid"": 9382
      },
      {
        ""name"": ""DDLProc"",
        ""pid"": 9413
      }
    ]
  },
  ""num_nodes"": 1
}
{noformat}

I tried the same test again and all nodes returned somethig like the following
{noformat}
[rocky8:root~]# mcsStatus
{
  ""timestamp"": ""2022-04-14 01:46:02.956786"",
  ""s1pm1"": {
    ""timestamp"": ""2022-04-14 01:46:02.963366"",
    ""uptime"": 1631,
    ""dbrm_mode"": ""offline"",
    ""cluster_mode"": ""readonly"",
    ""dbroots"": [],
    ""module_id"": 1,
    ""services"": []
  },
  ""num_nodes"": 1
}
{noformat}

Failover was not tested since there is only one node in the cluster now.

Test #3
On each node, added the following to /etc/columnstore/cmapi_server.conf 
{noformat}
[application]
auto_failover = True
{noformat}
and restarted cmapi
{noformat}
systemctl restart mariadb-columnstore-cmapi
{noformat}

I got the same result as Test #1 above
",23,"Build tested: 6.3.1-1 (#4234), CMAPI-1.6.3-1 (#623)

Preliminary test results for failover behavior. More functional tests will be done.

3-node cluster, with gluster, schema replication, MaxScale

For each of the follow tests, a newly installed 3-node cluster is used

Test #1
Default installation, auto_failover parameter has been removed from /etc/columnstore/cmapi_server.conf , default behavior is auto failover enabled.

Failover now works the same way as it used to be.  When putting PM1 back online, PM2 remained as the master node, in sync with MaxScale.

Test #2
On each node, added the following to /etc/columnstore/cmapi_server.conf 
{noformat}
[application]
auto_failover = False
{noformat}
and restarted cmapi
{noformat}
systemctl restart mariadb-columnstore-cmapi
{noformat}

mcsStatus on all three (3) nodes showed there is only one (1) node (pm1) in the cluster.  pm2 and pm3 are no longer part of the cluster.  Output is like the following:
{noformat}
[rocky8:root~]# mcsStatus
{
  ""timestamp"": ""2022-04-14 00:43:02.932548"",
  ""s1pm1"": {
    ""timestamp"": ""2022-04-14 00:43:02.938951"",
    ""uptime"": 1149,
    ""dbrm_mode"": ""master"",
    ""cluster_mode"": ""readwrite"",
    ""dbroots"": [],
    ""module_id"": 1,
    ""services"": [
      {
        ""name"": ""workernode"",
        ""pid"": 9290
      },
      {
        ""name"": ""controllernode"",
        ""pid"": 9301
      },
      {
        ""name"": ""PrimProc"",
        ""pid"": 9317
      },
      {
        ""name"": ""ExeMgr"",
        ""pid"": 9365
      },
      {
        ""name"": ""WriteEngine"",
        ""pid"": 9382
      },
      {
        ""name"": ""DDLProc"",
        ""pid"": 9413
      }
    ]
  },
  ""num_nodes"": 1
}
{noformat}

I tried the same test again and all nodes returned somethig like the following
{noformat}
[rocky8:root~]# mcsStatus
{
  ""timestamp"": ""2022-04-14 01:46:02.956786"",
  ""s1pm1"": {
    ""timestamp"": ""2022-04-14 01:46:02.963366"",
    ""uptime"": 1631,
    ""dbrm_mode"": ""offline"",
    ""cluster_mode"": ""readonly"",
    ""dbroots"": [],
    ""module_id"": 1,
    ""services"": []
  },
  ""num_nodes"": 1
}
{noformat}

Failover was not tested since there is only one node in the cluster now.

Test #3
On each node, added the following to /etc/columnstore/cmapi_server.conf 
{noformat}
[application]
auto_failover = True
{noformat}
and restarted cmapi
{noformat}
systemctl restart mariadb-columnstore-cmapi
{noformat}

I got the same result as Test #1 above
"
1170,MCOL-4939,MCOL,Daniel Lee,220898,2022-04-18 21:29:48,"Build verified: ColumnStore 6.3.1-1 (#4278), cmapi (#625)

Following the steps above and using the new cmapi build, test #2 worked as expected, failover did not take place, as it is disabled in the cmapi-server.cnf file.
",24,"Build verified: ColumnStore 6.3.1-1 (#4278), cmapi (#625)

Following the steps above and using the new cmapi build, test #2 worked as expected, failover did not take place, as it is disabled in the cmapi-server.cnf file.
"
1171,MCOL-4939,MCOL,Daniel Lee,221265,2022-04-20 20:22:59,"Build verified: ColumnStore 6.3.1-1 (#4299), cmapi 1.6.3 (#626)

cmapi package name has been corrected: MariaDB-columnstore-cmapi-1.6.3-1.x86_6.rpm. from 1.6.2 to 1.6.3

Verified along with the latest build of ColumnStore.  Created a 3-node docker cluster.",25,"Build verified: ColumnStore 6.3.1-1 (#4299), cmapi 1.6.3 (#626)

cmapi package name has been corrected: MariaDB-columnstore-cmapi-1.6.3-1.x86_6.rpm. from 1.6.2 to 1.6.3

Verified along with the latest build of ColumnStore.  Created a 3-node docker cluster."
1172,MCOL-4954,MCOL,Alan Mologorsky,212832,2022-02-01 15:30:45,"For testing.
Steps for the new test case:
*1*. add at least one node to the cluster 
{noformat}
curl -k -s -X PUT https://s2pm1:8640/cmapi/0.4.0/cluster/node \
  --header 'Content-Type:application/json' \
  --header ""x-api-key:$MCSAPIKEY"" \
  --data '{""timeout"":120, ""node"": ""s2pm1""}' \
  | jq
{noformat}

*2*. set cluster mode to *_readonly_*
{noformat}
curl -k -s -X PUT https://s2pm1:8640/cmapi/0.4.0/cluster/mode-set \
  --header 'Content-Type:application/json' \
  --header ""x-api-key:$MCSAPIKEY"" \
  --data '{""mode"": ""readonly""}' \
  | jq
{noformat}
 *3*. set cluster mode to *_readwrite_*
{noformat}
curl -k -s -X PUT https://s2pm1:8640/cmapi/0.4.0/cluster/mode-set \
  --header 'Content-Type:application/json' \
  --header ""x-api-key:$MCSAPIKEY"" \
  --data '{""mode"": ""readwrite""}' \
  | jq

{noformat}

",1,"For testing.
Steps for the new test case:
*1*. add at least one node to the cluster 
{noformat}
curl -k -s -X PUT URL \
  --header 'Content-Type:application/json' \
  --header ""x-api-key:$MCSAPIKEY"" \
  --data '{""timeout"":120, ""node"": ""s2pm1""}' \
  | jq
{noformat}

*2*. set cluster mode to *_readonly_*
{noformat}
curl -k -s -X PUT URL \
  --header 'Content-Type:application/json' \
  --header ""x-api-key:$MCSAPIKEY"" \
  --data '{""mode"": ""readonly""}' \
  | jq
{noformat}
 *3*. set cluster mode to *_readwrite_*
{noformat}
curl -k -s -X PUT URL \
  --header 'Content-Type:application/json' \
  --header ""x-api-key:$MCSAPIKEY"" \
  --data '{""mode"": ""readwrite""}' \
  | jq

{noformat}

"
1173,MCOL-4954,MCOL,Daniel Lee,213027,2022-02-02 21:32:00,"Build tested: 6.2.3-1 (#3752), CMAPI 1.6.2 (#600)
Test on a 3-node cluster using Centos 8

Reproduced the readonly and readwrite issues on 6.2.2

Tested the follow commands (aliases)

mcsModule
mcsShutdown
mcsStart
mcsReadOnly
mcsReadWrite

After setting the cluster in readonly mode, I tried an insert statement.  A DBRM read-only error returned as expected, but it left a table lock in place.  After setting the cluster back to readwrite mode, subsequent DML statements also failed because of the table lock.

When the cluster is in readonly, a failed DML state should clear the table lock.  I don't know if the reported issues is within CMAPI or the DMLPro.  Please investigate.  If it is a latter case, I will create a new ticket for it.

{noformat}
[centos8:root~]# $MCSCLIENT mytest -vvv -e ""insert into quicktest values (12, 'readwrite mode')""
--------------
insert into quicktest values (12, 'readwrite mode')
--------------

Query OK, 1 row affected (0.236 sec)

Bye
[centos8:root~]# mcsReadOnly
{
  ""timestamp"": ""2022-02-02 20:26:38.826101"",
  ""cluster-mode"": ""readonly""
}
[centos8:root~]# $MCSCLIENT mytest -vvv -e ""insert into quicktest values (12, 'readwrite mode')""
--------------
insert into quicktest values (12, 'readwrite mode')
--------------

ERROR 1815 (HY000) at line 1: Internal error: CAL0001: Insert Failed:   a BRM Set hwm error. [BRM error status: DBRM is in READ-ONLY mode]  
Bye
[centos8:root~]# viewtablelock
 There is 1 table lock

  Table             LockID  Process  PID   Session  Txn  CreationTime              State    DBRoots  
  mytest.quicktest  7       DMLProc  7782  86       15   Wed Feb  2 20:26:48 2022  LOADING  1,2,3    
[centos8:root~]# mcsReadWrite
{
  ""timestamp"": ""2022-02-02 20:29:41.277906"",
  ""cluster-mode"": ""readwrite""
}
[centos8:root~]# $MCSCLIENT mytest -vvv -e ""insert into quicktest values (12, 'readwrite mode')""
--------------
insert into quicktest values (12, 'readwrite mode')
--------------

ERROR 1815 (HY000) at line 1: Internal error: CAL0001: Insert Failed:  MCS-2009: Unable to perform the insert operation because DMLProc with PID 7782 is currently holding the table lock for session 86.  
{noformat}",2,"Build tested: 6.2.3-1 (#3752), CMAPI 1.6.2 (#600)
Test on a 3-node cluster using Centos 8

Reproduced the readonly and readwrite issues on 6.2.2

Tested the follow commands (aliases)

mcsModule
mcsShutdown
mcsStart
mcsReadOnly
mcsReadWrite

After setting the cluster in readonly mode, I tried an insert statement.  A DBRM read-only error returned as expected, but it left a table lock in place.  After setting the cluster back to readwrite mode, subsequent DML statements also failed because of the table lock.

When the cluster is in readonly, a failed DML state should clear the table lock.  I don't know if the reported issues is within CMAPI or the DMLPro.  Please investigate.  If it is a latter case, I will create a new ticket for it.

{noformat}
[centos8:root~]# $MCSCLIENT mytest -vvv -e ""insert into quicktest values (12, 'readwrite mode')""
--------------
insert into quicktest values (12, 'readwrite mode')
--------------

Query OK, 1 row affected (0.236 sec)

Bye
[centos8:root~]# mcsReadOnly
{
  ""timestamp"": ""2022-02-02 20:26:38.826101"",
  ""cluster-mode"": ""readonly""
}
[centos8:root~]# $MCSCLIENT mytest -vvv -e ""insert into quicktest values (12, 'readwrite mode')""
--------------
insert into quicktest values (12, 'readwrite mode')
--------------

ERROR 1815 (HY000) at line 1: Internal error: CAL0001: Insert Failed:   a BRM Set hwm error. [BRM error status: DBRM is in READ-ONLY mode]  
Bye
[centos8:root~]# viewtablelock
 There is 1 table lock

  Table             LockID  Process  PID   Session  Txn  CreationTime              State    DBRoots  
  mytest.quicktest  7       DMLProc  7782  86       15   Wed Feb  2 20:26:48 2022  LOADING  1,2,3    
[centos8:root~]# mcsReadWrite
{
  ""timestamp"": ""2022-02-02 20:29:41.277906"",
  ""cluster-mode"": ""readwrite""
}
[centos8:root~]# $MCSCLIENT mytest -vvv -e ""insert into quicktest values (12, 'readwrite mode')""
--------------
insert into quicktest values (12, 'readwrite mode')
--------------

ERROR 1815 (HY000) at line 1: Internal error: CAL0001: Insert Failed:  MCS-2009: Unable to perform the insert operation because DMLProc with PID 7782 is currently holding the table lock for session 86.  
{noformat}"
1174,MCOL-4954,MCOL,Daniel Lee,213228,2022-02-04 19:31:40,A ticket MCOL-4988 has been opened to track the reported table lock issue.  Closing this ticket,3,A ticket MCOL-4988 has been opened to track the reported table lock issue.  Closing this ticket
1175,MCOL-4954,MCOL,Alan Mologorsky,213882,2022-02-10 21:00:47,This feature fixes undefined behaviour in internal communication between CMAPI and Engine. Such behaviour described in [MCOL-4973|https://jira.mariadb.org/browse/MCOL-4973],4,This feature fixes undefined behaviour in internal communication between CMAPI and Engine. Such behaviour described in [MCOL-4973|URL
1176,MCOL-497,MCOL,Roman,104744,2017-12-20 10:02:31,"Greetings,

Here is the PR made according with the guidelines given: 
- separate mysql connection code into a separate lib that lives in engine/utils/libmysql_client
- made without configuration/monitoring tools. One must put three xml tags into Columnstore.xls to secure with TLS crossengine and querystats queries' connections. 

Here is a simple feature configuration reference.
To configure the feature one needs to change $INSTALL_DIR/etc/Columnstore.xml. There are three [new tags|https://hastebin.com/caziforelu.xml](there are in the separate block) added to SystemModuleConfig section. $INSTALL_DIR/mysql/my.cnf must be also changed [in this way|https://hastebin.com/ipisuvegod.ini]. These three options: ssl-ca, ssl-cert, ssl-key are sufficient to enable TLS security in the mariadb. 
To check the state of TLS facility in mariadb one should use SHOW VARIABLES LIKE '%ssl%' command. To check the number of sucessfull TLS handshakes SHOW STATUS LIKE '%ssl%'.    ",1,"Greetings,

Here is the PR made according with the guidelines given: 
- separate mysql connection code into a separate lib that lives in engine/utils/libmysql_client
- made without configuration/monitoring tools. One must put three xml tags into Columnstore.xls to secure with TLS crossengine and querystats queries' connections. 

Here is a simple feature configuration reference.
To configure the feature one needs to change $INSTALL_DIR/etc/Columnstore.xml. There are three [new tags|URL are in the separate block) added to SystemModuleConfig section. $INSTALL_DIR/mysql/my.cnf must be also changed [in this way|URL These three options: ssl-ca, ssl-cert, ssl-key are sufficient to enable TLS security in the mariadb. 
To check the state of TLS facility in mariadb one should use SHOW VARIABLES LIKE '%ssl%' command. To check the number of sucessfull TLS handshakes SHOW STATUS LIKE '%ssl%'.    "
1177,MCOL-497,MCOL,Andrew Hutchings,105139,2018-01-02 14:28:07,Set for me to review Roman's code,2,Set for me to review Roman's code
1178,MCOL-497,MCOL,Andrew Hutchings,105151,2018-01-02 17:07:42,Added one comment for a tiny addition before I approve it.,3,Added one comment for a tiny addition before I approve it.
1179,MCOL-497,MCOL,Andrew Hutchings,105511,2018-01-10 22:34:03,"To be documented, extra options in CrossEngineSupport:

{code:xml}
		<TLSCA></TLSCA>
		<TLSClientCert></TLSClientCert>
		<TLSClientKey></TLSClientKey>
{code}",4,"To be documented, extra options in CrossEngineSupport:

{code:xml}
		
		
		
{code}"
1180,MCOL-497,MCOL,Daniel Lee,117319,2018-10-02 17:55:22,"Build verified: Github source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3326be00de5f53ec365910f07a7fd882ba193d4d
Merge: ebbeb30 5cab6c4
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue Sep 18 13:57:17 2018 +0100

    Merge pull request #565 from drrtuy/MCOL-1601
    
    MCOL-1601 GROUP BY now supports subqueries in HAVING.

1) Setup MariaDB 10.3.9 with SSL.
2) Setup keys and certificates on ColumnStore 1.2.0
3) Manually and remotely logged into MariaDB server using SSL
4) Created schema and InnoDB table mdbe.t1 and inserted a row on MariaDB server
5) Created the same schema and InnoDB table in ColumnStore 1.2.0, leaving table empty
6) Configure cross-engine join in Columnstore.xml as the following:

	<CrossEngineSupport>
		<Host>10.0.0.201</Host>
		<Port>3306</Port>
		<User>root</User>
		<Password>mariadb1</Password>
		<TLSCA>/usr/local/mariadb/columnstore/etc/ca-cert.pem</TLSCA>
		<TLSClientCert>/usr/local/mariadb/columnstore/etc/client-cert.pem</TLSClientCert>
		<TLSClientKey>/usr/local/mariadb/columnstore/etc/client-key.pem</TLSClientKey>


7) Run cross-engine join query

[root@localhost bin]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 25
Server version: 10.3.9-MariaDB-log Columnstore 1.2.0-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> select t1.c2, o_comment from mdbe.t1, orders where c1=o_orderkey;
+--------------+------------------------------------+
| c2           | o_comment                          |
+--------------+------------------------------------+
| hello, World | nstructions sleep furiously among  |
+--------------+------------------------------------+
1 row in set (0.164 sec)


",5,"Build verified: Github source

/root/columnstore/mariadb-columnstore-server
commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 3326be00de5f53ec365910f07a7fd882ba193d4d
Merge: ebbeb30 5cab6c4
Author: Andrew Hutchings 
Date:   Tue Sep 18 13:57:17 2018 +0100

    Merge pull request #565 from drrtuy/MCOL-1601
    
    MCOL-1601 GROUP BY now supports subqueries in HAVING.

1) Setup MariaDB 10.3.9 with SSL.
2) Setup keys and certificates on ColumnStore 1.2.0
3) Manually and remotely logged into MariaDB server using SSL
4) Created schema and InnoDB table mdbe.t1 and inserted a row on MariaDB server
5) Created the same schema and InnoDB table in ColumnStore 1.2.0, leaving table empty
6) Configure cross-engine join in Columnstore.xml as the following:

	
		10.0.0.201
		3306
		root
		mariadb1
		/usr/local/mariadb/columnstore/etc/ca-cert.pem
		/usr/local/mariadb/columnstore/etc/client-cert.pem
		/usr/local/mariadb/columnstore/etc/client-key.pem


7) Run cross-engine join query

[root@localhost bin]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 25
Server version: 10.3.9-MariaDB-log Columnstore 1.2.0-1

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> select t1.c2, o_comment from mdbe.t1, orders where c1=o_orderkey;
+--------------+------------------------------------+
| c2           | o_comment                          |
+--------------+------------------------------------+
| hello, World | nstructions sleep furiously among  |
+--------------+------------------------------------+
1 row in set (0.164 sec)


"
1181,MCOL-4978,MCOL,Daniel Lee,215174,2022-02-23 01:08:36,"Build tested, CMAPI 6.1.2 (encrypted key support)

Chear text key works fine.

Instruction on where and how to specify key is needed.",1,"Build tested, CMAPI 6.1.2 (encrypted key support)

Chear text key works fine.

Instruction on where and how to specify key is needed."
1182,MCOL-4978,MCOL,Daniel Lee,215182,2022-02-23 02:49:15,"Build verified: 6.3.1 (b3885), CMAPI 1.6.2 (b612)

Verified on a 3-node cluster
Verified both clear-text and encrypted CEJ password.

1. Create a 3-node cluster
2. set user password in MariaDB
3. Use mcsSetConfigure to set CEJ password in Columnstore.xml
4. Run mcsShutdown, mcsStart, mcsStatus
5. Verify CEJ password has been broadcasted to all 3 nodes
6. Create Columnstore lineitem table
7. cpimport 10g dbt3 lineitem source to ensure data in all dbroots
8. Create a innodb table t1
9. Insert rows into t1
10. Run query joining innodb and Columnstore tables.
",2,"Build verified: 6.3.1 (b3885), CMAPI 1.6.2 (b612)

Verified on a 3-node cluster
Verified both clear-text and encrypted CEJ password.

1. Create a 3-node cluster
2. set user password in MariaDB
3. Use mcsSetConfigure to set CEJ password in Columnstore.xml
4. Run mcsShutdown, mcsStart, mcsStatus
5. Verify CEJ password has been broadcasted to all 3 nodes
6. Create Columnstore lineitem table
7. cpimport 10g dbt3 lineitem source to ensure data in all dbroots
8. Create a innodb table t1
9. Insert rows into t1
10. Run query joining innodb and Columnstore tables.
"
1183,MCOL-498,MCOL,Andrew Hutchings,91783,2017-02-13 13:31:04,"I think there is a good middle ground here that means we won't need an option and will optimise SSD and spinning disk file creation... If we use fallocate to set the size upon extent creation (and expansion) then the continuous space will be allocated without zeroing out the space. We wouldn't need an option for this as the default behaviour is good for SSD and spinning disk, it will improve the file creation performance of both and will remove the needless writes for SSDs.",1,"I think there is a good middle ground here that means we won't need an option and will optimise SSD and spinning disk file creation... If we use fallocate to set the size upon extent creation (and expansion) then the continuous space will be allocated without zeroing out the space. We wouldn't need an option for this as the default behaviour is good for SSD and spinning disk, it will improve the file creation performance of both and will remove the needless writes for SSDs."
1184,MCOL-498,MCOL,Allan,95095,2017-05-10 19:53:40,"*Some technical thoughts (not issues).*

Assume that there are two processes that each have open handles to the same underlying files on the server.  In one of them a truncate/delete of the file is done because it is doing a delete operation, and also assume that to do the delete it holds a lock on the files.  The truncate completes and the kernel marks the appropriate inodes and the lock is released.  That disk space is gone.  But it can't actually be released because the other process has a handle to the files.  The only way to actually release the space is for all processes that hold handles to close them so the kernel can actually release the disk space for other use.

My thought is that if the server holds on to the open files after a delete that that space will never be reclaimed until the server is restarted.  You should not have to restart the server to reclaim the space.

And of course you may already be doing all this.",2,"*Some technical thoughts (not issues).*

Assume that there are two processes that each have open handles to the same underlying files on the server.  In one of them a truncate/delete of the file is done because it is doing a delete operation, and also assume that to do the delete it holds a lock on the files.  The truncate completes and the kernel marks the appropriate inodes and the lock is released.  That disk space is gone.  But it can't actually be released because the other process has a handle to the files.  The only way to actually release the space is for all processes that hold handles to close them so the kernel can actually release the disk space for other use.

My thought is that if the server holds on to the open files after a delete that that space will never be reclaimed until the server is restarted.  You should not have to restart the server to reclaim the space.

And of course you may already be doing all this."
1185,MCOL-498,MCOL,Roman,107956,2018-03-06 11:29:00,"The tests shows that dedicated NULL values, used to fill the segment file up, aren't essential, so I implemented that good idea with fallocate() usage where possible. However some file systems lack fallocate() call,so I also add a setting to disable segment file preallocation on a per-PM basis. 
I use the same set to optimize dictionary files enlarging. ",3,"The tests shows that dedicated NULL values, used to fill the segment file up, aren't essential, so I implemented that good idea with fallocate() usage where possible. However some file systems lack fallocate() call,so I also add a setting to disable segment file preallocation on a per-PM basis. 
I use the same set to optimize dictionary files enlarging. "
1186,MCOL-498,MCOL,Roman,107957,2018-03-06 11:34:39,"Please, review the PR.",4,"Please, review the PR."
1187,MCOL-498,MCOL,Andrew Hutchings,107958,2018-03-06 12:07:09,"Unfortunately as it stands this patch causes a regression.

Aggregates are producing incorrect results. Likely because they are seeing whole blocks as used now rather than partly used when they were being filled with the 'empty' byte pattern.

This is one of the diffs from test000 failure:

{code:diff}
--- tpchValidation.sql.ref.log	2016-08-16 11:18:47.000000000 +0100
+++ tpchValidation.sql.log	2018-03-06 11:58:48.832982170 +0000
@@ -1,51 +1,51 @@
 count(l_orderkey)	min(l_orderkey)	max(l_orderkey)	sum(l_orderkey)	avg(l_orderkey)
-6001215	1	6000000	18005322964949	3000279.6042
+6002688	0	6000000	18005322964949	2999543.3654
 count(l_partkey)	min(l_partkey)	max(l_partkey)	sum(l_partkey)	avg(l_partkey)
-6001215	1	200000	600229457837	100017.9893
+6002688	0	200000	600229457837	99993.4459
 count(l_suppkey)	min(l_suppkey)	max(l_suppkey)	sum(l_suppkey)	avg(l_suppkey)
-6001215	1	10000	30009691369	5000.6026
+6002688	0	10000	30009691369	4999.3755
 count(l_linenumber)	min(l_linenumber)	max(l_linenumber)	sum(l_linenumber)	avg(l_linenumber)
-6001215	1	7	18007100	3.0006
+6001664	0	7	18007100	3.0004
 count(l_quantity)	min(l_quantity)	max(l_quantity)	sum(l_quantity)	avg(l_quantity)
-6001215	1.00	50.00	153078795.00	25.507967
+6001664	0.00	50.00	153078795.00	25.506059
 count(l_extendedprice)	min(l_extendedprice)	max(l_extendedprice)	sum(l_extendedprice)	avg(l_extendedprice)
-6001215	901.00	104949.50	229577310901.20	38255.138485
+6001664	0.00	104949.50	229577310901.20	38252.276519
 count(l_discount)	min(l_discount)	max(l_discount)	sum(l_discount)	avg(l_discount)
-6001215	0.00	0.10	300057.33	0.049999
+6001664	0.00	0.10	300057.33	0.049996
 count(l_tax)	min(l_tax)	max(l_tax)	sum(l_tax)	avg(l_tax)
-6001215	0.00	0.08	240129.67	0.040014
+6001664	0.00	0.08	240129.67	0.040011
 count(l_returnflag)	min(l_returnflag)	max(l_returnflag)
-6001215	A	R
+6004736	NULL	R
 count(l_linestatus)	min(l_linestatus)	max(l_linestatus)
-6001215	F	O
+6004736	NULL	O
 count(l_shipdate)	min(l_shipdate)	max(l_shipdate)
-6001215	1992-01-02	1998-12-01
+6002688	0000-00-00	1998-12-01
 count(l_commitdate)	min(l_commitdate)	max(l_commitdate)
-6001215	1992-01-31	1998-10-31
+6002688	0000-00-00	1998-10-31
 count(l_receiptdate)	min(l_receiptdate)	max(l_receiptdate)
-6001215	1992-01-04	1998-12-31
+6002688	0000-00-00	1998-12-31
 count(*)
-6001215
+6004736
 count(o_orderkey)	min(o_orderkey)	max(o_orderkey)	sum(o_orderkey)	avg(o_orderkey)
-1500000	1	6000000	4499987250000	2999991.5000
+1501184	0	6000000	4499987250000	2997625.3744
 count(o_custkey)	min(o_custkey)	max(o_custkey)	sum(o_custkey)	avg(o_custkey)
-1500000	1	149999	112509060862	75006.0406
+1501184	0	149999	112509060862	74946.8825
 count(o_orderstatus)	min(o_orderstatus)	max(o_orderstatus)
-1500000	F	P
+1507328	NULL	P
 count(o_totalprice)	min(o_totalprice)	max(o_totalprice)	sum(o_totalprice)	avg(o_totalprice)
-1500000	857.71	555285.16	226829306447.46	151219.537632
+1500160	0.00	555285.16	226829306447.46	151203.409268
 count(o_orderdate)	min(o_orderdate)	max(o_orderdate)
-1500000	1992-01-01	1998-08-02
+1501184	0000-00-00	1998-08-02
 count(o_orderpriority)	min(o_orderpriority)	max(o_orderpriority)
 1500000	1-URGENT	5-LOW
 count(o_clerk)	min(o_clerk)	max(o_clerk)
 1500000	Clerk#000000001	Clerk#000001000
 count(o_shippriority)	min(o_shippriority)	max(o_shippriority)
-1500000	0	0
+1501184	0	0
 count(o_comment)	min(o_comment)	max(o_comment)
 1500000	 Tiresias about the blithely ironic a	zzle? furiously ironic instructions among the unusual t
 count(*)
-1500000
+1507328
 count(n_nationkey)	min(n_nationkey)	max(n_nationkey)	sum(n_nationkey)	avg(n_nationkey)
 25	0	24	300	12.0000
 count(n_name)	min(n_name)	max(n_name)
@@ -99,17 +99,17 @@
 count(*)
 10000
 count(ps_partkey)	min(ps_partkey)	max(ps_partkey)	sum(ps_partkey)	avg(ps_partkey)
-800000	1	200000	80000400000	100000.5000
+800768	0	200000	80000400000	99904.5916
 count(ps_suppkey)	min(ps_suppkey)	max(ps_suppkey)	sum(ps_suppkey)	avg(ps_suppkey)
-800000	1	10000	4000400000	5000.5000
+800768	0	10000	4000400000	4995.7041
 count(ps_availqty)	min(ps_availqty)	max(ps_availqty)	sum(ps_availqty)	avg(ps_availqty)
-800000	1	9999	4002581547	5003.2269
+800768	0	9999	4002581547	4998.4284
 count(ps_supplycost)	min(ps_supplycost)	max(ps_supplycost)	sum(ps_supplycost)	avg(ps_supplycost)
-800000	1.00	1000.00	400420638.54	500.525798
+800768	0.00	1000.00	400420638.54	500.045754
 count(ps_comment)	min(ps_comment)	max(ps_comment)
 800000	 Tiresias according to the quiet courts sleep against the ironic, final requests. carefully unusual requests affix fluffily quickly ironic packages. regular 	zzle. unusual decoys detect slyly blithely express frays. furiously ironic packages about the bold accounts are close requests. slowly silent reque
 count(*)
-800000
+800768
 count(p_partkey)	min(p_partkey)	max(p_partkey)	sum(p_partkey)	avg(p_partkey)
 200000	1	200000	20000100000	100000.5000
 count(p_name)	min(p_name)	max(p_name)
{code}",5,"Unfortunately as it stands this patch causes a regression.

Aggregates are producing incorrect results. Likely because they are seeing whole blocks as used now rather than partly used when they were being filled with the 'empty' byte pattern.

This is one of the diffs from test000 failure:

{code:diff}
--- tpchValidation.sql.ref.log	2016-08-16 11:18:47.000000000 +0100
+++ tpchValidation.sql.log	2018-03-06 11:58:48.832982170 +0000
@@ -1,51 +1,51 @@
 count(l_orderkey)	min(l_orderkey)	max(l_orderkey)	sum(l_orderkey)	avg(l_orderkey)
-6001215	1	6000000	18005322964949	3000279.6042
+6002688	0	6000000	18005322964949	2999543.3654
 count(l_partkey)	min(l_partkey)	max(l_partkey)	sum(l_partkey)	avg(l_partkey)
-6001215	1	200000	600229457837	100017.9893
+6002688	0	200000	600229457837	99993.4459
 count(l_suppkey)	min(l_suppkey)	max(l_suppkey)	sum(l_suppkey)	avg(l_suppkey)
-6001215	1	10000	30009691369	5000.6026
+6002688	0	10000	30009691369	4999.3755
 count(l_linenumber)	min(l_linenumber)	max(l_linenumber)	sum(l_linenumber)	avg(l_linenumber)
-6001215	1	7	18007100	3.0006
+6001664	0	7	18007100	3.0004
 count(l_quantity)	min(l_quantity)	max(l_quantity)	sum(l_quantity)	avg(l_quantity)
-6001215	1.00	50.00	153078795.00	25.507967
+6001664	0.00	50.00	153078795.00	25.506059
 count(l_extendedprice)	min(l_extendedprice)	max(l_extendedprice)	sum(l_extendedprice)	avg(l_extendedprice)
-6001215	901.00	104949.50	229577310901.20	38255.138485
+6001664	0.00	104949.50	229577310901.20	38252.276519
 count(l_discount)	min(l_discount)	max(l_discount)	sum(l_discount)	avg(l_discount)
-6001215	0.00	0.10	300057.33	0.049999
+6001664	0.00	0.10	300057.33	0.049996
 count(l_tax)	min(l_tax)	max(l_tax)	sum(l_tax)	avg(l_tax)
-6001215	0.00	0.08	240129.67	0.040014
+6001664	0.00	0.08	240129.67	0.040011
 count(l_returnflag)	min(l_returnflag)	max(l_returnflag)
-6001215	A	R
+6004736	NULL	R
 count(l_linestatus)	min(l_linestatus)	max(l_linestatus)
-6001215	F	O
+6004736	NULL	O
 count(l_shipdate)	min(l_shipdate)	max(l_shipdate)
-6001215	1992-01-02	1998-12-01
+6002688	0000-00-00	1998-12-01
 count(l_commitdate)	min(l_commitdate)	max(l_commitdate)
-6001215	1992-01-31	1998-10-31
+6002688	0000-00-00	1998-10-31
 count(l_receiptdate)	min(l_receiptdate)	max(l_receiptdate)
-6001215	1992-01-04	1998-12-31
+6002688	0000-00-00	1998-12-31
 count(*)
-6001215
+6004736
 count(o_orderkey)	min(o_orderkey)	max(o_orderkey)	sum(o_orderkey)	avg(o_orderkey)
-1500000	1	6000000	4499987250000	2999991.5000
+1501184	0	6000000	4499987250000	2997625.3744
 count(o_custkey)	min(o_custkey)	max(o_custkey)	sum(o_custkey)	avg(o_custkey)
-1500000	1	149999	112509060862	75006.0406
+1501184	0	149999	112509060862	74946.8825
 count(o_orderstatus)	min(o_orderstatus)	max(o_orderstatus)
-1500000	F	P
+1507328	NULL	P
 count(o_totalprice)	min(o_totalprice)	max(o_totalprice)	sum(o_totalprice)	avg(o_totalprice)
-1500000	857.71	555285.16	226829306447.46	151219.537632
+1500160	0.00	555285.16	226829306447.46	151203.409268
 count(o_orderdate)	min(o_orderdate)	max(o_orderdate)
-1500000	1992-01-01	1998-08-02
+1501184	0000-00-00	1998-08-02
 count(o_orderpriority)	min(o_orderpriority)	max(o_orderpriority)
 1500000	1-URGENT	5-LOW
 count(o_clerk)	min(o_clerk)	max(o_clerk)
 1500000	Clerk#000000001	Clerk#000001000
 count(o_shippriority)	min(o_shippriority)	max(o_shippriority)
-1500000	0	0
+1501184	0	0
 count(o_comment)	min(o_comment)	max(o_comment)
 1500000	 Tiresias about the blithely ironic a	zzle? furiously ironic instructions among the unusual t
 count(*)
-1500000
+1507328
 count(n_nationkey)	min(n_nationkey)	max(n_nationkey)	sum(n_nationkey)	avg(n_nationkey)
 25	0	24	300	12.0000
 count(n_name)	min(n_name)	max(n_name)
@@ -99,17 +99,17 @@
 count(*)
 10000
 count(ps_partkey)	min(ps_partkey)	max(ps_partkey)	sum(ps_partkey)	avg(ps_partkey)
-800000	1	200000	80000400000	100000.5000
+800768	0	200000	80000400000	99904.5916
 count(ps_suppkey)	min(ps_suppkey)	max(ps_suppkey)	sum(ps_suppkey)	avg(ps_suppkey)
-800000	1	10000	4000400000	5000.5000
+800768	0	10000	4000400000	4995.7041
 count(ps_availqty)	min(ps_availqty)	max(ps_availqty)	sum(ps_availqty)	avg(ps_availqty)
-800000	1	9999	4002581547	5003.2269
+800768	0	9999	4002581547	4998.4284
 count(ps_supplycost)	min(ps_supplycost)	max(ps_supplycost)	sum(ps_supplycost)	avg(ps_supplycost)
-800000	1.00	1000.00	400420638.54	500.525798
+800768	0.00	1000.00	400420638.54	500.045754
 count(ps_comment)	min(ps_comment)	max(ps_comment)
 800000	 Tiresias according to the quiet courts sleep against the ironic, final requests. carefully unusual requests affix fluffily quickly ironic packages. regular 	zzle. unusual decoys detect slyly blithely express frays. furiously ironic packages about the bold accounts are close requests. slowly silent reque
 count(*)
-800000
+800768
 count(p_partkey)	min(p_partkey)	max(p_partkey)	sum(p_partkey)	avg(p_partkey)
 200000	1	200000	20000100000	100000.5000
 count(p_name)	min(p_name)	max(p_name)
{code}"
1188,MCOL-498,MCOL,Roman,121003,2018-12-20 07:03:10,Please review the code change.,6,Please review the code change.
1189,MCOL-498,MCOL,Roman,121005,2018-12-20 07:26:01,"The feature allows one to disable space preallocation when CS adds new or expands abbreviated extents. Nevertheless CS preallocates initial abbreviated extents using fallocate (if filesystem supports it) or sequential write as fallback. Abbreviated extents uses 2MB of disk space only.
How to enable the feature. Space preallocation could be disabled dbroot-wise. To disable preallocation for DBRoot10 one must add this snippet into Columnstore.xml and reboot CS.
{noformat}
<DBRoot1>
     	<PreallocSpace>OFF</PreallocSpace>
</DBRoot1>
{noformat}

It is easy to check that the feature works. Disable space preallocation as said before. Then create a table and insert > 8kk rows into it. Check for disk space used with this query.
{code:sql}
select * from information_schema.columnstore_files
{code}
Previous CS versions eventually allocate 64MB files and now it allocates the exact needed amount of disk space.

I used this sequence to create a test table.
{code:sql}
drop table cs27; set infinidb_compression_type = 2; create table cs27 (o_comment varchar(79)) engine=columnstore;
{code}
Here is the [link|https://drive.google.com/file/d/1b13LBLtHCJuAYaGwSluLWGrlP-W0dsbu/view?usp=sharing] to a 1.5kk dataset to use with the table.
",7,"The feature allows one to disable space preallocation when CS adds new or expands abbreviated extents. Nevertheless CS preallocates initial abbreviated extents using fallocate (if filesystem supports it) or sequential write as fallback. Abbreviated extents uses 2MB of disk space only.
How to enable the feature. Space preallocation could be disabled dbroot-wise. To disable preallocation for DBRoot10 one must add this snippet into Columnstore.xml and reboot CS.
{noformat}

     	OFF

{noformat}

It is easy to check that the feature works. Disable space preallocation as said before. Then create a table and insert > 8kk rows into it. Check for disk space used with this query.
{code:sql}
select * from information_schema.columnstore_files
{code}
Previous CS versions eventually allocate 64MB files and now it allocates the exact needed amount of disk space.

I used this sequence to create a test table.
{code:sql}
drop table cs27; set infinidb_compression_type = 2; create table cs27 (o_comment varchar(79)) engine=columnstore;
{code}
Here is the [link|URL to a 1.5kk dataset to use with the table.
"
1190,MCOL-498,MCOL,Roman,121080,2018-12-21 11:46:48,Please review the change.,8,Please review the change.
1191,MCOL-498,MCOL,Patrick LeBlanc,121404,2019-01-04 18:28:38,"Reviewed it, requested a couple changes.",9,"Reviewed it, requested a couple changes."
1192,MCOL-498,MCOL,Roman,125375,2019-03-26 17:22:07,Please review the feature that allows to disable disk space preallocation. ,10,Please review the feature that allows to disable disk space preallocation. 
1193,MCOL-498,MCOL,Roman,127008,2019-04-29 07:44:07,"For QA:

Here is the snippet from the XML config to disable disk space preallocation for dbroot 1:
{noformat}
<DBRoot1>
     	<PreallocSpace>OFF</PreallocSpace>
</DBRoot1>
{noformat}

Here is the scenario to check the feature works. Upload >= 300 000 records in the table then check the size of the segment/dictionary files:
- using 'du' utility 
- calling columnstore_info.table_usage()
- getting data from information_schema.columnstore_files",11,"For QA:

Here is the snippet from the XML config to disable disk space preallocation for dbroot 1:
{noformat}

     	OFF

{noformat}

Here is the scenario to check the feature works. Upload >= 300 000 records in the table then check the size of the segment/dictionary files:
- using 'du' utility 
- calling columnstore_info.table_usage()
- getting data from information_schema.columnstore_files"
1194,MCOL-498,MCOL,Daniel Lee,134524,2019-09-20 17:29:38,"Build verified: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
975463c

Test #1

Create database only, no dataloading
(disconenct source data so data would not be loaded)

1.2.5-1

[root@localhost data1]# du -sh 000.dir
58M	000.dir
[root@localhost data1]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost data1]# du -sh 000.dir
205M	000.dir
[root@localhost data1]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost data1]# du -sh 000.dir
353M	000.dir

Each database creation used about 147MB

1.4.0-1

[root@localhost data1]# du -sh 000.dir
58M	000.dir
[root@localhost ~]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost data1]# du -sh 000.dir
69M	000.dir
[root@localhost data1]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost data1]# du -sh 000.dir
80M	000.dir

Each database created used 11MB


Test #2
Create table and load data

1.2.5-1

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost columnstore]# du -sh data1/000.dir
205M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost columnstore]# du -sh data1/000.dir
2.5G	data1/000.dir

For the 1m database, it used 205M only, the same amount of disk space as create database only in test #1.
The 1m dataset (lineitem has only 6005 rows) fits into the pre-allocated space so no new extents are needed.

1.4.0-1

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost columnstore]# du -sh data1/000.dir
70M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost columnstore]# du -sh data1/000.dir
634M	data1/000.dir


Test #3

Compare disk space utilizaiton on a 1um2pm stack.
dbroot #2 configured not to pre-allocate disk space

with no user database

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
0	data2/000.dir

/data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
# create database without data loading (source data disconnected)

[root@localhost columnstore]# du -sh data1/000.dir
123M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
6.6M	data2/000.dir

/data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
# with data loading

[root@localhost columnstore]# du -sh data1/000.dir
2.8G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
272M	data2/000.dir

/data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10g columnstore 10g
# with data loading

[root@localhost columnstore]# du -sh data1/000.dir
8.4G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
2.4G	data2/000.dir

Start with an empty stack again

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
0	data2/000.dir

[root@localhost columnstore]# du -sh data1/000.dir
5.7G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
2.3G	data2/000.dir
",12,"Build verified: 1.4.0-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
975463c

Test #1

Create database only, no dataloading
(disconenct source data so data would not be loaded)

1.2.5-1

[root@localhost data1]# du -sh 000.dir
58M	000.dir
[root@localhost data1]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost data1]# du -sh 000.dir
205M	000.dir
[root@localhost data1]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost data1]# du -sh 000.dir
353M	000.dir

Each database creation used about 147MB

1.4.0-1

[root@localhost data1]# du -sh 000.dir
58M	000.dir
[root@localhost ~]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost data1]# du -sh 000.dir
69M	000.dir
[root@localhost data1]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost data1]# du -sh 000.dir
80M	000.dir

Each database created used 11MB


Test #2
Create table and load data

1.2.5-1

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost columnstore]# du -sh data1/000.dir
205M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost columnstore]# du -sh data1/000.dir
2.5G	data1/000.dir

For the 1m database, it used 205M only, the same amount of disk space as create database only in test #1.
The 1m dataset (lineitem has only 6005 rows) fits into the pre-allocated space so no new extents are needed.

1.4.0-1

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
[root@localhost columnstore]# du -sh data1/000.dir
70M	data1/000.dir
[root@localhost columnstore]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
[root@localhost columnstore]# du -sh data1/000.dir
634M	data1/000.dir


Test #3

Compare disk space utilizaiton on a 1um2pm stack.
dbroot #2 configured not to pre-allocate disk space

with no user database

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
0	data2/000.dir

/data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1m columnstore 1m
# create database without data loading (source data disconnected)

[root@localhost columnstore]# du -sh data1/000.dir
123M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
6.6M	data2/000.dir

/data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1g columnstore 1g
# with data loading

[root@localhost columnstore]# du -sh data1/000.dir
2.8G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
272M	data2/000.dir

/data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10g columnstore 10g
# with data loading

[root@localhost columnstore]# du -sh data1/000.dir
8.4G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
2.4G	data2/000.dir

Start with an empty stack again

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
0	data2/000.dir

[root@localhost columnstore]# du -sh data1/000.dir
5.7G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
2.3G	data2/000.dir
"
1195,MCOL-498,MCOL,Daniel Lee,134588,2019-09-23 14:24:39,"Test results for S3 installation wiht localStorage, single server


1.4.0-1

MCOL-498 not used

empty system

[root@localhost storagemanager]# du -sh *
60M	cache
60M	fake-cloud
0	journal
232K	metadata

[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database only, no data loading (disconnected source data)

MCOL-498 not used

[root@localhost storagemanager]# du -sh *
207M	cache
207M	fake-cloud
0	journal
588K	metadata

MCOL-498 used

[root@localhost storagemanager]# du -sh *
71M	cache
71M	fake-cloud
0	journal
588K	metadata


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database and load data

MCOL-498 not used

[root@localhost storagemanager]# du -sh *
1.9G	cache
2.3G	fake-cloud
0	journal
600K	metadata

MCOL-498 used

[root@localhost storagemanager]# du -sh *
496M	cache
496M	fake-cloud
0	journal
584K	metadata


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g
* create database and load data

MCOL-498 not used

[root@localhost storagemanager]# du -sh *
1.8G	cache
11G	fake-cloud
0	journal
1.4M	metadata

MCOL-498 used

[root@localhost storagemanager]# du -sh *
1.9G	cache
5.0G	fake-cloud
0	journal
1.3M	metadata
",13,"Test results for S3 installation wiht localStorage, single server


1.4.0-1

MCOL-498 not used

empty system

[root@localhost storagemanager]# du -sh *
60M	cache
60M	fake-cloud
0	journal
232K	metadata

[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database only, no data loading (disconnected source data)

MCOL-498 not used

[root@localhost storagemanager]# du -sh *
207M	cache
207M	fake-cloud
0	journal
588K	metadata

MCOL-498 used

[root@localhost storagemanager]# du -sh *
71M	cache
71M	fake-cloud
0	journal
588K	metadata


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database and load data

MCOL-498 not used

[root@localhost storagemanager]# du -sh *
1.9G	cache
2.3G	fake-cloud
0	journal
600K	metadata

MCOL-498 used

[root@localhost storagemanager]# du -sh *
496M	cache
496M	fake-cloud
0	journal
584K	metadata


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g
* create database and load data

MCOL-498 not used

[root@localhost storagemanager]# du -sh *
1.8G	cache
11G	fake-cloud
0	journal
1.4M	metadata

MCOL-498 used

[root@localhost storagemanager]# du -sh *
1.9G	cache
5.0G	fake-cloud
0	journal
1.3M	metadata
"
1196,MCOL-498,MCOL,Daniel Lee,134602,2019-09-23 16:09:19,"Test results for gluster, 1um2pm configuration

1.4.0-1

dbroot 1 used MCOL-498, dbroot 2 did not


empty system

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
1.5K	data2/000.dir
[root@localhost columnstore]# 


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database only, no data loading (disconnected source data)

[root@localhost columnstore]# du -sh data1/000.dir
63M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
83M	data2/000.dir


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database and load data

[root@localhost columnstore]# du -sh data1/000.dir
279M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
1.9G	data2/000.dir


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g
* create database and load data

[root@localhost columnstore]# du -sh data1/000.dir
2.5G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
7.1G	data2/000.dir
",14,"Test results for gluster, 1um2pm configuration

1.4.0-1

dbroot 1 used MCOL-498, dbroot 2 did not


empty system

[root@localhost columnstore]# du -sh data1/000.dir
58M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
1.5K	data2/000.dir
[root@localhost columnstore]# 


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database only, no data loading (disconnected source data)

[root@localhost columnstore]# du -sh data1/000.dir
63M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
83M	data2/000.dir


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1 columnstore 1g
* create database and load data

[root@localhost columnstore]# du -sh data1/000.dir
279M	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
1.9G	data2/000.dir


[root@localhost storagemanager]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g
* create database and load data

[root@localhost columnstore]# du -sh data1/000.dir
2.5G	data1/000.dir

[root@localhost columnstore]# du -sh data2/000.dir
7.1G	data2/000.dir
"
1197,MCOL-5,MCOL,David Hill,83761,2016-05-29 22:55:31,"CentOS 6.6 build works install on Ubuntu OS, so the changes to do community builds should allow a Ubuntu build to be done ",1,"CentOS 6.6 build works install on Ubuntu OS, so the changes to do community builds should allow a Ubuntu build to be done "
1198,MCOL-5,MCOL,David Hill,85065,2016-07-20 18:43:41,"still working in a private repo called mcol-5.

1. can successfully build a binary package for ubuntu 16 and centos 7 on this repo
2. can successfully install and run regression on U 16 and C 7
3. But its failing to build at this time on centos 6.6 build server. So currently working this",2,"still working in a private repo called mcol-5.

1. can successfully build a binary package for ubuntu 16 and centos 7 on this repo
2. can successfully install and run regression on U 16 and C 7
3. But its failing to build at this time on centos 6.6 build server. So currently working this"
1199,MCOL-5,MCOL,David Hill,85331,2016-08-02 14:30:25,There is a major issue in the ubuntu 16.04 builds. Its failing a lot of the test suites and the main issue to why is the LDI and cpimport mode 1 is failing...,3,There is a major issue in the ubuntu 16.04 builds. Its failing a lot of the test suites and the main issue to why is the LDI and cpimport mode 1 is failing...
1200,MCOL-5,MCOL,David Hall,85438,2016-08-08 21:33:52,"It turns out there are three separate issues involved. All of these could fail on any given OS, so it's been pure luck that they haven't (or have they and we just moved on?)

1) When setting up the cpimport command line for LDI, the command line is built, then parsed into a std::vector<std::string>. Why not put it in the vector to start? Anyway, the address of each string's c_ptr is then stored in another vector to be sent as the exec function's argv. The issue is that the addresses were retrieved in the same loop that was adding the strings to the first vector. When re-allocation was required, all the addresses taken so far were invalidated. Depending on what happened to that memory next determines the behavior. If left unmolested (as is probable in CentOS), then it worked. In Ubuntu, that memory was overwritten and it broke. The values sent in argv had been compromised. To fix, the code was re-written to finish the first vector before taking the addresses. This way re-allocations wouldn't happen after we got them.

2) ExeMgr was crashing. Different versions of the boost library have different implementations to some small degree. In this case, lock was being unlocked twice by typo. The line was supposed to re-lock the lock, but it said unlock. Later, during the destruction of the lock, it asserted that the count wasn't zero. Fixed the typo.

3) PrimProc was crashing during test005. This was during a specific query involving a Join. It could happen for any join, but the timing had to be right. PrimProc takes all the commands it gets from ExeMgr and loads them into various thread pools. The code executing any given thread can decide stuff isn't ready and return -1 to the thread pool, which tells the pool to re-schedule and try this task later. During a join, a lock is maintained where it's locked in one thread an unlocked in another with some nasty stuff trying to keep it right. Anyway, a lock must be unlocked before destroy, or it asserts and aborts. I don't think CentOS does this. When the BATCH_PRIMITIVE_END_JOINER command is received, it often (always?) reschedules. This command must be run before destroy, as it unlocks the lock. Sometime later, a BATCH_PRIMITIVE_DESTROY command arrives and destroys the object containing the lock, and thus the lock itself. Assert, abort. So I added code to the BATCH_PRIMITIVE_DESTROY handler to look for a specific flag -- joinDataReceived -- which only gets set by BATCH_PRIMITIVE_END_JOINER. If it isn't set, the destroy is rescheduled.",4,"It turns out there are three separate issues involved. All of these could fail on any given OS, so it's been pure luck that they haven't (or have they and we just moved on?)

1) When setting up the cpimport command line for LDI, the command line is built, then parsed into a std::vector. Why not put it in the vector to start? Anyway, the address of each string's c_ptr is then stored in another vector to be sent as the exec function's argv. The issue is that the addresses were retrieved in the same loop that was adding the strings to the first vector. When re-allocation was required, all the addresses taken so far were invalidated. Depending on what happened to that memory next determines the behavior. If left unmolested (as is probable in CentOS), then it worked. In Ubuntu, that memory was overwritten and it broke. The values sent in argv had been compromised. To fix, the code was re-written to finish the first vector before taking the addresses. This way re-allocations wouldn't happen after we got them.

2) ExeMgr was crashing. Different versions of the boost library have different implementations to some small degree. In this case, lock was being unlocked twice by typo. The line was supposed to re-lock the lock, but it said unlock. Later, during the destruction of the lock, it asserted that the count wasn't zero. Fixed the typo.

3) PrimProc was crashing during test005. This was during a specific query involving a Join. It could happen for any join, but the timing had to be right. PrimProc takes all the commands it gets from ExeMgr and loads them into various thread pools. The code executing any given thread can decide stuff isn't ready and return -1 to the thread pool, which tells the pool to re-schedule and try this task later. During a join, a lock is maintained where it's locked in one thread an unlocked in another with some nasty stuff trying to keep it right. Anyway, a lock must be unlocked before destroy, or it asserts and aborts. I don't think CentOS does this. When the BATCH_PRIMITIVE_END_JOINER command is received, it often (always?) reschedules. This command must be run before destroy, as it unlocks the lock. Sometime later, a BATCH_PRIMITIVE_DESTROY command arrives and destroys the object containing the lock, and thus the lock itself. Assert, abort. So I added code to the BATCH_PRIMITIVE_DESTROY handler to look for a specific flag -- joinDataReceived -- which only gets set by BATCH_PRIMITIVE_END_JOINER. If it isn't set, the destroy is rescheduled."
1201,MCOL-5,MCOL,Dipti Joshi,85590,2016-08-16 02:48:57,@dhill Is this fixed now ?,5,@dhill Is this fixed now ?
1202,MCOL-5,MCOL,David Hill,85614,2016-08-16 16:28:01,no fix for the mysqld crashing during the test211 run. Both D Hall and I have seen on the centos systems as well..,6,no fix for the mysqld crashing during the test211 run. Both D Hall and I have seen on the centos systems as well..
1203,MCOL-5,MCOL,Andrew Hutchings,85634,2016-08-17 13:10:21,Added a fix for D Hall's fix due to compile failing on CentOS 7 and Ubuntu 16.04: https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/3,7,Added a fix for D Hall's fix due to compile failing on CentOS 7 and Ubuntu 16.04: URL
1204,MCOL-5,MCOL,David Hill,85691,2016-08-19 14:41:32,"Hey andrew, can you review Halls changes since you already know the code with a fix for the compiler error",8,"Hey andrew, can you review Halls changes since you already know the code with a fix for the compiler error"
1205,MCOL-5,MCOL,Andrew Hutchings,85697,2016-08-19 20:38:02,"Review done. I've also tested these changes and haven't hit any related problems since.

Moving to Daniel for QA",9,"Review done. I've also tested these changes and haven't hit any related problems since.

Moving to Daniel for QA"
1206,MCOL-5,MCOL,Daniel Lee,85783,2016-08-23 19:40:55,"Have been testing binary files (not deb packages) on 16.04.
There is a separate ticket for generating deb package.
",10,"Have been testing binary files (not deb packages) on 16.04.
There is a separate ticket for generating deb package.
"
1207,MCOL-5012,MCOL,alexey vorovich,216486,2022-03-09 13:35:01," [~gdorman] [~roman.navrotskiy] [~toddstoffel][~alan.mologorsky][~drrtuy] 

Guys, I discussed  the idea of providing ""Sky ready "" docker with each build with you separately . This is a jira to track initial discussion. Let's express our opinions here and then have a short zoom to decide. ",1," [~gdorman] [~roman.navrotskiy] [~toddstoffel][~alan.mologorsky][~drrtuy] 

Guys, I discussed  the idea of providing ""Sky ready "" docker with each build with you separately . This is a jira to track initial discussion. Let's express our opinions here and then have a short zoom to decide. "
1208,MCOL-5012,MCOL,alexey vorovich,216505,2022-03-09 15:26:15,"The script to configure full cluster is worked by [~charles.newport] in https://jira.mariadb.org/browse/DBAAS-9276 . Codename Direct MOE

It will be a command line interface similar in spirit (but not in invocation details) to EZ MOE  https://mariadbcorp.atlassian.net/wiki/spaces/SKYENG/pages/1423835473/Cluster#Examples

Once this is done we try to deliver what [~gdorman] is asking for. ",2,"The script to configure full cluster is worked by [~charles.newport] in URL . Codename Direct MOE

It will be a command line interface similar in spirit (but not in invocation details) to EZ MOE  URL

Once this is done we try to deliver what [~gdorman] is asking for. "
1209,MCOL-5012,MCOL,alexey vorovich,216506,2022-03-09 15:31:47,"[~toddstoffel] and [~roman.navrotskiy] what do you say ? 

Should Roman start a POC ?

 (we will need to develop naming tag convention for docker in main and other branches.  

after the POC and when [~drrtuy] is back he can review this ..",3,"[~toddstoffel] and [~roman.navrotskiy] what do you say ? 

Should Roman start a POC ?

 (we will need to develop naming tag convention for docker in main and other branches.  

after the POC and when [~drrtuy] is back he can review this .."
1210,MCOL-5012,MCOL,alexey vorovich,216546,2022-03-09 19:10:07,"The major prty for all teams is to allow testing of each server build with Sky. This would be a step in this direction.

Daniel will not be using docker directly but via Direct MOE interface",4,"The major prty for all teams is to allow testing of each server build with Sky. This would be a step in this direction.

Daniel will not be using docker directly but via Direct MOE interface"
1211,MCOL-5013,MCOL,Todd Stoffel,216559,2022-03-09 21:15:13,Assigning to [~David.Hall] for triage.,1,Assigning to [~David.Hall] for triage.
1212,MCOL-5013,MCOL,David Hall,216563,2022-03-09 21:43:12,"Currently, Load Data Infile is implemented by the server. It opens the file, converts the rows to a binary format and streams that binary to the engine who then stores it according to their own rules.
To implement something like this would require coordination with the server team. It could be done two ways. I highly favor the first because it's universal and will work with all engines.

1) MDB would need to learn about s3 in order to open the file and do all the s3 streaming.
2) MDB would have to be taught the new syntax (in the parser) and know to pass the thing to us in some fashion.

In the first case, all the effort is on the Server team. For engines, it would just work.
In the second case, Columnstore team would do the lion's share but some Server work is needed to parse and pass.",2,"Currently, Load Data Infile is implemented by the server. It opens the file, converts the rows to a binary format and streams that binary to the engine who then stores it according to their own rules.
To implement something like this would require coordination with the server team. It could be done two ways. I highly favor the first because it's universal and will work with all engines.

1) MDB would need to learn about s3 in order to open the file and do all the s3 streaming.
2) MDB would have to be taught the new syntax (in the parser) and know to pass the thing to us in some fashion.

In the first case, all the effort is on the Server team. For engines, it would just work.
In the second case, Columnstore team would do the lion's share but some Server work is needed to parse and pass."
1213,MCOL-5013,MCOL,alexey vorovich,218887,2022-04-01 20:34:36,"[~David.Hall] thnks

1. Does the server  read the  large CVS file in memory and then calls us  with a full buffer  or they pass a stream descriptor that we use ?
2  who in server group do we need to discuss the change (any change) in this area",3,"[~David.Hall] thnks

1. Does the server  read the  large CVS file in memory and then calls us  with a full buffer  or they pass a stream descriptor that we use ?
2  who in server group do we need to discuss the change (any change) in this area"
1214,MCOL-5013,MCOL,alexey vorovich,218889,2022-04-01 20:49:03,"and 3.

lets say we adopt your method 2

will we have to implement all the parsing  options  of LDI ",4,"and 3.

lets say we adopt your method 2

will we have to implement all the parsing  options  of LDI "
1215,MCOL-5013,MCOL,Leonid Fedorov,228997,2022-07-08 17:11:27,"I think it's about drone build setting, but don't worry, feature is merged into current branch
We can use either this build, or any latest current build",5,"I think it's about drone build setting, but don't worry, feature is merged into current branch
We can use either this build, or any latest current build"
1216,MCOL-5013,MCOL,Daniel Lee,230244,2022-07-22 22:12:29,"Build tested:  Engine 22.08-1 (#5040), cmapi (#694)

Verified the following

1. loaded 1gb lineitem from aws s3
2. compared all columnstore data to lineitem1, which was imported locally.
3. loaded 10gb lineitem from aws s3

Currently having issues to update large datasets to aws S3
I will continue test when the issue is resolved.

Negative tests:

Error messages should be more informative

{noformat}
*** No credential setup

MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""1g/lineitem.tbl"", ""dleeqadata"", ""mytest"");
+-----------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname) |
+-----------------------------------------------------------+
| {""error"": ""key not provided""}                             |
+-----------------------------------------------------------+
1 row in set (0.025 sec)

Query OK, 0 rows affected (0.026 sec)


*** Invalid access and secret keys

MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""lineitem.tbl"", ""dleeqadata"", ""mytest"");
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)                                                                                                                                        |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {""error"": {""success"": false, ""error"": ""2022-07-22 19:58:09 (1201) ERR  : Error retrieving file lineitem.tbl from S3: The AWS Access Key Id you provided does not exist in our records. [1052]""}} |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (2.204 sec)


*** Correct credential, but wrong S3 data bucket

MariaDB [mytest]>  call calpontsys.columnstore_load_from_s3(""lineitem"", ""lineitem.tbl"", ""helloworld"", ""mytest"");
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)                                                                                                                                                                         |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {""error"": {""success"": false, ""error"": ""2022-07-22 20:00:16 (1272) ERR  : Error retrieving file lineitem.tbl from S3: The authorization header is malformed; the region 'us-west-2' is wrong; expecting 'ap-northeast-1' [1052]""}} |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (1.192 sec)

{noformat}
",6,"Build tested:  Engine 22.08-1 (#5040), cmapi (#694)

Verified the following

1. loaded 1gb lineitem from aws s3
2. compared all columnstore data to lineitem1, which was imported locally.
3. loaded 10gb lineitem from aws s3

Currently having issues to update large datasets to aws S3
I will continue test when the issue is resolved.

Negative tests:

Error messages should be more informative

{noformat}
*** No credential setup

MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""1g/lineitem.tbl"", ""dleeqadata"", ""mytest"");
+-----------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname) |
+-----------------------------------------------------------+
| {""error"": ""key not provided""}                             |
+-----------------------------------------------------------+
1 row in set (0.025 sec)

Query OK, 0 rows affected (0.026 sec)


*** Invalid access and secret keys

MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""lineitem.tbl"", ""dleeqadata"", ""mytest"");
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)                                                                                                                                        |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {""error"": {""success"": false, ""error"": ""2022-07-22 19:58:09 (1201) ERR  : Error retrieving file lineitem.tbl from S3: The AWS Access Key Id you provided does not exist in our records. [1052]""}} |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (2.204 sec)


*** Correct credential, but wrong S3 data bucket

MariaDB [mytest]>  call calpontsys.columnstore_load_from_s3(""lineitem"", ""lineitem.tbl"", ""helloworld"", ""mytest"");
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)                                                                                                                                                                         |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| {""error"": {""success"": false, ""error"": ""2022-07-22 20:00:16 (1272) ERR  : Error retrieving file lineitem.tbl from S3: The authorization header is malformed; the region 'us-west-2' is wrong; expecting 'ap-northeast-1' [1052]""}} |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (1.192 sec)

{noformat}
"
1217,MCOL-5013,MCOL,Daniel Lee,230773,2022-07-27 15:33:07,Close per last test results.,7,Close per last test results.
1218,MCOL-5013,MCOL,Daniel Lee,231344,2022-08-02 19:43:48,"Successfully loaded 75GB of lineitem dataset from an AWS S3 bucket.  It took almost 4.5 hours.

{noformat}
MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""1g/lineitem.tbl"", ""dleeqadata"", ""mytest"");
+------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)        |
+------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""6001215"", ""processed"": ""6001215""} |
+------------------------------------------------------------------+
1 row in set (40.242 sec)

Query OK, 0 rows affected (40.242 sec)

MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""dbt3/100g/lineitem.tbl"", ""mcsmtrdata"", ""mytest"");

+----------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)            |
+----------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""600037902"", ""processed"": ""600037902""} |
+----------------------------------------------------------------------+
1 row in set (4 hours 26 min 25.025 sec)

Query OK, 0 rows affected (4 hours 26 min 25.027 sec)

MariaDB [mytest]> 
MariaDB [mytest]> select count(*) from lineitem;
+-----------+
| count(*)  |
+-----------+
| 606039117 |
+-----------+
1 row in set (29.650 sec)

{noformat}


",8,"Successfully loaded 75GB of lineitem dataset from an AWS S3 bucket.  It took almost 4.5 hours.

{noformat}
MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""1g/lineitem.tbl"", ""dleeqadata"", ""mytest"");
+------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)        |
+------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""6001215"", ""processed"": ""6001215""} |
+------------------------------------------------------------------+
1 row in set (40.242 sec)

Query OK, 0 rows affected (40.242 sec)

MariaDB [mytest]> call calpontsys.columnstore_load_from_s3(""lineitem"", ""dbt3/100g/lineitem.tbl"", ""mcsmtrdata"", ""mytest"");

+----------------------------------------------------------------------+
| columnstore_dataload(tablename, filename, bucket, dbname)            |
+----------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""600037902"", ""processed"": ""600037902""} |
+----------------------------------------------------------------------+
1 row in set (4 hours 26 min 25.025 sec)

Query OK, 0 rows affected (4 hours 26 min 25.027 sec)

MariaDB [mytest]> 
MariaDB [mytest]> select count(*) from lineitem;
+-----------+
| count(*)  |
+-----------+
| 606039117 |
+-----------+
1 row in set (29.650 sec)

{noformat}


"
1219,MCOL-5013,MCOL,alexey vorovich,232434,2022-08-16 20:36:45,"[~leonid.fedorov], i suspect this (1 and 2)  is a small change. could u do this tmrw.
Testing can be brief as well",9,"[~leonid.fedorov], i suspect this (1 and 2)  is a small change. could u do this tmrw.
Testing can be brief as well"
1220,MCOL-5013,MCOL,Leonid Fedorov,232595,2022-08-18 13:06:27,"I've changed the order of params, function name and schema name.
Session variables are hard to change, because columnstore_ prefix is added by
server to variables declared by plugin

so syntax is 
{code}
CALL columnstore_info.load_from_s3(""<bucket>"", ""<file_name>"", ""<db_name>"", ""<table_name>"");
{code}

but settings are
{code}
set columnstore_s3_key='<s3_key>';
set columnstore_s3_secret='<s3_secret>';
set columnstore_s3_region='region';??
{code}",10,"I've changed the order of params, function name and schema name.
Session variables are hard to change, because columnstore_ prefix is added by
server to variables declared by plugin

so syntax is 
{code}
CALL columnstore_info.load_from_s3("""", """", """", """");
{code}

but settings are
{code}
set columnstore_s3_key='';
set columnstore_s3_secret='';
set columnstore_s3_region='region';??
{code}"
1221,MCOL-5013,MCOL,Leonid Fedorov,232596,2022-08-18 13:07:07,Changes can be tested within build 5319,11,Changes can be tested within build 5319
1222,MCOL-5013,MCOL,Daniel Lee,232626,2022-08-18 18:55:44,"Build 22.08-1 (#5319, #5321)

The change is not in these builds.  I noticed that the change was merged about 1 hour go.  I will wait for the next PR build or the cron build tomorrow. ",12,"Build 22.08-1 (#5319, #5321)

The change is not in these builds.  I noticed that the change was merged about 1 hour go.  I will wait for the next PR build or the cron build tomorrow. "
1223,MCOL-5013,MCOL,alexey vorovich,232690,2022-08-19 14:13:45,"there  was a  discussion last night about performance switches for cpimport https://mariadb.slack.com/archives/C03UMC283ND/p1660852966907919

[~leonid.fedorov] please post the progress",13,"there  was a  discussion last night about performance switches for cpimport URL

[~leonid.fedorov] please post the progress"
1224,MCOL-5013,MCOL,Todd Stoffel,236528,2022-10-04 01:18:43,"The goal for our load data from s3 project is to provide the same options as here:
https://mariadb.com/kb/en/load-data-infile/
*terminated by*, *enclosed by*, *escaped by*
in cpimport that would be *-s* , *-E* , *-C*",14,"The goal for our load data from s3 project is to provide the same options as here:
URL
*terminated by*, *enclosed by*, *escaped by*
in cpimport that would be *-s* , *-E* , *-C*"
1225,MCOL-5013,MCOL,Leonid Fedorov,238149,2022-10-19 13:15:35,"7 parameters are implemented, and using of AWS CLI to download s3 data as well
To test PR use 
cmapi https://cspkg.s3.amazonaws.com/index.html?prefix=cmapi/develop/pull_request/757/amd64/
columnstore https://cspkg.s3.amazonaws.com/index.html?prefix=develop/pull_request/5742/10.6-enterprise/amd64/

example of usage
{code}
create table tt (a int, b text) engine columnstore;

set columnstore_s3_region='us-east-1';
set columnstore_s3_key='correct key;
set columnstore_s3_secret='correct secret;
CALL columnstore_info.load_from_s3(""s3://mdb-s3-test"", ""test.cvs"", ""test"", ""tt"", ""|"", """", """");
{code}
",15,"7 parameters are implemented, and using of AWS CLI to download s3 data as well
To test PR use 
cmapi URL
columnstore URL

example of usage
{code}
create table tt (a int, b text) engine columnstore;

set columnstore_s3_region='us-east-1';
set columnstore_s3_key='correct key;
set columnstore_s3_secret='correct secret;
CALL columnstore_info.load_from_s3(""s3://mdb-s3-test"", ""test.cvs"", ""test"", ""tt"", ""|"", """", """");
{code}
"
1226,MCOL-5013,MCOL,Leonid Fedorov,238151,2022-10-19 13:25:05,"function is defined as 
{code}
CREATE OR REPLACE PROCEDURE load_from_s3 (in bucket varchar(256) CHARACTER SET utf8,
                                          in filename varchar(256) CHARACTER SET utf8,
                                          in dbname varchar(256) CHARACTER SET utf8,
                                          in table_name varchar(256) CHARACTER SET utf8,
                                          in terminated_by varchar(256) CHARACTER SET utf8,
                                          in enclosed_by varchar(1) CHARACTER SET utf8,
                                          in escaped_by varchar(1) CHARACTER SET utf8
                                          )
{code}",16,"function is defined as 
{code}
CREATE OR REPLACE PROCEDURE load_from_s3 (in bucket varchar(256) CHARACTER SET utf8,
                                          in filename varchar(256) CHARACTER SET utf8,
                                          in dbname varchar(256) CHARACTER SET utf8,
                                          in table_name varchar(256) CHARACTER SET utf8,
                                          in terminated_by varchar(256) CHARACTER SET utf8,
                                          in enclosed_by varchar(1) CHARACTER SET utf8,
                                          in escaped_by varchar(1) CHARACTER SET utf8
                                          )
{code}"
1227,MCOL-5013,MCOL,Leonid Fedorov,238291,2022-10-20 13:51:14,"Yes, defaults can be set with empty string. Functions don't support default params, so this is workaround",17,"Yes, defaults can be set with empty string. Functions don't support default params, so this is workaround"
1228,MCOL-5013,MCOL,Leonid Fedorov,238357,2022-10-20 22:39:48,"Column separatornhas nondefault value, just set it 5th parameter",18,"Column separatornhas nondefault value, just set it 5th parameter"
1229,MCOL-5013,MCOL,Daniel Lee,238509,2022-10-21 20:43:59,"Build tested: See drone link provided above

Verified terminated_by, enclosed_by, escaped_by characters.  terminated_by character is required and the last two are optional

I have tried loading a 1gb lineitem table, data size is about 750mb and the job never finish over night.  I have tried files with ""|"" or "","" as terminated_by character.
",19,"Build tested: See drone link provided above

Verified terminated_by, enclosed_by, escaped_by characters.  terminated_by character is required and the last two are optional

I have tried loading a 1gb lineitem table, data size is about 750mb and the job never finish over night.  I have tried files with ""|"" or "","" as terminated_by character.
"
1230,MCOL-5013,MCOL,alexey vorovich,238517,2022-10-21 22:03:25,"I tied a smaller file 10k , one column . seems ok .  will try bigger files later
{code:java}
2022-10-21 17:59:46.715517 connecting 0 localhost 3307 pgmabv99 Lena8484!
2022-10-21 17:59:46.718683 Connected ok ihost= 0
2022-10-21 17:59:46.718809 i_host 0 : SHOW STATUS LIKE 'columnstore%';
2022-10-21 17:59:46.719682 {'Variable_name': 'Columnstore_commit_hash', 'Value': '57df5e9-dirty'}
2022-10-21 17:59:46.719794 {'Variable_name': 'Columnstore_version', 'Value': '22.08.2'}
2022-10-21 17:59:46.719872 i_host 0 : drop  database  IF EXISTS d1
2022-10-21 17:59:47.010703 i_host 0 : create database d1
2022-10-21 17:59:47.011583 i_host 0 : use d1
2022-10-21 17:59:47.012231 i_host 0 : drop table IF EXISTS t1
2022-10-21 17:59:47.052555 i_host 0 : create table   t1 (f1 integer) engine=columnstore
2022-10-21 17:59:47.383463 i_host 0 : set columnstore_s3_key=""xxxxxxH3XSA7W"";
2022-10-21 17:59:47.384267 i_host 0 : set columnstore_s3_secret=""xxxxxxxxxxxxxxxX+mJQLbg9rup"";
2022-10-21 17:59:47.384953 i_host 0 : set columnstore_s3_region='us-east-2';
2022-10-21 17:59:47.385480 i_host 0 : CALL columnstore_info.load_from_s3(""s3://avorovich2"", ""data1.csv"", ""d1"", ""t1"", "","", """", """" )
2022-10-21 17:59:49.678133 {'columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by)': b'{""success"": true, ""inserted"": ""10000"", ""processed"": ""10000""}'}
2022-10-21 17:59:49.680068 i_host 0 : SELECT COUNT(*)  FROM t1
2022-10-21 17:59:49.730002 {'COUNT(*)': 10000}
2022-10-21 17:59:49.730170 i_host 0 : select * from information_schema.columnstore_tables
2022-10-21 17:59:49.746288 {'TABLE_SCHEMA': 'd1', 'TABLE_NAME': 't1', 'OBJECT_ID': 3015, 'CREATION_DATE': datetime.datetime(2022, 10, 21, 0, 0), 'COLUMN_COUNT': 1, 'AUTOINCREMENT': None}
2022-10-21 17:59:49.746433 i_host 0 : SELECT dbroot,min_value,max_value , high_water_mark, status  FROM information_schema.columnstore_extents order by status
2022-10-21 17:59:49.753978 {'dbroot': 2, 'min_value': Decimal('1'), 'max_value': Decimal('1'), 'high_water_mark': 2, 'status': 'Available'}
2022-10-21 17:59:49.754245 {'dbroot': 2, 'min_value': Decimal('0'), 'max_value': Decimal('9999'), 'high_water_mark': 8, 'status': 'Available'}
2022-10-21 17:59:49.754357 saving logs ++++++++++++++++++++++++++++++++
2022-10-21 17:59:49.761419 
2022-10-21 17:59:49.763677 
2022-10-21 17:59:50.275734 tar -cvf /root/tmp/docker-logs-dbms.tar /root/tmp/docker_logs_dbms
{code}


",20,"I tied a smaller file 10k , one column . seems ok .  will try bigger files later
{code:java}
2022-10-21 17:59:46.715517 connecting 0 localhost 3307 pgmabv99 Lena8484!
2022-10-21 17:59:46.718683 Connected ok ihost= 0
2022-10-21 17:59:46.718809 i_host 0 : SHOW STATUS LIKE 'columnstore%';
2022-10-21 17:59:46.719682 {'Variable_name': 'Columnstore_commit_hash', 'Value': '57df5e9-dirty'}
2022-10-21 17:59:46.719794 {'Variable_name': 'Columnstore_version', 'Value': '22.08.2'}
2022-10-21 17:59:46.719872 i_host 0 : drop  database  IF EXISTS d1
2022-10-21 17:59:47.010703 i_host 0 : create database d1
2022-10-21 17:59:47.011583 i_host 0 : use d1
2022-10-21 17:59:47.012231 i_host 0 : drop table IF EXISTS t1
2022-10-21 17:59:47.052555 i_host 0 : create table   t1 (f1 integer) engine=columnstore
2022-10-21 17:59:47.383463 i_host 0 : set columnstore_s3_key=""xxxxxxH3XSA7W"";
2022-10-21 17:59:47.384267 i_host 0 : set columnstore_s3_secret=""xxxxxxxxxxxxxxxX+mJQLbg9rup"";
2022-10-21 17:59:47.384953 i_host 0 : set columnstore_s3_region='us-east-2';
2022-10-21 17:59:47.385480 i_host 0 : CALL columnstore_info.load_from_s3(""s3://avorovich2"", ""data1.csv"", ""d1"", ""t1"", "","", """", """" )
2022-10-21 17:59:49.678133 {'columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by)': b'{""success"": true, ""inserted"": ""10000"", ""processed"": ""10000""}'}
2022-10-21 17:59:49.680068 i_host 0 : SELECT COUNT(*)  FROM t1
2022-10-21 17:59:49.730002 {'COUNT(*)': 10000}
2022-10-21 17:59:49.730170 i_host 0 : select * from information_schema.columnstore_tables
2022-10-21 17:59:49.746288 {'TABLE_SCHEMA': 'd1', 'TABLE_NAME': 't1', 'OBJECT_ID': 3015, 'CREATION_DATE': datetime.datetime(2022, 10, 21, 0, 0), 'COLUMN_COUNT': 1, 'AUTOINCREMENT': None}
2022-10-21 17:59:49.746433 i_host 0 : SELECT dbroot,min_value,max_value , high_water_mark, status  FROM information_schema.columnstore_extents order by status
2022-10-21 17:59:49.753978 {'dbroot': 2, 'min_value': Decimal('1'), 'max_value': Decimal('1'), 'high_water_mark': 2, 'status': 'Available'}
2022-10-21 17:59:49.754245 {'dbroot': 2, 'min_value': Decimal('0'), 'max_value': Decimal('9999'), 'high_water_mark': 8, 'status': 'Available'}
2022-10-21 17:59:49.754357 saving logs ++++++++++++++++++++++++++++++++
2022-10-21 17:59:49.761419 
2022-10-21 17:59:49.763677 
2022-10-21 17:59:50.275734 tar -cvf /root/tmp/docker-logs-dbms.tar /root/tmp/docker_logs_dbms
{code}


"
1231,MCOL-5013,MCOL,Daniel Lee,238978,2022-10-25 16:47:11,"Retested the build and at first, I still had issues loading 1 mb and 1 gb lineitem datasets, with 6005 and 6001215 rows respectively.  The commands hung for over 10 minutes without any terminal output and I could not find any log files.  After few tries, things just started to work.  I was even able to load a 10 gb lineitem dataset.  I am not sure what the hanging issue was and I have not been able to reproduce it.

Successful tests: 
{noformat}
MariaDB [mytest]> CALL columnstore_info.load_from_s3(""s3://dleeqadata"", ""1m/lineitem.tbl"", ""mytest"", ""lineitem"", ""|"", """", """" );
+----------------------------------------------------------------------------------------------------+
| columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by) |
+----------------------------------------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""6005"", ""processed"": ""6005""}                                         |
+----------------------------------------------------------------------------------------------------+
1 row in set (2.214 sec)

Query OK, 0 rows affected (2.214 sec)

MariaDB [mytest]> CALL columnstore_info.load_from_s3(""s3://dleeqadata"", ""1g/lineitem.tbl"", ""mytest"", ""lineitem"", ""|"", """", """" );
+----------------------------------------------------------------------------------------------------+
| columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by) |
+----------------------------------------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""6001215"", ""processed"": ""6001215""}                                   |
+----------------------------------------------------------------------------------------------------+
1 row in set (16.243 sec)


MariaDB [mytest]> CALL columnstore_info.load_from_s3(""s3://dleeqadata"", ""10g/lineitem.tbl"", ""mytest"", ""lineitem"", ""|"", """", """" );
+----------------------------------------------------------------------------------------------------+
| columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by) |
+----------------------------------------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""59986052"", ""processed"": ""59986052""}                                 |
+----------------------------------------------------------------------------------------------------+
1 row in set (2 min 25.325 sec)
{noformat}

",21,"Retested the build and at first, I still had issues loading 1 mb and 1 gb lineitem datasets, with 6005 and 6001215 rows respectively.  The commands hung for over 10 minutes without any terminal output and I could not find any log files.  After few tries, things just started to work.  I was even able to load a 10 gb lineitem dataset.  I am not sure what the hanging issue was and I have not been able to reproduce it.

Successful tests: 
{noformat}
MariaDB [mytest]> CALL columnstore_info.load_from_s3(""s3://dleeqadata"", ""1m/lineitem.tbl"", ""mytest"", ""lineitem"", ""|"", """", """" );
+----------------------------------------------------------------------------------------------------+
| columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by) |
+----------------------------------------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""6005"", ""processed"": ""6005""}                                         |
+----------------------------------------------------------------------------------------------------+
1 row in set (2.214 sec)

Query OK, 0 rows affected (2.214 sec)

MariaDB [mytest]> CALL columnstore_info.load_from_s3(""s3://dleeqadata"", ""1g/lineitem.tbl"", ""mytest"", ""lineitem"", ""|"", """", """" );
+----------------------------------------------------------------------------------------------------+
| columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by) |
+----------------------------------------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""6001215"", ""processed"": ""6001215""}                                   |
+----------------------------------------------------------------------------------------------------+
1 row in set (16.243 sec)


MariaDB [mytest]> CALL columnstore_info.load_from_s3(""s3://dleeqadata"", ""10g/lineitem.tbl"", ""mytest"", ""lineitem"", ""|"", """", """" );
+----------------------------------------------------------------------------------------------------+
| columnstore_dataload(bucket, filename, dbname, table_name, terminated_by, enclosed_by, escaped_by) |
+----------------------------------------------------------------------------------------------------+
| {""success"": true, ""inserted"": ""59986052"", ""processed"": ""59986052""}                                 |
+----------------------------------------------------------------------------------------------------+
1 row in set (2 min 25.325 sec)
{noformat}

"
1232,MCOL-5013,MCOL,Daniel Lee,243019,2022-11-21 21:31:58,"Build verified:

engine: 84bb4e56b81c2f1e99151f198bf939612b09004e
server: e3ed2f0ab1e287218ae72ecf77d95cc745795353
buildNo: 6088",22,"Build verified:

engine: 84bb4e56b81c2f1e99151f198bf939612b09004e
server: e3ed2f0ab1e287218ae72ecf77d95cc745795353
buildNo: 6088"
1233,MCOL-5013,MCOL,Todd Stoffel,248502,2023-01-24 15:59:42,[~allen.herrera] This feature is only for SkySQL. We'll document it when it goes GA in March.,23,[~allen.herrera] This feature is only for SkySQL. We'll document it when it goes GA in March.
1234,MCOL-5021,MCOL,David Hall,218195,2022-03-25 18:01:04,"Rather than writing to use uint_t value as 1, treat the aux as a bitmap and use one of the bits. This allows for easier expansion in the future if needed.",1,"Rather than writing to use uint_t value as 1, treat the aux as a bitmap and use one of the bits. This allows for easier expansion in the future if needed."
1235,MCOL-5021,MCOL,alexey vorovich,226087,2022-06-07 17:45:26,[~toddstoffel]  Do we document a generic  upgrade script that users have to run after they upgrade binary ?,2,[~toddstoffel]  Do we document a generic  upgrade script that users have to run after they upgrade binary ?
1236,MCOL-5021,MCOL,alexey vorovich,226113,2022-06-07 22:01:45,"[~toddstoffel] We discussed two options
1. document the following  command that customer must  to run during upgrade
ALTER TABLE calpontsys.systable ADD COLUMN (auxcolumnoid INT NOT NULL DEFAULT 0);
2. if we already have a script that must be run during upgrades , then we can add the command above to it. From  your response it seems that we have no such script. If you confirm , then option 1 is way to go",3,"[~toddstoffel] We discussed two options
1. document the following  command that customer must  to run during upgrade
ALTER TABLE calpontsys.systable ADD COLUMN (auxcolumnoid INT NOT NULL DEFAULT 0);
2. if we already have a script that must be run during upgrades , then we can add the command above to it. From  your response it seems that we have no such script. If you confirm , then option 1 is way to go"
1237,MCOL-5021,MCOL,David Hall,227068,2022-06-17 16:59:54,"FastDelete = false => min/max are updated and state is valid
FastDelete = true => min/max are not set and state is invalid
Before this feature, expect min/max to be updated and state to be valid (assuming it was valid before the delete). This is a from MCOL-2044 in 6.1.1
Prior to 6.1.1, expect min/max to be not set and state = invalid.
The purpose of this flag is to allow the user to choose which behavior they would like.",4,"FastDelete = false => min/max are updated and state is valid
FastDelete = true => min/max are not set and state is invalid
Before this feature, expect min/max to be updated and state to be valid (assuming it was valid before the delete). This is a from MCOL-2044 in 6.1.1
Prior to 6.1.1, expect min/max to be not set and state = invalid.
The purpose of this flag is to allow the user to choose which behavior they would like."
1238,MCOL-5021,MCOL,Gagan Goel,231507,2022-08-04 20:42:58,"A minor update:

To enable the FastDelete option, the .xml file should have a value 'y' or 'Y', instead of 'true':

{code:xml}
        <WriteEngine>
                <BulkRoot>/var/lib/columnstore/data/bulk</BulkRoot>
                <BulkRollbackDir>/var/lib/columnstore/data1/systemFiles/bulkRollback</BulkRollbackDir>
                <MaxFileSystemDiskUsagePct>98</MaxFileSystemDiskUsagePct>
                <CompressedPaddingBlocks>1</CompressedPaddingBlocks> <!-- Number of blocks used to pad compressed chunks -->
                <FastDelete>y</FastDelete>
        </WriteEngine>
{code}",5,"A minor update:

To enable the FastDelete option, the .xml file should have a value 'y' or 'Y', instead of 'true':

{code:xml}
        
                /var/lib/columnstore/data/bulk
                /var/lib/columnstore/data1/systemFiles/bulkRollback
                98
                1 
                y
        
{code}"
1239,MCOL-5021,MCOL,Daniel Lee,232539,2022-08-17 20:53:28,"Build verified: 22.08-1 (#5312)

Feature implemented.  Verified:

system catalog
delete functionality
Performance tests
MTR test suites

Closing this ticket.  New tickets will be opened if issues are later.",6,"Build verified: 22.08-1 (#5312)

Feature implemented.  Verified:

system catalog
delete functionality
Performance tests
MTR test suites

Closing this ticket.  New tickets will be opened if issues are later."
1240,MCOL-5021,MCOL,Gagan Goel,233030,2022-08-23 16:42:05,"Below are the final numbers on DELETE performance tests:

GCP VM:

16vcpus, 64GB memory, 128GB HDD
Dataset = 1,000,000 rows
Results are averaged over 10 runs.
OS: Ubuntu20.

 !DELETE_Performance_Test_GCP.png|! 

Local Machine:

8vcpus, 24GB memory, 500GB SSD
Dataset = 1,000,000 rows
Results are averaged over 10 runs.
OS: Ubuntu20

 !DELETE_Performance_Test_Local_Machine.png|! ",7,"Below are the final numbers on DELETE performance tests:

GCP VM:

16vcpus, 64GB memory, 128GB HDD
Dataset = 1,000,000 rows
Results are averaged over 10 runs.
OS: Ubuntu20.

 !DELETE_Performance_Test_GCP.png|! 

Local Machine:

8vcpus, 24GB memory, 500GB SSD
Dataset = 1,000,000 rows
Results are averaged over 10 runs.
OS: Ubuntu20

 !DELETE_Performance_Test_Local_Machine.png|! "
1241,MCOL-5021,MCOL,alexey vorovich,235105,2022-09-15 16:20:36,"[~jacob.moorman] [~GeoffMontee]

this is a significant performance improvement .  You do make a reference to this in release notes. 
Should you also mention the actual  numbers? 2x to 30x .. ",8,"[~jacob.moorman] [~GeoffMontee]

this is a significant performance improvement .  You do make a reference to this in release notes. 
Should you also mention the actual  numbers? 2x to 30x .. "
1242,MCOL-5037,MCOL,Daniel Lee,218764,2022-03-31 20:50:05,"Build tested: 6.3.1-1 (#4206)

Executed MTR regression suites
Executed tests in MCOL-4912.  Timing has been updated in
https://docs.google.com/spreadsheets/d/1AtXR9-D-KZT4hlJ5wA8Ih-_GpNh6CYai2tjz7RNzg2k/edit?usp=sharing
",1,"Build tested: 6.3.1-1 (#4206)

Executed MTR regression suites
Executed tests in MCOL-4912.  Timing has been updated in
URL
"
1243,MCOL-5044,MCOL,Roman,224952,2022-05-26 14:28:12,"There is a scheduling policy in the current implementation of a thread pool. It has 3 fixed priorities to allow to favor queries with a higher priority running primitive jobs in PrimProc. The scheduling policy picks a number(3 with default settings) of morsel tasks for execution out of a common queue. This scheduling doesn't fit for multiple parallel queries workload pattern b/c it tend to allocate all threads to run primitive jobs that belongs to a query that reaches PP first. 
The main idea is to replace existing scheduler policy AKA thread pool with a fair scheduling policy. Here is  the model:
-  every primitive job has a cost(initial version has a fixed cost) that is based on a set of operations and columns involved(the model doesn't take a morsel size into account b/c they are roughly the same in terms of records number). 
- every primitive job belongs to a certain transaction(!!! We are talking about SELECTs only b/c it is PPP. Every SELECT is a separate txn !!!)
FairThreadPool picks a primitive job that belongs to a transaction with a lowest combined cost of completed primitive jobs. (Who wants tech details about the implementation plz look at the commits)",1,"There is a scheduling policy in the current implementation of a thread pool. It has 3 fixed priorities to allow to favor queries with a higher priority running primitive jobs in PrimProc. The scheduling policy picks a number(3 with default settings) of morsel tasks for execution out of a common queue. This scheduling doesn't fit for multiple parallel queries workload pattern b/c it tend to allocate all threads to run primitive jobs that belongs to a query that reaches PP first. 
The main idea is to replace existing scheduler policy AKA thread pool with a fair scheduling policy. Here is  the model:
-  every primitive job has a cost(initial version has a fixed cost) that is based on a set of operations and columns involved(the model doesn't take a morsel size into account b/c they are roughly the same in terms of records number). 
- every primitive job belongs to a certain transaction(!!! We are talking about SELECTs only b/c it is PPP. Every SELECT is a separate txn !!!)
FairThreadPool picks a primitive job that belongs to a transaction with a lowest combined cost of completed primitive jobs. (Who wants tech details about the implementation plz look at the commits)"
1244,MCOL-5044,MCOL,Roman,224956,2022-05-26 14:41:17,"Here are the first benchmark setup that I run on c59xlarge AWS instance(36 cores). Dataset is flights that is available [here.|https://github.com/mariadb-corporation/mariadb-columnstore-samples/]. There were 13 mln records(to fit everything into memory). I run a single query that hits PP harder than EM in 5 threads using sysbench(see .lua script attached). Here is the query.
{noformat}
select s from (select count(*) as s from flights group by tail_num)sub;
{noformat}
Here are some results(see the attached latency distribution histograms also).
*develop-6, 44d326ef*
{noformat}
General statistics:                                                                  
    total time:                          54.7672s                                  
    total number of events:              500                                  

Latency (ms):                                                                        
         min:                                  321.53                                
         avg:                                  547.31                                
         max:                                  968.99                               
         95th percentile:                      682.06                         
         sum:                               273653.81      
 {noformat}
*MCOL-5044-3, 61a1242b*
{noformat}
 General statistics:
    total time:                          47.2456s
    total number of events:              500

Latency (ms):
         min:                                  212.46
         avg:                                  471.78
         max:                                  708.09
         95th percentile:                      601.29
         sum:                               235890.47
{noformat}
The total time, 95 percentile are 12% better with the fair scheduling policy. 

To be precise a mixed workload doesn't have significant positive effect as a PP-heavy queries, the improvement lies within statistical error.",2,"Here are the first benchmark setup that I run on c59xlarge AWS instance(36 cores). Dataset is flights that is available [here.|URL There were 13 mln records(to fit everything into memory). I run a single query that hits PP harder than EM in 5 threads using sysbench(see .lua script attached). Here is the query.
{noformat}
select s from (select count(*) as s from flights group by tail_num)sub;
{noformat}
Here are some results(see the attached latency distribution histograms also).
*develop-6, 44d326ef*
{noformat}
General statistics:                                                                  
    total time:                          54.7672s                                  
    total number of events:              500                                  

Latency (ms):                                                                        
         min:                                  321.53                                
         avg:                                  547.31                                
         max:                                  968.99                               
         95th percentile:                      682.06                         
         sum:                               273653.81      
 {noformat}
*MCOL-5044-3, 61a1242b*
{noformat}
 General statistics:
    total time:                          47.2456s
    total number of events:              500

Latency (ms):
         min:                                  212.46
         avg:                                  471.78
         max:                                  708.09
         95th percentile:                      601.29
         sum:                               235890.47
{noformat}
The total time, 95 percentile are 12% better with the fair scheduling policy. 

To be precise a mixed workload doesn't have significant positive effect as a PP-heavy queries, the improvement lies within statistical error."
1245,MCOL-5044,MCOL,Roman,225705,2022-06-03 09:40:46,"The mixed workload test(slap6.lua) doesn't make a benefit so obvious though.
develop-6 run on a 16 core Xeon E5620  @ 2.40GHz
{noformat}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    3267.187 |*                                        1
    3326.551 |**                                       2
    3386.993 |**                                       2
    3448.533 |*                                        1
    3511.192 |*                                        1
    3639.945 |********                                 8
    3706.081 |******                                   6
    3773.420 |*******                                  7
    3841.981 |*****************                        16
    3911.789 |**********************                   21
    3982.864 |*************************                24
    4055.231 |********************************         30
    4128.913 |************************************     34
    4203.934 |**************************************** 38
    4280.318 |***************************              26
    4358.090 |**********************                   21
    4437.275 |*******************                      18
    4517.898 |************                             11
    4599.987 |**************                           13
    4683.567 |************                             11
    4768.666 |****                                     4
    4855.311 |*                                        1
    4943.530 |*                                        1
    5033.352 |*                                        1
    5124.806 |*                                        1
    5409.260 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            1800
        write:                           0
        other:                           0
        total:                           1800
    transactions:                        300    (0.72 per sec.)
    queries:                             1800   (4.31 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          417.4668s
    total number of events:              300

Latency (ms):
         min:                                 3288.39
         avg:                                 4169.14
         max:                                 5418.50
         95th percentile:                     4683.57
         sum:                              1250742.04
 {noformat}
vs MCOL-5044
{noformat}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    2362.716 |*                                        1
    3511.192 |*                                        1
    3574.989 |**                                       3
    3639.945 |*                                        2
    3706.081 |******                                   8
    3773.420 |****                                     6
    3841.981 |***********                              15
    3911.789 |*************                            17
    3982.864 |******************                       24
    4055.231 |**************************               35
    4128.913 |*********************************        45
    4203.934 |**************************************** 54
    4280.318 |**********************                   30
    4358.090 |***********                              15
    4437.275 |**********                               13
    4517.898 |********                                 11
    4599.987 |******                                   8
    4683.567 |****                                     6
    4768.666 |**                                       3
    4855.311 |*                                        2
    5124.806 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            1800
        write:                           0
        other:                           0
        total:                           1800
    transactions:                        300    (0.72 per sec.)
    queries:                             1800   (4.33 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          416.1538s
    total number of events:              300

Latency (ms):
         min:                                 2381.25
         avg:                                 4153.23
         max:                                 5093.98
         95th percentile:                     4599.99
         sum:                              1245967.92

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   415.3226/0.59
{noformat}
",3,"The mixed workload test(slap6.lua) doesn't make a benefit so obvious though.
develop-6 run on a 16 core Xeon E5620  @ 2.40GHz
{noformat}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    3267.187 |*                                        1
    3326.551 |**                                       2
    3386.993 |**                                       2
    3448.533 |*                                        1
    3511.192 |*                                        1
    3639.945 |********                                 8
    3706.081 |******                                   6
    3773.420 |*******                                  7
    3841.981 |*****************                        16
    3911.789 |**********************                   21
    3982.864 |*************************                24
    4055.231 |********************************         30
    4128.913 |************************************     34
    4203.934 |**************************************** 38
    4280.318 |***************************              26
    4358.090 |**********************                   21
    4437.275 |*******************                      18
    4517.898 |************                             11
    4599.987 |**************                           13
    4683.567 |************                             11
    4768.666 |****                                     4
    4855.311 |*                                        1
    4943.530 |*                                        1
    5033.352 |*                                        1
    5124.806 |*                                        1
    5409.260 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            1800
        write:                           0
        other:                           0
        total:                           1800
    transactions:                        300    (0.72 per sec.)
    queries:                             1800   (4.31 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          417.4668s
    total number of events:              300

Latency (ms):
         min:                                 3288.39
         avg:                                 4169.14
         max:                                 5418.50
         95th percentile:                     4683.57
         sum:                              1250742.04
 {noformat}
vs MCOL-5044
{noformat}
Latency histogram (values are in milliseconds)
       value  ------------- distribution ------------- count
    2362.716 |*                                        1
    3511.192 |*                                        1
    3574.989 |**                                       3
    3639.945 |*                                        2
    3706.081 |******                                   8
    3773.420 |****                                     6
    3841.981 |***********                              15
    3911.789 |*************                            17
    3982.864 |******************                       24
    4055.231 |**************************               35
    4128.913 |*********************************        45
    4203.934 |**************************************** 54
    4280.318 |**********************                   30
    4358.090 |***********                              15
    4437.275 |**********                               13
    4517.898 |********                                 11
    4599.987 |******                                   8
    4683.567 |****                                     6
    4768.666 |**                                       3
    4855.311 |*                                        2
    5124.806 |*                                        1
 
SQL statistics:
    queries performed:
        read:                            1800
        write:                           0
        other:                           0
        total:                           1800
    transactions:                        300    (0.72 per sec.)
    queries:                             1800   (4.33 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          416.1538s
    total number of events:              300

Latency (ms):
         min:                                 2381.25
         avg:                                 4153.23
         max:                                 5093.98
         95th percentile:                     4599.99
         sum:                              1245967.92

Threads fairness:
    events (avg/stddev):           100.0000/0.00
    execution time (avg/stddev):   415.3226/0.59
{noformat}
"
1246,MCOL-5044,MCOL,Roman,230224,2022-07-22 16:45:53,"4QA the first test goal would be to have the same functionality for single-node and cluster.
The second thing is that a delay for a small query should be smaller when PrimProcs in the cluster is busy with a complex query(or queries) that consumes the system's resource. In general I expect the current develop to increase a throughput for a mixed workload with short lasting and long lasting queries.",4,"4QA the first test goal would be to have the same functionality for single-node and cluster.
The second thing is that a delay for a small query should be smaller when PrimProcs in the cluster is busy with a complex query(or queries) that consumes the system's resource. In general I expect the current develop to increase a throughput for a mixed workload with short lasting and long lasting queries."
1247,MCOL-5044,MCOL,Daniel Lee,230774,2022-07-27 15:34:48,Close by regression tests,5,Close by regression tests
1248,MCOL-505,MCOL,Andrew Hutchings,90571,2017-01-12 17:08:45,Commit message covers everything. I'll need to backport this to the develop-1.0 branch after review.,1,Commit message covers everything. I'll need to backport this to the develop-1.0 branch after review.
1249,MCOL-505,MCOL,Andrew Hutchings,90605,2017-01-14 03:04:38,Regression suite looks good. Merged into develop-1.0 as well.,2,Regression suite looks good. Merged into develop-1.0 as well.
1250,MCOL-506,MCOL,Andrew Hutchings,90773,2017-01-18 14:38:51,"Two pull requests for this, one for 1.0 and one for 1.1. The only difference is the README.md changes.",1,"Two pull requests for this, one for 1.0 and one for 1.1. The only difference is the README.md changes."
1251,MCOL-506,MCOL,David Hill,90881,2017-01-20 23:50:37,"closing, passing regression test",2,"closing, passing regression test"
1252,MCOL-507,MCOL,Andrew Hutchings,90647,2017-01-16 12:49:50,"This patch shows the following performance differences over MCOL-505 on my single server setup:

* First query after client connection reduced from 0.18 secs to 0.10 secs
* Time of first I_S.columnstore_columns query in a connection halved
* Time of subsequent I_S.columnstore_columns queries reduced to < 0.2 seconds

Regression suite passes. Targeting 1.1 only since it is a large change. The pull request contains details of the code level changes.",1,"This patch shows the following performance differences over MCOL-505 on my single server setup:

* First query after client connection reduced from 0.18 secs to 0.10 secs
* Time of first I_S.columnstore_columns query in a connection halved
* Time of subsequent I_S.columnstore_columns queries reduced to < 0.2 seconds

Regression suite passes. Targeting 1.1 only since it is a large change. The pull request contains details of the code level changes."
1253,MCOL-507,MCOL,David Hall,90652,2017-01-16 14:20:42,Regression tests pass. There are no further tests to be done.,2,Regression tests pass. There are no further tests to be done.
1254,MCOL-5089,MCOL,Roman,223869,2022-05-16 10:58:48,"Previously MCS5 w/o MCOL-4912, MCOL-4917 test000 converges in roughly 30 minutes. With MCOL-4917 + MCOL-4912 it takes 1 hour and 30 minutes(rbtree traversal takes more then simple array traversal). With the merge patch it takes 4 minutes, 40 seconds as usually. All runs were done are with EM image I mentioned.  ",1,"Previously MCS5 w/o MCOL-4912, MCOL-4917 test000 converges in roughly 30 minutes. With MCOL-4917 + MCOL-4912 it takes 1 hour and 30 minutes(rbtree traversal takes more then simple array traversal). With the merge patch it takes 4 minutes, 40 seconds as usually. All runs were done are with EM image I mentioned.  "
1255,MCOL-5089,MCOL,Roman,224062,2022-05-17 15:21:45,This issue says nothing about develop or develop-6 so develop-5.,2,This issue says nothing about develop or develop-6 so develop-5.
1256,MCOL-5089,MCOL,Daniel Lee,224274,2022-05-19 17:06:20,"Build tested: 5.6.6-1 (Drone 4443)

Unabled to reproduce the slowness using an older release 5.6.4-2 using the provided EM file.  MCS is unable to load the large EM file.  Therefore, got base line timing for the release when EM is virtually empty.

With 5.6.6-1, timings for both empty and large EMs are virutally the same, which is also comparable to the timing to 5.6.4-2.  It has been verified that a large EM no longer slow down LDI, query, and DMLs.


5.6.6-1
-------
----- empty dbrm

[centos8:root~]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	1m20.191s
user	3m40.372s
sys	0m6.609s

[centos8:root~]# time mariadb tpch10 < /data/qa/autopilot/performance/dbt3/sql/10g/19.sql
revenue
37135011.7543

real	0m41.344s
user	0m0.008s
sys	0m0.020s
[centos8:root~]# 

MariaDB [tpch10]> update lineitem set l_orderkey=l_partkey, l_shipdate = l_commitdate;
Query OK, 59986052 rows affected (4 min 26.296 sec)
Rows matched: 59986052  Changed: 59986052  Warnings: 0

MariaDB [tpch10]> delete from orders;
Query OK, 15000000 rows affected (56.704 sec)

----- large dbrm

[centos8:root~]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	1m21.478s
user	3m39.537s
sys	0m6.737s

[centos8:root~]# time mariadb tpch10 < /data/qa/autopilot/performance/dbt3/sql/10g/19.sql
revenue
37135011.7543

real	0m40.729s
user	0m0.025s
sys	0m0.005s

MariaDB [tpch10]> update lineitem set l_orderkey=l_partkey, l_shipdate = l_commitdate;
Query OK, 59986052 rows affected (4 min 17.451 sec)
Rows matched: 59986052  Changed: 59986052  Warnings: 0

MariaDB [tpch10]> delete from orders;
Query OK, 15000000 rows affected (1 min 3.001 sec)



5.6.4-2
-------
----- empty dbrm

time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	1m23.342s
user	3m43.255s
sys	0m6.843s


[centos8:root~]# time mariadb tpch10 < /data/qa/autopilot/performance/dbt3/sql/10g/19.sql
revenue
37135011.7543

real	0m42.270s
user	0m0.020s
sys	0m0.009s
[centos8:root~]# 

MariaDB [tpch10]> update lineitem set l_orderkey=l_partkey, l_shipdate = l_commitdate;
Query OK, 59986052 rows affected (4 min 21.257 sec)
Rows matched: 59986052  Changed: 59986052  Warnings: 0

MariaDB [tpch10]> delete from orders;
Query OK, 15000000 rows affected (1 min 4.749 sec)
",3,"Build tested: 5.6.6-1 (Drone 4443)

Unabled to reproduce the slowness using an older release 5.6.4-2 using the provided EM file.  MCS is unable to load the large EM file.  Therefore, got base line timing for the release when EM is virtually empty.

With 5.6.6-1, timings for both empty and large EMs are virutally the same, which is also comparable to the timing to 5.6.4-2.  It has been verified that a large EM no longer slow down LDI, query, and DMLs.


5.6.6-1
-------
----- empty dbrm

[centos8:root~]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	1m20.191s
user	3m40.372s
sys	0m6.609s

[centos8:root~]# time mariadb tpch10 < /data/qa/autopilot/performance/dbt3/sql/10g/19.sql
revenue
37135011.7543

real	0m41.344s
user	0m0.008s
sys	0m0.020s
[centos8:root~]# 

MariaDB [tpch10]> update lineitem set l_orderkey=l_partkey, l_shipdate = l_commitdate;
Query OK, 59986052 rows affected (4 min 26.296 sec)
Rows matched: 59986052  Changed: 59986052  Warnings: 0

MariaDB [tpch10]> delete from orders;
Query OK, 15000000 rows affected (56.704 sec)

----- large dbrm

[centos8:root~]# time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	1m21.478s
user	3m39.537s
sys	0m6.737s

[centos8:root~]# time mariadb tpch10 < /data/qa/autopilot/performance/dbt3/sql/10g/19.sql
revenue
37135011.7543

real	0m40.729s
user	0m0.025s
sys	0m0.005s

MariaDB [tpch10]> update lineitem set l_orderkey=l_partkey, l_shipdate = l_commitdate;
Query OK, 59986052 rows affected (4 min 17.451 sec)
Rows matched: 59986052  Changed: 59986052  Warnings: 0

MariaDB [tpch10]> delete from orders;
Query OK, 15000000 rows affected (1 min 3.001 sec)



5.6.4-2
-------
----- empty dbrm

time /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch10 columnstore 10g

real	1m23.342s
user	3m43.255s
sys	0m6.843s


[centos8:root~]# time mariadb tpch10 < /data/qa/autopilot/performance/dbt3/sql/10g/19.sql
revenue
37135011.7543

real	0m42.270s
user	0m0.020s
sys	0m0.009s
[centos8:root~]# 

MariaDB [tpch10]> update lineitem set l_orderkey=l_partkey, l_shipdate = l_commitdate;
Query OK, 59986052 rows affected (4 min 21.257 sec)
Rows matched: 59986052  Changed: 59986052  Warnings: 0

MariaDB [tpch10]> delete from orders;
Query OK, 15000000 rows affected (1 min 4.749 sec)
"
1257,MCOL-5089,MCOL,David Hall,225743,2022-06-03 15:59:41,"The 5.6.6-1 beta test build can be found at:
https://es-repo.mariadb.net/jenkins/ENTERPRISE/bb-10.5.16-11-cs-6.5.6-1/cee626c87b62b893dc539cf2af39a5521f40a27e/
",4,"The 5.6.6-1 beta test build can be found at:
URL
"
1258,MCOL-5089,MCOL,Roman,226470,2022-06-10 13:57:57,Plz review the bug fix.,5,Plz review the bug fix.
1259,MCOL-5089,MCOL,Roman,226474,2022-06-10 14:08:48,[~David.Hall] Could you make a package from develop-5?,6,[~David.Hall] Could you make a package from develop-5?
1260,MCOL-509,MCOL,David Hill,91909,2017-02-15 21:58:30,"I have the make installs working plus I committed a script that is used to start CS up so regression test can be done.
Working on regression testing next",1,"I have the make installs working plus I committed a script that is used to start CS up so regression test can be done.
Working on regression testing next"
1261,MCOL-509,MCOL,David Hill,91969,2017-02-16 19:37:09,"First step toward getting the regression test working on buildbot, I got test000 passing...
So will work on a small subset of the regression test cases that can be run bu BuildBot

Updating Columnstore.xml settings.
22restartsystem   Fri Feb 17 02:21:55 2017
23
24   System being restarted now ............
25   Successful restart of System 
26
27Running test000.sh.
28000 Create/Load Uncompressed Tables:  Passed
29Running test001.sh.

",2,"First step toward getting the regression test working on buildbot, I got test000 passing...
So will work on a small subset of the regression test cases that can be run bu BuildBot

Updating Columnstore.xml settings.
22restartsystem   Fri Feb 17 02:21:55 2017
23
24   System being restarted now ............
25   Successful restart of System 
26
27Running test000.sh.
28000 Create/Load Uncompressed Tables:  Passed
29Running test001.sh.

"
1262,MCOL-509,MCOL,David Hill,92257,2017-02-24 15:12:11,"I have everything working for centos6 ,7 and suse12. successfuly build, startsystem and run regression test.
Ubuntu and Debian are having issues for some reason starting up. So those 2 are still work in progress",3,"I have everything working for centos6 ,7 and suse12. successfuly build, startsystem and run regression test.
Ubuntu and Debian are having issues for some reason starting up. So those 2 are still work in progress"
1263,MCOL-509,MCOL,David Hill,92445,2017-03-01 21:20:48,"Woo Hoo...

With Andrews change to get the system started on ubuntu and debian and an additional change I had to make to get the regression test script to work on those 2, I have all the buildbot successfully running inclduing test a cutdown version of the regression test, test000 and test001.

still to do... 

1. Tie it into build packages for repo extractions
2. doing a run based on a time set where it will run the complete regression test suite.",4,"Woo Hoo...

With Andrews change to get the system started on ubuntu and debian and an additional change I had to make to get the regression test script to work on those 2, I have all the buildbot successfully running inclduing test a cutdown version of the regression test, test000 and test001.

still to do... 

1. Tie it into build packages for repo extractions
2. doing a run based on a time set where it will run the complete regression test suite."
1264,MCOL-509,MCOL,David Hill,97436,2017-07-07 15:08:12,"completed. now have all OS's successfully build, running regression test successfully and pushing packages up to the master-buildbot node. This happens on each github checking. test000.sh and test001.sh are running each time here. There are a few failures in the test runs, but should be resolved in the 1.1.0

also have a nightly centos 7 build that runs the full regression test and that is working great.

For this one and the github checkin runs, I out a copt of the regression test report. This is from the nightly

SoftwareVersion = 1.1.0
18000 Create/Load Uncompressed Tables:  Passed
19001 Working Folder Test:              Failed
20002 Concurrency Test:                 Passed
21005 Working DML Test:                 Passed
22006 Count while loading:              Passed (loads=100, rowsPerLoad=25000, queries=3076)
23007 Count while updating:             Passed (updates=1000, rowsPerUpdate=50000, queries=47020)
24008 Count while importing:            Passed (imports=200, rowsPerImport=500000, queries=305)
25009 Count while deleting:             Passed (imports=50, rowsPerImport=75000, queries=2253)
26010 Drop Partition Test:              Passed (18 counts all matched!)
27011 cpimport Features Test:           Passed
28012 Varbinary Test:                   Passed
29013 BLOB Test:                        Passed
30100 Create/Load Compressed Tables:    Passed
31101 Working Folder UM Join Comp On:   Passed
32102 Concurrency Test tpch1c:          Passed
33105 Working DML Test dmlc:            Passed
34106 Count while loading dmlc:         Passed (loads=100, rowsPerLoad=25000, queries=6324)
35107 Count while updating dmlc:        Passed (updates=1000, rowsPerUpdate=50000, queries=53430)
36108 Count while importing dmlc:       Passed (imports=200, rowsPerImport=500000, queries=357)
37109 Count while deleting dmlc:        Passed (imports=50, rowsPerImport=75000, queries=1472)
38110 Drop Partition Test dmlc:         Passed (18 counts all matched!)
39112 Varbinary Test:                   Passed
40200 Monitor TotalUmMemory:            Passed
41201 Version Buffer Test:              Passed
42202 Wide Table Tests:                 Passed
43203 EM Min/Max Valid L.D.I. Test:     Passed (200 loads)
44204 EM Min/Max Valid cpimport Test:   Passed (200 imports)
45210 VSS Flush Block in Block Cache:   Passed
46211 Concurrent Transactions Test:     Passed (seconds=1800, thr=15, rowFactor=10000, batches=4905, rows=345510000)
47212 Concurrent Transactions Test:     Passed (seconds=60, thr=5, tables=5, batches=108, rows=5184)
48295 DML schema EM Validation:         Passed
49296 DMLC schema EM Validation:        Passed
50297 Miscellaneous Tests:              Passed
51299 Japanese Language Test:           Passed (44 scripts all matched)
52-----------------------------------------------------------------------------------------------------------
53+-------+---------+------+----------+----------+-------+-------+--------+--------+----------+--------+
54| runId | version | rel  | buildDtm | run_time | maxPP | maxEM | maxDMP | maxImp | maxCntlr | maxAll |
55+-------+---------+------+----------+----------+-------+-------+--------+--------+----------+--------+
56|     1 |         |      |          | 03:00:58 | 26.8  | 36.9  | 1.5    | 2.4    | 0.0      | 59.0   |
57+-------+---------+------+----------+----------+-------+-------+--------+--------+----------+--------+
58+---------+---------------------+--------+---------+------+----------+------------+------------+-----------+
59| Run     | Start               | Run ID | Version | Rel  | Run Time | Max ExeMgr | Avg ExeMgr | Tests Run |
60+---------+---------------------+--------+---------+------+----------+------------+------------+-----------+
61| Current | 2017-07-07 05:18:29 |      1 |         |      | 03:00:58 | 36.9       | 5.40156    |        34 |
62+---------+---------------------+--------+---------+------+----------+------------+------------+-----------+
63+---------+----------+-------+-------+--------+--------+----------+---------+--------+
64| test    | run_time | maxPP | maxEM | maxDMP | maxImp | maxCntlr | maxWrkr | maxAll |
65+---------+----------+-------+-------+--------+--------+----------+---------+--------+
66| test000 | 00:17:10 | 11.2  | 0.1   | 0.1    | 0.8    | 0.0      | 0.0     | 11.4   |
67| test001 | 00:19:45 | 22.2  | 16.4  | 0.1    | 0.0    | 0.0      | 0.0     | 36.9   |
68| test002 | 00:01:12 | 24.5  | 13.0  | 0.1    | 0.0    | 0.0      | 0.0     | 37.4   |
69| test005 | 00:03:29 | 21.7  | 9.1   | 0.1    | 0.0    | 0.0      | 0.0     | 30.6   |
70| test006 | 00:03:01 | 21.3  | 8.6   | 0.1    | 0.0    | 0.0      | 0.0     | 30.0   |
71| test007 | 00:02:13 | 21.3  | 8.6   | 0.1    | 0.0    | 0.0      | 0.0     | 30.1   |
72| test008 | 00:09:43 | 21.3  | 8.6   | 0.1    | 0.4    | 0.0      | 0.0     | 30.4   |
73| test009 | 00:03:36 | 21.3  | 8.7   | 0.2    | 0.3    | 0.0      | 0.0     | 30.3   |
74| test010 | 00:02:23 | 21.3  | 8.7   | 0.1    | 0.0    | 0.0      | 0.0     | 30.2   |
75| test011 | 00:00:44 | 21.3  | 8.8   | 0.1    | 0.2    | 0.0      | 0.0     | 30.4   |
76| test012 | 00:00:46 | 21.3  | 9.6   | 0.2    | 0.2    | 0.0      | 0.0     | 31.2   |
77| test013 | 00:00:10 | 21.3  | 9.6   | 0.2    | 0.0    | 0.0      | 0.0     | 31.2   |
78| test100 | 00:02:47 | 21.7  | 9.7   | 0.2    | 2.4    | 0.0      | 0.0     | 33.7   |
79| test101 | 00:04:28 | 24.1  | 10.4  | 0.2    | 0.0    | 0.0      | 0.0     | 34.8   |
80| test102 | 00:01:09 | 26.8  | 12.8  | 0.2    | 0.0    | 0.0      | 0.0     | 39.6   |
81| test105 | 00:03:59 | 24.3  | 8.7   | 0.2    | 0.1    | 0.0      | 0.0     | 33.2   |
82| test106 | 00:05:20 | 24.2  | 8.4   | 0.2    | 0.0    | 0.0      | 0.0     | 32.9   |
83| test107 | 00:02:37 | 24.2  | 8.4   | 0.2    | 0.0    | 0.0      | 0.0     | 32.9   |
84| test108 | 00:09:48 | 24.3  | 8.5   | 0.2    | 0.6    | 0.0      | 0.0     | 33.6   |
85| test109 | 00:02:51 | 24.2  | 8.6   | 0.2    | 0.4    | 0.0      | 0.0     | 33.4   |
86| test110 | 00:02:17 | 24.2  | 8.6   | 0.2    | 0.0    | 0.0      | 0.0     | 33.1   |
87| test112 | 00:00:46 | 24.2  | 9.3   | 0.3    | 0.4    | 0.0      | 0.0     | 33.9   |
88| test200 | 00:20:25 | 26.5  | 36.9  | 0.3    | 0.6    | 0.0      | 0.1     | 59.0   |
89| test201 | 00:07:52 | 21.7  | 0.1   | 0.3    | 0.3    | 0.0      | 1.0     | 23.2   |
90| test202 | 00:01:16 | 23.6  | 2.2   | 0.3    | 2.4    | 0.0      | 1.0     | 26.9   |
91| test203 | 00:04:53 | 23.6  | 0.4   | 0.3    | 0.0    | 0.0      | 1.0     | 25.4   |
92| test204 | 00:04:59 | 23.6  | 0.4   | 0.3    | 0.2    | 0.0      | 1.0     | 25.6   |
93| test210 | 00:07:37 | 23.6  | 0.7   | 0.3    | 0.0    | 0.0      | 1.0     | 25.7   |
94| test211 | 00:30:44 | 23.7  | 1.1   | 1.5    | 0.2    | 0.0      | 1.0     | 27.4   |
95| test212 | 00:01:15 | 23.7  | 1.1   | 1.5    | 0.0    | 0.0      | 1.0     | 27.4   |
96| test297 | 00:00:02 | 23.7  | 1.1   | 1.5    | 0.0    | 0.0      | 1.0     | 27.4   |
97| test299 | 00:01:39 | 23.7  | 1.1   | 1.5    | 0.1    | 0.0      | 1.0     | 27.4   |
98+---------+----------+-------+-------+--------+--------+----------+---------+--------+
99-----------------------------------------------------------------------------------------------------------
100TEST100 : df -h output:
101Filesystem      Size  Used Avail Use% Mounted on
102/dev/xvda1       20G  2.3G   18G  12% /
103devtmpfs         15G     0   15G   0% /dev
104tmpfs            15G   11M   15G   1% /dev/shm
105tmpfs            15G   17M   15G   1% /run
106tmpfs            15G     0   15G   0% /sys/fs/cgroup
107/dev/xvdb       100G   24G   77G  24% /data
108tmpfs           3.0G     0  3.0G   0% /run/user/1001
109-----------------------------------------------------------------------------------------------------------
110Working Folder Test scripts that failed:
111Compare failed - working_tpch1_compareLogOnly/functionsAndExpressions/bug3784.sql
112Compare failed - working_tpch1_compareLogOnly/view/mts_view.80.sql
113Compare failed - working_tpch1_compareLogOnly/view/mts_view.sql
114-----------------------------------------------------------------------------------------------------------
115Working DML Test Details:
116Details from queryTester:
117Total Local Passed   = 70
118Total Local Failed   = 0
119Total Ref Passed     = 0
120Total Fef Failed     = 0
121Total Compare Passed = 0
122Total Compare Failed = 0
123Total Selects        = 1139
124Total Inserts        = 436
125Total Updates        = 139
126Total Deletes        = 26
127Total Creates        = 306
128Total Drops          = 408
129-----------------------------------------------------------------------------------------------------------
130Working Folder UM Join Test Details:
131Details from queryTester:
132Total Local Passed   = 706
133Total Local Failed   = 0
134Total Ref Passed     = 0
135Total Fef Failed     = 0
136Total Compare Passed = 0
137Total Compare Failed = 0
138Total Selects        = 26954
139Total Inserts        = 1754
140Total Updates        = 128
141Total Deletes        = 45
142Total Creates        = 1006
143Total Drops          = 1472
144Changed this Columnstore.xml setting:
145		<PmMaxMemorySmallSide>64M</PmMaxMemorySmallSide><!-- divide by 48 to get element count -->
146TEST101 : df -h output:
147Filesystem      Size  Used Avail Use% Mounted on
148/dev/xvda1       20G  2.3G   18G  12% /
149devtmpfs         15G     0   15G   0% /dev
150tmpfs            15G   14M   15G   1% /dev/shm
151tmpfs            15G  153M   15G   2% /run
152tmpfs            15G     0   15G   0% /sys/fs/cgroup
153/dev/xvdb       100G   43G   58G  43% /data
154tmpfs           3.0G     0  3.0G   0% /run/user/1001
155tmpfs           3.0G     0  3.0G   0% /run/user/0
156-----------------------------------------------------------------------------------------------------------
157TEST200 : df -h output:
158Filesystem      Size  Used Avail Use% Mounted on
159/dev/xvda1       20G  2.4G   18G  12% /
160devtmpfs         15G     0   15G   0% /dev
161tmpfs            15G   15M   15G   1% /dev/shm
162tmpfs            15G  225M   15G   2% /run
163tmpfs            15G     0   15G   0% /sys/fs/cgroup
164/dev/xvdb       100G   52G   49G  52% /data
165tmpfs           3.0G     0  3.0G   0% /run/user/1001
166-----------------------------------------------------------------------------------------------------------
167TEST299 : df -h output:
168Filesystem      Size  Used Avail Use% Mounted on
169/dev/xvda1       20G  2.4G   18G  12% /
170devtmpfs         15G     0   15G   0% /dev
171tmpfs            15G   15M   15G   1% /dev/shm
172tmpfs            15G  481M   15G   4% /run
173tmpfs            15G     0   15G   0% /sys/fs/cgroup
174/dev/xvdb       100G   60G   41G  60% /data
175tmpfs           3.0G     0  3.0G   0% /run/user/1001
176-----------------------------------------------------------------------------------------------------------
177Tests completed!
178Archive directory ./archive/2017-07-07@05:17:45
179program finished with exit code 0
180elapsedTime=0.004101",5,"completed. now have all OS's successfully build, running regression test successfully and pushing packages up to the master-buildbot node. This happens on each github checking. test000.sh and test001.sh are running each time here. There are a few failures in the test runs, but should be resolved in the 1.1.0

also have a nightly centos 7 build that runs the full regression test and that is working great.

For this one and the github checkin runs, I out a copt of the regression test report. This is from the nightly

SoftwareVersion = 1.1.0
18000 Create/Load Uncompressed Tables:  Passed
19001 Working Folder Test:              Failed
20002 Concurrency Test:                 Passed
21005 Working DML Test:                 Passed
22006 Count while loading:              Passed (loads=100, rowsPerLoad=25000, queries=3076)
23007 Count while updating:             Passed (updates=1000, rowsPerUpdate=50000, queries=47020)
24008 Count while importing:            Passed (imports=200, rowsPerImport=500000, queries=305)
25009 Count while deleting:             Passed (imports=50, rowsPerImport=75000, queries=2253)
26010 Drop Partition Test:              Passed (18 counts all matched!)
27011 cpimport Features Test:           Passed
28012 Varbinary Test:                   Passed
29013 BLOB Test:                        Passed
30100 Create/Load Compressed Tables:    Passed
31101 Working Folder UM Join Comp On:   Passed
32102 Concurrency Test tpch1c:          Passed
33105 Working DML Test dmlc:            Passed
34106 Count while loading dmlc:         Passed (loads=100, rowsPerLoad=25000, queries=6324)
35107 Count while updating dmlc:        Passed (updates=1000, rowsPerUpdate=50000, queries=53430)
36108 Count while importing dmlc:       Passed (imports=200, rowsPerImport=500000, queries=357)
37109 Count while deleting dmlc:        Passed (imports=50, rowsPerImport=75000, queries=1472)
38110 Drop Partition Test dmlc:         Passed (18 counts all matched!)
39112 Varbinary Test:                   Passed
40200 Monitor TotalUmMemory:            Passed
41201 Version Buffer Test:              Passed
42202 Wide Table Tests:                 Passed
43203 EM Min/Max Valid L.D.I. Test:     Passed (200 loads)
44204 EM Min/Max Valid cpimport Test:   Passed (200 imports)
45210 VSS Flush Block in Block Cache:   Passed
46211 Concurrent Transactions Test:     Passed (seconds=1800, thr=15, rowFactor=10000, batches=4905, rows=345510000)
47212 Concurrent Transactions Test:     Passed (seconds=60, thr=5, tables=5, batches=108, rows=5184)
48295 DML schema EM Validation:         Passed
49296 DMLC schema EM Validation:        Passed
50297 Miscellaneous Tests:              Passed
51299 Japanese Language Test:           Passed (44 scripts all matched)
52-----------------------------------------------------------------------------------------------------------
53+-------+---------+------+----------+----------+-------+-------+--------+--------+----------+--------+
54| runId | version | rel  | buildDtm | run_time | maxPP | maxEM | maxDMP | maxImp | maxCntlr | maxAll |
55+-------+---------+------+----------+----------+-------+-------+--------+--------+----------+--------+
56|     1 |         |      |          | 03:00:58 | 26.8  | 36.9  | 1.5    | 2.4    | 0.0      | 59.0   |
57+-------+---------+------+----------+----------+-------+-------+--------+--------+----------+--------+
58+---------+---------------------+--------+---------+------+----------+------------+------------+-----------+
59| Run     | Start               | Run ID | Version | Rel  | Run Time | Max ExeMgr | Avg ExeMgr | Tests Run |
60+---------+---------------------+--------+---------+------+----------+------------+------------+-----------+
61| Current | 2017-07-07 05:18:29 |      1 |         |      | 03:00:58 | 36.9       | 5.40156    |        34 |
62+---------+---------------------+--------+---------+------+----------+------------+------------+-----------+
63+---------+----------+-------+-------+--------+--------+----------+---------+--------+
64| test    | run_time | maxPP | maxEM | maxDMP | maxImp | maxCntlr | maxWrkr | maxAll |
65+---------+----------+-------+-------+--------+--------+----------+---------+--------+
66| test000 | 00:17:10 | 11.2  | 0.1   | 0.1    | 0.8    | 0.0      | 0.0     | 11.4   |
67| test001 | 00:19:45 | 22.2  | 16.4  | 0.1    | 0.0    | 0.0      | 0.0     | 36.9   |
68| test002 | 00:01:12 | 24.5  | 13.0  | 0.1    | 0.0    | 0.0      | 0.0     | 37.4   |
69| test005 | 00:03:29 | 21.7  | 9.1   | 0.1    | 0.0    | 0.0      | 0.0     | 30.6   |
70| test006 | 00:03:01 | 21.3  | 8.6   | 0.1    | 0.0    | 0.0      | 0.0     | 30.0   |
71| test007 | 00:02:13 | 21.3  | 8.6   | 0.1    | 0.0    | 0.0      | 0.0     | 30.1   |
72| test008 | 00:09:43 | 21.3  | 8.6   | 0.1    | 0.4    | 0.0      | 0.0     | 30.4   |
73| test009 | 00:03:36 | 21.3  | 8.7   | 0.2    | 0.3    | 0.0      | 0.0     | 30.3   |
74| test010 | 00:02:23 | 21.3  | 8.7   | 0.1    | 0.0    | 0.0      | 0.0     | 30.2   |
75| test011 | 00:00:44 | 21.3  | 8.8   | 0.1    | 0.2    | 0.0      | 0.0     | 30.4   |
76| test012 | 00:00:46 | 21.3  | 9.6   | 0.2    | 0.2    | 0.0      | 0.0     | 31.2   |
77| test013 | 00:00:10 | 21.3  | 9.6   | 0.2    | 0.0    | 0.0      | 0.0     | 31.2   |
78| test100 | 00:02:47 | 21.7  | 9.7   | 0.2    | 2.4    | 0.0      | 0.0     | 33.7   |
79| test101 | 00:04:28 | 24.1  | 10.4  | 0.2    | 0.0    | 0.0      | 0.0     | 34.8   |
80| test102 | 00:01:09 | 26.8  | 12.8  | 0.2    | 0.0    | 0.0      | 0.0     | 39.6   |
81| test105 | 00:03:59 | 24.3  | 8.7   | 0.2    | 0.1    | 0.0      | 0.0     | 33.2   |
82| test106 | 00:05:20 | 24.2  | 8.4   | 0.2    | 0.0    | 0.0      | 0.0     | 32.9   |
83| test107 | 00:02:37 | 24.2  | 8.4   | 0.2    | 0.0    | 0.0      | 0.0     | 32.9   |
84| test108 | 00:09:48 | 24.3  | 8.5   | 0.2    | 0.6    | 0.0      | 0.0     | 33.6   |
85| test109 | 00:02:51 | 24.2  | 8.6   | 0.2    | 0.4    | 0.0      | 0.0     | 33.4   |
86| test110 | 00:02:17 | 24.2  | 8.6   | 0.2    | 0.0    | 0.0      | 0.0     | 33.1   |
87| test112 | 00:00:46 | 24.2  | 9.3   | 0.3    | 0.4    | 0.0      | 0.0     | 33.9   |
88| test200 | 00:20:25 | 26.5  | 36.9  | 0.3    | 0.6    | 0.0      | 0.1     | 59.0   |
89| test201 | 00:07:52 | 21.7  | 0.1   | 0.3    | 0.3    | 0.0      | 1.0     | 23.2   |
90| test202 | 00:01:16 | 23.6  | 2.2   | 0.3    | 2.4    | 0.0      | 1.0     | 26.9   |
91| test203 | 00:04:53 | 23.6  | 0.4   | 0.3    | 0.0    | 0.0      | 1.0     | 25.4   |
92| test204 | 00:04:59 | 23.6  | 0.4   | 0.3    | 0.2    | 0.0      | 1.0     | 25.6   |
93| test210 | 00:07:37 | 23.6  | 0.7   | 0.3    | 0.0    | 0.0      | 1.0     | 25.7   |
94| test211 | 00:30:44 | 23.7  | 1.1   | 1.5    | 0.2    | 0.0      | 1.0     | 27.4   |
95| test212 | 00:01:15 | 23.7  | 1.1   | 1.5    | 0.0    | 0.0      | 1.0     | 27.4   |
96| test297 | 00:00:02 | 23.7  | 1.1   | 1.5    | 0.0    | 0.0      | 1.0     | 27.4   |
97| test299 | 00:01:39 | 23.7  | 1.1   | 1.5    | 0.1    | 0.0      | 1.0     | 27.4   |
98+---------+----------+-------+-------+--------+--------+----------+---------+--------+
99-----------------------------------------------------------------------------------------------------------
100TEST100 : df -h output:
101Filesystem      Size  Used Avail Use% Mounted on
102/dev/xvda1       20G  2.3G   18G  12% /
103devtmpfs         15G     0   15G   0% /dev
104tmpfs            15G   11M   15G   1% /dev/shm
105tmpfs            15G   17M   15G   1% /run
106tmpfs            15G     0   15G   0% /sys/fs/cgroup
107/dev/xvdb       100G   24G   77G  24% /data
108tmpfs           3.0G     0  3.0G   0% /run/user/1001
109-----------------------------------------------------------------------------------------------------------
110Working Folder Test scripts that failed:
111Compare failed - working_tpch1_compareLogOnly/functionsAndExpressions/bug3784.sql
112Compare failed - working_tpch1_compareLogOnly/view/mts_view.80.sql
113Compare failed - working_tpch1_compareLogOnly/view/mts_view.sql
114-----------------------------------------------------------------------------------------------------------
115Working DML Test Details:
116Details from queryTester:
117Total Local Passed   = 70
118Total Local Failed   = 0
119Total Ref Passed     = 0
120Total Fef Failed     = 0
121Total Compare Passed = 0
122Total Compare Failed = 0
123Total Selects        = 1139
124Total Inserts        = 436
125Total Updates        = 139
126Total Deletes        = 26
127Total Creates        = 306
128Total Drops          = 408
129-----------------------------------------------------------------------------------------------------------
130Working Folder UM Join Test Details:
131Details from queryTester:
132Total Local Passed   = 706
133Total Local Failed   = 0
134Total Ref Passed     = 0
135Total Fef Failed     = 0
136Total Compare Passed = 0
137Total Compare Failed = 0
138Total Selects        = 26954
139Total Inserts        = 1754
140Total Updates        = 128
141Total Deletes        = 45
142Total Creates        = 1006
143Total Drops          = 1472
144Changed this Columnstore.xml setting:
145		64M
146TEST101 : df -h output:
147Filesystem      Size  Used Avail Use% Mounted on
148/dev/xvda1       20G  2.3G   18G  12% /
149devtmpfs         15G     0   15G   0% /dev
150tmpfs            15G   14M   15G   1% /dev/shm
151tmpfs            15G  153M   15G   2% /run
152tmpfs            15G     0   15G   0% /sys/fs/cgroup
153/dev/xvdb       100G   43G   58G  43% /data
154tmpfs           3.0G     0  3.0G   0% /run/user/1001
155tmpfs           3.0G     0  3.0G   0% /run/user/0
156-----------------------------------------------------------------------------------------------------------
157TEST200 : df -h output:
158Filesystem      Size  Used Avail Use% Mounted on
159/dev/xvda1       20G  2.4G   18G  12% /
160devtmpfs         15G     0   15G   0% /dev
161tmpfs            15G   15M   15G   1% /dev/shm
162tmpfs            15G  225M   15G   2% /run
163tmpfs            15G     0   15G   0% /sys/fs/cgroup
164/dev/xvdb       100G   52G   49G  52% /data
165tmpfs           3.0G     0  3.0G   0% /run/user/1001
166-----------------------------------------------------------------------------------------------------------
167TEST299 : df -h output:
168Filesystem      Size  Used Avail Use% Mounted on
169/dev/xvda1       20G  2.4G   18G  12% /
170devtmpfs         15G     0   15G   0% /dev
171tmpfs            15G   15M   15G   1% /dev/shm
172tmpfs            15G  481M   15G   4% /run
173tmpfs            15G     0   15G   0% /sys/fs/cgroup
174/dev/xvdb       100G   60G   41G  60% /data
175tmpfs           3.0G     0  3.0G   0% /run/user/1001
176-----------------------------------------------------------------------------------------------------------
177Tests completed!
178Archive directory ./archive/2017-07-07@05:17:45
179program finished with exit code 0
180elapsedTime=0.004101"
1265,MCOL-51,MCOL,Dipti Joshi,83370,2016-05-11 17:18:47,"It needs to be documented in known issues and limitation  that ""max_length_for_sort_data "" should not be set",1,"It needs to be documented in known issues and limitation  that ""max_length_for_sort_data "" should not be set"
1266,MCOL-51,MCOL,Dipti Joshi,83371,2016-05-11 17:19:54,Please validate David Hall's analysis on max_length_for_sort_data ,2,Please validate David Hall's analysis on max_length_for_sort_data 
1267,MCOL-51,MCOL,Daniel Lee,83499,2016-05-18 15:45:38,"This test case, as well as this ""set max_length_for_sort_data = 4096;"" statement, worked in InfiniDB and MariaDB.  It is an issue in ColumnStore.  The issue needs to be fixed, instead of documenting it.  Without being able to set ma_length_for_sort_data, lots of queries would failed.  ",3,"This test case, as well as this ""set max_length_for_sort_data = 4096;"" statement, worked in InfiniDB and MariaDB.  It is an issue in ColumnStore.  The issue needs to be fixed, instead of documenting it.  Without being able to set ma_length_for_sort_data, lots of queries would failed.  "
1268,MCOL-51,MCOL,David Hall,83501,2016-05-18 15:58:39,"I have put in a fix that should eliminate the need to set max_length_for_sort_data in most cases. However, the problem here is that 4096 is now too small for this query, where it used to be sufficient. I believe the only thing that should be done is a change to the test to something a bit larger until it works. 4096 is too small. It may be that setting it smaller than the default is a problem, don't know for sure, but I think it should be OK.",4,"I have put in a fix that should eliminate the need to set max_length_for_sort_data in most cases. However, the problem here is that 4096 is now too small for this query, where it used to be sufficient. I believe the only thing that should be done is a change to the test to something a bit larger until it works. 4096 is too small. It may be that setting it smaller than the default is a problem, don't know for sure, but I think it should be OK."
1269,MCOL-51,MCOL,Daniel Lee,83503,2016-05-18 16:20:03,"I know we just started using Jira.  In this ticket, there was no information to for me to QA on.

With the default value for max_length_for_sort_data, which is 1024, the reference worked, but not in ColumnStore (same thing in InfiniDB).  The test case was to set the value to 4096 so that InfiniDB passed.  Now 4096 still failed in ColumnStore.  It worked when I set it to 8192 (not sure at what value it starts to work).

Not sure what ""eliminate the need to set max_length_for_sort_data in most cases"" means.  We should keep the behavior as close to MariaDB as possible, and if ColumnStore requires larger value, as InfiniDB did, due to system and/or storage design, it is ok.  We should not try to eliminate the use of the variable and cause it to go up even more.  4096 is already a huge value to use since the column type is a varchar(1000).  Now that we have to increase the value more seems to be unreasonable.
  

",5,"I know we just started using Jira.  In this ticket, there was no information to for me to QA on.

With the default value for max_length_for_sort_data, which is 1024, the reference worked, but not in ColumnStore (same thing in InfiniDB).  The test case was to set the value to 4096 so that InfiniDB passed.  Now 4096 still failed in ColumnStore.  It worked when I set it to 8192 (not sure at what value it starts to work).

Not sure what ""eliminate the need to set max_length_for_sort_data in most cases"" means.  We should keep the behavior as close to MariaDB as possible, and if ColumnStore requires larger value, as InfiniDB did, due to system and/or storage design, it is ok.  We should not try to eliminate the use of the variable and cause it to go up even more.  4096 is already a huge value to use since the column type is a varchar(1000).  Now that we have to increase the value more seems to be unreasonable.
  

"
1270,MCOL-51,MCOL,David Hall,83512,2016-05-18 19:20:22,"Regarding max_length_for_sort_data. It's true that 1024 is the default and 4096 seems big, but it's not. The max for this value is 8192*1024 (8,388,608) and this is the value that KKA always put in. I believe this should be our default value, set in the my.cnf. Any ORDER BY clause uses this space, and we don't play nice with ORDER BY, which would rather the engine do the work. We  force the server to do it all, and so it needs a lot of memory to do it.",6,"Regarding max_length_for_sort_data. It's true that 1024 is the default and 4096 seems big, but it's not. The max for this value is 8192*1024 (8,388,608) and this is the value that KKA always put in. I believe this should be our default value, set in the my.cnf. Any ORDER BY clause uses this space, and we don't play nice with ORDER BY, which would rather the engine do the work. We  force the server to do it all, and so it needs a lot of memory to do it."
1271,MCOL-51,MCOL,Dipti Joshi,83591,2016-05-23 07:25:28,"[~David.Hall], [~dleeyh] By reading both of your comments  - I understand that
(1) We should be able to set the value of max_lenthg_for_sort_data  in my,cnf, and use that value what ever it is set to
(2) For this particular bug to pass, it should be set to 8192

If you both agree with above, then I would have David Hill set the value to 8192 for this particular test script - so it can pass ",7,"[~David.Hall], [~dleeyh] By reading both of your comments  - I understand that
(1) We should be able to set the value of max_lenthg_for_sort_data  in my,cnf, and use that value what ever it is set to
(2) For this particular bug to pass, it should be set to 8192

If you both agree with above, then I would have David Hill set the value to 8192 for this particular test script - so it can pass "
1272,MCOL-51,MCOL,Daniel Lee,83596,2016-05-23 07:35:11,It worked when I set it to 8192.,8,It worked when I set it to 8192.
1273,MCOL-51,MCOL,David Hall,83638,2016-05-24 05:54:22,"This test does not modify the value in my.cnf, but rather with a SQL statement for setting at the session level.",9,"This test does not modify the value in my.cnf, but rather with a SQL statement for setting at the session level."
1274,MCOL-51,MCOL,Dipti Joshi,83654,2016-05-24 14:38:15,"[~David.Hall]Ok, thanks David, we should get David Hill to modify the SQL statement in the script to set max_length_for_sort_data to 8192",10,"[~David.Hall]Ok, thanks David, we should get David Hill to modify the SQL statement in the script to set max_length_for_sort_data to 8192"
1275,MCOL-51,MCOL,Dipti Joshi,83655,2016-05-24 14:52:08,"[~hill] Please update line in 2 of working_tpch1/misc/bug3783.sql

set max_length_for_sort_data = 8192;

That will allow this test script to pass",11,"[~hill] Please update line in 2 of working_tpch1/misc/bug3783.sql

set max_length_for_sort_data = 8192;

That will allow this test script to pass"
1276,MCOL-51,MCOL,David Hill,83656,2016-05-24 14:57:07,"working_tpch1/misc/bug3783.sql updated with

set max_length_for_sort_data = 8192;",12,"working_tpch1/misc/bug3783.sql updated with

set max_length_for_sort_data = 8192;"
1277,MCOL-51,MCOL,Dipti Joshi,84009,2016-06-04 02:09:42,Regression suite is updated by using max_length_of_sort_data to 8192,13,Regression suite is updated by using max_length_of_sort_data to 8192
1278,MCOL-510,MCOL,David Hill,91707,2017-02-09 22:11:33,"now generating binary and rpm packages for SuSE 12 on local build machine and buildbot system.

Passed regression test and additional OAm testing like addmodules amd reboot's. 

Still needs to be QA autoPilot tested, but besides that, it looks good to release to 1.0.7-1.

There was just 1 file change to a cmake file.

develop-1.0

commit 03aa2f3a7fc69efd861fbd336cf29b7805c97ed1
Author: David Hill <david.hill@mariadb.com>
Date:   Thu Feb 9 15:24:14 2017 -0600

    MCOL-105 - change suse boost-devel package check

 cpackEngineRPM.cmake | 15 +++++++++------


develop

commit dc67f19bf049837ca2b8d9c5657b103464570ecf
Author: david <david.hill@mariadb.com>
Date:   Thu Feb 9 15:29:44 2017 +0000

    MCOL-105 - add suse boost-devel package check

 cpackEngineRPM.cmake | 12 ++++++++++--
",1,"now generating binary and rpm packages for SuSE 12 on local build machine and buildbot system.

Passed regression test and additional OAm testing like addmodules amd reboot's. 

Still needs to be QA autoPilot tested, but besides that, it looks good to release to 1.0.7-1.

There was just 1 file change to a cmake file.

develop-1.0

commit 03aa2f3a7fc69efd861fbd336cf29b7805c97ed1
Author: David Hill 
Date:   Thu Feb 9 15:24:14 2017 -0600

    MCOL-105 - change suse boost-devel package check

 cpackEngineRPM.cmake | 15 +++++++++------


develop

commit dc67f19bf049837ca2b8d9c5657b103464570ecf
Author: david 
Date:   Thu Feb 9 15:29:44 2017 +0000

    MCOL-105 - add suse boost-devel package check

 cpackEngineRPM.cmake | 12 ++++++++++--
"
1279,MCOL-510,MCOL,David Hill,91712,2017-02-09 22:55:55,"We need to decide if this gets released as 1.0.7-1.

I will update the documents and github readme based on what is decided.",2,"We need to decide if this gets released as 1.0.7-1.

I will update the documents and github readme based on what is decided."
1280,MCOL-510,MCOL,David Thompson,91714,2017-02-09 23:18:49,"yes, lets do a 107 build.",3,"yes, lets do a 107 build."
1281,MCOL-510,MCOL,Daniel Lee,91910,2017-02-15 22:28:29,"Verified both BIN and RPM packages for Sure 12 sp 2, for both root and non-root users, for 1PM, 1UM2PM configurations.
",4,"Verified both BIN and RPM packages for Sure 12 sp 2, for both root and non-root users, for 1PM, 1UM2PM configurations.
"
1282,MCOL-5106,MCOL,alexey vorovich,225633,2022-06-02 14:09:15,"[~denis0x0D] So please adjust your test instructions to cover multi-node *with shared files (S3 or NFS)* , specifically

-proper shutdown/startup
-which node to run *mcsRebuild -v*  on
-how to distribute the resulting EM files  to multiple nodes ",1,"[~denis0x0D] So please adjust your test instructions to cover multi-node *with shared files (S3 or NFS)* , specifically

-proper shutdown/startup
-which node to run *mcsRebuild -v*  on
-how to distribute the resulting EM files  to multiple nodes "
1283,MCOL-5106,MCOL,alexey vorovich,225655,2022-06-02 16:21:47,[~roman.navrotskiy] Found the image with alans help/ no issue . thnks,2,[~roman.navrotskiy] Found the image with alans help/ no issue . thnks
1284,MCOL-5106,MCOL,alexey vorovich,226350,2022-06-09 15:49:55,"[~toddstoffel] i opened https://jira.mariadb.org/browse/MCOL-5126 for you
[~denis0x0D] who should  deal with other items
",3,"[~toddstoffel] i opened URL for you
[~denis0x0D] who should  deal with other items
"
1285,MCOL-5113,MCOL,alexey vorovich,226107,2022-06-07 21:29:39,"Good...

[~toddstoffel],   to preserve uniformity between compose and sky, we should improve   provision.sh script  and allow passing of the list of hosts from  Sky operator.
The same script technique should be used used for other events such stop/start 

I think  we should  start on this with target 22.08 . I can help debug /provide feedback",1,"Good...

[~toddstoffel],   to preserve uniformity between compose and sky, we should improve   provision.sh script  and allow passing of the list of hosts from  Sky operator.
The same script technique should be used used for other events such stop/start 

I think  we should  start on this with target 22.08 . I can help debug /provide feedback"
1286,MCOL-5126,MCOL,alexey vorovich,227955,2022-06-27 23:54:02,"[~toddstoffel] this is not internal. You may need to fix the docker repo for develop branch of the engine that does not have exemgr

please review ",1,"[~toddstoffel] this is not internal. You may need to fix the docker repo for develop branch of the engine that does not have exemgr

please review "
1287,MCOL-5126,MCOL,alexey vorovich,231039,2022-07-29 16:15:52,this is done,2,this is done
1288,MCOL-513,MCOL,David Hall,92004,2017-02-17 17:09:36,"class ThreadPool has been modified:
add a handle to be returned by Invoke. 
new methods Join(handle) and Join(vector of handles)
A monitor class was added for debugging.
The BeginThread() method has been streamlined.
new feature: if queuesize is 0, then maxthreads is a suggestion, not a hard rule. If more threads are needed, they are created. No one is rejected. After 10 minutes of idle, threads will end until we have idled down to maxthreads.

JobList destructor spun up threads to end jobstep threads. This seemed excessive, especially since most of the time, there was little or nothing to do. I experimented with ending things in a loop instead and discovered it to be generally faster.

All jobsteps are now run in jobstepThreadPool as well as all the threads the jobsteps create.

A separate ThreadPool is used to handle mysqld connections and yet another to handle FEMsgHandler.

No ThreadPool is created for DistributedEngineComm. It appears these are long lived threads and a pool would just muddy things up.",1,"class ThreadPool has been modified:
add a handle to be returned by Invoke. 
new methods Join(handle) and Join(vector of handles)
A monitor class was added for debugging.
The BeginThread() method has been streamlined.
new feature: if queuesize is 0, then maxthreads is a suggestion, not a hard rule. If more threads are needed, they are created. No one is rejected. After 10 minutes of idle, threads will end until we have idled down to maxthreads.

JobList destructor spun up threads to end jobstep threads. This seemed excessive, especially since most of the time, there was little or nothing to do. I experimented with ending things in a loop instead and discovered it to be generally faster.

All jobsteps are now run in jobstepThreadPool as well as all the threads the jobsteps create.

A separate ThreadPool is used to handle mysqld connections and yet another to handle FEMsgHandler.

No ThreadPool is created for DistributedEngineComm. It appears these are long lived threads and a pool would just muddy things up."
1289,MCOL-513,MCOL,Andrew Hutchings,92092,2017-02-20 17:49:51,"Reviewed the code and it looks good after a minor change made. Moving to test.

Running normal regressions tests should be adequate for this. Biggest things to look for are crashes and hangs.",2,"Reviewed the code and it looks good after a minor change made. Moving to test.

Running normal regressions tests should be adequate for this. Biggest things to look for are crashes and hangs."
1290,MCOL-513,MCOL,Daniel Lee,94818,2017-05-05 21:21:18,Assigned it to Mr. Hill for regression tests.,3,Assigned it to Mr. Hill for regression tests.
1291,MCOL-513,MCOL,David Thompson,95620,2017-05-22 16:08:53,This has been tested.,4,This has been tested.
1292,MCOL-5131,MCOL,Daniel Lee,227948,2022-06-27 22:22:23,"Build tested (#4661)

Executed tests using Docker containers.

Test scenario
{noformat}
create 1gb dbt database
load database
select count(*) from lineitem
stop ColumnStore
rm BRM_saves_em
mcsRebuildEM -v
start ColumnStore
select count(*) from lineitem
cpimport 1gb lineitem
select count(*) from lineitem
{noformat}

Dataset size: 1 gb DBT3 

1PM local storage - PASSED
1PM S3 storage    - PASSED
3PM local storage - PASSED
3PM S3 storage    - PASSED

Dataset size: 10 gb DBT3 

1PM local storage - FAILED * 
1PM S3 storage    - FAILED *
3PM local storage - PASSED
3PM S3 storage    - PASSED

* Both local and S3 tests return similar error messages after running 'mcsRebuildEM -v'
I am not sure if the issue is in the actual data files, or in this tool.

{noformat}
.
.
.
Setting a HWM for [OID: 3060, partition: 0, segment: 0, col width: 8, lbid:2061312, hwm: 0, isDict: 1]
Extent is created, allocated size 8192 actual LBID 2069504
For [OID: 3060, partition: 0, segment: 0, col width: 8, lbid:2069504, hwm: 124032, isDict: 1]
Setting a HWM for [OID: 3060, partition: 0, segment: 0, col width: 8, lbid:2069504, hwm: 124032, isDict: 1]
Cannot set local HWM: ExtentMap::setLocalHWM(): new HWM is past the end of the file for OID 3060; partition 0; segment 0; HWM 124032
Completed.
{noformat}

The two failed cases also failed in VMs, not just in Docker containers.
",1,"Build tested (#4661)

Executed tests using Docker containers.

Test scenario
{noformat}
create 1gb dbt database
load database
select count(*) from lineitem
stop ColumnStore
rm BRM_saves_em
mcsRebuildEM -v
start ColumnStore
select count(*) from lineitem
cpimport 1gb lineitem
select count(*) from lineitem
{noformat}

Dataset size: 1 gb DBT3 

1PM local storage - PASSED
1PM S3 storage    - PASSED
3PM local storage - PASSED
3PM S3 storage    - PASSED

Dataset size: 10 gb DBT3 

1PM local storage - FAILED * 
1PM S3 storage    - FAILED *
3PM local storage - PASSED
3PM S3 storage    - PASSED

* Both local and S3 tests return similar error messages after running 'mcsRebuildEM -v'
I am not sure if the issue is in the actual data files, or in this tool.

{noformat}
.
.
.
Setting a HWM for [OID: 3060, partition: 0, segment: 0, col width: 8, lbid:2061312, hwm: 0, isDict: 1]
Extent is created, allocated size 8192 actual LBID 2069504
For [OID: 3060, partition: 0, segment: 0, col width: 8, lbid:2069504, hwm: 124032, isDict: 1]
Setting a HWM for [OID: 3060, partition: 0, segment: 0, col width: 8, lbid:2069504, hwm: 124032, isDict: 1]
Cannot set local HWM: ExtentMap::setLocalHWM(): new HWM is past the end of the file for OID 3060; partition 0; segment 0; HWM 124032
Completed.
{noformat}

The two failed cases also failed in VMs, not just in Docker containers.
"
1293,MCOL-5131,MCOL,Daniel Lee,228058,2022-06-28 14:28:14,"Build tested (#4749)

Tested new build with 10gb, 20gb, and 50gb datasets on both 1PM and 3PM configurations.

Passed.
",2,"Build tested (#4749)

Tested new build with 10gb, 20gb, and 50gb datasets on both 1PM and 3PM configurations.

Passed.
"
1294,MCOL-5138,MCOL,Daniel Lee,230755,2022-07-27 13:56:33,"Build verified: 22.08.01 (engine #5094, cmapi #706)

For the purpose of this ticket, I verified that ColumnStore has been successfully installed.  The check for ExeMgr in the following script has been removed.  There are still references to ExeMgr that needs to be correct in long term.

/usr/share/columnstore/cmapi/mcs_node_control/custom_dispatchers/container.sh

",1,"Build verified: 22.08.01 (engine #5094, cmapi #706)

For the purpose of this ticket, I verified that ColumnStore has been successfully installed.  The check for ExeMgr in the following script has been removed.  There are still references to ExeMgr that needs to be correct in long term.

/usr/share/columnstore/cmapi/mcs_node_control/custom_dispatchers/container.sh

"
1295,MCOL-5138,MCOL,alexey vorovich,230758,2022-07-27 14:03:16,[~dleeyh] Did you check the docker-compose invocation and which branch of docker repo ?,2,[~dleeyh] Did you check the docker-compose invocation and which branch of docker repo ?
1296,MCOL-5138,MCOL,Daniel Lee,230777,2022-07-27 15:45:12,"Testing was done using Docker, on single-node and multi-node clusters.   Retested with cron build #715 after merge.",3,"Testing was done using Docker, on single-node and multi-node clusters.   Retested with cron build #715 after merge."
1297,MCOL-514,MCOL,David Thompson,94324,2017-04-24 20:56:13,This has been documented. MCOL-597  tracks the actual code merge.,1,This has been documented. MCOL-597  tracks the actual code merge.
1298,MCOL-5140,MCOL,Andrey Piskunov,229669,2022-07-15 15:10:04,"Done,  after testing (on 10**6 values, for a 30 s period) the increase shown is from 3826 queries to 5612, about 45%.",1,"Done,  after testing (on 10**6 values, for a 30 s period) the increase shown is from 3826 queries to 5612, about 45%."
1299,MCOL-5140,MCOL,Roman,231483,2022-08-04 14:08:32,4QA this optimization should benefit query processing(that has filters on numbers or column types that are reduced to numbers) in general. The query example is mentioned in the description.,2,4QA this optimization should benefit query processing(that has filters on numbers or column types that are reduced to numbers) in general. The query example is mentioned in the description.
1300,MCOL-5140,MCOL,Daniel Lee,231985,2022-08-10 20:15:05,"Build verified: 22.08-1 (#5243)

Closed by regression test",3,"Build verified: 22.08-1 (#5243)

Closed by regression test"
1301,MCOL-5152,MCOL,Roman,228550,2022-07-04 18:14:47,4QA this patch enables PP to bypass network hop sending data from PP to the local EM. It should reduce the query execution timings in general. The expected profit should be around 15%.,1,4QA this patch enables PP to bypass network hop sending data from PP to the local EM. It should reduce the query execution timings in general. The expected profit should be around 15%.
1302,MCOL-5152,MCOL,Daniel Lee,230771,2022-07-27 15:30:14,"Build verified: 22.08-1 (#5095)

Passed by regression test
",2,"Build verified: 22.08-1 (#5095)

Passed by regression test
"
1303,MCOL-5160,MCOL,alexey vorovich,230175,2022-07-21 20:15:09,"[~roman.navrotskiy]
are you working on this ?",1,"[~roman.navrotskiy]
are you working on this ?"
1304,MCOL-5160,MCOL,alexey vorovich,230780,2022-07-27 15:55:17,[~roman.navrotskiy] you have PR for but it is not linked here ... Please fix,2,[~roman.navrotskiy] you have PR for but it is not linked here ... Please fix
1305,MCOL-5162,MCOL,Roman,229844,2022-07-18 21:36:30,[Here|https://docs.google.com/document/d/1xinxpqKCsejGBkdd1FwaSQeUQWDTM6zZ20MQNB6NwVE/edit?usp=sharing] is the semi-design document that contains initial info about the project.,1,[Here|URL is the semi-design document that contains initial info about the project.
1306,MCOL-5162,MCOL,Roman,232413,2022-08-16 18:31:55,[~tntnatbry] Could elaborate on how to test this feature?,2,[~tntnatbry] Could elaborate on how to test this feature?
1307,MCOL-5162,MCOL,Gagan Goel,232420,2022-08-16 19:17:49,"Testing instructions:
# Create a CS table (say t1) pre-MCOL-5021 (use develop nightly build before build number 5299. 5299 is the nightly develop build with MCOL-5021). Run some DMLs and SELECTs statements.
#  Uninstall the CS package. Download and install the new package with the code changes for MCOL-5021 and MCOL-5162. MCOL-5162 is merged on 2022-08-16, so use a nightly develop build after this date for the CS package that includes both the MCOL-5021 and MCOL-5162 code changes. During the package install, you will notice a dbbuilder executable output saying the syscat is getting upgraded and it will report which OIDs are getting upgraded (by upgrade it means populating that OID/column with default values).
# Run the following query on the syscat table:
{code:sql}
      SELECT * FROM calpontsys.systable;
{code}
    Notice a new column {{auxcolumnoid}} should now be in the calpontsys.systable table DDL. For table t1, this column should have a value of 0.
# Run the SELECTs on t1 again as in Step 1 and it should return the same output as earlier. You can run more DMLs and SELECTs on table t1 now to confirm t1 is not impacted in any way with the upgrade.
# Create a new CS table (say t2) with the new CS version. Now notice the value of `auxcolumnoid` in calpontsys.systable. It should be > 3000.
# Run some DML and SELECT statements on t2 to verify the outputs are correct.
# Optional step: To compare the DELETE performance between t1 (no AUX column) and t2 (with AUX column) (both tables should have same DDL), insert say, 1million records to both t1 and t2 and then run DELETE statements with a WHERE clause.",3,"Testing instructions:
# Create a CS table (say t1) pre-MCOL-5021 (use develop nightly build before build number 5299. 5299 is the nightly develop build with MCOL-5021). Run some DMLs and SELECTs statements.
#  Uninstall the CS package. Download and install the new package with the code changes for MCOL-5021 and MCOL-5162. MCOL-5162 is merged on 2022-08-16, so use a nightly develop build after this date for the CS package that includes both the MCOL-5021 and MCOL-5162 code changes. During the package install, you will notice a dbbuilder executable output saying the syscat is getting upgraded and it will report which OIDs are getting upgraded (by upgrade it means populating that OID/column with default values).
# Run the following query on the syscat table:
{code:sql}
      SELECT * FROM calpontsys.systable;
{code}
    Notice a new column {{auxcolumnoid}} should now be in the calpontsys.systable table DDL. For table t1, this column should have a value of 0.
# Run the SELECTs on t1 again as in Step 1 and it should return the same output as earlier. You can run more DMLs and SELECTs on table t1 now to confirm t1 is not impacted in any way with the upgrade.
# Create a new CS table (say t2) with the new CS version. Now notice the value of `auxcolumnoid` in calpontsys.systable. It should be > 3000.
# Run some DML and SELECT statements on t2 to verify the outputs are correct.
# Optional step: To compare the DELETE performance between t1 (no AUX column) and t2 (with AUX column) (both tables should have same DDL), insert say, 1million records to both t1 and t2 and then run DELETE statements with a WHERE clause."
1308,MCOL-5162,MCOL,Daniel Lee,232623,2022-08-18 17:54:49,"Build verified: 22.08-1 (#5313)

1. upgraded from 6.4.2-1 to 2.2.08-1
2. performed upgrades using two ways: package and local repos
3. performed upgrades using on rocky 8 and ubuntu 20.04 (rpm and deb)
4. performed upgrades for single-node and 3pm clusters
5. verified that old table has 0 for auxcolumnoid and new tables have auxcolumnoid > 3000.
",4,"Build verified: 22.08-1 (#5313)

1. upgraded from 6.4.2-1 to 2.2.08-1
2. performed upgrades using two ways: package and local repos
3. performed upgrades using on rocky 8 and ubuntu 20.04 (rpm and deb)
4. performed upgrades for single-node and 3pm clusters
5. verified that old table has 0 for auxcolumnoid and new tables have auxcolumnoid > 3000.
"
1309,MCOL-5163,MCOL,Roman,232411,2022-08-16 18:27:04,4QA Try to crash ExeMgr or PP and it will force WE and DMLProc to restart even if they are in the middle of a txn. The current version doesn't restart WE or DMLProc if PP has crashed.,1,4QA Try to crash ExeMgr or PP and it will force WE and DMLProc to restart even if they are in the middle of a txn. The current version doesn't restart WE or DMLProc if PP has crashed.
1310,MCOL-5163,MCOL,Daniel Lee,232699,2022-08-19 15:40:05,"Build verified: 22.08-1 (#5324)

Also verified 6.x (#5262) since it was also checked into develop-6.",2,"Build verified: 22.08-1 (#5324)

Also verified 6.x (#5262) since it was also checked into develop-6."
1311,MCOL-5166,MCOL,Daniel Lee,231986,2022-08-10 20:16:10,"Build verified: 22.08-1 (#5243)

Verified by regression test",1,"Build verified: 22.08-1 (#5243)

Verified by regression test"
1312,MCOL-5167,MCOL,Daniel Lee,232264,2022-08-15 15:10:16,"Build verified: 22.08-1 (#5290)

Reproduced the issue in 6.4.2-1
Verified the fix in 22.08-1
{noformat}
MariaDB [mytest]> select * from t1 left join t2 on (t1.a = t2.a) left join t3 on (t1.b = t3.b and t2.a > 1);
+------+------+------+------+------+------+
| a    | b    | a    | b    | a    | b    |
+------+------+------+------+------+------+
|    1 |    3 |    1 |    2 | NULL | NULL |
|    2 |    3 |    2 |    4 |    2 |    3 |
|    3 |    4 | NULL | NULL | NULL | NULL |
+------+------+------+------+------+------+
3 rows in set (0.086 sec)
{noformat}

Also executed MTR test suites and verified results against MariaDB server",1,"Build verified: 22.08-1 (#5290)

Reproduced the issue in 6.4.2-1
Verified the fix in 22.08-1
{noformat}
MariaDB [mytest]> select * from t1 left join t2 on (t1.a = t2.a) left join t3 on (t1.b = t3.b and t2.a > 1);
+------+------+------+------+------+------+
| a    | b    | a    | b    | a    | b    |
+------+------+------+------+------+------+
|    1 |    3 |    1 |    2 | NULL | NULL |
|    2 |    3 |    2 |    4 |    2 |    3 |
|    3 |    4 | NULL | NULL | NULL | NULL |
+------+------+------+------+------+------+
3 rows in set (0.086 sec)
{noformat}

Also executed MTR test suites and verified results against MariaDB server"
1313,MCOL-5172,MCOL,Daniel Lee,232600,2022-08-18 13:28:44,"Build verified: 22.08-1 (#5313)

1. Created a 3pm docker cluster
2. Verified calpontsys.systable system catalog has the auxcolumnoid column
3. Created a 10g DBT3 database
4. Verified row counts for tables
5. mcsShutdown
6. removed BRM_saves_em file
7. executed  ""mcsRebuildEM -v""
8. mcsStart
9. verified row counts
",1,"Build verified: 22.08-1 (#5313)

1. Created a 3pm docker cluster
2. Verified calpontsys.systable system catalog has the auxcolumnoid column
3. Created a 10g DBT3 database
4. Verified row counts for tables
5. mcsShutdown
6. removed BRM_saves_em file
7. executed  ""mcsRebuildEM -v""
8. mcsStart
9. verified row counts
"
1314,MCOL-518,MCOL,David Thompson,95618,2017-05-22 16:06:38,This has been tested and verified. New tickets for anything else.,1,This has been tested and verified. New tickets for anything else.
1315,MCOL-5184,MCOL,David Hall,231979,2022-08-10 19:06:27,6.4.4 released Aug 9 2022,1,6.4.4 released Aug 9 2022
1316,MCOL-519,MCOL,Daniel Lee,100079,2017-09-12 18:56:00,"The feature has been implemented and is in 1.1.0-1.  I have been testing it for a whiles. For QA, it should be closed now and issues identified will be tracked by separate tickets.

Assigning it to Ben for finish the documentation.
",1,"The feature has been implemented and is in 1.1.0-1.  I have been testing it for a whiles. For QA, it should be closed now and issues identified will be tracked by separate tickets.

Assigning it to Ben for finish the documentation.
"
1317,MCOL-519,MCOL,Daniel Lee,100217,2017-09-14 20:52:31,"Build verified: 1.1.0-1

Feature has been implemented.  Issues identified are being tracked by individual tickets.  Subtask MCOL-748 is not yet done and will be tracked individually.

Closing it.",2,"Build verified: 1.1.0-1

Feature has been implemented.  Issues identified are being tracked by individual tickets.  Subtask MCOL-748 is not yet done and will be tracked individually.

Closing it."
1318,MCOL-5191,MCOL,Daniel Lee,244516,2022-12-05 22:56:12,"Build verified: 23.02

engine: a1d89d8f311d8187d3357536a64d77ef6f9c2b8e
server: bf7f6987c8fb7ceda9ae048ada129d11798d4392
buildNo: 6151

Retested the test case in MCOL-1205

{noformat}
MariaDB [tpch1m]> select n_name, sum(l_extendedprice * (1 - l_discount)) as revenue from customer, orders, lineitem, supplier, nation, region where c_custkey = o_custkey and l_orderkey = o_orderkey and l_suppkey = s_suppkey and c_nationkey = s_nationkey and s_nationkey = n_nationkey and n_regionkey = r_regionkey and r_name = 'AMERICA' and o_orderdate >= '1993-01-01' and o_orderdate < date_add( '1993-01-01' , interval 1 year) group by n_name order by revenue desc;
+-----------+-------------+
| n_name    | revenue     |
+-----------+-------------+
| PERU      | 527161.1575 |
| ARGENTINA |  34521.3330 |
+-----------+-------------+
2 rows in set (0.290 sec)

MariaDB [tpch1m]> analyze table customer, orders, lineitem,supplier, nation, region
    -> ;
+-----------------+---------+----------+----------+
| Table           | Op      | Msg_type | Msg_text |
+-----------------+---------+----------+----------+
| tpch1m.customer | analyze | status   | OK       |
| tpch1m.orders   | analyze | status   | OK       |
| tpch1m.lineitem | analyze | status   | OK       |
| tpch1m.supplier | analyze | status   | OK       |
| tpch1m.nation   | analyze | status   | OK       |
| tpch1m.region   | analyze | status   | OK       |
+-----------------+---------+----------+----------+
6 rows in set (0.035 sec)

MariaDB [tpch1m]> select n_name, sum(l_extendedprice * (1 - l_discount)) as revenue from customer, orders, lineitem, supplier, nation, region where c_custkey = o_custkey and l_orderkey = o_orderkey and l_suppkey = s_suppkey and c_nationkey = s_nationkey and s_nationkey = n_nationkey and n_regionkey = r_regionkey and r_name = 'AMERICA' and o_orderdate >= '1993-01-01' and o_orderdate < date_add( '1993-01-01' , interval 1 year) group by n_name order by revenue desc;
+-----------+-------------+
| n_name    | revenue     |
+-----------+-------------+
| PERU      | 527161.1575 |
| ARGENTINA |  34521.3330 |
+-----------+-------------+
2 rows in set (0.036 sec)
{noformat}",1,"Build verified: 23.02

engine: a1d89d8f311d8187d3357536a64d77ef6f9c2b8e
server: bf7f6987c8fb7ceda9ae048ada129d11798d4392
buildNo: 6151

Retested the test case in MCOL-1205

{noformat}
MariaDB [tpch1m]> select n_name, sum(l_extendedprice * (1 - l_discount)) as revenue from customer, orders, lineitem, supplier, nation, region where c_custkey = o_custkey and l_orderkey = o_orderkey and l_suppkey = s_suppkey and c_nationkey = s_nationkey and s_nationkey = n_nationkey and n_regionkey = r_regionkey and r_name = 'AMERICA' and o_orderdate >= '1993-01-01' and o_orderdate < date_add( '1993-01-01' , interval 1 year) group by n_name order by revenue desc;
+-----------+-------------+
| n_name    | revenue     |
+-----------+-------------+
| PERU      | 527161.1575 |
| ARGENTINA |  34521.3330 |
+-----------+-------------+
2 rows in set (0.290 sec)

MariaDB [tpch1m]> analyze table customer, orders, lineitem,supplier, nation, region
    -> ;
+-----------------+---------+----------+----------+
| Table           | Op      | Msg_type | Msg_text |
+-----------------+---------+----------+----------+
| tpch1m.customer | analyze | status   | OK       |
| tpch1m.orders   | analyze | status   | OK       |
| tpch1m.lineitem | analyze | status   | OK       |
| tpch1m.supplier | analyze | status   | OK       |
| tpch1m.nation   | analyze | status   | OK       |
| tpch1m.region   | analyze | status   | OK       |
+-----------------+---------+----------+----------+
6 rows in set (0.035 sec)

MariaDB [tpch1m]> select n_name, sum(l_extendedprice * (1 - l_discount)) as revenue from customer, orders, lineitem, supplier, nation, region where c_custkey = o_custkey and l_orderkey = o_orderkey and l_suppkey = s_suppkey and c_nationkey = s_nationkey and s_nationkey = n_nationkey and n_regionkey = r_regionkey and r_name = 'AMERICA' and o_orderdate >= '1993-01-01' and o_orderdate < date_add( '1993-01-01' , interval 1 year) group by n_name order by revenue desc;
+-----------+-------------+
| n_name    | revenue     |
+-----------+-------------+
| PERU      | 527161.1575 |
| ARGENTINA |  34521.3330 |
+-----------+-------------+
2 rows in set (0.036 sec)
{noformat}"
1319,MCOL-5195,MCOL,Roman,232723,2022-08-19 18:28:16,"Here is the Postgres 14 query plan for the original TPC-H q2 without indices. 
{noformat}
QUERY PLAN
Hash Join (cost=68.38..217.00 rows=1 width=220)
Hash Cond: ((part.p_partkey = partsupp.ps_partkey) AND ((SubPlan 1) = partsupp.ps_supplycost))
-> Seq Scan on part (cost=0.00..16.15 rows=1 width=33)
Filter: (((p_type)::text ~~ '%STEEL'::text) AND (p_size = 5))
-> Hash (cost=67.78..67.78 rows=40 width=195)
-> Hash Join (cost=49.88..67.78 rows=40 width=195)
Hash Cond: (partsupp.ps_suppkey = supplier.s_suppkey)
-> Seq Scan on partsupp (cost=0.00..15.00 rows=500 width=12)
-> Hash (cost=49.67..49.67 rows=16 width=191)
-> Hash Join (cost=33.73..49.67 rows=16 width=191)
Hash Cond: (supplier.s_nationkey = nation.n_nationkey)
-> Seq Scan on supplier (cost=0.00..14.20 rows=420 width=166)
-> Hash (cost=33.63..33.63 rows=8 width=33)
-> Hash Join (cost=16.54..33.63 rows=8 width=33)
Hash Cond: (nation.n_regionkey = region.r_regionkey)
-> Seq Scan on nation (cost=0.00..15.10 rows=510 width=37)
-> Hash (cost=16.50..16.50 rows=3 width=4)
-> Seq Scan on region (cost=0.00..16.50 rows=3 width=4)
Filter: (r_name = 'ASIA'::bpchar)
SubPlan 1
-> Aggregate (cost=66.21..66.22 rows=1 width=4)
-> Nested Loop (cost=32.14..66.21 rows=1 width=4)
Join Filter: (nation_1.n_regionkey = region_1.r_regionkey)
-> Hash Join (cost=32.14..49.25 rows=10 width=8)
Hash Cond: (nation_1.n_nationkey = supplier_1.s_nationkey)
-> Seq Scan on nation nation_1 (cost=0.00..15.10 rows=510 width=8)
-> Hash (cost=32.09..32.09 rows=4 width=8)
-> Hash Join (cost=16.27..32.09 rows=4 width=8)
Hash Cond: (supplier_1.s_suppkey = partsupp_1.ps_suppkey)
-> Seq Scan on supplier supplier_1 (cost=0.00..14.20 rows=420 width=8)
-> Hash (cost=16.25..16.25 rows=2 width=8)
-> Seq Scan on partsupp partsupp_1 (cost=0.00..16.25 rows=2 width=8)
Filter: (part.p_partkey = ps_partkey)
-> Materialize (cost=0.00..16.52 rows=3 width=4)
-> Seq Scan on region region_1 (cost=0.00..16.50 rows=3 width=4)
Filter: (r_name = 'ASIA'::bpchar)
{noformat}

As you can see Subplan 1 does an aggregation on top of number of hash joins with the top nested loop join. It is worth to mention that the plan will surely change with the data ingested. ",1,"Here is the Postgres 14 query plan for the original TPC-H q2 without indices. 
{noformat}
QUERY PLAN
Hash Join (cost=68.38..217.00 rows=1 width=220)
Hash Cond: ((part.p_partkey = partsupp.ps_partkey) AND ((SubPlan 1) = partsupp.ps_supplycost))
-> Seq Scan on part (cost=0.00..16.15 rows=1 width=33)
Filter: (((p_type)::text ~~ '%STEEL'::text) AND (p_size = 5))
-> Hash (cost=67.78..67.78 rows=40 width=195)
-> Hash Join (cost=49.88..67.78 rows=40 width=195)
Hash Cond: (partsupp.ps_suppkey = supplier.s_suppkey)
-> Seq Scan on partsupp (cost=0.00..15.00 rows=500 width=12)
-> Hash (cost=49.67..49.67 rows=16 width=191)
-> Hash Join (cost=33.73..49.67 rows=16 width=191)
Hash Cond: (supplier.s_nationkey = nation.n_nationkey)
-> Seq Scan on supplier (cost=0.00..14.20 rows=420 width=166)
-> Hash (cost=33.63..33.63 rows=8 width=33)
-> Hash Join (cost=16.54..33.63 rows=8 width=33)
Hash Cond: (nation.n_regionkey = region.r_regionkey)
-> Seq Scan on nation (cost=0.00..15.10 rows=510 width=37)
-> Hash (cost=16.50..16.50 rows=3 width=4)
-> Seq Scan on region (cost=0.00..16.50 rows=3 width=4)
Filter: (r_name = 'ASIA'::bpchar)
SubPlan 1
-> Aggregate (cost=66.21..66.22 rows=1 width=4)
-> Nested Loop (cost=32.14..66.21 rows=1 width=4)
Join Filter: (nation_1.n_regionkey = region_1.r_regionkey)
-> Hash Join (cost=32.14..49.25 rows=10 width=8)
Hash Cond: (nation_1.n_nationkey = supplier_1.s_nationkey)
-> Seq Scan on nation nation_1 (cost=0.00..15.10 rows=510 width=8)
-> Hash (cost=32.09..32.09 rows=4 width=8)
-> Hash Join (cost=16.27..32.09 rows=4 width=8)
Hash Cond: (supplier_1.s_suppkey = partsupp_1.ps_suppkey)
-> Seq Scan on supplier supplier_1 (cost=0.00..14.20 rows=420 width=8)
-> Hash (cost=16.25..16.25 rows=2 width=8)
-> Seq Scan on partsupp partsupp_1 (cost=0.00..16.25 rows=2 width=8)
Filter: (part.p_partkey = ps_partkey)
-> Materialize (cost=0.00..16.52 rows=3 width=4)
-> Seq Scan on region region_1 (cost=0.00..16.50 rows=3 width=4)
Filter: (r_name = 'ASIA'::bpchar)
{noformat}

As you can see Subplan 1 does an aggregation on top of number of hash joins with the top nested loop join. It is worth to mention that the plan will surely change with the data ingested. "
1320,MCOL-5195,MCOL,Roman,232727,2022-08-19 18:57:43,"Here is the plan for MariaDB 10.9.
{noformat}
""query_block"": {
    ""select_id"": 1,
    ""nested_loop"": [
      {
        ""table"": {
          ""table_name"": ""part"",
          ""access_type"": ""ALL"",
          ""rows"": 1,
          ""filtered"": 100,
          ""attached_condition"": ""part.p_size = 5 and part.p_type like '%STEEL'""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""supplier"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100
          },
          ""buffer_type"": ""flat"",
          ""buffer_size"": ""713"",
          ""join_type"": ""BNL""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""partsupp"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100
          },
          ""buffer_type"": ""incremental"",
          ""buffer_size"": ""2Kb"",
          ""join_type"": ""BNL"",
          ""attached_condition"": ""partsupp.ps_partkey = part.p_partkey and partsupp.ps_suppkey = supplier.s_suppkey and partsupp.ps_supplycost = (subquery#2)""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""nation"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100
          },
          ""buffer_type"": ""incremental"",
          ""buffer_size"": ""261"",
          ""join_type"": ""BNL"",
          ""attached_condition"": ""nation.n_nationkey = supplier.s_nationkey""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""region"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100,
            ""attached_condition"": ""region.r_name = 'ASIA'""
          },
          ""buffer_type"": ""incremental"",
          ""buffer_size"": ""450"",
          ""join_type"": ""BNL"",
          ""attached_condition"": ""region.r_regionkey = nation.n_regionkey""
        }
      }
    ],
    ""subqueries"": [
      {
        ""expression_cache"": {
          ""state"": ""uninitialized"",
          ""query_block"": {
            ""select_id"": 2,
            ""nested_loop"": [
              {
                ""table"": {
                  ""table_name"": ""partsupp"",
                  ""access_type"": ""ALL"",
                  ""rows"": 1,
                  ""filtered"": 100,
                  ""attached_condition"": ""part.p_partkey = partsupp.ps_partkey""
                }
              },
              {
                ""block-nl-join"": {
                  ""table"": {
                    ""table_name"": ""supplier"",
                    ""access_type"": ""ALL"",
                    ""rows"": 1,
                    ""filtered"": 100
                  },
                  ""buffer_type"": ""flat"",
                  ""buffer_size"": ""217"",
                  ""join_type"": ""BNL"",
                  ""attached_condition"": ""supplier.s_suppkey = partsupp.ps_suppkey""
                }
              },
              {
                ""block-nl-join"": {
                  ""table"": {
                    ""table_name"": ""nation"",
                    ""access_type"": ""ALL"",
                    ""rows"": 1,
                    ""filtered"": 100
                  },
                  ""buffer_type"": ""incremental"",
                  ""buffer_size"": ""163"",
                  ""join_type"": ""BNL"",
                  ""attached_condition"": ""nation.n_nationkey = supplier.s_nationkey""
                }
              },
              {
                ""block-nl-join"": {
                  ""table"": {
                    ""table_name"": ""region"",
                    ""access_type"": ""ALL"",
                    ""rows"": 1,
                    ""filtered"": 100,
                    ""attached_condition"": ""region.r_name = 'ASIA'""
                  },
                  ""buffer_type"": ""incremental"",
                  ""buffer_size"": ""163"",
                  ""join_type"": ""BNL"",
                  ""attached_condition"": ""region.r_regionkey = nation.n_regionkey""
                }
              }
            ]
          }
        }
      }
    ]
  }
}
{noformat}",2,"Here is the plan for MariaDB 10.9.
{noformat}
""query_block"": {
    ""select_id"": 1,
    ""nested_loop"": [
      {
        ""table"": {
          ""table_name"": ""part"",
          ""access_type"": ""ALL"",
          ""rows"": 1,
          ""filtered"": 100,
          ""attached_condition"": ""part.p_size = 5 and part.p_type like '%STEEL'""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""supplier"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100
          },
          ""buffer_type"": ""flat"",
          ""buffer_size"": ""713"",
          ""join_type"": ""BNL""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""partsupp"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100
          },
          ""buffer_type"": ""incremental"",
          ""buffer_size"": ""2Kb"",
          ""join_type"": ""BNL"",
          ""attached_condition"": ""partsupp.ps_partkey = part.p_partkey and partsupp.ps_suppkey = supplier.s_suppkey and partsupp.ps_supplycost = (subquery#2)""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""nation"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100
          },
          ""buffer_type"": ""incremental"",
          ""buffer_size"": ""261"",
          ""join_type"": ""BNL"",
          ""attached_condition"": ""nation.n_nationkey = supplier.s_nationkey""
        }
      },
      {
        ""block-nl-join"": {
          ""table"": {
            ""table_name"": ""region"",
            ""access_type"": ""ALL"",
            ""rows"": 1,
            ""filtered"": 100,
            ""attached_condition"": ""region.r_name = 'ASIA'""
          },
          ""buffer_type"": ""incremental"",
          ""buffer_size"": ""450"",
          ""join_type"": ""BNL"",
          ""attached_condition"": ""region.r_regionkey = nation.n_regionkey""
        }
      }
    ],
    ""subqueries"": [
      {
        ""expression_cache"": {
          ""state"": ""uninitialized"",
          ""query_block"": {
            ""select_id"": 2,
            ""nested_loop"": [
              {
                ""table"": {
                  ""table_name"": ""partsupp"",
                  ""access_type"": ""ALL"",
                  ""rows"": 1,
                  ""filtered"": 100,
                  ""attached_condition"": ""part.p_partkey = partsupp.ps_partkey""
                }
              },
              {
                ""block-nl-join"": {
                  ""table"": {
                    ""table_name"": ""supplier"",
                    ""access_type"": ""ALL"",
                    ""rows"": 1,
                    ""filtered"": 100
                  },
                  ""buffer_type"": ""flat"",
                  ""buffer_size"": ""217"",
                  ""join_type"": ""BNL"",
                  ""attached_condition"": ""supplier.s_suppkey = partsupp.ps_suppkey""
                }
              },
              {
                ""block-nl-join"": {
                  ""table"": {
                    ""table_name"": ""nation"",
                    ""access_type"": ""ALL"",
                    ""rows"": 1,
                    ""filtered"": 100
                  },
                  ""buffer_type"": ""incremental"",
                  ""buffer_size"": ""163"",
                  ""join_type"": ""BNL"",
                  ""attached_condition"": ""nation.n_nationkey = supplier.s_nationkey""
                }
              },
              {
                ""block-nl-join"": {
                  ""table"": {
                    ""table_name"": ""region"",
                    ""access_type"": ""ALL"",
                    ""rows"": 1,
                    ""filtered"": 100,
                    ""attached_condition"": ""region.r_name = 'ASIA'""
                  },
                  ""buffer_type"": ""incremental"",
                  ""buffer_size"": ""163"",
                  ""join_type"": ""BNL"",
                  ""attached_condition"": ""region.r_regionkey = nation.n_regionkey""
                }
              }
            ]
          }
        }
      }
    ]
  }
}
{noformat}"
1321,MCOL-5195,MCOL,Daniel Lee,248513,2023-01-24 16:33:57,"This ticket covers support for queries 2 and 17, which have been verified and closed",3,"This ticket covers support for queries 2 and 17, which have been verified and closed"
1322,MCOL-52,MCOL,Dipti Joshi,84212,2016-06-12 19:59:26,[~ratzpo] [~jswanhart] Is this complete now ?,1,[~ratzpo] [~jswanhart] Is this complete now ?
1323,MCOL-52,MCOL,Justin Swanhart,84216,2016-06-13 01:26:40,"no - the product does not build and install yet so I can't write up instructions for the process

I am strongly considering removing ExternalProject from CMakeLists.txt and just making user manually build engine at this time.  Integration with CMake as actual CMake project can then proceed.  Using autotools+cmake is not working well.  ",2,"no - the product does not build and install yet so I can't write up instructions for the process

I am strongly considering removing ExternalProject from CMakeLists.txt and just making user manually build engine at this time.  Integration with CMake as actual CMake project can then proceed.  Using autotools+cmake is not working well.  "
1324,MCOL-52,MCOL,David Hill,84343,2016-06-17 15:40:47,"With additional changes to both source and Makefile's, you can now build and install successfully... Woo Hoo.

",3,"With additional changes to both source and Makefile's, you can now build and install successfully... Woo Hoo.

"
1325,MCOL-52,MCOL,David Hill,84556,2016-06-24 20:56:11,"can now build successfully and install columnstore product.
README.md instructions are now up-to-date on how to do so.",4,"can now build successfully and install columnstore product.
README.md instructions are now up-to-date on how to do so."
1326,MCOL-520,MCOL,David Hill,96385,2017-06-08 18:16:34,"Initial investigation...

1. to perform the install including postConfigure, I left things set to full sudo access.
2. Then I tried just doing a shutdown, stop, startsystem and made sure the OAm functionality was working. nobody was complaining about any sudo error, I STILL had to have this specific commands setup to allow sudo access:

Cmnd_Alias DB = /usr/sbin/fuser , /bin/chmod , /usr/sbin/lsof , /bin/chown , /bin/mount , /bin/systemctl , /bin/grep , /bin/echo , /usr/sbin/service , /bin/touch , /bin/pkill , /bin/rm , /bin/mkdir , /usr/sbin/sysctl

So this means at a minimum with NO code changes, we need sudo access to allow oam processes, scripts, and APIs to call these commands.

To allow things to work without any sudo access, code changes would need to be made and the Unix command list above would need to be replaced with some other C++ API command. Which I dont know if any or all are doable by the API.",1,"Initial investigation...

1. to perform the install including postConfigure, I left things set to full sudo access.
2. Then I tried just doing a shutdown, stop, startsystem and made sure the OAm functionality was working. nobody was complaining about any sudo error, I STILL had to have this specific commands setup to allow sudo access:

Cmnd_Alias DB = /usr/sbin/fuser , /bin/chmod , /usr/sbin/lsof , /bin/chown , /bin/mount , /bin/systemctl , /bin/grep , /bin/echo , /usr/sbin/service , /bin/touch , /bin/pkill , /bin/rm , /bin/mkdir , /usr/sbin/sysctl

So this means at a minimum with NO code changes, we need sudo access to allow oam processes, scripts, and APIs to call these commands.

To allow things to work without any sudo access, code changes would need to be made and the Unix command list above would need to be replaced with some other C++ API command. Which I dont know if any or all are doable by the API."
1327,MCOL-520,MCOL,David Thompson,96390,2017-06-08 19:46:51,"Yes, what i was thinking is can we can look at the specific cases for shutdown / startup and see if there are alternatives including not doing these calls.",2,"Yes, what i was thinking is can we can look at the specific cases for shutdown / startup and see if there are alternatives including not doing these calls."
1328,MCOL-520,MCOL,David Hill,96520,2017-06-13 15:15:33,"plan is to document where sudo is used and review to see where we can either remove the use by not doing the call at all or replacing when something else, like a unix api call.",3,"plan is to document where sudo is used and review to see where we can either remove the use by not doing the call at all or replacing when something else, like a unix api call."
1329,MCOL-520,MCOL,David Hill,111480,2018-05-24 20:54:00,"Have non-root install on single-node and multi-node working WITHOUT having to setup 'sudo'.

At this time, this is what the user will need to do as root user before and during the install process. The main one is having to setup the system logging as root user.

the chmod on some of the directories might work using 666 instead of 777, will be looking into that more.

** before package install as root user:

chmod 777 /etc/profile.d
chmod 777 /etc/rc.local
chmod 777 /dev/shm
chmod 777 /etc/default
chmod 777 /tmp
chmod 777 /etc/fstab   // only for amazon ec2 with ebs external storage

mkdir -p /var/log/mariadb/columnstore
chmod 666 /var/log/mariadb/columnstore
chown mariadb-user:mariadb-user /var/log/mariadb/columnstore

// set user file limits file /etc/security/limits.conf 

mariadb-user hard nofile 65536
mariadb-user soft nofile 65536

// systems that support systemctl
// rc-local is used to start columnstore service at boot time

systemctl start rc-local
systemctl enable rc-local

// add to /etc/rc.d/rc.local or /etc/rc.local

/home/mariadb-user/mariadb/columnstore/bin/columnstore start


** after package install and post-install is executed:

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore install


** on uninstall as root user before pre-uninstall is run

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore uninstall
",4,"Have non-root install on single-node and multi-node working WITHOUT having to setup 'sudo'.

At this time, this is what the user will need to do as root user before and during the install process. The main one is having to setup the system logging as root user.

the chmod on some of the directories might work using 666 instead of 777, will be looking into that more.

** before package install as root user:

chmod 777 /etc/profile.d
chmod 777 /etc/rc.local
chmod 777 /dev/shm
chmod 777 /etc/default
chmod 777 /tmp
chmod 777 /etc/fstab   // only for amazon ec2 with ebs external storage

mkdir -p /var/log/mariadb/columnstore
chmod 666 /var/log/mariadb/columnstore
chown mariadb-user:mariadb-user /var/log/mariadb/columnstore

// set user file limits file /etc/security/limits.conf 

mariadb-user hard nofile 65536
mariadb-user soft nofile 65536

// systems that support systemctl
// rc-local is used to start columnstore service at boot time

systemctl start rc-local
systemctl enable rc-local

// add to /etc/rc.d/rc.local or /etc/rc.local

/home/mariadb-user/mariadb/columnstore/bin/columnstore start


** after package install and post-install is executed:

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore install


** on uninstall as root user before pre-uninstall is run

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore uninstall
"
1330,MCOL-520,MCOL,David Hill,111481,2018-05-24 20:54:53,"A script could be written that a user would run to do the root commands, to make the install easier..",5,"A script could be written that a user would run to do the root commands, to make the install easier.."
1331,MCOL-520,MCOL,David Hill,111676,2018-05-29 21:34:25,"final procedure and code before pull request...

** before package install as root user:

chmod 777 /tmp           // temp area for Columnstore to write temp files
chmod 777 /dev/shm       // shared memory
chmod 777 /etc/default   // Columnstore libraries directories
chmod 777 /etc/rc.local  // file that will auto start COlumnstore at reboot

chmod 666 /etc/fstab   // only for amazon ec2 with ebs external storage

// directory for Columnstore logs

mkdir -p /var/log/mariadb/columnstore
chmod 777 /var/log/mariadb/columnstore
chown mariadb-user:mariadb-user /var/log/mariadb/columnstore

// set user file limits file /etc/security/limits.conf 

mariadb-user hard nofile 65536
mariadb-user soft nofile 65536

// systems that support systemctl
// rc-local is used to start columnstore service at boot time

systemctl start rc-local
systemctl enable rc-local

// add to /etc/rc.d/rc.local or /etc/rc.local

runuser -l mariadb-user -c ""/home/mariadb-user/mariadb/columnstore/bin/columnstore restart""


** after package install and post-install is executed:

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore --user=mariadb-user install


** on uninstall as root user after pre-uninstall is run

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore --user=mariadb-user uninstall

--------------------------------------------------------------------------------------------

changes added to post-install and pre-uninstall to reminder users to run syslogSetup.sh

./mariadb/columnstore/bin/post-install --installdir=/home/mariadb-user/mariadb/columnstore

NOTE: For non-root install, you will need to run the following command as root user to
      setup the MariaDB ColumnStore System Logging

      /home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore --user=mariadb-user install


",6,"final procedure and code before pull request...

** before package install as root user:

chmod 777 /tmp           // temp area for Columnstore to write temp files
chmod 777 /dev/shm       // shared memory
chmod 777 /etc/default   // Columnstore libraries directories
chmod 777 /etc/rc.local  // file that will auto start COlumnstore at reboot

chmod 666 /etc/fstab   // only for amazon ec2 with ebs external storage

// directory for Columnstore logs

mkdir -p /var/log/mariadb/columnstore
chmod 777 /var/log/mariadb/columnstore
chown mariadb-user:mariadb-user /var/log/mariadb/columnstore

// set user file limits file /etc/security/limits.conf 

mariadb-user hard nofile 65536
mariadb-user soft nofile 65536

// systems that support systemctl
// rc-local is used to start columnstore service at boot time

systemctl start rc-local
systemctl enable rc-local

// add to /etc/rc.d/rc.local or /etc/rc.local

runuser -l mariadb-user -c ""/home/mariadb-user/mariadb/columnstore/bin/columnstore restart""


** after package install and post-install is executed:

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore --user=mariadb-user install


** on uninstall as root user after pre-uninstall is run

/home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore --user=mariadb-user uninstall

--------------------------------------------------------------------------------------------

changes added to post-install and pre-uninstall to reminder users to run syslogSetup.sh

./mariadb/columnstore/bin/post-install --installdir=/home/mariadb-user/mariadb/columnstore

NOTE: For non-root install, you will need to run the following command as root user to
      setup the MariaDB ColumnStore System Logging

      /home/mariadb-user/mariadb/columnstore/bin/syslogSetup.sh --installdir=/home/mariadb-user/mariadb/columnstore --user=mariadb-user install


"
1332,MCOL-520,MCOL,David Hill,111677,2018-05-29 21:36:12,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/484,7,URL
1333,MCOL-520,MCOL,David Hill,112024,2018-06-05 15:41:52,"Successfully perform a 3 pm gluster install with current 2.1 non-root non-sudo code. So that install works...

Looking into remove some of the need for using the root level directories like 
/tmp
/etc/defaults

I dont know how to get rid of /dev/shm since its used by the C++ sharedmemory apis that ProcMon calls. Will need to see if this can be redirected to use a local non-root user directory.

I did replace the need to not use these 2 directories and used local non-root files.

/etc/profile.d  to /home/guest/.bash_profile
/var/subsys/lock to /home/guest/.lock",8,"Successfully perform a 3 pm gluster install with current 2.1 non-root non-sudo code. So that install works...

Looking into remove some of the need for using the root level directories like 
/tmp
/etc/defaults

I dont know how to get rid of /dev/shm since its used by the C++ sharedmemory apis that ProcMon calls. Will need to see if this can be redirected to use a local non-root user directory.

I did replace the need to not use these 2 directories and used local non-root files.

/etc/profile.d  to /home/guest/.bash_profile
/var/subsys/lock to /home/guest/.lock"
1334,MCOL-520,MCOL,Patrick LeBlanc,117058,2018-09-25 15:58:03,I can help with this if necessary.,9,I can help with this if necessary.
1335,MCOL-520,MCOL,David Hill,117062,2018-09-25 18:52:43,"Finished the code change to make the use of /tmp dynamic based on install type.

root install will continue to use /tmp
non-root install will use /home/xxxxxx/.tmp/columnstore_tmp_files",10,"Finished the code change to make the use of /tmp dynamic based on install type.

root install will continue to use /tmp
non-root install will use /home/xxxxxx/.tmp/columnstore_tmp_files"
1336,MCOL-520,MCOL,Patrick LeBlanc,117065,2018-09-25 21:36:50,"I started looking into procmon's /dev/shm usage, and I think the only thing we need root for is the chmod calls on /dev/shm itself.  It looks like we're setting the mode to 666, which shouldn't be necessary.  I think we can safely get rid of the chmods, replacing them with a test and an error msg at startup.

I'm working on confirming that users do in fact have write access to /dev/shm by default for our target distros.",11,"I started looking into procmon's /dev/shm usage, and I think the only thing we need root for is the chmod calls on /dev/shm itself.  It looks like we're setting the mode to 666, which shouldn't be necessary.  I think we can safely get rid of the chmods, replacing them with a test and an error msg at startup.

I'm working on confirming that users do in fact have write access to /dev/shm by default for our target distros."
1337,MCOL-520,MCOL,David Hill,117066,2018-09-25 21:53:57,"I don't like this option from previous comment

""I think we can safely get rid of the chmods, replacing them with a test and an error msg at startup.""

IMHO we cant have installs failing. Better to find a solution that will always work and not depend on /dev/shm having the correct permissions for a non-root install. The main point with the non-root install is to not use any system level directories like /dev/shm so we wont run into install failures. still best to look for a solution for the non-root install to use a non-root directory like /home/mysql/.shm/dev instead of /dev/shm, if possible..",12,"I don't like this option from previous comment

""I think we can safely get rid of the chmods, replacing them with a test and an error msg at startup.""

IMHO we cant have installs failing. Better to find a solution that will always work and not depend on /dev/shm having the correct permissions for a non-root install. The main point with the non-root install is to not use any system level directories like /dev/shm so we wont run into install failures. still best to look for a solution for the non-root install to use a non-root directory like /home/mysql/.shm/dev instead of /dev/shm, if possible.."
1338,MCOL-520,MCOL,Patrick LeBlanc,117083,2018-09-26 13:55:02,"/dev/shm isn't a system dir, it's just a handle for the shared mem system.  But I see that we're also trying not to use /tmp, which is odd.  I'll take another look at the doc to figure out what we're trying to do here.",13,"/dev/shm isn't a system dir, it's just a handle for the shared mem system.  But I see that we're also trying not to use /tmp, which is odd.  I'll take another look at the doc to figure out what we're trying to do here."
1339,MCOL-520,MCOL,David Hill,117152,2018-09-27 15:45:47,"/tmp updates..

This is how the directories will look with latest updates with a new console command:

root install

mcsadmin> getSystemDirectories
getsystemdirectories   Thu Sep 27 14:53:00 2018

System Installation and Temporary Logging Directories

System Installation Directory = /usr/local/mariadb/columnstore
System Temporary Logging Directory = /tmp/columnstore_tmp_files

mcsadmin> 

non-root

mcsadmin> getsystemd
getsystemdirectories   Thu Sep 27 15:44:35 2018

System Installation and Temporary Logging Directories

System Installation Directory = /home/mysql/mariadb/columnstore
System Temporary Logging Directory = /home/mysql/.tmp

",14,"/tmp updates..

This is how the directories will look with latest updates with a new console command:

root install

mcsadmin> getSystemDirectories
getsystemdirectories   Thu Sep 27 14:53:00 2018

System Installation and Temporary Logging Directories

System Installation Directory = /usr/local/mariadb/columnstore
System Temporary Logging Directory = /tmp/columnstore_tmp_files

mcsadmin> 

non-root

mcsadmin> getsystemd
getsystemdirectories   Thu Sep 27 15:44:35 2018

System Installation and Temporary Logging Directories

System Installation Directory = /home/mysql/mariadb/columnstore
System Temporary Logging Directory = /home/mysql/.tmp

"
1340,MCOL-520,MCOL,David Hill,117263,2018-10-01 19:54:09,"finished the changes for /tmp and $HOME/.tmp for non-root user. and completed testing.

2 changes to go, if these are doable

1. non-root use something different than /dev/shm for shared memory
2. possible to setup and use system logging without running anything as root user.

But at this time, im going to run a system regression test to compare the run times from the 'develop' branch and my 'MCOL-520' branch. Want to make sure things are slower with the changes so for.. also testing for functionality in the new branch

will be doing both root and non-root comparions",15,"finished the changes for /tmp and $HOME/.tmp for non-root user. and completed testing.

2 changes to go, if these are doable

1. non-root use something different than /dev/shm for shared memory
2. possible to setup and use system logging without running anything as root user.

But at this time, im going to run a system regression test to compare the run times from the 'develop' branch and my 'MCOL-520' branch. Want to make sure things are slower with the changes so for.. also testing for functionality in the new branch

will be doing both root and non-root comparions"
1341,MCOL-520,MCOL,David Hill,117557,2018-10-08 19:25:02,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/587/,16,URL
1342,MCOL-520,MCOL,David Hill,118270,2018-10-24 19:39:34,"Woo Hoo... Got non-root gluster install to work....

It will require sudo setup for 1 command and the changing permission on the gluster log file.

Installing for non-root user gluster will require this extra:

1. install gluster packages on each pm

2. chmod 777 -R /var/log/glusterfs

3. edit /etc/sudoers (with correct gluster path)

mysql		ALL=NOPASSWD:	/sbin/gluster
mysql ALL=NOPASSWD: /usr/bin/mount
mysql ALL=NOPASSWD: /usr/bin/umount
mysql ALL=NOPASSWD: /usr/bin/chmod",17,"Woo Hoo... Got non-root gluster install to work....

It will require sudo setup for 1 command and the changing permission on the gluster log file.

Installing for non-root user gluster will require this extra:

1. install gluster packages on each pm

2. chmod 777 -R /var/log/glusterfs

3. edit /etc/sudoers (with correct gluster path)

mysql		ALL=NOPASSWD:	/sbin/gluster
mysql ALL=NOPASSWD: /usr/bin/mount
mysql ALL=NOPASSWD: /usr/bin/umount
mysql ALL=NOPASSWD: /usr/bin/chmod"
1343,MCOL-520,MCOL,David Hill,118323,2018-10-26 20:05:46,"Got Amazon AMI non-root install working. 

chmod to this file /etc/rc.d/rc.local 

# chmod 777 /etc/rc.d/rc.local

If using EBS storage:

Add to /etc/sudoers

mysql ALL=NOPASSWD: /usr/sbin/mkfs.ext2
mysql ALL=NOPASSWD: /usr/bin/chmod
mysql ALL=NOPASSWD: /usr/bin/chown
mysql ALL=NOPASSWD: /usr/bin/sed
mysql ALL=NOPASSWD: /usr/bin/mount
mysql ALL=NOPASSWD: /usr/bin/umount


",18,"Got Amazon AMI non-root install working. 

chmod to this file /etc/rc.d/rc.local 

# chmod 777 /etc/rc.d/rc.local

If using EBS storage:

Add to /etc/sudoers

mysql ALL=NOPASSWD: /usr/sbin/mkfs.ext2
mysql ALL=NOPASSWD: /usr/bin/chmod
mysql ALL=NOPASSWD: /usr/bin/chown
mysql ALL=NOPASSWD: /usr/bin/sed
mysql ALL=NOPASSWD: /usr/bin/mount
mysql ALL=NOPASSWD: /usr/bin/umount


"
1344,MCOL-520,MCOL,David Hill,118668,2018-11-03 21:52:44,"added back in code to run the mysql_upgrade script on upgrade installs..

This is done at the end of postConfigure after the system is up and running. It only runs the script when it detects that a DB already exist",19,"added back in code to run the mysql_upgrade script on upgrade installs..

This is done at the end of postConfigure after the system is up and running. It only runs the script when it detects that a DB already exist"
1345,MCOL-520,MCOL,David Hill,118674,2018-11-04 19:53:53,"new pull request. I have finished tested all install setups excluding External storage non amazon, so system available to test that. But I beleive the sudo requestments will be the same as EBS excluding 

mariadb-user ALL=NOPASSWD: /usr/sbin/mkfs.ext2
mariadb-user ALL=NOPASSWD: /usr/bin/sed

https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/607/

",20,"new pull request. I have finished tested all install setups excluding External storage non amazon, so system available to test that. But I beleive the sudo requestments will be the same as EBS excluding 

mariadb-user ALL=NOPASSWD: /usr/sbin/mkfs.ext2
mariadb-user ALL=NOPASSWD: /usr/bin/sed

URL

"
1346,MCOL-520,MCOL,David Hill,118789,2018-11-06 20:44:31,"change to do the mysql_update script during the mariadb/msqyl post-install setup. There was an issue when it was done at the end of postConfigure.

",21,"change to do the mysql_update script during the mariadb/msqyl post-install setup. There was an issue when it was done at the end of postConfigure.

"
1347,MCOL-520,MCOL,David Hill,118919,2018-11-08 14:58:58,checked into develop for 1.2.1... Patrick and I will continue retesting it from the develop branch.,22,checked into develop for 1.2.1... Patrick and I will continue retesting it from the develop branch.
1348,MCOL-520,MCOL,David Hill,119126,2018-11-12 19:51:18,"I have successful done non-root installs for centos 7 and ubuntu 18. On centos 7, I did the standard way and the DBS way (no sudo access during uninstall)

Will test other OS's",23,"I have successful done non-root installs for centos 7 and ubuntu 18. On centos 7, I did the standard way and the DBS way (no sudo access during uninstall)

Will test other OS's"
1349,MCOL-520,MCOL,David Hill,119177,2018-11-13 15:14:11,Tested debian8/9 ubuntu 16/18 ,24,Tested debian8/9 ubuntu 16/18 
1350,MCOL-520,MCOL,David Hill,119478,2018-11-19 18:02:10,"These are the items that this issue was going to try to achieve. Some of them were and others we place into other JIRAs for future releases. Here is more information on each item

Move away from the dedicated split of root and non root installs.
Officially support only os package installs (i.e. rpm deb).
Binary install will still be needed unofficially for dev setups or supporting non standard linux distros.
The os package install should install the columnstore components and run under the mysql user (like server).

This is all covered by new JIRA https://jira.mariadb.org/browse/MCOL-1501

What was done for this jira for 1.2.1

1. Still have separate root and non-root installs
2. Non-root still require the use of binary packages
3. In the documents, we recommend installing and running in the 'mysql' user. but its not required.
4. Only 1 command is required to be done as root and its running the script syslogSetup.sh
5. Some installs will still require sudo commands to be added to /etc/sudoers and its documented here:

https://mariadb.com/kb/en/library/preparing-for-and-installing-columnstore-version-121/#update-sudo-configuration-if-needed-by-root-user",25,"These are the items that this issue was going to try to achieve. Some of them were and others we place into other JIRAs for future releases. Here is more information on each item

Move away from the dedicated split of root and non root installs.
Officially support only os package installs (i.e. rpm deb).
Binary install will still be needed unofficially for dev setups or supporting non standard linux distros.
The os package install should install the columnstore components and run under the mysql user (like server).

This is all covered by new JIRA URL

What was done for this jira for 1.2.1

1. Still have separate root and non-root installs
2. Non-root still require the use of binary packages
3. In the documents, we recommend installing and running in the 'mysql' user. but its not required.
4. Only 1 command is required to be done as root and its running the script syslogSetup.sh
5. Some installs will still require sudo commands to be added to /etc/sudoers and its documented here:

URL"
1351,MCOL-5206,MCOL,Roman,233200,2022-08-24 18:56:22,Plz review,1,Plz review
1352,MCOL-521,MCOL,David Thompson,96161,2017-06-03 16:53:55,This has been scoped out of 1.1 due to the fact that there is currently no support for aggregate functions with multiple parameters. This would need to be added first.,1,This has been scoped out of 1.1 due to the fact that there is currently no support for aggregate functions with multiple parameters. This would need to be added first.
1353,MCOL-521,MCOL,David Hall,114695,2018-07-31 18:43:18,"For convenience, I've copied the requirements here:
Regression aggregate and window functions are part of ANSI SQL. However MySQL and MariaDB does not support these functions. For predictive analytics application ability to do regression analysis is key to be able to do forecasting. These functions are:

Function
Input data type
Result data type
Description
regr_avgx(Y, X)
double precision
double precision
average of the independent variable (sum(X)/N)
regr_avgy(Y, X)
double precision
double precision
average of the dependent variable (sum(Y)/N)
regr_count(Y, X)
double precision
bigint
number of input rows in which both expressions are nonnull
regr_slope(Y, X)
double precision
double precision
slope of the least-squares-fit linear equation determined by the (X, Y) pairs
regr_intercept(Y, X)
double precision
double precision
y-intercept of the least-squares-fit linear equation determined by the (X, Y) pairs
regr_r2(Y, X)
double precision
double precision
square of the correlation coefficient. correlation coefficient is the regr_intercept(Y, X) for linear model

When regression line is linear for dependant variable y and independent variable  x such that it can be represented by y = a + bx , the regression coefficient is the constant (a) that represents the rate of change of one variable (y) as a function of changes in the other (x).

Slope(b) = (NΣXY - (ΣX)(ΣY)) / (NΣX2 - (ΣX)2)
Intercept(a) = (ΣY - b(ΣX)) / N

The purpose of this feature is to support the above listed regression functions as aggregate and window functions.

MariaDB ColumnStore shall support following aggregate functions
regr_avgx(Y, X)
regr_avgy(Y, X)
regr_count(Y, X)
regr_slope(Y, X)
regr_intercept(Y, X)
regr_r2(Y, X)
MariaDB ColumnStore shall support following window functions
regr_avgx(Y, X)
regr_avgy(Y, X)
regr_count(Y, X)
regr_slope(Y, X)
regr_intercept(Y, X)
regr_r2(Y, X)

Example Details to be added: TBD
",2,"For convenience, I've copied the requirements here:
Regression aggregate and window functions are part of ANSI SQL. However MySQL and MariaDB does not support these functions. For predictive analytics application ability to do regression analysis is key to be able to do forecasting. These functions are:

Function
Input data type
Result data type
Description
regr_avgx(Y, X)
double precision
double precision
average of the independent variable (sum(X)/N)
regr_avgy(Y, X)
double precision
double precision
average of the dependent variable (sum(Y)/N)
regr_count(Y, X)
double precision
bigint
number of input rows in which both expressions are nonnull
regr_slope(Y, X)
double precision
double precision
slope of the least-squares-fit linear equation determined by the (X, Y) pairs
regr_intercept(Y, X)
double precision
double precision
y-intercept of the least-squares-fit linear equation determined by the (X, Y) pairs
regr_r2(Y, X)
double precision
double precision
square of the correlation coefficient. correlation coefficient is the regr_intercept(Y, X) for linear model

When regression line is linear for dependant variable y and independent variable  x such that it can be represented by y = a + bx , the regression coefficient is the constant (a) that represents the rate of change of one variable (y) as a function of changes in the other (x).

Slope(b) = (NΣXY - (ΣX)(ΣY)) / (NΣX2 - (ΣX)2)
Intercept(a) = (ΣY - b(ΣX)) / N

The purpose of this feature is to support the above listed regression functions as aggregate and window functions.

MariaDB ColumnStore shall support following aggregate functions
regr_avgx(Y, X)
regr_avgy(Y, X)
regr_count(Y, X)
regr_slope(Y, X)
regr_intercept(Y, X)
regr_r2(Y, X)
MariaDB ColumnStore shall support following window functions
regr_avgx(Y, X)
regr_avgy(Y, X)
regr_count(Y, X)
regr_slope(Y, X)
regr_intercept(Y, X)
regr_r2(Y, X)

Example Details to be added: TBD
"
1354,MCOL-521,MCOL,David Hall,117267,2018-10-01 22:36:02,"Due to the upgrade to MariaDB 10.3, a manual merge was necessary and branch MCOL-521-b was created. So the merges required are MCOL-521-b #578 for code and MCOL-521 #76 for test.",3,"Due to the upgrade to MariaDB 10.3, a manual merge was necessary and branch MCOL-521-b was created. So the merges required are MCOL-521-b #578 for code and MCOL-521 #76 for test."
1355,MCOL-521,MCOL,Andrew Hutchings,117288,2018-10-02 14:10:58,"Unfortunately CentOS 6 buildbot failed:

{code}
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp: In member function ‘virtual void windowfunction::WF_udaf::operator()(int64_t, int64_t, int64_t)’:
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:819: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:853: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:892: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:925: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:958: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:994: error: using ‘typename’ outside of template
{code}",4,"Unfortunately CentOS 6 buildbot failed:

{code}
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp: In member function ‘virtual void windowfunction::WF_udaf::operator()(int64_t, int64_t, int64_t)’:
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:819: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:853: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:892: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:925: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:958: error: using ‘typename’ outside of template
/data/buildbot/bb-worker/centos6_PR/mariadb-columnstore-engine/utils/windowfunction/wf_udaf.cpp:994: error: using ‘typename’ outside of template
{code}"
1356,MCOL-521,MCOL,David Hall,117315,2018-10-02 16:17:04,"Removed the ""typename"". Interesting that it compiled on my CentOS 6 with that in there.",5,"Removed the ""typename"". Interesting that it compiled on my CentOS 6 with that in there."
1357,MCOL-521,MCOL,Daniel Lee,117756,2018-10-11 15:05:58,"Build verified: Forgot the capture git info.  The build was made in the morning of Oct 10.

Verified the regression*() functions using the datatypetestm and 1mb DBT3 orders tables and against Oracle 18.  Test results were within expectation, with some expected variance.

Also verified the same regr*() functions in windowing function syntax.  Each function has 247 queries and all of them execute without any errors.  The test was done on a build made today.

commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 39c283281af045e5b5fb3fe3f399b21a6b1236ca
Merge: 46775f8 19c8a2b
Author: Roman Nozdrin <drrtuy@gmail.com>
Date:   Wed Oct 10 20:11:12 2018 +0300

    Merge pull request #588 from mariadb-corporation/MCOL-266
    
    MCOL-266 Support true/false DDL default values

/root/columnstore/mariadb-columnstore-tools
commit c2a70128825ba497add8fcf0d0a6d7bbe2af9893
Merge: 1697bdb 3bc9cb0
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Oct 10 08:39:45 2018 +0100

    Merge pull request #14 from mariadb-corporation/MCOL-1242
    
    MCOL-1242 - remote cpimport



",6,"Build verified: Forgot the capture git info.  The build was made in the morning of Oct 10.

Verified the regression*() functions using the datatypetestm and 1mb DBT3 orders tables and against Oracle 18.  Test results were within expectation, with some expected variance.

Also verified the same regr*() functions in windowing function syntax.  Each function has 247 queries and all of them execute without any errors.  The test was done on a build made today.

commit 6b44f0d9c453ede53024f525b7ddf32b5323171b
Merge: 7db44a7 853a0f7
Author: Andrew Hutchings 
Date:   Thu Sep 27 20:37:03 2018 +0100

    Merge pull request #134 from mariadb-corporation/versionCmakeFix
    
    port changes for mysql_version cmake to fix columnstore RPM packaging

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 39c283281af045e5b5fb3fe3f399b21a6b1236ca
Merge: 46775f8 19c8a2b
Author: Roman Nozdrin 
Date:   Wed Oct 10 20:11:12 2018 +0300

    Merge pull request #588 from mariadb-corporation/MCOL-266
    
    MCOL-266 Support true/false DDL default values

/root/columnstore/mariadb-columnstore-tools
commit c2a70128825ba497add8fcf0d0a6d7bbe2af9893
Merge: 1697bdb 3bc9cb0
Author: Andrew Hutchings 
Date:   Wed Oct 10 08:39:45 2018 +0100

    Merge pull request #14 from mariadb-corporation/MCOL-1242
    
    MCOL-1242 - remote cpimport



"
1358,MCOL-521,MCOL,Daniel Lee,117775,2018-10-11 19:38:45,"I checked some of the window function test results for each of the regr*() functions, there seemed to be issues with regr_r2() and regr_slope().  The results between MCS 1.2 and Oracle 18 are more then just significant digits/precisions.
The other four functions seemed to be having similar results.

one slope example.  
select o_custkey, regr_slope(o_custkey,o_orderkey) OVER (PARTITION BY abs(o_custkey) ORDER BY o_custkey ,o_orderkey ROWS BETWEEN 15 PRECEDING AND 15 PRECEDING) from (select * from orders where o_custkey <= 20000) s order by 1, 2;

Below are the last 10 rows of results

Oracle:
149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000

columnstore:
149    0.09149093
149    0.09149093
149    0.09651662
149    0.10292585
149    0.11371947
149    0.12166083
149    0.13115944
149    0.13731343
149    0.15562813
149    0.33258929
",7,"I checked some of the window function test results for each of the regr*() functions, there seemed to be issues with regr_r2() and regr_slope().  The results between MCS 1.2 and Oracle 18 are more then just significant digits/precisions.
The other four functions seemed to be having similar results.

one slope example.  
select o_custkey, regr_slope(o_custkey,o_orderkey) OVER (PARTITION BY abs(o_custkey) ORDER BY o_custkey ,o_orderkey ROWS BETWEEN 15 PRECEDING AND 15 PRECEDING) from (select * from orders where o_custkey <= 20000) s order by 1, 2;

Below are the last 10 rows of results

Oracle:
149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000
       149.00000000               0.0000

columnstore:
149    0.09149093
149    0.09149093
149    0.09651662
149    0.10292585
149    0.11371947
149    0.12166083
149    0.13115944
149    0.13731343
149    0.15562813
149    0.33258929
"
1359,MCOL-521,MCOL,Daniel Lee,117784,2018-10-11 22:01:41,Closing this ticket.  Identified issue is being tracked by MCOL-1793,8,Closing this ticket.  Identified issue is being tracked by MCOL-1793
1360,MCOL-521,MCOL,David Hall,118735,2018-11-05 18:36:28,Missing the stub code for distinct_count(),9,Missing the stub code for distinct_count()
1361,MCOL-521,MCOL,David Hall,118736,2018-11-05 18:40:59,This is the stub code for distinct_count. Just close this JIRA again after merge.,10,This is the stub code for distinct_count. Just close this JIRA again after merge.
1362,MCOL-521,MCOL,David Hall,118739,2018-11-05 19:29:56,Missing code has been merged. There's no need for Test to be involved,11,Missing code has been merged. There's no need for Test to be involved
1363,MCOL-522,MCOL,David Hill,95298,2017-05-15 20:46:36,"testing

here is what gets reported when the remote node doesn't have mariadb columnstore install when running the non-distributed option:

# ./postConfigure -n -p Calpont1


** Performance Module #2 Configuration ***

Enter Nic Interface #1 Host Name (pm2-hostname) > 
Enter Nic Interface #1 IP Address of pm2-hostname (10.128.0.18) > 
Enter Nic Interface #2 Host Name (unassigned) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm2' (2) > 

===== System Installation =====

System Configuration is complete, System Installation is the next step.
Would you like to continue with the System Installation? [y,n] (y) > 



===== Running the MariaDB ColumnStore MariaDB ColumnStore setup scripts =====

post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

----- Performing Install Check on 'pm2 / pm2-hostname' -----


Error: MariaDB ColumnStore not installed on pm2 / pm2-hostname
Install and re-run postConfigure. Exiting...
",1,"testing

here is what gets reported when the remote node doesn't have mariadb columnstore install when running the non-distributed option:

# ./postConfigure -n -p Calpont1


** Performance Module #2 Configuration ***

Enter Nic Interface #1 Host Name (pm2-hostname) > 
Enter Nic Interface #1 IP Address of pm2-hostname (10.128.0.18) > 
Enter Nic Interface #2 Host Name (unassigned) > 
Enter the list (Nx,Ny,Nz) or range (Nx-Nz) of DBRoot IDs assigned to module 'pm2' (2) > 

===== System Installation =====

System Configuration is complete, System Installation is the next step.
Would you like to continue with the System Installation? [y,n] (y) > 



===== Running the MariaDB ColumnStore MariaDB ColumnStore setup scripts =====

post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

----- Performing Install Check on 'pm2 / pm2-hostname' -----


Error: MariaDB ColumnStore not installed on pm2 / pm2-hostname
Install and re-run postConfigure. Exiting...
"
1364,MCOL-522,MCOL,David Hill,95302,2017-05-15 21:30:20,"nonDistrubuted install when pm2 had package preinstalled - this is root user and level installs

----- Performing Install Check on 'pm2 / pm2-hostname' -----

Install log file is located here: /tmp/pm2_binary_install.log


MariaDB ColumnStore Package being installed, please wait ...  DONE

===== Checking MariaDB ColumnStore System Logging Functionality =====

The MariaDB ColumnStore system logging is setup and working on local server

MariaDB ColumnStore System Configuration and Installation is Completed

===== MariaDB ColumnStore System Startup =====

System Installation is complete. If any part of the install failed,
the problem should be investigated and resolved before continuing.

Would you like to startup the MariaDB ColumnStore System? [y,n] (y) > 

----- Starting MariaDB ColumnStore on 'pm2' -----

MariaDB ColumnStore successfully started

----- Starting MariaDB ColumnStore on local server -----

MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait ....... DONE

System Catalog Successfully Created

Run MariaDB ColumnStore Replication Setup..  DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /usr/local/mariadb/columnstore/bin/columnstoreAlias

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

[root@centos-7-pm1 bin]# 


-------------------------------------------

centos7 root installs looking good, testing non-root installs next",2,"nonDistrubuted install when pm2 had package preinstalled - this is root user and level installs

----- Performing Install Check on 'pm2 / pm2-hostname' -----

Install log file is located here: /tmp/pm2_binary_install.log


MariaDB ColumnStore Package being installed, please wait ...  DONE

===== Checking MariaDB ColumnStore System Logging Functionality =====

The MariaDB ColumnStore system logging is setup and working on local server

MariaDB ColumnStore System Configuration and Installation is Completed

===== MariaDB ColumnStore System Startup =====

System Installation is complete. If any part of the install failed,
the problem should be investigated and resolved before continuing.

Would you like to startup the MariaDB ColumnStore System? [y,n] (y) > 

----- Starting MariaDB ColumnStore on 'pm2' -----

MariaDB ColumnStore successfully started

----- Starting MariaDB ColumnStore on local server -----

MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait ....... DONE

System Catalog Successfully Created

Run MariaDB ColumnStore Replication Setup..  DONE

MariaDB ColumnStore Install Successfully Completed, System is Active

Enter the following command to define MariaDB ColumnStore Alias Commands

. /usr/local/mariadb/columnstore/bin/columnstoreAlias

Enter 'mcsmysql' to access the MariaDB ColumnStore SQL console
Enter 'mcsadmin' to access the MariaDB ColumnStore Admin console

[root@centos-7-pm1 bin]# 


-------------------------------------------

centos7 root installs looking good, testing non-root installs next"
1365,MCOL-522,MCOL,David Hill,95569,2017-05-21 19:28:31,"coding is completed

pull request is done, so that needs to be reviewed. Ben, please review when you get a chance. let me know if you have any questions
once its merged into develop, 1.1.0. I will retest
then I will provide a writeup on how to use

Additional info for reviewing and testing:

Current multi-node install is considered a Distributed Install Process. What this means is during postConfigure and AddModule, the MCS package is taken from the $HOME directory and scped to the remote nodes and installed. Part of that process is to check if the packages exist in $HOME. Also during the removeModule, the rpm/deb are erase (when they are installed) from the module being removed.

New Feature is to support Non-Distrusted Install Process. What this means is the user will install the MCS packages on the remote modules before postConfigure or addModule is run. A new option is added to postConfigure '-n', for non-distrusted install. When this option is selected, then on both postConfigure and addmodule, it will check if MCS is installed on the remote node instead of scping a copy there. An error will be reported if MCS is not already installed on the remote module. If it is, then postConfigure and ProcMgr (addModule) will commincate with it and get it configured.
And on the removeModule, the packages arent removed with the system was instaleld with a non-distrubuted feature set. Also all future installs will use the non-distributed installs via a flag that is set in the ColumnStore.xml.",3,"coding is completed

pull request is done, so that needs to be reviewed. Ben, please review when you get a chance. let me know if you have any questions
once its merged into develop, 1.1.0. I will retest
then I will provide a writeup on how to use

Additional info for reviewing and testing:

Current multi-node install is considered a Distributed Install Process. What this means is during postConfigure and AddModule, the MCS package is taken from the $HOME directory and scped to the remote nodes and installed. Part of that process is to check if the packages exist in $HOME. Also during the removeModule, the rpm/deb are erase (when they are installed) from the module being removed.

New Feature is to support Non-Distrusted Install Process. What this means is the user will install the MCS packages on the remote modules before postConfigure or addModule is run. A new option is added to postConfigure '-n', for non-distrusted install. When this option is selected, then on both postConfigure and addmodule, it will check if MCS is installed on the remote node instead of scping a copy there. An error will be reported if MCS is not already installed on the remote module. If it is, then postConfigure and ProcMgr (addModule) will commincate with it and get it configured.
And on the removeModule, the packages arent removed with the system was instaleld with a non-distrubuted feature set. Also all future installs will use the non-distributed installs via a flag that is set in the ColumnStore.xml."
1366,MCOL-522,MCOL,David Hill,95570,2017-05-21 19:28:58,please review pull request,4,please review pull request
1367,MCOL-522,MCOL,Ben Thompson,95688,2017-05-23 17:32:33,Reviewed and merged.,5,Reviewed and merged.
1368,MCOL-522,MCOL,David Hill,95691,2017-05-23 18:15:19,"has been merged, rebuilding and will retest in offical repo",6,"has been merged, rebuilding and will retest in offical repo"
1369,MCOL-522,MCOL,David Hill,95754,2017-05-24 19:26:58,"Added logic to verify matching MCS versions on local (pm1) and remote nodes

output when MCS package not installed on remote node

----- Performing Install Check on 'pm2 / pm2' -----

Error: MariaDB ColumnStore not installed on pm2 / pm2
Install and re-run postConfigure. Exiting...

output when a different version os MCS is installed on remote node. does this by comparing the contains of mariadb/columnstore/releasenum

----- Performing Install Check on 'pm2 / pm2' -----

Error: Local version of MariaDB ColumnStore doesn't match installed version on pm2 / pm2
Install matching version and re-run postConfigure. Exiting...",7,"Added logic to verify matching MCS versions on local (pm1) and remote nodes

output when MCS package not installed on remote node

----- Performing Install Check on 'pm2 / pm2' -----

Error: MariaDB ColumnStore not installed on pm2 / pm2
Install and re-run postConfigure. Exiting...

output when a different version os MCS is installed on remote node. does this by comparing the contains of mariadb/columnstore/releasenum

----- Performing Install Check on 'pm2 / pm2' -----

Error: Local version of MariaDB ColumnStore doesn't match installed version on pm2 / pm2
Install matching version and re-run postConfigure. Exiting..."
1370,MCOL-522,MCOL,Daniel Lee,96015,2017-05-31 20:01:51,"Build tested: Github source 1.1.0

commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Thu May 4 16:06:16 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 2cc5fc7195a76ce6ff475ba85f67123ec94b9fbe
Author: david hill <david.hill@mariadb.com>
Date:   Tue May 30 15:12:50 2017 -0500

I did a non-distributed installation for a 1um2pm stack, the tried to add pm3, using a node that did not have ColumnStore files installed.  The addmodule command failed expected.  But the getprocessstatus command showed pm3 as part of the stack.
",8,"Build tested: Github source 1.1.0

commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 
Date:   Thu May 4 16:06:16 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 2cc5fc7195a76ce6ff475ba85f67123ec94b9fbe
Author: david hill 
Date:   Tue May 30 15:12:50 2017 -0500

I did a non-distributed installation for a 1um2pm stack, the tried to add pm3, using a node that did not have ColumnStore files installed.  The addmodule command failed expected.  But the getprocessstatus command showed pm3 as part of the stack.
"
1371,MCOL-522,MCOL,David Hill,96016,2017-05-31 20:08:17,"correct, this is how it currently works...

it got added in the module configuration, but then failed to get installed. User would need to do a removeMdoule pm3 and start over.",9,"correct, this is how it currently works...

it got added in the module configuration, but then failed to get installed. User would need to do a removeMdoule pm3 and start over."
1372,MCOL-522,MCOL,Daniel Lee,96019,2017-05-31 20:20:34,"Created MCOL-733 to track this issue separately, since it is an existing issue that affects both distributed and non-distributed installations.
",10,"Created MCOL-733 to track this issue separately, since it is an existing issue that affects both distributed and non-distributed installations.
"
1373,MCOL-522,MCOL,David Hill,97150,2017-06-30 19:53:18,phase II has been tested and can be closed,11,phase II has been tested and can be closed
1374,MCOL-523,MCOL,David Thompson,95624,2017-05-22 17:07:49,MDEV-7773 documents the server requirements for supporting this.  We can't completely follow this since we need to be able to support distributed execution at the pm level.,1,MDEV-7773 documents the server requirements for supporting this.  We can't completely follow this since we need to be able to support distributed execution at the pm level.
1375,MCOL-523,MCOL,David Hall,95626,2017-05-22 17:48:34,"MDEV-7773 is for writing UDAF and possibly UDAnF in SQL. It doesn't talk about plugins or anything implemented in a lower level language.

How UDAF implemented in SQL would be handled is unclear, so how it would affect Columnstore is also an unknown.",2,"MDEV-7773 is for writing UDAF and possibly UDAnF in SQL. It doesn't talk about plugins or anything implemented in a lower level language.

How UDAF implemented in SQL would be handled is unclear, so how it would affect Columnstore is also an unknown."
1376,MCOL-523,MCOL,David Thompson,95628,2017-05-22 17:59:05,"Ah good point, yes that would definitely be out of scope in my mind.",3,"Ah good point, yes that would definitely be out of scope in my mind."
1377,MCOL-523,MCOL,David Hall,98229,2017-08-02 20:15:41,"Code complete. Changes to the test suite are in pull request #40, possibly mislabeled as MCOL-597. We're still not done with the project, as we're working on a loader and building more test suites. However, it's time to merge this into develop and get the nightlies running against it.",4,"Code complete. Changes to the test suite are in pull request #40, possibly mislabeled as MCOL-597. We're still not done with the project, as we're working on a loader and building more test suites. However, it's time to merge this into develop and get the nightlies running against it."
1378,MCOL-523,MCOL,David Hall,98230,2017-08-02 20:17:22,The code needs to be reviewed and merged into develop. Re-assign the JIRA to Ben when done if all goes well. He's working on adding stuff.,5,The code needs to be reviewed and merged into develop. Re-assign the JIRA to Ben when done if all goes well. He's working on adding stuff.
1379,MCOL-523,MCOL,David Hall,98253,2017-08-03 16:09:15,Pull Request #41 for regression tests,6,Pull Request #41 for regression tests
1380,MCOL-523,MCOL,Andrew Hutchings,98257,2017-08-03 16:59:32,Review looks good. Moving to Ben as requested.,7,Review looks good. Moving to Ben as requested.
1381,MCOL-528,MCOL,Ben Thompson,90889,2017-01-21 02:04:32,Reviewed and merged,1,Reviewed and merged
1382,MCOL-528,MCOL,David Hill,90894,2017-01-21 05:21:29,"DEVELOPER TEST FROM BUILD

when local query feature is answered as no, the prompt to schema sync occurrs


NOTE: Local Query Feature allows the ability to query data from a single Performance
      Module. Check MariaDB ColumnStore Admin Guide for additional information.

Enable Local Query feature? [y,n] (n) > 

NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
      schemas and InnoDB tables across the User Module nodes. This feature can be enabled
      or disabled, for example, if you wish to configure your own replication post installation.

MariaDB ColumnStore Schema Sync feature, do you want to enable? [y,n] (y) >


------------------------------------------------------------------------------------------------------------------

Why local query is answered yes, schema sync is not prompted. its default to on.

NOTE: Local Query Feature allows the ability to query data from a single Performance
      Module. Check MariaDB ColumnStore Admin Guide for additional information.

Enable Local Query feature? [y,n] (n) > y

NOTE: Amazon AWS CLI Tools are installed and allow MariaDB ColumnStore to create Instances and ABS Volumes",2,"DEVELOPER TEST FROM BUILD

when local query feature is answered as no, the prompt to schema sync occurrs


NOTE: Local Query Feature allows the ability to query data from a single Performance
      Module. Check MariaDB ColumnStore Admin Guide for additional information.

Enable Local Query feature? [y,n] (n) > 

NOTE: The MariaDB ColumnStore Schema Sync feature will replicate all of the
      schemas and InnoDB tables across the User Module nodes. This feature can be enabled
      or disabled, for example, if you wish to configure your own replication post installation.

MariaDB ColumnStore Schema Sync feature, do you want to enable? [y,n] (y) >


------------------------------------------------------------------------------------------------------------------

Why local query is answered yes, schema sync is not prompted. its default to on.

NOTE: Local Query Feature allows the ability to query data from a single Performance
      Module. Check MariaDB ColumnStore Admin Guide for additional information.

Enable Local Query feature? [y,n] (n) > y

NOTE: Amazon AWS CLI Tools are installed and allow MariaDB ColumnStore to create Instances and ABS Volumes"
1383,MCOL-528,MCOL,Daniel Lee,90916,2017-01-22 23:06:34,"Build verified: 1.0.7-0

mcsadmin> getsoft
getsoftwareinfo   Sun Jan 22 17:06:08 2017

Name        : mariadb-columnstore-platform
Version     : 1.0.7
Release     : 1
Architecture: x86_64
Install Date: Sun 22 Jan 2017 12:21:18 PM CST
Group       : Applications/Databases
Size        : 10013744
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.7-1.src.rpm
",3,"Build verified: 1.0.7-0

mcsadmin> getsoft
getsoftwareinfo   Sun Jan 22 17:06:08 2017

Name        : mariadb-columnstore-platform
Version     : 1.0.7
Release     : 1
Architecture: x86_64
Install Date: Sun 22 Jan 2017 12:21:18 PM CST
Group       : Applications/Databases
Size        : 10013744
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.7-1.src.rpm
"
1384,MCOL-529,MCOL,Andrew Hutchings,90964,2017-01-23 17:24:17,Attached debug log from the time it happened. Seems to start at timestamp 17:49:22,1,Attached debug log from the time it happened. Seems to start at timestamp 17:49:22
1385,MCOL-529,MCOL,Andrew Hutchings,90987,2017-01-23 21:06:37,"The cause of this is twofold:

1. With the 1.0.7 source there is some acceleration with regards to new connections
2. CentOS 7 has a limited number of ports by default

test007 creates and destroys many connections per second. With the performance changes in 1.0.7 this is around 300 per second. The ports go into TCP time wait for a short time before they can be reused. CentOS 7 has a maximum of around 30,000 ports by default and we now have a high chance of just going over that.

The workaround is to increase the amount of available ports as follows or by changing the TCP time wait kernel settings:

/sbin/sysctl net.ipv4.ip_local_port_range=""1024 65000""",2,"The cause of this is twofold:

1. With the 1.0.7 source there is some acceleration with regards to new connections
2. CentOS 7 has a limited number of ports by default

test007 creates and destroys many connections per second. With the performance changes in 1.0.7 this is around 300 per second. The ports go into TCP time wait for a short time before they can be reused. CentOS 7 has a maximum of around 30,000 ports by default and we now have a high chance of just going over that.

The workaround is to increase the amount of available ports as follows or by changing the TCP time wait kernel settings:

/sbin/sysctl net.ipv4.ip_local_port_range=""1024 65000"""
1386,MCOL-529,MCOL,Andrew Hutchings,91012,2017-01-24 11:38:04,"The 300 connections/sec are primarily coming from DBRM connections as each DBRM instance requires a new connection. We should look at pooling these connections / instances. We do use SO_REUSEADDR on connection but the TIME_WAIT is at the client end which isn't affected by this socket option.

The error handling is correct so nothing we need to do there. Changing subject of ticket to reflect the DBRM optimisations required.",3,"The 300 connections/sec are primarily coming from DBRM connections as each DBRM instance requires a new connection. We should look at pooling these connections / instances. We do use SO_REUSEADDR on connection but the TIME_WAIT is at the client end which isn't affected by this socket option.

The error handling is correct so nothing we need to do there. Changing subject of ticket to reflect the DBRM optimisations required."
1387,MCOL-529,MCOL,Andrew Hutchings,94025,2017-04-14 13:19:27,"Pull request for develop and develop-1.0 available.

test007 should now pass with this patch.",4,"Pull request for develop and develop-1.0 available.

test007 should now pass with this patch."
1388,MCOL-529,MCOL,Daniel Lee,94799,2017-05-05 14:26:18,Assigned it to Mr. Hill for regression test.  I still need to setup my regression test.,5,Assigned it to Mr. Hill for regression test.  I still need to setup my regression test.
1389,MCOL-529,MCOL,David Hill,95091,2017-05-10 18:15:04,test007 without the workaround now passes regression test without app failures. ,6,test007 without the workaround now passes regression test without app failures. 
1390,MCOL-53,MCOL,Daniel Lee,83862,2016-05-31 15:19:41,Done for Alpha.,1,Done for Alpha.
1391,MCOL-5361,MCOL,alexey vorovich,245579,2022-12-16 16:40:52,"https://mariadb.slack.com/archives/C046YH1GVRT/p1671114896644769

",1,"URL

"
1392,MCOL-5361,MCOL,David Hall,245736,2022-12-19 23:31:18,"In drone, test001 failed. Investigation shows that test working_tpch1_compareLogOnly/sub/sub.select.04 fails on all platforms with and without the MDEV_29988 patch
To verify, I ran the test locally with the MDEV_29988 patch in server:
Before patch from working_tpch1_compareLogOnly/sub/sub.select.04.sql using this sequence:
*   run query
*   cherry-pick patch
*   run query

Before Patch:
{code:java}
select r_name, 
     (select count(*) 
     from supplier
     where s_nationkey = n_nationkey) suppcount,
    (select count(*) 
     from customer
     where c_nationkey = n_nationkey) custcount
    from region
    join nation on r_regionkey = n_regionkey
    group by 1, 2, 3
    order by 1, 2, 3;

+-------------+-----------+-----------+
| r_name      | suppcount | custcount |
+-------------+-----------+-----------+
| AFRICA      |       373 |      5921 |
| AFRICA      |       376 |      5992 |
| AFRICA      |       380 |      5952 |
| AFRICA      |       406 |      5974 |
| AFRICA      |       420 |      5925 |
| AMERICA     |       393 |      5983 |
| AMERICA     |       397 |      5999 |
| AMERICA     |       412 |      6020 |
| AMERICA     |       413 |      5975 |
| AMERICA     |       421 |      5975 |
| ASIA        |       377 |      5948 |
| ASIA        |       399 |      6008 |
| ASIA        |       405 |      6161 |
| ASIA        |       407 |      6024 |
| ASIA        |       415 |      6042 |
| EUROPE      |       390 |      6011 |
| EUROPE      |       396 |      5908 |
| EUROPE      |       398 |      6100 |
| EUROPE      |       401 |      6078 |
| EUROPE      |       402 |      6100 |
| MIDDLE EAST |       362 |      6033 |
| MIDDLE EAST |       393 |      6009 |
| MIDDLE EAST |       411 |      5904 |
| MIDDLE EAST |       415 |      5995 |
| MIDDLE EAST |       438 |      5963 |
+-------------+-----------+-----------+
25 rows in set (0.303 sec)
4:19
{code}
After r patch

{code:java}
select r_name, 
     (select count(*) 
     from supplier
     where s_nationkey = n_nationkey) suppcount,
    (select count(*) 
     from customer
     where c_nationkey = n_nationkey) custcount
    from region
    join nation on r_regionkey = n_regionkey
    group by 1, 2, 3
    order by 1, 2, 3;

ERROR 1815 (HY000): Internal error: MCS-1000: 'supplier' and 'nation' are not joined.
{code}
",2,"In drone, test001 failed. Investigation shows that test working_tpch1_compareLogOnly/sub/sub.select.04 fails on all platforms with and without the MDEV_29988 patch
To verify, I ran the test locally with the MDEV_29988 patch in server:
Before patch from working_tpch1_compareLogOnly/sub/sub.select.04.sql using this sequence:
*   run query
*   cherry-pick patch
*   run query

Before Patch:
{code:java}
select r_name, 
     (select count(*) 
     from supplier
     where s_nationkey = n_nationkey) suppcount,
    (select count(*) 
     from customer
     where c_nationkey = n_nationkey) custcount
    from region
    join nation on r_regionkey = n_regionkey
    group by 1, 2, 3
    order by 1, 2, 3;

+-------------+-----------+-----------+
| r_name      | suppcount | custcount |
+-------------+-----------+-----------+
| AFRICA      |       373 |      5921 |
| AFRICA      |       376 |      5992 |
| AFRICA      |       380 |      5952 |
| AFRICA      |       406 |      5974 |
| AFRICA      |       420 |      5925 |
| AMERICA     |       393 |      5983 |
| AMERICA     |       397 |      5999 |
| AMERICA     |       412 |      6020 |
| AMERICA     |       413 |      5975 |
| AMERICA     |       421 |      5975 |
| ASIA        |       377 |      5948 |
| ASIA        |       399 |      6008 |
| ASIA        |       405 |      6161 |
| ASIA        |       407 |      6024 |
| ASIA        |       415 |      6042 |
| EUROPE      |       390 |      6011 |
| EUROPE      |       396 |      5908 |
| EUROPE      |       398 |      6100 |
| EUROPE      |       401 |      6078 |
| EUROPE      |       402 |      6100 |
| MIDDLE EAST |       362 |      6033 |
| MIDDLE EAST |       393 |      6009 |
| MIDDLE EAST |       411 |      5904 |
| MIDDLE EAST |       415 |      5995 |
| MIDDLE EAST |       438 |      5963 |
+-------------+-----------+-----------+
25 rows in set (0.303 sec)
4:19
{code}
After r patch

{code:java}
select r_name, 
     (select count(*) 
     from supplier
     where s_nationkey = n_nationkey) suppcount,
    (select count(*) 
     from customer
     where c_nationkey = n_nationkey) custcount
    from region
    join nation on r_regionkey = n_regionkey
    group by 1, 2, 3
    order by 1, 2, 3;

ERROR 1815 (HY000): Internal error: MCS-1000: 'supplier' and 'nation' are not joined.
{code}
"
1393,MCOL-5361,MCOL,Daniel Lee,245894,2022-12-21 14:41:39,"Build tested:

MCS   22.08.7 RC2 bb-10.6.11-5-cs-22.08.7-1/27466a7e3a7183de364ed1bb426f3dc941849938
CMAPI 22.08.2, build 823

Related MTR test cases passed.

",3,"Build tested:

MCS   22.08.7 RC2 bb-10.6.11-5-cs-22.08.7-1/27466a7e3a7183de364ed1bb426f3dc941849938
CMAPI 22.08.2, build 823

Related MTR test cases passed.

"
1394,MCOL-5400,MCOL,Roman,248857,2023-01-27 13:18:35,4QA regression tests must pass.,1,4QA regression tests must pass.
1395,MCOL-5400,MCOL,Daniel Lee,250207,2023-02-09 19:40:33,"Build verified: latest build for the develop branch

engine: b2206521cea8c15c0bf6e8d217a08783c483a963
server: 2b721f6864d135aa9a1916798e8daf29e213211e
buildNo: 6716

Execute all MTR test suites.  All but few cases with known issues passed.

Test result is the same as 22.08.8.


",2,"Build verified: latest build for the develop branch

engine: b2206521cea8c15c0bf6e8d217a08783c483a963
server: 2b721f6864d135aa9a1916798e8daf29e213211e
buildNo: 6716

Execute all MTR test suites.  All but few cases with known issues passed.

Test result is the same as 22.08.8.


"
1396,MCOL-548,MCOL,Andrew Hutchings,91566,2017-02-07 15:51:22,They happen due to our builds requiring cpp files that don't exist yet. They don't exist because boost hasn't been run yet. We should be able to suppress them.,1,They happen due to our builds requiring cpp files that don't exist yet. They don't exist because boost hasn't been run yet. We should be able to suppress them.
1397,MCOL-548,MCOL,Roman,106146,2018-01-24 08:14:58,Cmake generates the warnings when it doesn't find cpp files generated by flex and bison at the configuration step since ADD_CUSTOM_COMMAND used for both files run later in the build step. I [added|https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/384] custom targets that depend on generated files to get over with the [warnings|https://hastebin.com/dutetipuki.rb] so please take a look at it.,2,Cmake generates the warnings when it doesn't find cpp files generated by flex and bison at the configuration step since ADD_CUSTOM_COMMAND used for both files run later in the build step. I [added|URL custom targets that depend on generated files to get over with the [warnings|URL so please take a look at it.
1398,MCOL-548,MCOL,Daniel Lee,107202,2018-02-12 16:36:58,"Build tested: Github source for 1.2.0-1

root@vagrant:~/columnstore# cd mariadb-columnstore-server/
root@vagrant:~/columnstore/mariadb-columnstore-server# git show
commit 891620d77d3d3244858c188404d74577d007960c
Author: david hill <david.hill@mariadb.com>
Date:   Mon Nov 20 20:42:05 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index ef9ee50..a6abf8d 100644
--- a/README.md
+++ b/README.md
@@ -274,7 +274,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
root@vagrant:~/columnstore/mariadb-columnstore-server# cd mariadb-columnstore-engine/
root@vagrant:~/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine# git show
commit 96bed8b09c267f9fa024a2ee723c6d8ac8eb15b3
Merge: d511917 81c5183
Author: david hill <david.hill@mariadb.com>
Date:   Mon Nov 20 13:15:00 2017 -0600

    Merge branch 'master' into develop-1.1

root@vagrant:~/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine#

Should the following build warnings be suppressed?

root@vagrant:~# cat buildMCS.log |grep -i ""cmake warning""
CMake Warning at storage/tokudb/CMakeLists.txt:37 (MESSAGE):
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):

",3,"Build tested: Github source for 1.2.0-1

root@vagrant:~/columnstore# cd mariadb-columnstore-server/
root@vagrant:~/columnstore/mariadb-columnstore-server# git show
commit 891620d77d3d3244858c188404d74577d007960c
Author: david hill 
Date:   Mon Nov 20 20:42:05 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index ef9ee50..a6abf8d 100644
--- a/README.md
+++ b/README.md
@@ -274,7 +274,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
root@vagrant:~/columnstore/mariadb-columnstore-server# cd mariadb-columnstore-engine/
root@vagrant:~/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine# git show
commit 96bed8b09c267f9fa024a2ee723c6d8ac8eb15b3
Merge: d511917 81c5183
Author: david hill 
Date:   Mon Nov 20 13:15:00 2017 -0600

    Merge branch 'master' into develop-1.1

root@vagrant:~/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine#

Should the following build warnings be suppressed?

root@vagrant:~# cat buildMCS.log |grep -i ""cmake warning""
CMake Warning at storage/tokudb/CMakeLists.txt:37 (MESSAGE):
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
CMake Warning (dev) at dbcon/ddlpackage/CMakeLists.txt:35 (add_dependencies):
CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):
CMake Warning (dev) at dbcon/dmlpackage/CMakeLists.txt:35 (add_dependencies):

"
1399,MCOL-548,MCOL,Daniel Lee,107275,2018-02-13 17:55:56,"Build verified: Github source 1.2.0-1

/root/columnstore/mariadb-columnstore-server
commit 960853c58bcfd0b92a48fca6f823e0b43fce17a8
Author: david hill <david.hill@mariadb.com>
Date:   Mon Nov 20 20:42:55 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index 1e4a6c6..5d12495 100644
--- a/README.md
+++ b/README.md
@@ -273,7 +273,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ad2f469811d9bfc989174da13343e72ee2599af2
Merge: 070fc37 7c0086c
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Feb 7 10:24:23 2018 +0200

    Merge pull request #399 from drrtuy/MCOL-876
    
    MCOL-876. CS now supports RENAME TABLE sql statement.


Built project using Ubuntu 16.04 and verified cmake output",4,"Build verified: Github source 1.2.0-1

/root/columnstore/mariadb-columnstore-server
commit 960853c58bcfd0b92a48fca6f823e0b43fce17a8
Author: david hill 
Date:   Mon Nov 20 20:42:55 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index 1e4a6c6..5d12495 100644
--- a/README.md
+++ b/README.md
@@ -273,7 +273,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ad2f469811d9bfc989174da13343e72ee2599af2
Merge: 070fc37 7c0086c
Author: Andrew Hutchings 
Date:   Wed Feb 7 10:24:23 2018 +0200

    Merge pull request #399 from drrtuy/MCOL-876
    
    MCOL-876. CS now supports RENAME TABLE sql statement.


Built project using Ubuntu 16.04 and verified cmake output"
1400,MCOL-552,MCOL,David Hill,92377,2017-02-28 17:03:43,"commit 923e995b2920ce0aad1db739440257fbb34a7109
Author: David Hill <david.hill@mariadb.com>
Date:   Tue Feb 28 11:03:08 2017 -0600

    MCOL-552 - send stdout to null

 dbcon/mysql/mysql-Columnstore | 4 ++--
",1,"commit 923e995b2920ce0aad1db739440257fbb34a7109
Author: David Hill 
Date:   Tue Feb 28 11:03:08 2017 -0600

    MCOL-552 - send stdout to null

 dbcon/mysql/mysql-Columnstore | 4 ++--
"
1401,MCOL-552,MCOL,David Hill,92378,2017-02-28 17:19:41,"Fixed - this is from a buildbot suse run.

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

Starting MariaDB Columnstore Database Platform",2,"Fixed - this is from a buildbot suse run.

Running the MariaDB ColumnStore setup scripts

post-mysqld-install Successfully Completed
post-mysql-install Successfully Completed

Starting MariaDB Columnstore Database Platform"
1402,MCOL-552,MCOL,David Hill,93339,2017-03-22 20:23:16,fixed,3,fixed
1403,MCOL-552,MCOL,Daniel Lee,93385,2017-03-23 14:44:19,"mcsadmin> getsoftware
getsoftwareinfo   Thu Mar 23 09:43:45 2017

Name        : mariadb-columnstore-platform
Version     : 1.0.8
Release     : 1
Architecture: x86_64
Install Date: Thu 23 Mar 2017 09:21:09 AM CDT
Group       : Applications
Size        : 10229637
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.8-1.src.rpm
Build Date  : Wed 22 Mar 2017 03:47:58 PM CDT

output from autooam suse rpm installation

==> pm1: 
==> pm1: Running the MariaDB ColumnStore setup scripts
==> pm1: post-mysqld-install Successfully Completed
==> pm1: post-mysql-install Successfully Completed
==> pm1: Starting MariaDB Columnstore Database Platform
==> pm1: 
",4,"mcsadmin> getsoftware
getsoftwareinfo   Thu Mar 23 09:43:45 2017

Name        : mariadb-columnstore-platform
Version     : 1.0.8
Release     : 1
Architecture: x86_64
Install Date: Thu 23 Mar 2017 09:21:09 AM CDT
Group       : Applications
Size        : 10229637
License     : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature   : (none)
Source RPM  : mariadb-columnstore-platform-1.0.8-1.src.rpm
Build Date  : Wed 22 Mar 2017 03:47:58 PM CDT

output from autooam suse rpm installation

==> pm1: 
==> pm1: Running the MariaDB ColumnStore setup scripts
==> pm1: post-mysqld-install Successfully Completed
==> pm1: post-mysql-install Successfully Completed
==> pm1: Starting MariaDB Columnstore Database Platform
==> pm1: 
"
1404,MCOL-56,MCOL,David Hill,83928,2016-06-02 01:35:05,mariadb-columnstore-engine *.cpp and *.h files have been updated,1,mariadb-columnstore-engine *.cpp and *.h files have been updated
1405,MCOL-56,MCOL,Dipti Joshi,84003,2016-06-04 01:08:07,If this has been complete - can you close it please [~hill],2,If this has been complete - can you close it please [~hill]
1406,MCOL-56,MCOL,Dipti Joshi,84010,2016-06-04 02:23:13,"[~hill]Since you have done the engine side of source code, there is still the server side remaining. [~David.Hall] Are you making changes to those source code (.c, cpp, header files) in mariadb-columnstore-server repo that you have changed as part of porting that has been requested in the description of this item - for reference here again
""In mariadb-columnstore-server source for every file that we modify for porting
In the copyright text in the top part of the file does not already have a line with ""Copyright (c) 2016, MariaDB Corporation"" in it, it needs to be added. There might be already Oracle or Monty Program Ab copyright line - that needs to be kept - the MariaDB Corporation line just needs to be added""

[~David.Hall]If it has been already done - we can close this item.",3,"[~hill]Since you have done the engine side of source code, there is still the server side remaining. [~David.Hall] Are you making changes to those source code (.c, cpp, header files) in mariadb-columnstore-server repo that you have changed as part of porting that has been requested in the description of this item - for reference here again
""In mariadb-columnstore-server source for every file that we modify for porting
In the copyright text in the top part of the file does not already have a line with ""Copyright (c) 2016, MariaDB Corporation"" in it, it needs to be added. There might be already Oracle or Monty Program Ab copyright line - that needs to be kept - the MariaDB Corporation line just needs to be added""

[~David.Hall]If it has been already done - we can close this item."
1407,MCOL-56,MCOL,David Hill,84301,2016-06-15 21:44:30,server code never was changed,4,server code never was changed
1408,MCOL-56,MCOL,David Hill,84958,2016-07-14 15:41:52,review list of files for Copyright mariadb 2016 header file changes,5,review list of files for Copyright mariadb 2016 header file changes
1409,MCOL-563,MCOL,Daniel Lee,188205,2021-04-30 21:38:19,"Build tested: 5.6.1 ( Drone #2273 )

VM memory=16gb
Dataset: number 0 to 32M, loaded 10 times, a total of 320000010


1. Reproduced issue
{noformat}
5.5.2-1

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
ERROR 1815 (HY000): Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.

5.6.1, with AllowDiskBasedAggregation disabled:

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
ERROR 1815 (HY000): Internal error: TupleAggregateStep::threadedAggregateRowGroups()[2] IDB-2003: Aggregation/Distinct memory limit is exceeded.
{noformat}

2. Functional test
{noformat}
After setting AllowDiskBasedAggregation to 'Y' and restarted mariadb and mariadb-columnstore, the /var/lib/columnstore/disk-based-aggr-tmpdir directory is automatically created.

/var/lib/columnstore/disk-based-aggr-tmpdir contains files like the following:

Agg-p10099-t0x7f44656160a0-rg54-g8    Agg-p10099-t0x7f446561a100-rg147-g5   Agg-PosHash-p10099-t0x7f44656368e0-g5
Agg-p10099-t0x7f44656160a0-rg54-g9    Agg-p10099-t0x7f446561a100-rg147-g9   Agg-PosHash-p10099-t0x7f44656368e0-g6
Agg-p10099-t0x7f44656160a0-rg55-g0    Agg-p10099-t0x7f446561a100-rg148-g0   Agg-PosHash-p10099-t0x7f44656368e0-g7
Agg-p10099-t0x7f44656160a0-rg55-g1    Agg-p10099-t0x7f446561a100-rg148-g20  Agg-PosHash-p10099-t0x7f44656368e0-g8
Agg-p10099-t0x7f44656160a0-rg55-g10   Agg-p10099-t0x7f446561a100-rg148-g25  Agg-PosHash-p10099-t0x7f44656368e0-g9

[centos8:root~]# du -sh disk-based-aggr-tmpdir
31G	disk-based-aggr-tmpdir

These temp files were removed after query execution

[centos8:root~]# ls -sh /data/qa/shares/mcol563/mcol563.dat
2.9G /data/qa/shares/mcol563/mcol563.dat

The same query executed successfully.
{noformat}

3. Performance comparison
{noformat}
5.6.1-1
TotalUmMemory=25%
AllowDiskBasedAggregation=N

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;

+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (1 min 43.414 sec)

TotalUmMemory=25%
AllowDiskBasedAggregation=Y

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (13 min 6.513 sec)

It took 7.6x long to execute the query using disk.

I am using virtual machine and virtual disk.  This could be contributing to the slowness.

{noformat}
",1,"Build tested: 5.6.1 ( Drone #2273 )

VM memory=16gb
Dataset: number 0 to 32M, loaded 10 times, a total of 320000010


1. Reproduced issue
{noformat}
5.5.2-1

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
ERROR 1815 (HY000): Internal error: IDB-2003: Aggregation/Distinct memory limit is exceeded.

5.6.1, with AllowDiskBasedAggregation disabled:

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
ERROR 1815 (HY000): Internal error: TupleAggregateStep::threadedAggregateRowGroups()[2] IDB-2003: Aggregation/Distinct memory limit is exceeded.
{noformat}

2. Functional test
{noformat}
After setting AllowDiskBasedAggregation to 'Y' and restarted mariadb and mariadb-columnstore, the /var/lib/columnstore/disk-based-aggr-tmpdir directory is automatically created.

/var/lib/columnstore/disk-based-aggr-tmpdir contains files like the following:

Agg-p10099-t0x7f44656160a0-rg54-g8    Agg-p10099-t0x7f446561a100-rg147-g5   Agg-PosHash-p10099-t0x7f44656368e0-g5
Agg-p10099-t0x7f44656160a0-rg54-g9    Agg-p10099-t0x7f446561a100-rg147-g9   Agg-PosHash-p10099-t0x7f44656368e0-g6
Agg-p10099-t0x7f44656160a0-rg55-g0    Agg-p10099-t0x7f446561a100-rg148-g0   Agg-PosHash-p10099-t0x7f44656368e0-g7
Agg-p10099-t0x7f44656160a0-rg55-g1    Agg-p10099-t0x7f446561a100-rg148-g20  Agg-PosHash-p10099-t0x7f44656368e0-g8
Agg-p10099-t0x7f44656160a0-rg55-g10   Agg-p10099-t0x7f446561a100-rg148-g25  Agg-PosHash-p10099-t0x7f44656368e0-g9

[centos8:root~]# du -sh disk-based-aggr-tmpdir
31G	disk-based-aggr-tmpdir

These temp files were removed after query execution

[centos8:root~]# ls -sh /data/qa/shares/mcol563/mcol563.dat
2.9G /data/qa/shares/mcol563/mcol563.dat

The same query executed successfully.
{noformat}

3. Performance comparison
{noformat}
5.6.1-1
TotalUmMemory=25%
AllowDiskBasedAggregation=N

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;

+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (1 min 43.414 sec)

TotalUmMemory=25%
AllowDiskBasedAggregation=Y

MariaDB [mytest]> select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (13 min 6.513 sec)

It took 7.6x long to execute the query using disk.

I am using virtual machine and virtual disk.  This could be contributing to the slowness.

{noformat}
"
1410,MCOL-563,MCOL,Daniel Lee,188206,2021-04-30 22:07:02,"Another test run

5.6.1-1
TotalUmMemory=25%
AllowDiskBasedAggregation=Y
10g DBT3 lineitem dataset, loaded 5 times (299930260 rows)

disk run (right after cpimport)
{noformat}
MariaDB [mytest]> select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;
+------------+----------+----------------------+-----------------+
| l_orderkey | count(*) | sum(l_extendedprice) | avg(l_discount) |
+------------+----------+----------------------+-----------------+
|   60000000 |       35 |           1812257.45 |        0.032857 |
|   59999975 |        5 |            211748.85 |        0.100000 |
|   59999974 |        5 |            430638.55 |        0.070000 |
|   59999973 |        5 |            122330.00 |        0.050000 |
|   59999972 |       35 |           1051620.90 |        0.060000 |
|   59999971 |       15 |            352397.85 |        0.056667 |
|   59999970 |       20 |           1055263.00 |        0.050000 |
|   59999969 |       30 |            743641.60 |        0.036667 |
|   59999968 |       30 |           1448658.35 |        0.063333 |
|   59999943 |       25 |            903624.00 |        0.052000 |
|   59999942 |       20 |            802560.90 |        0.040000 |
|   59999941 |        5 |             23182.05 |        0.020000 |
|   59999940 |       10 |            543875.35 |        0.045000 |
|   59999939 |        5 |             95581.80 |        0.070000 |
|   59999938 |       10 |            248193.30 |        0.085000 |
|   59999937 |       25 |            979903.15 |        0.076000 |
|   59999936 |       35 |           1121067.90 |        0.064286 |
|   59999911 |       15 |            492170.45 |        0.036667 |
|   59999910 |        5 |            286220.25 |        0.050000 |
|   59999909 |       30 |            877935.40 |        0.036667 |
|   59999908 |        5 |            297051.75 |        0.090000 |
|   59999907 |       15 |            697294.25 |        0.063333 |
|   59999906 |       25 |            956472.50 |        0.054000 |
|   59999905 |       35 |           1694750.65 |        0.045714 |
|   59999904 |       30 |           1352077.20 |        0.035000 |
|   59999879 |       25 |           1333028.20 |        0.040000 |
|   59999878 |       25 |            854084.70 |        0.036000 |
|   59999877 |       30 |           1193381.25 |        0.051667 |
|   59999876 |        5 |            177069.00 |        0.020000 |
|   59999875 |       30 |           1203320.70 |        0.055000 |
+------------+----------+----------------------+-----------------+
30 rows in set (2 min 7.935 sec)
{noformat}
cached run (2nd run)
{noformat}
MariaDB [mytest]> select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;
+------------+----------+----------------------+-----------------+
| l_orderkey | count(*) | sum(l_extendedprice) | avg(l_discount) |
+------------+----------+----------------------+-----------------+
|   60000000 |       35 |           1812257.45 |        0.032857 |
|   59999975 |        5 |            211748.85 |        0.100000 |
|   59999974 |        5 |            430638.55 |        0.070000 |
|   59999973 |        5 |            122330.00 |        0.050000 |
|   59999972 |       35 |           1051620.90 |        0.060000 |
|   59999971 |       15 |            352397.85 |        0.056667 |
|   59999970 |       20 |           1055263.00 |        0.050000 |
|   59999969 |       30 |            743641.60 |        0.036667 |
|   59999968 |       30 |           1448658.35 |        0.063333 |
|   59999943 |       25 |            903624.00 |        0.052000 |
|   59999942 |       20 |            802560.90 |        0.040000 |
|   59999941 |        5 |             23182.05 |        0.020000 |
|   59999940 |       10 |            543875.35 |        0.045000 |
|   59999939 |        5 |             95581.80 |        0.070000 |
|   59999938 |       10 |            248193.30 |        0.085000 |
|   59999937 |       25 |            979903.15 |        0.076000 |
|   59999936 |       35 |           1121067.90 |        0.064286 |
|   59999911 |       15 |            492170.45 |        0.036667 |
|   59999910 |        5 |            286220.25 |        0.050000 |
|   59999909 |       30 |            877935.40 |        0.036667 |
|   59999908 |        5 |            297051.75 |        0.090000 |
|   59999907 |       15 |            697294.25 |        0.063333 |
|   59999906 |       25 |            956472.50 |        0.054000 |
|   59999905 |       35 |           1694750.65 |        0.045714 |
|   59999904 |       30 |           1352077.20 |        0.035000 |
|   59999879 |       25 |           1333028.20 |        0.040000 |
|   59999878 |       25 |            854084.70 |        0.036000 |
|   59999877 |       30 |           1193381.25 |        0.051667 |
|   59999876 |        5 |            177069.00 |        0.020000 |
|   59999875 |       30 |           1203320.70 |        0.055000 |
+------------+----------+----------------------+-----------------+
30 rows in set (1 min 59.342 sec)
{noformat}
",2,"Another test run

5.6.1-1
TotalUmMemory=25%
AllowDiskBasedAggregation=Y
10g DBT3 lineitem dataset, loaded 5 times (299930260 rows)

disk run (right after cpimport)
{noformat}
MariaDB [mytest]> select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;
+------------+----------+----------------------+-----------------+
| l_orderkey | count(*) | sum(l_extendedprice) | avg(l_discount) |
+------------+----------+----------------------+-----------------+
|   60000000 |       35 |           1812257.45 |        0.032857 |
|   59999975 |        5 |            211748.85 |        0.100000 |
|   59999974 |        5 |            430638.55 |        0.070000 |
|   59999973 |        5 |            122330.00 |        0.050000 |
|   59999972 |       35 |           1051620.90 |        0.060000 |
|   59999971 |       15 |            352397.85 |        0.056667 |
|   59999970 |       20 |           1055263.00 |        0.050000 |
|   59999969 |       30 |            743641.60 |        0.036667 |
|   59999968 |       30 |           1448658.35 |        0.063333 |
|   59999943 |       25 |            903624.00 |        0.052000 |
|   59999942 |       20 |            802560.90 |        0.040000 |
|   59999941 |        5 |             23182.05 |        0.020000 |
|   59999940 |       10 |            543875.35 |        0.045000 |
|   59999939 |        5 |             95581.80 |        0.070000 |
|   59999938 |       10 |            248193.30 |        0.085000 |
|   59999937 |       25 |            979903.15 |        0.076000 |
|   59999936 |       35 |           1121067.90 |        0.064286 |
|   59999911 |       15 |            492170.45 |        0.036667 |
|   59999910 |        5 |            286220.25 |        0.050000 |
|   59999909 |       30 |            877935.40 |        0.036667 |
|   59999908 |        5 |            297051.75 |        0.090000 |
|   59999907 |       15 |            697294.25 |        0.063333 |
|   59999906 |       25 |            956472.50 |        0.054000 |
|   59999905 |       35 |           1694750.65 |        0.045714 |
|   59999904 |       30 |           1352077.20 |        0.035000 |
|   59999879 |       25 |           1333028.20 |        0.040000 |
|   59999878 |       25 |            854084.70 |        0.036000 |
|   59999877 |       30 |           1193381.25 |        0.051667 |
|   59999876 |        5 |            177069.00 |        0.020000 |
|   59999875 |       30 |           1203320.70 |        0.055000 |
+------------+----------+----------------------+-----------------+
30 rows in set (2 min 7.935 sec)
{noformat}
cached run (2nd run)
{noformat}
MariaDB [mytest]> select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;
+------------+----------+----------------------+-----------------+
| l_orderkey | count(*) | sum(l_extendedprice) | avg(l_discount) |
+------------+----------+----------------------+-----------------+
|   60000000 |       35 |           1812257.45 |        0.032857 |
|   59999975 |        5 |            211748.85 |        0.100000 |
|   59999974 |        5 |            430638.55 |        0.070000 |
|   59999973 |        5 |            122330.00 |        0.050000 |
|   59999972 |       35 |           1051620.90 |        0.060000 |
|   59999971 |       15 |            352397.85 |        0.056667 |
|   59999970 |       20 |           1055263.00 |        0.050000 |
|   59999969 |       30 |            743641.60 |        0.036667 |
|   59999968 |       30 |           1448658.35 |        0.063333 |
|   59999943 |       25 |            903624.00 |        0.052000 |
|   59999942 |       20 |            802560.90 |        0.040000 |
|   59999941 |        5 |             23182.05 |        0.020000 |
|   59999940 |       10 |            543875.35 |        0.045000 |
|   59999939 |        5 |             95581.80 |        0.070000 |
|   59999938 |       10 |            248193.30 |        0.085000 |
|   59999937 |       25 |            979903.15 |        0.076000 |
|   59999936 |       35 |           1121067.90 |        0.064286 |
|   59999911 |       15 |            492170.45 |        0.036667 |
|   59999910 |        5 |            286220.25 |        0.050000 |
|   59999909 |       30 |            877935.40 |        0.036667 |
|   59999908 |        5 |            297051.75 |        0.090000 |
|   59999907 |       15 |            697294.25 |        0.063333 |
|   59999906 |       25 |            956472.50 |        0.054000 |
|   59999905 |       35 |           1694750.65 |        0.045714 |
|   59999904 |       30 |           1352077.20 |        0.035000 |
|   59999879 |       25 |           1333028.20 |        0.040000 |
|   59999878 |       25 |            854084.70 |        0.036000 |
|   59999877 |       30 |           1193381.25 |        0.051667 |
|   59999876 |        5 |            177069.00 |        0.020000 |
|   59999875 |       30 |           1203320.70 |        0.055000 |
+------------+----------+----------------------+-----------------+
30 rows in set (1 min 59.342 sec)
{noformat}
"
1411,MCOL-563,MCOL,Daniel Lee,188239,2021-05-03 01:44:28,Fixed typos and added additional info for clarification.,3,Fixed typos and added additional info for clarification.
1412,MCOL-563,MCOL,Daniel Lee,188355,2021-05-04 02:55:17,"With disk-base aggregation disabled and TotalUmMemory set to 50M, The test in my last comment failed with the aggregation error.

With disk-base aggregation disabled and TotalUmMemory set to back to the default value of 25%, I got the follow timing:

disk run (hot) = 57.488 seconds
cached run (code) = 53.324 seconds
",4,"With disk-base aggregation disabled and TotalUmMemory set to 50M, The test in my last comment failed with the aggregation error.

With disk-base aggregation disabled and TotalUmMemory set to back to the default value of 25%, I got the follow timing:

disk run (hot) = 57.488 seconds
cached run (code) = 53.324 seconds
"
1413,MCOL-563,MCOL,Daniel Lee,188400,2021-05-04 12:25:37,"Concurrentcy test:

disk-base aggregation enabled.

This test is to verify temp files are not overwriting each other and affecting final test results.

Execute the same test queries in three concurrency sessions and got identical results.  Repeated the same concurrency test many times and got matching results.",5,"Concurrentcy test:

disk-base aggregation enabled.

This test is to verify temp files are not overwriting each other and affecting final test results.

Execute the same test queries in three concurrency sessions and got identical results.  Repeated the same concurrency test many times and got matching results."
1414,MCOL-563,MCOL,Daniel Lee,188408,2021-05-04 14:11:05,"Performance comparison when there is enough memory to run the query in-memory.
My understanding is that when there is enough to run the query in-memory, 
even if disk-base aggr is enabled, in-memory aggr will be used
Therefore, performance timing should be similar.

25% toalummemory

Disk-based aggr enabled
disk run (cold):  30 rows in set (56.534 sec)
cached run (hot): 30 rows in set (55.388 sec)

Disk-based aggr disabled
disk run (cold):  30 rows in set (55.777 sec)
cached run (hot): 30 rows in set (52.182 sec)",6,"Performance comparison when there is enough memory to run the query in-memory.
My understanding is that when there is enough to run the query in-memory, 
even if disk-base aggr is enabled, in-memory aggr will be used
Therefore, performance timing should be similar.

25% toalummemory

Disk-based aggr enabled
disk run (cold):  30 rows in set (56.534 sec)
cached run (hot): 30 rows in set (55.388 sec)

Disk-based aggr disabled
disk run (cold):  30 rows in set (55.777 sec)
cached run (hot): 30 rows in set (52.182 sec)"
1415,MCOL-563,MCOL,David Hall,188459,2021-05-04 20:25:26,"I ran the same performance test that Daniel did against the same table:

{code:java}
select count(*) from test ;
+-----------+
| count(*)  |
+-----------+
| 320000010 |
+-----------+
{code}

The following tests were run with a debug build with 25% totalummemory and cached data. The aggregation is expected to fit into memory.

With disk aggregation off
{code:java}
 select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (3 min 1.853 sec)
{code}

With disk aggregation on
{code:java}
select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (3 min 5.229 sec)
{code}

",7,"I ran the same performance test that Daniel did against the same table:

{code:java}
select count(*) from test ;
+-----------+
| count(*)  |
+-----------+
| 320000010 |
+-----------+
{code}

The following tests were run with a debug build with 25% totalummemory and cached data. The aggregation is expected to fit into memory.

With disk aggregation off
{code:java}
 select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (3 min 1.853 sec)
{code}

With disk aggregation on
{code:java}
select a, sum(a), count(*), avg(a) from test group by a order by 1 desc limit 30;
+----------+-----------+----------+---------------+
| a        | sum(a)    | count(*) | avg(a)        |
+----------+-----------+----------+---------------+
| 32000000 | 320000000 |       10 | 32000000.0000 |
| 31999999 | 319999990 |       10 | 31999999.0000 |
| 31999998 | 319999980 |       10 | 31999998.0000 |
| 31999997 | 319999970 |       10 | 31999997.0000 |
| 31999996 | 319999960 |       10 | 31999996.0000 |
| 31999995 | 319999950 |       10 | 31999995.0000 |
| 31999994 | 319999940 |       10 | 31999994.0000 |
| 31999993 | 319999930 |       10 | 31999993.0000 |
| 31999992 | 319999920 |       10 | 31999992.0000 |
| 31999991 | 319999910 |       10 | 31999991.0000 |
| 31999990 | 319999900 |       10 | 31999990.0000 |
| 31999989 | 319999890 |       10 | 31999989.0000 |
| 31999988 | 319999880 |       10 | 31999988.0000 |
| 31999987 | 319999870 |       10 | 31999987.0000 |
| 31999986 | 319999860 |       10 | 31999986.0000 |
| 31999985 | 319999850 |       10 | 31999985.0000 |
| 31999984 | 319999840 |       10 | 31999984.0000 |
| 31999983 | 319999830 |       10 | 31999983.0000 |
| 31999982 | 319999820 |       10 | 31999982.0000 |
| 31999981 | 319999810 |       10 | 31999981.0000 |
| 31999980 | 319999800 |       10 | 31999980.0000 |
| 31999979 | 319999790 |       10 | 31999979.0000 |
| 31999978 | 319999780 |       10 | 31999978.0000 |
| 31999977 | 319999770 |       10 | 31999977.0000 |
| 31999976 | 319999760 |       10 | 31999976.0000 |
| 31999975 | 319999750 |       10 | 31999975.0000 |
| 31999974 | 319999740 |       10 | 31999974.0000 |
| 31999973 | 319999730 |       10 | 31999973.0000 |
| 31999972 | 319999720 |       10 | 31999972.0000 |
| 31999971 | 319999710 |       10 | 31999971.0000 |
+----------+-----------+----------+---------------+
30 rows in set (3 min 5.229 sec)
{code}

"
1416,MCOL-563,MCOL,Daniel Lee,188472,2021-05-04 22:07:20,"Build test: 5.6.1 ( Drone #2319 )

I executed the same lineitem test for the following:

TotalUMMemory: 50M
Disk-base aggregation: enabled

The query returned the following error:

ERROR 1815 (HY000) at line 1: Internal error: TupleAggregateStep::doThreadedAggregate() IDB-2056: There was an IO error during a disk-based aggregation.

I repeated the test few times, and got the same error.

[~alexey.antipovsky]Please run a test on you system using the same build to see if this also hapens to you.  Thanks

",8,"Build test: 5.6.1 ( Drone #2319 )

I executed the same lineitem test for the following:

TotalUMMemory: 50M
Disk-base aggregation: enabled

The query returned the following error:

ERROR 1815 (HY000) at line 1: Internal error: TupleAggregateStep::doThreadedAggregate() IDB-2056: There was an IO error during a disk-based aggregation.

I repeated the test few times, and got the same error.

[~alexey.antipovsky]Please run a test on you system using the same build to see if this also hapens to you.  Thanks

"
1417,MCOL-563,MCOL,Daniel Lee,188775,2021-05-07 21:24:31,"Build verified: 5.6.1-1 ( Drone #2338 )

",9,"Build verified: 5.6.1-1 ( Drone #2338 )

"
1418,MCOL-563,MCOL,David Hall,190067,2021-05-21 16:07:27,There are additional improvements waiting for unit test. When this PR is ready we'll get it back to testing.,10,There are additional improvements waiting for unit test. When this PR is ready we'll get it back to testing.
1419,MCOL-563,MCOL,Daniel Lee,190288,2021-05-25 16:23:42,"Build tested: 5.6.1 (Drone #2434) 

VM memory: 4GB
TotalUmMemory=25%
AllowDiskBasedAggregation=Y
10g DBT3 lineitem dataset, loaded 5 times (299930260 rows)
Test query: select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;

1. Execute time is about 8.5% faster than previous build for disk run
2. Execute time is virtually the same for cached run
{noformat}
Drone #2273 
disk-run: 30 rows in set (2 min 7.935 sec)
cached-run: 30 rows in set (1 min 59.342 sec)

Drone #2434
disk-run: 30 rows in set (1 min 57.961 sec)
cached-run: 30 rows in set (1 min 59.468 sec)
{noformat}

3. Max tmp file utilization is way down, 1.3gb compared to 31gb used by previous build
4. tmp files are 4 to 20kb each
{noformat}
[centos8:root~]# ls -al disk-based-aggr-tmpdir/
total 76
[centos8:root~]# ls disk-based-aggr-tmpdir -al
total 220
drwxr-xr-x 19 mysql mysql  4096 May 25 17:42 .
drwxr-xr-x  6 mysql mysql  4096 May 25 17:42 ..
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657000
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657140
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657280
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa1326573c0
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657500
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657640
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657780
drwxr-xr-x  2 mysql mysql 16384 May 25 17:43 p6793-t0x7fa1326578c0
drwxr-xr-x  2 mysql mysql 16384 May 25 17:43 p6793-t0x7fa132657a00
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657b40
drwxr-xr-x  2 mysql mysql 20480 May 25 17:43 p6793-t0x7fa132657c80
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657dc0
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657f00
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132658040
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132658180
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa1326582c0
drwxr-xr-x  2 mysql mysql  4096 May 25 17:42 p6793-t0x7fa137e82440

{noformat}

5. What is the actual location of the aggr temp files?
   Columnstore.xml indicates ""<!-- <TempDir>/tmp/cs-agg</TempDir> -->""
   But I found files in /var/lib/columnstore/disk-based-aggr-tmpdir",11,"Build tested: 5.6.1 (Drone #2434) 

VM memory: 4GB
TotalUmMemory=25%
AllowDiskBasedAggregation=Y
10g DBT3 lineitem dataset, loaded 5 times (299930260 rows)
Test query: select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;

1. Execute time is about 8.5% faster than previous build for disk run
2. Execute time is virtually the same for cached run
{noformat}
Drone #2273 
disk-run: 30 rows in set (2 min 7.935 sec)
cached-run: 30 rows in set (1 min 59.342 sec)

Drone #2434
disk-run: 30 rows in set (1 min 57.961 sec)
cached-run: 30 rows in set (1 min 59.468 sec)
{noformat}

3. Max tmp file utilization is way down, 1.3gb compared to 31gb used by previous build
4. tmp files are 4 to 20kb each
{noformat}
[centos8:root~]# ls -al disk-based-aggr-tmpdir/
total 76
[centos8:root~]# ls disk-based-aggr-tmpdir -al
total 220
drwxr-xr-x 19 mysql mysql  4096 May 25 17:42 .
drwxr-xr-x  6 mysql mysql  4096 May 25 17:42 ..
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657000
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657140
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657280
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa1326573c0
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657500
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657640
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657780
drwxr-xr-x  2 mysql mysql 16384 May 25 17:43 p6793-t0x7fa1326578c0
drwxr-xr-x  2 mysql mysql 16384 May 25 17:43 p6793-t0x7fa132657a00
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657b40
drwxr-xr-x  2 mysql mysql 20480 May 25 17:43 p6793-t0x7fa132657c80
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657dc0
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132657f00
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132658040
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa132658180
drwxr-xr-x  2 mysql mysql 12288 May 25 17:43 p6793-t0x7fa1326582c0
drwxr-xr-x  2 mysql mysql  4096 May 25 17:42 p6793-t0x7fa137e82440

{noformat}

5. What is the actual location of the aggr temp files?
   Columnstore.xml indicates ""/tmp/cs-agg -->""
   But I found files in /var/lib/columnstore/disk-based-aggr-tmpdir"
1420,MCOL-563,MCOL,David Hall,190373,2021-05-26 14:48:27,Default temp space is /var/lib/columnstore/disk-based-aggr-tmpdir. The line in the XML is commented as a placeholder if you wish to change the tempdir. I have to do that because my /var/lib disk is tiny and I need to move it to a larger disk.,12,Default temp space is /var/lib/columnstore/disk-based-aggr-tmpdir. The line in the XML is commented as a placeholder if you wish to change the tempdir. I have to do that because my /var/lib disk is tiny and I need to move it to a larger disk.
1421,MCOL-563,MCOL,Daniel Lee,190401,2021-05-26 17:00:59,"Build tested: 5.6.1 (Drone #2452)

The consistency issue for aggregation tmp directory in Columnstore.xml has been corrected.

	<!-- <TempDir>/var/lib/columnstore/disk-based-aggr-tmpdir</TempDir> -->
",13,"Build tested: 5.6.1 (Drone #2452)

The consistency issue for aggregation tmp directory in Columnstore.xml has been corrected.

	/var/lib/columnstore/disk-based-aggr-tmpdir -->
"
1422,MCOL-563,MCOL,Todd Stoffel,190405,2021-05-26 17:14:22,Let's simplify the tmpdir path.,14,Let's simplify the tmpdir path.
1423,MCOL-563,MCOL,David Hall,190522,2021-05-27 17:25:01,"Just for fun, let me note that the entry
<TempFileDir>/columnstore_tmp_files</TempFileDir>
doesn't appear to be used anymore.

On Thu, May 27, 2021 at 11:13 AM Gregory Dorman (Jira) <jira@mariadb.org>

",15,"Just for fun, let me note that the entry
/columnstore_tmp_files
doesn't appear to be used anymore.

On Thu, May 27, 2021 at 11:13 AM Gregory Dorman (Jira) 

"
1424,MCOL-563,MCOL,Daniel Lee,190664,2021-05-28 16:59:12,"Build tested: 5.6.1 ( Drone #2466)

/tmp/columnstore_tmp_files/aggregates is not used as the tmp directory for aggregation.  But this directory is no longer in the Columnstore.xml file as the default tmp directory. Can user still modify the Columnstore.xml file to relocate the tmp directory?

",16,"Build tested: 5.6.1 ( Drone #2466)

/tmp/columnstore_tmp_files/aggregates is not used as the tmp directory for aggregation.  But this directory is no longer in the Columnstore.xml file as the default tmp directory. Can user still modify the Columnstore.xml file to relocate the tmp directory?

"
1425,MCOL-563,MCOL,Daniel Lee,191046,2021-06-03 18:45:18,"Build tested: 5.6.1 Engine: Drone #2503, CMAPI: Drone #490

VM memory: 16gb
Dataset: 10G DBT3 lineitem, loaded 5 times.

Test #1

TotalUmMemory: 25%
AllowDiskBasedAggregation: N

Tests performed as expected

Test #2
TotalUmMemory: 25%
AllowDiskBasedAggregation: Y

Tests performed as expected.  Disk-based aggregation was not used since there is enough memory to perform in-memory aggregation

Test #3
TotalUmMemory: 1GB
AllowDiskBasedAggregation: Y

Disk-based aggregation was used as expected.

After installation and data loading, my first test hit the following error.  I repeated the test 9 more times and all worked fine.  I restarted ColumnStore and tried it again.  That also worked.  I also truncated and loaded the lineitem table.  The test also worked 5 times in a row.  I don't know what triggered the first failure.
{noformat}
select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;

ERROR 1815 (HY000): Internal error: TupleAggregateStep::doThreadedAggregate() IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
{noformat}

err.log
Jun  3 17:10:23 centos-8 threadpool[8235]: 23.816630 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 threadpool[8235]: 24.012870 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 threadpool[8235]: 24.106256 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 threadpool[8235]: 24.352455 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 joblist[8235]: 24.913216 |3|0|0| C 05 CAL0000: TupleAggregateStep::doThreadedAggregate() IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory",17,"Build tested: 5.6.1 Engine: Drone #2503, CMAPI: Drone #490

VM memory: 16gb
Dataset: 10G DBT3 lineitem, loaded 5 times.

Test #1

TotalUmMemory: 25%
AllowDiskBasedAggregation: N

Tests performed as expected

Test #2
TotalUmMemory: 25%
AllowDiskBasedAggregation: Y

Tests performed as expected.  Disk-based aggregation was not used since there is enough memory to perform in-memory aggregation

Test #3
TotalUmMemory: 1GB
AllowDiskBasedAggregation: Y

Disk-based aggregation was used as expected.

After installation and data loading, my first test hit the following error.  I repeated the test 9 more times and all worked fine.  I restarted ColumnStore and tried it again.  That also worked.  I also truncated and loaded the lineitem table.  The test also worked 5 times in a row.  I don't know what triggered the first failure.
{noformat}
select l_orderkey, count(*), sum(l_extendedprice), avg(l_discount) from lineitem group by l_orderkey order by 1 desc limit 30;

ERROR 1815 (HY000): Internal error: TupleAggregateStep::doThreadedAggregate() IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
{noformat}

err.log
Jun  3 17:10:23 centos-8 threadpool[8235]: 23.816630 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 threadpool[8235]: 24.012870 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 threadpool[8235]: 24.106256 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 threadpool[8235]: 24.352455 |0|0|0| E 22 CAL0005: ThreadPool: Caught exception during execution:  IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory
Jun  3 17:10:24 centos-8 joblist[8235]: 24.913216 |3|0|0| C 05 CAL0000: TupleAggregateStep::doThreadedAggregate() IDB-2056: There was an IO error during a disk-based aggregation: No such file or directory"
1426,MCOL-563,MCOL,Daniel Lee,191122,2021-06-04 15:59:13,"Build verified: 5.6.1 ( Drone #2524)

Verified that the first disk-based aggregate queries no longer reporting the missing file error.  The query works now.
",18,"Build verified: 5.6.1 ( Drone #2524)

Verified that the first disk-based aggregate queries no longer reporting the missing file error.  The query works now.
"
1427,MCOL-563,MCOL,Daniel Lee,191139,2021-06-05 02:56:44,"Build verified: 5.6.1 ( Drone #2537)

Verified with this latest build

",19,"Build verified: 5.6.1 ( Drone #2537)

Verified with this latest build

"
1428,MCOL-563,MCOL,Daniel Lee,191312,2021-06-07 16:25:27,"Build verified: 5.6.1 ( Drone #2551)

Reproduced the issue with build 2537 using info from the Google Drive link above and verified the fix in build #2551.

Both queries did use the tmp directory for aggregation temp files.  I used the temp directory for a very short time and the customer may not have caught it in time.

",20,"Build verified: 5.6.1 ( Drone #2551)

Reproduced the issue with build 2537 using info from the Google Drive link above and verified the fix in build #2551.

Both queries did use the tmp directory for aggregation temp files.  I used the temp directory for a very short time and the customer may not have caught it in time.

"
1429,MCOL-569,MCOL,David Thompson,91964,2017-02-16 18:32:01,Don't see this as critical priority as there is a simple workaround to change the table name. It should be fixed however as there may be cases where a code generator or ported product might run into this.,1,Don't see this as critical priority as there is a simple workaround to change the table name. It should be fixed however as there may be cases where a code generator or ported product might run into this.
1430,MCOL-569,MCOL,Justin Swanhart,91973,2017-02-16 20:54:59,it is not simple to change the table name of an existing application without changing the whole application.  This should certainly be critical as the workaround isn't really a workaround - it requires too much work.  ,2,it is not simple to change the table name of an existing application without changing the whole application.  This should certainly be critical as the workaround isn't really a workaround - it requires too much work.  
1431,MCOL-569,MCOL,Andrew Hutchings,106158,2018-01-24 11:10:07,Fix contributed along with MCOL-573,3,Fix contributed along with MCOL-573
1432,MCOL-569,MCOL,Daniel Lee,107270,2018-02-13 17:18:21,"Build verified: Github source 1.2.0-1

/root/columnstore/mariadb-columnstore-server
commit 960853c58bcfd0b92a48fca6f823e0b43fce17a8
Author: david hill <david.hill@mariadb.com>
Date:   Mon Nov 20 20:42:55 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index 1e4a6c6..5d12495 100644
--- a/README.md
+++ b/README.md
@@ -273,7 +273,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ad2f469811d9bfc989174da13343e72ee2599af2
Merge: 070fc37 7c0086c
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Feb 7 10:24:23 2018 +0200

    Merge pull request #399 from drrtuy/MCOL-876
    
    MCOL-876. CS now supports RENAME TABLE sql statement.

Verified DDL, DML, query and cpimport
",4,"Build verified: Github source 1.2.0-1

/root/columnstore/mariadb-columnstore-server
commit 960853c58bcfd0b92a48fca6f823e0b43fce17a8
Author: david hill 
Date:   Mon Nov 20 20:42:55 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index 1e4a6c6..5d12495 100644
--- a/README.md
+++ b/README.md
@@ -273,7 +273,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ad2f469811d9bfc989174da13343e72ee2599af2
Merge: 070fc37 7c0086c
Author: Andrew Hutchings 
Date:   Wed Feb 7 10:24:23 2018 +0200

    Merge pull request #399 from drrtuy/MCOL-876
    
    MCOL-876. CS now supports RENAME TABLE sql statement.

Verified DDL, DML, query and cpimport
"
1433,MCOL-57,MCOL,Dipti Joshi,84004,2016-06-04 01:08:31,If this has been complete - can you close it please [~hill],1,If this has been complete - can you close it please [~hill]
1434,MCOL-574,MCOL,Andrew Hutchings,92003,2017-02-17 16:51:34,"test001 should trigger this, but just telling CrossEngine to use 'localhost' and trying to use it is a good manual test.",1,"test001 should trigger this, but just telling CrossEngine to use 'localhost' and trying to use it is a good manual test."
1435,MCOL-574,MCOL,Daniel Lee,92452,2017-03-01 22:49:26,"Build tested: Github source

[root@localhost columnstore]# cd mariadb-columnstore-server/
[root@localhost mariadb-columnstore-server]# git show
commit 3da188e5c8a2630019ea810fb8c1bd3ece5e058b
Merge: 5d9686c 53c1df7
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Fri Feb 10 15:07:31 2017 +0000

    Merge pull request #31 from jbfavre/fix_deb_package_dependency
    
    MCOL-562 Fix Debian package dependencies

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 16cef50caedd9ec7585b04c096996a9441bdf2d5
Author: David Hill <david.hill@mariadb.com>
Date:   Wed Mar 1 10:39:11 2017 -0600

    change the check for prompt back to the previous code


Performed cross engine join test using localhost as the host.
MariaDB [mytest]> show create table t1;
+-------+----------------------------------------------------------------------------------------+
| Table | Create Table                                                                           |
+-------+----------------------------------------------------------------------------------------+
| t1    | CREATE TABLE `t1` (
  `c1` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 |
+-------+----------------------------------------------------------------------------------------+
1 row in set (0.00 sec)

MariaDB [mytest]> quit
Bye
[root@localhost ~]# cd /usr/local/mariadb/columnstore/etc
[root@localhost etc]# nano Columnstore.xml
[root@localhost etc]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 22
Server version: 10.1.21-MariaDB Columnstore 1.1.0-1

Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> insert into t1 values (1);
Query OK, 1 row affected (0.00 sec)

MariaDB [mytest]> select * from lineitem, t1 where c1=l_orderkey;
+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+------------------------------------+------+
| l_orderkey | l_partkey | l_suppkey | l_linenumber | l_quantity | l_extendedprice | l_discount | l_tax | l_returnflag | l_linestatus | l_shipdate | l_commitdate | l_receiptdate | l_shipinstruct    | l_shipmode | l_comment                          | c1   |
+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+------------------------------------+------+
|          1 |    155190 |      7706 |            1 |      17.00 |        21168.23 |       0.04 |  0.02 | N            | O            | 1996-03-13 | 1996-02-12   | 1996-03-22    | DELIVER IN PERSON | TRUCK      | egular courts above the            |    1 |
|          1 |     67310 |      7311 |            2 |      36.00 |        45983.16 |       0.09 |  0.06 | N            | O            | 1996-04-12 | 1996-02-28   | 1996-04-20    | TAKE BACK RETURN  | MAIL       | ly final dependencies: slyly bold  |    1 |
|          1 |     63700 |      3701 |            3 |       8.00 |        13309.60 |       0.10 |  0.02 | N            | O            | 1996-01-29 | 1996-03-05   | 1996-01-31    | TAKE BACK RETURN  | REG AIR    | riously. regular, express dep      |    1 |
|          1 |      2132 |      4633 |            4 |      28.00 |        28955.64 |       0.09 |  0.06 | N            | O            | 1996-04-21 | 1996-03-30   | 1996-05-16    | NONE              | AIR        | lites. fluffily even de            |    1 |
|          1 |     24027 |      1534 |            5 |      24.00 |        22824.48 |       0.10 |  0.04 | N            | O            | 1996-03-30 | 1996-03-14   | 1996-04-01    | NONE              | FOB        |  pending foxes. slyly re           |    1 |
|          1 |     15635 |       638 |            6 |      32.00 |        49620.16 |       0.07 |  0.02 | N            | O            | 1996-01-30 | 1996-02-07   | 1996-02-03    | DELIVER IN PERSON | MAIL       | arefully slyly ex                  |    1 |
+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+------------------------------------+------+
6 rows in set (0.41 sec)
",2,"Build tested: Github source

[root@localhost columnstore]# cd mariadb-columnstore-server/
[root@localhost mariadb-columnstore-server]# git show
commit 3da188e5c8a2630019ea810fb8c1bd3ece5e058b
Merge: 5d9686c 53c1df7
Author: Andrew Hutchings 
Date:   Fri Feb 10 15:07:31 2017 +0000

    Merge pull request #31 from jbfavre/fix_deb_package_dependency
    
    MCOL-562 Fix Debian package dependencies

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 16cef50caedd9ec7585b04c096996a9441bdf2d5
Author: David Hill 
Date:   Wed Mar 1 10:39:11 2017 -0600

    change the check for prompt back to the previous code


Performed cross engine join test using localhost as the host.
MariaDB [mytest]> show create table t1;
+-------+----------------------------------------------------------------------------------------+
| Table | Create Table                                                                           |
+-------+----------------------------------------------------------------------------------------+
| t1    | CREATE TABLE `t1` (
  `c1` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 |
+-------+----------------------------------------------------------------------------------------+
1 row in set (0.00 sec)

MariaDB [mytest]> quit
Bye
[root@localhost ~]# cd /usr/local/mariadb/columnstore/etc
[root@localhost etc]# nano Columnstore.xml
[root@localhost etc]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 22
Server version: 10.1.21-MariaDB Columnstore 1.1.0-1

Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [mytest]> insert into t1 values (1);
Query OK, 1 row affected (0.00 sec)

MariaDB [mytest]> select * from lineitem, t1 where c1=l_orderkey;
+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+------------------------------------+------+
| l_orderkey | l_partkey | l_suppkey | l_linenumber | l_quantity | l_extendedprice | l_discount | l_tax | l_returnflag | l_linestatus | l_shipdate | l_commitdate | l_receiptdate | l_shipinstruct    | l_shipmode | l_comment                          | c1   |
+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+------------------------------------+------+
|          1 |    155190 |      7706 |            1 |      17.00 |        21168.23 |       0.04 |  0.02 | N            | O            | 1996-03-13 | 1996-02-12   | 1996-03-22    | DELIVER IN PERSON | TRUCK      | egular courts above the            |    1 |
|          1 |     67310 |      7311 |            2 |      36.00 |        45983.16 |       0.09 |  0.06 | N            | O            | 1996-04-12 | 1996-02-28   | 1996-04-20    | TAKE BACK RETURN  | MAIL       | ly final dependencies: slyly bold  |    1 |
|          1 |     63700 |      3701 |            3 |       8.00 |        13309.60 |       0.10 |  0.02 | N            | O            | 1996-01-29 | 1996-03-05   | 1996-01-31    | TAKE BACK RETURN  | REG AIR    | riously. regular, express dep      |    1 |
|          1 |      2132 |      4633 |            4 |      28.00 |        28955.64 |       0.09 |  0.06 | N            | O            | 1996-04-21 | 1996-03-30   | 1996-05-16    | NONE              | AIR        | lites. fluffily even de            |    1 |
|          1 |     24027 |      1534 |            5 |      24.00 |        22824.48 |       0.10 |  0.04 | N            | O            | 1996-03-30 | 1996-03-14   | 1996-04-01    | NONE              | FOB        |  pending foxes. slyly re           |    1 |
|          1 |     15635 |       638 |            6 |      32.00 |        49620.16 |       0.07 |  0.02 | N            | O            | 1996-01-30 | 1996-02-07   | 1996-02-03    | DELIVER IN PERSON | MAIL       | arefully slyly ex                  |    1 |
+------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+-------------------+------------+------------------------------------+------+
6 rows in set (0.41 sec)
"
1436,MCOL-579,MCOL,Andrew Hutchings,99695,2017-09-05 19:32:07,For QA: regression suite should still work after this change. We are improving memory/stack protection.,1,For QA: regression suite should still work after this change. We are improving memory/stack protection.
1437,MCOL-579,MCOL,Daniel Lee,99743,2017-09-06 21:33:03,"Build verified: 1.1.0 Github source

[root@localhost ~]# cat mariadb-columnstore-1.1.0-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 9e855a6415e0edd6771c449a6591c21c3915bfec
Merge: 6ed33d1 c206e51
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Sep 5 09:43:29 2017 -0500

    Merge pull request #68 from mariadb-corporation/MCOL-887
    
    MCOL-887 Merge MariaDB 10.2.8

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 4441206050b07986f31402652c3299d36007d78d
Merge: 90353b9 230d013
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Tue Sep 5 20:36:02 2017 +0100

    Merge pull request #247 from mariadb-corporation/MCOL-579
    
    Add compiler flag checks and hardening flags

Regression tests passed.
",2,"Build verified: 1.1.0 Github source

[root@localhost ~]# cat mariadb-columnstore-1.1.0-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 9e855a6415e0edd6771c449a6591c21c3915bfec
Merge: 6ed33d1 c206e51
Author: David.Hall 
Date:   Tue Sep 5 09:43:29 2017 -0500

    Merge pull request #68 from mariadb-corporation/MCOL-887
    
    MCOL-887 Merge MariaDB 10.2.8

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 4441206050b07986f31402652c3299d36007d78d
Merge: 90353b9 230d013
Author: Andrew Hutchings 
Date:   Tue Sep 5 20:36:02 2017 +0100

    Merge pull request #247 from mariadb-corporation/MCOL-579
    
    Add compiler flag checks and hardening flags

Regression tests passed.
"
1438,MCOL-59,MCOL,Dipti Joshi,83811,2016-05-31 06:40:19,[~hill]Is this change going to be available in Alpha version ?,1,[~hill]Is this change going to be available in Alpha version ?
1439,MCOL-59,MCOL,David Hill,83848,2016-05-31 13:13:10,"There are too many file changes (both OAM and DB files) and tool changes required for this to be done in Alpha timeframe.
Should be planned for Beta.",2,"There are too many file changes (both OAM and DB files) and tool changes required for this to be done in Alpha timeframe.
Should be planned for Beta."
1440,MCOL-59,MCOL,David Hill,84548,2016-06-24 19:42:13,"changes applied to repo engine, branch mcol-59.

under test now, ready for review",3,"changes applied to repo engine, branch mcol-59.

under test now, ready for review"
1441,MCOL-59,MCOL,David Hill,84549,2016-06-24 19:42:57,"code in mcol-59, repo engine

Changed all references of Calpont.xml to Columnstore.xml",4,"code in mcol-59, repo engine

Changed all references of Calpont.xml to Columnstore.xml"
1442,MCOL-593,MCOL,Andrew Hutchings,126316,2019-04-15 13:57:44,"Patch adds a MariaDB replication option which can be enabled using the following in the SystemConfig section of Columnstore.xml:

<ReplicationEnabled>Y</ReplicationEnabled>

For QA:
1. Create a MariaDB server with an InnoDB table
2. Create a ColumnStore 1UM setup with a ColumnStore table of the same/similar schema
3. Configure slave-ids and enable binlog on the MariaDB server
4. Use CHANGE MASTER TO on the ColumnStore UM to enable replication
5. Do a few DML statements.

There may be issues with multi-UM setups for now. Improvements for this will come later, this is why it is a hidden option for now.",1,"Patch adds a MariaDB replication option which can be enabled using the following in the SystemConfig section of Columnstore.xml:

Y

For QA:
1. Create a MariaDB server with an InnoDB table
2. Create a ColumnStore 1UM setup with a ColumnStore table of the same/similar schema
3. Configure slave-ids and enable binlog on the MariaDB server
4. Use CHANGE MASTER TO on the ColumnStore UM to enable replication
5. Do a few DML statements.

There may be issues with multi-UM setups for now. Improvements for this will come later, this is why it is a hidden option for now."
1443,MCOL-593,MCOL,Daniel Lee,126621,2019-04-22 19:11:49,"Build verified: 1.2.2-1 nightly

[root@localhost centos7]# cat gitversionInfo.txt 
server commit:
137b9a8
engine commit:
b3a7559

 1. installed MariaDB Server 10.3.14, set it as replication master
  2. created InnoDB table repdb.orders
  3. installed ColumnStore 1.2.4-1 (nightly build), set it as replication Slave
  4. created ColumnStore table repdb.orders
  5. created InnoDB tables in MariaDB server and verified tables were replicated to ColumnStore
  6. inserted rows into repdb.orders in MariaDB Server
  7. verified inserted row was not replicated (Expected, Columnstore.xml not yet setup)
  8. added <ReplicationEnabled>Y</ReplicationEnabled> and restarted ColumnStore
  9. inserted rows into repdb.orders in MariaDB Server
  10. verified inserted row was replicated

  After replication has been setup, tables can be setup as the following:

  1. stop replication slave in ColumnStore (stop slave)
  2. reset replication slave in ColumnStore (reset slave)
  3. create InnoDB table in MariaDB Server
  4. create ColumnStore table in ColumnStore
  5. ""CHANGE MASTER TO"" in ColumnStore
  6. start slave in ColumnStore (start slave)
  7. insert rows in master
  8. verify inserted rows in slave


Notes:

On Master

MariaDB [repdb]> CREATE USER 'replication_user'@'%' IDENTIFIED BY 'bigs3cret';
Query OK, 0 rows affected (0.000 sec)

MariaDB [repdb]> GRANT REPLICATION SLAVE ON *.* TO 'replication_user'@'%';
Query OK, 0 rows affected (0.001 sec)

MariaDB [repdb]> show master status;
+--------------------+----------+--------------+------------------+
| File               | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+--------------------+----------+--------------+------------------+
| master1-bin.000001 |     2452 |              |                  |
+--------------------+----------+--------------+------------------+
1 row in set (0.000 sec)


On Slave:

update /etc/hosts file

10.0.0.15 repmaster

MariaDB [repdb]> CHANGE MASTER TO
    ->   MASTER_HOST='repmaster',
    ->   MASTER_USER='replication_user',
    ->   MASTER_PASSWORD='bigs3cret',
    ->   MASTER_PORT=3306,
    ->   MASTER_LOG_FILE='master1-bin.000001',
    ->   MASTER_LOG_POS=2452,
    ->   MASTER_CONNECT_RETRY=10;
Query OK, 0 rows affected (0.023 sec)

MariaDB [repdb]> start slave;
Query OK, 0 rows affected (0.002 sec)

MariaDB [repdb]> show slave status\G;
*************************** 1. row ***************************
                Slave_IO_State: Waiting for master to send event
                   Master_Host: repmaster
                   Master_User: replication_user
                   Master_Port: 3306
                 Connect_Retry: 10
               Master_Log_File: master1-bin.000001
           Read_Master_Log_Pos: 2452
                Relay_Log_File: relay-bin.000002
                 Relay_Log_Pos: 557
         Relay_Master_Log_File: master1-bin.000001
              Slave_IO_Running: Yes
             Slave_SQL_Running: Yes
               Replicate_Do_DB: 
           Replicate_Ignore_DB: 
            Replicate_Do_Table: 
        Replicate_Ignore_Table: 
       Replicate_Wild_Do_Table: 
   Replicate_Wild_Ignore_Table: 
                    Last_Errno: 0
                    Last_Error: 
                  Skip_Counter: 0
           Exec_Master_Log_Pos: 2452
               Relay_Log_Space: 860
               Until_Condition: None
                Until_Log_File: 
                 Until_Log_Pos: 0
            Master_SSL_Allowed: No
            Master_SSL_CA_File: 
            Master_SSL_CA_Path: 
               Master_SSL_Cert: 
             Master_SSL_Cipher: 
                Master_SSL_Key: 
         Seconds_Behind_Master: 0
 Master_SSL_Verify_Server_Cert: No
                 Last_IO_Errno: 0
                 Last_IO_Error: 
                Last_SQL_Errno: 0
                Last_SQL_Error: 
   Replicate_Ignore_Server_Ids: 
              Master_Server_Id: 1
                Master_SSL_Crl: 
            Master_SSL_Crlpath: 
                    Using_Gtid: No
                   Gtid_IO_Pos: 
       Replicate_Do_Domain_Ids: 
   Replicate_Ignore_Domain_Ids: 
                 Parallel_Mode: conservative
                     SQL_Delay: 0
           SQL_Remaining_Delay: NULL
       Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it
              Slave_DDL_Groups: 1
Slave_Non_Transactional_Groups: 0
    Slave_Transactional_Groups: 1
1 row in set (0.001 sec)


",2,"Build verified: 1.2.2-1 nightly

[root@localhost centos7]# cat gitversionInfo.txt 
server commit:
137b9a8
engine commit:
b3a7559

 1. installed MariaDB Server 10.3.14, set it as replication master
  2. created InnoDB table repdb.orders
  3. installed ColumnStore 1.2.4-1 (nightly build), set it as replication Slave
  4. created ColumnStore table repdb.orders
  5. created InnoDB tables in MariaDB server and verified tables were replicated to ColumnStore
  6. inserted rows into repdb.orders in MariaDB Server
  7. verified inserted row was not replicated (Expected, Columnstore.xml not yet setup)
  8. added Y and restarted ColumnStore
  9. inserted rows into repdb.orders in MariaDB Server
  10. verified inserted row was replicated

  After replication has been setup, tables can be setup as the following:

  1. stop replication slave in ColumnStore (stop slave)
  2. reset replication slave in ColumnStore (reset slave)
  3. create InnoDB table in MariaDB Server
  4. create ColumnStore table in ColumnStore
  5. ""CHANGE MASTER TO"" in ColumnStore
  6. start slave in ColumnStore (start slave)
  7. insert rows in master
  8. verify inserted rows in slave


Notes:

On Master

MariaDB [repdb]> CREATE USER 'replication_user'@'%' IDENTIFIED BY 'bigs3cret';
Query OK, 0 rows affected (0.000 sec)

MariaDB [repdb]> GRANT REPLICATION SLAVE ON *.* TO 'replication_user'@'%';
Query OK, 0 rows affected (0.001 sec)

MariaDB [repdb]> show master status;
+--------------------+----------+--------------+------------------+
| File               | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+--------------------+----------+--------------+------------------+
| master1-bin.000001 |     2452 |              |                  |
+--------------------+----------+--------------+------------------+
1 row in set (0.000 sec)


On Slave:

update /etc/hosts file

10.0.0.15 repmaster

MariaDB [repdb]> CHANGE MASTER TO
    ->   MASTER_HOST='repmaster',
    ->   MASTER_USER='replication_user',
    ->   MASTER_PASSWORD='bigs3cret',
    ->   MASTER_PORT=3306,
    ->   MASTER_LOG_FILE='master1-bin.000001',
    ->   MASTER_LOG_POS=2452,
    ->   MASTER_CONNECT_RETRY=10;
Query OK, 0 rows affected (0.023 sec)

MariaDB [repdb]> start slave;
Query OK, 0 rows affected (0.002 sec)

MariaDB [repdb]> show slave status\G;
*************************** 1. row ***************************
                Slave_IO_State: Waiting for master to send event
                   Master_Host: repmaster
                   Master_User: replication_user
                   Master_Port: 3306
                 Connect_Retry: 10
               Master_Log_File: master1-bin.000001
           Read_Master_Log_Pos: 2452
                Relay_Log_File: relay-bin.000002
                 Relay_Log_Pos: 557
         Relay_Master_Log_File: master1-bin.000001
              Slave_IO_Running: Yes
             Slave_SQL_Running: Yes
               Replicate_Do_DB: 
           Replicate_Ignore_DB: 
            Replicate_Do_Table: 
        Replicate_Ignore_Table: 
       Replicate_Wild_Do_Table: 
   Replicate_Wild_Ignore_Table: 
                    Last_Errno: 0
                    Last_Error: 
                  Skip_Counter: 0
           Exec_Master_Log_Pos: 2452
               Relay_Log_Space: 860
               Until_Condition: None
                Until_Log_File: 
                 Until_Log_Pos: 0
            Master_SSL_Allowed: No
            Master_SSL_CA_File: 
            Master_SSL_CA_Path: 
               Master_SSL_Cert: 
             Master_SSL_Cipher: 
                Master_SSL_Key: 
         Seconds_Behind_Master: 0
 Master_SSL_Verify_Server_Cert: No
                 Last_IO_Errno: 0
                 Last_IO_Error: 
                Last_SQL_Errno: 0
                Last_SQL_Error: 
   Replicate_Ignore_Server_Ids: 
              Master_Server_Id: 1
                Master_SSL_Crl: 
            Master_SSL_Crlpath: 
                    Using_Gtid: No
                   Gtid_IO_Pos: 
       Replicate_Do_Domain_Ids: 
   Replicate_Ignore_Domain_Ids: 
                 Parallel_Mode: conservative
                     SQL_Delay: 0
           SQL_Remaining_Delay: NULL
       Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it
              Slave_DDL_Groups: 1
Slave_Non_Transactional_Groups: 0
    Slave_Transactional_Groups: 1
1 row in set (0.001 sec)


"
1444,MCOL-593,MCOL,YURII KANTONISTOV,142384,2020-01-20 12:03:49,"Tried this feature with MariaDB 10.3.21 innodb as a master and Columnstore 1.2.5(MariaDB 10.3.16) as a slave, simplest 1xUM+1xPM configuration.

Replication can be established - both master and slave has pre-created table:

master:
create table xxx (id int) ENGINE=InnoDB;

slave:
create table xxx (id int) ENGINE=ColumnStore;

""CHANGE MASTER TO/start slave/show slave status"" commands work smoothly, but when doing very first ""insert into xxx values (1);"" on the master - 
slave's mysqld process goes into crash-restart loop (see attached fies1hal08.tellabs.fi.log,zip):

...
2020-01-20 13:40:21 10 [Note] Slave I/O thread: Start asynchronous replication to master 'mycolumnrep@172.19.132.52:2048' in log '1.000002' at position 539
2020-01-20 13:40:21 11 [Note] Slave SQL thread initialized, starting replication in log '1.000002' at position 334, relay log '/usr/local/mariadb/columnstore/mysql/db/relay-bin.000002' position: 547
2020-01-20 13:40:21 0 [Note] /usr/local/mariadb/columnstore/mysql//bin/mysqld: ready for connections.
Version: '10.3.16-MariaDB-log'  socket: '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock'  port: 3306  Columnstore 1.2.5-1
2020-01-20 13:40:21 10 [Note] Slave I/O thread: connected to master 'mycolumnrep@172.19.132.52:2048',replication started in log '1.000002' at position 539
terminate called after throwing an instance of 'std::logic_error'
  what():  basic_string::_S_construct null not valid
200120 13:40:21 [ERROR] mysqld got signal 6 ;
...

Will appreciate any hint how to fix it or at least how to investigate it further.
",3,"Tried this feature with MariaDB 10.3.21 innodb as a master and Columnstore 1.2.5(MariaDB 10.3.16) as a slave, simplest 1xUM+1xPM configuration.

Replication can be established - both master and slave has pre-created table:

master:
create table xxx (id int) ENGINE=InnoDB;

slave:
create table xxx (id int) ENGINE=ColumnStore;

""CHANGE MASTER TO/start slave/show slave status"" commands work smoothly, but when doing very first ""insert into xxx values (1);"" on the master - 
slave's mysqld process goes into crash-restart loop (see attached fies1hal08.tellabs.fi.log,zip):

...
2020-01-20 13:40:21 10 [Note] Slave I/O thread: Start asynchronous replication to master 'mycolumnrep@172.19.132.52:2048' in log '1.000002' at position 539
2020-01-20 13:40:21 11 [Note] Slave SQL thread initialized, starting replication in log '1.000002' at position 334, relay log '/usr/local/mariadb/columnstore/mysql/db/relay-bin.000002' position: 547
2020-01-20 13:40:21 0 [Note] /usr/local/mariadb/columnstore/mysql//bin/mysqld: ready for connections.
Version: '10.3.16-MariaDB-log'  socket: '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock'  port: 3306  Columnstore 1.2.5-1
2020-01-20 13:40:21 10 [Note] Slave I/O thread: connected to master 'mycolumnrep@172.19.132.52:2048',replication started in log '1.000002' at position 539
terminate called after throwing an instance of 'std::logic_error'
  what():  basic_string::_S_construct null not valid
200120 13:40:21 [ERROR] mysqld got signal 6 ;
...

Will appreciate any hint how to fix it or at least how to investigate it further.
"
1445,MCOL-597,MCOL,David Hall,95021,2017-05-09 20:28:41,I put off asking for a review until the merge had settled down.,1,I put off asking for a review until the merge had settled down.
1446,MCOL-597,MCOL,Andrew Hutchings,95059,2017-05-10 11:08:24,Happy to put this to test. Any remaining issues are already know about and it seems to be working well so far.,2,Happy to put this to test. Any remaining issues are already know about and it seems to be working well so far.
1447,MCOL-597,MCOL,Daniel Lee,95613,2017-05-22 14:16:00,"Build verified: 1.1.0 GitHub source

[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Thu May 4 16:06:16 2017 -0500

    Merge pull request #50 from mariadb-corporation/10.2-fixes
    
    10.2 fixes

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 42ec73fa8c1f56fecf193f758fba2484f4c51e81
Merge: d3d2caa cfb3a10
Author: David Hill <david.hill@mariadb.com>
Date:   Thu May 18 09:28:20 2017 -0500

    Merge branch 'develop' of https://github.com/mariadb-corporation/mariadb-columnstore-engine into develop

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 12
Server version: 10.2.5-MariaDB-log Columnstore 1.1.0-1

Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.



",3,"Build verified: 1.1.0 GitHub source

[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 
Date:   Thu May 4 16:06:16 2017 -0500

    Merge pull request #50 from mariadb-corporation/10.2-fixes
    
    10.2 fixes

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit 42ec73fa8c1f56fecf193f758fba2484f4c51e81
Merge: d3d2caa cfb3a10
Author: David Hill 
Date:   Thu May 18 09:28:20 2017 -0500

    Merge branch 'develop' of URL into develop

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 12
Server version: 10.2.5-MariaDB-log Columnstore 1.1.0-1

Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.



"
1448,MCOL-598,MCOL,David Thompson,96478,2017-06-12 17:34:29,"I did some initial tests and it appears that non-recursive CTE's worked for the cases i tested. Testing a recursive CTE caused a mysqld crash, should verify that and capture backtrace to review and file a seperate bug.   

Please review and add some test cases to verify this for varying cases including views and use in store procs.",1,"I did some initial tests and it appears that non-recursive CTE's worked for the cases i tested. Testing a recursive CTE caused a mysqld crash, should verify that and capture backtrace to review and file a seperate bug.   

Please review and add some test cases to verify this for varying cases including views and use in store procs."
1449,MCOL-598,MCOL,Daniel Lee,96764,2017-06-21 22:54:16,"Build tested: Github source 1.1.0-1
[root@localhost mariadb-columnstore-server]# git show
commit 594ef1807a5d6cba45cf7c2bed03cccdc32f177a
Merge: a5f191d ce815f9
Author: David.Hall <david.hall@mariadb.com>
Date: Thu Jun 8 10:12:50 2017 -0500
[root@localhost mariadb-columnstore-engine]# git show
commit ebaf24473c0838989bf504a7c104c511b876fcb8
Author: david hill <david.hill@mariadb.com>
Date: Fri Jun 16 16:53:48 2017 -0500

Non-recursive CTE worked fine.

select * from nation n ,region r where n.n_regionkey = r.r_regionkey and r.r_regionkey in (with t as (select * from region where r_regionkey <=3) select r_regionkey from t where r_name <> ""ASIA""); 


Recursive CTE caused mysqld to crash.

create table folks (id int, name varchar(20), father int, mother int) engine=columnstore;
insert into folks values (100, 'Alex', 20, 30);
insert into folks values (20, 'Dad', 10, NULL);
insert into folks values (30, 'Mom', NULL, NULL);
insert into folks values (10, 'Grandpa', NULL, NULL);
insert into folks values (98, 'Sister Amy', 20, 30);
select * from folks;


with recursive ancestors as (
select * from folks
where name = 'Alex'
union
select f.*
from folks as f, ancestors as a
where
f.id = a.father or f.id = a.mother
)
select * from ancestors;

with recursive ancestors as ( select * from folks where name = 'Alex' union select f.* from folks as f, ancestors as a where f.id = a.father or f.id = a.mother ) select * from ancestors;

ERROR 2013 (HY000): Lost connection to MySQL server during query

content in /usr/local/mariadb/columnstore/mysql/db/localhost.localdomain.err

170621 22:52:02 mysqld_safe Number of processes running now: 0
170621 22:52:02 mysqld_safe mysqld restarted
2017-06-21 22:52:02 140282559596608 [Note] /usr/local/mariadb/columnstore/mysql//bin/mysqld (mysqld 10.2.6-MariaDB-log) starting as process 26696 ...
170621 22:52:02 Columnstore: Started; Version: 1.0.2-1
170621 22:52:02 InfiniDB: Started; Version: 1.0.2-1
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Uses event mutexes
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Compressed tables use zlib 1.2.7
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Using Linux native AIO
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Number of pools: 1
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Using SSE2 crc32 instructions
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Completed initialization of buffer pool
2017-06-21 22:52:02 140281482487552 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Highest supported file format is Barracuda.
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Starting crash recovery from checkpoint LSN=7231013722
InnoDB: Last MySQL binlog file position 0 1526, file name /usr/local/mariadb/columnstore/mysql/db/mysql-bin.000003
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: 128 out of 128 rollback segments are active.
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Creating shared tablespace for temporary tables
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Waiting for purge to start
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: 5.7.14 started; log sequence number 7231013731
2017-06-21 22:52:02 140281332619008 [Note] InnoDB: Loading buffer pool(s) from /usr/local/mariadb/columnstore/mysql/db/ib_buffer_pool
2017-06-21 22:52:02 140281332619008 [Note] InnoDB: Buffer pool(s) load completed at 170621 22:52:02
2017-06-21 22:52:02 140282559596608 [Note] Plugin 'FEEDBACK' is disabled.
2017-06-21 22:52:02 140282559596608 [Note] Recovering after a crash using /usr/local/mariadb/columnstore/mysql/db/mysql-bin
2017-06-21 22:52:02 140282559596608 [Note] Starting crash recovery...
2017-06-21 22:52:02 140282559596608 [Note] Crash recovery finished.
2017-06-21 22:52:02 140282559596608 [Note] Server socket created on IP: '::'.
2017-06-21 22:52:02 140282559596608 [Note] Reading of all Master_info entries succeded
2017-06-21 22:52:02 140282559596608 [Note] Added new Master_info '' to hash table
2017-06-21 22:52:02 140282559596608 [Note] /usr/local/mariadb/columnstore/mysql//bin/mysqld: ready for connections.
Version: '10.2.6-MariaDB-log'  socket: '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock'  port: 3306  Columnstore 1.1.0-1



",2,"Build tested: Github source 1.1.0-1
[root@localhost mariadb-columnstore-server]# git show
commit 594ef1807a5d6cba45cf7c2bed03cccdc32f177a
Merge: a5f191d ce815f9
Author: David.Hall 
Date: Thu Jun 8 10:12:50 2017 -0500
[root@localhost mariadb-columnstore-engine]# git show
commit ebaf24473c0838989bf504a7c104c511b876fcb8
Author: david hill 
Date: Fri Jun 16 16:53:48 2017 -0500

Non-recursive CTE worked fine.

select * from nation n ,region r where n.n_regionkey = r.r_regionkey and r.r_regionkey in (with t as (select * from region where r_regionkey  ""ASIA""); 


Recursive CTE caused mysqld to crash.

create table folks (id int, name varchar(20), father int, mother int) engine=columnstore;
insert into folks values (100, 'Alex', 20, 30);
insert into folks values (20, 'Dad', 10, NULL);
insert into folks values (30, 'Mom', NULL, NULL);
insert into folks values (10, 'Grandpa', NULL, NULL);
insert into folks values (98, 'Sister Amy', 20, 30);
select * from folks;


with recursive ancestors as (
select * from folks
where name = 'Alex'
union
select f.*
from folks as f, ancestors as a
where
f.id = a.father or f.id = a.mother
)
select * from ancestors;

with recursive ancestors as ( select * from folks where name = 'Alex' union select f.* from folks as f, ancestors as a where f.id = a.father or f.id = a.mother ) select * from ancestors;

ERROR 2013 (HY000): Lost connection to MySQL server during query

content in /usr/local/mariadb/columnstore/mysql/db/localhost.localdomain.err

170621 22:52:02 mysqld_safe Number of processes running now: 0
170621 22:52:02 mysqld_safe mysqld restarted
2017-06-21 22:52:02 140282559596608 [Note] /usr/local/mariadb/columnstore/mysql//bin/mysqld (mysqld 10.2.6-MariaDB-log) starting as process 26696 ...
170621 22:52:02 Columnstore: Started; Version: 1.0.2-1
170621 22:52:02 InfiniDB: Started; Version: 1.0.2-1
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Uses event mutexes
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Compressed tables use zlib 1.2.7
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Using Linux native AIO
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Number of pools: 1
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Using SSE2 crc32 instructions
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Completed initialization of buffer pool
2017-06-21 22:52:02 140281482487552 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Highest supported file format is Barracuda.
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Starting crash recovery from checkpoint LSN=7231013722
InnoDB: Last MySQL binlog file position 0 1526, file name /usr/local/mariadb/columnstore/mysql/db/mysql-bin.000003
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: 128 out of 128 rollback segments are active.
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Removed temporary tablespace data file: ""ibtmp1""
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Creating shared tablespace for temporary tables
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: Waiting for purge to start
2017-06-21 22:52:02 140282559596608 [Note] InnoDB: 5.7.14 started; log sequence number 7231013731
2017-06-21 22:52:02 140281332619008 [Note] InnoDB: Loading buffer pool(s) from /usr/local/mariadb/columnstore/mysql/db/ib_buffer_pool
2017-06-21 22:52:02 140281332619008 [Note] InnoDB: Buffer pool(s) load completed at 170621 22:52:02
2017-06-21 22:52:02 140282559596608 [Note] Plugin 'FEEDBACK' is disabled.
2017-06-21 22:52:02 140282559596608 [Note] Recovering after a crash using /usr/local/mariadb/columnstore/mysql/db/mysql-bin
2017-06-21 22:52:02 140282559596608 [Note] Starting crash recovery...
2017-06-21 22:52:02 140282559596608 [Note] Crash recovery finished.
2017-06-21 22:52:02 140282559596608 [Note] Server socket created on IP: '::'.
2017-06-21 22:52:02 140282559596608 [Note] Reading of all Master_info entries succeded
2017-06-21 22:52:02 140282559596608 [Note] Added new Master_info '' to hash table
2017-06-21 22:52:02 140282559596608 [Note] /usr/local/mariadb/columnstore/mysql//bin/mysqld: ready for connections.
Version: '10.2.6-MariaDB-log'  socket: '/usr/local/mariadb/columnstore/mysql/lib/mysql/mysql.sock'  port: 3306  Columnstore 1.1.0-1



"
1450,MCOL-598,MCOL,Daniel Lee,96766,2017-06-21 23:21:56,"Ticket MCOL-782 has been opened to track the issues found above, plus an issue for non-recursive CTE in views.
",3,"Ticket MCOL-782 has been opened to track the issues found above, plus an issue for non-recursive CTE in views.
"
1451,MCOL-598,MCOL,Daniel Lee,96801,2017-06-22 14:03:20,Also created MCOL-783 to track recursive CTE issue.  MCOL-782 has been modified to track non-recursive CTE issue only.,4,Also created MCOL-783 to track recursive CTE issue.  MCOL-782 has been modified to track non-recursive CTE issue only.
1452,MCOL-598,MCOL,Daniel Lee,96863,2017-06-23 21:40:04,Completed investigation for this ticket. Two tickets have been created and linked.,5,Completed investigation for this ticket. Two tickets have been created and linked.
1453,MCOL-599,MCOL,David Thompson,96481,2017-06-12 17:40:16,"These won't work as distributed functions since there is no implementation yet, but can you verify if these will work in the select list and also with text/blob?",1,"These won't work as distributed functions since there is no implementation yet, but can you verify if these will work in the select list and also with text/blob?"
1454,MCOL-599,MCOL,Daniel Lee,96748,2017-06-21 16:58:42,"Build tested: Github source 1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 594ef1807a5d6cba45cf7c2bed03cccdc32f177a
Merge: a5f191d ce815f9
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu Jun 8 10:12:50 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit ebaf24473c0838989bf504a7c104c511b876fcb8
Author: david hill <david.hill@mariadb.com>
Date:   Fri Jun 16 16:53:48 2017 -0500

1) According to the MariaDB's JSON KB articles, there are 26 JSON functions.  Syntax wise, they all worked in ColumnStore.

2) Nine (9) of the functions returned the ""Row size too large"" error when it is applied to a TEXT column.

3) One (1) function, JSON_REPLACE() failed complaining about a missing JSON_UPDATE() function.  This error occurred when the JSON_REPLACE is applied on ColumnStore columns (used varchar and text for testing).  JSON_UPDATE is not one of the supported function and I could not find information about it.  Maybe we need to inherit that function to ColumnStore since the same error did not occur in MariaDB.

4) I have created a test suite for JSON functions in Autopilot for automatic testing.

Function with non-zero diff count failed to match results from MariaDB:

0 Passed JSON_ARRAY_APPEND.sql.diff.log
0 Passed JSON_ARRAY_INSERT.sql.diff.log
0 Passed JSON_ARRAY.sql.diff.log
5 Failed JSON_COMPACT.sql.diff.log
0 Passed JSON_CONTAINS_PATH.sql.diff.log
0 Passed JSON_CONTAINS.sql.diff.log
0 Passed JSON_DEPTH.sql.diff.log
5 Failed JSON_DETAILED.sql.diff.log
0 Passed JSON_EXISTS.sql.diff.log
13 Failed JSON_EXTRACT.sql.diff.log
0 Passed JSON_INSERT.sql.diff.log
11 Failed JSON_KEYS.sql.diff.log
0 Passed JSON_LENGTH.sql.diff.log
5 Failed JSON_LOOSE.sql.diff.log
0 Passed JSON_MERGE.sql.diff.log
0 Passed JSON_OBJECT.sql.diff.log
6 Failed JSON_QUERY.sql.diff.log
0 Passed JSON_QUOTE.sql.diff.log
11 Failed JSON_REMOVE.sql.diff.log
5 Failed JSON_REPLACE.sql.diff.log
0 Passed JSON_SEARCH.sql.diff.log
0 Passed JSON_SET.sql.diff.log
0 Passed JSON_TYPE.sql.diff.log
18 Failed JSON_UNQUOTE.sql.diff.log
0 Passed JSON_VALID.sql.diff.log
6 Failed JSON_VALUE.sql.diff.log


Error message returned for failed functions:

MariaDB [mytest]> SELECT cVarchar, JSON_COMPACT(cVarchar), JSON_COMPACT(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_DETAILED(cVarchar), cText, JSON_DETAILED(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_EXTRACT(cVarchar, '$[1]'), cText, JSON_EXTRACT(cText, '$[1]') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_KEYS(cVarchar, '$.C'), cText, JSON_KEYS(cText, '$.C') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_LOOSE(cVarchar), cText, JSON_LOOSE(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_QUERY(cVarchar, '$.key1'), cText, JSON_QUERY(cText, '$.key1') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_REMOVE(cVarchar, '$.C'), cText, JSON_REMOVE(cText, '$.C') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_UNQUOTE(cVarchar), cText, JSON_UNQUOTE(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_VALUE(cVarchar, '$.key1'), cText, JSON_VALUE(cText, '$.key1') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs



MariaDB [mytest]> SELECT cVarchar, JSON_REPLACE(cVarchar, '$.B[1]', 4), cText, JSON_REPLACE(cText, '$.B[1]', 4) from jsontest;
ERROR 1305 (42000): FUNCTION mytest.json_update does not exist

",2,"Build tested: Github source 1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 594ef1807a5d6cba45cf7c2bed03cccdc32f177a
Merge: a5f191d ce815f9
Author: David.Hall 
Date:   Thu Jun 8 10:12:50 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit ebaf24473c0838989bf504a7c104c511b876fcb8
Author: david hill 
Date:   Fri Jun 16 16:53:48 2017 -0500

1) According to the MariaDB's JSON KB articles, there are 26 JSON functions.  Syntax wise, they all worked in ColumnStore.

2) Nine (9) of the functions returned the ""Row size too large"" error when it is applied to a TEXT column.

3) One (1) function, JSON_REPLACE() failed complaining about a missing JSON_UPDATE() function.  This error occurred when the JSON_REPLACE is applied on ColumnStore columns (used varchar and text for testing).  JSON_UPDATE is not one of the supported function and I could not find information about it.  Maybe we need to inherit that function to ColumnStore since the same error did not occur in MariaDB.

4) I have created a test suite for JSON functions in Autopilot for automatic testing.

Function with non-zero diff count failed to match results from MariaDB:

0 Passed JSON_ARRAY_APPEND.sql.diff.log
0 Passed JSON_ARRAY_INSERT.sql.diff.log
0 Passed JSON_ARRAY.sql.diff.log
5 Failed JSON_COMPACT.sql.diff.log
0 Passed JSON_CONTAINS_PATH.sql.diff.log
0 Passed JSON_CONTAINS.sql.diff.log
0 Passed JSON_DEPTH.sql.diff.log
5 Failed JSON_DETAILED.sql.diff.log
0 Passed JSON_EXISTS.sql.diff.log
13 Failed JSON_EXTRACT.sql.diff.log
0 Passed JSON_INSERT.sql.diff.log
11 Failed JSON_KEYS.sql.diff.log
0 Passed JSON_LENGTH.sql.diff.log
5 Failed JSON_LOOSE.sql.diff.log
0 Passed JSON_MERGE.sql.diff.log
0 Passed JSON_OBJECT.sql.diff.log
6 Failed JSON_QUERY.sql.diff.log
0 Passed JSON_QUOTE.sql.diff.log
11 Failed JSON_REMOVE.sql.diff.log
5 Failed JSON_REPLACE.sql.diff.log
0 Passed JSON_SEARCH.sql.diff.log
0 Passed JSON_SET.sql.diff.log
0 Passed JSON_TYPE.sql.diff.log
18 Failed JSON_UNQUOTE.sql.diff.log
0 Passed JSON_VALID.sql.diff.log
6 Failed JSON_VALUE.sql.diff.log


Error message returned for failed functions:

MariaDB [mytest]> SELECT cVarchar, JSON_COMPACT(cVarchar), JSON_COMPACT(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_DETAILED(cVarchar), cText, JSON_DETAILED(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_EXTRACT(cVarchar, '$[1]'), cText, JSON_EXTRACT(cText, '$[1]') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_KEYS(cVarchar, '$.C'), cText, JSON_KEYS(cText, '$.C') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_LOOSE(cVarchar), cText, JSON_LOOSE(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_QUERY(cVarchar, '$.key1'), cText, JSON_QUERY(cText, '$.key1') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_REMOVE(cVarchar, '$.C'), cText, JSON_REMOVE(cText, '$.C') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_UNQUOTE(cVarchar), cText, JSON_UNQUOTE(cText) from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs

MariaDB [mytest]> SELECT cVarchar, JSON_VALUE(cVarchar, '$.key1'), cText, JSON_VALUE(cText, '$.key1') from jsontest;
ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs



MariaDB [mytest]> SELECT cVarchar, JSON_REPLACE(cVarchar, '$.B[1]', 4), cText, JSON_REPLACE(cText, '$.B[1]', 4) from jsontest;
ERROR 1305 (42000): FUNCTION mytest.json_update does not exist

"
1455,MCOL-599,MCOL,David Thompson,96759,2017-06-21 19:06:39,MCOL-713 is the likely cause of the row size too larger error.,3,MCOL-713 is the likely cause of the row size too larger error.
1456,MCOL-599,MCOL,David Thompson,96760,2017-06-21 19:06:48,"we should look into the missing json_update function, could be a merge issue.",4,"we should look into the missing json_update function, could be a merge issue."
1457,MCOL-599,MCOL,Daniel Lee,96862,2017-06-23 21:38:29,Completed investigation for this ticket.  Two tickets have been created and linked.,5,Completed investigation for this ticket.  Two tickets have been created and linked.
1458,MCOL-6,MCOL,Dipti Joshi,83369,2016-05-11 16:35:26,"The update to Quick Start Guide has been started by Dipti, completion is pending the task MCOL-3, so that the guide can be updated with changed directories and paths.",1,"The update to Quick Start Guide has been started by Dipti, completion is pending the task MCOL-3, so that the guide can be updated with changed directories and paths."
1459,MCOL-6,MCOL,David Hill,83477,2016-05-17 22:25:35,quick start guide completed and in atatchments,2,quick start guide completed and in atatchments
1460,MCOL-6,MCOL,David Hill,83478,2016-05-17 22:45:47,reopening to assign to dipti for review,3,reopening to assign to dipti for review
1461,MCOL-6,MCOL,Dipti Joshi,83541,2016-05-20 04:12:16,"[~greenman] Please see the attached Quick Start Guide written by David Hill - This one is completely written here at MariaDB Corp and have all the changes for MariaDB ColumnStore path/configuration changes. Please use this to put in KB as ""Getting Started""  chapter under ColumnStore Engine
",4,"[~greenman] Please see the attached Quick Start Guide written by David Hill - This one is completely written here at MariaDB Corp and have all the changes for MariaDB ColumnStore path/configuration changes. Please use this to put in KB as ""Getting Started""  chapter under ColumnStore Engine
"
1462,MCOL-6,MCOL,Dipti Joshi,84001,2016-06-04 01:04:41,This is now in review for alpha and can be closed.,5,This is now in review for alpha and can be closed.
1463,MCOL-601,MCOL,David Thompson,96479,2017-06-12 17:35:45,Can you review / verify these and add new test cases or bugs as appropriate.,1,Can you review / verify these and add new test cases or bugs as appropriate.
1464,MCOL-601,MCOL,Daniel Lee,96859,2017-06-23 18:30:11,"Build tested: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit f126f3b019cea81e3c363db47cb23aeaac1e2ef9
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jun 19 15:28:24 2017 -0500

Tested again.  The “CREATE USER”, “ALTER USER”, “SHOW CREATE USER” commands worked fine with ColumnStore.  I ran into issue only when I tried SSL connections.

As a requirement for a new user, the new user must have privileges to the infinidb_vtable database.

I believe these three commands affect only the front end.  Developer should add their opinions whether or not any modification is needed in the ColumnStore engine to support these three commands.

I did run into an issue when I tried to setup SSL connection on the server.  I have issue connecting from a client.  Most likely, I did not have all the pieces put together for secured connection to work.  This should not affect the scope of purpose of this ticket.

",2,"Build tested: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill 
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit f126f3b019cea81e3c363db47cb23aeaac1e2ef9
Author: david hill 
Date:   Mon Jun 19 15:28:24 2017 -0500

Tested again.  The “CREATE USER”, “ALTER USER”, “SHOW CREATE USER” commands worked fine with ColumnStore.  I ran into issue only when I tried SSL connections.

As a requirement for a new user, the new user must have privileges to the infinidb_vtable database.

I believe these three commands affect only the front end.  Developer should add their opinions whether or not any modification is needed in the ColumnStore engine to support these three commands.

I did run into an issue when I tried to setup SSL connection on the server.  I have issue connecting from a client.  Most likely, I did not have all the pieces put together for secured connection to work.  This should not affect the scope of purpose of this ticket.

"
1465,MCOL-601,MCOL,Daniel Lee,96864,2017-06-23 21:41:01,Completed investigation for this ticket.,3,Completed investigation for this ticket.
1466,MCOL-609,MCOL,Andrew Hutchings,93034,2017-03-15 11:29:06,Merge conflicts resolved. Regression suite passed.,1,Merge conflicts resolved. Regression suite passed.
1467,MCOL-609,MCOL,Daniel Lee,93150,2017-03-17 19:08:10,"Build verified: 1.0.8-1

",2,"Build verified: 1.0.8-1

"
1468,MCOL-61,MCOL,David Hill,83521,2016-05-19 19:49:52,"Also look at using binary package install instead of RPMS, which we used in InfiniDB  and currently using in columnstore testing. There might be some benifits in using a binary package instead of rpms, like faster addmodule commands.",1,"Also look at using binary package install instead of RPMS, which we used in InfiniDB  and currently using in columnstore testing. There might be some benifits in using a binary package instead of rpms, like faster addmodule commands."
1469,MCOL-61,MCOL,David Hill,83522,2016-05-19 19:51:45,README file has been update in latest test version..,2,README file has been update in latest test version..
1470,MCOL-61,MCOL,David Hill,83995,2016-06-03 22:45:55,"Created Private AMI for alpha testing and it only exist in the us-west-2 region.

When ready, i can copy it out to other regions...

NAME

MariaDB-Columnstore-1.0.0",3,"Created Private AMI for alpha testing and it only exist in the us-west-2 region.

When ready, i can copy it out to other regions...

NAME

MariaDB-Columnstore-1.0.0"
1471,MCOL-61,MCOL,David Hill,87945,2016-11-01 13:59:47,"We do have a CS 1.0.4 Amazon AMI that is private at this time that both Daniel and I have used to setup amazon systems... 

So this probably needs to be reviewed as far as the README file and then have some installation documents that go along with it...",4,"We do have a CS 1.0.4 Amazon AMI that is private at this time that both Daniel and I have used to setup amazon systems... 

So this probably needs to be reviewed as far as the README file and then have some installation documents that go along with it..."
1472,MCOL-61,MCOL,David Hill,87946,2016-11-01 14:01:44,One change that needs to be made is you access as root user. This needs to be changed for security reasons... So there is some work to be done arund that,5,One change that needs to be made is you access as root user. This needs to be changed for security reasons... So there is some work to be done arund that
1473,MCOL-61,MCOL,David Hill,88163,2016-11-07 22:08:36,"doing a centos 7 version of the amazon AMI will require changes to IDBInstanceCMds.sh and IDBVolumeCmds.sh... The names will change also
",6,"doing a centos 7 version of the amazon AMI will require changes to IDBInstanceCMds.sh and IDBVolumeCmds.sh... The names will change also
"
1474,MCOL-61,MCOL,David Hill,88305,2016-11-14 15:05:12,"arg, this has not been going well...

1. Amazon provides instances with the need amazon tool apis in already built in them, but they are based on centos 6. We wanted to do a centos 7, so I had to build one from scatch that would work. I got that done...
2. we want to setup an ami without root login, so I setup an user account called mariadb that the user would log into. But when I tried to then access root and do the install, it,o f course, failed because I don't have root access to other nodes
3. Then thought it would be best to do a non-root install, but have a few days, come to the conclusion that 1.0.5 non-root install on centos-7 doesnt work. I opened a jira on that.

So going back to do the root install. Will setup a temporary root log access during the install to complete the install, then will disable the root login access. And at this point, the user will be required to setup ssh-keys.",7,"arg, this has not been going well...

1. Amazon provides instances with the need amazon tool apis in already built in them, but they are based on centos 6. We wanted to do a centos 7, so I had to build one from scatch that would work. I got that done...
2. we want to setup an ami without root login, so I setup an user account called mariadb that the user would log into. But when I tried to then access root and do the install, it,o f course, failed because I don't have root access to other nodes
3. Then thought it would be best to do a non-root install, but have a few days, come to the conclusion that 1.0.5 non-root install on centos-7 doesnt work. I opened a jira on that.

So going back to do the root install. Will setup a temporary root log access during the install to complete the install, then will disable the root login access. And at this point, the user will be required to setup ssh-keys."
1475,MCOL-61,MCOL,David Hill,88540,2016-11-20 23:52:01,"development branch has been updated with fixes for the non-root multi-node install.
Some came in during the merge of mcol-61 branch and others were made after the fact..
commit 9ce117f521cf8335ebcbc17ddd1b2445b42f8c2a
Author: David Hill <david.hill@mariadb.com>
Date: Sun Nov 20 22:47:54 2016 +0000
MCOL-61, ami install
oam/oamcpp/liboamcpp.cpp | 24 ++++-
oamapps/postConfigure/postConfigure.cpp | 165 ++++++++++++++++++---------------
2 files changed, 109 insertions, 80 deletions
commit 591ff9326fecada2f9e299ef5fa013a875749592
Author: David Hill <david.hill@mariadb.com>
Date: Sun Nov 20 17:52:57 2016 +0000
mcol-404
oamapps/postConfigure/postConfigure.cpp | 37 +++++++++++++++-------------------
1 file changed, 16 insertions, 21 deletions
commit 02a5624cb34674d1fc1f526f574c191187438c36
Author: David Hill <david.hill@mariadb.com>
Date: Sun Nov 20 16:29:42 2016 +0000
MCOL-404 AND MCOL-61
oam/install_scripts/master-rep-columnstore.sh | 7 ++-
oam/install_scripts/slave-rep-columnstore.sh | 7 ++-
oamapps/postConfigure/postConfigure.cpp | 72 +++++++++-------------------
3 files changed, 32 insertions, 54 deletions
commit 90706f1c93cb86604ecd6c1d5a3e8ef917d99ed5
Merge: 90ea3e2 3affcd4
Author: David Hill <david.hill@mariadb.com>
Date: Sun Nov 20 01:42:29 2016 +0000
Merge branch 'mcol-61' into develop",8,"development branch has been updated with fixes for the non-root multi-node install.
Some came in during the merge of mcol-61 branch and others were made after the fact..
commit 9ce117f521cf8335ebcbc17ddd1b2445b42f8c2a
Author: David Hill 
Date: Sun Nov 20 22:47:54 2016 +0000
MCOL-61, ami install
oam/oamcpp/liboamcpp.cpp | 24 ++++-
oamapps/postConfigure/postConfigure.cpp | 165 ++++++++++++++++++---------------
2 files changed, 109 insertions, 80 deletions
commit 591ff9326fecada2f9e299ef5fa013a875749592
Author: David Hill 
Date: Sun Nov 20 17:52:57 2016 +0000
mcol-404
oamapps/postConfigure/postConfigure.cpp | 37 +++++++++++++++-------------------
1 file changed, 16 insertions, 21 deletions
commit 02a5624cb34674d1fc1f526f574c191187438c36
Author: David Hill 
Date: Sun Nov 20 16:29:42 2016 +0000
MCOL-404 AND MCOL-61
oam/install_scripts/master-rep-columnstore.sh | 7 ++-
oam/install_scripts/slave-rep-columnstore.sh | 7 ++-
oamapps/postConfigure/postConfigure.cpp | 72 +++++++++-------------------
3 files changed, 32 insertions, 54 deletions
commit 90706f1c93cb86604ecd6c1d5a3e8ef917d99ed5
Merge: 90ea3e2 3affcd4
Author: David Hill 
Date: Sun Nov 20 01:42:29 2016 +0000
Merge branch 'mcol-61' into develop"
1476,MCOL-61,MCOL,David Thompson,88803,2016-11-29 00:23:33,An initial version has been produced and is in early beta test with a beta customer.,9,An initial version has been produced and is in early beta test with a beta customer.
1477,MCOL-610,MCOL,Daniel Lee,93336,2017-03-22 19:47:22,"I reproduced the reported issue when upgrading from 1.0.4-1 to 1.0.6-1.  After chatted with developer, we decided that it needs further investigation as the issue has observed since the fix.

My understand is that the messages were outputted incorrectly.  The developer should provide a brief explanation on the situation so that others, myself included, would not think we are just suppressing the messages and the root cause of the issue was not fixed.
",1,"I reproduced the reported issue when upgrading from 1.0.4-1 to 1.0.6-1.  After chatted with developer, we decided that it needs further investigation as the issue has observed since the fix.

My understand is that the messages were outputted incorrectly.  The developer should provide a brief explanation on the situation so that others, myself included, would not think we are just suppressing the messages and the root cause of the issue was not fixed.
"
1478,MCOL-610,MCOL,Daniel Lee,93348,2017-03-22 22:41:41,"Build tested: 1.0.8-1
mcsadmin> getsoftware
getsoftwareinfo Wed Mar 22 21:53:39 2017
Name : mariadb-columnstore-platform
Version : 1.0.8
Release : 1
Architecture: x86_64
Install Date: Wed 22 Mar 2017 09:37:39 PM UTC
Group : Applications/Databases
Size : 10012539
License : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature : (none)
Source RPM : mariadb-columnstore-platform-1.0.8-1.src.rpm
Build Date : Wed 22 Mar 2017 08:41:24 PM UTC

Have not seen the reported messages in a new installation.

The bug description talks about an upgrade.  I don't know if it is an issue with the upgrade process.  I will retest when the ticket is ready for QA again.",2,"Build tested: 1.0.8-1
mcsadmin> getsoftware
getsoftwareinfo Wed Mar 22 21:53:39 2017
Name : mariadb-columnstore-platform
Version : 1.0.8
Release : 1
Architecture: x86_64
Install Date: Wed 22 Mar 2017 09:37:39 PM UTC
Group : Applications/Databases
Size : 10012539
License : Copyright (c) 2016 MariaDB Corporation Ab., all rights reserved; redistributable under the terms of the GPL, see the file COPYING for details.
Signature : (none)
Source RPM : mariadb-columnstore-platform-1.0.8-1.src.rpm
Build Date : Wed 22 Mar 2017 08:41:24 PM UTC

Have not seen the reported messages in a new installation.

The bug description talks about an upgrade.  I don't know if it is an issue with the upgrade process.  I will retest when the ticket is ready for QA again."
1479,MCOL-610,MCOL,Daniel Lee,93388,2017-03-23 15:35:52,New installation of 1.0.6-1 also had the same issue.  The issue was not related to the upgrade process.  Closed per my test result in the last messages.,3,New installation of 1.0.6-1 also had the same issue.  The issue was not related to the upgrade process.  Closed per my test result in the last messages.
1480,MCOL-611,MCOL,Andrew Hutchings,92720,2017-03-07 21:32:04,Backport of MCOL-353 to 1.0 pending review,1,Backport of MCOL-353 to 1.0 pending review
1481,MCOL-611,MCOL,Daniel Lee,93151,2017-03-17 19:26:10,"Build verified: 1.0.8-1

Tested on a 1um4pm configuration.

[root@localhost ~]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1c columnstore 1g
[root@localhost ~]# time /usr/local/mariadb/columnstore/bin/cpimport tpch1c lineitem /data/qa/source/dbt3/10g/lineitem.tbl 
2017-03-17 19:20:39 (8791) INFO : Running distributed import (mode 1) on all PMs...
2017-03-17 19:24:12 (8791) INFO : For table tpch1c.lineitem: 59986052 rows processed and 59986052 rows inserted.
2017-03-17 19:24:12 (8791) INFO : Bulk load completed, total run time : 213.948 seconds

real	3m34.530s
user	0m15.634s
sys	0m18.734s

",2,"Build verified: 1.0.8-1

Tested on a 1um4pm configuration.

[root@localhost ~]# /data/qa/autopilot/databases/dbt3/sh/buildDatabase.sh tpch1c columnstore 1g
[root@localhost ~]# time /usr/local/mariadb/columnstore/bin/cpimport tpch1c lineitem /data/qa/source/dbt3/10g/lineitem.tbl 
2017-03-17 19:20:39 (8791) INFO : Running distributed import (mode 1) on all PMs...
2017-03-17 19:24:12 (8791) INFO : For table tpch1c.lineitem: 59986052 rows processed and 59986052 rows inserted.
2017-03-17 19:24:12 (8791) INFO : Bulk load completed, total run time : 213.948 seconds

real	3m34.530s
user	0m15.634s
sys	0m18.734s

"
1482,MCOL-613,MCOL,David Hill,92833,2017-03-09 18:59:23,additional changes need to be made based on review input,1,additional changes need to be made based on review input
1483,MCOL-613,MCOL,Ben Thompson,92845,2017-03-09 21:19:30,Reviewed and Merged.,2,Reviewed and Merged.
1484,MCOL-62,MCOL,Dipti Joshi,83548,2016-05-20 14:46:30,"Test script fail log is 
[root@srvss2 srvswdev11]# cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log
diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
—
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB


[~hill] The script is failing because the reference log is using MySQL error number 138, where as MariaDB ColumnStore uses error number from MariaDB Server 1178 - other wise essentially the error text is the same message.
Please update the reference log file to use ""ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB."" instead of :ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB""
Once reference log file is updated rerun the test and if it passes close this Jira item.",1,"Test script fail log is 
[root@srvss2 srvswdev11]# cat test012/logs/diff.txt 
logs/createn.sql.log does not match logs/createn.sql.ref.log
logs/create.sql.log does not match logs/create.sql.ref.log
diff test012/logs/createn.sql.log test012/logs/createn.sql.ref.log
1c1
< ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB.
—
> ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB


[~hill] The script is failing because the reference log is using MySQL error number 138, where as MariaDB ColumnStore uses error number from MariaDB Server 1178 - other wise essentially the error text is the same message.
Please update the reference log file to use ""ERROR 1178 (42000) at line 3: The storage engine for the table doesn't support Varbinary is currently not supported by InfiniDB."" instead of :ERROR 138 (HY000) at line 3: Varbinary is currently not supported by InfiniDB""
Once reference log file is updated rerun the test and if it passes close this Jira item."
1485,MCOL-62,MCOL,David Hill,83560,2016-05-20 18:33:59,ref logs changed,2,ref logs changed
1486,MCOL-636,MCOL,Andrew Hutchings,97763,2017-07-18 13:56:39,"Pull request for 1.1 open.

For QA... I guess just make sure it doesn't break anything. Regression suite ran fine in my tests. VARCHAR/CHAR return performance between the plugin and MariaDB should be a little better.",1,"Pull request for 1.1 open.

For QA... I guess just make sure it doesn't break anything. Regression suite ran fine in my tests. VARCHAR/CHAR return performance between the plugin and MariaDB should be a little better."
1487,MCOL-636,MCOL,Daniel Lee,99693,2017-09-05 19:13:38,"Build verified: 1.1.0 Github source

/root/columnstore/mariadb-columnstore-server
commit 9e855a6415e0edd6771c449a6591c21c3915bfec
Merge: 6ed33d1 c206e51
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Sep 5 09:43:29 2017 -0500

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 90353b9b908e1c9ee241c4a156a2a377c53cc807
Author: david hill <david.hill@mariadb.com>
Date:   Fri Sep 1 14:46:07 2017 -0500

Not noticeable timing difference.  On a 10g dbt3 database, I ran:

select max(l_comment) from lineitem:

1.0.10-1: 12.41 sec
1.1.0-1: 12.37 sec
",2,"Build verified: 1.1.0 Github source

/root/columnstore/mariadb-columnstore-server
commit 9e855a6415e0edd6771c449a6591c21c3915bfec
Merge: 6ed33d1 c206e51
Author: David.Hall 
Date:   Tue Sep 5 09:43:29 2017 -0500

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 90353b9b908e1c9ee241c4a156a2a377c53cc807
Author: david hill 
Date:   Fri Sep 1 14:46:07 2017 -0500

Not noticeable timing difference.  On a 10g dbt3 database, I ran:

select max(l_comment) from lineitem:

1.0.10-1: 12.41 sec
1.1.0-1: 12.37 sec
"
1488,MCOL-638,MCOL,David Hill,93462,2017-03-24 23:07:13,"package is created, testing scenario out now...

user would download the boost 1.55 library package from out FTP site",1,"package is created, testing scenario out now...

user would download the boost 1.55 library package from out FTP site"
1489,MCOL-638,MCOL,David Hill,93511,2017-03-27 13:44:56,plan is to make to downloaded from the web page,2,plan is to make to downloaded from the web page
1490,MCOL-638,MCOL,David Thompson,94027,2017-04-14 20:09:56,This has been made available on the download site and documented here: https://mariadb.com/kb/en/mariadb/preparing-for-columnstore-installation/,3,This has been made available on the download site and documented here: URL
1491,MCOL-641,MCOL,Andrew Hutchings,103663,2017-11-27 14:12:35,As a first version this should be implemented using dictionary columns rather than fixed length when more than 18 digits are required.,1,As a first version this should be implemented using dictionary columns rather than fixed length when more than 18 digits are required.
1492,MCOL-641,MCOL,Lohit Marla,124792,2019-03-14 12:30:39,"Hello,  my name is lohit.  Looking forward  to be part of this community ",2,"Hello,  my name is lohit.  Looking forward  to be part of this community "
1493,MCOL-641,MCOL,Vicențiu Ciorbaru,125315,2019-03-25 16:50:59,"Hi! [~Lohitmarla]

If you are interested in contributing this as part of GSoC 2019, I suggest you send an email to maria-developers mailing list
https://launchpad.net/~maria-developers and ask for more guidance. You are guaranteed to have more people notice your messages there and also plenty of folks willing to help you out.
",3,"Hi! [~Lohitmarla]

If you are interested in contributing this as part of GSoC 2019, I suggest you send an email to maria-developers mailing list
URL and ask for more guidance. You are guaranteed to have more people notice your messages there and also plenty of folks willing to help you out.
"
1494,MCOL-641,MCOL,Gagan Goel,158118,2020-06-27 00:22:30,The functional tests for FLOOR/CEIL distributed functions need to be updated once MDEV-23032 is fixed.,4,The functional tests for FLOOR/CEIL distributed functions need to be updated once MDEV-23032 is fixed.
1495,MCOL-642,MCOL,Andrew Hutchings,93534,2017-03-27 20:49:31,"* TEXT and BLOB now have separate identifiers internally
* TEXT columns are identified as such in system catalog
* cpimport only requires hex input for BLOB, not TEXT

For testing the existing BLOB test case covers this. You can see TEXT columns in editem and other places that probe the extent map.",1,"* TEXT and BLOB now have separate identifiers internally
* TEXT columns are identified as such in system catalog
* cpimport only requires hex input for BLOB, not TEXT

For testing the existing BLOB test case covers this. You can see TEXT columns in editem and other places that probe the extent map."
1496,MCOL-642,MCOL,Daniel Lee,94630,2017-05-02 16:22:55,"Build verified:

[root@localhost mariadb-columnstore-server]# git show
commit 5a090c64bced6532578dd8910f15530fd37fce2c
Merge: 9efb0a7 b062156
Author: david hill <david.hill@mariadb.com>
Date:   Thu Apr 27 16:13:15 2017 -0500

    Merge pull request #45 from mariadb-corporation/FixPackageName
    
    Change the package name to match engines format

[root@localhost mariadb-columnstore-engine]# git show
commit 715a514f155338c616d8408e5a7e7edc6cadba2f
Merge: 6320a59 f6caa88
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Apr 27 17:38:57 2017 +0100

    Merge branch 'MCOL-597' into develop

MariaDB [mytest]> create table t1 (c1 int, c2 text, b3 blob, c4 longtext, c5 longblob) engine=columnstore;
Query OK, 0 rows affected (0.19 sec)

MariaDB [calpontsys]> select columnname, datatype from syscolumn where tablename='t1';
+------------+----------+
| columnname | datatype |
+------------+----------+
| c1         |        6 |
| c2         |       24 |
| b3         |       15 |
| c4         |       24 |
| c5         |       15 |
+------------+----------+
5 rows in set (0.01 sec)",2,"Build verified:

[root@localhost mariadb-columnstore-server]# git show
commit 5a090c64bced6532578dd8910f15530fd37fce2c
Merge: 9efb0a7 b062156
Author: david hill 
Date:   Thu Apr 27 16:13:15 2017 -0500

    Merge pull request #45 from mariadb-corporation/FixPackageName
    
    Change the package name to match engines format

[root@localhost mariadb-columnstore-engine]# git show
commit 715a514f155338c616d8408e5a7e7edc6cadba2f
Merge: 6320a59 f6caa88
Author: Andrew Hutchings 
Date:   Thu Apr 27 17:38:57 2017 +0100

    Merge branch 'MCOL-597' into develop

MariaDB [mytest]> create table t1 (c1 int, c2 text, b3 blob, c4 longtext, c5 longblob) engine=columnstore;
Query OK, 0 rows affected (0.19 sec)

MariaDB [calpontsys]> select columnname, datatype from syscolumn where tablename='t1';
+------------+----------+
| columnname | datatype |
+------------+----------+
| c1         |        6 |
| c2         |       24 |
| b3         |       15 |
| c4         |       24 |
| c5         |       15 |
+------------+----------+
5 rows in set (0.01 sec)"
1497,MCOL-657,MCOL,David Thompson,94017,2017-04-13 02:36:15,This is a query pattern that tableau will generate for the mysql connector so is hard to workaround for tableau users.,1,This is a query pattern that tableau will generate for the mysql connector so is hard to workaround for tableau users.
1498,MCOL-657,MCOL,Andrew Hutchings,94975,2017-05-09 11:09:29,"Four pull requests, two for engine (develop, develop-1.0), two for regression suite (master, 1.0).

Test case:
{code:sql}
create table if not exists MCOL657 (a int, b int) engine=columnstore;
insert into MCOL657 values (1,1), (2,2), (3,4), (NULL, NULL), (NULL, 2), (3, NULL);
select * from MCOL657 where a<=>b;
{code}

Should return:
{code}
a	b
1	1
2	2
NULL	NULL
{code}",2,"Four pull requests, two for engine (develop, develop-1.0), two for regression suite (master, 1.0).

Test case:
{code:sql}
create table if not exists MCOL657 (a int, b int) engine=columnstore;
insert into MCOL657 values (1,1), (2,2), (3,4), (NULL, NULL), (NULL, 2), (3, NULL);
select * from MCOL657 where ab;
{code}

Should return:
{code}
a	b
1	1
2	2
NULL	NULL
{code}"
1499,MCOL-657,MCOL,Daniel Lee,95151,2017-05-11 16:20:55,"Build verified: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Thu May 4 16:06:16 2017 -0500

    Merge pull request #50 from mariadb-corporation/10.2-fixes
    
    10.2 fixes

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit b4883e6abd18fb63c4af450679c2720cdb537dad
Merge: 85a5eaa 83331f4
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Thu May 11 09:43:28 2017 -0500

    Merge pull request #182 from mariadb-corporation/MCOL-712
    
    MCOL-712 Support TEXT for GROUP BY


",3,"Build verified: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 
Date:   Thu May 4 16:06:16 2017 -0500

    Merge pull request #50 from mariadb-corporation/10.2-fixes
    
    10.2 fixes

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit b4883e6abd18fb63c4af450679c2720cdb537dad
Merge: 85a5eaa 83331f4
Author: dhall-InfiniDB 
Date:   Thu May 11 09:43:28 2017 -0500

    Merge pull request #182 from mariadb-corporation/MCOL-712
    
    MCOL-712 Support TEXT for GROUP BY


"
1500,MCOL-657,MCOL,Daniel Lee,95152,2017-05-11 16:22:27,"Build verified: 1.0.9-1

Repeat test case.",4,"Build verified: 1.0.9-1

Repeat test case."
1501,MCOL-664,MCOL,Daniel Lee,95158,2017-05-11 17:13:34,"Build verified: github source 1.1.0

Build verified: Github source 1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 <ben.thompson@mariadb.com>
Date:   Thu May 4 16:06:16 2017 -0500

    Merge pull request #50 from mariadb-corporation/10.2-fixes
    
    10.2 fixes

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit b4883e6abd18fb63c4af450679c2720cdb537dad
Merge: 85a5eaa 83331f4
Author: dhall-InfiniDB <david.hall@mariadb.com>
Date:   Thu May 11 09:43:28 2017 -0500

    Merge pull request #182 from mariadb-corporation/MCOL-712
    
    MCOL-712 Support TEXT for GROUP BY


Finished testing functions by running the FuncForText test suite in Autopilot.  Few issues were identified and are being tracked by separate tickets.

",1,"Build verified: github source 1.1.0

Build verified: Github source 1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 349cae544b6bc71910267a3b3b0fa3fb57b0a587
Merge: bd13090 2ecb85c
Author: benthompson15 
Date:   Thu May 4 16:06:16 2017 -0500

    Merge pull request #50 from mariadb-corporation/10.2-fixes
    
    10.2 fixes

[root@localhost mariadb-columnstore-server]# cd mariadb-columnstore-engine/
[root@localhost mariadb-columnstore-engine]# git show
commit b4883e6abd18fb63c4af450679c2720cdb537dad
Merge: 85a5eaa 83331f4
Author: dhall-InfiniDB 
Date:   Thu May 11 09:43:28 2017 -0500

    Merge pull request #182 from mariadb-corporation/MCOL-712
    
    MCOL-712 Support TEXT for GROUP BY


Finished testing functions by running the FuncForText test suite in Autopilot.  Few issues were identified and are being tracked by separate tickets.

"
1502,MCOL-704,MCOL,Andrew Hutchings,94914,2017-05-08 20:09:50,Cherry picked MCOL-686 fix into a pull request for MCOL-704,1,Cherry picked MCOL-686 fix into a pull request for MCOL-704
1503,MCOL-704,MCOL,Daniel Lee,95157,2017-05-11 16:59:31,"Build verified: 1.0.9-1

Ran each query after a system restart so they disk runs for fair comparison.

MariaDB [tpch10]> select count(l_linenumber) from lineitem where l_shipdate between current_date() and current_date();
+---------------------+
| count(l_linenumber) |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.14 sec)

MariaDB [tpch10]> select count(l_linenumber) from lineitem where l_shipdate between '2017-05-11' and '2017-05-11';
+---------------------+
| count(l_linenumber) |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.13 sec)

",2,"Build verified: 1.0.9-1

Ran each query after a system restart so they disk runs for fair comparison.

MariaDB [tpch10]> select count(l_linenumber) from lineitem where l_shipdate between current_date() and current_date();
+---------------------+
| count(l_linenumber) |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.14 sec)

MariaDB [tpch10]> select count(l_linenumber) from lineitem where l_shipdate between '2017-05-11' and '2017-05-11';
+---------------------+
| count(l_linenumber) |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.13 sec)

"
1504,MCOL-706,MCOL,Andrew Hutchings,94913,2017-05-08 19:57:28,"Pull request pending. 4 minor merge conflicts resolved.

One potential issue is there appears to be a new RPM in the cpack file for a new backup tool. We may want to remove this.",1,"Pull request pending. 4 minor merge conflicts resolved.

One potential issue is there appears to be a new RPM in the cpack file for a new backup tool. We may want to remove this."
1505,MCOL-706,MCOL,David Thompson,94915,2017-05-08 20:12:50,Decision is to exclude the alpha mariadb backup rpm since the goal for now is to get early feedback from core server use cases first.,2,Decision is to exclude the alpha mariadb backup rpm since the goal for now is to get early feedback from core server use cases first.
1506,MCOL-706,MCOL,Andrew Hutchings,94916,2017-05-08 20:17:24,Backup RPM removed.,3,Backup RPM removed.
1507,MCOL-706,MCOL,Daniel Lee,95153,2017-05-11 16:25:56,"Build verified: 1.0.9-1

[root@localhost ~]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 6
Server version: 10.1.23-MariaDB Columnstore 1.0.9-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

Also verified no backup rpm included in the distribution package.",4,"Build verified: 1.0.9-1

[root@localhost ~]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 6
Server version: 10.1.23-MariaDB Columnstore 1.0.9-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

Also verified no backup rpm included in the distribution package."
1508,MCOL-716,MCOL,Roman,106274,2018-01-26 14:20:06,"It occures, that major DDL/DML functions support a subset of UTF-8(according with [this|https://mariadb.com/kb/en/library/identifier-names/]) for table/column identifiers and only 'ALTER TABLE RENAME' doesn't support it because the mariadb plugin function ha_calpont_impl_rename_table_() replaces the initial query. I changed this and also replace push_warning() arguments according with recomendations given in server_code/sql/sql_error.cc to avoid possible sigabort. 

Kindly take a look at the tiny [functional tests results|https://hastebin.com/bojufapave.vbs].",1,"It occures, that major DDL/DML functions support a subset of UTF-8(according with [this|URL for table/column identifiers and only 'ALTER TABLE RENAME' doesn't support it because the mariadb plugin function ha_calpont_impl_rename_table_() replaces the initial query. I changed this and also replace push_warning() arguments according with recomendations given in server_code/sql/sql_error.cc to avoid possible sigabort. 

Kindly take a look at the tiny [functional tests results|URL"
1509,MCOL-716,MCOL,Andrew Hutchings,106405,2018-01-29 12:37:59,For QA: Please see Roman's tests.,2,For QA: Please see Roman's tests.
1510,MCOL-716,MCOL,Daniel Lee,107276,2018-02-13 17:58:08,"Build verified: Github source 1.2.0-1

/root/columnstore/mariadb-columnstore-server
commit 960853c58bcfd0b92a48fca6f823e0b43fce17a8
Author: david hill <david.hill@mariadb.com>
Date:   Mon Nov 20 20:42:55 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index 1e4a6c6..5d12495 100644
--- a/README.md
+++ b/README.md
@@ -273,7 +273,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ad2f469811d9bfc989174da13343e72ee2599af2
Merge: 070fc37 7c0086c
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Wed Feb 7 10:24:23 2018 +0200

    Merge pull request #399 from drrtuy/MCOL-876
    
    MCOL-876. CS now supports RENAME TABLE sql statement.

Verified test cases in mentioned test results, as well as additional tests using Asian characters.",3,"Build verified: Github source 1.2.0-1

/root/columnstore/mariadb-columnstore-server
commit 960853c58bcfd0b92a48fca6f823e0b43fce17a8
Author: david hill 
Date:   Mon Nov 20 20:42:55 2017 -0600

    Update README.md

diff --git a/README.md b/README.md
index 1e4a6c6..5d12495 100644
--- a/README.md
+++ b/README.md
@@ -273,7 +273,7 @@ apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libre
 These packages need to be installed:
 
 ```bash
-apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5
+apt-get install expect perl openssl file sudo libdbi-perl libboost-all-dev libreadline-dev rsync  net-tools libsnappy1v5 libreadline5
 ```
 ## For SUSE 12
 
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ad2f469811d9bfc989174da13343e72ee2599af2
Merge: 070fc37 7c0086c
Author: Andrew Hutchings 
Date:   Wed Feb 7 10:24:23 2018 +0200

    Merge pull request #399 from drrtuy/MCOL-876
    
    MCOL-876. CS now supports RENAME TABLE sql statement.

Verified test cases in mentioned test results, as well as additional tests using Asian characters."
1511,MCOL-723,MCOL,David Hill,95561,2017-05-20 22:45:15,"completed..

./columnstoreClusterTester.sh -h


This is the MariaDB ColumnStore Cluster System Test tool.

It will run a set of test to validate the setup of the MariaDB Columnstore system.
This can be run prior to the install of MariaDB ColumnStore to make sure the
servers/nodes are configured properly. It should be run as the user of the planned
install. Meaning if MariaDB ColumnStore is going to be installed as root user,
then run from root user. Also the assumption is that the servers/node have be
setup based on the Preparing for ColumnStore Installation.
It should also be run on the server that is designated as Performance Module #1.
This is the same server where the MariaDB ColumnStore package would be installed
 and where the install script would be executed from, postConfigure.

Additional information on Tool is documented at:

https://mariadb.com/kb/en/mariadb/*****/

Items that are checked:
	Node Ping test
	Node SSH test
	ColumnStore Port test
	OS version
	Locale settings
	Firewall settings
     Date/time settings
	Dependent packages installed
     For non-root user install - test permissions on /tmp and /dev/shm

Usage: ./columnstoreClusterTester.sh [options]
OPTIONS:
   -h,--help			Help
   --ipaddr=[ipaddresses]	Remote Node IP-Addresses/Hostnames, if not provide, will only check local node
                           	examples: 192.168.1.1,192.168.1.2 or serverum1,serverpm2
   --os=[os]			Optional: Set OS Version (centos6, centos7, debian8, suse12, ubuntu16)
   --password=[password]	Provide a user password. (Default: ssh-keys setup will be assumed)
   -c,--continue		Continue on failures
   --logfile=[fileName] 	Output results to a log file

NOTE: Dependent package : 'nmap' and 'expect' packages need to be installed locally

",1,"completed..

./columnstoreClusterTester.sh -h


This is the MariaDB ColumnStore Cluster System Test tool.

It will run a set of test to validate the setup of the MariaDB Columnstore system.
This can be run prior to the install of MariaDB ColumnStore to make sure the
servers/nodes are configured properly. It should be run as the user of the planned
install. Meaning if MariaDB ColumnStore is going to be installed as root user,
then run from root user. Also the assumption is that the servers/node have be
setup based on the Preparing for ColumnStore Installation.
It should also be run on the server that is designated as Performance Module #1.
This is the same server where the MariaDB ColumnStore package would be installed
 and where the install script would be executed from, postConfigure.

Additional information on Tool is documented at:

URL

Items that are checked:
	Node Ping test
	Node SSH test
	ColumnStore Port test
	OS version
	Locale settings
	Firewall settings
     Date/time settings
	Dependent packages installed
     For non-root user install - test permissions on /tmp and /dev/shm

Usage: ./columnstoreClusterTester.sh [options]
OPTIONS:
   -h,--help			Help
   --ipaddr=[ipaddresses]	Remote Node IP-Addresses/Hostnames, if not provide, will only check local node
                           	examples: 192.168.1.1,192.168.1.2 or serverum1,serverpm2
   --os=[os]			Optional: Set OS Version (centos6, centos7, debian8, suse12, ubuntu16)
   --password=[password]	Provide a user password. (Default: ssh-keys setup will be assumed)
   -c,--continue		Continue on failures
   --logfile=[fileName] 	Output results to a log file

NOTE: Dependent package : 'nmap' and 'expect' packages need to be installed locally

"
1512,MCOL-723,MCOL,David Hill,95562,2017-05-20 22:46:43,"Added into the engine report in the utils directory

mariadb-columnstore-engine/utils/clusterTester

commit fa21239a7efa9e6a260348eef1ecb0937532c248
Author: David Hill <david.hill@mariadb.com>
Date:   Sat May 20 17:41:22 2017 -0500

",2,"Added into the engine report in the utils directory

mariadb-columnstore-engine/utils/clusterTester

commit fa21239a7efa9e6a260348eef1ecb0937532c248
Author: David Hill 
Date:   Sat May 20 17:41:22 2017 -0500

"
1513,MCOL-723,MCOL,David Hill,95684,2017-05-23 15:50:41,"commit 37ead3c076bf1be5c30a2a76f50b521c0911ccb2
Author: David Hill <david.hill@mariadb.com>
Date:   Tue May 23 10:47:01 2017 -0500

    add ipaddr to the /tmp log files

 utils/clusterTester/columnstoreClusterTester.sh | 70 +++++++++++++++++++++++++++++++++++-----------------------------------
 1 file changed, 35 insertions(+), 35 deletions(-)

",3,"commit 37ead3c076bf1be5c30a2a76f50b521c0911ccb2
Author: David Hill 
Date:   Tue May 23 10:47:01 2017 -0500

    add ipaddr to the /tmp log files

 utils/clusterTester/columnstoreClusterTester.sh | 70 +++++++++++++++++++++++++++++++++++-----------------------------------
 1 file changed, 35 insertions(+), 35 deletions(-)

"
1514,MCOL-723,MCOL,Daniel Lee,95960,2017-05-30 19:49:38,"Build verified: Git source 1.0.10

[root@localhost mariadb-columnstore-server]# git show
commit 478209c9d58e0c34d0a177b39b42ed865ad30ccf
Author: David Hill <david.hill@mariadb.com>
Date:   Thu May 18 15:11:05 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit ccbdb07007dfde95c610fcead008b62e30d7411f
Author: david hill <david.hill@mariadb.com>
Date:   Fri May 26 10:52:17 2017 -0500

Have been testing with the prior builds and the above is the latest build tested.",4,"Build verified: Git source 1.0.10

[root@localhost mariadb-columnstore-server]# git show
commit 478209c9d58e0c34d0a177b39b42ed865ad30ccf
Author: David Hill 
Date:   Thu May 18 15:11:05 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit ccbdb07007dfde95c610fcead008b62e30d7411f
Author: david hill 
Date:   Fri May 26 10:52:17 2017 -0500

Have been testing with the prior builds and the above is the latest build tested."
1515,MCOL-724,MCOL,Andrew Hutchings,96121,2017-06-02 16:18:30,Only doing 10.2.6 tag for now because it was a complex enough merge to do that one and doing it tag to tag makes things easier to track.,1,Only doing 10.2.6 tag for now because it was a complex enough merge to do that one and doing it tag to tag makes things easier to track.
1516,MCOL-724,MCOL,Andrew Hutchings,96122,2017-06-02 16:58:04,"Pull request open, regression suite looks good.",2,"Pull request open, regression suite looks good."
1517,MCOL-724,MCOL,Daniel Lee,96302,2017-06-06 21:27:34,"Build verified: Github source 1.1.0

[root@localhost columnstore]# cd mariadb-columnstore-server/
[root@localhost mariadb-columnstore-server]# cat VERSION
MYSQL_VERSION_MAJOR=10
MYSQL_VERSION_MINOR=2
MYSQL_VERSION_PATCH=6
COLUMNSTORE_VERSION_MAJOR=1
COLUMNSTORE_VERSION_MINOR=1
COLUMNSTORE_VERSION_PATCH=0
COLUMNSTORE_VERSION_RELEASE=1

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.6-MariaDB-log Columnstore 1.1.0-1


",3,"Build verified: Github source 1.1.0

[root@localhost columnstore]# cd mariadb-columnstore-server/
[root@localhost mariadb-columnstore-server]# cat VERSION
MYSQL_VERSION_MAJOR=10
MYSQL_VERSION_MINOR=2
MYSQL_VERSION_PATCH=6
COLUMNSTORE_VERSION_MAJOR=1
COLUMNSTORE_VERSION_MINOR=1
COLUMNSTORE_VERSION_PATCH=0
COLUMNSTORE_VERSION_RELEASE=1

[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.6-MariaDB-log Columnstore 1.1.0-1


"
1518,MCOL-732,MCOL,Andrew Hutchings,96120,2017-06-02 16:17:28,"Merge in pull request, regression suite seems good.",1,"Merge in pull request, regression suite seems good."
1519,MCOL-732,MCOL,Daniel Lee,96331,2017-06-07 13:26:17,"Build verified: Github source 1.0.10

[root@localhost mariadb-columnstore-server]# git show
commit a02e74550b0e5e5e4995b799ce31e7e5d11f467a
Merge: 478209c dd07cf6
Author: David.Hall <david.hall@mariadb.com>
Date:   Fri Jun 2 11:47:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 4d5a59d3d104cda2a7715b4e619318e43e964214
Author: david hill <david.hill@mariadb.com>
Date:   Fri Jun 2 11:03:12 2017 -0500

[root@localhost mariadb-columnstore-server]# cat VERSION
MYSQL_VERSION_MAJOR=10
MYSQL_VERSION_MINOR=1
MYSQL_VERSION_PATCH=24
COLUMNSTORE_VERSION_MAJOR=1
COLUMNSTORE_VERSION_MINOR=0
COLUMNSTORE_VERSION_PATCH=10
COLUMNSTORE_VERSION_RELEASE=1

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.1.24-MariaDB Columnstore 1.0.10-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

",2,"Build verified: Github source 1.0.10

[root@localhost mariadb-columnstore-server]# git show
commit a02e74550b0e5e5e4995b799ce31e7e5d11f467a
Merge: 478209c dd07cf6
Author: David.Hall 
Date:   Fri Jun 2 11:47:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 4d5a59d3d104cda2a7715b4e619318e43e964214
Author: david hill 
Date:   Fri Jun 2 11:03:12 2017 -0500

[root@localhost mariadb-columnstore-server]# cat VERSION
MYSQL_VERSION_MAJOR=10
MYSQL_VERSION_MINOR=1
MYSQL_VERSION_PATCH=24
COLUMNSTORE_VERSION_MAJOR=1
COLUMNSTORE_VERSION_MINOR=0
COLUMNSTORE_VERSION_PATCH=10
COLUMNSTORE_VERSION_RELEASE=1

Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 13
Server version: 10.1.24-MariaDB Columnstore 1.0.10-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

"
1520,MCOL-739,MCOL,Andrew Hutchings,96272,2017-06-06 13:22:00,Implemented in pull request and debugging test output for the API making the call attached.,1,Implemented in pull request and debugging test output for the API making the call attached.
1521,MCOL-739,MCOL,Andrew Hutchings,96273,2017-06-06 13:31:51,"I should note, there is no way for QA to test this at the moment, hence the attachment",2,"I should note, there is no way for QA to test this at the moment, hence the attachment"
1522,MCOL-740,MCOL,Andrew Hutchings,97317,2017-07-05 09:17:15,C++ API is code complete for the first version.,1,C++ API is code complete for the first version.
1523,MCOL-741,MCOL,Andrew Hutchings,96205,2017-06-05 14:09:13,API codebase already compiles with Avro support. We just need to add the API calls.,1,API codebase already compiles with Avro support. We just need to add the API calls.
1524,MCOL-741,MCOL,Andrew Hutchings,110979,2018-05-15 06:33:53,This exists now in the data adapters tree,2,This exists now in the data adapters tree
1525,MCOL-743,MCOL,Andrew Hutchings,97457,2017-07-10 10:58:28,Documentation written and included with source to generate PDFs.,1,Documentation written and included with source to generate PDFs.
1526,MCOL-750,MCOL,David Hill,97148,2017-06-30 19:51:40,code part of the MCOL-770 branch pull request,1,code part of the MCOL-770 branch pull request
1527,MCOL-750,MCOL,David Hill,97339,2017-07-05 14:23:04,still doing some develop testing before hand off to QA,2,still doing some develop testing before hand off to QA
1528,MCOL-750,MCOL,David Hill,97354,2017-07-05 18:41:07,"for testing, do the same multi-node install and addmodule. The postConfigure remote scripts and the addmodule command will be faster due to the scripts that setup the remote node are doing less and have better command completion checks.",3,"for testing, do the same multi-node install and addmodule. The postConfigure remote scripts and the addmodule command will be faster due to the scripts that setup the remote node are doing less and have better command completion checks."
1529,MCOL-750,MCOL,Daniel Lee,97415,2017-07-06 18:53:20,"Build tested: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3601c344ba8eaedb7ed780eab5da4377db361b07
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jul 4 03:51:27 2017 -0500

Did not see the performance gain expected when compared to 1.0.9-1.  The addmodule for took 96 seconds for 1.0.9-1 and 95 seconds for 1.10.0-1.
",4,"Build tested: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill 
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3601c344ba8eaedb7ed780eab5da4377db361b07
Author: david hill 
Date:   Tue Jul 4 03:51:27 2017 -0500

Did not see the performance gain expected when compared to 1.0.9-1.  The addmodule for took 96 seconds for 1.0.9-1 and 95 seconds for 1.10.0-1.
"
1530,MCOL-750,MCOL,Daniel Lee,97418,2017-07-06 19:55:06,"It turned out that I did not use the latest build package file (yes, cp error).  Tried the test again and addmodule took 45 seconds instead of 96.
",5,"It turned out that I did not use the latest build package file (yes, cp error).  Tried the test again and addmodule took 45 seconds instead of 96.
"
1531,MCOL-750,MCOL,David Hill,98563,2017-08-10 17:45:16,"original changes seem to destabilize the install, so need some changes.",6,"original changes seem to destabilize the install, so need some changes."
1532,MCOL-750,MCOL,David Hill,101052,2017-10-02 14:08:42,"These changes were made in the 1.1.0 builds...

So just needs to be closed by Daniel, we have tested quite a bit on 1.1.0 beta install testing",7,"These changes were made in the 1.1.0 builds...

So just needs to be closed by Daniel, we have tested quite a bit on 1.1.0 beta install testing"
1533,MCOL-750,MCOL,Daniel Lee,101810,2017-10-23 19:56:38,"Build verified: 1.1.1-1 GitHub source
/root/columnstore/mariadb-columnstore-server
commit 3d846d3277ba970b32ba3f471323fcac58b5c35d
Author: david hill <david.hill@mariadb.com>
Date: Mon Oct 23 09:57:05 2017 -0500
change to 1.1.1
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 4aa7eb1830ddf585706f804b1982589c7d67ff0a
Author: root <root@srvhill01.lan>
Date: Mon Oct 23 09:56:07 2017 -0500
change to 1.1.1

We did run lots of tests for the 1.1.0-1 beta.  I also did one more installation tests with the latest source to make sure it still works.
",8,"Build verified: 1.1.1-1 GitHub source
/root/columnstore/mariadb-columnstore-server
commit 3d846d3277ba970b32ba3f471323fcac58b5c35d
Author: david hill 
Date: Mon Oct 23 09:57:05 2017 -0500
change to 1.1.1
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 4aa7eb1830ddf585706f804b1982589c7d67ff0a
Author: root 
Date: Mon Oct 23 09:56:07 2017 -0500
change to 1.1.1

We did run lots of tests for the 1.1.0-1 beta.  I also did one more installation tests with the latest source to make sure it still works.
"
1534,MCOL-752,MCOL,David Hall,96291,2017-06-06 19:04:06,Done for UDAF. UDAnF yet to do,1,Done for UDAF. UDAnF yet to do
1535,MCOL-753,MCOL,David Hall,96292,2017-06-06 19:05:07,"UDAF done, except for testing and bug fixing. UDAnF yet to do.",1,"UDAF done, except for testing and bug fixing. UDAnF yet to do."
1536,MCOL-769,MCOL,Andrew Hutchings,96508,2017-06-13 12:22:40,Branch created for this but mcsapi additions are needed to test it. So putting on hold for now.,1,Branch created for this but mcsapi additions are needed to test it. So putting on hold for now.
1537,MCOL-769,MCOL,Andrew Hutchings,96592,2017-06-16 11:25:24,Also added GET_UNCOMMITTED_LBIDS which is a wrapper around a VSS call which doesn't use network. We need this to commit the lbids and update the HWM.,2,Also added GET_UNCOMMITTED_LBIDS which is a wrapper around a VSS call which doesn't use network. We need this to commit the lbids and update the HWM.
1538,MCOL-769,MCOL,Andrew Hutchings,96593,2017-06-16 11:30:01,"Two commands added in the tree as indicated.

Not testable by QA yet but I've used these in mcsapi to write and commit thousands of rows of data.",3,"Two commands added in the tree as indicated.

Not testable by QA yet but I've used these in mcsapi to write and commit thousands of rows of data."
1539,MCOL-769,MCOL,Andrew Hutchings,96612,2017-06-16 16:21:03,Moving back to me. We are using the version buffer when we really don't want to,4,Moving back to me. We are using the version buffer when we really don't want to
1540,MCOL-769,MCOL,Andrew Hutchings,96672,2017-06-19 20:24:08,"Performance isn't great yet, 4,000,000 rows of two int columns takes a minute in debug build. Profiling shows this mostly down to the liberal use of boost::any which has a performance hit with the copys we do in it. We should consider boost::spirit::hold_any for this function. In addition we rather liberally fsync when it isn't needed.",5,"Performance isn't great yet, 4,000,000 rows of two int columns takes a minute in debug build. Profiling shows this mostly down to the liberal use of boost::any which has a performance hit with the copys we do in it. We should consider boost::spirit::hold_any for this function. In addition we rather liberally fsync when it isn't needed."
1541,MCOL-769,MCOL,Andrew Hutchings,97022,2017-06-27 19:36:59,Current version has been running pretty well with mcsapi.,6,Current version has been running pretty well with mcsapi.
1542,MCOL-770,MCOL,David Hill,96654,2017-06-19 14:17:12,"I'm trying out option 2 where ProcMon will do the setup on a newly installed node once it detects that Procmgr is up on the pm1 node. If this works, user would just need to do the following to setup the non-pm1 nodes

1. install packages (if binary, run post-install)
2. start the columnstore service",1,"I'm trying out option 2 where ProcMon will do the setup on a newly installed node once it detects that Procmgr is up on the pm1 node. If this works, user would just need to do the following to setup the non-pm1 nodes

1. install packages (if binary, run post-install)
2. start the columnstore service"
1543,MCOL-770,MCOL,David Hill,97118,2017-06-29 19:44:22,"spent a of time this week working on getting the new changes working in amazon world. did a lot of addmodules and dbroot commands. That is looking good now.

rebuilding with the amazon changes and will retest the binary/rpm installs on centos and then will test on debian versions.
",2,"spent a of time this week working on getting the new changes working in amazon world. did a lot of addmodules and dbroot commands. That is looking good now.

rebuilding with the amazon changes and will retest the binary/rpm installs on centos and then will test on debian versions.
"
1544,MCOL-770,MCOL,David Hill,97149,2017-06-30 19:52:16,MCOL-770 pull request,3,MCOL-770 pull request
1545,MCOL-770,MCOL,David Hill,97250,2017-07-03 21:08:32,"one thing that changed between phase 1 and 2 on the non-distributed install is now no ssh/scp or anything of messaging happens between postConfigure and the remote nodes. So this means that it will not do any verification if the packages and the version of package that is installed. That is still being done in ProcMon/ProcMgr.

",4,"one thing that changed between phase 1 and 2 on the non-distributed install is now no ssh/scp or anything of messaging happens between postConfigure and the remote nodes. So this means that it will not do any verification if the packages and the version of package that is installed. That is still being done in ProcMon/ProcMgr.

"
1546,MCOL-770,MCOL,David Hill,97340,2017-07-05 14:23:23,still doing some develop testing before hand off to QA,5,still doing some develop testing before hand off to QA
1547,MCOL-770,MCOL,David Hill,97355,2017-07-05 18:44:05,"for non-distributed installs on remote nodes, just need to install packages and start the service.

This is also a new message that comes out when post-install runs:

./post-install 
The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distrubuted option:

/usr/local/mariadb/columnstore/bin/columnstore start


",6,"for non-distributed installs on remote nodes, just need to install packages and start the service.

This is also a new message that comes out when post-install runs:

./post-install 
The next step is:

If installing on a pm1 node:

/usr/local/mariadb/columnstore/bin/postConfigure

If installing on a non-pm1 using the non-distrubuted option:

/usr/local/mariadb/columnstore/bin/columnstore start


"
1548,MCOL-770,MCOL,Daniel Lee,97419,2017-07-06 20:30:02,"Build verified: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3601c344ba8eaedb7ed780eab5da4377db361b07
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jul 4 03:51:27 2017 -0500

Verified for Centos 7 binary installation only.  We will need to test for other packages when 1.1.0 is to be released.
",7,"Build verified: Github source 1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill 
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3601c344ba8eaedb7ed780eab5da4377db361b07
Author: david hill 
Date:   Tue Jul 4 03:51:27 2017 -0500

Verified for Centos 7 binary installation only.  We will need to test for other packages when 1.1.0 is to be released.
"
1549,MCOL-770,MCOL,Daniel Lee,100218,2017-09-14 21:37:14,"Build tested: 1.1.0-1

Performed the following tests, only ubuntu16.04 failed.

Passed /home/dlee/tests/1.1.0-1/centos6.7/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/centos7/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/suse12/root.bin.1um2pm.default.s1.n
Failed /home/dlee/tests/1.1.0-1/ubuntu16.04/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/debian8.5/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/debian9.1/root.bin.1um2pm.default.s1.n

There is an issue with non-distributed installation for ubuntu16.04, binary. After installing the bin package on UM1.

/usr/local/mariadb/columnstore/bin/columnstore start
Please run the postConfigure install script, check the Installation Guide
for additional details
",8,"Build tested: 1.1.0-1

Performed the following tests, only ubuntu16.04 failed.

Passed /home/dlee/tests/1.1.0-1/centos6.7/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/centos7/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/suse12/root.bin.1um2pm.default.s1.n
Failed /home/dlee/tests/1.1.0-1/ubuntu16.04/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/debian8.5/root.bin.1um2pm.default.s1.n
Passed /home/dlee/tests/1.1.0-1/debian9.1/root.bin.1um2pm.default.s1.n

There is an issue with non-distributed installation for ubuntu16.04, binary. After installing the bin package on UM1.

/usr/local/mariadb/columnstore/bin/columnstore start
Please run the postConfigure install script, check the Installation Guide
for additional details
"
1550,MCOL-770,MCOL,Daniel Lee,100274,2017-09-15 16:10:00,"With 1.1.0-1 packages released last night, I centos6.7 failed with the same error, but ubuntu16.04 passed.

Failed with errors /home/dlee/tests/1.1.0-1/centos6.7/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/centos7/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/suse12/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/ubuntu16.04/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/debian8.5/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/debian9.1/root.bin.1um2pm.default.n.s1
",9,"With 1.1.0-1 packages released last night, I centos6.7 failed with the same error, but ubuntu16.04 passed.

Failed with errors /home/dlee/tests/1.1.0-1/centos6.7/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/centos7/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/suse12/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/ubuntu16.04/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/debian8.5/root.bin.1um2pm.default.n.s1
Passed /home/dlee/tests/1.1.0-1/debian9.1/root.bin.1um2pm.default.n.s1
"
1551,MCOL-770,MCOL,Daniel Lee,100361,2017-09-18 19:41:04,"Build tested: 1.1.0 beta (packages released on Sun 09/17/2017 evening)

All non-distributed installation on all 6 supported OSs passed.
",10,"Build tested: 1.1.0 beta (packages released on Sun 09/17/2017 evening)

All non-distributed installation on all 6 supported OSs passed.
"
1552,MCOL-777,MCOL,Andrew Hutchings,98028,2017-07-27 10:50:01,"Pull request cleans out autotools. Some kdevelop stuff and some other stale files.

We need to do a second pass at some point as I think some stuff can go from the build/ directory amongst other things.",1,"Pull request cleans out autotools. Some kdevelop stuff and some other stale files.

We need to do a second pass at some point as I think some stuff can go from the build/ directory amongst other things."
1553,MCOL-777,MCOL,Andrew Hutchings,98029,2017-07-27 10:50:43,For QA: it should compile as normal after these files are removed. Just removing some things that aren't used any more.,2,For QA: it should compile as normal after these files are removed. Just removing some things that aren't used any more.
1554,MCOL-777,MCOL,Ben Thompson,98361,2017-08-07 15:05:16,Verified build and merged.,3,Verified build and merged.
1555,MCOL-777,MCOL,Daniel Lee,99681,2017-09-05 15:02:35,"Build verified: 1.1.0 Github source

/root/columnstore/mariadb-columnstore-server
commit 6ed33d194819aaa5f2521c888639f44546fb7ce2
Merge: 97284ea 770537e
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Aug 3 20:54:13 2017 +0100

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 90353b9b908e1c9ee241c4a156a2a377c53cc807
Author: david hill <david.hill@mariadb.com>
Date:   Fri Sep 1 14:46:07 2017 -0500
",4,"Build verified: 1.1.0 Github source

/root/columnstore/mariadb-columnstore-server
commit 6ed33d194819aaa5f2521c888639f44546fb7ce2
Merge: 97284ea 770537e
Author: Andrew Hutchings 
Date:   Thu Aug 3 20:54:13 2017 +0100

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 90353b9b908e1c9ee241c4a156a2a377c53cc807
Author: david hill 
Date:   Fri Sep 1 14:46:07 2017 -0500
"
1556,MCOL-787,MCOL,David Hill,96931,2017-06-26 14:52:39,"how to test on a  fresh install.
when postConfigure gets to the spot where it ask to start system, exit out and run mcsadmin startsystem.. then try to create tables.

===== MariaDB ColumnStore System Startup =====

System Installation is complete. If any part of the install failed,
the problem should be investigated and resolved before continuing.

Would you like to startup the MariaDB ColumnStore System? [y,n] (y) > n

You choose not to Start the MariaDB ColumnStore Software at this time.
[root@ip-172-30-0-176 bin]# mcsadmin startsystem",1,"how to test on a  fresh install.
when postConfigure gets to the spot where it ask to start system, exit out and run mcsadmin startsystem.. then try to create tables.

===== MariaDB ColumnStore System Startup =====

System Installation is complete. If any part of the install failed,
the problem should be investigated and resolved before continuing.

Would you like to startup the MariaDB ColumnStore System? [y,n] (y) > n

You choose not to Start the MariaDB ColumnStore Software at this time.
[root@ip-172-30-0-176 bin]# mcsadmin startsystem"
1557,MCOL-787,MCOL,David Hill,96932,2017-06-26 14:55:26,"this will show up in the pm1 debug logs

Jun 26 14:53:42 ip-172-30-0-176 ProcessManager[9439]: 42.996536 |0|0|0| D 17 CAL0000: Set System State = ACTIVE
Jun 26 14:53:42 ip-172-30-0-176 ProcessMonitor[9287]: 42.996758 |0|0|0| D 18 CAL0000: statusControl: REQUEST RECEIVED: Set System State = ACTIVE
Jun 26 14:53:43 ip-172-30-0-176 ProcessManager[9439]: 43.001794 |0|0|0| D 17 CAL0000: sendMsgProcMon: Process module pm1
Jun 26 14:53:43 ip-172-30-0-176 ProcessMonitor[9287]: 43.001967 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jun 26 14:53:43 ip-172-30-0-176 ProcessMonitor[9287]: 43.003988 |0|0|0| D 18 CAL0000: buildSystemTables: dbbuilder 7 Successfully Launched
Jun 26 14:53:43 ip-172-30-0-176 ProcessMonitor[9287]: 43.004046 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 0
Jun 26 14:53:43 ip-172-30-0-176 ProcessManager[9439]: 43.004099 |0|0|0| D 17 CAL0000: startSystemThread Exit

what is logging after a future startsystem or restartsystem

Jun 26 14:56:41 ip-172-30-0-176 ProcessMonitor[9287]: 41.583212 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jun 26 14:56:41 ip-172-30-0-176 ProcessMonitor[9287]: 41.583286 |0|0|0| E 18 CAL0000: buildSystemTables: System Tables Already Exist
Jun 26 14:56:41 ip-172-30-0-176 ProcessMonitor[9287]: 41.583319 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 6
",2,"this will show up in the pm1 debug logs

Jun 26 14:53:42 ip-172-30-0-176 ProcessManager[9439]: 42.996536 |0|0|0| D 17 CAL0000: Set System State = ACTIVE
Jun 26 14:53:42 ip-172-30-0-176 ProcessMonitor[9287]: 42.996758 |0|0|0| D 18 CAL0000: statusControl: REQUEST RECEIVED: Set System State = ACTIVE
Jun 26 14:53:43 ip-172-30-0-176 ProcessManager[9439]: 43.001794 |0|0|0| D 17 CAL0000: sendMsgProcMon: Process module pm1
Jun 26 14:53:43 ip-172-30-0-176 ProcessMonitor[9287]: 43.001967 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jun 26 14:53:43 ip-172-30-0-176 ProcessMonitor[9287]: 43.003988 |0|0|0| D 18 CAL0000: buildSystemTables: dbbuilder 7 Successfully Launched
Jun 26 14:53:43 ip-172-30-0-176 ProcessMonitor[9287]: 43.004046 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 0
Jun 26 14:53:43 ip-172-30-0-176 ProcessManager[9439]: 43.004099 |0|0|0| D 17 CAL0000: startSystemThread Exit

what is logging after a future startsystem or restartsystem

Jun 26 14:56:41 ip-172-30-0-176 ProcessMonitor[9287]: 41.583212 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jun 26 14:56:41 ip-172-30-0-176 ProcessMonitor[9287]: 41.583286 |0|0|0| E 18 CAL0000: buildSystemTables: System Tables Already Exist
Jun 26 14:56:41 ip-172-30-0-176 ProcessMonitor[9287]: 41.583319 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 6
"
1558,MCOL-787,MCOL,David Hill,96936,2017-06-26 15:15:39,"1 line code fix in ProcMgr

fixed in 1.0.x branch

commit 3b1864621fa6cb6a04245ee62d0c2464433868c1
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jun 26 10:13:43 2017 -0500

    MCOL-787 - add code to create system catlog on startup

 procmgr/processmanager.cpp | 3 +++

fix in 1.1.0 branch

commit b424a02c2e431be6f4cf39c12f0c7fd0155c23f5
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jun 26 09:58:38 2017 -0500

    MCOL-787 - add code to create system catlog on startup

 procmgr/processmanager.cpp | 3 +++
",3,"1 line code fix in ProcMgr

fixed in 1.0.x branch

commit 3b1864621fa6cb6a04245ee62d0c2464433868c1
Author: david hill 
Date:   Mon Jun 26 10:13:43 2017 -0500

    MCOL-787 - add code to create system catlog on startup

 procmgr/processmanager.cpp | 3 +++

fix in 1.1.0 branch

commit b424a02c2e431be6f4cf39c12f0c7fd0155c23f5
Author: david hill 
Date:   Mon Jun 26 09:58:38 2017 -0500

    MCOL-787 - add code to create system catlog on startup

 procmgr/processmanager.cpp | 3 +++
"
1559,MCOL-787,MCOL,Daniel Lee,97075,2017-06-28 17:09:41,"Build verified: Github source for 1.0.10 and 1.1.0

Verified both root and non-root install for 1um2pm configuration.
Verified DDL, DML, query, and cpimport

1.0.10-1

[root@localhost mariadb-columnstore-server]# git show
commit e8bf04ce2b6b67ee399b3626dff6055f35d50c60
Merge: a02e745 9c0aa3c
Author: David.Hall <david.hall@mariadb.com>
Date:   Thu Jun 8 10:13:15 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3b1864621fa6cb6a04245ee62d0c2464433868c1
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jun 26 10:13:43 2017 -0500


1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill <david.hill@mariadb.com>
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit b424a02c2e431be6f4cf39c12f0c7fd0155c23f5
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jun 26 09:58:38 2017 -0500

",4,"Build verified: Github source for 1.0.10 and 1.1.0

Verified both root and non-root install for 1um2pm configuration.
Verified DDL, DML, query, and cpimport

1.0.10-1

[root@localhost mariadb-columnstore-server]# git show
commit e8bf04ce2b6b67ee399b3626dff6055f35d50c60
Merge: a02e745 9c0aa3c
Author: David.Hall 
Date:   Thu Jun 8 10:13:15 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3b1864621fa6cb6a04245ee62d0c2464433868c1
Author: david hill 
Date:   Mon Jun 26 10:13:43 2017 -0500


1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 60f2f261f81d994307762d6d93380873513a0be8
Author: david hill 
Date:   Tue Jun 20 16:05:32 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit b424a02c2e431be6f4cf39c12f0c7fd0155c23f5
Author: david hill 
Date:   Mon Jun 26 09:58:38 2017 -0500

"
1560,MCOL-787,MCOL,David Hill,97780,2017-07-18 19:46:28,"problem in ubuntu16 VM testing, both postConfigure and procmgr are trying to run dbbuilder 7 causing the startup to hang

root     10796  7327  0 14:37 pts/19   00:00:00 sh -c /usr/local/mariadb/columnstore/bin/dbbuilder 7 > /tmp/dbbuilder.log
root     10797 10796  0 14:37 pts/19   00:00:00 /usr/local/mariadb/columnstore/bin/dbbuilder 7
root     11000     1  0 14:37 pts/19   00:00:00 /bin/sh /usr/local/mariadb/columnstore/mysql//bin/mysqld_safe --datadir=/usr/local/mar
mysql    11188 11000  0 14:37 pts/19   00:00:00 /usr/local/mariadb/columnstore/mysql//bin/mysqld --basedir=/usr/local/mariadb/columnst
root     11683     1  0 14:38 pts/19   00:00:00 /usr/local/mariadb/columnstore/bin/dbbuilder 7

----- Starting MariaDB ColumnStore on local server -----

MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait ....... DONE

",5,"problem in ubuntu16 VM testing, both postConfigure and procmgr are trying to run dbbuilder 7 causing the startup to hang

root     10796  7327  0 14:37 pts/19   00:00:00 sh -c /usr/local/mariadb/columnstore/bin/dbbuilder 7 > /tmp/dbbuilder.log
root     10797 10796  0 14:37 pts/19   00:00:00 /usr/local/mariadb/columnstore/bin/dbbuilder 7
root     11000     1  0 14:37 pts/19   00:00:00 /bin/sh /usr/local/mariadb/columnstore/mysql//bin/mysqld_safe --datadir=/usr/local/mar
mysql    11188 11000  0 14:37 pts/19   00:00:00 /usr/local/mariadb/columnstore/mysql//bin/mysqld --basedir=/usr/local/mariadb/columnst
root     11683     1  0 14:38 pts/19   00:00:00 /usr/local/mariadb/columnstore/bin/dbbuilder 7

----- Starting MariaDB ColumnStore on local server -----

MariaDB ColumnStore successfully started

MariaDB ColumnStore Database Platform Starting, please wait ....... DONE

"
1561,MCOL-787,MCOL,David Hill,97802,2017-07-19 14:51:57,"fix - added check in ProcMon fucntion that builds the system catalog to not run if postConfigure or dbbuilder executibles are running. This function is called by ProcMgr at the end of the successfull startsystem.

commit 6a0bceaa9bed412912691ed775da82d54bf8a6a9
Author: david hill <david.hill@mariadb.com>
Date:   Wed Jul 19 09:49:31 2017 -0500

    MCOL-787 - add checks before dbbuilder 7

 procmgr/processmanager.cpp |  9 ++++++++-
 procmon/processmonitor.cpp | 13 ++++++++++++-
 2 files changed, 20 insertions(+), 2 deletions(-)
",6,"fix - added check in ProcMon fucntion that builds the system catalog to not run if postConfigure or dbbuilder executibles are running. This function is called by ProcMgr at the end of the successfull startsystem.

commit 6a0bceaa9bed412912691ed775da82d54bf8a6a9
Author: david hill 
Date:   Wed Jul 19 09:49:31 2017 -0500

    MCOL-787 - add checks before dbbuilder 7

 procmgr/processmanager.cpp |  9 ++++++++-
 procmon/processmonitor.cpp | 13 ++++++++++++-
 2 files changed, 20 insertions(+), 2 deletions(-)
"
1562,MCOL-787,MCOL,David Hill,97803,2017-07-19 14:54:38,"This would be the debug log to show that ProcMgr did the call, but its didn't try to do the build. This return code is ALREADY_IN_PROGRESS. 

Jul 19 09:45:13 virtualbox-centos7 ProcessMonitor[3937]: 13.443277 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jul 19 09:45:13 virtualbox-centos7 ProcessMonitor[3937]: 13.455789 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 7
Jul 19 09:45:13 virtualbox-centos7 ProcessManager[4025]: 13.455830 |0|0|0| D 17 CAL0000: System Catalog Successfully not built by ProcMgr, ret code = 7
",7,"This would be the debug log to show that ProcMgr did the call, but its didn't try to do the build. This return code is ALREADY_IN_PROGRESS. 

Jul 19 09:45:13 virtualbox-centos7 ProcessMonitor[3937]: 13.443277 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jul 19 09:45:13 virtualbox-centos7 ProcessMonitor[3937]: 13.455789 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 7
Jul 19 09:45:13 virtualbox-centos7 ProcessManager[4025]: 13.455830 |0|0|0| D 17 CAL0000: System Catalog Successfully not built by ProcMgr, ret code = 7
"
1563,MCOL-787,MCOL,David Hill,97804,2017-07-19 14:57:33,"this is what you would see on startsystem when the catalog already existed

Jul 19 09:56:19 virtualbox-centos7 ProcessMonitor[8296]: 19.114912 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jul 19 09:56:19 virtualbox-centos7 ProcessMonitor[8296]: 19.138419 |0|0|0| D 18 CAL0000: buildSystemTables: System Tables Already Exist
Jul 19 09:56:19 virtualbox-centos7 ProcessMonitor[8296]: 19.138533 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 6
Jul 19 09:56:19 virtualbox-centos7 ProcessManager[8384]: 19.138621 |0|0|0| D 17 CAL0000: System Catalog Successfully not built by ProcMgr, ret code = 6
",8,"this is what you would see on startsystem when the catalog already existed

Jul 19 09:56:19 virtualbox-centos7 ProcessMonitor[8296]: 19.114912 |0|0|0| I 18 CAL0000: MSG RECEIVED: Check and Build System Tables
Jul 19 09:56:19 virtualbox-centos7 ProcessMonitor[8296]: 19.138419 |0|0|0| D 18 CAL0000: buildSystemTables: System Tables Already Exist
Jul 19 09:56:19 virtualbox-centos7 ProcessMonitor[8296]: 19.138533 |0|0|0| I 18 CAL0000: PROCBUILDSYSTEMTABLES: ACK back to ProcMgr, return status = 6
Jul 19 09:56:19 virtualbox-centos7 ProcessManager[8384]: 19.138621 |0|0|0| D 17 CAL0000: System Catalog Successfully not built by ProcMgr, ret code = 6
"
1564,MCOL-787,MCOL,David Hill,98039,2017-07-27 15:27:22,"checkins for 1.1.0

commit f17c2e1820dd4379af8b52f82519ea9bb38aaa0c
Author: David Hill <david.hill@mariadb.com>
Date:   Thu Jul 27 10:26:43 2017 -0500

    MCOL-787 - merged code from 1.0.10

 procmgr/processmanager.cpp | 11 ++++++++---
 procmon/processmonitor.cpp | 33 ++++++++++++++++++++++-----------
",9,"checkins for 1.1.0

commit f17c2e1820dd4379af8b52f82519ea9bb38aaa0c
Author: David Hill 
Date:   Thu Jul 27 10:26:43 2017 -0500

    MCOL-787 - merged code from 1.0.10

 procmgr/processmanager.cpp | 11 ++++++++---
 procmon/processmonitor.cpp | 33 ++++++++++++++++++++++-----------
"
1565,MCOL-787,MCOL,Daniel Lee,100154,2017-09-13 21:58:58,"Build verified: 1.1.0-1, 1.0.11-1",10,"Build verified: 1.1.0-1, 1.0.11-1"
1566,MCOL-792,MCOL,David Hill,98692,2017-08-14 18:15:30,"successfully got a 1.0.11 build...

next will be regression test, install test, and need to upgrade the columnstoreClusterTester for debian 9..

",1,"successfully got a 1.0.11 build...

next will be regression test, install test, and need to upgrade the columnstoreClusterTester for debian 9..

"
1567,MCOL-792,MCOL,David Hill,98706,2017-08-14 20:54:19,"debian 9 support added to columnstoreCluster Tester..
",2,"debian 9 support added to columnstoreCluster Tester..
"
1568,MCOL-792,MCOL,David Hill,98709,2017-08-14 22:17:23,regression test looked good... assign to QA to test now,3,regression test looked good... assign to QA to test now
1569,MCOL-792,MCOL,Daniel Lee,98796,2017-08-16 16:58:12,"Build verified: 1.0.11-1

Created QA vagrant box using debian/stretch64 base box for testing.
",4,"Build verified: 1.0.11-1

Created QA vagrant box using debian/stretch64 base box for testing.
"
1570,MCOL-792,MCOL,Daniel Lee,98810,2017-08-16 21:23:24,"For 1.0.11-1 testing,  I tested:

Root installation: single-server, 1um2pm, both deb and binary packages
Nonroot installation: single-server, 1um2pm, binary packages

All tests passed.

For 1.1.0-1, I ran the same tests, al passed except

binary package for non root 1um2pm installation.  It worked for single-server, but not multi-servers.

postConfigure returned these errors:

cannot create /etc/rc.local: Permission denied
cannot create /etc/rc.local: Permission denied

NOTE:  I saw errors on Centos 7 recently and I thought they have been fixed.




",5,"For 1.0.11-1 testing,  I tested:

Root installation: single-server, 1um2pm, both deb and binary packages
Nonroot installation: single-server, 1um2pm, binary packages

All tests passed.

For 1.1.0-1, I ran the same tests, al passed except

binary package for non root 1um2pm installation.  It worked for single-server, but not multi-servers.

postConfigure returned these errors:

cannot create /etc/rc.local: Permission denied
cannot create /etc/rc.local: Permission denied

NOTE:  I saw errors on Centos 7 recently and I thought they have been fixed.




"
1571,MCOL-792,MCOL,Daniel Lee,98811,2017-08-16 21:23:52,reopen per my last comment,6,reopen per my last comment
1572,MCOL-792,MCOL,David Hill,98812,2017-08-16 21:31:52,"Oh, there isnt a /etc/rc.local on debian 9

ll /etc/rc
rc0.d/ rc1.d/ rc2.d/ rc3.d/ rc4.d/ rc5.d/ rc6.d/ rcS.d/ 
",7,"Oh, there isnt a /etc/rc.local on debian 9

ll /etc/rc
rc0.d/ rc1.d/ rc2.d/ rc3.d/ rc4.d/ rc5.d/ rc6.d/ rcS.d/ 
"
1573,MCOL-792,MCOL,Daniel Lee,98814,2017-08-16 21:44:18,"created MCOL-880 to track the multi-node non-root installation issue for 1.1.0.

Closing this ticket for 1.0.11-1
",8,"created MCOL-880 to track the multi-node non-root installation issue for 1.1.0.

Closing this ticket for 1.0.11-1
"
1574,MCOL-802,MCOL,Daniel Lee,97616,2017-07-13 19:47:50,"Builds tested: Github source 1.0.10, 1.1.0

1.0.10-1

[root@localhost mariadb-columnstore-server]# git show
commit 435972e50ee33911ce39696ce101d1cd23ed9c2b
Merge: b1d1ca1 5d3fcfe
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Jul 12 13:07:55 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3501c1a17a920ee765c6255c5a5fd8c64fed7c8e
Author: david hill <david.hill@mariadb.com>
Date:   Wed Jul 12 09:52:28 2017 -0500


1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 8e07495da650d922c4d1f3f09d77382168132b11
Merge: 80e57a8 c27e1e5
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed Jul 12 13:07:42 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit d1386928dcaf1d9acc92ab11e2415c808c75dd49
Author: david hill <david.hill@mariadb.com>
Date:   Thu Jul 13 11:20:08 2017 -0500

[root@localhost ~]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 14
Server version: 10.1.25-MariaDB Columnstore 1.0.10-1
 

1.1.0-1 is showing MariaDB 10.2.6.

",1,"Builds tested: Github source 1.0.10, 1.1.0

1.0.10-1

[root@localhost mariadb-columnstore-server]# git show
commit 435972e50ee33911ce39696ce101d1cd23ed9c2b
Merge: b1d1ca1 5d3fcfe
Author: David.Hall 
Date:   Wed Jul 12 13:07:55 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit 3501c1a17a920ee765c6255c5a5fd8c64fed7c8e
Author: david hill 
Date:   Wed Jul 12 09:52:28 2017 -0500


1.1.0-1

[root@localhost mariadb-columnstore-server]# git show
commit 8e07495da650d922c4d1f3f09d77382168132b11
Merge: 80e57a8 c27e1e5
Author: David.Hall 
Date:   Wed Jul 12 13:07:42 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit d1386928dcaf1d9acc92ab11e2415c808c75dd49
Author: david hill 
Date:   Thu Jul 13 11:20:08 2017 -0500

[root@localhost ~]# mcsmysql mytest
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 14
Server version: 10.1.25-MariaDB Columnstore 1.0.10-1
 

1.1.0-1 is showing MariaDB 10.2.6.

"
1575,MCOL-802,MCOL,Daniel Lee,97617,2017-07-13 19:50:50,Merge applied to both branches?,2,Merge applied to both branches?
1576,MCOL-802,MCOL,Daniel Lee,97620,2017-07-13 20:23:01,"The merge is for 1.0.10 only.  1.1.0 has been removed from the ""Fixed version"" field.",3,"The merge is for 1.0.10 only.  1.1.0 has been removed from the ""Fixed version"" field."
1577,MCOL-809,MCOL,Andrew Hutchings,99680,2017-09-05 14:34:03,Unit tests pass with the changes,1,Unit tests pass with the changes
1578,MCOL-809,MCOL,David Thompson,100146,2017-09-13 20:21:12,looks good,2,looks good
1579,MCOL-813,MCOL,Daniel Lee,98250,2017-08-03 14:42:41,"Build verified: Github source 1.1.0

commit 97284ea4ba429726fe9e3573590464b420501a89
Merge: 4074adf effb08d
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Thu Aug 3 10:14:21 2017 +0100

commit 630b113565a624c5a73438d51f2d3422ff7f2e92
Merge: 606846e 6aeb1bf
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Aug 1 12:29:15 2017 -0500


Server version: 10.2.7-MariaDB-log Columnstore 1.1.0-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
",1,"Build verified: Github source 1.1.0

commit 97284ea4ba429726fe9e3573590464b420501a89
Merge: 4074adf effb08d
Author: Andrew Hutchings 
Date:   Thu Aug 3 10:14:21 2017 +0100

commit 630b113565a624c5a73438d51f2d3422ff7f2e92
Merge: 606846e 6aeb1bf
Author: David.Hall 
Date:   Tue Aug 1 12:29:15 2017 -0500


Server version: 10.2.7-MariaDB-log Columnstore 1.1.0-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
"
1580,MCOL-821,MCOL,Andrew Hutchings,97873,2017-07-21 15:26:14,For testing the changes to the 1.0 regression suite should cover it. It is just API renaming.,1,For testing the changes to the 1.0 regression suite should cover it. It is just API renaming.
1581,MCOL-821,MCOL,David Thompson,97929,2017-07-24 17:07:25,"Specifically what was changed:
- The 2 reference implementations are renamed to use MCS rather than IDB. These are not installed by default.
- Instructions in the readme under utils/udfsdk were updated and clarified.

Regression test is enough to cover.",2,"Specifically what was changed:
- The 2 reference implementations are renamed to use MCS rather than IDB. These are not installed by default.
- Instructions in the readme under utils/udfsdk were updated and clarified.

Regression test is enough to cover."
1582,MCOL-821,MCOL,Daniel Lee,98050,2017-07-27 18:18:57,It passed regression tests.,3,It passed regression tests.
1583,MCOL-825,MCOL,Daniel Lee,100080,2017-09-12 18:58:40,"Build verified: 1.1.0-1

Same as MCOL-821, it passed regression tests.",1,"Build verified: 1.1.0-1

Same as MCOL-821, it passed regression tests."
1584,MCOL-827,MCOL,David Thompson,100502,2017-09-20 22:22:08,"[~GeoffMontee] I think this was already updated, can you confirm and let us know if this can be closed?",1,"[~GeoffMontee] I think this was already updated, can you confirm and let us know if this can be closed?"
1585,MCOL-827,MCOL,Geoff Montee,100504,2017-09-20 22:53:40,"I still do not see the requested information on the ""MariaDB ColumnStore troubleshooting utilities"" section of that page:

https://mariadb.com/kb/en/library/system-troubleshooting-mariadb-columnstore/#mariadb-columnstore-troubleshooting-utilities

It explains that there are tools like ""cleartablelock"", ""viewtablelock"", etc., but it does not say where to run those tools.",2,"I still do not see the requested information on the ""MariaDB ColumnStore troubleshooting utilities"" section of that page:

URL

It explains that there are tools like ""cleartablelock"", ""viewtablelock"", etc., but it does not say where to run those tools."
1586,MCOL-827,MCOL,David Thompson,100506,2017-09-20 23:10:22,these can run from columnstore server but the default  / convention is always to run from pm1. [~hill] - can you update the doc?,3,these can run from columnstore server but the default  / convention is always to run from pm1. [~hill] - can you update the doc?
1587,MCOL-827,MCOL,David Hill,100572,2017-09-21 14:53:43,"Done...I also broken to the utilities in sections to help better explain them and where they should be used.

https://mariadb.com/kb/en/library/system-troubleshooting-mariadb-columnstore/#mariadb-columnstore-utilities

Geoff Montee - please review",4,"Done...I also broken to the utilities in sections to help better explain them and where they should be used.

URL

Geoff Montee - please review"
1588,MCOL-827,MCOL,David Hill,100573,2017-09-21 14:54:17,"update made, please review",5,"update made, please review"
1589,MCOL-827,MCOL,Geoff Montee,100596,2017-09-21 20:06:11,The latest updates are great. Thanks!,6,The latest updates are great. Thanks!
1590,MCOL-829,MCOL,Andrew Hutchings,97931,2017-07-24 18:23:52,"Test case:

{code}
create table mcol829a (a int, b int) engine=columnstore;
create table mcol829b (a int, b int) engine=columnstore;
insert into mcol829a values (1,1),(2,2),(3,3);
delimiter //
create procedure mcol829() begin insert into mcol829b select * from mcol829a; end//
delimiter ;
call mcol829();
{code}

Before the fix this will error with:
{code}
ERROR 1178 (42000): The storage engine for the table doesn't support This stored procedure syntax is not supported by Columnstore in this version
{code}

After the fix it will execute the INSERT...SELECT correctly.",1,"Test case:

{code}
create table mcol829a (a int, b int) engine=columnstore;
create table mcol829b (a int, b int) engine=columnstore;
insert into mcol829a values (1,1),(2,2),(3,3);
delimiter //
create procedure mcol829() begin insert into mcol829b select * from mcol829a; end//
delimiter ;
call mcol829();
{code}

Before the fix this will error with:
{code}
ERROR 1178 (42000): The storage engine for the table doesn't support This stored procedure syntax is not supported by Columnstore in this version
{code}

After the fix it will execute the INSERT...SELECT correctly."
1591,MCOL-829,MCOL,Andrew Hutchings,97932,2017-07-24 18:35:36,Pull requests for 1.0 and 1.1 as well as both versions of the regression suite.,2,Pull requests for 1.0 and 1.1 as well as both versions of the regression suite.
1592,MCOL-829,MCOL,Daniel Lee,97967,2017-07-25 16:16:25,"Builds verified: Github source 1.0.10, 1.1.0

Reproduced the issue in 1.0.9-1.

1.0.10

[root@localhost mariadb-columnstore-server]# git show
commit 6e32a494b4387f3a501bc09addeffacb68eb8e99
Merge: 435972e 87f4873
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Jul 25 00:25:21 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit aa27537874868745ea4086b0a1279191e07779df
Merge: 93794c9 7e568f5
Author: david hill <david.hill@mariadb.com>
Date:   Mon Jul 24 10:09:17 2017 -0500

1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit e475edfb8c267e4a4917a2437bef887bb4c7e630
Merge: 8e07495 81e053d
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Jul 25 00:25:40 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit bbad7882d25578ff6c5ef6df7458b10c37941bc6
Merge: 853eb13 1c032cb
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date:   Mon Jul 24 15:07:27 2017 +0100
",3,"Builds verified: Github source 1.0.10, 1.1.0

Reproduced the issue in 1.0.9-1.

1.0.10

[root@localhost mariadb-columnstore-server]# git show
commit 6e32a494b4387f3a501bc09addeffacb68eb8e99
Merge: 435972e 87f4873
Author: David.Hall 
Date:   Tue Jul 25 00:25:21 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit aa27537874868745ea4086b0a1279191e07779df
Merge: 93794c9 7e568f5
Author: david hill 
Date:   Mon Jul 24 10:09:17 2017 -0500

1.1.0

[root@localhost mariadb-columnstore-server]# git show
commit e475edfb8c267e4a4917a2437bef887bb4c7e630
Merge: 8e07495 81e053d
Author: David.Hall 
Date:   Tue Jul 25 00:25:40 2017 -0500

[root@localhost mariadb-columnstore-engine]# git show
commit bbad7882d25578ff6c5ef6df7458b10c37941bc6
Merge: 853eb13 1c032cb
Author: Andrew Hutchings 
Date:   Mon Jul 24 15:07:27 2017 +0100
"
1593,MCOL-84,MCOL,Dipti Joshi,83808,2016-05-31 06:27:06,[~hill]As discussed on slack - we shall just use the postconfigure installer. ,1,[~hill]As discussed on slack - we shall just use the postconfigure installer. 
1594,MCOL-84,MCOL,Dipti Joshi,84011,2016-06-04 02:36:16,Since postConfigure allows pre-creating and attaching to EBS volume - this is not needed. Hence closing,2,Since postConfigure allows pre-creating and attaching to EBS volume - this is not needed. Hence closing
1595,MCOL-868,MCOL,David Hill,98685,2017-08-14 16:09:33,"Ill take this one.. it does build, will make sure it passes regression test",1,"Ill take this one.. it does build, will make sure it passes regression test"
1596,MCOL-868,MCOL,David Hill,98748,2017-08-15 19:16:03,regression test successful,2,regression test successful
1597,MCOL-868,MCOL,Daniel Lee,98802,2017-08-16 18:57:26,"Build verified 1.0.11-1

Verified correct server and engine versions in all packages for all supported OSs.
",3,"Build verified 1.0.11-1

Verified correct server and engine versions in all packages for all supported OSs.
"
1598,MCOL-887,MCOL,Daniel Lee,99687,2017-09-05 16:05:10,"Build verified: 1.1.0 Github source

/root/columnstore/mariadb-columnstore-server
commit 9e855a6415e0edd6771c449a6591c21c3915bfec
Merge: 6ed33d1 c206e51
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Sep 5 09:43:29 2017 -0500

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 90353b9b908e1c9ee241c4a156a2a377c53cc807
Author: david hill <david.hill@mariadb.com>
Date:   Fri Sep 1 14:46:07 2017 -0500
",1,"Build verified: 1.1.0 Github source

/root/columnstore/mariadb-columnstore-server
commit 9e855a6415e0edd6771c449a6591c21c3915bfec
Merge: 6ed33d1 c206e51
Author: David.Hall 
Date:   Tue Sep 5 09:43:29 2017 -0500

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 90353b9b908e1c9ee241c4a156a2a377c53cc807
Author: david hill 
Date:   Fri Sep 1 14:46:07 2017 -0500
"
1599,MCOL-894,MCOL,Andrew Hutchings,99396,2017-08-29 10:16:51,"This is definitely something that has come up a lot recently. We have some ideas on how to solve this. It is too late to implement this in 1.1 but we could do this for a later version.

There is current a workaround if you are doing ""order by X limit Y"" and that is to wrap it in a subquery such as:

{code:sql}
select * from (select * from t2 order by b limit 4) sq;
{code}

You need the ""limit"" though because ""order by"" will be optimised out without it.",1,"This is definitely something that has come up a lot recently. We have some ideas on how to solve this. It is too late to implement this in 1.1 but we could do this for a later version.

There is current a workaround if you are doing ""order by X limit Y"" and that is to wrap it in a subquery such as:

{code:sql}
select * from (select * from t2 order by b limit 4) sq;
{code}

You need the ""limit"" though because ""order by"" will be optimised out without it."
1600,MCOL-894,MCOL,Andrew Hutchings,136751,2019-10-31 16:29:01,"This has been implemented in 1.2 for subqueries, etc... That effort is tracked in the subtask. It will be ported to 1.4 for all ColumnStore queries which will be tracked here.",2,"This has been implemented in 1.2 for subqueries, etc... That effort is tracked in the subtask. It will be ported to 1.4 for all ColumnStore queries which will be tracked here."
1601,MCOL-894,MCOL,Daniel Lee,137414,2019-11-08 20:56:15,"Build verified: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
bd41ffd

1.4.0-1 beta and 1.4.1-1 is much faster than 1.2.5-1 and 1.2.6-1, using single or multiple thread.

In 1.4.0-1, using multiple also has a great performance gain.

1.4.1-1

MariaDB [tpch10]> set columnstore_orderby_threads=1;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (4.232 sec)

MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|     753413 |  Tiresias  |
|    9714791 |  Tiresias  |
|   58301447 |  Tiresias  |
|       7299 |  Tiresias  |
|      85090 |  Tiresias  |
|    5277956 |  Tiresias  |
|    2570021 |  Tiresias  |
|   44041447 |  Tiresias  |
|    2475204 |  Tiresias  |
|    1141568 |  Tiresias  |
+------------+------------+
10 rows in set (35.206 sec)



MariaDB [tpch10]> set columnstore_orderby_threads=8;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (2.524 sec)

MariaDB [tpch10]> set columnstore_orderby_threads=8;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|   30202439 |  Tiresias  |
|   43662849 |  Tiresias  |
|   46411616 |  Tiresias  |
|   24971398 |  Tiresias  |
|   42581319 |  Tiresias  |
|   39185219 |  Tiresias  |
|   56893315 |  Tiresias  |
|   42593603 |  Tiresias  |
|   16359393 |  Tiresias  |
|   39959232 |  Tiresias  |
+------------+------------+
10 rows in set (16.405 sec)


",3,"Build verified: 1.4.1-1

[dlee@master centos7]$ cat gitversionInfo.txt 
engine commit:
bd41ffd

1.4.0-1 beta and 1.4.1-1 is much faster than 1.2.5-1 and 1.2.6-1, using single or multiple thread.

In 1.4.0-1, using multiple also has a great performance gain.

1.4.1-1

MariaDB [tpch10]> set columnstore_orderby_threads=1;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (4.232 sec)

MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|     753413 |  Tiresias  |
|    9714791 |  Tiresias  |
|   58301447 |  Tiresias  |
|       7299 |  Tiresias  |
|      85090 |  Tiresias  |
|    5277956 |  Tiresias  |
|    2570021 |  Tiresias  |
|   44041447 |  Tiresias  |
|    2475204 |  Tiresias  |
|    1141568 |  Tiresias  |
+------------+------------+
10 rows in set (35.206 sec)



MariaDB [tpch10]> set columnstore_orderby_threads=8;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select l_orderkey from lineitem order by l_orderkey limit 10;
+------------+
| l_orderkey |
+------------+
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          1 |
|          2 |
|          3 |
|          3 |
|          3 |
+------------+
10 rows in set (2.524 sec)

MariaDB [tpch10]> set columnstore_orderby_threads=8;
Query OK, 0 rows affected (0.001 sec)

MariaDB [tpch10]> select l_orderkey, l_comment from lineitem order by l_comment limit 10;
+------------+------------+
| l_orderkey | l_comment  |
+------------+------------+
|   30202439 |  Tiresias  |
|   43662849 |  Tiresias  |
|   46411616 |  Tiresias  |
|   24971398 |  Tiresias  |
|   42581319 |  Tiresias  |
|   39185219 |  Tiresias  |
|   56893315 |  Tiresias  |
|   42593603 |  Tiresias  |
|   16359393 |  Tiresias  |
|   39959232 |  Tiresias  |
+------------+------------+
10 rows in set (16.405 sec)


"
1602,MCOL-894,MCOL,Daniel Lee,137417,2019-11-08 21:09:49,closing ticket per test results for both this ticket and its subtask.,4,closing ticket per test results for both this ticket and its subtask.
1603,MCOL-9,MCOL,Dipti Joshi,83347,2016-05-10 23:29:22,[~greenman]  has created initial KB pages at https://mariadb.com/kb/en/mariadb/columnstore-sql-structure-and-commands/ based on earlier GPL version of syntax guide,1,[~greenman]  has created initial KB pages at URL based on earlier GPL version of syntax guide
1604,MCOL-9,MCOL,Dipti Joshi,84002,2016-06-04 01:05:32,This is in review for alpha and hence closing it,2,This is in review for alpha and hence closing it
1605,MCOL-904,MCOL,David Thompson,100147,2017-09-13 20:24:16,Verified this exists and functions. Covered by regression tests,1,Verified this exists and functions. Covered by regression tests
1606,MCOL-905,MCOL,Andrew Hutchings,99699,2017-09-05 20:43:13,Fixed in develop,1,Fixed in develop
1607,MCOL-905,MCOL,David Hill,99700,2017-09-05 20:46:58,also tested by DH...,2,also tested by DH...
1608,MCOL-940,MCOL,Daniel Lee,104046,2017-12-04 21:06:32,"This ticket has been superseded by MCOL-1032, merge for 10.1.29.
",1,"This ticket has been superseded by MCOL-1032, merge for 10.1.29.
"
1609,MCOL-942,MCOL,David Hill,100817,2017-09-27 19:33:49,"might have jumped the gun on master/slave is setup after reboot. I get the following error

CREATE TABLE `brptab1` ( `id` int(11) NOT NULL, `name` varchar(100) NOT NULL ) ENGINE=InfiniDB DEFAULT CHARSET=utf8;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-4016: DML and DDL statements for Columnstore tables can only be run from the replication master.
MariaDB [david]> 
MariaDB [david]> 
MariaDB [david]> show master status;
+------------------+----------+--------------+------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+------------------+
| mysql-bin.000007 |      327 |              |                  |
+------------------+----------+--------------+------------------+
1 row in set (0.00 sec)

MariaDB [david]> 
",1,"might have jumped the gun on master/slave is setup after reboot. I get the following error

CREATE TABLE `brptab1` ( `id` int(11) NOT NULL, `name` varchar(100) NOT NULL ) ENGINE=InfiniDB DEFAULT CHARSET=utf8;
ERROR 1178 (42000): The storage engine for the table doesn't support IDB-4016: DML and DDL statements for Columnstore tables can only be run from the replication master.
MariaDB [david]> 
MariaDB [david]> 
MariaDB [david]> show master status;
+------------------+----------+--------------+------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
+------------------+----------+--------------+------------------+
| mysql-bin.000007 |      327 |              |                  |
+------------------+----------+--------------+------------------+
1 row in set (0.00 sec)

MariaDB [david]> 
"
1610,MCOL-942,MCOL,David Hill,103740,2017-11-28 19:22:18,"testing - should see the distribute to pm only on local-query and combo um/pm installs. you should see this on separate install when local-query is disabled

Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.283781 |0|0|0| D 18 CAL0000: runMasterDist function called
Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.284224 |0|0|0| D 18 CAL0000: cmd = /usr/local/mariadb/columnstore/bin/rsync.sh 172.30.0.161 'Calpont1' /usr/local/mariadb/columnstore 1 > /tmp/master-dist_pm1.log
Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.680937 |0|0|0| D 18 CAL0000: runMasterDist: Success rsync to module: pm1
Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.680990 |0|0|0| D 18 CAL0000: cmd = /usr/local/mariadb/columnstore/bin/rsync.sh 172.30.0.152 'Calpont1' /usr/local/mariadb/columnstore 1 > /tmp/master-dist_pm2.log
Nov 28 17:26:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.966580 |0|0|0| D 18 CAL0000: runMasterDist: Success rsync to module: pm2

and verified the server-id in the different my.cnf files are setup with um1 or active um being '1'.

",2,"testing - should see the distribute to pm only on local-query and combo um/pm installs. you should see this on separate install when local-query is disabled

Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.283781 |0|0|0| D 18 CAL0000: runMasterDist function called
Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.284224 |0|0|0| D 18 CAL0000: cmd = /usr/local/mariadb/columnstore/bin/rsync.sh 172.30.0.161 'Calpont1' /usr/local/mariadb/columnstore 1 > /tmp/master-dist_pm1.log
Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.680937 |0|0|0| D 18 CAL0000: runMasterDist: Success rsync to module: pm1
Nov 28 17:25:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.680990 |0|0|0| D 18 CAL0000: cmd = /usr/local/mariadb/columnstore/bin/rsync.sh 172.30.0.152 'Calpont1' /usr/local/mariadb/columnstore 1 > /tmp/master-dist_pm2.log
Nov 28 17:26:10 ip-172-30-0-59 ProcessMonitor[13973]: 10.966580 |0|0|0| D 18 CAL0000: runMasterDist: Success rsync to module: pm2

and verified the server-id in the different my.cnf files are setup with um1 or active um being '1'.

"
1611,MCOL-942,MCOL,David Hill,103741,2017-11-28 19:22:41,https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/331,3,URL
1612,MCOL-942,MCOL,Ben Thompson,103747,2017-11-28 21:37:03,Reviewed / Merged,4,Reviewed / Merged
1613,MCOL-942,MCOL,Daniel Lee,104120,2017-12-05 22:33:18,"Build verified: GitHub source

1.0.12-1

[root@localhost ~]# cat mariadb-columnstore-1.0.12-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 25e9d054cd3d05683fade1b974e1730316d256ed
Merge: 89b2ea1 7c52a83
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Nov 21 10:49:11 2017 -0600

    Merge pull request #79 from mariadb-corporation/MCOL-954-1.0
    
    MCOL-954 Init vtable state

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b112e826a2793228f5f3c1312fec5291fc1d8bf5
Merge: 7c2640f b657938
Author: David.Hall <david.hall@mariadb.com>
Date:   Fri Dec 1 16:17:28 2017 -0600

    Merge pull request #338 from mariadb-corporation/MCOL-1068
    
    MCOL-1068 Improve compression_ratio() procedure


Verified both 1um2pm localquery and 2pm combo systems.  Initial installation and restart system.",5,"Build verified: GitHub source

1.0.12-1

[root@localhost ~]# cat mariadb-columnstore-1.0.12-1-centos7.x86_64.bin.tar.txt
/root/columnstore/mariadb-columnstore-server
commit 25e9d054cd3d05683fade1b974e1730316d256ed
Merge: 89b2ea1 7c52a83
Author: David.Hall 
Date:   Tue Nov 21 10:49:11 2017 -0600

    Merge pull request #79 from mariadb-corporation/MCOL-954-1.0
    
    MCOL-954 Init vtable state

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit b112e826a2793228f5f3c1312fec5291fc1d8bf5
Merge: 7c2640f b657938
Author: David.Hall 
Date:   Fri Dec 1 16:17:28 2017 -0600

    Merge pull request #338 from mariadb-corporation/MCOL-1068
    
    MCOL-1068 Improve compression_ratio() procedure


Verified both 1um2pm localquery and 2pm combo systems.  Initial installation and restart system."
1614,MCOL-946,MCOL,Andrew Hutchings,101251,2017-10-06 14:32:16,New pull request to fix for Python 3. All looks good apart from that,1,New pull request to fix for Python 3. All looks good apart from that
1615,MCOL-962,MCOL,David Thompson,101362,2017-10-10 03:14:59,"A nagios plugin compatible shell script wrapper is here:
https://github.com/mariadb-corporation/mariadb-columnstore-tools/tree/master/monitoring

However i agree that for certain use cases a SQL based approach might be easier to consume. I suspect a udf / stored proc would be the most natural / easy to implement.",1,"A nagios plugin compatible shell script wrapper is here:
URL

However i agree that for certain use cases a SQL based approach might be easier to consume. I suspect a udf / stored proc would be the most natural / easy to implement."
1616,MCOL-962,MCOL,David Thompson,104329,2017-12-11 19:58:25,This won't make 1.0.12,2,This won't make 1.0.12
1617,MCOL-962,MCOL,David Hall,105431,2018-01-09 19:01:49,"Added the functions mcssystemready() and mcssystemreadonly()

mcssystemready() tests if the system is ACTIVE as shown in getsysteminfo, if the DBRM is READY, and if the system is ready for queries (which means DMLProc is done with rollback). These are three independent statuses. All must be correct for systemready() to return 1.

There is an undocumented feature in mcsadmin setsystemqueryready to test this state.

Pass 1 to set the sytsem as query ready and 0 to make it not ready. 

MariaDB [(none)]> select mcssystemready();
+------------------+
| mcssystemready() |
+------------------+
|                1 |
+------------------+
1 row in set (0.01 sec)


ma setsystemqueryready 0

MariaDB [(none)]> select mcssystemready();
+------------------+
| mcssystemready() |
+------------------+
|                0 |
+------------------+
1 row in set (0.01 sec)


ma setsystemqueryready 0

Also, restarting while a large insert is taking place, causing a big rollback during the start is also a good way to test.


To test mcssystemreadonly(), use the utility dbrmctl to set the system into readonly mode:

ariaDB [(none)]> select mcssystemreadonly();
+---------------------+
| mcssystemreadonly() |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.01 sec)


[root@local]#mariadb/columnstore/bin/dbrmctl readonly
OK.

MariaDB [(none)]> select mcssystemreadonly();
+---------------------+
| mcssystemreadonly() |
+---------------------+
|                   1 |
+---------------------+
1 row in set (0.01 sec)



[root@local]# mariadb/columnstore/bin/dbrmctl readwrite
OK.
",3,"Added the functions mcssystemready() and mcssystemreadonly()

mcssystemready() tests if the system is ACTIVE as shown in getsysteminfo, if the DBRM is READY, and if the system is ready for queries (which means DMLProc is done with rollback). These are three independent statuses. All must be correct for systemready() to return 1.

There is an undocumented feature in mcsadmin setsystemqueryready to test this state.

Pass 1 to set the sytsem as query ready and 0 to make it not ready. 

MariaDB [(none)]> select mcssystemready();
+------------------+
| mcssystemready() |
+------------------+
|                1 |
+------------------+
1 row in set (0.01 sec)


ma setsystemqueryready 0

MariaDB [(none)]> select mcssystemready();
+------------------+
| mcssystemready() |
+------------------+
|                0 |
+------------------+
1 row in set (0.01 sec)


ma setsystemqueryready 0

Also, restarting while a large insert is taking place, causing a big rollback during the start is also a good way to test.


To test mcssystemreadonly(), use the utility dbrmctl to set the system into readonly mode:

ariaDB [(none)]> select mcssystemreadonly();
+---------------------+
| mcssystemreadonly() |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.01 sec)


[root@local]#mariadb/columnstore/bin/dbrmctl readonly
OK.

MariaDB [(none)]> select mcssystemreadonly();
+---------------------+
| mcssystemreadonly() |
+---------------------+
|                   1 |
+---------------------+
1 row in set (0.01 sec)



[root@local]# mariadb/columnstore/bin/dbrmctl readwrite
OK.
"
1618,MCOL-962,MCOL,David Hall,105432,2018-01-09 19:02:18,Do we want to add a similar function for writes suspended?,4,Do we want to add a similar function for writes suspended?
1619,MCOL-962,MCOL,David Thompson,105435,2018-01-09 21:36:03,if we have systemReadOnly() wouldn't that returning true indicate that writes are suspended?,5,if we have systemReadOnly() wouldn't that returning true indicate that writes are suspended?
1620,MCOL-962,MCOL,David Hall,105437,2018-01-09 22:19:55,"Readonly is caused by an operating fault. It's there to prevent writes to a potentially broken data set.

Writes Suspended is a state the user puts the system in when doing maintenance to prevent writes during the maintenance window since they could be harmful.

The two are not really related. ",6,"Readonly is caused by an operating fault. It's there to prevent writes to a potentially broken data set.

Writes Suspended is a state the user puts the system in when doing maintenance to prevent writes during the maintenance window since they could be harmful.

The two are not really related. "
1621,MCOL-962,MCOL,David Hall,105441,2018-01-09 23:18:49,Added mcsWritesSuspended() which returns 1 only if mcsadmin suspendDataBaseWrites is active.,7,Added mcsWritesSuspended() which returns 1 only if mcsadmin suspendDataBaseWrites is active.
1622,MCOL-962,MCOL,David Hall,105442,2018-01-09 23:20:21,"The function names are easy enough to change before we publish. It's very easy to add functions from the functionality found in mcsadmin that return a single value. With a little effort, set functions could be added.",8,"The function names are easy enough to change before we publish. It's very easy to add functions from the functionality found in mcsadmin that return a single value. With a little effort, set functions could be added."
1623,MCOL-962,MCOL,Ben Thompson,105753,2018-01-16 18:33:58,"Review / Merged

Probably should check out addmodule command with glusterFS storage in testing.",9,"Review / Merged

Probably should check out addmodule command with glusterFS storage in testing."
1624,MCOL-962,MCOL,Daniel Lee,105937,2018-01-19 23:00:48,"Build verified: Github source 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Fri Jan 19 12:39:05 2018 -0600
Merge pull request #84 from mariadb-corporation/MCOL-1159
MCOL-1159 Merge mariadb-10.2.12
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 201813d63d63e99e50f2474d1bf7d8428ac72119
Merge: 3748036 a002d33
Author: Andrew Hutchings <andrew@linuxjedi.co.uk>
Date: Fri Jan 19 19:40:18 2018 +0000
Merge pull request #373 from mariadb-corporation/MergeFix
Merge deleted change to include columnstoreversion.h


1.  mcssystemready() worked correctly

2. setsystemqueryready returns more like debug values, not meant to end users

mcsadmin> setsystemqueryready
setsystemqueryready   Fri Jan 19 22:50:57 2018

Enter 1 for set and 0 for clear
           Please enter: 0

getSystemQueryReady = 64

setSystemQueryReady = 0

getSystemQueryReady = 0
mcsadmin> setsystemqueryready 1
setsystemqueryready   Fri Jan 19 22:51:06 2018

getSystemQueryReady = 0

setSystemQueryReady = 1

getSystemQueryReady = 64

The command is also not in the help menu.

3. Having both mcssystemreadonly() and mcswritessuspended() is confusing.
   There maybe a difference between system ready only and write suspended in terms of internal status or programming sense.  The net effect is that the database is in ready mode.  This SQL client function is user facing and i think we need to have just one of them.
   
   The following is confusing:
   
MariaDB [mytest]> select mcssystemreadonly();
+---------------------+
| mcssystemreadonly() |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.01 sec)

MariaDB [mytest]> create table t1 (c1 int) engine=columnstore;
ERROR 1815 (HY000): Internal error: CAL0002: Writing to the database is disabled. 
MariaDB [mytest]> select mcsWritesSuspended();
+----------------------+
| mcsWritesSuspended() |
+----------------------+
|                    1 |
+----------------------+
1 row in set (0.00 sec)

or vise versa.
",10,"Build verified: Github source 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e0ae0d2fecf9941887478d9aa669c8b2d1092090
Merge: 21ec501 2490ddf
Author: benthompson15 
Date: Fri Jan 19 12:39:05 2018 -0600
Merge pull request #84 from mariadb-corporation/MCOL-1159
MCOL-1159 Merge mariadb-10.2.12
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 201813d63d63e99e50f2474d1bf7d8428ac72119
Merge: 3748036 a002d33
Author: Andrew Hutchings 
Date: Fri Jan 19 19:40:18 2018 +0000
Merge pull request #373 from mariadb-corporation/MergeFix
Merge deleted change to include columnstoreversion.h


1.  mcssystemready() worked correctly

2. setsystemqueryready returns more like debug values, not meant to end users

mcsadmin> setsystemqueryready
setsystemqueryready   Fri Jan 19 22:50:57 2018

Enter 1 for set and 0 for clear
           Please enter: 0

getSystemQueryReady = 64

setSystemQueryReady = 0

getSystemQueryReady = 0
mcsadmin> setsystemqueryready 1
setsystemqueryready   Fri Jan 19 22:51:06 2018

getSystemQueryReady = 0

setSystemQueryReady = 1

getSystemQueryReady = 64

The command is also not in the help menu.

3. Having both mcssystemreadonly() and mcswritessuspended() is confusing.
   There maybe a difference between system ready only and write suspended in terms of internal status or programming sense.  The net effect is that the database is in ready mode.  This SQL client function is user facing and i think we need to have just one of them.
   
   The following is confusing:
   
MariaDB [mytest]> select mcssystemreadonly();
+---------------------+
| mcssystemreadonly() |
+---------------------+
|                   0 |
+---------------------+
1 row in set (0.01 sec)

MariaDB [mytest]> create table t1 (c1 int) engine=columnstore;
ERROR 1815 (HY000): Internal error: CAL0002: Writing to the database is disabled. 
MariaDB [mytest]> select mcsWritesSuspended();
+----------------------+
| mcsWritesSuspended() |
+----------------------+
|                    1 |
+----------------------+
1 row in set (0.00 sec)

or vise versa.
"
1625,MCOL-962,MCOL,David Hall,106558,2018-01-30 22:26:45,"Modified mcssystemreadonly to return 0 if writable, 1 if suspended (user action) and 2 if dbrm is read only (error condition)",11,"Modified mcssystemreadonly to return 0 if writable, 1 if suspended (user action) and 2 if dbrm is read only (error condition)"
1626,MCOL-962,MCOL,Ben Thompson,106835,2018-02-05 21:56:24,Reviewed / Merged,12,Reviewed / Merged
1627,MCOL-962,MCOL,Daniel Lee,107000,2018-02-07 19:33:40,"Build verified: Github source 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e5499e513d88a3dfefbe9a356e20a1bceb1bde38
Merge: 99cdb0a 4840a43
Author: david hill <david.hill@mariadb.com>
Date: Wed Jan 31 16:53:52 2018 -0600
Merge pull request #92 from mariadb-corporation/MCOL-1152
MCOL-1152: Change the debian package names.
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ffcc4e94ccd07b6bbdfc47531c4c2f065f3fab47
Merge: 05431bf 3fb5dde
Author: benthompson15 <ben.thompson@mariadb.com>
Date: Mon Feb 5 22:37:05 2018 -0600
Merge pull request #398 from mariadb-corporation/statuscontrol-fix
add ProcStatusControl to postConfigure setup

As today's Slack discussion concluded, these commands are not for human consumption.  They are for tools and processes to interact with.  Skipping testing for user friendliness.

",13,"Build verified: Github source 1.1.3-1
/root/columnstore/mariadb-columnstore-server
commit e5499e513d88a3dfefbe9a356e20a1bceb1bde38
Merge: 99cdb0a 4840a43
Author: david hill 
Date: Wed Jan 31 16:53:52 2018 -0600
Merge pull request #92 from mariadb-corporation/MCOL-1152
MCOL-1152: Change the debian package names.
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit ffcc4e94ccd07b6bbdfc47531c4c2f065f3fab47
Merge: 05431bf 3fb5dde
Author: benthompson15 
Date: Mon Feb 5 22:37:05 2018 -0600
Merge pull request #398 from mariadb-corporation/statuscontrol-fix
add ProcStatusControl to postConfigure setup

As today's Slack discussion concluded, these commands are not for human consumption.  They are for tools and processes to interact with.  Skipping testing for user friendliness.

"
1628,MCOL-978,MCOL,Andrew Hutchings,104400,2017-12-12 21:18:20,"For QA:

{code:sql}
set global query_cache_type=on;
set query_cache_type=on;
set global query_cache_size=1024*1024;
set infinidb_vtable_mode=0;
{code}

Execute a query against a ColumnStore table.

{code:sql}
show status like 'Qcache_queries_in_cache';
{code}

Before the patch this will equal 1. After the patch it will equal 0.",1,"For QA:

{code:sql}
set global query_cache_type=on;
set query_cache_type=on;
set global query_cache_size=1024*1024;
set infinidb_vtable_mode=0;
{code}

Execute a query against a ColumnStore table.

{code:sql}
show status like 'Qcache_queries_in_cache';
{code}

Before the patch this will equal 1. After the patch it will equal 0."
1629,MCOL-978,MCOL,Daniel Lee,110886,2018-05-11 19:11:07,"Build verified: 1.2.0 source

/root/columnstore/mariadb-columnstore-server
commit 4334641df0df040e0f53332c11dc8e29dc34b8b7
Merge: 960853c cd5e845
Author: David.Hall <david.hall@mariadb.com>
Date:   Mon Apr 9 13:23:44 2018 -0500

    Merge pull request #107 from mariadb-corporation/dev-merge-up-20180409
    
    Dev merge up 20180409

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit d8d386b960cbfa77d3077fc84f31e076f867efaa
Merge: 24b8c79 305bae1
Author: David.Hall <david.hall@mariadb.com>
Date:   Wed May 9 15:13:47 2018 -0500

    Merge pull request #468 from mariadb-corporation/MCOL-1402
    
    MCOL-1402 Fix addtime/subtime



",2,"Build verified: 1.2.0 source

/root/columnstore/mariadb-columnstore-server
commit 4334641df0df040e0f53332c11dc8e29dc34b8b7
Merge: 960853c cd5e845
Author: David.Hall 
Date:   Mon Apr 9 13:23:44 2018 -0500

    Merge pull request #107 from mariadb-corporation/dev-merge-up-20180409
    
    Dev merge up 20180409

/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit d8d386b960cbfa77d3077fc84f31e076f867efaa
Merge: 24b8c79 305bae1
Author: David.Hall 
Date:   Wed May 9 15:13:47 2018 -0500

    Merge pull request #468 from mariadb-corporation/MCOL-1402
    
    MCOL-1402 Fix addtime/subtime



"
1630,MCOL-982,MCOL,Andrew Hutchings,101794,2017-10-23 16:17:50,Pull request for develop and develop-1.1 trees,1,Pull request for develop and develop-1.1 trees
1631,MCOL-982,MCOL,Daniel Lee,101853,2017-10-24 19:59:04,"Build verified: Github source

/root/columnstore/mariadb-columnstore-server
commit f6cd94ea167789970db7b5b501569a6549495d10
Merge: 3d846d3 91b2553
Author: David.Hall <david.hall@mariadb.com>
Date:   Tue Oct 24 09:15:58 2017 -0500

    Merge pull request #72 from mariadb-corporation/MCOL-982
    
    MCOL-982 Merge MariaDB 10.2.9
       
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 751f9fbd2f26026983915a0677d6d600be273073
Author: david hill <david.hill@mariadb.com>
Date:   Tue Oct 24 14:05:48 2017 -0500

    removed duplicaue entries


[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.9-MariaDB-log Columnstore 1.1.1-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
",2,"Build verified: Github source

/root/columnstore/mariadb-columnstore-server
commit f6cd94ea167789970db7b5b501569a6549495d10
Merge: 3d846d3 91b2553
Author: David.Hall 
Date:   Tue Oct 24 09:15:58 2017 -0500

    Merge pull request #72 from mariadb-corporation/MCOL-982
    
    MCOL-982 Merge MariaDB 10.2.9
       
/root/columnstore/mariadb-columnstore-server/mariadb-columnstore-engine
commit 751f9fbd2f26026983915a0677d6d600be273073
Author: david hill 
Date:   Tue Oct 24 14:05:48 2017 -0500

    removed duplicaue entries


[root@localhost ~]# mcsmysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 19
Server version: 10.2.9-MariaDB-log Columnstore 1.1.1-1

Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
"
1632,MCOL-987,MCOL,Costin Stefan,124635,2019-03-12 13:23:43,"Are there any chances to implement this feature in near future?

We are evaluating MariaDB ColumnStore engine and found the compression ratio of the snappy algorithm a ""no go"".
For our data, the compression ratio is less than two (1.5 to 1.3). 

Thank you in advance,

Costin",1,"Are there any chances to implement this feature in near future?

We are evaluating MariaDB ColumnStore engine and found the compression ratio of the snappy algorithm a ""no go"".
For our data, the compression ratio is less than two (1.5 to 1.3). 

Thank you in advance,

Costin"
1633,MCOL-987,MCOL,Denis Khalikov,184350,2021-03-31 09:56:12,"First pull request is added for review https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1837
It modifies current compression interface to be able to add new compression algo.
The next one is LZ4 itself.",2,"First pull request is added for review URL
It modifies current compression interface to be able to add new compression algo.
The next one is LZ4 itself."
1634,MCOL-987,MCOL,Denis Khalikov,185008,2021-04-06 18:51:18,"Final version on review: https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1842
Also added tests to compare the compression ratio `snappy` vs `lz4` for different input data.

On top of this patch added a commit https://github.com/mariadb-corporation/mariadb-columnstore-engine/pull/1842/commits/5d3d766e4bc55cc018632178cb4cf1256f47842d which set LZ4 as default to trigger all tests suite under LZ4 compresison, this commit should be removed before merging",3,"Final version on review: URL
Also added tests to compare the compression ratio `snappy` vs `lz4` for different input data.

On top of this patch added a commit URL which set LZ4 as default to trigger all tests suite under LZ4 compresison, this commit should be removed before merging"
1635,MCOL-987,MCOL,Roman,193748,2021-07-07 10:14:57,4QA JFYI the output of call columnstore_info.compression_ratio() has changed.,4,4QA JFYI the output of call columnstore_info.compression_ratio() has changed.
1636,MCOL-987,MCOL,Daniel Lee,193831,2021-07-07 19:05:57,"Build verified: 6.6.1 (#2742)

MariaDB [mytest]> call columnstore_info.compression_ratio();
+--------------------+-------------------+
| compression_method | compression_ratio |
+--------------------+-------------------+
| Snappy             | 2.2256:1          |
| LZ4                | 0.5288:1          |
+--------------------+-------------------+

Also tested default value, as well as setting new value, for the columnstore_compression_type variable.",5,"Build verified: 6.6.1 (#2742)

MariaDB [mytest]> call columnstore_info.compression_ratio();
+--------------------+-------------------+
| compression_method | compression_ratio |
+--------------------+-------------------+
| Snappy             | 2.2256:1          |
| LZ4                | 0.5288:1          |
+--------------------+-------------------+

Also tested default value, as well as setting new value, for the columnstore_compression_type variable."
1637,MCOL-990,MCOL,Andrew Hutchings,102262,2017-10-31 11:11:00,"Pull request for resetRow(), test case and documentation.",1,"Pull request for resetRow(), test case and documentation."
1638,MCOL-990,MCOL,David Thompson,102298,2017-11-01 00:37:29,Verified test case and also that can use this in tweepy streaming example.,2,Verified test case and also that can use this in tweepy streaming example.
1639,MCOL-992,MCOL,David Thompson,102152,2017-10-30 00:13:56,"Add Java binding support to mcsapi.  Required 2 changes to c++ api to make easier to use:
- use const std:string& for return types as java binding can't handle these due to java String being immutable.
- Made ColumnStoreDateTime int part constructor use uint32 even for values which are really uint8_t as this avoids the need for ugly (short) casts in java.

Neither have impact on test cases or python bindings.

See pull request: https://github.com/mariadb-corporation/mariadb-columnstore-api/pull/12

Also see Readme.md for details, obviously you will need a JDK installed, i tested with 8 on centos7. The build uses gradle for the java side which also assists in pulling down junit and mariadb connector jar deps for the tests. This uses the gradle wrapper checked in which will automatically install gradle dependencies if not already present on the box.",1,"Add Java binding support to mcsapi.  Required 2 changes to c++ api to make easier to use:
- use const std:string& for return types as java binding can't handle these due to java String being immutable.
- Made ColumnStoreDateTime int part constructor use uint32 even for values which are really uint8_t as this avoids the need for ugly (short) casts in java.

Neither have impact on test cases or python bindings.

See pull request: URL

Also see Readme.md for details, obviously you will need a JDK installed, i tested with 8 on centos7. The build uses gradle for the java side which also assists in pulling down junit and mariadb connector jar deps for the tests. This uses the gradle wrapper checked in which will automatically install gradle dependencies if not already present on the box."
1640,MCOL-992,MCOL,Andrew Hutchings,102236,2017-10-30 22:27:32,Moved to test but keeping me as assignee as I'm probably best to QA it.,2,Moved to test but keeping me as assignee as I'm probably best to QA it.
1641,MDEV-10059,MDEV,Vicențiu Ciorbaru,86149,2016-09-06 13:15:26,"While refactoring code for this issue, I have reduced the algorithms that we have for computing window functions to only one. Stream-able functions such as RANK and ROW_NUMBER are now computed by making use of cursors, instead of a specific construct just for them. Performance wise they behave the same as before.",1,"While refactoring code for this issue, I have reduced the algorithms that we have for computing window functions to only one. Stream-able functions such as RANK and ROW_NUMBER are now computed by making use of cursors, instead of a specific construct just for them. Performance wise they behave the same as before."
1642,MDEV-10059,MDEV,Vicențiu Ciorbaru,86412,2016-09-13 13:50:31,"Implemented with:

[https://github.com/MariaDB/server/commit/23e8b508a00a23653da436519371943487ad6fe4]",2,"Implemented with:

[URL"
1643,MDEV-10084,MDEV,Oleksandr Byelkin,84671,2016-06-29 14:29:55,Even on mysql-test test 8% speedup visible.,1,Even on mysql-test test 8% speedup visible.
1644,MDEV-10084,MDEV,Oleksandr Byelkin,84674,2016-06-29 15:24:21,"revision-id: 85d3bb41f209433f6d517cc00aa84685e9d7a75e (mariadb-10.2.0-93-g85d3bb4)
parent(s): 8bec9746f0d5b363f385713035ca3f2daff34e1c
committer: Oleksandr Byelkin
timestamp: 2016-06-29 16:29:06 +0200
message:

MDEV-10084: SQL batch united response

Send OK of SQL batch in one packet

---",2,"revision-id: 85d3bb41f209433f6d517cc00aa84685e9d7a75e (mariadb-10.2.0-93-g85d3bb4)
parent(s): 8bec9746f0d5b363f385713035ca3f2daff34e1c
committer: Oleksandr Byelkin
timestamp: 2016-06-29 16:29:06 +0200
message:

MDEV-10084: SQL batch united response

Send OK of SQL batch in one packet

---"
1645,MDEV-10084,MDEV,Vladislav Vaintroub,84743,2016-07-04 12:07:35,"Looks good.  Suggest to change change comment slightly to reflect the change, like

Send multiple OKs for SQL batch in one go
or

Reduce number of network send() calls for batch update requests",3,"Looks good.  Suggest to change change comment slightly to reflect the change, like

Send multiple OKs for SQL batch in one go
or

Reduce number of network send() calls for batch update requests"
1646,MDEV-10134,MDEV,VAROQUI Stephane,84171,2016-06-10 14:02:55,"Default for the primary key should affect last_insert_id a long awaiting feature is id BIGINT UNSIGNED DEFAULT UUID_SHORT()  see MDEV-6445
",1,"Default for the primary key should affect last_insert_id a long awaiting feature is id BIGINT UNSIGNED DEFAULT UUID_SHORT()  see MDEV-6445
"
1647,MDEV-10138,MDEV,Elena Stepanova,84403,2016-06-20 17:17:32,"[~monty], [~serg],
What is expected to happen upon migration from a long-decimal version of MariaDB to a non-long-decimal version (be it previous MariaDB or MySQL)? ",1,"[~monty], [~serg],
What is expected to happen upon migration from a long-decimal version of MariaDB to a non-long-decimal version (be it previous MariaDB or MySQL)? "
1648,MDEV-10138,MDEV,Michael Widenius,84428,2016-06-21 11:49:11,"The migration issue will be fixed as part of the extending default handling task (MDEV-10134).
As part of this, the .frm will be marked as incompatible (version 11) if more than 30 decimals is used and one will get an error in the earlier MariaDB version if one tries to use it.

",2,"The migration issue will be fixed as part of the extending default handling task (MDEV-10134).
As part of this, the .frm will be marked as incompatible (version 11) if more than 30 decimals is used and one will get an error in the earlier MariaDB version if one tries to use it.

"
1649,MDEV-10138,MDEV,Michael Widenius,84533,2016-06-24 08:18:31,Pushed into 10.2,3,Pushed into 10.2
1650,MDEV-10141,MDEV,Michael Widenius,93493,2017-03-26 21:28:28,This is pushed into bb-10.2-compatibility and will soon be in the 10.3 tree,1,This is pushed into bb-10.2-compatibility and will soon be in the 10.3 tree
1651,MDEV-10141,MDEV,Oleksandr Byelkin,97180,2017-07-02 18:25:56,ALL is not standard option.,2,ALL is not standard option.
1652,MDEV-10142,MDEV,Mark Callaghan,86032,2016-09-01 05:22:38,Don't forget Oracle-style cursor isolation. You need read committed and more. Oracle has special and useful semantics for read committed -> https://github.com/mdcallag/mytools/wiki/Cursor-Isolation,1,Don't forget Oracle-style cursor isolation. You need read committed and more. Oracle has special and useful semantics for read committed -> URL
1653,MDEV-10296,MDEV,Sergey Vojtovich,84667,2016-06-29 12:51:48,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
1654,MDEV-10296,MDEV,Sergey Vojtovich,85358,2016-08-03 07:14:46,Waiting for feedback.,2,Waiting for feedback.
1655,MDEV-10296,MDEV,Sergey Vojtovich,86316,2016-09-08 12:09:15,"[~serg], please review 2 last patches for this task.

I'm still not happy with autosizing:
- numbers used for autosizing are valid for my host, not sure if they'll work properly for others
- we do trylock() and then lock() rather often: up to 30% of cases (performance concern)
- additional code on rather a hot path (performance concern)
- I couldn't get perfect 3 instances for my host with autosizing: it either gets 2 or raising number of instances up to limit (everything under 480 for waits)
- we can't avoid warm-up (bad for benchmarks)",3,"[~serg], please review 2 last patches for this task.

I'm still not happy with autosizing:
- numbers used for autosizing are valid for my host, not sure if they'll work properly for others
- we do trylock() and then lock() rather often: up to 30% of cases (performance concern)
- additional code on rather a hot path (performance concern)
- I couldn't get perfect 3 instances for my host with autosizing: it either gets 2 or raising number of instances up to limit (everything under 480 for waits)
- we can't avoid warm-up (bad for benchmarks)"
1656,MDEV-10296,MDEV,Sergei Golubchik,86338,2016-09-09 07:37:04,"bq. we do trylock() and then lock() rather often: up to 30% of cases (performance concern)
You increase the number of instances when lock/trylock ratio reaches 50%. May be you should do it earlier? At 30%, may be?
bq. additional code on rather a hot path (performance concern)
That should normally be just {{++mutex_nowaits}}, shouldn't it?
bq. I couldn't get perfect 3 instances for my host with autosizing: it either gets 2 or raising number of instances up to limit (everything under 480 for waits)
Interesting. Why would you think is that? What did you do in your benchmarks? You've never had only 1 instance?
bq. we can't avoid warm-up (bad for benchmarks)
True. How long a warm-up is needed, what was your impression?
Anyway, any proper benchmark does a warm-up anyway, so it this your warm-up with shorter than what benchmarks typically do, it should be fine.",4,"bq. we do trylock() and then lock() rather often: up to 30% of cases (performance concern)
You increase the number of instances when lock/trylock ratio reaches 50%. May be you should do it earlier? At 30%, may be?
bq. additional code on rather a hot path (performance concern)
That should normally be just {{++mutex_nowaits}}, shouldn't it?
bq. I couldn't get perfect 3 instances for my host with autosizing: it either gets 2 or raising number of instances up to limit (everything under 480 for waits)
Interesting. Why would you think is that? What did you do in your benchmarks? You've never had only 1 instance?
bq. we can't avoid warm-up (bad for benchmarks)
True. How long a warm-up is needed, what was your impression?
Anyway, any proper benchmark does a warm-up anyway, so it this your warm-up with shorter than what benchmarks typically do, it should be fine."
1657,MDEV-10296,MDEV,Sergey Vojtovich,86339,2016-09-09 08:51:12,"Yes, number of instances is increased at 50%. As I mentioned, if I increase it at 48%, number of instances is quickly raising up to the limit.

It's a bit more than ++mutex_nowaits, but close enough.

It was multi-table OLTP RO benchmark with 40 threads. I had 1 instance initially.

With current numbers warm-up up to 2 instances takes under 5 seconds. With lower numbers it was raising up to the limit in under 1 minute.

There's another option, but it's a bit more expensive: count number of waiting threads and activate instances when there're e.g. 10 waiters. This will add 2 atomic adds per lock.",5,"Yes, number of instances is increased at 50%. As I mentioned, if I increase it at 48%, number of instances is quickly raising up to the limit.

It's a bit more than ++mutex_nowaits, but close enough.

It was multi-table OLTP RO benchmark with 40 threads. I had 1 instance initially.

With current numbers warm-up up to 2 instances takes under 5 seconds. With lower numbers it was raising up to the limit in under 1 minute.

There's another option, but it's a bit more expensive: count number of waiting threads and activate instances when there're e.g. 10 waiters. This will add 2 atomic adds per lock."
1658,MDEV-10296,MDEV,Sergey Vojtovich,86392,2016-09-12 15:13:32,Changed assignee while waiting for feedback.,6,Changed assignee while waiting for feedback.
1659,MDEV-10296,MDEV,Sergei Golubchik,86550,2016-09-16 11:01:05,ok to push (cd1b39b and whatever you have in the same branch),7,ok to push (cd1b39b and whatever you have in the same branch)
1660,MDEV-10296,MDEV,Sergey Vojtovich,86552,2016-09-16 11:45:25,"Final autosizing implementation:
{noformat}
Instance is considered contested if more than 20% of mutex acquisiotions
can't be served immediately. Up to 100 000 probes may be performed to avoid
instance activation on short sporadic peaks. 100 000 is estimated maximum
number of queries one instance can serve in one second.

These numbers work well on a 2 socket / 20 core / 40 threads Intel Broadwell
system, that is expected number of instances is activated within reasonable
warmup time. It may have to be adjusted for other systems.

Only TABLE object acquistion is instrumented. We intentionally avoid this
overhead on TABLE object release. All other table cache mutex acquistions
are considered out of hot path and are not instrumented either.
{noformat}",8,"Final autosizing implementation:
{noformat}
Instance is considered contested if more than 20% of mutex acquisiotions
can't be served immediately. Up to 100 000 probes may be performed to avoid
instance activation on short sporadic peaks. 100 000 is estimated maximum
number of queries one instance can serve in one second.

These numbers work well on a 2 socket / 20 core / 40 threads Intel Broadwell
system, that is expected number of instances is activated within reasonable
warmup time. It may have to be adjusted for other systems.

Only TABLE object acquistion is instrumented. We intentionally avoid this
overhead on TABLE object release. All other table cache mutex acquistions
are considered out of hot path and are not instrumented either.
{noformat}"
1661,MDEV-10297,MDEV,Vladislav Vaintroub,85339,2016-08-02 16:41:12,the repository that contains the patch is here https://github.com/MariaDB/server/commits/bb-10.2-wlad-threadpool,1,the repository that contains the patch is here URL
1662,MDEV-10340,MDEV,Oleksandr Byelkin,87493,2016-10-17 15:03:05,"revision-id: dabf6cac60987e88266396a28e40b341899704e6 (mariadb-10.2.2-49-gdabf6ca)
parent(s): 8303aded294ce905bbc513e7ee42623d5f1fdb50
committer: Oleksandr Byelkin
timestamp: 2016-10-17 16:59:36 +0200
message:

MDEV-10340: support COM_RESET_CONNECTION

draft to check with client

---",1,"revision-id: dabf6cac60987e88266396a28e40b341899704e6 (mariadb-10.2.2-49-gdabf6ca)
parent(s): 8303aded294ce905bbc513e7ee42623d5f1fdb50
committer: Oleksandr Byelkin
timestamp: 2016-10-17 16:59:36 +0200
message:

MDEV-10340: support COM_RESET_CONNECTION

draft to check with client

---"
1663,MDEV-10340,MDEV,Oleksandr Byelkin,89467,2016-12-12 13:47:31,"revision-id: e6875772b0d65a6341456a6f4aca3353bbdae3e4 (mariadb-10.2.2-131-ge687577)
parent(s): 7ca1e2abad42a7436e6b668b4568d6fadc2ca165
committer: Oleksandr Byelkin
timestamp: 2016-12-12 14:41:45 +0100
message:

MDEV-10340: support COM_RESET_CONNECTION

draft to check with client

---",2,"revision-id: e6875772b0d65a6341456a6f4aca3353bbdae3e4 (mariadb-10.2.2-131-ge687577)
parent(s): 7ca1e2abad42a7436e6b668b4568d6fadc2ca165
committer: Oleksandr Byelkin
timestamp: 2016-12-12 14:41:45 +0100
message:

MDEV-10340: support COM_RESET_CONNECTION

draft to check with client

---"
1664,MDEV-10340,MDEV,Sergei Golubchik,90008,2016-12-27 16:42:38,please add test cases. I think you can do it with {{simple_command}} from {{mysql_client_test.c}},3,please add test cases. I think you can do it with {{simple_command}} from {{mysql_client_test.c}}
1665,MDEV-10340,MDEV,Oleksandr Byelkin,90012,2016-12-27 20:19:46,"Not really (nead some cleanup on client side), but I have other patch for client and even test there (Georg was reviewing it, I'll send it)",4,"Not really (nead some cleanup on client side), but I have other patch for client and even test there (Georg was reviewing it, I'll send it)"
1666,MDEV-10340,MDEV,Oleksandr Byelkin,90013,2016-12-27 20:23:45,I attached client changes,5,I attached client changes
1667,MDEV-10343,MDEV,Paul Pogonyshev,110535,2018-05-04 11:22:58,Why wasn't NVARCHAR2 made a synonym to NVARCHAR too? In our (currently Oracle) database NVARCHAR2 is used even more often than VARCHAR2..,1,Why wasn't NVARCHAR2 made a synonym to NVARCHAR too? In our (currently Oracle) database NVARCHAR2 is used even more often than VARCHAR2..
1668,MDEV-10411,MDEV,Dmitry Tolpeko,85084,2016-07-21 11:24:34,"{code}
CREATE PROCEDURE sp1 (p1 IN VARCHAR2, p2 OUT VARCHAR2)
IS
  v1 VARCHAR2(100);
BEGIN
  v1 := p1;
  p2 := v1;
END;
{code}",1,"{code}
CREATE PROCEDURE sp1 (p1 IN VARCHAR2, p2 OUT VARCHAR2)
IS
  v1 VARCHAR2(100);
BEGIN
  v1 := p1;
  p2 := v1;
END;
{code}"
1669,MDEV-10411,MDEV,Alexander Barkov,85400,2016-08-05 06:12:50,"Syntax difference:
h2. 1. [DONE] Labels
MariaDB:
{code:sql}
label:
{code}
Oracle:
{code:sql}
<<label>>
{code}

h2. 2. [DONE] Different order of {{IN}}, {{OUT}}, {{INOUT}} keywords
MariaDB:
{code:sql}
CREATE PROCEDURE p1 (OUT param INT)
{code}
Oracle:
{code:sql}
CREATE PROCEDURE p1 (param OUT INT)
{code}

h2. 3. [DEFERRED] {{IN}}, {{OUT}}, {{INOUT}} in {{CREATE FUNCTION}}.
See MDEV-10654.

h2. 4. [DONE] {{AS}}/{{IS}} keyword before a function or a procedure body
Oracle requires {{AS}} or {{IS}} keyword before the body:
{code:sql}
CREATE FUNCTION f1 RETURN NUMBER
AS
BEGIN
  RETURN 10;
END; 
{code}

{code:sql}
CREATE PROCEDURE sp1 (p1 IN VARCHAR2(20), p2 OUT VARCHAR2(30))
IS
 BEGIN
 END
{code}

h2. 5. [DONE] {{EXIT}} statement

Oracle supports this syntax to leave a loop block:
{code:sql}
EXIT [ label ] [ WHEN bool_expr ]
{code}
{{label}} is optional.
The {{WHEN}} clause is optional.

{code:sql}
EXIT label WHEN bool_expr
{code}
is similar to MariaDB syntax:
{code:sql}
IF bool_expr THEN LEAVE label
{code}

Note, unlike {{LEAVE}}, {{EXIT}} is valid only inside a loop block. This definition is not valid:
{code:sql}
CREATE PROCEDURE p1
AS
BEGIN;
  EXIT;
END;
{code}
because {{EXIT}} is not inside a loop.

h2. 6. [DONE] Assignment operator
MariaDB:
{code:sql}
SET var= 10;
{code}
Oracle:
{code:sql}
var:= 10;
{code}
Assignment operator will work for system variables:
{code:SQL}
max_sort_length:=1025;
{code}

h2. 7. [DONE] Variable declarations
In MariaDB, the DECLARE statements are listed inside BEGIN..END blocks. One block can have multiple DECLARE statements:
{code:sql}
BEGIN
  DECLARE a INT;
  DECLARE b VARCHAR(10);
  v= 10;
END;
{code}

In Oracle, DECLARE is a special optional section in the beginning of  a DECLARE .. BEGIN .. END block. One block can have only one DECLARE section, with multiple declarations.
If a block does not need any declarations, then the DECLARE section is omitted.
If the DECLARE section presents, then at least one declaration is required.

{code:sql}
DECLARE 
  a INT;
  b VARCHAR(10);
BEGIN
  v:= 10;
END;
{code}

In the top level block variables are declared directly after the ""AS"" keyword:
{code:sql}
CREATE PROCEDURE p1()
AS
  a INT;
  b VARCHAR(10);
BEGIN
  v:= 10;
END;
{code}

h2. 8. [DEREFFED] Anonymous blocks
See MDEV-10655.

h2. 9. [DONE] EXCEPTION handlers
MariaDB uses DECLARE HANDLER to catch exceptions:
{code:sql}
BEGIN
  DECLARE EXIT HANDLER FOR SQLEXCEPTION
  BEGIN
  ..
  END;
END;
{code}
In Oracle, exception handlers are declared in the end of a block:
{code:sql}
BEGIN
  ...
EXCEPTION
  WHEN OTHERS THEN
  BEGIN ..
  END;
END;
{code}

{{OTHERS}} should catch all exceptions, including warnings. For example, although {{NO_DATA_FOUND}} is more like a warning than an error, it should still be handled by {{OTHERS}}.
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
CREATE FUNCTION f1 RETURN VARCHAR
AS
  a INT:=10;
BEGIN
  SELECT a INTO a FROM t1;
  RETURN 'OK';
EXCEPTION
  WHEN OTHERS THEN RETURN 'Exception';
END;
/
SHOW ERRORS;
SELECT f1 FROM DUAL;
{code}
{noformat}
F1
---------
Exception
{noformat}


h2.10 [DONE] Default variable value: {{x INT := 10;}}
In addition to the DEFAULT keyword, Oracle supports the := operator to assign a default value to a variable:
{code:sql}
a NUMBER(10) := 10;
{code}

h2. 11. [DONE] {{NULL}} as a statement
Oracle does not support empty blocks with {{BEGIN}} immediately followed by {{END}}.
There must be at least one statement.
{{NULL}} is a valid statement. {{BEGIN NULL; END;}} is actively used in Oracle: 
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1() AS
BEGIN
  NULL;
END;
/
{code}
{{NULL}} can appear in other syntactic constructs as a statement:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1() AS
  a INT:=10;
BEGIN
  IF a=10 THEN NULL; ELSE NULL; END IF;
END;
/
{code}

h2. 12. [DONE] No parentheses if no arguments
If a function or a procedure has no parameters, then parentheses must be omitted:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
BEGIN
  NULL;
END;
/
{code}

h2. 13. [DONE] RETURN vs RETURNS
Oracle uses {{RETURN}} rather than {{RETURNS}}:
{code:sql}
CREATE FUNCTION f1(a INT) RETURN INT ...
{code}

h2. 14. [DONE] {{IN OUT}} instead of {{INOUT}}
Instead of {{INOUT}}, Oracle uses two separate keywords: {{IN}} followed by {{OUT}} in an SP parameter declaration:
{code:sql}
CREATE PROCEDURE p1 (a IN OUT INT)
AS
BEGIN
END;
{code}

h2. 15. [DONE] ELSIF vs ELSEIF
{code:sq}
DROP FUNCTION f1;
CREATE FUNCTION f1(a INT) RETURN VARCHAR
AS
BEGIN
  IF a=1 THEN RETURN 'a is 1';
  ELSIF a=2 THEN RETURN 'a is 2';
  ELSE RETURN 'a is unknown';
  END IF;
END;
/
SELECT f1(1) FROM DUAL;
{code}
{noformat}
F1(1)
--------------------------------------------------------------------------------
a is 1
{noformat}

h2. 16. [DONE] Cursor declaration
MariaDB:
{code:sql}
DECLARE cr CURSOR FOR SELECT * FROM t1;
{code}

Oracle:
{code:sql}
CURSOR cr IS SELECT * FROM t1;
{code}

Example:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT);
DECLARE
  CURSOR cr IS SELECT * FROM t1;
BEGIN
  NULL;
END;
/
{code}

h2. 17. [DONE] {{RETURN}} in stored procedures
MariaDB supports {{RETURN}} only in stored functions.
Oracle supports {{RETURN}} in stored procedures as well, including the {{EXCEPTION}} section:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1 (a IN OUT INT)
AS
BEGIN
  IF a < 10 THEN
    RETURN;
  END IF;
  a:=a+1;
EXCEPTION
  WHEN OTHERS THEN RETURN;
END;
/
{code}

h2. 18. [DONE] WHILE syntax
MariaDB:
{code:sql}
[begin_label:] WHILE search_condition DO
    statement_list
END WHILE [end_label]
{code}

Oracle
{code:sql}
[<<label>>]
WHILE boolean_expression
  LOOP statement... END LOOP [ label ] ;
{code}


h2. 19. [DONE] CONTINUE statement
{code:sq}
CONTINUE [ label ] [ WHEN boolean_expression ] ;
{code}
This is a replacement for the {{ITERATE}} statement in MariaDB.
{{CONTINUE}} is valid only inside a {{LOOP}}.
",2,"Syntax difference:
h2. 1. [DONE] Labels
MariaDB:
{code:sql}
label:
{code}
Oracle:
{code:sql}
>
{code}

h2. 2. [DONE] Different order of {{IN}}, {{OUT}}, {{INOUT}} keywords
MariaDB:
{code:sql}
CREATE PROCEDURE p1 (OUT param INT)
{code}
Oracle:
{code:sql}
CREATE PROCEDURE p1 (param OUT INT)
{code}

h2. 3. [DEFERRED] {{IN}}, {{OUT}}, {{INOUT}} in {{CREATE FUNCTION}}.
See MDEV-10654.

h2. 4. [DONE] {{AS}}/{{IS}} keyword before a function or a procedure body
Oracle requires {{AS}} or {{IS}} keyword before the body:
{code:sql}
CREATE FUNCTION f1 RETURN NUMBER
AS
BEGIN
  RETURN 10;
END; 
{code}

{code:sql}
CREATE PROCEDURE sp1 (p1 IN VARCHAR2(20), p2 OUT VARCHAR2(30))
IS
 BEGIN
 END
{code}

h2. 5. [DONE] {{EXIT}} statement

Oracle supports this syntax to leave a loop block:
{code:sql}
EXIT [ label ] [ WHEN bool_expr ]
{code}
{{label}} is optional.
The {{WHEN}} clause is optional.

{code:sql}
EXIT label WHEN bool_expr
{code}
is similar to MariaDB syntax:
{code:sql}
IF bool_expr THEN LEAVE label
{code}

Note, unlike {{LEAVE}}, {{EXIT}} is valid only inside a loop block. This definition is not valid:
{code:sql}
CREATE PROCEDURE p1
AS
BEGIN;
  EXIT;
END;
{code}
because {{EXIT}} is not inside a loop.

h2. 6. [DONE] Assignment operator
MariaDB:
{code:sql}
SET var= 10;
{code}
Oracle:
{code:sql}
var:= 10;
{code}
Assignment operator will work for system variables:
{code:SQL}
max_sort_length:=1025;
{code}

h2. 7. [DONE] Variable declarations
In MariaDB, the DECLARE statements are listed inside BEGIN..END blocks. One block can have multiple DECLARE statements:
{code:sql}
BEGIN
  DECLARE a INT;
  DECLARE b VARCHAR(10);
  v= 10;
END;
{code}

In Oracle, DECLARE is a special optional section in the beginning of  a DECLARE .. BEGIN .. END block. One block can have only one DECLARE section, with multiple declarations.
If a block does not need any declarations, then the DECLARE section is omitted.
If the DECLARE section presents, then at least one declaration is required.

{code:sql}
DECLARE 
  a INT;
  b VARCHAR(10);
BEGIN
  v:= 10;
END;
{code}

In the top level block variables are declared directly after the ""AS"" keyword:
{code:sql}
CREATE PROCEDURE p1()
AS
  a INT;
  b VARCHAR(10);
BEGIN
  v:= 10;
END;
{code}

h2. 8. [DEREFFED] Anonymous blocks
See MDEV-10655.

h2. 9. [DONE] EXCEPTION handlers
MariaDB uses DECLARE HANDLER to catch exceptions:
{code:sql}
BEGIN
  DECLARE EXIT HANDLER FOR SQLEXCEPTION
  BEGIN
  ..
  END;
END;
{code}
In Oracle, exception handlers are declared in the end of a block:
{code:sql}
BEGIN
  ...
EXCEPTION
  WHEN OTHERS THEN
  BEGIN ..
  END;
END;
{code}

{{OTHERS}} should catch all exceptions, including warnings. For example, although {{NO_DATA_FOUND}} is more like a warning than an error, it should still be handled by {{OTHERS}}.
{code:sql}
DROP TABLE t1;
DROP FUNCTION f1;
CREATE TABLE t1 (a INT);
CREATE FUNCTION f1 RETURN VARCHAR
AS
  a INT:=10;
BEGIN
  SELECT a INTO a FROM t1;
  RETURN 'OK';
EXCEPTION
  WHEN OTHERS THEN RETURN 'Exception';
END;
/
SHOW ERRORS;
SELECT f1 FROM DUAL;
{code}
{noformat}
F1
---------
Exception
{noformat}


h2.10 [DONE] Default variable value: {{x INT := 10;}}
In addition to the DEFAULT keyword, Oracle supports the := operator to assign a default value to a variable:
{code:sql}
a NUMBER(10) := 10;
{code}

h2. 11. [DONE] {{NULL}} as a statement
Oracle does not support empty blocks with {{BEGIN}} immediately followed by {{END}}.
There must be at least one statement.
{{NULL}} is a valid statement. {{BEGIN NULL; END;}} is actively used in Oracle: 
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1() AS
BEGIN
  NULL;
END;
/
{code}
{{NULL}} can appear in other syntactic constructs as a statement:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1() AS
  a INT:=10;
BEGIN
  IF a=10 THEN NULL; ELSE NULL; END IF;
END;
/
{code}

h2. 12. [DONE] No parentheses if no arguments
If a function or a procedure has no parameters, then parentheses must be omitted:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
BEGIN
  NULL;
END;
/
{code}

h2. 13. [DONE] RETURN vs RETURNS
Oracle uses {{RETURN}} rather than {{RETURNS}}:
{code:sql}
CREATE FUNCTION f1(a INT) RETURN INT ...
{code}

h2. 14. [DONE] {{IN OUT}} instead of {{INOUT}}
Instead of {{INOUT}}, Oracle uses two separate keywords: {{IN}} followed by {{OUT}} in an SP parameter declaration:
{code:sql}
CREATE PROCEDURE p1 (a IN OUT INT)
AS
BEGIN
END;
{code}

h2. 15. [DONE] ELSIF vs ELSEIF
{code:sq}
DROP FUNCTION f1;
CREATE FUNCTION f1(a INT) RETURN VARCHAR
AS
BEGIN
  IF a=1 THEN RETURN 'a is 1';
  ELSIF a=2 THEN RETURN 'a is 2';
  ELSE RETURN 'a is unknown';
  END IF;
END;
/
SELECT f1(1) FROM DUAL;
{code}
{noformat}
F1(1)
--------------------------------------------------------------------------------
a is 1
{noformat}

h2. 16. [DONE] Cursor declaration
MariaDB:
{code:sql}
DECLARE cr CURSOR FOR SELECT * FROM t1;
{code}

Oracle:
{code:sql}
CURSOR cr IS SELECT * FROM t1;
{code}

Example:
{code:sql}
DROP TABLE t1;
CREATE TABLE t1 (a INT);
DECLARE
  CURSOR cr IS SELECT * FROM t1;
BEGIN
  NULL;
END;
/
{code}

h2. 17. [DONE] {{RETURN}} in stored procedures
MariaDB supports {{RETURN}} only in stored functions.
Oracle supports {{RETURN}} in stored procedures as well, including the {{EXCEPTION}} section:
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1 (a IN OUT INT)
AS
BEGIN
  IF a < 10 THEN
    RETURN;
  END IF;
  a:=a+1;
EXCEPTION
  WHEN OTHERS THEN RETURN;
END;
/
{code}

h2. 18. [DONE] WHILE syntax
MariaDB:
{code:sql}
[begin_label:] WHILE search_condition DO
    statement_list
END WHILE [end_label]
{code}

Oracle
{code:sql}
[>]
WHILE boolean_expression
  LOOP statement... END LOOP [ label ] ;
{code}


h2. 19. [DONE] CONTINUE statement
{code:sq}
CONTINUE [ label ] [ WHEN boolean_expression ] ;
{code}
This is a replacement for the {{ITERATE}} statement in MariaDB.
{{CONTINUE}} is valid only inside a {{LOOP}}.
"
1670,MDEV-10411,MDEV,Krishnadas,87178,2016-10-07 10:19:55,DBS Test cases [^MDEV-10411.txt] ,3,DBS Test cases [^MDEV-10411.txt] 
1671,MDEV-10570,MDEV,Lixun Peng,86482,2016-09-14 18:46:55,"Hi Sergei,

I have sent an email to you, please review it.

Thank you very much!",1,"Hi Sergei,

I have sent an email to you, please review it.

Thank you very much!"
1672,MDEV-10577,MDEV,Alexander Barkov,93850,2017-04-06 09:33:31,"Oracle's {{%TYPE}} syntax is similar to:

- IBM:
{code:sql}
DECLARE va ANCHOR DATA TYPE TO t1.a;
{code}

- Firebird:
{code:sql}
DECLARE va TYPE OF COLUMN t1.a;
{code}
",1,"Oracle's {{%TYPE}} syntax is similar to:

- IBM:
{code:sql}
DECLARE va ANCHOR DATA TYPE TO t1.a;
{code}

- Firebird:
{code:sql}
DECLARE va TYPE OF COLUMN t1.a;
{code}
"
1673,MDEV-10581,MDEV,Alexander Barkov,91009,2017-01-24 10:42:28,"See also:
WL#3309: Stored Procedures: FOR statement
https://dev.mysql.com/worklog/task/?id=3309
",1,"See also:
WL#3309: Stored Procedures: FOR statement
URL
"
1674,MDEV-10581,MDEV,Alexander Barkov,92855,2017-03-10 10:18:38,Pushed to bb-10.2-compatibility,2,Pushed to bb-10.2-compatibility
1675,MDEV-10581,MDEV,Robert Dyas,113289,2018-06-30 15:31:46,"Is there any way we can use this particular syntax WITHOUT sql_mode=ORACLE ?

{code:sql}
FOR rec IN cur
LOOP
  -- statements
END LOOP;
{code}

It would be very, very helpful if we could. Even if we had to set a special variable like sql_mode=ALLOW_FOR_IN_LOOP or something.
",3,"Is there any way we can use this particular syntax WITHOUT sql_mode=ORACLE ?

{code:sql}
FOR rec IN cur
LOOP
  -- statements
END LOOP;
{code}

It would be very, very helpful if we could. Even if we had to set a special variable like sql_mode=ALLOW_FOR_IN_LOOP or something.
"
1676,MDEV-10581,MDEV,Alexander Barkov,113414,2018-07-03 09:28:00,"Yes, FOR loops are possible without sql_mode=ORACLE, but using slightly a different syntax. See MDEV-14415 for details.
",4,"Yes, FOR loops are possible without sql_mode=ORACLE, but using slightly a different syntax. See MDEV-14415 for details.
"
1677,MDEV-10583,MDEV,Michael Widenius,85655,2016-08-18 12:45:55,"This can be fixed by either providing a conversion in the parser layer or by having a token-translation layer between lex and parser.  The benefit of a translation layer is that this is trivially to manage and extend.
In this case we would just translate ""SQL%ROWCOUNT"" to ""row_count()'
",1,"This can be fixed by either providing a conversion in the parser layer or by having a token-translation layer between lex and parser.  The benefit of a translation layer is that this is trivially to manage and extend.
In this case we would just translate ""SQL%ROWCOUNT"" to ""row_count()'
"
1678,MDEV-10585,MDEV,Michael Widenius,86701,2016-09-22 10:52:52,"Review done. Code looks good, asked to add one comment to one place before pushing",1,"Review done. Code looks good, asked to add one comment to one place before pushing"
1679,MDEV-10585,MDEV,Elena Stepanova,87696,2016-10-23 19:58:46,Why does it have fix version 10.3.0 if it's been already pushed?,2,Why does it have fix version 10.3.0 if it's been already pushed?
1680,MDEV-10585,MDEV,Alexander Barkov,87702,2016-10-24 03:48:03,"Originally this was planned for 10.3.0, but then we decided to backport it to 10.2. I forgot to update ""Fix version"" after backporting.

Added 10.2.3 to the ""Fix version"" list. Thanks for noticing this!
",3,"Originally this was planned for 10.3.0, but then we decided to backport it to 10.2. I forgot to update ""Fix version"" after backporting.

Added 10.2.3 to the ""Fix version"" list. Thanks for noticing this!
"
1681,MDEV-10588,MDEV,Michael Widenius,85654,2016-08-18 12:32:06,"For first version, lets just ignore the extra options for truncate that only affects performance.
",1,"For first version, lets just ignore the extra options for truncate that only affects performance.
"
1682,MDEV-10591,MDEV,Alexander Barkov,89156,2016-12-05 10:09:05,"If one needs only a package with procedures and functions, then there is a workaround possible:
Instead of a package, one can just create procedures and functions in a separate database with the same name.
",1,"If one needs only a package with procedures and functions, then there is a workaround possible:
Instead of a package, one can just create procedures and functions in a separate database with the same name.
"
1683,MDEV-10598,MDEV,Alexander Barkov,93775,2017-04-04 11:51:35,Pushed to bb-10.2-compatibility,1,Pushed to bb-10.2-compatibility
1684,MDEV-10655,MDEV,Alexander Barkov,91196,2017-01-30 11:04:38,"Hi Alvin,
I'm almost done with it. Sending for Monty's review today.
",1,"Hi Alvin,
I'm almost done with it. Sending for Monty's review today.
"
1685,MDEV-10655,MDEV,Michael Widenius,91246,2017-01-31 11:00:07,Reviewed. Ok to push,2,Reviewed. Ok to push
1686,MDEV-10655,MDEV,Alexander Barkov,91324,2017-02-01 19:13:18,Pushed to bb-10.2-compatibility,3,Pushed to bb-10.2-compatibility
1687,MDEV-10664,MDEV,Jean-François Gagné,85885,2016-08-27 14:37:04,"Thinking about this a little more, those should not be global statuses but be property for the replication channel.  This should be the same for slave_retried_transactions (not being a global status, but be a property of a replication channel).  Having this information at the channel level instead of the global statues would allow to be able to monitor multi-source replication in a better way.

At the same time, it would probably be interesting to add more information for replication applier monitoring.  I am thinking about the slave equivalent of Binlog_commits and Binlog_group_commits.  Those 2 statuses allow to monitor transaction grouping on the master.  Having the same thing for the SQL_THREAD (which transaction grouping is the SQL_THREAD seeing from the master) would allow to be able to have an holistic view on the slave.",1,"Thinking about this a little more, those should not be global statuses but be property for the replication channel.  This should be the same for slave_retried_transactions (not being a global status, but be a property of a replication channel).  Having this information at the channel level instead of the global statues would allow to be able to monitor multi-source replication in a better way.

At the same time, it would probably be interesting to add more information for replication applier monitoring.  I am thinking about the slave equivalent of Binlog_commits and Binlog_group_commits.  Those 2 statuses allow to monitor transaction grouping on the master.  Having the same thing for the SQL_THREAD (which transaction grouping is the SQL_THREAD seeing from the master) would allow to be able to have an holistic view on the slave."
1688,MDEV-10664,MDEV,Andrei Elkin,107122,2018-02-10 12:14:43,Review notes are mailed to commits@ list. The patch requires a minor work to address them.,2,Review notes are mailed to commits@ list. The patch requires a minor work to address them.
1689,MDEV-10664,MDEV,Sachin Setiya,109161,2018-04-02 07:39:58,"[~julien.fritsch], Actually I thought that we need this 10.3 rc , so I have set the issue to blocker.",3,"[~julien.fritsch], Actually I thought that we need this 10.3 rc , so I have set the issue to blocker."
1690,MDEV-10664,MDEV,Jean-François Gagné,109218,2018-04-03 07:58:03,"As this should not introduce any risk of regression, maybe this can also be back-ported to 10.2.",4,"As this should not introduce any risk of regression, maybe this can also be back-ported to 10.2."
1691,MDEV-10664,MDEV,Sachin Setiya,109481,2018-04-09 11:31:13,"Hi [~jeanfrancois.gagne], although you are right, but 10.2 is ga
I am not sure whether we can backport this in 10.2 or not",5,"Hi [~jeanfrancois.gagne], although you are right, but 10.2 is ga
I am not sure whether we can backport this in 10.2 or not"
1692,MDEV-10664,MDEV,Sergei Golubchik,110555,2018-05-04 16:00:28,"I suspect that adding new columns to SHOW MASTER INFO might break existing applications, so it's somewhat reckless to do it in a GA version.",6,"I suspect that adding new columns to SHOW MASTER INFO might break existing applications, so it's somewhat reckless to do it in a GA version."
1693,MDEV-10813,MDEV,Sergey Vojtovich,86876,2016-09-28 15:44:34,"[~serg], please review patches in bb-10.2-mdev10813.",1,"[~serg], please review patches in bb-10.2-mdev10813."
1694,MDEV-10813,MDEV,Sergey Vojtovich,86882,2016-09-28 19:43:02,Apparently the only failure detected by buildbot is the windows one. Failures on fulltest occur in 10.2 as well.,2,Apparently the only failure detected by buildbot is the windows one. Failures on fulltest occur in 10.2 as well.
1695,MDEV-10813,MDEV,Vladislav Vaintroub,86883,2016-09-28 19:46:58,"Apparently, Windows failure is not exactly a sign of health . IT is a crash in in bootstrap ",3,"Apparently, Windows failure is not exactly a sign of health . IT is a crash in in bootstrap "
1696,MDEV-10813,MDEV,Sergey Vojtovich,86884,2016-09-28 19:54:51,"[~wlad], it definitely is. I was going to ask you if you will be willing to help me to solve this. The problem should be fairly simple: it's startup, so no heavy concurrency should be involved. I guess either I used wrong atomic operation (different type) or something is missing to make my_atomic.h to emit the right code. I'm mostly curious about dump of rwlock structure.",4,"[~wlad], it definitely is. I was going to ask you if you will be willing to help me to solve this. The problem should be fairly simple: it's startup, so no heavy concurrency should be involved. I guess either I used wrong atomic operation (different type) or something is missing to make my_atomic.h to emit the right code. I'm mostly curious about dump of rwlock structure."
1697,MDEV-10813,MDEV,Vladislav Vaintroub,86887,2016-09-28 21:28:20,"
But first the crash - it is an assertion 
	ut_ad(lock->lock_word <= threshold);

lock->lock_word	= 0x00000000f0000000	volatile __int64
threshold = 	0xfffffffff0000000	__int64



ulint which is the datatype of lock->lock_word,  in Innodb is not synonymous to datatype ""long"". It is unsigned integer of the size of a pointer. The best standard approximation for that is size_t datatype.
 
Now, long is 32 bit on all Windows, pointer is of course 64 bit on Win64.

Thus my_atomic_caslong, my_atomic_addlong  are wrong inside the sync0rw.ic
It needs to be substituted with int32 or int64 I think.

",5,"
But first the crash - it is an assertion 
	ut_ad(lock->lock_word <= threshold);

lock->lock_word	= 0x00000000f0000000	volatile __int64
threshold = 	0xfffffffff0000000	__int64



ulint which is the datatype of lock->lock_word,  in Innodb is not synonymous to datatype ""long"". It is unsigned integer of the size of a pointer. The best standard approximation for that is size_t datatype.
 
Now, long is 32 bit on all Windows, pointer is of course 64 bit on Win64.

Thus my_atomic_caslong, my_atomic_addlong  are wrong inside the sync0rw.ic
It needs to be substituted with int32 or int64 I think.

"
1698,MDEV-10813,MDEV,Jan Lindström,86894,2016-09-29 05:35:34,Wish: MDEV-6654 if relevant to new implementation would be nice to be fixed in this contents also.,6,Wish: MDEV-6654 if relevant to new implementation would be nice to be fixed in this contents also.
1699,MDEV-10813,MDEV,Sergey Vojtovich,86895,2016-09-29 05:45:18,"[~jplindst], InnoDB 5.7 comes with this idea for futex and  spin-lock implementation. I fixed event based too.",7,"[~jplindst], InnoDB 5.7 comes with this idea for futex and  spin-lock implementation. I fixed event based too."
1700,MDEV-10813,MDEV,Sergey Vojtovich,86906,2016-09-29 11:05:19,"Just pushed a patch that attempts to fix WIn64 build failure. I don't completely like it: I'd better change to types with deterministic size, but it can be fairly intrusive.",8,"Just pushed a patch that attempts to fix WIn64 build failure. I don't completely like it: I'd better change to types with deterministic size, but it can be fairly intrusive."
1701,MDEV-10813,MDEV,Vladislav Vaintroub,86907,2016-09-29 11:15:01,"Or, we can have my_atomic_cas_ssize_t and my_atomic_add_ssize_t .  Absolutely everything is better than  ""long""
",9,"Or, we can have my_atomic_cas_ssize_t and my_atomic_add_ssize_t .  Absolutely everything is better than  ""long""
"
1702,MDEV-10813,MDEV,Vladislav Vaintroub,86925,2016-09-29 17:30:59,pushed a fix for Win64 compilation (missing casts) to bb-10.2-mdev10813,10,pushed a fix for Win64 compilation (missing casts) to bb-10.2-mdev10813
1703,MDEV-10813,MDEV,Vladislav Vaintroub,86962,2016-09-30 15:20:38,"This failed on OSX, too http://buildbot.askmonty.org/buildbot/builders/labrador/builds/8036/steps/compile/logs/stdio

/private/var/lib/buildslave/maria-slave/labrador/build/storage/innobase/include/sync0rw.ic:341: error: invalid conversion from '_opaque_pthread_t*' to 'int64'
/private/var/lib/buildslave/maria-slave/labrador/build/storage/innobase/include/sync0rw.ic:341: error:   initializing argument 2 of 'void my_atomic_store64(volatile int64*, int64)
",11,"This failed on OSX, too URL

/private/var/lib/buildslave/maria-slave/labrador/build/storage/innobase/include/sync0rw.ic:341: error: invalid conversion from '_opaque_pthread_t*' to 'int64'
/private/var/lib/buildslave/maria-slave/labrador/build/storage/innobase/include/sync0rw.ic:341: error:   initializing argument 2 of 'void my_atomic_store64(volatile int64*, int64)
"
1704,MDEV-10813,MDEV,Sergei Golubchik,87212,2016-10-09 12:25:47,"I suppose [bb-10.2-mdev10813|https://github.com/MariaDB/server/tree/bb-10.2-mdev10813] branch (at commit [d2a4536be9|https://github.com/MariaDB/server/commit/d2a4536be9cd8a2e78fb606c48a0348f331556a1]) is good for review now.

Looking at the [diff|https://github.com/MariaDB/server/compare/10.2...d2a4536be9cd8a2e78fb606c48a0348f331556a1]
",12,"I suppose [bb-10.2-mdev10813|URL branch (at commit [d2a4536be9|URL is good for review now.

Looking at the [diff|URL
"
1705,MDEV-10813,MDEV,Axel Schwenke,87372,2016-10-13 11:37:41,"I did a round of sysbench OLTP. On our 16 core / 32 thread benchmark machine the latest changes in bb-10.2-mdev10813 make nearly no difference. Here are numbers (queries per second - bigger is better)

{code}
# data set 01 -> mdev-10813 head
# data set 02 -> before (commit 5058ced)

# read only
#thd    01      02
1       9676.6  9977.2
2       21338   21649
4       39786   40760
8       71194   75239
16      137518  136853
32      195938  196105
64      191771  191567
128     188573  188105
256     191704  191259

# read/write (10% writes)
#thd    01      02
1       6530.6  6663.4
2       13575   13533
4       24298   24414
8       45236   45446
16      86590   87654
32      148783  154582
64      194451  196712
128     206457  206319
256     203748  203821

# read/write (22% writes)
#thd    01      02
1       4238.5  4221.1
2       8492.5  8608.8
4       14292   14354
8       26833   27170
16      51832   52092
32      104019  104240
64      151609  149283
128     168852  169022
256     170582  170851

# read/write (80% writes)
#thd    01      02
1       8890.4  9261.9
2       17155   17560
4       32284   32438
8       60511   60207
16      112681  115927
32      183503  186966
64      219418  221771
128     227306  229264
256     225811  215256
{code}

On the 64 core / 128 thread machine it looks a bit different. Benchmark is still running, I'll add numbers when they arrive.",13,"I did a round of sysbench OLTP. On our 16 core / 32 thread benchmark machine the latest changes in bb-10.2-mdev10813 make nearly no difference. Here are numbers (queries per second - bigger is better)

{code}
# data set 01 -> mdev-10813 head
# data set 02 -> before (commit 5058ced)

# read only
#thd    01      02
1       9676.6  9977.2
2       21338   21649
4       39786   40760
8       71194   75239
16      137518  136853
32      195938  196105
64      191771  191567
128     188573  188105
256     191704  191259

# read/write (10% writes)
#thd    01      02
1       6530.6  6663.4
2       13575   13533
4       24298   24414
8       45236   45446
16      86590   87654
32      148783  154582
64      194451  196712
128     206457  206319
256     203748  203821

# read/write (22% writes)
#thd    01      02
1       4238.5  4221.1
2       8492.5  8608.8
4       14292   14354
8       26833   27170
16      51832   52092
32      104019  104240
64      151609  149283
128     168852  169022
256     170582  170851

# read/write (80% writes)
#thd    01      02
1       8890.4  9261.9
2       17155   17560
4       32284   32438
8       60511   60207
16      112681  115927
32      183503  186966
64      219418  221771
128     227306  229264
256     225811  215256
{code}

On the 64 core / 128 thread machine it looks a bit different. Benchmark is still running, I'll add numbers when they arrive."
1706,MDEV-10813,MDEV,Axel Schwenke,87374,2016-10-13 12:59:24,"Here the numbers (again queries per second) from the 64 core / 128 thread machine. Read-only is a bit faster for the head of the tree. All in all performance on this hardware is rather poor, probably caused by contention in the server layers above InnoDB.

{code}
# data set 01 -> mdev-10813 head
# data set 02 -> before (commit 5058ced)

# read only
#thd    01      02
1       8543.9  8550.4
2       16886   16951
4       32710   31471
8       60611   59759
16      84002   82922
32      61391   60169
64      38369   36371
128     39930   39849
256     20656   20974
512     23212   23343
1024    78925   73784

# read/write (10% writes)
#thd    01      02
1       5987.6  5849.4
2       12810   12402
4       24669   24228
8       45750   45463
16      67585   67701
32      67475   66547
64      52460   50705
128     28924   30246
256     17840   17841
512     17094   17108
1024    22048   20821

# read/write (22% writes)
#thd    01      02
1       4166.1  4110.3
2       9475.2  9261.5
4       16945   16598
8       31238   30851
16      53908   54055
32      64890   63974
64      49891   50204
128     23642   23546
256     16068   15895
512     15360   15484
1024    15319   15389

# read/write (80% writes)
#thd    01      02
1       8007.6  7786.9
2       15966   15724
4       25405   25198
8       43806   43682
16      69070   67875
32      81656   81641
64      77996   78498
128     80387   77480
256     64941   63605
512     57655   58633
1024    56908   56282
{code}
",14,"Here the numbers (again queries per second) from the 64 core / 128 thread machine. Read-only is a bit faster for the head of the tree. All in all performance on this hardware is rather poor, probably caused by contention in the server layers above InnoDB.

{code}
# data set 01 -> mdev-10813 head
# data set 02 -> before (commit 5058ced)

# read only
#thd    01      02
1       8543.9  8550.4
2       16886   16951
4       32710   31471
8       60611   59759
16      84002   82922
32      61391   60169
64      38369   36371
128     39930   39849
256     20656   20974
512     23212   23343
1024    78925   73784

# read/write (10% writes)
#thd    01      02
1       5987.6  5849.4
2       12810   12402
4       24669   24228
8       45750   45463
16      67585   67701
32      67475   66547
64      52460   50705
128     28924   30246
256     17840   17841
512     17094   17108
1024    22048   20821

# read/write (22% writes)
#thd    01      02
1       4166.1  4110.3
2       9475.2  9261.5
4       16945   16598
8       31238   30851
16      53908   54055
32      64890   63974
64      49891   50204
128     23642   23546
256     16068   15895
512     15360   15484
1024    15319   15389

# read/write (80% writes)
#thd    01      02
1       8007.6  7786.9
2       15966   15724
4       25405   25198
8       43806   43682
16      69070   67875
32      81656   81641
64      77996   78498
128     80387   77480
256     64941   63605
512     57655   58633
1024    56908   56282
{code}
"
1707,MDEV-10813,MDEV,Sergey Vojtovich,87397,2016-10-14 07:39:20,According to the above there seem to be no performance/scalability regression introduced by relevant patches.,15,According to the above there seem to be no performance/scalability regression introduced by relevant patches.
1708,MDEV-10856,MDEV,Phil Sweeney,86853,2016-09-27 13:00:45,[~serg] Will 10.0 get an interim TokuDB merge that will come into 10.1.18/19.. or will we have to wait until December? (given there's not 10.0 release scheduled until late November).  Thanks.,1,[~serg] Will 10.0 get an interim TokuDB merge that will come into 10.1.18/19.. or will we have to wait until December? (given there's not 10.0 release scheduled until late November).  Thanks.
1709,MDEV-10866,MDEV,Alexander Barkov,87078,2016-10-04 16:24:55,Reviewed by Monty. Small suggestions were given.,1,Reviewed by Monty. Small suggestions were given.
1710,MDEV-10871,MDEV,Geoff Montee,99104,2017-08-22 19:04:28,Do we have plans to implement this at some point? Problems with pam_user_map.so are currently very difficult to debug.,1,Do we have plans to implement this at some point? Problems with pam_user_map.so are currently very difficult to debug.
1711,MDEV-10871,MDEV,Alexey Botchkov,108545,2018-03-18 08:36:01,"Implemented the 'debug' option, that would write excessive comments to the syslog.
http://lists.askmonty.org/pipermail/commits/2018-March/012101.html",2,"Implemented the 'debug' option, that would write excessive comments to the syslog.
URL"
1712,MDEV-10871,MDEV,Alexey Botchkov,108546,2018-03-18 08:38:24,"As far as i see the PAM libraries tend to write messages to the syslog, so my proposal is to send the debug output of the pam_user_map to the syslog as well.
Syslog is pretty flexible and can be directed to a file or other computer.",3,"As far as i see the PAM libraries tend to write messages to the syslog, so my proposal is to send the debug output of the pam_user_map to the syslog as well.
Syslog is pretty flexible and can be directed to a file or other computer."
1713,MDEV-10871,MDEV,Alexey Botchkov,108776,2018-03-22 20:51:01,http://lists.askmonty.org/pipermail/commits/2018-March/012126.html,4,URL
1714,MDEV-10914,MDEV,Alexander Barkov,91377,2017-02-02 19:05:29,Approved by Monty.,1,Approved by Monty.
1715,MDEV-10966,MDEV,Sergei Petrunia,90126,2017-01-02 10:07:06,"Another thing to check: https://github.com/facebook/mysql-5.6/commit/e804f86b203bb9384740e1fe92796dada6f865c9 .

MyRocks makes use of STL's Regex library.  Apparently, regex support is present but broken in GCC 4.8.
",1,"Another thing to check: URL .

MyRocks makes use of STL's Regex library.  Apparently, regex support is present but broken in GCC 4.8.
"
1716,MDEV-10966,MDEV,Sergei Petrunia,90127,2017-01-02 10:09:24,"Yet another thing to fix: I've added this into ha_rocksdb.h : 

{code:cpp}
#include ""../storage/xtradb/include/ut0counter.h""
/*
  'EMPTY' from field.h conflicts with EMPTY from
  /usr/include/x86_64-linux-gnu/bits/utmpx.h
  MARIAROCKS_NOT_YET: Sort out #include order so that we don't have to resort 
  to #undef
*/
#undef EMPTY
{code}

A better solution would be to get the #include order right.",2,"Yet another thing to fix: I've added this into ha_rocksdb.h : 

{code:cpp}
#include ""../storage/xtradb/include/ut0counter.h""
/*
  'EMPTY' from field.h conflicts with EMPTY from
  /usr/include/x86_64-linux-gnu/bits/utmpx.h
  MARIAROCKS_NOT_YET: Sort out #include order so that we don't have to resort 
  to #undef
*/
#undef EMPTY
{code}

A better solution would be to get the #include order right."
1717,MDEV-10966,MDEV,Sergei Petrunia,90128,2017-01-02 10:34:38,"If one looks here https://github.com/MariaDB/server/commits/10.2-mariarocks, one can see that Travis-CI succeeds in compiling MyRocks.",3,If one looks here URL one can see that Travis-CI succeeds in compiling MyRocks.
1718,MDEV-10966,MDEV,Sergei Petrunia,90538,2017-01-11 19:36:28,"I've got buildbot to build the current code
http://buildbot.askmonty.org/buildbot/grid?category=main&branch=bb-10.2-mariarocks

Unfortunately, no builder seems to be able to build it. The builders
- have old version gcc
- build from source tarball, which does not include rocksdb so they just dont build it
- using windows and having some issue with get_rocksdb_files.sh
- ...

This MDEV needs some attention.",4,"I've got buildbot to build the current code
URL

Unfortunately, no builder seems to be able to build it. The builders
- have old version gcc
- build from source tarball, which does not include rocksdb so they just dont build it
- using windows and having some issue with get_rocksdb_files.sh
- ...

This MDEV needs some attention."
1719,MDEV-10966,MDEV,Sergei Petrunia,90555,2017-01-12 09:37:59,"Actually, kvm-deb-trusty-amd64 has a recent gcc and builds from git.  It builds MyRocks SE and runs tests for it. Some tests pass, some (predictably) fail. ",5,"Actually, kvm-deb-trusty-amd64 has a recent gcc and builds from git.  It builds MyRocks SE and runs tests for it. Some tests pass, some (predictably) fail. "
1720,MDEV-11042,MDEV,Alexey Botchkov,90997,2017-01-23 22:50:11,http://lists.askmonty.org/pipermail/commits/2017-January/010493.html,1,URL
1721,MDEV-11097,MDEV,Elena Stepanova,87693,2016-10-23 15:49:40,https://github.com/MariaDB/server/commit/0c925aa9356ee9d31283510c2420d1b5f21f5c9c,1,URL
1722,MDEV-11130,MDEV,Elena Stepanova,87995,2016-11-02 13:36:26,https://github.com/MariaDB/server/commit/82780a7c0031af3fe6d8b9bc9f46ace2876c33f6,1,URL
1723,MDEV-11153,MDEV,Sergey Vojtovich,98532,2017-08-10 11:48:45,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
1724,MDEV-11159,MDEV,Dipti Joshi,87812,2016-10-27 12:18:24,This will help integration with MaxScale as well.,1,This will help integration with MaxScale as well.
1725,MDEV-11159,MDEV,Daniel Black,88767,2016-11-28 06:28:17,"interesting. I'd probably give this its own TCP port and vio->type to be checked in vio_peer_addr for avoiding complexity in the other cases.

Seem ssl client auth could be done also based on v2 metadata.",2,"interesting. I'd probably give this its own TCP port and vio->type to be checked in vio_peer_addr for avoiding complexity in the other cases.

Seem ssl client auth could be done also based on v2 metadata."
1726,MDEV-11159,MDEV,Laurynas Biveinis,93841,2017-04-06 02:49:51,"If you merge Percona Server implementation, consider reviewing https://bugs.launchpad.net/percona-server/+bugs?field.tag=proxy-protocol",3,"If you merge Percona Server implementation, consider reviewing URL"
1727,MDEV-11159,MDEV,Sergei Golubchik,93843,2017-04-06 05:58:07,"Thanks, [~laurynas]!",4,"Thanks, [~laurynas]!"
1728,MDEV-11159,MDEV,Vladislav Vaintroub,98781,2017-08-16 12:00:11,"I did not use Percona implementation, but kept server parameter proxy_protocol_networks compatible to Percona.
I believe our implementation does not have bugs mentioned in https://bugs.launchpad.net/percona-server/+bugs?field.tag=proxy-protocol . In addition, it is also tested  with mtr :)

the patch is there https://github.com/MariaDB/server/tree/bb-10.3-proxy-protocol",5,"I did not use Percona implementation, but kept server parameter proxy_protocol_networks compatible to Percona.
I believe our implementation does not have bugs mentioned in URL . In addition, it is also tested  with mtr :)

the patch is there URL"
1729,MDEV-11200,MDEV,Elena Stepanova,87948,2016-11-01 14:18:03,"While doing 10.0=>10.1 merges, please ignore mysql-test/unstable-tests completely (don't merge any changes at all). I'll update it separately in 10.0 and 10.1, it makes more sense this way.",1,"While doing 10.0=>10.1 merges, please ignore mysql-test/unstable-tests completely (don't merge any changes at all). I'll update it separately in 10.0 and 10.1, it makes more sense this way."
1730,MDEV-11212,MDEV,Sergey Vojtovich,87990,2016-11-02 12:49:00,"[~serg], please review top 6 patches at bb-10.2-mdev11212",1,"[~serg], please review top 6 patches at bb-10.2-mdev11212"
1731,MDEV-11212,MDEV,Sergey Vojtovich,88028,2016-11-03 08:08:59,"Build failed on kvm-bintar-centos5-x86 (~10 years old gcc-4.1.2), but succeeded on kvm-bintar-centos5-amd64. I can guess the problem is that x86 version didn't support atomic operations on a 64bit target.

CentOS 5 EOL is Mar 31, 2017. Do we need to care about it?",2,"Build failed on kvm-bintar-centos5-x86 (~10 years old gcc-4.1.2), but succeeded on kvm-bintar-centos5-amd64. I can guess the problem is that x86 version didn't support atomic operations on a 64bit target.

CentOS 5 EOL is Mar 31, 2017. Do we need to care about it?"
1732,MDEV-11212,MDEV,Sergey Vojtovich,88032,2016-11-03 09:22:23,7 patches,3,7 patches
1733,MDEV-11212,MDEV,Sergey Vojtovich,88547,2016-11-21 07:00:57,8 patches,4,8 patches
1734,MDEV-11212,MDEV,Sergei Golubchik,88925,2016-11-30 11:31:11,"I'd think we need to care about CentOS 5 until it's EOLed.
And then (or may be sooner) we'll need to find another bintar builder, perhaps.
Then we won't care about gcc 4.1.2",5,"I'd think we need to care about CentOS 5 until it's EOLed.
And then (or may be sooner) we'll need to find another bintar builder, perhaps.
Then we won't care about gcc 4.1.2"
1735,MDEV-11212,MDEV,Sergei Golubchik,88929,2016-11-30 11:49:56,ok to push into 10.3 tree,6,ok to push into 10.3 tree
1736,MDEV-11212,MDEV,Sergey Vojtovich,88931,2016-11-30 12:09:44,"[~serg], I can accept this, but it sounds a bit strange:
- we do not build 10.2 on somewhat more recent precise (EOL April 2017)
- 10.2 GA is scheduled for late Jan 2016, which leaves about a month of valid 10.2 usage till CentOS 5 EOL
- and that's only x86

Anyway, there's nothing of a big value for 10.2 here.",7,"[~serg], I can accept this, but it sounds a bit strange:
- we do not build 10.2 on somewhat more recent precise (EOL April 2017)
- 10.2 GA is scheduled for late Jan 2016, which leaves about a month of valid 10.2 usage till CentOS 5 EOL
- and that's only x86

Anyway, there's nothing of a big value for 10.2 here."
1737,MDEV-11239,MDEV,Eric Howey,88196,2016-11-08 17:18:47,Thank you very much for accepting this feature request! Is it not possible to add this to the next release of 5.5 as well?,1,Thank you very much for accepting this feature request! Is it not possible to add this to the next release of 5.5 as well?
1738,MDEV-11245,MDEV,Oleksandr Byelkin,88521,2016-11-19 20:29:25,OK to push.,1,OK to push.
1739,MDEV-11297,MDEV,Sergei Petrunia,88471,2016-11-17 19:04:01,"> Supporting LIMIT N[,M] could simplify queries, 

This is the goal of this MDEV.

> avoid passing lots of data to the client 
This is already achieved by using SUBSTRING_INDEX

> and may be optimized on the server side by not extracting complete GROUP BY ... ORDER BY result.

This is outside of scope of this MDEV. 

Let's limit this task to providing LIMIT syntax, and query results.

Loose-scan like optimization for reading only a few rows per group is theoretically possible, but hard to do in the current MariaDB's (or MySQL's) GROUP/ORDER BY optimizer.
",1,"> Supporting LIMIT N[,M] could simplify queries, 

This is the goal of this MDEV.

> avoid passing lots of data to the client 
This is already achieved by using SUBSTRING_INDEX

> and may be optimized on the server side by not extracting complete GROUP BY ... ORDER BY result.

This is outside of scope of this MDEV. 

Let's limit this task to providing LIMIT syntax, and query results.

Loose-scan like optimization for reading only a few rows per group is theoretically possible, but hard to do in the current MariaDB's (or MySQL's) GROUP/ORDER BY optimizer.
"
1740,MDEV-11297,MDEV,Varun Gupta,88788,2016-11-28 15:07:09,"The progress can be tracked here:

https://github.com/MariaDB/server/tree/bb-10.2-mdev11297",2,"The progress can be tracked here:

URL"
1741,MDEV-11297,MDEV,Sergei Petrunia,89066,2016-12-02 15:16:57,Review feedback was provided over email. Re-assigning to Varun as the ball is on his side now.,3,Review feedback was provided over email. Re-assigning to Varun as the ball is on his side now.
1742,MDEV-11297,MDEV,Varun Gupta,89107,2016-12-03 10:43:55,"Made changes according to the review, sent again for review to Sergei",4,"Made changes according to the review, sent again for review to Sergei"
1743,MDEV-11297,MDEV,Sergei Petrunia,89595,2016-12-15 16:32:14,"See also: MDEV-11563. It turns out, {{GROUP_CONCAT(DISTINCT x ORDER BY y)}} doesn't work.  We've discovered that when  discussing how  {{DISTINCT ... ORDER BY ...LIMIT}} should be done.",5,"See also: MDEV-11563. It turns out, {{GROUP_CONCAT(DISTINCT x ORDER BY y)}} doesn't work.  We've discovered that when  discussing how  {{DISTINCT ... ORDER BY ...LIMIT}} should be done."
1744,MDEV-11297,MDEV,Varun Gupta,103622,2017-11-26 18:26:04,"The branch was based on 10.2, so a rebase is done on top of 10.3. 
The code is currently pushed to the branch 10.3-varun",6,"The branch was based on 10.2, so a rebase is done on top of 10.3. 
The code is currently pushed to the branch 10.3-varun"
1745,MDEV-11297,MDEV,Oleksandr Byelkin,104214,2017-12-07 15:10:08,"Add yet another test like:
{code}
set @x=-1;
execute STMT using @x;
{code}
and if it return error it is ok to push (if no let us talk)",7,"Add yet another test like:
{code}
set @x=-1;
execute STMT using @x;
{code}
and if it return error it is ok to push (if no let us talk)"
1746,MDEV-11297,MDEV,Varun Gupta,104228,2017-12-07 22:36:51,pushed the code to bb-10.3-varun,8,pushed the code to bb-10.3-varun
1747,MDEV-11297,MDEV,A Prins,124860,2019-03-15 11:08:00,"LIMIT clause in GROUP_CONCAT() does not work in a view. 
This can be tested using the example on https://mariadb.com/kb/en/library/group_concat/. If you create a view from the statement 'SELECT SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",dd,cc) ORDER BY cc DESC),"","",1) FROM d;' than LIMIT does not do anything. Run the statement on its own and LIMIT works. I tested this in 10.3 and 10.4 docker container versions. 
Is that how it should be? I would like to be able to use LIMIT in a view.",9,"LIMIT clause in GROUP_CONCAT() does not work in a view. 
This can be tested using the example on URL If you create a view from the statement 'SELECT SUBSTRING_INDEX(GROUP_CONCAT(CONCAT_WS("":"",dd,cc) ORDER BY cc DESC),"","",1) FROM d;' than LIMIT does not do anything. Run the statement on its own and LIMIT works. I tested this in 10.3 and 10.4 docker container versions. 
Is that how it should be? I would like to be able to use LIMIT in a view."
1748,MDEV-11297,MDEV,Varun Gupta,124886,2019-03-15 19:00:07,"[~antoonp], thanks for reporting the bug, I have opened issue MDEV-18943 for this bug.",10,"[~antoonp], thanks for reporting the bug, I have opened issue MDEV-18943 for this bug."
1749,MDEV-11340,MDEV,Jabbar Memon,92919,2017-03-13 05:57:45,"hello sir,
i want to know more about this issue.can you plz elaborate..

Thank You
Jabbar Memon",1,"hello sir,
i want to know more about this issue.can you plz elaborate..

Thank You
Jabbar Memon"
1750,MDEV-11340,MDEV,Liam  Keller,93008,2017-03-14 16:19:05,"Hi, I am Liam Keller a SE student and I wish to work on this project for GSoC 2017. I think this feature is supported in Oracle from my findings and it will be great for MariaDB to have it too. I have been going through the grammar of SQL used by MariaDB especially for CREATE USER ... statement. My initial thoughts on the new syntax should allow us preserve the current syntax but extend it to support enumerating authentication methods. Something like should work for the new syntax.
{code}
CREATE [OR REPLACE] USER [IF NOT EXISTS] 
           user_specification [,user_specification] ...
           [REQUIRE {NONE | tls_option [[AND] tls_option] ...}]
           [WITH resource_option [resource_option] ...]

user_specification:
          username [authentication_option]

authentication_option:
           IDENTIFIED BY 'authentication_string' 
           | IDENTIFIED BY PASSWORD 'hash_string'
           | IDENTIFIED {VIA|WITH} authentication_plugin
           | IDENTIFIED {VIA|WITH} authentication_plugin BY 'authentication_string'
           | IDENTIFIED {VIA|WITH} authentication_plugin {USING|AS} 'hash_string'
         *| IDENTIFIED {VIA|WITH} auth_option [auth_option ...]*

tls_option:
        SSL 
       | X509
       | CIPHER 'cipher'
       | ISSUER 'issuer'
       | SUBJECT 'subject'

resource_option:
      MAX_QUERIES_PER_HOUR count
      | MAX_UPDATE_PER_HOUR count
      | MAX_CONNECTIONS_PER_HOUR count
      | MAX_USER_CONNECTIONS count
{code}

I will be working on a draft proposal for this project and I will share it with the Mailing list for review.
",2,"Hi, I am Liam Keller a SE student and I wish to work on this project for GSoC 2017. I think this feature is supported in Oracle from my findings and it will be great for MariaDB to have it too. I have been going through the grammar of SQL used by MariaDB especially for CREATE USER ... statement. My initial thoughts on the new syntax should allow us preserve the current syntax but extend it to support enumerating authentication methods. Something like should work for the new syntax.
{code}
CREATE [OR REPLACE] USER [IF NOT EXISTS] 
           user_specification [,user_specification] ...
           [REQUIRE {NONE | tls_option [[AND] tls_option] ...}]
           [WITH resource_option [resource_option] ...]

user_specification:
          username [authentication_option]

authentication_option:
           IDENTIFIED BY 'authentication_string' 
           | IDENTIFIED BY PASSWORD 'hash_string'
           | IDENTIFIED {VIA|WITH} authentication_plugin
           | IDENTIFIED {VIA|WITH} authentication_plugin BY 'authentication_string'
           | IDENTIFIED {VIA|WITH} authentication_plugin {USING|AS} 'hash_string'
         *| IDENTIFIED {VIA|WITH} auth_option [auth_option ...]*

tls_option:
        SSL 
       | X509
       | CIPHER 'cipher'
       | ISSUER 'issuer'
       | SUBJECT 'subject'

resource_option:
      MAX_QUERIES_PER_HOUR count
      | MAX_UPDATE_PER_HOUR count
      | MAX_CONNECTIONS_PER_HOUR count
      | MAX_USER_CONNECTIONS count
{code}

I will be working on a draft proposal for this project and I will share it with the Mailing list for review.
"
1751,MDEV-11340,MDEV,Daniel Black,93355,2017-03-23 00:58:02,"Good start - submissions are open now on https://summerofcode.withgoogle.com
Try to address and consider the other questions asked too.",3,"Good start - submissions are open now on URL
Try to address and consider the other questions asked too."
1752,MDEV-11340,MDEV,Sergei Golubchik,95252,2017-05-15 09:43:48,"h1. Assorted thoughts
h2. 1.
We _could_ try to stick everything into the existing API and protocol, by creating {{AND}} and {{OR}} authentication plugins. Or one {{META}} (or, say, {{MULTI}}, or {{POLICY}}) plugin. Like
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA POLICY USING ""(unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'""
{code}
(n) This is not particularly readable
(y) It can be stored in the existing mysql.user table
(n) Needs policy-client plugin
(y) Does not change the protocol, at least formally. Practically it introduces sub-protocol that is implemented by a policy plugin, but there’s no way around it
h2. 2.
The opposite, fully native built-in support. The syntax can look like
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA (unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'
{code}
Or even
{code:sql}
CREATE POLICY pol1 USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'
CREATE USER foo@bar IDENTIFIED VIA pol1
{code}
Plugins (or policies) are stored in a separate table {{mysql.policy}}, and {{mysql.user}} just stores a policy id.
(n) Rather more intrusive
(y) Policies can be used also to specify password validation, usage limits (max queries per hour, etc) and so on. Per user settings (e.g. mqh) overwrite policy.
(y) Easier to maintain for setups with many users (just one command to alter mqh for all users).
(n) Changes the protocol, needs client-side changes (but it cannot be helped)
(n) needs to move actual password out of the policy, they still need to be stored somewhere.
h2. 3.
Something in the middle, no policies, still stored in a {{mysql.user table}}, but a dedicated syntax.
h2. 4.
May be, PAM semantics?
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA pam AS 'mariadb_local' required, unix_socket sufficient,  mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF' sufficient
{code}
(n) Not quite the same, AND/OR syntax seems to be more powerful and more natural too.
h2. 5.
Our main use case seems to be really just {{unix_socket OR mysql_native_password}}, so we could limit the scope and only implement OR, not AND or parentheses. This wouldn’t simplify much and wouldn’t avoid the implementation question (1. Plugins 2. Native 3. Middle ground)
",4,"h1. Assorted thoughts
h2. 1.
We _could_ try to stick everything into the existing API and protocol, by creating {{AND}} and {{OR}} authentication plugins. Or one {{META}} (or, say, {{MULTI}}, or {{POLICY}}) plugin. Like
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA POLICY USING ""(unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'""
{code}
(n) This is not particularly readable
(y) It can be stored in the existing mysql.user table
(n) Needs policy-client plugin
(y) Does not change the protocol, at least formally. Practically it introduces sub-protocol that is implemented by a policy plugin, but there’s no way around it
h2. 2.
The opposite, fully native built-in support. The syntax can look like
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA (unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'
{code}
Or even
{code:sql}
CREATE POLICY pol1 USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'
CREATE USER foo@bar IDENTIFIED VIA pol1
{code}
Plugins (or policies) are stored in a separate table {{mysql.policy}}, and {{mysql.user}} just stores a policy id.
(n) Rather more intrusive
(y) Policies can be used also to specify password validation, usage limits (max queries per hour, etc) and so on. Per user settings (e.g. mqh) overwrite policy.
(y) Easier to maintain for setups with many users (just one command to alter mqh for all users).
(n) Changes the protocol, needs client-side changes (but it cannot be helped)
(n) needs to move actual password out of the policy, they still need to be stored somewhere.
h2. 3.
Something in the middle, no policies, still stored in a {{mysql.user table}}, but a dedicated syntax.
h2. 4.
May be, PAM semantics?
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA pam AS 'mariadb_local' required, unix_socket sufficient,  mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF' sufficient
{code}
(n) Not quite the same, AND/OR syntax seems to be more powerful and more natural too.
h2. 5.
Our main use case seems to be really just {{unix_socket OR mysql_native_password}}, so we could limit the scope and only implement OR, not AND or parentheses. This wouldn’t simplify much and wouldn’t avoid the implementation question (1. Plugins 2. Native 3. Middle ground)
"
1753,MDEV-11340,MDEV,Andrii Nikitin,95257,2017-05-15 10:40:16,"We can consider using existing table mysql.roles_mapping for authentication
E.g. we can have these roles:
authentication_pam_allowed
authentication_unix_socket_allowed
authentication_unix_socket_required
And if user is part of those roles, then he is allowed/required to use particular authentication.",5,"We can consider using existing table mysql.roles_mapping for authentication
E.g. we can have these roles:
authentication_pam_allowed
authentication_unix_socket_allowed
authentication_unix_socket_required
And if user is part of those roles, then he is allowed/required to use particular authentication."
1754,MDEV-11340,MDEV,Sergei Golubchik,95260,2017-05-15 10:48:53,"In some cases the client wants to be able to choose. As the extension of the above idea, for a case of {{IDENTIFIED WITH A OR B OR C}} a client can choose to force plugin *B*, and the server will automatically consider *A* and *C* as failed.",6,"In some cases the client wants to be able to choose. As the extension of the above idea, for a case of {{IDENTIFIED WITH A OR B OR C}} a client can choose to force plugin *B*, and the server will automatically consider *A* and *C* as failed."
1755,MDEV-11340,MDEV,Sergei Golubchik,95292,2017-05-15 17:35:56,"Because 1) a password shouldn't be part of a policy, but 2) the server doesn't know what plugin parameters are passwords, a possible solution can be
{code:sql}
CREATE POLICY pol1 USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS ?) AND pam AS 'mariadb_local';
CREATE USER foo@bar IDENTIFIED VIA pol1('*F3A2A51A9B0F2BE2468926B4132313728C250DBF');
{code}
Or even
{code:sql}
CREATE POLICY pol1 USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS ?) AND pam AS 'mariadb_local';
CREATE USER foo@bar IDENTIFIED VIA pol1 USING '*F3A2A51A9B0F2BE2468926B4132313728C250DBF';
{code}
Note that in both cases {{'mariadb_local'}} was still hard-coded in the policy

This syntax becomes rather problematic if one wants to prepare {{CREATE POLICY}} statement. Alternatively, one can do named parameters:
{code:sql}
CREATE POLICY pol1(pass) USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS pass) AND pam AS 'mariadb_local';
CREATE USER foo@bar IDENTIFIED VIA pol1('*F3A2A51A9B0F2BE2468926B4132313728C250DBF');
{code}
but that looks like an overkill",7,"Because 1) a password shouldn't be part of a policy, but 2) the server doesn't know what plugin parameters are passwords, a possible solution can be
{code:sql}
CREATE POLICY pol1 USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS ?) AND pam AS 'mariadb_local';
CREATE USER foo@bar IDENTIFIED VIA pol1('*F3A2A51A9B0F2BE2468926B4132313728C250DBF');
{code}
Or even
{code:sql}
CREATE POLICY pol1 USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS ?) AND pam AS 'mariadb_local';
CREATE USER foo@bar IDENTIFIED VIA pol1 USING '*F3A2A51A9B0F2BE2468926B4132313728C250DBF';
{code}
Note that in both cases {{'mariadb_local'}} was still hard-coded in the policy

This syntax becomes rather problematic if one wants to prepare {{CREATE POLICY}} statement. Alternatively, one can do named parameters:
{code:sql}
CREATE POLICY pol1(pass) USER IDENTIFIED VIA (unix_socket OR mysql_native_password AS pass) AND pam AS 'mariadb_local';
CREATE USER foo@bar IDENTIFIED VIA pol1('*F3A2A51A9B0F2BE2468926B4132313728C250DBF');
{code}
but that looks like an overkill"
1756,MDEV-11340,MDEV,Sergei Golubchik,95360,2017-05-16 14:51:50,About protocol changes: current protocol already supports server telling client what plugin to switch to. Perhaps this can be used to avoid any protocol changes?,8,About protocol changes: current protocol already supports server telling client what plugin to switch to. Perhaps this can be used to avoid any protocol changes?
1757,MDEV-11340,MDEV,Vladislav Vaintroub,95361,2017-05-16 14:53:27,"Protocol also supports client telling server which plugin to use. I do not think it works, but the field is there.",9,"Protocol also supports client telling server which plugin to use. I do not think it works, but the field is there."
1758,MDEV-11340,MDEV,Sergei Golubchik,95589,2017-05-22 07:52:02,"On the other hand, if the policy is strictly a way to simplify user administration, a way to give some name to a set of user settings, then everything a policy can do, should be doable without. In that case, this should work too:
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA (unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'
{code}
and the concept of ""policy"" could be moved to a separate independent project.",10,"On the other hand, if the policy is strictly a way to simplify user administration, a way to give some name to a set of user settings, then everything a policy can do, should be doable without. In that case, this should work too:
{code:sql}
CREATE USER foo@bar IDENTIFIED VIA (unix_socket OR mysql_native_password AS '*F3A2A51A9B0F2BE2468926B4132313728C250DBF') AND pam AS 'mariadb_local'
{code}
and the concept of ""policy"" could be moved to a separate independent project."
1759,MDEV-11340,MDEV,Andrii Nikitin,95592,2017-05-22 09:14:09,"My concern is that it will be quite headache to parse/build these AND / OR expressions, e.g. for external tools or to build some advanced analytics from these expressions. Therefore I'd stick to Codd's rule #1 ""All information in a relational data base is represented explicitly at the logical level and in exactly one way – by values in tables.""  and rule #4 ""The data base description is represented at the logical level in the same way as ordinary data, so that authorized users can apply the same relational language to its interrogation as they apply to the regular data.""
E.g. that should:
- provide possibility to retrieve list of all users who is allowed to connect trough 'pam' and is not allowed to connect trough 'unix_socket' with simple SELECT .. WHERE query (joins allowed, but advanced parsing of content of column(s) is not allowed).
- provide easy way to revoke / add 'policies' (i.e. with simple INSERT / DELETE or UPDATE commands).

Of course one can say that after all that information will be stored in relational tables and tools could use such SQL commands to avoid parsing AND / OR . But that would mean that we will have to support two notations and always try to make sure that these notations interact in consistent way.

Thus I prefer to have single notation where we define / retrieve info with (simple) INSERT / DELETE / UPDATE / SELECT commands and should design syntax basing on that.",11,"My concern is that it will be quite headache to parse/build these AND / OR expressions, e.g. for external tools or to build some advanced analytics from these expressions. Therefore I'd stick to Codd's rule #1 ""All information in a relational data base is represented explicitly at the logical level and in exactly one way – by values in tables.""  and rule #4 ""The data base description is represented at the logical level in the same way as ordinary data, so that authorized users can apply the same relational language to its interrogation as they apply to the regular data.""
E.g. that should:
- provide possibility to retrieve list of all users who is allowed to connect trough 'pam' and is not allowed to connect trough 'unix_socket' with simple SELECT .. WHERE query (joins allowed, but advanced parsing of content of column(s) is not allowed).
- provide easy way to revoke / add 'policies' (i.e. with simple INSERT / DELETE or UPDATE commands).

Of course one can say that after all that information will be stored in relational tables and tools could use such SQL commands to avoid parsing AND / OR . But that would mean that we will have to support two notations and always try to make sure that these notations interact in consistent way.

Thus I prefer to have single notation where we define / retrieve info with (simple) INSERT / DELETE / UPDATE / SELECT commands and should design syntax basing on that."
1760,MDEV-11340,MDEV,Sergei Golubchik,95599,2017-05-22 10:21:37,"Right, sorry. I had this in mind but forgot to write. If we do this AND/OR syntax (and, perhaps, if we don't) there should be some INFORMATION_SCHEMA tables presenting current authentication rules to users. Nobody should need to parse {{SHOW CREATE USER}} to understand how to authentication is configured.

but if you want to suggest some other notation, please, feel free to.",12,"Right, sorry. I had this in mind but forgot to write. If we do this AND/OR syntax (and, perhaps, if we don't) there should be some INFORMATION_SCHEMA tables presenting current authentication rules to users. Nobody should need to parse {{SHOW CREATE USER}} to understand how to authentication is configured.

but if you want to suggest some other notation, please, feel free to."
1761,MDEV-11340,MDEV,Andrii Nikitin,95604,2017-05-22 11:02:22,"Reading from I_S will solve part of problem, but it still will be problematic to modify methods, e.g. revoke 'unix_socket' from those who can connect trough 'pam'.

Below is one of ways to implement the suggestion with roles_mapping:

{code:sql}
# define roles
insert into mysql.user(user, plugin, is_role) 
select 'authentication_unix_socket', 'unix_socket', 1,
union 'authentication_pam', 'pam', 1;

# let foo@bar connect trough unix_socket
insert into mysql.roles_mapping (Host, User, Role)
select 'bar', 'foo', 'authentication_unix_socket';

# now later revoke such possibility
delete from mysql.roles_mapping where (host, user, role) in ('bar', 'foo', 'authentication_unix_socket');

# or revoke  'authentication_unix_socket' from those who have  'authentication_pam'
delete from mysql.roles_mapping a where (host, user, role) in (select host, user, 'authentication_unix_socket' from mysql.roles_mapping where role = 'authentication_pam');
{code}


Now, to define 'optional vs forced'  unix_socket authentication, we can either reuse some column from 'user' table, or add new column to either 'user' or 'roles_mapping'.

If we have strong reasons to not abuse roles with authentication methods, we can probably define new table 'authentication_role' or 'authentication_group' and try to use the same way as suggestion above",13,"Reading from I_S will solve part of problem, but it still will be problematic to modify methods, e.g. revoke 'unix_socket' from those who can connect trough 'pam'.

Below is one of ways to implement the suggestion with roles_mapping:

{code:sql}
# define roles
insert into mysql.user(user, plugin, is_role) 
select 'authentication_unix_socket', 'unix_socket', 1,
union 'authentication_pam', 'pam', 1;

# let foo@bar connect trough unix_socket
insert into mysql.roles_mapping (Host, User, Role)
select 'bar', 'foo', 'authentication_unix_socket';

# now later revoke such possibility
delete from mysql.roles_mapping where (host, user, role) in ('bar', 'foo', 'authentication_unix_socket');

# or revoke  'authentication_unix_socket' from those who have  'authentication_pam'
delete from mysql.roles_mapping a where (host, user, role) in (select host, user, 'authentication_unix_socket' from mysql.roles_mapping where role = 'authentication_pam');
{code}


Now, to define 'optional vs forced'  unix_socket authentication, we can either reuse some column from 'user' table, or add new column to either 'user' or 'roles_mapping'.

If we have strong reasons to not abuse roles with authentication methods, we can probably define new table 'authentication_role' or 'authentication_group' and try to use the same way as suggestion above"
1762,MDEV-11340,MDEV,Oleksandr Byelkin,111733,2018-05-30 14:27:08,I read all this and for me it is still not clear even which syntax to use.,14,I read all this and for me it is still not clear even which syntax to use.
1763,MDEV-11340,MDEV,Vladislav Vaintroub,122255,2019-01-22 19:25:44,commented on the https://github.com/MariaDB/server/commit/1e4d7ba29ed19f8edfdcc97f408ac1d5d07956e6,15,commented on the URL
1764,MDEV-11346,MDEV,Alexander Barkov,88673,2016-11-24 17:59:28,Approved by Sanja. Pushed to 10.3.0,1,Approved by Sanja. Pushed to 10.3.0
1765,MDEV-11347,MDEV,Alexander Barkov,88698,2016-11-25 18:46:55,Approved by Sanja.,1,Approved by Sanja.
1766,MDEV-11347,MDEV,Alexander Barkov,92867,2017-03-10 11:55:47,Reviewed by Sanja.,2,Reviewed by Sanja.
1767,MDEV-11369,MDEV,Marko Mäkelä,88822,2016-11-29 11:48:14,"The original InnoDB ROW_FORMAT=REDUNDANT should support Postgres-style instant operations out of the box, because the record header explicitly stores the number of fields as well as the start address of each field. The Postgres-style operations are ADD COLUMN…LAST and DROP COLUMN (the latter is implemented by hiding columns and storing dummy columns for new records).

A simple proof of concept exists in MariaDB 10.2 and upstream MySQL 5.7: The column MERGE_THRESHOLD was appended to the InnoDB data dictionary table SYS_INDEXES without rewriting existing records. Old data dictionary entries will deliver a default value.

While ROW_FORMAT=REDUNDANT introduces some storage overhead, it should also provide faster access, because the start address of each field is explicitly stored in the index record header. No changes to applications are needed if innodb_default_row_format=redundant is set in the configuration files.

Also in this task, ALTER TABLE will not be instantaneous if it includes any data-writing operations, such as ADD [UNIQUE] INDEX. DROP COLUMN may imply DROP INDEX, but that is reasonably fast (not traversing the entire index tree).

In a later MariaDB version, we could support a wider range of instant ALTER TABLE operations by repurposing some bits or bytes in the clustered index leaf page record header for representing a data dictionary version, by keeping all versions of the table definition in the data dictionary, and by converting old-format records to the current format. Note that some operations will need to write to data files and cannot be instantaneous. Such operations may also fail. Examples include introducing a NOT NULL attribute, changing the data type of a column, or ADD UNIQUE INDEX.",1,"The original InnoDB ROW_FORMAT=REDUNDANT should support Postgres-style instant operations out of the box, because the record header explicitly stores the number of fields as well as the start address of each field. The Postgres-style operations are ADD COLUMN…LAST and DROP COLUMN (the latter is implemented by hiding columns and storing dummy columns for new records).

A simple proof of concept exists in MariaDB 10.2 and upstream MySQL 5.7: The column MERGE_THRESHOLD was appended to the InnoDB data dictionary table SYS_INDEXES without rewriting existing records. Old data dictionary entries will deliver a default value.

While ROW_FORMAT=REDUNDANT introduces some storage overhead, it should also provide faster access, because the start address of each field is explicitly stored in the index record header. No changes to applications are needed if innodb_default_row_format=redundant is set in the configuration files.

Also in this task, ALTER TABLE will not be instantaneous if it includes any data-writing operations, such as ADD [UNIQUE] INDEX. DROP COLUMN may imply DROP INDEX, but that is reasonably fast (not traversing the entire index tree).

In a later MariaDB version, we could support a wider range of instant ALTER TABLE operations by repurposing some bits or bytes in the clustered index leaf page record header for representing a data dictionary version, by keeping all versions of the table definition in the data dictionary, and by converting old-format records to the current format. Note that some operations will need to write to data files and cannot be instantaneous. Such operations may also fail. Examples include introducing a NOT NULL attribute, changing the data type of a column, or ADD UNIQUE INDEX."
1768,MDEV-11369,MDEV,vinchen,88901,2016-11-30 07:56:36,"Hi, I am the main designer of instant adding column from Tencent.

It looks exciting of your new ideas of  storeing dictionary version in record. Maybe it's a big project.

But I have some questions of it:
1. create table t1(a int, b int, c int); -- dictionary version 1
alter table t1 drop column c; --  dictionary version 2
alter table t1 add column c int not null default 10; -- dictionary version 3

How should the record of version 1 to identify the value of column ""c“ ? 
Maybe the correct value is 10(default value). Not the value stored in the record(version 1).
Maybe it should check all the dictionary versions to get the true result?

2. Do it bring the complexity of parsing the record, and resulting in the performance loss?
",2,"Hi, I am the main designer of instant adding column from Tencent.

It looks exciting of your new ideas of  storeing dictionary version in record. Maybe it's a big project.

But I have some questions of it:
1. create table t1(a int, b int, c int); -- dictionary version 1
alter table t1 drop column c; --  dictionary version 2
alter table t1 add column c int not null default 10; -- dictionary version 3

How should the record of version 1 to identify the value of column ""c“ ? 
Maybe the correct value is 10(default value). Not the value stored in the record(version 1).
Maybe it should check all the dictionary versions to get the true result?

2. Do it bring the complexity of parsing the record, and resulting in the performance loss?
"
1769,MDEV-11369,MDEV,Marko Mäkelä,88919,2016-11-30 09:52:21,"Hi Vinchen, it was nice to get your feedback.

Here are short answers to your questions.
1. MDEV-11424 contains an extended version of your example that answers the question. We must use the value 10.
2. Yes. To reduce the amount of repeated conversion of readers (which would not write back the converted records), we could say that any write to a page could update all records in the page to the newest version. In this case, we would only need one version identifier per page.

I think that we should also defer DROP COLUMN to MDEV-11424.
I am looking forward to your comments in MDEV-11424.
In this ticket, let us discuss only the ADD COLUMN.",3,"Hi Vinchen, it was nice to get your feedback.

Here are short answers to your questions.
1. MDEV-11424 contains an extended version of your example that answers the question. We must use the value 10.
2. Yes. To reduce the amount of repeated conversion of readers (which would not write back the converted records), we could say that any write to a page could update all records in the page to the newest version. In this case, we would only need one version identifier per page.

I think that we should also defer DROP COLUMN to MDEV-11424.
I am looking forward to your comments in MDEV-11424.
In this ticket, let us discuss only the ADD COLUMN."
1770,MDEV-11369,MDEV,Sergei Golubchik,88920,2016-11-30 09:53:14,"I thought (it's not what Marko has written, but pretty close) about storing a ""table definition version"" per row. And — in some separate place, for example, in a new InnoDB system table — a set of steps of how to convert the row from the version N-1 to the version N. In the first version the only supported step could be ""add column X at the end with the default value Y"". Then this feature will be identical to your ""instant add column"", only in-row format will be different. But later this version-based idea could be extended to support other steps, like, ""drop column X"", ""move column X to be after column Y"", and so on. When supporting only adding a column, this is not a big project.

1. yes, to convert a row from the version 1 to version 3, InnoDB would need to do two steps ""drop column c"", ""add column c, default 10"". it should execute all steps to transform from version 1 to 3.

2. may be, but not necessarily. it depends on what these steps will be. if they're defined in terms of bytes, than executing a step will be a memcpy, no need to parse a row. On the other hand, InnoDB needs to parse a row anyway, to convert it to mysql row format. So, doing the conversion on the parsed row is also possible, it will not cause additional re-parsing.",4,"I thought (it's not what Marko has written, but pretty close) about storing a ""table definition version"" per row. And — in some separate place, for example, in a new InnoDB system table — a set of steps of how to convert the row from the version N-1 to the version N. In the first version the only supported step could be ""add column X at the end with the default value Y"". Then this feature will be identical to your ""instant add column"", only in-row format will be different. But later this version-based idea could be extended to support other steps, like, ""drop column X"", ""move column X to be after column Y"", and so on. When supporting only adding a column, this is not a big project.

1. yes, to convert a row from the version 1 to version 3, InnoDB would need to do two steps ""drop column c"", ""add column c, default 10"". it should execute all steps to transform from version 1 to 3.

2. may be, but not necessarily. it depends on what these steps will be. if they're defined in terms of bytes, than executing a step will be a memcpy, no need to parse a row. On the other hand, InnoDB needs to parse a row anyway, to convert it to mysql row format. So, doing the conversion on the parsed row is also possible, it will not cause additional re-parsing."
1771,MDEV-11369,MDEV,Michael Widenius,89759,2016-12-20 10:48:46,"I am against having a separate table for for table version as this makes it hard to do easy export/import of tables.

What we need first and foremost is to be able to quickly add new columns last, as this is one of the most common ALTER TABLE operations that can cause problems in production.  I don't want to see this task being delayed infinitely just because we want be over ambitious with what we want to do.  Now we need something that we can have stable in February!
",5,"I am against having a separate table for for table version as this makes it hard to do easy export/import of tables.

What we need first and foremost is to be able to quickly add new columns last, as this is one of the most common ALTER TABLE operations that can cause problems in production.  I don't want to see this task being delayed infinitely just because we want be over ambitious with what we want to do.  Now we need something that we can have stable in February!
"
1772,MDEV-11369,MDEV,Marko Mäkelä,89761,2016-12-20 12:21:06,"[~serg], [~monty], I think that your comments apply to MDEV-11424. Please let us keep this MDEV-11369 to be clean of that.

My proposal for this MDEV-11369 is that we cover ALTER TABLE…ADD COLUMN…LAST for tables that are already in ROW_FORMAT=REDUNDANT. In these tables, each record contains the number of columns. We can easily append columns to the end; upon reading, we would replace missing column values with the DEFAULT value.

However, the .frm file should somehow remember the original column count, so that we can refuse the change of column DEFAULT values for those columns that have been added after the table was originally created. This would prevent the problematic scenario that [~vinchen] presented on 2016-11-30.

I think that related to this change, we should introduce a number of ALGORITHM variants. Quoting from MDEV-11424: We might want to introduce ALTER TABLE qualifiers to prevent negative surprises. For example, ALGORITHM=QUICK would refuse ADD INDEX, but it would allow DROP INDEX and any metadata-only changes. ALGORITHM=NOCOPY would allow ADD INDEX, but it would refuse to rebuild the whole table.

In MDEV-11369, changing the DEFAULT value would be allowed with ALGORITHM=QUICK when we are not changing the DEFAULT of already added columns. And DROP COLUMN would always be refused with ALGORITHM=NOCOPY.",6,"[~serg], [~monty], I think that your comments apply to MDEV-11424. Please let us keep this MDEV-11369 to be clean of that.

My proposal for this MDEV-11369 is that we cover ALTER TABLE…ADD COLUMN…LAST for tables that are already in ROW_FORMAT=REDUNDANT. In these tables, each record contains the number of columns. We can easily append columns to the end; upon reading, we would replace missing column values with the DEFAULT value.

However, the .frm file should somehow remember the original column count, so that we can refuse the change of column DEFAULT values for those columns that have been added after the table was originally created. This would prevent the problematic scenario that [~vinchen] presented on 2016-11-30.

I think that related to this change, we should introduce a number of ALGORITHM variants. Quoting from MDEV-11424: We might want to introduce ALTER TABLE qualifiers to prevent negative surprises. For example, ALGORITHM=QUICK would refuse ADD INDEX, but it would allow DROP INDEX and any metadata-only changes. ALGORITHM=NOCOPY would allow ADD INDEX, but it would refuse to rebuild the whole table.

In MDEV-11369, changing the DEFAULT value would be allowed with ALGORITHM=QUICK when we are not changing the DEFAULT of already added columns. And DROP COLUMN would always be refused with ALGORITHM=NOCOPY."
1773,MDEV-11369,MDEV,Marko Mäkelä,89867,2016-12-23 14:02:05,"h1. Page-level format tracking instead of row-level

Tencent has implemented instant ADD COLUMN by introducing new ROW_FORMAT variants that explicitly store the number of fields in index records. With the original InnoDB format (ROW_FORMAT=REDUNDANT) we could have something like that as is, because that format stores the number of fields.

However, there is an alternative that is compatible with old data files that have been created in any ROW_FORMAT. Instead of tracking the number of columns for each record, we would track it for each page.

h2. Implementation of the instant ADD COLUMN operation

As suggested in MDEV-11424 for more general instant ALTER, we can repurpose the unused field PAGE_MAX_TRX_ID in clustered index leaf pages. All InnoDB versions at least since MySQL 3.23.49 wrote this field as 0. We would only support ADD COLUMN with the following restrictions:
# Only ADD COLUMN…LAST. Any DROP COLUMN or change of column order will require a full table rebuild.
# Only constant DEFAULT values.
# Changing the DEFAULT of instantly-added columns will require a full table rebuild.

A new field PAGE_CLUST_LEAF_FIELDS will be aliased to PAGE_MAX_TRX_ID and defined as follows:
# On non-leaf pages, there is no PAGE_CLUST_LEAF_FIELDS. (In clustered index non-leaf pages other than the root, the PAGE_MAX_TRX_ID will remain unused (0). For the clustered index root page, MDEV-6076 introduced PAGE_ROOT_AUTO_INC.)
# If the clustered index consists of a single page (root and leaf), PAGE_CLUST_LEAF_FIELDS does not exist, and the records must be in the newest format. (MDEV-6076 repurposed the PAGE_MAX_TRX_ID of the clustered index root page as PAGE_ROOT_AUTO_INC.)
# If the PAGE_CLUST_LEAF_FIELDS of the leftmost leaf page is 0, all PAGE_CLUST_LEAF_FIELDS must be 0. (These are tables where instant ALTER TABLE (MDEV-11369 or MDEV-11424) has not been used, or the table has been rebuilt since then.)
# In the leftmost leaf page, PAGE_CLUST_LEAF_FIELDS contains the original number of fields in the clustered index, before any instant ADD COLUMN operations. This field will be cached in dict_table_t::clust_leaf_fields. The records in the leftmost leaf page must always have the latest number of columns.
# In other leaf pages of the clustered index, if PAGE_CLUST_LEAF_FIELDS is 0, the number of fields in the records will be dict_table_t::clust_leaf_fields. Else, PAGE_CLUST_LEAF_FIELDS is the number of fields in the records, and it must be between index->table->clust_leaf_fields+1 and index->n_fields.
# Only the PAGE_CLUST_LEAF_FIELDS values 1 to 1023 will be reserved for this purpose. (InnoDB does not support indexes with more fields.) The values 1024 to (1<<64)-1 could be used by MDEV-11424 later.

From the above it follows that instant ADD COLUMN must convert the leftmost leaf page or the root-leaf page to the newest format. Unless the conversion results into a page split, only a single data page will be modified.

The data dictionary (both the .frm file and the InnoDB SYS_ tables) will contain the latest table definition. The number of clustered index fields before the first instant ADD COLUMN is only stored in dict_table_t::clust_leaf_fields the PAGE_CLUST_LEAF_FIELDS field of the leftmost leaf page. If multiple ALTER TABLE…ADD COLUMN statements are executed on the same table, the dict_table_t::clust_leaf_fields will not change.

When determining whether changing the DEFAULT value of some columns requires a table rebuild, InnoDB must determine if the clustered index position of any of the affected columns is dict_table_t::clust_leaf_fields or greater. If table->clust_leaf_fields == 0, instant ADD COLUMN has not been used, and any DEFAULT values can be changed instantly.

h2. Data format changes

The format of secondary index pages will be unaffected by instant ALTER TABLE.

The clustered index leaf page format is affected by the instant ADD COLUMN, by the introduction of the PAGE_CLUST_LEAF_FIELDS (repurposed from PAGE_MAX_TRX_ID).

Non-leaf, non-root clustered index pages will be unaffected.

The clustered index root page format will have to be slightly changed to avoid an unintended change to the node pointer records when ROW_FORMAT is COMPACT, DYNAMIC or COMPRESSED. There is no problem with ROW_FORMAT=REDUNDANT.

In clustered index node pointer records, all fields will be NOT NULL, because the PRIMARY KEY columns cannot be NULL. Nevertheless, the COMPACT, DYNAMIC and COMPRESSED clustered index node pointer record header will unnecessarily allocate 0‥128 zero bytes to represent NULL flags in the node pointers. The 0‥128 corresponds to UT_BITS_IN_BYTES(index->n_nullable). index->n_nullable can be 0‥1017 in the clustered index.

So, we must find one byte in the clustered index root page to store the number of the redundant zero bytes in the node pointer record header. Luckily, only 3 bits of the 16-bit field PAGE_DIRECTION are used.

We will repurpose the most significant byte of PAGE_DIRECTION in all COMPACT, DYNAMIC, REDUNDANT index root pages as follows: If the field is nonzero, it will be set to 1+UT_BITS_IN_BYTES(index->n_nullable), to be used when interpreting the node pointer records. (The byte was always set to 0 until now.) The value will also be cached in a new field dict_index_t::node_ptr_null_bytes (always 0 if ROW_FORMAT=REDUNDANT). This will not only solve the problem at hand, but it will also allow us to avoid wasting space on node pointer pages in the future.

h2. Changes to read operations

When reading records from a page, we must use the current number of fields in the page. This will require some modifications to rec_get_offsets() or its callers, especially because UT_BITS_IN_BYTES(index->n_nullable) may change. When returning a record to the SQL layer, InnoDB has to fill in the DEFAULT values of any instantly-added columns that were missing from the leaf page.

If an index is created on an instantly added column, InnoDB may need to access the DEFAULT values of instantly added columns. For this purpose, we probably should move row_prebuilt_t::default_rec to dict_table_t::default_rec.

h2. Changes to write operations

Because we maintain the number of fields at the page level, all records within the clustered index leaf page must have the same number of fields.

This means that whenever any record within the page is inserted, or a not-yet-instantiated instantly added column is updated, all records on the page must be converted. This could require the leaf page to be split into multiple pages.

Page splits and merges will convert all records in all affected leaf pages to the newest format.

Page reorganize does not need to convert records, and maybe it should, because the conversion could cause a page overflow. This could mean that page reorganize should memcpy() the individual records instead of invoking page_copy_rec_list_end_no_locks() which would by default use the latest index definition.

It is optional to convert pages when records are purged (deleted) or the delete-mark bit is changed (for example, BEGIN; DELETE FROM instantly_altered_table; ROLLBACK).

h2. Example

{code:SQL}
CREATE TABLE t(
  a INT NOT NULL, b INT NOT NULL,
  c1 CHAR(255) NOT NULL DEFAULT '',
  c2 CHAR(255) NOT NULL DEFAULT '',
  c3 CHAR(255) NOT NULL DEFAULT '',
  c4 CHAR(255) NOT NULL DEFAULT '',
  c5 CHAR(255) NOT NULL DEFAULT '',
  c6 CHAR(255) NOT NULL DEFAULT '',
  c7 CHAR(255) NOT NULL DEFAULT '',
  c8 CHAR(255) NOT NULL DEFAULT '',
  PRIMARY KEY(a,b), INDEX(b)
) ENGINE=InnoDB;

BEGIN;
INSERT INTO t(a,b) VALUES(1,1),(1,2),(1,3),(1,4),(1,5);
INSERT INTO t(a,b) SELECT a+1,b FROM t;
INSERT INTO t(a,b) SELECT a+2,b FROM t;
INSERT INTO t(a,b) SELECT a+4,b FROM t;
COMMIT;
{code}

Our example table consists of multiple B-tree pages in the clustered index. There also is a secondary index which contains entries (b,a). The secondary index pages and the non-leaf clustered index pages (other than the clustered index root page) will be unaffected by instant ADD COLUMN.

Let us demonstrate a few instant ADD COLUMN operations:

{code:SQL}
ALTER TABLE t ADD COLUMN d CHAR(10) NOT NULL DEFAULT '';
# Above, only the leftmost leaf page was converted to include column d.
UPDATE t SET c8='eight' WHERE a=8;
# The UPDATE may convert the rightmost leaf page(s) to include column d.
# Column d was not updated, so the page conversion is optional.
ALTER TABLE t ADD COLUMN e INT NULL;
# Again, the ALTER only converted the leftmost leaf page.
UPDATE t SET e=a*b WHERE a=7;
# The UPDATE will convert the affected leaf page records to include columns d,e.
{code}

Finally, let us demonstrate a few limitations:

{code:SQL}
# Indexes can be added to instantly added columns, but not instantly.
ALTER TABLE t ADD INDEX(e);
{code}

Changing the DEFAULT value of a column was an instant operation until now. However, changing the DEFAULT of an instantly added column will rebuild the table, no matter if the column is declared as NULL or NOT NULL. Likewise, DROP COLUMN of non-virtual columns will rebuild the table:
{code:SQL}
ALTER TABLE t CHANGE d d CHAR(10) NOT NULL DEFAULT 'unknown';
ALTER TABLE t CHANGE e e INT NULL DEFAULT 10;
# Dropping any non-virtual columns will rebuild the table.
ALTER TABLE t DROP COLUMN c1, DROP COLUMN d, DROP COLUMN e;
DROP TABLE t;
{code}
Maybe we should introduce ALGORITHM=NOCOPY so that time-consuming table-rebuilding operations can be refused.",7,"h1. Page-level format tracking instead of row-level

Tencent has implemented instant ADD COLUMN by introducing new ROW_FORMAT variants that explicitly store the number of fields in index records. With the original InnoDB format (ROW_FORMAT=REDUNDANT) we could have something like that as is, because that format stores the number of fields.

However, there is an alternative that is compatible with old data files that have been created in any ROW_FORMAT. Instead of tracking the number of columns for each record, we would track it for each page.

h2. Implementation of the instant ADD COLUMN operation

As suggested in MDEV-11424 for more general instant ALTER, we can repurpose the unused field PAGE_MAX_TRX_ID in clustered index leaf pages. All InnoDB versions at least since MySQL 3.23.49 wrote this field as 0. We would only support ADD COLUMN with the following restrictions:
# Only ADD COLUMN…LAST. Any DROP COLUMN or change of column order will require a full table rebuild.
# Only constant DEFAULT values.
# Changing the DEFAULT of instantly-added columns will require a full table rebuild.

A new field PAGE_CLUST_LEAF_FIELDS will be aliased to PAGE_MAX_TRX_ID and defined as follows:
# On non-leaf pages, there is no PAGE_CLUST_LEAF_FIELDS. (In clustered index non-leaf pages other than the root, the PAGE_MAX_TRX_ID will remain unused (0). For the clustered index root page, MDEV-6076 introduced PAGE_ROOT_AUTO_INC.)
# If the clustered index consists of a single page (root and leaf), PAGE_CLUST_LEAF_FIELDS does not exist, and the records must be in the newest format. (MDEV-6076 repurposed the PAGE_MAX_TRX_ID of the clustered index root page as PAGE_ROOT_AUTO_INC.)
# If the PAGE_CLUST_LEAF_FIELDS of the leftmost leaf page is 0, all PAGE_CLUST_LEAF_FIELDS must be 0. (These are tables where instant ALTER TABLE (MDEV-11369 or MDEV-11424) has not been used, or the table has been rebuilt since then.)
# In the leftmost leaf page, PAGE_CLUST_LEAF_FIELDS contains the original number of fields in the clustered index, before any instant ADD COLUMN operations. This field will be cached in dict_table_t::clust_leaf_fields. The records in the leftmost leaf page must always have the latest number of columns.
# In other leaf pages of the clustered index, if PAGE_CLUST_LEAF_FIELDS is 0, the number of fields in the records will be dict_table_t::clust_leaf_fields. Else, PAGE_CLUST_LEAF_FIELDS is the number of fields in the records, and it must be between index->table->clust_leaf_fields+1 and index->n_fields.
# Only the PAGE_CLUST_LEAF_FIELDS values 1 to 1023 will be reserved for this purpose. (InnoDB does not support indexes with more fields.) The values 1024 to (1<<64)-1 could be used by MDEV-11424 later.

From the above it follows that instant ADD COLUMN must convert the leftmost leaf page or the root-leaf page to the newest format. Unless the conversion results into a page split, only a single data page will be modified.

The data dictionary (both the .frm file and the InnoDB SYS_ tables) will contain the latest table definition. The number of clustered index fields before the first instant ADD COLUMN is only stored in dict_table_t::clust_leaf_fields the PAGE_CLUST_LEAF_FIELDS field of the leftmost leaf page. If multiple ALTER TABLE…ADD COLUMN statements are executed on the same table, the dict_table_t::clust_leaf_fields will not change.

When determining whether changing the DEFAULT value of some columns requires a table rebuild, InnoDB must determine if the clustered index position of any of the affected columns is dict_table_t::clust_leaf_fields or greater. If table->clust_leaf_fields == 0, instant ADD COLUMN has not been used, and any DEFAULT values can be changed instantly.

h2. Data format changes

The format of secondary index pages will be unaffected by instant ALTER TABLE.

The clustered index leaf page format is affected by the instant ADD COLUMN, by the introduction of the PAGE_CLUST_LEAF_FIELDS (repurposed from PAGE_MAX_TRX_ID).

Non-leaf, non-root clustered index pages will be unaffected.

The clustered index root page format will have to be slightly changed to avoid an unintended change to the node pointer records when ROW_FORMAT is COMPACT, DYNAMIC or COMPRESSED. There is no problem with ROW_FORMAT=REDUNDANT.

In clustered index node pointer records, all fields will be NOT NULL, because the PRIMARY KEY columns cannot be NULL. Nevertheless, the COMPACT, DYNAMIC and COMPRESSED clustered index node pointer record header will unnecessarily allocate 0‥128 zero bytes to represent NULL flags in the node pointers. The 0‥128 corresponds to UT_BITS_IN_BYTES(index->n_nullable). index->n_nullable can be 0‥1017 in the clustered index.

So, we must find one byte in the clustered index root page to store the number of the redundant zero bytes in the node pointer record header. Luckily, only 3 bits of the 16-bit field PAGE_DIRECTION are used.

We will repurpose the most significant byte of PAGE_DIRECTION in all COMPACT, DYNAMIC, REDUNDANT index root pages as follows: If the field is nonzero, it will be set to 1+UT_BITS_IN_BYTES(index->n_nullable), to be used when interpreting the node pointer records. (The byte was always set to 0 until now.) The value will also be cached in a new field dict_index_t::node_ptr_null_bytes (always 0 if ROW_FORMAT=REDUNDANT). This will not only solve the problem at hand, but it will also allow us to avoid wasting space on node pointer pages in the future.

h2. Changes to read operations

When reading records from a page, we must use the current number of fields in the page. This will require some modifications to rec_get_offsets() or its callers, especially because UT_BITS_IN_BYTES(index->n_nullable) may change. When returning a record to the SQL layer, InnoDB has to fill in the DEFAULT values of any instantly-added columns that were missing from the leaf page.

If an index is created on an instantly added column, InnoDB may need to access the DEFAULT values of instantly added columns. For this purpose, we probably should move row_prebuilt_t::default_rec to dict_table_t::default_rec.

h2. Changes to write operations

Because we maintain the number of fields at the page level, all records within the clustered index leaf page must have the same number of fields.

This means that whenever any record within the page is inserted, or a not-yet-instantiated instantly added column is updated, all records on the page must be converted. This could require the leaf page to be split into multiple pages.

Page splits and merges will convert all records in all affected leaf pages to the newest format.

Page reorganize does not need to convert records, and maybe it should, because the conversion could cause a page overflow. This could mean that page reorganize should memcpy() the individual records instead of invoking page_copy_rec_list_end_no_locks() which would by default use the latest index definition.

It is optional to convert pages when records are purged (deleted) or the delete-mark bit is changed (for example, BEGIN; DELETE FROM instantly_altered_table; ROLLBACK).

h2. Example

{code:SQL}
CREATE TABLE t(
  a INT NOT NULL, b INT NOT NULL,
  c1 CHAR(255) NOT NULL DEFAULT '',
  c2 CHAR(255) NOT NULL DEFAULT '',
  c3 CHAR(255) NOT NULL DEFAULT '',
  c4 CHAR(255) NOT NULL DEFAULT '',
  c5 CHAR(255) NOT NULL DEFAULT '',
  c6 CHAR(255) NOT NULL DEFAULT '',
  c7 CHAR(255) NOT NULL DEFAULT '',
  c8 CHAR(255) NOT NULL DEFAULT '',
  PRIMARY KEY(a,b), INDEX(b)
) ENGINE=InnoDB;

BEGIN;
INSERT INTO t(a,b) VALUES(1,1),(1,2),(1,3),(1,4),(1,5);
INSERT INTO t(a,b) SELECT a+1,b FROM t;
INSERT INTO t(a,b) SELECT a+2,b FROM t;
INSERT INTO t(a,b) SELECT a+4,b FROM t;
COMMIT;
{code}

Our example table consists of multiple B-tree pages in the clustered index. There also is a secondary index which contains entries (b,a). The secondary index pages and the non-leaf clustered index pages (other than the clustered index root page) will be unaffected by instant ADD COLUMN.

Let us demonstrate a few instant ADD COLUMN operations:

{code:SQL}
ALTER TABLE t ADD COLUMN d CHAR(10) NOT NULL DEFAULT '';
# Above, only the leftmost leaf page was converted to include column d.
UPDATE t SET c8='eight' WHERE a=8;
# The UPDATE may convert the rightmost leaf page(s) to include column d.
# Column d was not updated, so the page conversion is optional.
ALTER TABLE t ADD COLUMN e INT NULL;
# Again, the ALTER only converted the leftmost leaf page.
UPDATE t SET e=a*b WHERE a=7;
# The UPDATE will convert the affected leaf page records to include columns d,e.
{code}

Finally, let us demonstrate a few limitations:

{code:SQL}
# Indexes can be added to instantly added columns, but not instantly.
ALTER TABLE t ADD INDEX(e);
{code}

Changing the DEFAULT value of a column was an instant operation until now. However, changing the DEFAULT of an instantly added column will rebuild the table, no matter if the column is declared as NULL or NOT NULL. Likewise, DROP COLUMN of non-virtual columns will rebuild the table:
{code:SQL}
ALTER TABLE t CHANGE d d CHAR(10) NOT NULL DEFAULT 'unknown';
ALTER TABLE t CHANGE e e INT NULL DEFAULT 10;
# Dropping any non-virtual columns will rebuild the table.
ALTER TABLE t DROP COLUMN c1, DROP COLUMN d, DROP COLUMN e;
DROP TABLE t;
{code}
Maybe we should introduce ALGORITHM=NOCOPY so that time-consuming table-rebuilding operations can be refused."
1774,MDEV-11369,MDEV,vinchen,90347,2017-01-06 08:04:21,"Hi Marko,

---
>> In clustered index node pointer records, all fields will be NOT NULL, because the PRIMARY KEY columns cannot be NULL. Nevertheless, the COMPACT, DYNAMIC and COMPRESSED clustered index node pointer record header will unnecessarily allocate 0‥128 zero bytes to represent NULL flags in the node pointers. The 0‥128 corresponds to UT_BITS_IN_BYTES(index->n_nullable). index->n_nullable can be 0‥1017 in the clustered index.
>> So, we must find one byte in the clustered index root page to store the number of the redundant zero bytes in the node pointer record header. Luckily, only 3 bits of the 16-bit field PAGE_DIRECTION are used.


I think we don't need to change the ""clustered index node pointer page""
In our Tencent's instant add column implementation, we introduce dict_index_t::n_nullable_core, just mean that nullable column number of ""the first dict_table_t::clust_leaf_fields in the cluster index"". 
And it can calculate easily by dict_table_t::clust_leaf_fields and dict_index_t::n_fields when loading dictionary of one table.
When the dict_table_t::clust_leaf_fields is zero, dict_index_t::n_nullable_core is always equal to dict_index_t::n_nullable.
When the dict_table_t::clust_leaf_fields is nonzero, dict_index_t::n_nullable_core is always less than or equal to dict_index_t::n_nullable.
For clustered index node pointer page, we should always use UT_BITS_IN_BYTES(index->n_nullable_core) to represent NULL flags in the node pointers(instead of UT_BITS_IN_BYTES(index->n_nullable)). 
When the dict_table_t::clust_leaf_fields is nonzero and PAGE_CLUST_LEAF_FIELDS is zero, rec_get_offsets() of cluster index leaf page should use UT_BITS_IN_BYTES(index->n_nullable_core) also.
Whenever instant add column happened, index->n_nullable_core would be not change.

---


In other hand, we think that the leftmost page brings some complexity of page spliting, page merging or page rasing, and so on.
It maybe brings logic procesing of the leftmost page in lots of code. (Maybe I am wrong)
I think we should reuse the page FSP_DICT_HDR_PAGE_NO(when space_id != 0) to store the addition dictonary information for MDEV-11369 or MDEV-11424 or other.
And for MDEV-11369, we should only storage the dict_table_t::clust_leaf_fields in the page.
When loading one table(space_id > 0), we can get dict_table_t::clust_leaf_fields from page FSP_DICT_HDR_PAGE_NO, instead of the leafmost page.
Therefore, the leftmost page is the same as other cluster index page.
And there are more space to storage the addition dictionary information for other feature.

But, when space_id = 0 in a table, the instant add column shoule be refused.
However, the ""innodb_file_per_table"" is on defalut when mysql version >= 5.6.6.
And in most of production environment, innodb_file_per_table is always on.

I think that this constraint can be tolerated, and more common for the new feature (like MDEV-11424), and simplifies the implementation also.
How do you think? ",8,"Hi Marko,

---
>> In clustered index node pointer records, all fields will be NOT NULL, because the PRIMARY KEY columns cannot be NULL. Nevertheless, the COMPACT, DYNAMIC and COMPRESSED clustered index node pointer record header will unnecessarily allocate 0‥128 zero bytes to represent NULL flags in the node pointers. The 0‥128 corresponds to UT_BITS_IN_BYTES(index->n_nullable). index->n_nullable can be 0‥1017 in the clustered index.
>> So, we must find one byte in the clustered index root page to store the number of the redundant zero bytes in the node pointer record header. Luckily, only 3 bits of the 16-bit field PAGE_DIRECTION are used.


I think we don't need to change the ""clustered index node pointer page""
In our Tencent's instant add column implementation, we introduce dict_index_t::n_nullable_core, just mean that nullable column number of ""the first dict_table_t::clust_leaf_fields in the cluster index"". 
And it can calculate easily by dict_table_t::clust_leaf_fields and dict_index_t::n_fields when loading dictionary of one table.
When the dict_table_t::clust_leaf_fields is zero, dict_index_t::n_nullable_core is always equal to dict_index_t::n_nullable.
When the dict_table_t::clust_leaf_fields is nonzero, dict_index_t::n_nullable_core is always less than or equal to dict_index_t::n_nullable.
For clustered index node pointer page, we should always use UT_BITS_IN_BYTES(index->n_nullable_core) to represent NULL flags in the node pointers(instead of UT_BITS_IN_BYTES(index->n_nullable)). 
When the dict_table_t::clust_leaf_fields is nonzero and PAGE_CLUST_LEAF_FIELDS is zero, rec_get_offsets() of cluster index leaf page should use UT_BITS_IN_BYTES(index->n_nullable_core) also.
Whenever instant add column happened, index->n_nullable_core would be not change.

---


In other hand, we think that the leftmost page brings some complexity of page spliting, page merging or page rasing, and so on.
It maybe brings logic procesing of the leftmost page in lots of code. (Maybe I am wrong)
I think we should reuse the page FSP_DICT_HDR_PAGE_NO(when space_id != 0) to store the addition dictonary information for MDEV-11369 or MDEV-11424 or other.
And for MDEV-11369, we should only storage the dict_table_t::clust_leaf_fields in the page.
When loading one table(space_id > 0), we can get dict_table_t::clust_leaf_fields from page FSP_DICT_HDR_PAGE_NO, instead of the leafmost page.
Therefore, the leftmost page is the same as other cluster index page.
And there are more space to storage the addition dictionary information for other feature.

But, when space_id = 0 in a table, the instant add column shoule be refused.
However, the ""innodb_file_per_table"" is on defalut when mysql version >= 5.6.6.
And in most of production environment, innodb_file_per_table is always on.

I think that this constraint can be tolerated, and more common for the new feature (like MDEV-11424), and simplifies the implementation also.
How do you think? "
1775,MDEV-11369,MDEV,Marko Mäkelä,90497,2017-01-11 07:28:55,"Hi Vinchen,
Thank you for your feedback.
I have a long-term goal of making the InnoDB data files self-descriptive, so that eventually we can remove the InnoDB data dictionary tables (MDEV-11655) and even the system tablespace (MDEV-11633).

Because of this goal, I think that all data regarding the past history of ‘instant ADD COLUMN’ operations must be available in the data file itself.

The data dictionary (both the .frm file and the SYS_* table records) would only keep the newest table definition. We would be unable to retrieve the original number of columns or fields from the data dictionary. That data should be stored in the index tree only. Any data that is not reported by SHOW CREATE TABLE and is only written to the SYS_* table records is working against the long-term goal set in MDEV-11655.

So, the dict_table_t::clust_leaf_fields or n_nullable_core would have to be stored in some way in the clustered index itself.
In my plan, the original number of fields (before any instant ADD COLUMN) would be stored in the leftmost leaf page of the clustered index. (You challenged that suggestion; let me address that separately.)

h2. The need for storing the original n_null_bytes

How can we get to a leaf page from the clustered index root? We are unfortunately suffering from a two bad design decisions here. (1) Instead of storing the page number of the leftmost leaf at a fixed location, InnoDB allocates a large number of useless bytes (which are all ignored when the REC_MIN_REC_FLAG is set, and that flag is only set for the leftmost node pointer record at each level). (2) In ROW_FORMAT=COMPACT and later, we are unnecessarily allocating null flags for clustered index node pointer records.

An instant ADD COLUMN operation cannot possibly rewrite all node pointer pages, because the operation would not be instant any more. (In my proposal above, it would write the root page and the leftmost leaf page.) So, when reading or writing clustered index node pointer records we must act as if no ADD COLUMN had been executed.

Let us consider an example:
{code:SQL}
CREATE TABLE t (
 pk VARCHAR(255) PRIMARY KEY,
 c0 INT, c1 INT, c2 INT, c3 INT, c4 INT, c5 INT, c6 INT, c7 INT
) ROW_FORMAT=DYNAMIC;
INSERT INTO t(pk) VALUES(''),('1'),('2'),…,('1000');
ALTER TABLE t ADD COLUMN c8 INT;
UPDATE t SET c8=1;
ALTER TABLE t ADD COLUMN c9 INT;
{code}

In this example, the format of the node pointer records would be (pk,child_page_number). In ROW_FORMAT!=REDUNDANT, the actual length of the field pk is stored in the record header, before the null flags. (Yes, storing the null flags for the clustered index node pointer is a complete waste; no PRIMARY KEY column can be NULL.) In this case, originally there was exactly 1 byte allocated for the null flags for the 8 columns c1…c8, at byte offset -5. If there had been 9 nullable columns, there would be 2 bytes of NULL flags, at byte offset -5 and -6.
Because we cannot afford to modify all node pointer pages in instant ADD COLUMN, we must interpret the node pointer pages in an appropriate way after the instant ADD COLUMN. That is, we must know how many bytes of null flags were written before any instant ADD COLUMN operation, and we must keep using the same number of bytes.

By the time the first ALTER is executed, there should be multiple leaf pages. Let us say that we are interpreting a node pointer ('500',child_page_number). If this was the leftmost node pointer (REC_INFO_MIN_REC_FLAG is set), all comparisons would ignore the actual key because of the flag, but we still must know the length of the PK fields so that we can read the child_page_number. Otherwise, we must know the length of the PK fields already for the purpose of comparing to our search key.

In ROW_FORMAT!=REDUNDANT, the lengths of variable-length fields are stored in the record header before the null flags. We must use the correct number of null flags, or otherwise we could read the length of the PK fields from the wrong place. For example, if we read the length from a null_flags byte which would be 0, we would incorrectly treat the length as 0, and then we would interpret the PK field contents as the child_page_number.

I think that this example demonstrates that we do need some information in the root page that allows us to avoid rewriting all node pointer pages in instant ADD COLUMN operations. The simplest fix that I was able to come up with was to repurpose the high-order 8 bits of the PAGE_DIRECTION field.

h2. Where to store the original number of columns?

My proposal makes the leftmost leaf page special, because it would store the original number of clustered index fields, that is, the number of fields before any instant ADD COLUMN operations, in the repurposed PAGE_MAX_TRX_ID field.

Vinchen suggests that this could complicate code that deals with splitting or merging pages or adding or removing tree levels. I think that we would only know the answer after prototyping it. We could introduce a simple predicate to make code readable:
{code:C}
#define page_is_left_leaf(page) \
page_is_leaf(page) && *static_cast<uint32_t*>(page + FIL_PAGE_PREV) == FIL_NULL
{code}

If we indeed store the dict_table_t::clust_leaf_fields somewhere else than the leftmost leaf page, all leaf pages would be treated equal, and instant ADD COLUMN would never have to convert any leaf page, except when there is a single leaf page (which is also a root page). On any leaf page, the PAGE_CLUST_LEAF_FIELDS would either be the number of fields stored in the page, or if it is 0, it would refer to dict_table_t::clust_leaf_fields. We could also store the number of null flags in this page, instead of repurposing PAGE_DIRECTION.

Vinchen’s proposal for off-index storage is FSP_DICT_HDR_PAGE_NO. Currently, this is the DICT_HDR page that currently only exists in the system tablespace (page number 7 in tablespace 0). We cannot use a fixed page number, because this scheme should be compatible with old data files. But we could allocate a new page and store its number in one of the first pages of the tablespace. Oracle did something similar in MySQL 8.0.0 to reserve space for ‘serialized dictionary information’ in [WL#7053|http://dev.mysql.com/worklog/task/?id=7053]. Like Vinchen pointed out, it would be challenging to make this work with the system tablespace (or shared tablespaces in general).

I agree that it is a good long-term goal to store low-level dictionary information in the data file itself. Something like that is definitely needed for more generic instant ALTER operations (MDEV-11424) and also for the transactional data dictionary (MDEV-11655; at least we should store the secondary index root page numbers).

However, I would prefer not to introduce a new subformat at this point of time, because we would have to support the format in all future versions. Once we get around to designing the details of MDEV-11424 or MDEV-11655, we should have a better idea of what the format should look like. If it turns out that the page splits and merges indeed get very messy, we may of course revisit this decision.",9,"Hi Vinchen,
Thank you for your feedback.
I have a long-term goal of making the InnoDB data files self-descriptive, so that eventually we can remove the InnoDB data dictionary tables (MDEV-11655) and even the system tablespace (MDEV-11633).

Because of this goal, I think that all data regarding the past history of ‘instant ADD COLUMN’ operations must be available in the data file itself.

The data dictionary (both the .frm file and the SYS_* table records) would only keep the newest table definition. We would be unable to retrieve the original number of columns or fields from the data dictionary. That data should be stored in the index tree only. Any data that is not reported by SHOW CREATE TABLE and is only written to the SYS_* table records is working against the long-term goal set in MDEV-11655.

So, the dict_table_t::clust_leaf_fields or n_nullable_core would have to be stored in some way in the clustered index itself.
In my plan, the original number of fields (before any instant ADD COLUMN) would be stored in the leftmost leaf page of the clustered index. (You challenged that suggestion; let me address that separately.)

h2. The need for storing the original n_null_bytes

How can we get to a leaf page from the clustered index root? We are unfortunately suffering from a two bad design decisions here. (1) Instead of storing the page number of the leftmost leaf at a fixed location, InnoDB allocates a large number of useless bytes (which are all ignored when the REC_MIN_REC_FLAG is set, and that flag is only set for the leftmost node pointer record at each level). (2) In ROW_FORMAT=COMPACT and later, we are unnecessarily allocating null flags for clustered index node pointer records.

An instant ADD COLUMN operation cannot possibly rewrite all node pointer pages, because the operation would not be instant any more. (In my proposal above, it would write the root page and the leftmost leaf page.) So, when reading or writing clustered index node pointer records we must act as if no ADD COLUMN had been executed.

Let us consider an example:
{code:SQL}
CREATE TABLE t (
 pk VARCHAR(255) PRIMARY KEY,
 c0 INT, c1 INT, c2 INT, c3 INT, c4 INT, c5 INT, c6 INT, c7 INT
) ROW_FORMAT=DYNAMIC;
INSERT INTO t(pk) VALUES(''),('1'),('2'),…,('1000');
ALTER TABLE t ADD COLUMN c8 INT;
UPDATE t SET c8=1;
ALTER TABLE t ADD COLUMN c9 INT;
{code}

In this example, the format of the node pointer records would be (pk,child_page_number). In ROW_FORMAT!=REDUNDANT, the actual length of the field pk is stored in the record header, before the null flags. (Yes, storing the null flags for the clustered index node pointer is a complete waste; no PRIMARY KEY column can be NULL.) In this case, originally there was exactly 1 byte allocated for the null flags for the 8 columns c1…c8, at byte offset -5. If there had been 9 nullable columns, there would be 2 bytes of NULL flags, at byte offset -5 and -6.
Because we cannot afford to modify all node pointer pages in instant ADD COLUMN, we must interpret the node pointer pages in an appropriate way after the instant ADD COLUMN. That is, we must know how many bytes of null flags were written before any instant ADD COLUMN operation, and we must keep using the same number of bytes.

By the time the first ALTER is executed, there should be multiple leaf pages. Let us say that we are interpreting a node pointer ('500',child_page_number). If this was the leftmost node pointer (REC_INFO_MIN_REC_FLAG is set), all comparisons would ignore the actual key because of the flag, but we still must know the length of the PK fields so that we can read the child_page_number. Otherwise, we must know the length of the PK fields already for the purpose of comparing to our search key.

In ROW_FORMAT!=REDUNDANT, the lengths of variable-length fields are stored in the record header before the null flags. We must use the correct number of null flags, or otherwise we could read the length of the PK fields from the wrong place. For example, if we read the length from a null_flags byte which would be 0, we would incorrectly treat the length as 0, and then we would interpret the PK field contents as the child_page_number.

I think that this example demonstrates that we do need some information in the root page that allows us to avoid rewriting all node pointer pages in instant ADD COLUMN operations. The simplest fix that I was able to come up with was to repurpose the high-order 8 bits of the PAGE_DIRECTION field.

h2. Where to store the original number of columns?

My proposal makes the leftmost leaf page special, because it would store the original number of clustered index fields, that is, the number of fields before any instant ADD COLUMN operations, in the repurposed PAGE_MAX_TRX_ID field.

Vinchen suggests that this could complicate code that deals with splitting or merging pages or adding or removing tree levels. I think that we would only know the answer after prototyping it. We could introduce a simple predicate to make code readable:
{code:C}
#define page_is_left_leaf(page) \
page_is_leaf(page) && *static_cast(page + FIL_PAGE_PREV) == FIL_NULL
{code}

If we indeed store the dict_table_t::clust_leaf_fields somewhere else than the leftmost leaf page, all leaf pages would be treated equal, and instant ADD COLUMN would never have to convert any leaf page, except when there is a single leaf page (which is also a root page). On any leaf page, the PAGE_CLUST_LEAF_FIELDS would either be the number of fields stored in the page, or if it is 0, it would refer to dict_table_t::clust_leaf_fields. We could also store the number of null flags in this page, instead of repurposing PAGE_DIRECTION.

Vinchen’s proposal for off-index storage is FSP_DICT_HDR_PAGE_NO. Currently, this is the DICT_HDR page that currently only exists in the system tablespace (page number 7 in tablespace 0). We cannot use a fixed page number, because this scheme should be compatible with old data files. But we could allocate a new page and store its number in one of the first pages of the tablespace. Oracle did something similar in MySQL 8.0.0 to reserve space for ‘serialized dictionary information’ in [WL#7053|URL Like Vinchen pointed out, it would be challenging to make this work with the system tablespace (or shared tablespaces in general).

I agree that it is a good long-term goal to store low-level dictionary information in the data file itself. Something like that is definitely needed for more generic instant ALTER operations (MDEV-11424) and also for the transactional data dictionary (MDEV-11655; at least we should store the secondary index root page numbers).

However, I would prefer not to introduce a new subformat at this point of time, because we would have to support the format in all future versions. Once we get around to designing the details of MDEV-11424 or MDEV-11655, we should have a better idea of what the format should look like. If it turns out that the page splits and merges indeed get very messy, we may of course revisit this decision."
1776,MDEV-11369,MDEV,Marko Mäkelä,93936,2017-04-08 17:37:40,"As noted in the [MDEV-12123 fix|https://github.com/MariaDB/server/commit/fbaa04709f2ab577437e920cef6fce7c72ad5eda], after IMPORT TABLESPACE all the clustered index pages may contain a (bogus) transaction ID in PAGE_MAX_TRX_ID. We can detect this to some extent. While seeking to the leftmost leaf page, we can check if the PAGE_MAX_TRX_ID is nonzero on any of the non-leaf pages, or if the ID on the root page matches the one on the leaf page.
There would appear to be a problem if the clustered index tree height is only 1. In this case, there are no other node pointer pages than the root page, where PAGE_MAX_TRX_ID was repurposed as PAGE_ROOT_AUTO_INC. Perhaps in this case, we can check all leaf pages if they are carrying the same PAGE_MAX_TRX_ID value, and then decide that this is an old imported table, not one where instant ALTER TABLE…ADD COLUMN was used.",10,"As noted in the [MDEV-12123 fix|URL after IMPORT TABLESPACE all the clustered index pages may contain a (bogus) transaction ID in PAGE_MAX_TRX_ID. We can detect this to some extent. While seeking to the leftmost leaf page, we can check if the PAGE_MAX_TRX_ID is nonzero on any of the non-leaf pages, or if the ID on the root page matches the one on the leaf page.
There would appear to be a problem if the clustered index tree height is only 1. In this case, there are no other node pointer pages than the root page, where PAGE_MAX_TRX_ID was repurposed as PAGE_ROOT_AUTO_INC. Perhaps in this case, we can check all leaf pages if they are carrying the same PAGE_MAX_TRX_ID value, and then decide that this is an old imported table, not one where instant ALTER TABLE…ADD COLUMN was used."
1777,MDEV-11369,MDEV,Marko Mäkelä,93949,2017-04-10 01:39:38,"Hi [~vinchen], it was a pleasure for me to see your live presentation of a revised idea that uses per-record tagging while maintaining backward compatibility with old data files.

The following detailed design proposal is based on what we agreed upon today. Some consistency checks and tests could be omitted from the first prototype. Also support for ROW_FORMAT=REDUNDANT can be omitted from the first prototype. I would like to have good test coverage for this. I can try to help with tests if needed.

h2. ALTER TABLE syntax changes
It would be nice to introduce new syntax to prevent nasty surprises. When an operation is expected to be quick, it could be better to return an error than to perform a disruptive (resource-intensive and time-consuming) operation.

ALGORITHM=INSTANT will refuse any operation that must modify any data files.

ALGORITHM=NOCOPY will refuse any operation that would rebuild the clustered index (and the whole table).

With respect to the allowed operations, ALGORITHM=INSTANT is a subset of ALGORITHM=NOCOPY which is a subset of ALGORITHM=INPLACE.

Example:
{code:SQL}
# No data file change (instant operation)
ALTER TABLE t ADD COLUMN b INT, ALGORITHM=INSTANT;
# The following will change data files (ADD INDEX), but not rebuild the table:
ALTER TABLE t ADD COLUMN c INT, ADD INDEX(c), ALGORITHM=NOCOPY;
# The following are changing data files (not instant operation)
--error ER_ALTER_OPERATION_NOT_SUPPORTED_REASON
ALTER TABLE t DROP INDEX c, ALGORITHM=INSTANT;
--error ER_ALTER_OPERATION_NOT_SUPPORTED_REASON
ALTER TABLE t ADD COLUMN d INT, ADD INDEX(d), ALGORITHM=INSTANT;
# All of the above will be allowed with any other ALGORITHM.
# With ALGORITHM=DEFAULT or with no ALGORITHM, the most efficient
# available algorithm will be used.
{code}

No new ROW_FORMAT keywords will be introduced. Instead, ROW_FORMAT=REDUNDANT, ROW_FORMAT=COMPACT and ROW_FORMAT=DYNAMIC will be extended at the file level.

h2. Limitations

h3. Limitations on ALTER TABLE
Instant ADD COLUMN will not be supported on ROW_FORMAT=COMPRESSED tables. (ALTER TABLE…ADD COLUMN…ALGORITHM=NOCOPY must return an error; the operation can be executed with ALGORITHM=INPLACE or ALGORITHM=COPY just like before.)

Instant ADD COLUMN will not allow non-constant DEFAULT values, not as part of the ADD COLUMN operation nor later on columns that were created by instant ADD COLUMN. If such DEFAULT value is used, then ALGORITHM=INSTANT or ALGORITHM=NOCOPY will return an error, but ALGORITHM=INPLACE or ALGORITHM=COPY will rebuild the table.

Instant ADD COLUMN is only allowed when the column is added last. Adding columns between or before existing columns will be refused by ALGORITHM=INSTANT or ALGORITHM=NOCOPY, but will be executed by ALGORITHM=INPLACE or ALGORITHM=COPY just like before.

h3. Limitations on IMPORT and EXPORT

ALTER TABLE…IMPORT TABLESPACE is refused after instant ADD COLUMN was used on the table. (You must do TRUNCATE TABLE and ALTER TABLE…FORCE, or just DROP TABLE and CREATE TABLE before ALTER TABLE…DISCARD TABLESPACE and ALTER TABLE…IMPORT TABLESPACE.)

ALTER TABLE…IMPORT TABLESPACE is refused if the clustered index root page of the to-be-imported data file contains a flag indicating that instant ADD COLUMN was executed.

FLUSH TABLES FOR EXPORT must return an error or at least a warning if instant ADD COLUMN has been used. If you want to export tables, use ALTER TABLE…FORCE first to convert it to compatible format. (This is only to prevent a surprise when the copied .ibd file would cause an error later on IMPORT TABLESPACE.)

h2. Data dictionary changes
For now, we will not care about the future goal of removing the InnoDB system tables. If/when a future version of MariaDB implements that, it will perform a conversion step. This conversion step could repurpose fields in index pages in some way that we do not need to account for now. We will not need to repurpose PAGE_MAX_TRX_ID in any way now.

Just like now, *.frm files will store the latest table definition, including the latest DEFAULT values and the latest number of columns.

Just like now, SYS_TABLES and SYS_COLUMNS will store all column definitions and the clustered index field definitions.

(SYS_FIELDS entries for the SYS_INDEXES entry of the clustered index only reflect the PRIMARY KEY columns, so instant ADD COLUMN would not modify SYS_FIELDS.)

A new InnoDB data dictionary table is added for identifying instantly added columns and for storing the initial DEFAULT values of these columns. This table would be created using the InnoDB SQL interpreter at startup, just like SYS_FOREIGN and other tables. Note that this will require the addition of a BIGINT literal to the InnoDB SQL parser, because INT is only 32 bits, and SYS_TABLES.TABLE_ID is 64 bits:
{code:SQL}
PROCEDURE CREATE_SYS_COLUMNS_ADDED_PROC () IS
BEGIN
CREATE TABLE SYS_COLUMNS_ADDED(
   TABLE_ID BIGINT UNSIGNED NOT NULL,
   POS INT NOT NULL,
   DEFAULT_VALUE CHAR);
CREATE UNIQUE CLUSTERED INDEX COL_IND ON SYS_FOREIGN (TABLE_ID, POS);
END;
{code}
The view INFORMATION_SCHEMA.INNODB_SYS_COLUMNS_ADDED will be created, to allow an easy way of checking which tables require a conversion (ALTER TABLE…FORCE) for moving to MySQL or older versions of MariaDB. Example:
{code:SQL}
SELECT name FROM INFORMATION_SCHEMA.INNODB_SYS_TABLES
WHERE table_id IN (SELECT table_id FROM INFORMATION_SCHEMA.INNODB_SYS_COLUMNS_ADDED);
{code}

If any record with SYS_COLUMNS_ADDED.TABLE_ID matches the table definition that is being loaded from SYS_TABLES and SYS_COLUMNS, then the values of SYS_COLUMNS_ADDED.POS must be in strict sequential order (no gaps allowed) and run up to the number of columns in SYS_COLUMNS.

The value SYS_COLUMNS_ADDED.DEFAULT_VALUE = NULL is not allowed if the column is declared NOT NULL.

When SYS_COLUMNS_ADDED.DEFAULT_VALUE is not NULL, it must match the allowed length and value of the column (it must match the fixed length of the column declared in SYS_COLUMNS, or it must not exceed the maximum length of a variable-length column.

The code that loads the table definition must report that the table is corrupted if any inconsistency between SYS_COLUMNS and SYS_COLUMNS_ADDED is detected.

h3. Example of the InnoDB data dictionary changes
{code:SQL}
CREATE TABLE t1(a INT PRIMARY KEY, b INT) ENGINE=InnoDB ROW_FORMAT=REDUNDANT;
INSERT INTO t1 SET a=1;
# instantly add columns
ALTER TABLE t1 ADD COLUMN c INT, d CHAR(10) NULL DEFAULT 'foo', e INT NOT NULL DEFAULT 42;
INSERT INTO t1 SET a=2;
# change some DEFAULT values of old columns, and add one column
ALTER TABLE t1 CHANGE COLUMN b b INT DEFAULT 5,
CHANGE COLUMN c c INT DEFAULT 10,
CHANGE COLUMN d d DEFAULT NULL, ADD COLUMN f INT UNSIGNED DEFAULT 0;
INSERT INTO t1 SET a=3;
# change DEFAULTs in .frm file only (nothing inside InnoDB)
ALTER TABLE t1 CHANGE COLUMN a a INT 101, COLUMN b b INT DEFAULT 102,
CHANGE COLUMN c c INT DEFAULT 103,
CHANGE COLUMN d d DEFAULT 'eleventy',
CHANGE COLUMN e e INT UNSIGNED DEFAULT 105,
CHANGE COLUMN e e INT UNSIGNED DEFAULT 106;
SELECT * FROM t1;
# must return (1,NULL,NULL,'foo',42,0),(2,NULL,NULL,'foo',42,0),(3,5,10,NULL,0)
{code}

(For simplicity, the ALTER TABLE statements in the example are not renaming the columns while changing defaults. Such a combination must be supported.)

After the CREATE TABLE, we will have 2 SYS_COLUMNS entries for the table and no SYS_COLUMNS_ADDED entries.

After the first INSERT, the clustered index will contain the following record:
(a,DB_TRX_ID,DB_ROLL_PTR,b)=(1,…,…,NULL). The table format is still compatible with MySQL or older MariaDB versions.

After the first ALTER TABLE, we would have 5 entries in SYS_COLUMNS and 3 entries in SYS_COLUMNS_ADDED for the user table. The SYS_COLUMNS_ADDED entries would be: (table_id,2,NULL),(table_id,3,'foo'),(table_id,4,'\200\0\0*'). The columns in SYS_COLUMNS_ADDED are numbered from 0 onwards, so the first entry will have SYS_COLUMNS_ADDED.POS equal to the number of ‘core columns’. Initially, a table must always be created with at least 1 column. The last value is the binary value corresponding to the internal representation of the INT value 42 in InnoDB.

After the second INSERT, the latest clustered index definition will be
(a,DB_TRX_ID,DB_ROLL_PTR,b,c,d,e) and the index will contain the following:
(1,…,…,NULL),(2,…,…,NULL,NULL,'foo',42).

After the second ALTER TABLE, we will *not* update the existing SYS_COLUMNS_ADDED entries to reflect the changed DEFAULT values. They will keep the initial DEFAULT values that were in effect during the instant ADD COLUMN. The .frm file will contain the latest DEFAULT value. Inside InnoDB, the only changes will reflect the instant ADD COLUMN e: a SYS_COLUMNS record and the SYS_COLUMNS_ADDED record (table_id,5,'\0\0\0\0').

After the third INSERT, the latest clustered index definition will be
(a,DB_TRX_ID,DB_ROLL_PTR,b,c,d,e,f) and the index will contain the following:
(1,…,…,NULL),(2,…,…,NULL,NULL,'foo',42),(3,…,…,5,10,NULL,0).

The last ALTER does nothing inside InnoDB. It is only changing the DEFAULT values of already existing columns. Those are only stored in the .frm file. The SYS_COLUMNS_ADDED table only stores the initial DEFAULT values of instantly added columns.

The SELECT will fill in the *initial DEFAULT values* of the columns that are missing from the index entries. That is, for the first record, we will fill in all defaults that are stored in SYS_COLUMNS_ADDED: (c=NULL,d='foo',e=42,f=0). The second and third INSERT stored the current DEFAULT values, not the initial ones. So, for the second record, we will fill in (f=0).

h2. InnoDB file format changes

Instant ADD COLUMN will not be supported for ROW_FORMAT=COMPRESSED. It will be supported for all other ROW_FORMAT (REDUNDANT, COMPACT, DYNAMIC).

We will not introduce new ROW_FORMAT names, but instead refine the binary format in the InnoDB data files.

h3. Feature flag in the clustered index root page
If instant ADD COLUMN has been used, the following flag will be set in the clustered index root page:
{code:C}
/** Determine if instant ADD COLUMN was used.
This flag is only supposed to be set on the clustered index root page.
@param[in]	page	index page
@return	whether instant ADD COLUMN was used
(should be false if the page is not a clustered index root page) */
inline
bool
page_is_instant(const page_t* page)
{
         return(page[PAGE_HEADER + PAGE_N_HEAP] & 0x40);
}
{code}
It is an error (data corruption) if page_is_instant(clust_root) does not reflect the existence of SYS_COLUMNS_ADDED records for the table.
At the same time, the functions page_dir_set_n_heap() and page_dir_get_n_heap() must be revised to mask the 2 most significant bits of PAGE_N_HEAP. Previously, only one bit (indicating ROW_FORMAT!=REDUNDANT) was masked, starting with MySQL 5.0.3 which introduced ROW_FORMAT=COMPACT.

It is an error (data corruption) if page_is_instant(page) holds on any other index page than a clustered index root page. The corruption must be reported by CHECK TABLE and by btr_assert_not_corrupted().

(Why use the second-most-significant bit in PAGE_N_HEAP? The most significant bit identifies ROW_FORMAT!=REDUNDANT. The second-most-significant bit is always 0 so far, because there can never be more than 16383 records in a page. The largest possible seems to be innodb_page_size/(5+2) which is 65536/7=9362. Yes, the minimum record size would be 5+1 bytes for a secondary index leaf page record, say CREATE TABLE(a CHAR(1) PRIMARY KEY, INDEX(a)), but you cannot have more than 256 distinct values for 1-byte payload. Even with 65536/6 we get 10922, which is less than 16384.)

h3. Record format changes
Any data corruption must be detected by CHECK TABLE and by btr_index_rec_validate() and similar functions.

After instant ADD COLUMN on a ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC table, INSERT or UPDATE will set a new flag REC_INFO_ADDED_FLAG=0x80 in info_bits and explicitly store the number of ‘non-core’ fields in the record header, immediately before the REC_N_NEW_EXTRA_BYTES, using a variable-length encoding of 2 bytes, something like this:
{code:C}
	const byte* nulls = rec - REC_N_NEW_EXTRA_BYTES;
	ulint n_nullable = index->n_nullable;
	if (rec[-REC_NEW_INFO_BITS] & REC_INFO_ADDED_FLAG) {
		/* This must be ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC.
		ROW_FORMAT=COMPRESSED pages cannot even store this flag! */
		ut_ad(page_rec_is_comp(rec));
		ut_ad(!DICT_TF_GET_ZIP_SSIZE(index->table->flags));
		ulint n = *nulls--;
		if (n >= 0x80) {
			n = (n & 0x7f) << 8 | *nulls--;
		}
		n_fields = n_core_fields + n + 1;
		if (n_fields >= index->n_fields) ...; // error
		n_nullable += ...;
	}
	const byte* lens = nulls - UT_BITS_IN_BYTES(n_nullable);
{code}

It is an error (data corruption) if REC_INFO_ADDED_FLAG is set in a ROW_FORMAT=REDUNDANT table or in a node pointer record or in secondary indexes, or when instant ADD COLUMN has not been used.

On ROW_FORMAT=REDUNDANT pages, the records already explicitly contain the number of fields.

It is an error (data corruption) if a record is marked as containing more fields than the table definition implies, or if a stored field does not match the table definition.

It is an error (data corruption) if the n_fields in a ROW_FORMAT=REDUNDANT record is less than the number of index record fields derived from the number of core columns.

When REC_INFO_ADDED_FLAG is set (COMPACT or DYNAMIC), the above-mentioned variable-length encoding of n_fields is guaranteed by design to be more than the number of ‘core fields’.

It is an error (data corruption) if instant ADD COLUMN has not been used, but the n_fields in a ROW_FORMAT=REDUNDANT clustered index definition does not match the clustered index definition.

It is an error (data corruption) if the n_fields in a ROW_FORMAT=REDUNDANT node pointer record or secondary index record does not match the index definition.

The n_nullable flags in the clustered index node pointer records in ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC must reflect the number of ‘core columns’. These flags are useless garbage, and they are only reserved because of file format compatibility. (Clustered index node pointer records only contain the PRIMARY KEY columns, which are always NOT NULL, so we should have used n_nullable=0.) Because the size of n_nullable is not explicitly stored in the page and because we do not want to change the format of node pointer records or pages, this is our only choice.

The size of the n_nullable flags in the ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC clustered index leaf page records must correspond to the number of columns and the stored n_fields in the record. (More bytes may be allocated for new records after ALTER TABLE…ADD COLUMN…NULL.)

h3. Redo log format changes
Because the InnoDB redo log format (somewhat unnecessarily) is a mix of physical and logical operations, some change is needed to the way how INSERT and UPDATE operations are redo logged for ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC tables. Some ideas of simplifying the redo log format (and speeding up recovery) are listed in MDEV-12353.

It is possible that actual changes to the redo log format (such as the function mlog_open_and_write_index()) are unavoidable.

Even if the redo log format is not changed, it would seem safest to try to refuse startup with an older server version. We can do this by introducing a new redo log format identifier, similar to what was done in MDEV-11782.

While we could theoretically retain support for crash recovery from old-format redo log, it is probably best to not try that. (The GA version of MySQL 5.7 or MariaDB 10.2 will refuse crash recovery from older server versions, and MySQL 8.0.0 will refuse crash recovery from MySQL 5.7 or older versions. It is acceptable that MariaDB 10.3 would not support crash recovery from 10.2.)

h2. Changes to DML operations
If instant ADD COLUMN has not been used, the file format will not be changed. This means that any table-rebuilding ALTER operation (such as ALTER TABLE…FORCE or ALTER TABLE…ALGORITHM=COPY) will convert the table to the old format.

If instant ADD COLUMN has been used, any INSERT or UPDATE to the clustered index leaf page must store the current number of columns in the record. (For ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC we must also set REC_INFO_ADDED_FLAG to signal the presence of the n_fields in the record header.)

The REC_INFO_DELETED_FLAG of a record can be set or unset without changing other parts of the record. Usually this is done by a DELETE or an UPDATE of a PRIMARY KEY column, or the ROLLBACK of those operations.

On SELECT (also as part of UPDATE), if the clustered index leaf page record is found to contain fewer index fields than the current table definition implies, the missing fields will be filled in from SYS_COLUMNS_ADDED.DEFAULT_VALUE.",11,"Hi [~vinchen], it was a pleasure for me to see your live presentation of a revised idea that uses per-record tagging while maintaining backward compatibility with old data files.

The following detailed design proposal is based on what we agreed upon today. Some consistency checks and tests could be omitted from the first prototype. Also support for ROW_FORMAT=REDUNDANT can be omitted from the first prototype. I would like to have good test coverage for this. I can try to help with tests if needed.

h2. ALTER TABLE syntax changes
It would be nice to introduce new syntax to prevent nasty surprises. When an operation is expected to be quick, it could be better to return an error than to perform a disruptive (resource-intensive and time-consuming) operation.

ALGORITHM=INSTANT will refuse any operation that must modify any data files.

ALGORITHM=NOCOPY will refuse any operation that would rebuild the clustered index (and the whole table).

With respect to the allowed operations, ALGORITHM=INSTANT is a subset of ALGORITHM=NOCOPY which is a subset of ALGORITHM=INPLACE.

Example:
{code:SQL}
# No data file change (instant operation)
ALTER TABLE t ADD COLUMN b INT, ALGORITHM=INSTANT;
# The following will change data files (ADD INDEX), but not rebuild the table:
ALTER TABLE t ADD COLUMN c INT, ADD INDEX(c), ALGORITHM=NOCOPY;
# The following are changing data files (not instant operation)
--error ER_ALTER_OPERATION_NOT_SUPPORTED_REASON
ALTER TABLE t DROP INDEX c, ALGORITHM=INSTANT;
--error ER_ALTER_OPERATION_NOT_SUPPORTED_REASON
ALTER TABLE t ADD COLUMN d INT, ADD INDEX(d), ALGORITHM=INSTANT;
# All of the above will be allowed with any other ALGORITHM.
# With ALGORITHM=DEFAULT or with no ALGORITHM, the most efficient
# available algorithm will be used.
{code}

No new ROW_FORMAT keywords will be introduced. Instead, ROW_FORMAT=REDUNDANT, ROW_FORMAT=COMPACT and ROW_FORMAT=DYNAMIC will be extended at the file level.

h2. Limitations

h3. Limitations on ALTER TABLE
Instant ADD COLUMN will not be supported on ROW_FORMAT=COMPRESSED tables. (ALTER TABLE…ADD COLUMN…ALGORITHM=NOCOPY must return an error; the operation can be executed with ALGORITHM=INPLACE or ALGORITHM=COPY just like before.)

Instant ADD COLUMN will not allow non-constant DEFAULT values, not as part of the ADD COLUMN operation nor later on columns that were created by instant ADD COLUMN. If such DEFAULT value is used, then ALGORITHM=INSTANT or ALGORITHM=NOCOPY will return an error, but ALGORITHM=INPLACE or ALGORITHM=COPY will rebuild the table.

Instant ADD COLUMN is only allowed when the column is added last. Adding columns between or before existing columns will be refused by ALGORITHM=INSTANT or ALGORITHM=NOCOPY, but will be executed by ALGORITHM=INPLACE or ALGORITHM=COPY just like before.

h3. Limitations on IMPORT and EXPORT

ALTER TABLE…IMPORT TABLESPACE is refused after instant ADD COLUMN was used on the table. (You must do TRUNCATE TABLE and ALTER TABLE…FORCE, or just DROP TABLE and CREATE TABLE before ALTER TABLE…DISCARD TABLESPACE and ALTER TABLE…IMPORT TABLESPACE.)

ALTER TABLE…IMPORT TABLESPACE is refused if the clustered index root page of the to-be-imported data file contains a flag indicating that instant ADD COLUMN was executed.

FLUSH TABLES FOR EXPORT must return an error or at least a warning if instant ADD COLUMN has been used. If you want to export tables, use ALTER TABLE…FORCE first to convert it to compatible format. (This is only to prevent a surprise when the copied .ibd file would cause an error later on IMPORT TABLESPACE.)

h2. Data dictionary changes
For now, we will not care about the future goal of removing the InnoDB system tables. If/when a future version of MariaDB implements that, it will perform a conversion step. This conversion step could repurpose fields in index pages in some way that we do not need to account for now. We will not need to repurpose PAGE_MAX_TRX_ID in any way now.

Just like now, *.frm files will store the latest table definition, including the latest DEFAULT values and the latest number of columns.

Just like now, SYS_TABLES and SYS_COLUMNS will store all column definitions and the clustered index field definitions.

(SYS_FIELDS entries for the SYS_INDEXES entry of the clustered index only reflect the PRIMARY KEY columns, so instant ADD COLUMN would not modify SYS_FIELDS.)

A new InnoDB data dictionary table is added for identifying instantly added columns and for storing the initial DEFAULT values of these columns. This table would be created using the InnoDB SQL interpreter at startup, just like SYS_FOREIGN and other tables. Note that this will require the addition of a BIGINT literal to the InnoDB SQL parser, because INT is only 32 bits, and SYS_TABLES.TABLE_ID is 64 bits:
{code:SQL}
PROCEDURE CREATE_SYS_COLUMNS_ADDED_PROC () IS
BEGIN
CREATE TABLE SYS_COLUMNS_ADDED(
   TABLE_ID BIGINT UNSIGNED NOT NULL,
   POS INT NOT NULL,
   DEFAULT_VALUE CHAR);
CREATE UNIQUE CLUSTERED INDEX COL_IND ON SYS_FOREIGN (TABLE_ID, POS);
END;
{code}
The view INFORMATION_SCHEMA.INNODB_SYS_COLUMNS_ADDED will be created, to allow an easy way of checking which tables require a conversion (ALTER TABLE…FORCE) for moving to MySQL or older versions of MariaDB. Example:
{code:SQL}
SELECT name FROM INFORMATION_SCHEMA.INNODB_SYS_TABLES
WHERE table_id IN (SELECT table_id FROM INFORMATION_SCHEMA.INNODB_SYS_COLUMNS_ADDED);
{code}

If any record with SYS_COLUMNS_ADDED.TABLE_ID matches the table definition that is being loaded from SYS_TABLES and SYS_COLUMNS, then the values of SYS_COLUMNS_ADDED.POS must be in strict sequential order (no gaps allowed) and run up to the number of columns in SYS_COLUMNS.

The value SYS_COLUMNS_ADDED.DEFAULT_VALUE = NULL is not allowed if the column is declared NOT NULL.

When SYS_COLUMNS_ADDED.DEFAULT_VALUE is not NULL, it must match the allowed length and value of the column (it must match the fixed length of the column declared in SYS_COLUMNS, or it must not exceed the maximum length of a variable-length column.

The code that loads the table definition must report that the table is corrupted if any inconsistency between SYS_COLUMNS and SYS_COLUMNS_ADDED is detected.

h3. Example of the InnoDB data dictionary changes
{code:SQL}
CREATE TABLE t1(a INT PRIMARY KEY, b INT) ENGINE=InnoDB ROW_FORMAT=REDUNDANT;
INSERT INTO t1 SET a=1;
# instantly add columns
ALTER TABLE t1 ADD COLUMN c INT, d CHAR(10) NULL DEFAULT 'foo', e INT NOT NULL DEFAULT 42;
INSERT INTO t1 SET a=2;
# change some DEFAULT values of old columns, and add one column
ALTER TABLE t1 CHANGE COLUMN b b INT DEFAULT 5,
CHANGE COLUMN c c INT DEFAULT 10,
CHANGE COLUMN d d DEFAULT NULL, ADD COLUMN f INT UNSIGNED DEFAULT 0;
INSERT INTO t1 SET a=3;
# change DEFAULTs in .frm file only (nothing inside InnoDB)
ALTER TABLE t1 CHANGE COLUMN a a INT 101, COLUMN b b INT DEFAULT 102,
CHANGE COLUMN c c INT DEFAULT 103,
CHANGE COLUMN d d DEFAULT 'eleventy',
CHANGE COLUMN e e INT UNSIGNED DEFAULT 105,
CHANGE COLUMN e e INT UNSIGNED DEFAULT 106;
SELECT * FROM t1;
# must return (1,NULL,NULL,'foo',42,0),(2,NULL,NULL,'foo',42,0),(3,5,10,NULL,0)
{code}

(For simplicity, the ALTER TABLE statements in the example are not renaming the columns while changing defaults. Such a combination must be supported.)

After the CREATE TABLE, we will have 2 SYS_COLUMNS entries for the table and no SYS_COLUMNS_ADDED entries.

After the first INSERT, the clustered index will contain the following record:
(a,DB_TRX_ID,DB_ROLL_PTR,b)=(1,…,…,NULL). The table format is still compatible with MySQL or older MariaDB versions.

After the first ALTER TABLE, we would have 5 entries in SYS_COLUMNS and 3 entries in SYS_COLUMNS_ADDED for the user table. The SYS_COLUMNS_ADDED entries would be: (table_id,2,NULL),(table_id,3,'foo'),(table_id,4,'\200\0\0*'). The columns in SYS_COLUMNS_ADDED are numbered from 0 onwards, so the first entry will have SYS_COLUMNS_ADDED.POS equal to the number of ‘core columns’. Initially, a table must always be created with at least 1 column. The last value is the binary value corresponding to the internal representation of the INT value 42 in InnoDB.

After the second INSERT, the latest clustered index definition will be
(a,DB_TRX_ID,DB_ROLL_PTR,b,c,d,e) and the index will contain the following:
(1,…,…,NULL),(2,…,…,NULL,NULL,'foo',42).

After the second ALTER TABLE, we will *not* update the existing SYS_COLUMNS_ADDED entries to reflect the changed DEFAULT values. They will keep the initial DEFAULT values that were in effect during the instant ADD COLUMN. The .frm file will contain the latest DEFAULT value. Inside InnoDB, the only changes will reflect the instant ADD COLUMN e: a SYS_COLUMNS record and the SYS_COLUMNS_ADDED record (table_id,5,'\0\0\0\0').

After the third INSERT, the latest clustered index definition will be
(a,DB_TRX_ID,DB_ROLL_PTR,b,c,d,e,f) and the index will contain the following:
(1,…,…,NULL),(2,…,…,NULL,NULL,'foo',42),(3,…,…,5,10,NULL,0).

The last ALTER does nothing inside InnoDB. It is only changing the DEFAULT values of already existing columns. Those are only stored in the .frm file. The SYS_COLUMNS_ADDED table only stores the initial DEFAULT values of instantly added columns.

The SELECT will fill in the *initial DEFAULT values* of the columns that are missing from the index entries. That is, for the first record, we will fill in all defaults that are stored in SYS_COLUMNS_ADDED: (c=NULL,d='foo',e=42,f=0). The second and third INSERT stored the current DEFAULT values, not the initial ones. So, for the second record, we will fill in (f=0).

h2. InnoDB file format changes

Instant ADD COLUMN will not be supported for ROW_FORMAT=COMPRESSED. It will be supported for all other ROW_FORMAT (REDUNDANT, COMPACT, DYNAMIC).

We will not introduce new ROW_FORMAT names, but instead refine the binary format in the InnoDB data files.

h3. Feature flag in the clustered index root page
If instant ADD COLUMN has been used, the following flag will be set in the clustered index root page:
{code:C}
/** Determine if instant ADD COLUMN was used.
This flag is only supposed to be set on the clustered index root page.
@param[in]	page	index page
@return	whether instant ADD COLUMN was used
(should be false if the page is not a clustered index root page) */
inline
bool
page_is_instant(const page_t* page)
{
         return(page[PAGE_HEADER + PAGE_N_HEAP] & 0x40);
}
{code}
It is an error (data corruption) if page_is_instant(clust_root) does not reflect the existence of SYS_COLUMNS_ADDED records for the table.
At the same time, the functions page_dir_set_n_heap() and page_dir_get_n_heap() must be revised to mask the 2 most significant bits of PAGE_N_HEAP. Previously, only one bit (indicating ROW_FORMAT!=REDUNDANT) was masked, starting with MySQL 5.0.3 which introduced ROW_FORMAT=COMPACT.

It is an error (data corruption) if page_is_instant(page) holds on any other index page than a clustered index root page. The corruption must be reported by CHECK TABLE and by btr_assert_not_corrupted().

(Why use the second-most-significant bit in PAGE_N_HEAP? The most significant bit identifies ROW_FORMAT!=REDUNDANT. The second-most-significant bit is always 0 so far, because there can never be more than 16383 records in a page. The largest possible seems to be innodb_page_size/(5+2) which is 65536/7=9362. Yes, the minimum record size would be 5+1 bytes for a secondary index leaf page record, say CREATE TABLE(a CHAR(1) PRIMARY KEY, INDEX(a)), but you cannot have more than 256 distinct values for 1-byte payload. Even with 65536/6 we get 10922, which is less than 16384.)

h3. Record format changes
Any data corruption must be detected by CHECK TABLE and by btr_index_rec_validate() and similar functions.

After instant ADD COLUMN on a ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC table, INSERT or UPDATE will set a new flag REC_INFO_ADDED_FLAG=0x80 in info_bits and explicitly store the number of ‘non-core’ fields in the record header, immediately before the REC_N_NEW_EXTRA_BYTES, using a variable-length encoding of 2 bytes, something like this:
{code:C}
	const byte* nulls = rec - REC_N_NEW_EXTRA_BYTES;
	ulint n_nullable = index->n_nullable;
	if (rec[-REC_NEW_INFO_BITS] & REC_INFO_ADDED_FLAG) {
		/* This must be ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC.
		ROW_FORMAT=COMPRESSED pages cannot even store this flag! */
		ut_ad(page_rec_is_comp(rec));
		ut_ad(!DICT_TF_GET_ZIP_SSIZE(index->table->flags));
		ulint n = *nulls--;
		if (n >= 0x80) {
			n = (n & 0x7f) << 8 | *nulls--;
		}
		n_fields = n_core_fields + n + 1;
		if (n_fields >= index->n_fields) ...; // error
		n_nullable += ...;
	}
	const byte* lens = nulls - UT_BITS_IN_BYTES(n_nullable);
{code}

It is an error (data corruption) if REC_INFO_ADDED_FLAG is set in a ROW_FORMAT=REDUNDANT table or in a node pointer record or in secondary indexes, or when instant ADD COLUMN has not been used.

On ROW_FORMAT=REDUNDANT pages, the records already explicitly contain the number of fields.

It is an error (data corruption) if a record is marked as containing more fields than the table definition implies, or if a stored field does not match the table definition.

It is an error (data corruption) if the n_fields in a ROW_FORMAT=REDUNDANT record is less than the number of index record fields derived from the number of core columns.

When REC_INFO_ADDED_FLAG is set (COMPACT or DYNAMIC), the above-mentioned variable-length encoding of n_fields is guaranteed by design to be more than the number of ‘core fields’.

It is an error (data corruption) if instant ADD COLUMN has not been used, but the n_fields in a ROW_FORMAT=REDUNDANT clustered index definition does not match the clustered index definition.

It is an error (data corruption) if the n_fields in a ROW_FORMAT=REDUNDANT node pointer record or secondary index record does not match the index definition.

The n_nullable flags in the clustered index node pointer records in ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC must reflect the number of ‘core columns’. These flags are useless garbage, and they are only reserved because of file format compatibility. (Clustered index node pointer records only contain the PRIMARY KEY columns, which are always NOT NULL, so we should have used n_nullable=0.) Because the size of n_nullable is not explicitly stored in the page and because we do not want to change the format of node pointer records or pages, this is our only choice.

The size of the n_nullable flags in the ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC clustered index leaf page records must correspond to the number of columns and the stored n_fields in the record. (More bytes may be allocated for new records after ALTER TABLE…ADD COLUMN…NULL.)

h3. Redo log format changes
Because the InnoDB redo log format (somewhat unnecessarily) is a mix of physical and logical operations, some change is needed to the way how INSERT and UPDATE operations are redo logged for ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC tables. Some ideas of simplifying the redo log format (and speeding up recovery) are listed in MDEV-12353.

It is possible that actual changes to the redo log format (such as the function mlog_open_and_write_index()) are unavoidable.

Even if the redo log format is not changed, it would seem safest to try to refuse startup with an older server version. We can do this by introducing a new redo log format identifier, similar to what was done in MDEV-11782.

While we could theoretically retain support for crash recovery from old-format redo log, it is probably best to not try that. (The GA version of MySQL 5.7 or MariaDB 10.2 will refuse crash recovery from older server versions, and MySQL 8.0.0 will refuse crash recovery from MySQL 5.7 or older versions. It is acceptable that MariaDB 10.3 would not support crash recovery from 10.2.)

h2. Changes to DML operations
If instant ADD COLUMN has not been used, the file format will not be changed. This means that any table-rebuilding ALTER operation (such as ALTER TABLE…FORCE or ALTER TABLE…ALGORITHM=COPY) will convert the table to the old format.

If instant ADD COLUMN has been used, any INSERT or UPDATE to the clustered index leaf page must store the current number of columns in the record. (For ROW_FORMAT=COMPACT or ROW_FORMAT=DYNAMIC we must also set REC_INFO_ADDED_FLAG to signal the presence of the n_fields in the record header.)

The REC_INFO_DELETED_FLAG of a record can be set or unset without changing other parts of the record. Usually this is done by a DELETE or an UPDATE of a PRIMARY KEY column, or the ROLLBACK of those operations.

On SELECT (also as part of UPDATE), if the clustered index leaf page record is found to contain fewer index fields than the current table definition implies, the missing fields will be filled in from SYS_COLUMNS_ADDED.DEFAULT_VALUE."
1778,MDEV-11369,MDEV,vinchen,94073,2017-04-19 03:46:13,"Hi Marko. I am very glad that we had a good face-to-face communication for instant add columns at MariaDB Developer Conference 2017. And we reached a consensus on it.

Thanks for the detailed design proposal above, it is very clear for me. And most of the design had been done in TMySQL from Tencent.

I will start to merge the feature from TMySQL to MariaDB 10.3 as soon as possible.
",12,"Hi Marko. I am very glad that we had a good face-to-face communication for instant add columns at MariaDB Developer Conference 2017. And we reached a consensus on it.

Thanks for the detailed design proposal above, it is very clear for me. And most of the design had been done in TMySQL from Tencent.

I will start to merge the feature from TMySQL to MariaDB 10.3 as soon as possible.
"
1779,MDEV-11369,MDEV,Marko Mäkelä,96699,2017-06-20 12:12:23,"To reduce the scope of this task, I filed a follow-up task:
MDEV-13134 Introduce ALTER TABLE attributes ALGORITHM=NOCOPY and ALGORITHM=INSTANT",13,"To reduce the scope of this task, I filed a follow-up task:
MDEV-13134 Introduce ALTER TABLE attributes ALGORITHM=NOCOPY and ALGORITHM=INSTANT"
1780,MDEV-11369,MDEV,Marko Mäkelä,99632,2017-09-04 11:35:09,"To avoid complicating import, export, and future data dictionary related tasks such as MDEV-11655, I am trying to prototype a design change that minimizes the format changes to the InnoDB data dictionary tables.
Instead of introducing a new table SYS_COLUMNS_ADDED to store the default values of instantly-added columns, my revised design would introduce a special 'default row' record at the start of the clustered index. This record would carry the magic REC_INFO_MIN_REC_FLAG, which was until now only set on the leftmost node pointer record of each non-leaf level.
We must also store the original number of columns (’core columns’) somewhere in the clustered index tree, instead of only storing it somewhere in the InnoDB data dictionary.
There is a chicken-and-egg problem here because of my unfortunate design mistake in MySQL 5.0.3 that reserves the n_nullable bits in node pointer records for ROW_FORMAT!=REDUNDANT. It is clearly not possible to navigate to the leftmost leaf page by reading child page numbers from mach_read_from_4(rec_get_next_ptr(node_ptr_with_min_rec_flag, 1)-4) because the next-record pointer may have been changed, for example if some pages were merged and the original next-record pointer was moved to the PAGE_FREE list.

It looks like instant ADD COLUMN should write 1+UT_BITS_IN_BYTES(n_nullable) to spare 8 bits in the clustered index root page. A good candidate would be the high-order byte of PAGE_DIRECTION, which was always written as 0 [starting with MySQL 4.0.14 and 4.1.1|https://github.com/MariaDB/server/commit/97fe74ac418488bee5ed270e10956d9b340540a3], so always for ROW_FORMAT!=REDUNDANT. Originally, it was sometimes written as uninitialized garbage for ROW_FORMAT=REDUNDANT in the [initial InnoDB version|https://github.com/MariaDB/server/commit/132e667b0bbbe33137b6baeb59f3f22b7524f066].",14,"To avoid complicating import, export, and future data dictionary related tasks such as MDEV-11655, I am trying to prototype a design change that minimizes the format changes to the InnoDB data dictionary tables.
Instead of introducing a new table SYS_COLUMNS_ADDED to store the default values of instantly-added columns, my revised design would introduce a special 'default row' record at the start of the clustered index. This record would carry the magic REC_INFO_MIN_REC_FLAG, which was until now only set on the leftmost node pointer record of each non-leaf level.
We must also store the original number of columns (’core columns’) somewhere in the clustered index tree, instead of only storing it somewhere in the InnoDB data dictionary.
There is a chicken-and-egg problem here because of my unfortunate design mistake in MySQL 5.0.3 that reserves the n_nullable bits in node pointer records for ROW_FORMAT!=REDUNDANT. It is clearly not possible to navigate to the leftmost leaf page by reading child page numbers from mach_read_from_4(rec_get_next_ptr(node_ptr_with_min_rec_flag, 1)-4) because the next-record pointer may have been changed, for example if some pages were merged and the original next-record pointer was moved to the PAGE_FREE list.

It looks like instant ADD COLUMN should write 1+UT_BITS_IN_BYTES(n_nullable) to spare 8 bits in the clustered index root page. A good candidate would be the high-order byte of PAGE_DIRECTION, which was always written as 0 [starting with MySQL 4.0.14 and 4.1.1|URL so always for ROW_FORMAT!=REDUNDANT. Originally, it was sometimes written as uninitialized garbage for ROW_FORMAT=REDUNDANT in the [initial InnoDB version|URL"
1781,MDEV-11369,MDEV,Marko Mäkelä,100045,2017-09-12 08:21:03,"I think that we must actually store the original number of clustered index fields in the root page, using 10 bits.
Because the PAGE_DIRECTION field may contain garbage for ROW_FORMAT=REDUNDANT tables, we must introduce a new FIL_PAGE_TYPE to tell if the new field PAGE_INSTANT is present in the former most significant 10 bits of PAGE_DIRECTION.
The new FIL_PAGE_TYPE on the clustered index root page will also prevent older MariaDB versions from opening or importing tables where instant ADD COLUMN has been used.

The special ‘default row’ would contain the values of instantly added columns. When the table is opened for the first time and the clustered index root page indicates that instant ADD COLUMN may have been used, we will fetch the ‘default row’. If its field count disagrees with the field count of the clustered index in the data dictionary, we will treat the table as corrupted.

Note: Whenever the clustered index becomes empty of user records, the root page should be reset to ‘non-instant’ form (FIL_PAGE_TYPE reset to FIL_PAGE_INDEX, and the PAGE_INSTANT bits be cleared, and the ‘default row’ deleted). This improves compatibility with old versions.",15,"I think that we must actually store the original number of clustered index fields in the root page, using 10 bits.
Because the PAGE_DIRECTION field may contain garbage for ROW_FORMAT=REDUNDANT tables, we must introduce a new FIL_PAGE_TYPE to tell if the new field PAGE_INSTANT is present in the former most significant 10 bits of PAGE_DIRECTION.
The new FIL_PAGE_TYPE on the clustered index root page will also prevent older MariaDB versions from opening or importing tables where instant ADD COLUMN has been used.

The special ‘default row’ would contain the values of instantly added columns. When the table is opened for the first time and the clustered index root page indicates that instant ADD COLUMN may have been used, we will fetch the ‘default row’. If its field count disagrees with the field count of the clustered index in the data dictionary, we will treat the table as corrupted.

Note: Whenever the clustered index becomes empty of user records, the root page should be reset to ‘non-instant’ form (FIL_PAGE_TYPE reset to FIL_PAGE_INDEX, and the PAGE_INSTANT bits be cleared, and the ‘default row’ deleted). This improves compatibility with old versions."
1782,MDEV-11369,MDEV,Marko Mäkelä,100631,2017-09-22 16:45:37,"I have now mostly implemented the revised design.
The record header format changes will be slightly different too. We will not introduce any new info_bits.
For ROW_FORMAT=REDUNDANT records, there is no change to the record format. For ROW_FORMAT=COMPACT and ROW_FORMAT=DYNAMIC, the status bit value REC_STATUS_COLUMNS_ADDED=4 will signal that the clustered index leaf page record has additional fields.

One change that I did not implement yet will be that for REC_STATUS_COLUMNS_ADDED records, the ‘number of fields’ stored in the record header will be n_fields-n_core_fields-1. That is, if 1 field was added, the value 0 will be encoded. This allows up to 128 instantly added columns to be represented in 1 additional header byte.",16,"I have now mostly implemented the revised design.
The record header format changes will be slightly different too. We will not introduce any new info_bits.
For ROW_FORMAT=REDUNDANT records, there is no change to the record format. For ROW_FORMAT=COMPACT and ROW_FORMAT=DYNAMIC, the status bit value REC_STATUS_COLUMNS_ADDED=4 will signal that the clustered index leaf page record has additional fields.

One change that I did not implement yet will be that for REC_STATUS_COLUMNS_ADDED records, the ‘number of fields’ stored in the record header will be n_fields-n_core_fields-1. That is, if 1 field was added, the value 0 will be encoded. This allows up to 128 instantly added columns to be represented in 1 additional header byte."
1783,MDEV-11369,MDEV,Marko Mäkelä,100668,2017-09-25 05:39:21,"h1. Changes to the undo log format
For inserting the ‘default row’ record, we need a new undo log record type TRX_UNDO_INSERT_DEFAULT. This is because the normal type TRX_UNDO_INSERT_REC can only be used for searching by key, not by the magic REC_INFO_MIN_REC_FLAG at the start of the index.

For updating records so that the number of clustered index fields is changing, we can use the existing undo log record types. We must merely treat the rollback of the ‘default row’ record specially. This is demonstrated by the tests innodb.instant_alter_crash and innodb.instant_alter_inject. The trick is that on rollback, we will truncate the ‘default row’ record just like we would truncate user records, as described below.

h1. Optimizing storage
To optimize storage, if a record is inserted or updated in such a way that the clustered index fields corresponding to the last instantly added columns have the same value as in the instant ADD COLUMN…DEFAULT clause (same values as in the ‘default row’ record), these fields will be omitted from the record. Example:
{code:sql}
CREATE TABLE t1 (id INT PRIMARY KEY) ENGINE=InnoDB;
SET @ts = @@session.timestamp;
ALTER TABLE t1 ADD COLUMN (t TIMESTAMP NOT NULL DEFAULT current_timestamp(), i INT, v VARCHAR(20) NOT NULL DEFAULT 'abcde');
INSERT INTO t1 SET id=1;
{code}
In this example, the ‘default row’ record would store (current_timestamp(),NULL,'abcde') for the instantly added columns. The value of current_timestamp() would be the one that was evaluated during the ALTER TABLE statement. The clustered index record for the INSERT would explicitly store the values of the columns (id), or (id,t) if the default value expression current_timestamp() during the ALTER and INSERT evaluate to a different value. The values of the instantly added columns i,v would correspond to the ‘default row’ record and could be omitted. If the last instantly added column differs from the ‘default row’ record, then all fields would have to be stored in the clustered index record.
The same rules apply to update:
{code:sql}
BEGIN;
UPDATE t1 SET v='foo';
ROLLBACK;
ALTER TABLE t1 CHANGE t t TIMESTAMP NOT NULL DEFAULT '2000-07-31 21:10:05';
SET timestamp=@ts;
UPDATE t1 SET t=current_timestamp(), v=DEFAULT;
{code}
The first UPDATE would store all columns (id,t,i,v) in the clustered index. The ROLLBACK would shrink the record back to (id) or (id,t). The second UPDATE would shrink the record back to (id), because the timestamp is being assigned to the same value that existed when the table was created.
Note: the current DEFAULT value of the column t does not matter. It is the DEFAULT value that was specified during instant ADD COLUMN that matters. This value is what the ‘default row’ record will contain.",17,"h1. Changes to the undo log format
For inserting the ‘default row’ record, we need a new undo log record type TRX_UNDO_INSERT_DEFAULT. This is because the normal type TRX_UNDO_INSERT_REC can only be used for searching by key, not by the magic REC_INFO_MIN_REC_FLAG at the start of the index.

For updating records so that the number of clustered index fields is changing, we can use the existing undo log record types. We must merely treat the rollback of the ‘default row’ record specially. This is demonstrated by the tests innodb.instant_alter_crash and innodb.instant_alter_inject. The trick is that on rollback, we will truncate the ‘default row’ record just like we would truncate user records, as described below.

h1. Optimizing storage
To optimize storage, if a record is inserted or updated in such a way that the clustered index fields corresponding to the last instantly added columns have the same value as in the instant ADD COLUMN…DEFAULT clause (same values as in the ‘default row’ record), these fields will be omitted from the record. Example:
{code:sql}
CREATE TABLE t1 (id INT PRIMARY KEY) ENGINE=InnoDB;
SET @ts = @@session.timestamp;
ALTER TABLE t1 ADD COLUMN (t TIMESTAMP NOT NULL DEFAULT current_timestamp(), i INT, v VARCHAR(20) NOT NULL DEFAULT 'abcde');
INSERT INTO t1 SET id=1;
{code}
In this example, the ‘default row’ record would store (current_timestamp(),NULL,'abcde') for the instantly added columns. The value of current_timestamp() would be the one that was evaluated during the ALTER TABLE statement. The clustered index record for the INSERT would explicitly store the values of the columns (id), or (id,t) if the default value expression current_timestamp() during the ALTER and INSERT evaluate to a different value. The values of the instantly added columns i,v would correspond to the ‘default row’ record and could be omitted. If the last instantly added column differs from the ‘default row’ record, then all fields would have to be stored in the clustered index record.
The same rules apply to update:
{code:sql}
BEGIN;
UPDATE t1 SET v='foo';
ROLLBACK;
ALTER TABLE t1 CHANGE t t TIMESTAMP NOT NULL DEFAULT '2000-07-31 21:10:05';
SET timestamp=@ts;
UPDATE t1 SET t=current_timestamp(), v=DEFAULT;
{code}
The first UPDATE would store all columns (id,t,i,v) in the clustered index. The ROLLBACK would shrink the record back to (id) or (id,t). The second UPDATE would shrink the record back to (id), because the timestamp is being assigned to the same value that existed when the table was created.
Note: the current DEFAULT value of the column t does not matter. It is the DEFAULT value that was specified during instant ADD COLUMN that matters. This value is what the ‘default row’ record will contain."
1784,MDEV-11369,MDEV,Marko Mäkelä,101115,2017-10-04 07:51:23,"A counter will be introduced to keep track of the instant ALTER TABLE operations that affect the columns of the table:
{code:sql}
SELECT variable_value FROM information_schema.global_status
WHERE variable_name = 'innodb_instant_alter_column'
{code}
This same counter can be used in the follow-up task MDEV-11424. For now, it is only incremented for instant ADD COLUMN operations.",18,"A counter will be introduced to keep track of the instant ALTER TABLE operations that affect the columns of the table:
{code:sql}
SELECT variable_value FROM information_schema.global_status
WHERE variable_name = 'innodb_instant_alter_column'
{code}
This same counter can be used in the follow-up task MDEV-11424. For now, it is only incremented for instant ADD COLUMN operations."
1785,MDEV-11371,MDEV,Sergey Vojtovich,99523,2017-08-31 14:40:50,Pushed https://github.com/MariaDB/server/commit/fdc47792354c820aa4a8542d7c00d434424a63fb,1,Pushed URL
1786,MDEV-11424,MDEV,Marko Mäkelä,101580,2017-10-15 13:40:17,"I filed the following prerequisite tasks:
MDEV-15562 Instant {{DROP COLUMN}} or changing the order of columns
MDEV-15563 Instant {{NOT NULL}} removal and {{CHAR}} or {{VARCHAR}} extension for {{ROW_FORMAT=REDUNDANT}}

This task would address the remaining {{ALTER TABLE}} operations that currently require the table to be rebuilt. The solution would involve storing a format identifier in each record and storing multiple versions of the table definition by somehow extending the hidden ‘default row’ record that was introduced in MDEV-11369 and will be extended by MDEV-15562.

Secondary indexes of full columns (not column prefixes) in {{ROW_FORMAT=COMPACT}} or {{ROW_FORMAT=DYNAMIC}} will have to be rebuilt when the indexed columns are modified as follows:
# Changing the length of a fixed-length column ({{CHAR}}, {{BINARY}}). (For {{ROW_FORMAT=REDUNDANT}}, this will be determined in MDEV-15563.)
# Changing a {{VARCHAR}} maximum length from up to 255 bytes to more than 255 bytes.

Lifting these restrictions on secondary indexes would require a change to the secondary index format and a rewrite of the InnoDB change buffer (MDEV-11634) so that the full table metadata would be available on change buffer merge operations.",1,"I filed the following prerequisite tasks:
MDEV-15562 Instant {{DROP COLUMN}} or changing the order of columns
MDEV-15563 Instant {{NOT NULL}} removal and {{CHAR}} or {{VARCHAR}} extension for {{ROW_FORMAT=REDUNDANT}}

This task would address the remaining {{ALTER TABLE}} operations that currently require the table to be rebuilt. The solution would involve storing a format identifier in each record and storing multiple versions of the table definition by somehow extending the hidden ‘default row’ record that was introduced in MDEV-11369 and will be extended by MDEV-15562.

Secondary indexes of full columns (not column prefixes) in {{ROW_FORMAT=COMPACT}} or {{ROW_FORMAT=DYNAMIC}} will have to be rebuilt when the indexed columns are modified as follows:
# Changing the length of a fixed-length column ({{CHAR}}, {{BINARY}}). (For {{ROW_FORMAT=REDUNDANT}}, this will be determined in MDEV-15563.)
# Changing a {{VARCHAR}} maximum length from up to 255 bytes to more than 255 bytes.

Lifting these restrictions on secondary indexes would require a change to the secondary index format and a rewrite of the InnoDB change buffer (MDEV-11634) so that the full table metadata would be available on change buffer merge operations."
1787,MDEV-11424,MDEV,Marko Mäkelä,118179,2018-10-22 03:22:58,"The following were filed as follow-up work for MDEV-15562:
MDEV-17459 Allow instant ALTER TABLE even if FULLTEXT INDEX exists
MDEV-17468 Avoid table rebuild on operations on generated columns
MDEV-17494 Refuse ALGORITHM=INSTANT when the row size is too large",2,"The following were filed as follow-up work for MDEV-15562:
MDEV-17459 Allow instant ALTER TABLE even if FULLTEXT INDEX exists
MDEV-17468 Avoid table rebuild on operations on generated columns
MDEV-17494 Refuse ALGORITHM=INSTANT when the row size is too large"
1788,MDEV-11424,MDEV,Marko Mäkelä,120681,2018-12-12 14:48:51,Fixing MDEV-12836 would allow instantaneous changes of {{AUTO_INCREMENT}} settings.,3,Fixing MDEV-12836 would allow instantaneous changes of {{AUTO_INCREMENT}} settings.
1789,MDEV-11424,MDEV,Marko Mäkelä,123611,2019-02-20 10:11:38,"MDEV-17520 will not be in MariaDB Server 10.4 due to the size overhead of the current implementation (of using a format like {{ROW_FORMAT=REDUNDANT}} on clustered index leaf pages). We could implement it later in a different form, using a per-page or per-record format identifier.",4,"MDEV-17520 will not be in MariaDB Server 10.4 due to the size overhead of the current implementation (of using a format like {{ROW_FORMAT=REDUNDANT}} on clustered index leaf pages). We could implement it later in a different form, using a per-page or per-record format identifier."
1790,MDEV-11424,MDEV,Marko Mäkelä,123616,2019-02-20 11:44:31,"Some related bugs will be fixed later. The MariaDB 10.4 tasks related to instant ALTER TABLE (mainly MDEV-15562, MDEV-15563, MDEV-15564) have been completed.",5,"Some related bugs will be fixed later. The MariaDB 10.4 tasks related to instant ALTER TABLE (mainly MDEV-15562, MDEV-15563, MDEV-15564) have been completed."
1791,MDEV-11426,MDEV,Marko Mäkelä,88985,2016-12-01 11:04:01,"While doing this, I also fixed some broken (skipped or disabled) tests in --suite=innodb_zip. I imported some missing .inc files from MySQL 5.7.",1,"While doing this, I also fixed some broken (skipped or disabled) tests in --suite=innodb_zip. I imported some missing .inc files from MySQL 5.7."
1792,MDEV-11426,MDEV,Jan Lindström,88986,2016-12-01 11:08:13,Looks correct to me and good that we have those innodb_zip suite test back there.,2,Looks correct to me and good that we have those innodb_zip suite test back there.
1793,MDEV-11426,MDEV,Marco Marsala,134576,2019-09-23 11:32:38,It also fixes a security issue related tothis table,3,It also fixes a security issue related tothis table
1794,MDEV-11429,MDEV,Michael Widenius,89051,2016-12-02 12:20:08,Fixed and pushed,1,Fixed and pushed
1795,MDEV-11429,MDEV,Michael Widenius,110225,2018-04-26 11:23:00,Should also be fixed in 10.0,2,Should also be fixed in 10.0
1796,MDEV-11429,MDEV,Michael Widenius,110226,2018-04-26 11:23:25,Fix pushed,3,Fix pushed
1797,MDEV-11557,MDEV,Alexey Botchkov,91034,2017-01-24 13:54:30,http://lists.askmonty.org/pipermail/commits/2017-January/010507.html,1,URL
1798,MDEV-11714,MDEV,Vicențiu Ciorbaru,90374,2017-01-07 14:01:21,https://github.com/MariaDB/server/commit/084b9237cf45208079715e50e83c61077777f208,1,URL
1799,MDEV-11743,MDEV,Georg Richter,92797,2017-03-09 08:27:45,"Required changes for Connector/C already pushed. Connector/C uses environment variables MASTER_MYSOCK and MASTER_MYPORT (still open: host, user and schema).",1,"Required changes for Connector/C already pushed. Connector/C uses environment variables MASTER_MYSOCK and MASTER_MYPORT (still open: host, user and schema)."
1800,MDEV-11743,MDEV,Sergei Golubchik,92908,2017-03-12 10:03:24,"Comments:
# I presume new commands you've added to {{mysql-test/lib/generate-ssl-certs.sh}} don't generate new certificates, but only convert existing ones to something else? I mean, the expiration date is the same as for other certs? This script creates a bunch of certs, that all expire at the same day and this script is used to recreate them. If different certs will expire at different dates, it'll be inconvenient to maintain.
# In C/C: Using {{CONFIGURE_FILE}} to replace {{@CERT_PATH@}} is rather hackish. A proper way to do it is to use {{ADD_DEFINITIONS(-DCERT_PATH=$\{CERT_PATH\}}} (or {{SET_PROPERTY}} to do it only for one file). But it's still wrong — you cannot use compile-time path in tests, because the tests could've been moved (e.g. packaged and installed). Use simply {{std_data}}, tests are run from the test dir. Or {{$MYSQL_TEST_DIR/std_data}} or even {{$SECURE_LOAD_PATH}}.",2,"Comments:
# I presume new commands you've added to {{mysql-test/lib/generate-ssl-certs.sh}} don't generate new certificates, but only convert existing ones to something else? I mean, the expiration date is the same as for other certs? This script creates a bunch of certs, that all expire at the same day and this script is used to recreate them. If different certs will expire at different dates, it'll be inconvenient to maintain.
# In C/C: Using {{CONFIGURE_FILE}} to replace {{@CERT_PATH@}} is rather hackish. A proper way to do it is to use {{ADD_DEFINITIONS(-DCERT_PATH=$\{CERT_PATH\}}} (or {{SET_PROPERTY}} to do it only for one file). But it's still wrong — you cannot use compile-time path in tests, because the tests could've been moved (e.g. packaged and installed). Use simply {{std_data}}, tests are run from the test dir. Or {{$MYSQL_TEST_DIR/std_data}} or even {{$SECURE_LOAD_PATH}}."
1801,MDEV-11751,MDEV,Marko Mäkelä,94390,2017-04-26 13:02:16,"This is now running on [bb-10.2-marko|https://github.com/MariaDB/server/commit/61b7998d84c5b1d00399b213496eee59995e7de6].
I omitted a number of commits. Some commits may be considered for inclusion later.
# [Bug#23264552 -- XA: ASSERT `M_STATUS == DA_ERROR'|https://github.com/mysql/mysql-server/commit/09b9845d0250d233d512ce64f68cf66635aa7858] (some of the changed files do dot exist in MariaDB, and we are probably missing some XA tests, such as those for [the XA disconnect bug|https://bugs.mysql.com/bug.php?id=12161])
# [Bug#23481444 OPTIMISER CALL ROW_SEARCH_MVCC() AND READ THE INDEX APPLIED BY UNCOMMITTED ROWS|https://github.com/mysql/mysql-server/commit/2f6741ef8e729e8ea6723ca434f95a40036e1d25] is an optimization that requires some changes to the SQL layer, which differs from MariaDB. We could optimize this further, by performing the end-range check also when switching pages, to terminate earlier when the index is filled with records that do not exist in the read view. Also, the number of follow-up commits suggests that this could be risky in a GA release: [1|https://github.com/mysql/mysql-server/commit/88248ad3db3a4da09184a36036193a91d6acd7c2], [2|https://github.com/mysql/mysql-server/commit/1b717fabeb111bd85eb19f35c0f0351aa4a5904f], [3|https://github.com/mysql/mysql-server/commit/f0db5227666184c3567b84834949c500c4656992], [4|https://github.com/mysql/mysql-server/commit/6ca4f693c1ce472e2b1bf7392607c2d1124b4293], [5|https://github.com/mysql/mysql-server/commit/b2368702d401f6cd1a4104fe590a6e6dbd14cd6c], [6|https://github.com/mysql/mysql-server/commit/d301d32d77698d2a5190224fc320d363163d6f8b].
# [Bug #14025581 FILE IO INSTRUMENTATION DISABLED IN PERFORMANCE SCHEMA FOR INNODB|https://github.com/mysql/mysql-server/commit/3fd5f795034150f5ada83abf501686e4d165e1dc]. This is a large patch with little benefit. Our I/O code is somewhat different, too. Follow-up patches: [1|https://github.com/mysql/mysql-server/commit/4dec97d5f9ad1afef59f8d0e05497222bbc5f769], [2|https://github.com/mysql/mysql-server/commit/a095abbd91233d8702ccd2bee62ec036b40febe1], [3|https://github.com/mysql/mysql-server/commit/46f4dd2b9c85deddbc3589ba52f0e2b1c0c4bb22].
# [BUG#24686908 INNODB SHOULD HAVE A MECHANISM TO CHECK IF ALL PFS KEYS ARE REGISTERED WITH PFS|https://github.com/mysql/mysql-server/commit/2202b677b747930dfeb9755b7daa08ca67119747] Again, MariaDB maybe does not care that much about performance_schema, to make it worth the effort to merge this 1900-line patch.
# [Bug #25297593 BUILD ON NATIVE FEDORA 25 FAILS DUE TO DEPRECATED LZ4_COMPRESS_LIMITEDOUTPUT|https://github.com/mysql/mysql-server/commit/b79f55016619c5fb040f5d897ef06ddff7016334] The patch is only applicable to the Oracle reimplementation of the MariaDB 10.1 page_compression. Maybe we should consider something similar for that?
# [Bug#24329079 Crash with InnoDB Encryption, 5.7.13, FusionIO, innodb_flush_method=O_DIRECT|https://github.com/mysql/mysql-server/commit/efa00ae791536ece487f2f18f66da7a0249100e3] This only applies to the Oracle reimplementation of MariaDB 10.1 encryption. Our I/O subsystem is very different. But maybe we should run some tests on FusionIO and other SSD.
# [Bug#25551311 BACKPORT BUG #23517560 REMOVE SPACE_ID RESTRICTION FOR UNDO TABLESPACES|https://github.com/mysql/mysql-server/commit/7c1e99893f70d39bb1f4cbeb3fab34cdbd2ab55c] The purpose of this is to allow dedicated undo tablespaces to be created without reinitializing the whole InnoDB instance. We do not test innodb_undo_tablespaces much, and I am a bit worried about possible regressions. I think that the proper solution would be in MDEV-11633 and its subtasks.",1,"This is now running on [bb-10.2-marko|URL
I omitted a number of commits. Some commits may be considered for inclusion later.
# [Bug#23264552 -- XA: ASSERT `M_STATUS == DA_ERROR'|URL (some of the changed files do dot exist in MariaDB, and we are probably missing some XA tests, such as those for [the XA disconnect bug|URL
# [Bug#23481444 OPTIMISER CALL ROW_SEARCH_MVCC() AND READ THE INDEX APPLIED BY UNCOMMITTED ROWS|URL is an optimization that requires some changes to the SQL layer, which differs from MariaDB. We could optimize this further, by performing the end-range check also when switching pages, to terminate earlier when the index is filled with records that do not exist in the read view. Also, the number of follow-up commits suggests that this could be risky in a GA release: [1|URL [2|URL [3|URL [4|URL [5|URL [6|URL
# [Bug #14025581 FILE IO INSTRUMENTATION DISABLED IN PERFORMANCE SCHEMA FOR INNODB|URL This is a large patch with little benefit. Our I/O code is somewhat different, too. Follow-up patches: [1|URL [2|URL [3|URL
# [BUG#24686908 INNODB SHOULD HAVE A MECHANISM TO CHECK IF ALL PFS KEYS ARE REGISTERED WITH PFS|URL Again, MariaDB maybe does not care that much about performance_schema, to make it worth the effort to merge this 1900-line patch.
# [Bug #25297593 BUILD ON NATIVE FEDORA 25 FAILS DUE TO DEPRECATED LZ4_COMPRESS_LIMITEDOUTPUT|URL The patch is only applicable to the Oracle reimplementation of the MariaDB 10.1 page_compression. Maybe we should consider something similar for that?
# [Bug#24329079 Crash with InnoDB Encryption, 5.7.13, FusionIO, innodb_flush_method=O_DIRECT|URL This only applies to the Oracle reimplementation of MariaDB 10.1 encryption. Our I/O subsystem is very different. But maybe we should run some tests on FusionIO and other SSD.
# [Bug#25551311 BACKPORT BUG #23517560 REMOVE SPACE_ID RESTRICTION FOR UNDO TABLESPACES|URL The purpose of this is to allow dedicated undo tablespaces to be created without reinitializing the whole InnoDB instance. We do not test innodb_undo_tablespaces much, and I am a bit worried about possible regressions. I think that the proper solution would be in MDEV-11633 and its subtasks."
1802,MDEV-11751,MDEV,Jan Lindström,94392,2017-04-26 13:21:59,About 5. in 10.2 we already use lz4_compress_default() on fil0pagecompress.cc if system has lz4 library supporting it. Opened https://jira.mariadb.org/browse/MDEV-12593 for 10.1.,2,About 5. in 10.2 we already use lz4_compress_default() on fil0pagecompress.cc if system has lz4 library supporting it. Opened URL for 10.1.
1803,MDEV-11751,MDEV,Marko Mäkelä,94430,2017-04-27 11:09:55,"The changes 2, 3, 7 were also introduced in MySQL 5.6.36, so it looks like we will be getting them when merging that to 10.0 and then via 10.1 to 10.2.
Change 1 should definitely be done. Maybe change 4 as well.",3,"The changes 2, 3, 7 were also introduced in MySQL 5.6.36, so it looks like we will be getting them when merging that to 10.0 and then via 10.1 to 10.2.
Change 1 should definitely be done. Maybe change 4 as well."
1804,MDEV-11751,MDEV,Marko Mäkelä,95591,2017-05-22 08:57:38,"I forgot to bump the InnoDB version from 5.7.14 to 5.7.18 when doing the merge. So, MariaDB 10.2.6 will report the InnoDB version as 5.7.14.

Change 2 was omitted when merging MySQL 5.6.36 to MariaDB 10.0, because it would require a handler API change.
Changes 3 and 7 were merged to 10.0, and from there to 10.1 and 10.2. So, they will appear in MariaDB 10.2.7.
MariaDB 10.2.7 will report InnoDB version as 5.7.18.",4,"I forgot to bump the InnoDB version from 5.7.14 to 5.7.18 when doing the merge. So, MariaDB 10.2.6 will report the InnoDB version as 5.7.14.

Change 2 was omitted when merging MySQL 5.6.36 to MariaDB 10.0, because it would require a handler API change.
Changes 3 and 7 were merged to 10.0, and from there to 10.1 and 10.2. So, they will appear in MariaDB 10.2.7.
MariaDB 10.2.7 will report InnoDB version as 5.7.18."
1805,MDEV-11782,MDEV,Marko Mäkelä,91003,2017-01-24 08:19:35,"As part of this task, we must test if MariaDB can be upgraded from 10.1 to 10.2 when log encryption is in use. Here is my test on Debian GNU/Linux Sid:
{noformat}
sudo apt install mariadb-server-10.1
mkdir /dev/shm/f; cd /dev/shm/f; cat > logkey.txt << EOF
1;36D6CB74CA7D4586CCC7261E174079CC5639E5F681D500ADFA887C165AD49301
2;F51F5108CF6048B4C9C88BA6CE1C13F9F1CBEE82080F7FA0F979DEF5D4B94509
EOF
/usr/sbin/mysqld --file-key-management-filename=logkey.txt --plugin-load=file_key_management.so  --basedir=/usr --datadir=/dev/shm/f --innodb --innodb-encrypt-log
{noformat}
After shutting down the 10.1 server (for me, it would should shut down because another mysqld instance was already started on port 3306), start 10.2 in gdb on the same files:
{code:gdb}
break recv_log_format_0_recover
run --gdb --datadir=/dev/shm/f --file-key-management-filename=logkey.txt --plugin-dir=/dev/shm/t/plugin/file_key_management --plugin-load=file_key_management.so --innodb --innodb-encrypt-log --lc-messages-dir=/dev/shm/t/sql/share
continue
{code}
Observe that the following message is displayed:
{noformat}
2017-01-24 10:18:21 140737353952960 [ERROR] InnoDB: Upgrade after a crash is not supported. This redo log was created before MySQL 5.7.9, and it appears corrupted. Please follow the instructions at http://dev.mysql.com/doc/refman/5.7/en/upgrading.html
{noformat}
I think that the message should be revised to refer to MariaDB 10.2.2, and that we should attempt to decrypt the data. Upgrade with a clean encrypted redo log from 10.1 should definitely be allowed.",1,"As part of this task, we must test if MariaDB can be upgraded from 10.1 to 10.2 when log encryption is in use. Here is my test on Debian GNU/Linux Sid:
{noformat}
sudo apt install mariadb-server-10.1
mkdir /dev/shm/f; cd /dev/shm/f; cat > logkey.txt << EOF
1;36D6CB74CA7D4586CCC7261E174079CC5639E5F681D500ADFA887C165AD49301
2;F51F5108CF6048B4C9C88BA6CE1C13F9F1CBEE82080F7FA0F979DEF5D4B94509
EOF
/usr/sbin/mysqld --file-key-management-filename=logkey.txt --plugin-load=file_key_management.so  --basedir=/usr --datadir=/dev/shm/f --innodb --innodb-encrypt-log
{noformat}
After shutting down the 10.1 server (for me, it would should shut down because another mysqld instance was already started on port 3306), start 10.2 in gdb on the same files:
{code:gdb}
break recv_log_format_0_recover
run --gdb --datadir=/dev/shm/f --file-key-management-filename=logkey.txt --plugin-dir=/dev/shm/t/plugin/file_key_management --plugin-load=file_key_management.so --innodb --innodb-encrypt-log --lc-messages-dir=/dev/shm/t/sql/share
continue
{code}
Observe that the following message is displayed:
{noformat}
2017-01-24 10:18:21 140737353952960 [ERROR] InnoDB: Upgrade after a crash is not supported. This redo log was created before MySQL 5.7.9, and it appears corrupted. Please follow the instructions at URL
{noformat}
I think that the message should be revised to refer to MariaDB 10.2.2, and that we should attempt to decrypt the data. Upgrade with a clean encrypted redo log from 10.1 should definitely be allowed."
1806,MDEV-11782,MDEV,Marko Mäkelä,91269,2017-02-01 07:59:38,"I think that as part of this effort, in 10.2 we must rebuild the redo log files on startup if the requested innodb_encrypt_log setting differs from the redo log header, just like we would rebuild the redo log files when upgrading from 10.1 or MySQL 5.6 or earlier.
We should also remove the log checkpoint call after fil_crypt_threads_init(), which according to [~jplindst] was added to handle the case that the innodb_encrypt_log setting was changed.",2,"I think that as part of this effort, in 10.2 we must rebuild the redo log files on startup if the requested innodb_encrypt_log setting differs from the redo log header, just like we would rebuild the redo log files when upgrading from 10.1 or MySQL 5.6 or earlier.
We should also remove the log checkpoint call after fil_crypt_threads_init(), which according to [~jplindst] was added to handle the case that the innodb_encrypt_log setting was changed."
1807,MDEV-11782,MDEV,Marko Mäkelä,91316,2017-02-01 17:27:20,"In the new encrypted redo log format for MySQL 10.2, the function log_block_checksum_is_ok() should be used so that we always know beforehand (based on the redo log header block) whether subsequent blocks are supposed to be encrypted. I would forcibly enable the redo log checksum when redo log encryption is used.",3,"In the new encrypted redo log format for MySQL 10.2, the function log_block_checksum_is_ok() should be used so that we always know beforehand (based on the redo log header block) whether subsequent blocks are supposed to be encrypted. I would forcibly enable the redo log checksum when redo log encryption is used."
1808,MDEV-11782,MDEV,Marko Mäkelä,91492,2017-02-06 14:56:58,"To support an upgrade from an encrypted 10.1 redo log, the function recv_log_format_0_recover() needs to call log_decrypt_after_read() after reading the redo log block.",4,"To support an upgrade from an encrypted 10.1 redo log, the function recv_log_format_0_recover() needs to call log_decrypt_after_read() after reading the redo log block."
1809,MDEV-11782,MDEV,Marko Mäkelä,91498,2017-02-06 16:49:08,"The code so far is available in [10.2|https://github.com/MariaDB/server/commit/92bbf4ad0477e09bcc86907696cd114ef42e6914]. The test encryption.innodb_encrypt_log_corruption demonstrates that we can upgrade from an encrypted 10.1 redo log file.
But we still need to introduce a redo log format tag that distinguishes the MariaDB encrypted redo log files from the MySQL 5.7.9 format.",5,"The code so far is available in [10.2|URL The test encryption.innodb_encrypt_log_corruption demonstrates that we can upgrade from an encrypted 10.1 redo log file.
But we still need to introduce a redo log format tag that distinguishes the MariaDB encrypted redo log files from the MySQL 5.7.9 format."
1810,MDEV-11782,MDEV,Marko Mäkelä,91741,2017-02-10 18:42:02,"I committed two preparatory patches to bb-10.2-marko for review:
[Remove recv_sys->last_block|https://github.com/MariaDB/server/commit/f984061e3f267eed892dbb2e1327ad1a2a2ee5f8]
[Add separate functions for reading 10.1 encrypted log|https://github.com/MariaDB/server/commit/ea1e472bba7403f7fbb0533f6898d2f2399d63ee] ",6,"I committed two preparatory patches to bb-10.2-marko for review:
[Remove recv_sys->last_block|URL
[Add separate functions for reading 10.1 encrypted log|URL "
1811,MDEV-11782,MDEV,Jan Lindström,91757,2017-02-12 10:52:09,ok to push above preparatory patches to 10.2.,7,ok to push above preparatory patches to 10.2.
1812,MDEV-11782,MDEV,Marko Mäkelä,91799,2017-02-13 17:35:58,[bb-10.2-mdev-11782|https://github.com/MariaDB/server/commit/97c2682805ebcfac75764ead99748d458f3e0573],8,[bb-10.2-mdev-11782|URL
1813,MDEV-11782,MDEV,Marko Mäkelä,91800,2017-02-13 18:24:53,Revised patch with less memcpy() and consistently using a 4-byte cleartext log block header: [bb-10.2-mdev-11782|https://github.com/MariaDB/server/commit/3135ecd7e4e2f99c24f5e7e8463041d2b7f723df],9,Revised patch with less memcpy() and consistently using a 4-byte cleartext log block header: [bb-10.2-mdev-11782|URL
1814,MDEV-11782,MDEV,Jan Lindström,91830,2017-02-14 09:43:31,ok to push after addressing review comments that are open.,10,ok to push after addressing review comments that are open.
1815,MDEV-11782,MDEV,Marko Mäkelä,91842,2017-02-14 13:04:29,"Revised patch, addressing the review comments and test failures, and also with some changes to the startup logic: We must not generate redo log before rebuilding the redo logs.
[bb-10.2-marko|https://github.com/MariaDB/server/commit/c15ecf5fdf077eb3ec7fc9ee0a0902f55dc02475] (includes also MDEV-12061)",11,"Revised patch, addressing the review comments and test failures, and also with some changes to the startup logic: We must not generate redo log before rebuilding the redo logs.
[bb-10.2-marko|URL (includes also MDEV-12061)"
1816,MDEV-11782,MDEV,Marko Mäkelä,91874,2017-02-15 06:11:05,"commit 2af28a363c0ac55c9b91aa9eb26949fc9ecf043a
Author: Marko Mäkelä <marko.makela@mariadb.com>
Date:   Fri Feb 10 12:11:42 2017 +0200

    MDEV-11782: Redefine the innodb_encrypt_log format
    
    Write only one encryption key to the checkpoint page.
    Use 4 bytes of nonce. Encrypt more of each redo log block,
    only skipping the 4-byte field LOG_BLOCK_HDR_NO which the
    initialization vector is derived from.
    
    Issue notes, not warning messages for rewriting the redo log files.
    
    recv_recovery_from_checkpoint_finish(): Do not generate any redo log,
    because we must avoid that before rewriting the redo log files, or
    otherwise a crash during a redo log rewrite (removing or adding
    encryption) may end up making the database unrecoverable.
    Instead, do these tasks in innobase_start_or_create_for_mysql().
    
    Issue a firm ""Missing MLOG_CHECKPOINT"" error message. Remove some
    unreachable code and duplicated error messages for log corruption.
    
    LOG_HEADER_FORMAT_ENCRYPTED: A flag for identifying an encrypted redo
    log format.
    
    log_group_t::is_encrypted(), log_t::is_encrypted(): Determine
    if the redo log is in encrypted format.
    
    recv_find_max_checkpoint(): Interpret LOG_HEADER_FORMAT_ENCRYPTED.
    
    srv_prepare_to_delete_redo_log_files(): Display NOTE messages about
    adding or removing encryption. Do not issue warnings for redo log
    resizing any more.
    
    innobase_start_or_create_for_mysql(): Rebuild the redo logs also when
    the encryption changes.
    
    innodb_log_checksums_func_update(): Always use the CRC-32C checksum
    if innodb_encrypt_log. If needed, issue a warning
    that innodb_encrypt_log implies innodb_log_checksums.
    
    log_group_write_buf(): Compute the checksum on the encrypted
    block contents, so that transmission errors or incomplete blocks can be
    detected without decrypting.
    
    Rewrite most of the redo log encryption code. Only remember one
    encryption key at a time (but remember up to 5 when upgrading from the
    MariaDB 10.1 format.)",12,"commit 2af28a363c0ac55c9b91aa9eb26949fc9ecf043a
Author: Marko Mäkelä 
Date:   Fri Feb 10 12:11:42 2017 +0200

    MDEV-11782: Redefine the innodb_encrypt_log format
    
    Write only one encryption key to the checkpoint page.
    Use 4 bytes of nonce. Encrypt more of each redo log block,
    only skipping the 4-byte field LOG_BLOCK_HDR_NO which the
    initialization vector is derived from.
    
    Issue notes, not warning messages for rewriting the redo log files.
    
    recv_recovery_from_checkpoint_finish(): Do not generate any redo log,
    because we must avoid that before rewriting the redo log files, or
    otherwise a crash during a redo log rewrite (removing or adding
    encryption) may end up making the database unrecoverable.
    Instead, do these tasks in innobase_start_or_create_for_mysql().
    
    Issue a firm ""Missing MLOG_CHECKPOINT"" error message. Remove some
    unreachable code and duplicated error messages for log corruption.
    
    LOG_HEADER_FORMAT_ENCRYPTED: A flag for identifying an encrypted redo
    log format.
    
    log_group_t::is_encrypted(), log_t::is_encrypted(): Determine
    if the redo log is in encrypted format.
    
    recv_find_max_checkpoint(): Interpret LOG_HEADER_FORMAT_ENCRYPTED.
    
    srv_prepare_to_delete_redo_log_files(): Display NOTE messages about
    adding or removing encryption. Do not issue warnings for redo log
    resizing any more.
    
    innobase_start_or_create_for_mysql(): Rebuild the redo logs also when
    the encryption changes.
    
    innodb_log_checksums_func_update(): Always use the CRC-32C checksum
    if innodb_encrypt_log. If needed, issue a warning
    that innodb_encrypt_log implies innodb_log_checksums.
    
    log_group_write_buf(): Compute the checksum on the encrypted
    block contents, so that transmission errors or incomplete blocks can be
    detected without decrypting.
    
    Rewrite most of the redo log encryption code. Only remember one
    encryption key at a time (but remember up to 5 when upgrading from the
    MariaDB 10.1 format.)"
1817,MDEV-11825,MDEV,Oleksandr Byelkin,90766,2017-01-18 11:47:30,"revision-id: 10c0278bc08f4e1b179dfcaae8fbfc4c4a0f51c1 (mariadb-10.2.3-76-g10c0278bc08)
parent(s): 9ea0b44c5696d9357465625c3fc9b7f57a856009
committer: Oleksandr Byelkin
timestamp: 2017-01-18 10:40:17 +0100
message:

MDEV-11825: Make session variables TRACKING enabled by default

---",1,"revision-id: 10c0278bc08f4e1b179dfcaae8fbfc4c4a0f51c1 (mariadb-10.2.3-76-g10c0278bc08)
parent(s): 9ea0b44c5696d9357465625c3fc9b7f57a856009
committer: Oleksandr Byelkin
timestamp: 2017-01-18 10:40:17 +0100
message:

MDEV-11825: Make session variables TRACKING enabled by default

---"
1818,MDEV-11825,MDEV,Sergei Golubchik,93251,2017-03-20 17:45:55,What's the effect on the performance?,2,What's the effect on the performance?
1819,MDEV-11825,MDEV,Oleksandr Byelkin,93702,2017-04-03 08:09:27,"For query which do not change something (which most of the query) I do not see difference:
I make 100000 SELECT 1 and result definitely do not depend on the variable set:
without variables run:  6924 6483 6509 6731
with the variable: 7001 6533 6283 6615

test suite is (with first 2 lines interchangeable of course)
{code:sql}
SET session_track_system_variables="""";
SET session_track_system_variables=""autocommit,character_set_client,character_set_connection,character_set_results,time_zone"";

--disable_query_log
--disable_result_log
let $1= 100000;
while ($1)
{
  SELECT 1;
  dec $1;
}
--enable_query_log
--enable_result_log
{code}

other extreme case when each execution leads to change:
the same number of SET names:
without variables run:  4739 4721 4744 4700 
with the variable: 5120 5340 4873 5124

in this extreme case difference is 7.5% (but I do not think that it is real scenario)

here is test suite for it:
{code:sql}
SET session_track_system_variables="""";
SET session_track_system_variables=""autocommit,character_set_client,character_set_connection,character_set_results,time_zone"";

--disable_query_log
--disable_result_log
let $1= 50000;
while ($1)
{
  SET names utf8;
  SET names latin1;
  dec $1;
}
--enable_query_log
--enable_result_log
{code}
",3,"For query which do not change something (which most of the query) I do not see difference:
I make 100000 SELECT 1 and result definitely do not depend on the variable set:
without variables run:  6924 6483 6509 6731
with the variable: 7001 6533 6283 6615

test suite is (with first 2 lines interchangeable of course)
{code:sql}
SET session_track_system_variables="""";
SET session_track_system_variables=""autocommit,character_set_client,character_set_connection,character_set_results,time_zone"";

--disable_query_log
--disable_result_log
let $1= 100000;
while ($1)
{
  SELECT 1;
  dec $1;
}
--enable_query_log
--enable_result_log
{code}

other extreme case when each execution leads to change:
the same number of SET names:
without variables run:  4739 4721 4744 4700 
with the variable: 5120 5340 4873 5124

in this extreme case difference is 7.5% (but I do not think that it is real scenario)

here is test suite for it:
{code:sql}
SET session_track_system_variables="""";
SET session_track_system_variables=""autocommit,character_set_client,character_set_connection,character_set_results,time_zone"";

--disable_query_log
--disable_result_log
let $1= 50000;
while ($1)
{
  SET names utf8;
  SET names latin1;
  dec $1;
}
--enable_query_log
--enable_result_log
{code}
"
1820,MDEV-11825,MDEV,Sergei Golubchik,93872,2017-04-06 15:00:55,"ok to push, then",4,"ok to push, then"
1821,MDEV-11880,MDEV,Michael Widenius,91227,2017-01-30 20:27:33,"Code reviewed. Looks fine, with a few small fixes needed",1,"Code reviewed. Looks fine, with a few small fixes needed"
1822,MDEV-11880,MDEV,Alexander Barkov,91326,2017-02-01 19:16:02,Pushed to bb-10.2-compatibilty,2,Pushed to bb-10.2-compatibilty
1823,MDEV-11934,MDEV,Sergei Petrunia,91180,2017-01-29 22:09:34,"Debugging MariaDB/InnoDB with 
sync_binlog=1, innodb_flush_logs_at_trx_commit=1 (default), 

== Prepare ==
- innobase_xa_prepare() does the prepare operation and flushes the logs 
  (note: it seems there is an innodb-internal group flushing there.. there's a comment about it but didn't find the code so far)

== innobase_commit_ordered ==
- Fetch the binlog position from the SQL layer 
- set trx->flush_log_later= true (that is, we don't flush here)
- trx_sys_update_mysql_binlog_offset ...
- Logical commit happened, but flushing the log didnt
- Sets trx->active_commit_ordered=1 to remember this
- Also sets trx->must_flush_log_later=true

== innobase_commit ==

- Check that we are just doing the final steps after innobase_commit_ordered.
- Call trx_commit_complete_for_mysql
  - call trx_flush_log_if_needed, which flushes the log

== innobase_checkpoint_request ==
- This is only called on binlog rotation (that is, rarely)
",1,"Debugging MariaDB/InnoDB with 
sync_binlog=1, innodb_flush_logs_at_trx_commit=1 (default), 

== Prepare ==
- innobase_xa_prepare() does the prepare operation and flushes the logs 
  (note: it seems there is an innodb-internal group flushing there.. there's a comment about it but didn't find the code so far)

== innobase_commit_ordered ==
- Fetch the binlog position from the SQL layer 
- set trx->flush_log_later= true (that is, we don't flush here)
- trx_sys_update_mysql_binlog_offset ...
- Logical commit happened, but flushing the log didnt
- Sets trx->active_commit_ordered=1 to remember this
- Also sets trx->must_flush_log_later=true

== innobase_commit ==

- Check that we are just doing the final steps after innobase_commit_ordered.
- Call trx_commit_complete_for_mysql
  - call trx_flush_log_if_needed, which flushes the log

== innobase_checkpoint_request ==
- This is only called on binlog rotation (that is, rarely)
"
1824,MDEV-11934,MDEV,Sergei Petrunia,91181,2017-01-29 22:35:11,"So, MariaRocks could work like this:

- prepare_ordered() is not implemented (same as in InnoDB)
- prepare() does trx->prepare(). It will have to sync to disk.
- commit_ordered() would commit without writing the WAL.
- commit() (when called after commit_ordered) would just flush the WAL
- rocksdb_checkpoint_request would flush the WAL


But will this be performant? Reading MariaDB's handler.h:
{noformat}
     The intention is that commit_ordered() should do the minimal amount of
     work that needs to happen in consistent commit order among handlers. To
     preserve ordering, calls need to be serialised on a global mutex, so
     doing any time-consuming or blocking operations in commit_ordered() will
     limit scalability.

     Handlers can rely on commit_ordered() calls to be serialised (no two
     calls can run in parallel, so no extra locking on the handler part is
     required to ensure this).
{noformat}

So, if we flush the WAL in rocksdb_commit(), then requests to flush the WAL from different threads will go in parallel and hopefully that will be grouped (although it looks a bit inefficient).

However, the commit itself (making the changes visible to other transactions, etc) will not be done in parallel.

",2,"So, MariaRocks could work like this:

- prepare_ordered() is not implemented (same as in InnoDB)
- prepare() does trx->prepare(). It will have to sync to disk.
- commit_ordered() would commit without writing the WAL.
- commit() (when called after commit_ordered) would just flush the WAL
- rocksdb_checkpoint_request would flush the WAL


But will this be performant? Reading MariaDB's handler.h:
{noformat}
     The intention is that commit_ordered() should do the minimal amount of
     work that needs to happen in consistent commit order among handlers. To
     preserve ordering, calls need to be serialised on a global mutex, so
     doing any time-consuming or blocking operations in commit_ordered() will
     limit scalability.

     Handlers can rely on commit_ordered() calls to be serialised (no two
     calls can run in parallel, so no extra locking on the handler part is
     required to ensure this).
{noformat}

So, if we flush the WAL in rocksdb_commit(), then requests to flush the WAL from different threads will go in parallel and hopefully that will be grouped (although it looks a bit inefficient).

However, the commit itself (making the changes visible to other transactions, etc) will not be done in parallel.

"
1825,MDEV-11934,MDEV,Sergei Petrunia,91182,2017-01-29 22:35:26,"Another thing to note:

{noformat}
     Note that commit_ordered() can be called from a different thread than the
     one handling the transaction! So it can not do anything that depends on
     thread local storage, in particular it can not call my_error() and
     friends
{noformat}

Does RocksDB API allow this?

EDIT: most likely yes, because one can find this in facebook/mysql-5.6: 

{noformat}
MYSQL_BIN_LOG::process_commit_stage_queue()
{
  ...
  Thread_excursion excursion(thd);
  for (THD *head= first ; head ; head = head->next_to_commit)
  {
    ...
    excursion.try_to_attach_to(head);
    ...
    if (ha_commit_low(head, all, async))
{noformat}

Looks like rocksdb_commit() can be invoked from a different thread.",3,"Another thing to note:

{noformat}
     Note that commit_ordered() can be called from a different thread than the
     one handling the transaction! So it can not do anything that depends on
     thread local storage, in particular it can not call my_error() and
     friends
{noformat}

Does RocksDB API allow this?

EDIT: most likely yes, because one can find this in facebook/mysql-5.6: 

{noformat}
MYSQL_BIN_LOG::process_commit_stage_queue()
{
  ...
  Thread_excursion excursion(thd);
  for (THD *head= first ; head ; head = head->next_to_commit)
  {
    ...
    excursion.try_to_attach_to(head);
    ...
    if (ha_commit_low(head, all, async))
{noformat}

Looks like rocksdb_commit() can be invoked from a different thread."
1826,MDEV-11934,MDEV,Sergei Petrunia,91263,2017-01-31 18:24:05,"An interesting related issue: https://github.com/facebook/mysql-5.6/issues/474 :

{quote}
 But in MyRocks, commit() phase is much more expensive than InnoDB's commit() phase, because writing to MemTable happens there. If you run pmp/quickstack, you may consistently find only one thread is doing myrocks::Rdb_transaction::commit() and other threads are waiting at MYSQL_BIN_LOG::change_stage(). And you'll find commit() is calling rocksdb::MemTableInserter::PutCF() and other MemTable related functions. Serialized MemTable write calls is pretty inefficient.
{quote}

So MariaDB has {{commit}} and {{commit_ordered}}. However,  RocksDB does not provide an API call that would be suitable for calling from {{rocksdb_commit_ordered}}.

* {{rocksdb_commit_ordered()}} would call something that would just put the {{COMMIT}} record into WAL (without flushing)
* Then, {{rocksdb_commit}} would apply the transaction's MemTable contents into the global MemTable.
* 
",4,"An interesting related issue: URL :

{quote}
 But in MyRocks, commit() phase is much more expensive than InnoDB's commit() phase, because writing to MemTable happens there. If you run pmp/quickstack, you may consistently find only one thread is doing myrocks::Rdb_transaction::commit() and other threads are waiting at MYSQL_BIN_LOG::change_stage(). And you'll find commit() is calling rocksdb::MemTableInserter::PutCF() and other MemTable related functions. Serialized MemTable write calls is pretty inefficient.
{quote}

So MariaDB has {{commit}} and {{commit_ordered}}. However,  RocksDB does not provide an API call that would be suitable for calling from {{rocksdb_commit_ordered}}.

* {{rocksdb_commit_ordered()}} would call something that would just put the {{COMMIT}} record into WAL (without flushing)
* Then, {{rocksdb_commit}} would apply the transaction's MemTable contents into the global MemTable.
* 
"
1827,MDEV-11934,MDEV,Sergei Petrunia,91440,2017-02-03 22:47:51,See also MDEV-11900 for the situation with 10.2 and InnoDB,5,See also MDEV-11900 for the situation with 10.2 and InnoDB
1828,MDEV-11934,MDEV,Sergei Petrunia,99352,2017-08-28 10:29:21,"Looking at how ""Prepare"" operation for the storage engine is persisted on disk.

In MariaDB, each Prepare operation *tries to flush the logs itself* :

{noformat}
  #0  log_write_up_to (lsn=1908254, flush_to_disk=true) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/log/log0log.cc:1234
  #1  0x00005555562f5c08 in trx_flush_log_if_needed_low (lsn=1908254) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:1600
  #2  0x00005555562f5c51 in trx_flush_log_if_needed (lsn=1908254, trx=0x7fffe64c1878) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:1622
  #3  0x00005555562f8fd2 in trx_prepare (trx=0x7fffe64c1878) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:2774
  #4  0x00005555562f9084 in trx_prepare_for_mysql (trx=0x7fffe64c1878) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:2796
  #5  0x000055555610cc78 in innobase_xa_prepare (hton=0x5555579371c0, thd=0x7fff78000b00, prepare_trx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/handler/ha_innodb.cc:17869
  #6  0x0000555555d7923c in prepare_or_error (ht=0x5555579371c0, thd=0x7fff78000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #7  0x0000555555d79adf in ha_commit_trans (thd=0x7fff78000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #8  0x0000555555c60990 in trans_commit_stmt (thd=0x7fff78000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #9  0x0000555555b0c3ef in mysql_execute_command (thd=0x7fff78000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #10 0x0000555555b10aa6 in mysql_parse (thd=0x7fff78000b00, rawbuf=0x7fff780111f8 ""insert into t20 values (2)"", length=26, parser_state=0x7ffff4463210, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
{noformat}

Concurrent Prepare operations can get grouped (one Prepare will make a flush for many),  this is achieved by relying on InnoDB's native group commit mechanism (which predates XA group commit):

See this comment in trx_prepare:
{noformat}
		The idea in InnoDB's group prepare is that a group of
		transactions gather behind a trx doing a physical disk write
		to log files, and when that physical write has been completed,
		one of those transactions does a write which prepares the whole
		group. Note that this group prepare will only bring benefit if
		there are > 2 users in the database. Then at least 2 users can
		gather behind one doing the physical log write to disk.
{noformat}

In fb/mysql-5.6, it works differently.

InnoDB's Prepare calls thd_requested_durability() which returns HA_IGNORE_DURABILITY, and flushing is not done:

{noformat}
(gdb) wher
  #0  thd_requested_durability (thd=0x2bb4cd0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/handler/ha_innodb.cc:1830
  #1  0x000000000150c35a in trx_prepare (trx=0x7fffe0d01e60, async=0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/trx/trx0trx.cc:2365
  #2  0x000000000150c421 in trx_prepare_for_mysql (trx=0x7fffe0d01e60, async=0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/trx/trx0trx.cc:2409
  #3  0x000000000139bea8 in innobase_xa_prepare (hton=0x2949ad0, thd=0x2bb4cd0, prepare_trx=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/handler/ha_innodb.cc:15170
  #4  0x0000000000d85aa5 in ha_prepare_low (thd=0x2bb4cd0, all=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:2264
  #5  0x0000000001237ee0 in MYSQL_BIN_LOG::prepare (this=0x2744000 <mysql_bin_log>, thd=0x2bb4cd0, all=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7054
  #6  0x0000000000d83c9c in ha_commit_trans (thd=0x2bb4cd0, all=false, async=false, ignore_global_read_lock=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:1540
  #7  0x000000000106f0a3 in trans_commit_stmt (thd=0x2bb4cd0, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/transaction.cc:452
  #8  0x0000000000f8cf3f in mysql_execute_command (thd=0x2bb4cd0, statement_start_time=0x7ffff430aaa8, post_parse=0x7ffff430ac00) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:6254
  #9  0x0000000000f90913 in mysql_parse (thd=0x2bb4cd0, rawbuf=0x7fffa4005680 ""insert into t21 values (4)"", length=26, parser_state=0x7ffff430b4b0, last_timer=0x7ffff430ac00, async_commit=0x7ffff430abcc """") at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:7770

(gdb) fini
  Run till exit from #0  thd_requested_durability (thd=0x2bb4cd0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/handler/ha_innodb.cc:1830
  0x000000000150c35a in trx_prepare (trx=0x7fffe0d01e60, async=0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/trx/trx0trx.cc:2365
  Value returned is $17 = HA_IGNORE_DURABILITY
{noformat}

and then the SQL layer makes a special call to make the effects of all Prepare transactions persistent:

{noformat}
  Breakpoint 3, ha_flush_logs (db_type=0x0, engine_map=0x7fffa40086b0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:2452
(gdb) wher
  #0  ha_flush_logs (db_type=0x0, engine_map=0x7fffa40086b0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:2452
  #1  0x0000000001238999 in MYSQL_BIN_LOG::process_flush_stage_queue (this=0x2744000 <mysql_bin_log>, total_bytes_var=0x7ffff4308468, rotate_var=0x7ffff430844f, out_queue_var=0x7ffff4308470, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7322
  #2  0x0000000001239b85 in MYSQL_BIN_LOG::ordered_commit (this=0x2744000 <mysql_bin_log>, thd=0x2bb4cd0, all=false, skip_commit=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7891
  #3  0x00000000012385fd in MYSQL_BIN_LOG::commit (this=0x2744000 <mysql_bin_log>, thd=0x2bb4cd0, all=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7223
  #4  0x0000000000d83cda in ha_commit_trans (thd=0x2bb4cd0, all=false, async=false, ignore_global_read_lock=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:1542
  #5  0x000000000106f0a3 in trans_commit_stmt (thd=0x2bb4cd0, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/transaction.cc:452
  #6  0x0000000000f8cf3f in mysql_execute_command (thd=0x2bb4cd0, statement_start_time=0x7ffff430aaa8, post_parse=0x7ffff430ac00) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:6254
  #7  0x0000000000f90913 in mysql_parse (thd=0x2bb4cd0, rawbuf=0x7fffa4005680 ""insert into t21 values (4)"", length=26, parser_state=0x7ffff430b4b0, last_timer=0x7ffff430ac00, async_commit=0x7ffff430abcc """") at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:7770
{noformat}
",6,"Looking at how ""Prepare"" operation for the storage engine is persisted on disk.

In MariaDB, each Prepare operation *tries to flush the logs itself* :

{noformat}
  #0  log_write_up_to (lsn=1908254, flush_to_disk=true) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/log/log0log.cc:1234
  #1  0x00005555562f5c08 in trx_flush_log_if_needed_low (lsn=1908254) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:1600
  #2  0x00005555562f5c51 in trx_flush_log_if_needed (lsn=1908254, trx=0x7fffe64c1878) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:1622
  #3  0x00005555562f8fd2 in trx_prepare (trx=0x7fffe64c1878) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:2774
  #4  0x00005555562f9084 in trx_prepare_for_mysql (trx=0x7fffe64c1878) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/trx/trx0trx.cc:2796
  #5  0x000055555610cc78 in innobase_xa_prepare (hton=0x5555579371c0, thd=0x7fff78000b00, prepare_trx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/innobase/handler/ha_innodb.cc:17869
  #6  0x0000555555d7923c in prepare_or_error (ht=0x5555579371c0, thd=0x7fff78000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #7  0x0000555555d79adf in ha_commit_trans (thd=0x7fff78000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #8  0x0000555555c60990 in trans_commit_stmt (thd=0x7fff78000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #9  0x0000555555b0c3ef in mysql_execute_command (thd=0x7fff78000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #10 0x0000555555b10aa6 in mysql_parse (thd=0x7fff78000b00, rawbuf=0x7fff780111f8 ""insert into t20 values (2)"", length=26, parser_state=0x7ffff4463210, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
{noformat}

Concurrent Prepare operations can get grouped (one Prepare will make a flush for many),  this is achieved by relying on InnoDB's native group commit mechanism (which predates XA group commit):

See this comment in trx_prepare:
{noformat}
		The idea in InnoDB's group prepare is that a group of
		transactions gather behind a trx doing a physical disk write
		to log files, and when that physical write has been completed,
		one of those transactions does a write which prepares the whole
		group. Note that this group prepare will only bring benefit if
		there are > 2 users in the database. Then at least 2 users can
		gather behind one doing the physical log write to disk.
{noformat}

In fb/mysql-5.6, it works differently.

InnoDB's Prepare calls thd_requested_durability() which returns HA_IGNORE_DURABILITY, and flushing is not done:

{noformat}
(gdb) wher
  #0  thd_requested_durability (thd=0x2bb4cd0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/handler/ha_innodb.cc:1830
  #1  0x000000000150c35a in trx_prepare (trx=0x7fffe0d01e60, async=0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/trx/trx0trx.cc:2365
  #2  0x000000000150c421 in trx_prepare_for_mysql (trx=0x7fffe0d01e60, async=0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/trx/trx0trx.cc:2409
  #3  0x000000000139bea8 in innobase_xa_prepare (hton=0x2949ad0, thd=0x2bb4cd0, prepare_trx=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/handler/ha_innodb.cc:15170
  #4  0x0000000000d85aa5 in ha_prepare_low (thd=0x2bb4cd0, all=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:2264
  #5  0x0000000001237ee0 in MYSQL_BIN_LOG::prepare (this=0x2744000 , thd=0x2bb4cd0, all=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7054
  #6  0x0000000000d83c9c in ha_commit_trans (thd=0x2bb4cd0, all=false, async=false, ignore_global_read_lock=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:1540
  #7  0x000000000106f0a3 in trans_commit_stmt (thd=0x2bb4cd0, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/transaction.cc:452
  #8  0x0000000000f8cf3f in mysql_execute_command (thd=0x2bb4cd0, statement_start_time=0x7ffff430aaa8, post_parse=0x7ffff430ac00) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:6254
  #9  0x0000000000f90913 in mysql_parse (thd=0x2bb4cd0, rawbuf=0x7fffa4005680 ""insert into t21 values (4)"", length=26, parser_state=0x7ffff430b4b0, last_timer=0x7ffff430ac00, async_commit=0x7ffff430abcc """") at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:7770

(gdb) fini
  Run till exit from #0  thd_requested_durability (thd=0x2bb4cd0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/handler/ha_innodb.cc:1830
  0x000000000150c35a in trx_prepare (trx=0x7fffe0d01e60, async=0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/storage/innobase/trx/trx0trx.cc:2365
  Value returned is $17 = HA_IGNORE_DURABILITY
{noformat}

and then the SQL layer makes a special call to make the effects of all Prepare transactions persistent:

{noformat}
  Breakpoint 3, ha_flush_logs (db_type=0x0, engine_map=0x7fffa40086b0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:2452
(gdb) wher
  #0  ha_flush_logs (db_type=0x0, engine_map=0x7fffa40086b0) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:2452
  #1  0x0000000001238999 in MYSQL_BIN_LOG::process_flush_stage_queue (this=0x2744000 , total_bytes_var=0x7ffff4308468, rotate_var=0x7ffff430844f, out_queue_var=0x7ffff4308470, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7322
  #2  0x0000000001239b85 in MYSQL_BIN_LOG::ordered_commit (this=0x2744000 , thd=0x2bb4cd0, all=false, skip_commit=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7891
  #3  0x00000000012385fd in MYSQL_BIN_LOG::commit (this=0x2744000 , thd=0x2bb4cd0, all=false, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/binlog.cc:7223
  #4  0x0000000000d83cda in ha_commit_trans (thd=0x2bb4cd0, all=false, async=false, ignore_global_read_lock=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/handler.cc:1542
  #5  0x000000000106f0a3 in trans_commit_stmt (thd=0x2bb4cd0, async=false) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/transaction.cc:452
  #6  0x0000000000f8cf3f in mysql_execute_command (thd=0x2bb4cd0, statement_start_time=0x7ffff430aaa8, post_parse=0x7ffff430ac00) at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:6254
  #7  0x0000000000f90913 in mysql_parse (thd=0x2bb4cd0, rawbuf=0x7fffa4005680 ""insert into t21 values (4)"", length=26, parser_state=0x7ffff430b4b0, last_timer=0x7ffff430ac00, async_commit=0x7ffff430abcc """") at /home/psergey/dev-git/mysql-5.6-rocksdb-look400/sql/sql_parse.cc:7770
{noformat}
"
1829,MDEV-11934,MDEV,Sergei Petrunia,99518,2017-08-31 13:00:33,"Created a simple testcase and tried it on both InnoDB and MyRocks.  InnoDB results were:

h2. InnoDB
{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, rotating disk
Innodb_data_fsyncs= 540
Binlog_commits= 10000
Binlog_group_commits= 2223
{noformat}

{noformat}
# This is  --concurrency=50 --number-of-queries=10000, --mem
Innodb_data_fsyncs = 9912
Binlog_commits = 10000
Binlog_group_commits= 9851
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, aws, ssd
Innodb_data_fsyncs 590
Binlog_commits 10000
Binlog_group_commits 4559
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, aws, memory
Innodb_data_fsyncs 9075  (the value is around 9K)
Binlog_commits 10000
Binlog_group_commits 2517 (the value is 2K -4K)
{noformat}

* I am not sure why running with {{--mem}} makes {{Binlog_group_commits}} be ~2K  on aws host and ~9.8K on my desktop. (Both numbers are stable and do not fluctuate).
* There's more grouping on rotating disk than on SSD, makes sense.",7,"Created a simple testcase and tried it on both InnoDB and MyRocks.  InnoDB results were:

h2. InnoDB
{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, rotating disk
Innodb_data_fsyncs= 540
Binlog_commits= 10000
Binlog_group_commits= 2223
{noformat}

{noformat}
# This is  --concurrency=50 --number-of-queries=10000, --mem
Innodb_data_fsyncs = 9912
Binlog_commits = 10000
Binlog_group_commits= 9851
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, aws, ssd
Innodb_data_fsyncs 590
Binlog_commits 10000
Binlog_group_commits 4559
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, aws, memory
Innodb_data_fsyncs 9075  (the value is around 9K)
Binlog_commits 10000
Binlog_group_commits 2517 (the value is 2K -4K)
{noformat}

* I am not sure why running with {{--mem}} makes {{Binlog_group_commits}} be ~2K  on aws host and ~9.8K on my desktop. (Both numbers are stable and do not fluctuate).
* There's more grouping on rotating disk than on SSD, makes sense."
1830,MDEV-11934,MDEV,Sergei Petrunia,99519,2017-08-31 13:12:30,"Trying the same with MyRocks and experimental group commit patch :

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, home, --mem
ROCKSDB_WAL_GROUP_SYNCS 0
ROCKSDB_WAL_SYNCED 8247  (aws, --mem: 7K)
Binlog_commits 10000
Binlog_group_commits 726  (aws, --mem: 800)
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, home, rotating disk
ROCKSDB_WAL_GROUP_SYNCS 0
ROCKSDB_WAL_SYNCED 8540
Binlog_commits 10000
Binlog_group_commits 652
# This takes ~300 seconds.
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, aws, ssd
ROCKSDB_WAL_GROUP_SYNCS 0
ROCKSDB_WAL_SYNCED 6994
Binlog_commits 10000
Binlog_group_commits 659
{noformat}

* ROCKSDB_WAL_GROUP_SYNCS=0 is expected.  Look in the previous comments for "" the SQL layer makes a special call to make the effects of all Prepare transactions persistent"" and how that call is lacking in MariaDB.
* The values of Binlog_group_commits show there's some amount of grouping
* ROCKSDB_WAL_SYNCED is too high for both on-disk workloads. 
** Are these real sync operations or just requests to sync the WAL?  (That is, if two Transaction::Prepare() calls issue requests to sync the WAL, and one of them happens to sync it for both, will ROCKSDB_WAL_SYNCED be incremented by 1 or by 2?)
* Execution time for rotating disk is too high?",8,"Trying the same with MyRocks and experimental group commit patch :

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, home, --mem
ROCKSDB_WAL_GROUP_SYNCS 0
ROCKSDB_WAL_SYNCED 8247  (aws, --mem: 7K)
Binlog_commits 10000
Binlog_group_commits 726  (aws, --mem: 800)
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, home, rotating disk
ROCKSDB_WAL_GROUP_SYNCS 0
ROCKSDB_WAL_SYNCED 8540
Binlog_commits 10000
Binlog_group_commits 652
# This takes ~300 seconds.
{noformat}

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000, aws, ssd
ROCKSDB_WAL_GROUP_SYNCS 0
ROCKSDB_WAL_SYNCED 6994
Binlog_commits 10000
Binlog_group_commits 659
{noformat}

* ROCKSDB_WAL_GROUP_SYNCS=0 is expected.  Look in the previous comments for "" the SQL layer makes a special call to make the effects of all Prepare transactions persistent"" and how that call is lacking in MariaDB.
* The values of Binlog_group_commits show there's some amount of grouping
* ROCKSDB_WAL_SYNCED is too high for both on-disk workloads. 
** Are these real sync operations or just requests to sync the WAL?  (That is, if two Transaction::Prepare() calls issue requests to sync the WAL, and one of them happens to sync it for both, will ROCKSDB_WAL_SYNCED be incremented by 1 or by 2?)
* Execution time for rotating disk is too high?"
1831,MDEV-11934,MDEV,Sergei Petrunia,99520,2017-08-31 13:15:11, [^_b.test.innodb]  [^_b.test.myrocks] ,9, [^_b.test.innodb]  [^_b.test.myrocks] 
1832,MDEV-11934,MDEV,Sergei Petrunia,99521,2017-08-31 13:37:40,Probably related: MDEV-11937,10,Probably related: MDEV-11937
1833,MDEV-11934,MDEV,Sergei Petrunia,99551,2017-09-01 00:57:29,"_c.test is a testcase that
- runs a serial workload 
- runs a parallel workload

the workload is run with {$engine}_flush_log_at_trx_commit=1, log_bin=0.
the results do make sense:

aws ssd:
{noformat}
For --concurrency=1 --number-of-queries=10000
Innodb_os_log_fsyncs=10K
time: 12 sec (1K syncs/sec)

For --concurrency=50 --number-of-queries=100000
Innodb_os_log_fsyncs=4735
time: 7 sec (676 syncs/sec.)

For --concurrency=1 --number-of-queries=10000
ROCKSDB_WAL_SYNCED: 10000
time: 25 sec (400 syncs/sec)

For --concurrency=50 --number-of-queries=100000
ROCKSDB_WAL_SYNCED: 3952
time: 9 sec (439 syncs/sec)
{noformat}

rotating desktop hdd:
{noformat}
For --concurrency=1 --number-of-queries=1000
Innodb_os_log_fsyncs: 1005
time: 30 sec  (33 syncs/sec)

For --concurrency=50 --number-of-queries=10000
Innodb_os_log_fsyncs: 450
time: 16 sec (28 syncs/sec).

For --concurrency=1 --number-of-queries=1000 
ROCKSDB_WAL_SYNCED: 1000
time: 36 sec (27.7 syncs /sec)

For --concurrency=50 --number-of-queries=10000
ROCKSDB_WAL_SYNCED: 354
time: 13 sec (27.2 syncs/ sec)
{noformat}

The numbers are fairly stable and consistent across the InnoDB and MyRocks.",11,"_c.test is a testcase that
- runs a serial workload 
- runs a parallel workload

the workload is run with {$engine}_flush_log_at_trx_commit=1, log_bin=0.
the results do make sense:

aws ssd:
{noformat}
For --concurrency=1 --number-of-queries=10000
Innodb_os_log_fsyncs=10K
time: 12 sec (1K syncs/sec)

For --concurrency=50 --number-of-queries=100000
Innodb_os_log_fsyncs=4735
time: 7 sec (676 syncs/sec.)

For --concurrency=1 --number-of-queries=10000
ROCKSDB_WAL_SYNCED: 10000
time: 25 sec (400 syncs/sec)

For --concurrency=50 --number-of-queries=100000
ROCKSDB_WAL_SYNCED: 3952
time: 9 sec (439 syncs/sec)
{noformat}

rotating desktop hdd:
{noformat}
For --concurrency=1 --number-of-queries=1000
Innodb_os_log_fsyncs: 1005
time: 30 sec  (33 syncs/sec)

For --concurrency=50 --number-of-queries=10000
Innodb_os_log_fsyncs: 450
time: 16 sec (28 syncs/sec).

For --concurrency=1 --number-of-queries=1000 
ROCKSDB_WAL_SYNCED: 1000
time: 36 sec (27.7 syncs /sec)

For --concurrency=50 --number-of-queries=10000
ROCKSDB_WAL_SYNCED: 354
time: 13 sec (27.2 syncs/ sec)
{noformat}

The numbers are fairly stable and consistent across the InnoDB and MyRocks."
1834,MDEV-11934,MDEV,Sergei Petrunia,99552,2017-09-01 02:13:54,"Now, trying InnoDB with binlog enabled

Rotating hdd, log_bin=1, sync_binlog=1, innodb_flush_logs_at_trx_commit=1
{noformat}
For--concurrency=1 --number-of-queries=1000
Innodb_os_log_fsyncs 1006
Binlog_commits 1000
Binlog_group_commits 1000
time: 30 sec  
{noformat}
# Both InnoDB and binlog manage to do 30 commits/sec.
# The interesting part is that they manage to do this together.

{noformat}
For --concurrency=50 --number-of-queries=10000
Innodb_os_log_fsyncs 473
Binlog_commits 10000
Binlog_group_commits 2246
time: 16 sec
{noformat}
# Binlog commit numbers do not look realistic: 2246/16=140 commits/sec ?
# InnoDB's fsyncs agree with previous observations: 473/16 = 29.6 commits/sec

AWS ssd, log_bin=1, sync_binlog=1, innodb_flush_logs_at_trx_commit=1
{noformat}
# THIS IS: --concurrency=1 --number-of-queries=10000
Innodb_os_log_fsyncs 10008
Binlog_commits 10000
Binlog_group_commits 10000
time: 14 sec
{noformat}
# Both InnoDB and binlog do 714 commits/sec.

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=100000
Innodb_os_log_fsyncs 4992
Binlog_commits 100000
Binlog_group_commits 47449
time: 7 sec.
{noformat}
# InnoDB does 713 commits/sec (agrees with previous observations)
# Binlog does 6778 commits/sec  (looks too high also?)
",12,"Now, trying InnoDB with binlog enabled

Rotating hdd, log_bin=1, sync_binlog=1, innodb_flush_logs_at_trx_commit=1
{noformat}
For--concurrency=1 --number-of-queries=1000
Innodb_os_log_fsyncs 1006
Binlog_commits 1000
Binlog_group_commits 1000
time: 30 sec  
{noformat}
# Both InnoDB and binlog manage to do 30 commits/sec.
# The interesting part is that they manage to do this together.

{noformat}
For --concurrency=50 --number-of-queries=10000
Innodb_os_log_fsyncs 473
Binlog_commits 10000
Binlog_group_commits 2246
time: 16 sec
{noformat}
# Binlog commit numbers do not look realistic: 2246/16=140 commits/sec ?
# InnoDB's fsyncs agree with previous observations: 473/16 = 29.6 commits/sec

AWS ssd, log_bin=1, sync_binlog=1, innodb_flush_logs_at_trx_commit=1
{noformat}
# THIS IS: --concurrency=1 --number-of-queries=10000
Innodb_os_log_fsyncs 10008
Binlog_commits 10000
Binlog_group_commits 10000
time: 14 sec
{noformat}
# Both InnoDB and binlog do 714 commits/sec.

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=100000
Innodb_os_log_fsyncs 4992
Binlog_commits 100000
Binlog_group_commits 47449
time: 7 sec.
{noformat}
# InnoDB does 713 commits/sec (agrees with previous observations)
# Binlog does 6778 commits/sec  (looks too high also?)
"
1835,MDEV-11934,MDEV,Sergei Petrunia,99574,2017-09-01 13:37:20,"Trying the current MyRocks+MariaDB group commit patch

Rotating hdd
{noformat}
# THIS IS: --concurrency=1 --number-of-queries=1000
ROCKSDB_WAL_SYNCED 1000
Binlog_commits  1000
time: 40 sec
{noformat}
25 syncs/sec, slightly lower than without the binlog

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000
ROCKSDB_WAL_SYNCED 7647
Binlog_commits 10000
Binlog_group_commits 651
time: 293 sec
{noformat}
* This is very slow
* RocksDB commit rate was 7647/293.=26 rocksdb commits/sec. which looks like no commit grouping.

aws ssd
{noformat}
# THIS IS: --concurrency=1 --number-of-queries=10000
ROCKSDB_WAL_SYNCED 10000
Binlog_commits 10000
Binlog_group_commits 10000
time: 25 sec
{noformat}
this gives 400 commits/sec for both binlog and RocksDB. this matches previous numbers we've got for this SSD

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=100000
ROCKSDB_WAL_SYNCED 65737
Binlog_commits 100000
Binlog_group_commits 6535
time: 153 sec
{noformat}
this is 429 WAL syncs/sec. Again looks as if WAL sync operations were not grouped at all? ",13,"Trying the current MyRocks+MariaDB group commit patch

Rotating hdd
{noformat}
# THIS IS: --concurrency=1 --number-of-queries=1000
ROCKSDB_WAL_SYNCED 1000
Binlog_commits  1000
time: 40 sec
{noformat}
25 syncs/sec, slightly lower than without the binlog

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=10000
ROCKSDB_WAL_SYNCED 7647
Binlog_commits 10000
Binlog_group_commits 651
time: 293 sec
{noformat}
* This is very slow
* RocksDB commit rate was 7647/293.=26 rocksdb commits/sec. which looks like no commit grouping.

aws ssd
{noformat}
# THIS IS: --concurrency=1 --number-of-queries=10000
ROCKSDB_WAL_SYNCED 10000
Binlog_commits 10000
Binlog_group_commits 10000
time: 25 sec
{noformat}
this gives 400 commits/sec for both binlog and RocksDB. this matches previous numbers we've got for this SSD

{noformat}
# THIS IS: --concurrency=50 --number-of-queries=100000
ROCKSDB_WAL_SYNCED 65737
Binlog_commits 100000
Binlog_group_commits 6535
time: 153 sec
{noformat}
this is 429 WAL syncs/sec. Again looks as if WAL sync operations were not grouped at all? "
1836,MDEV-11934,MDEV,Sergei Petrunia,99608,2017-09-02 12:05:06,"Trying to figure out
# if indeed Transaction::Prepare operations do not use RocksDB's internal Group Commit for some reason
# If the above is true, why.

* Found DBOptions::concurrent_prepare parameter. It is off (by default)
** perphaps, I should have it ON (update: this doesn't seem to be true)
** attempt to switch it ON caused mysqld to fail an assertion on start

* Debugged (with concurrent_prepare=off) to the point where PREPARE command
does sync:

{noformat}
(gdb) wher
  #0  rocksdb::PosixWritableFile::Sync (this=0x555557bbef20) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/env/io_posix.cc:849
  #1  0x00007ffff4aab9f7 in rocksdb::WritableFileWriter::SyncInternal (this=0x555557bb81b0, use_fsync=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/util/file_reader_writer.cc:352
  #2  0x00007ffff4aab568 in rocksdb::WritableFileWriter::Sync (this=0x555557bb81b0, use_fsync=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/util/file_reader_writer.cc:323
  #3  0x00007ffff48db9fd in rocksdb::DBImpl::WriteToWAL (this=0x555557baee50, write_group=..., log_writer=0x555557bbed90, log_used=0x7fffb4025f58, need_log_sync=true, need_log_dir_sync=false, sequence=10) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:719
  #4  0x00007ffff48d8eb7 in rocksdb::DBImpl::WriteImpl (this=0x555557baee50, write_options=..., my_batch=0x7fffb4026140, callback=0x0, log_used=0x7fffb4025f58, log_ref=0, disable_memtable=true) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:234
  #5  0x00007ffff4b0a6ba in rocksdb::TransactionImpl::Prepare (this=0x7fffb4025f50) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/utilities/transactions/transaction_impl.cc:195
  #6  0x00007ffff47ec27a in myrocks::Rdb_transaction_impl::prepare (this=0x7fffb4024940, name=""\000\000\000\000\000\000\000\001\030\000MySQLXid\f\000\000\000\000\000\000\000\004\000\000\000\000\000\000"") at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2129
  #7  0x00007ffff47c1030 in myrocks::rocksdb_prepare (hton=0x555557b96e70, thd=0x7fffb4000b00, prepare_tx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2733
  #8  0x0000555555d70582 in prepare_or_error (ht=0x555557b96e70, thd=0x7fffb4000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #9  0x0000555555d70e2d in ha_commit_trans (thd=0x7fffb4000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #10 0x0000555555c5d5c7 in trans_commit_stmt (thd=0x7fffb4000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #11 0x0000555555b09c5b in mysql_execute_command (thd=0x7fffb4000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #12 0x0000555555b0e332 in mysql_parse (thd=0x7fffb4000b00, rawbuf=0x7fffb40110f8 ""insert into t32 values (101)"", length=28, parser_state=0x7fffe6bd7250, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
{noformat}

DBImpl::WriteImpl has some grouping logic but I am not sure what grouping applies to:
- just putting new data into the MemTable
- or syncing the WAL also? ",14,"Trying to figure out
# if indeed Transaction::Prepare operations do not use RocksDB's internal Group Commit for some reason
# If the above is true, why.

* Found DBOptions::concurrent_prepare parameter. It is off (by default)
** perphaps, I should have it ON (update: this doesn't seem to be true)
** attempt to switch it ON caused mysqld to fail an assertion on start

* Debugged (with concurrent_prepare=off) to the point where PREPARE command
does sync:

{noformat}
(gdb) wher
  #0  rocksdb::PosixWritableFile::Sync (this=0x555557bbef20) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/env/io_posix.cc:849
  #1  0x00007ffff4aab9f7 in rocksdb::WritableFileWriter::SyncInternal (this=0x555557bb81b0, use_fsync=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/util/file_reader_writer.cc:352
  #2  0x00007ffff4aab568 in rocksdb::WritableFileWriter::Sync (this=0x555557bb81b0, use_fsync=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/util/file_reader_writer.cc:323
  #3  0x00007ffff48db9fd in rocksdb::DBImpl::WriteToWAL (this=0x555557baee50, write_group=..., log_writer=0x555557bbed90, log_used=0x7fffb4025f58, need_log_sync=true, need_log_dir_sync=false, sequence=10) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:719
  #4  0x00007ffff48d8eb7 in rocksdb::DBImpl::WriteImpl (this=0x555557baee50, write_options=..., my_batch=0x7fffb4026140, callback=0x0, log_used=0x7fffb4025f58, log_ref=0, disable_memtable=true) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:234
  #5  0x00007ffff4b0a6ba in rocksdb::TransactionImpl::Prepare (this=0x7fffb4025f50) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/utilities/transactions/transaction_impl.cc:195
  #6  0x00007ffff47ec27a in myrocks::Rdb_transaction_impl::prepare (this=0x7fffb4024940, name=""\000\000\000\000\000\000\000\001\030\000MySQLXid\f\000\000\000\000\000\000\000\004\000\000\000\000\000\000"") at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2129
  #7  0x00007ffff47c1030 in myrocks::rocksdb_prepare (hton=0x555557b96e70, thd=0x7fffb4000b00, prepare_tx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2733
  #8  0x0000555555d70582 in prepare_or_error (ht=0x555557b96e70, thd=0x7fffb4000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #9  0x0000555555d70e2d in ha_commit_trans (thd=0x7fffb4000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #10 0x0000555555c5d5c7 in trans_commit_stmt (thd=0x7fffb4000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #11 0x0000555555b09c5b in mysql_execute_command (thd=0x7fffb4000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #12 0x0000555555b0e332 in mysql_parse (thd=0x7fffb4000b00, rawbuf=0x7fffb40110f8 ""insert into t32 values (101)"", length=28, parser_state=0x7fffe6bd7250, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
{noformat}

DBImpl::WriteImpl has some grouping logic but I am not sure what grouping applies to:
- just putting new data into the MemTable
- or syncing the WAL also? "
1837,MDEV-11934,MDEV,Sergei Petrunia,99609,2017-09-02 12:38:30,"Re {{concurrent_prepare}}: the assertion I am hitting is this one in DBImpl::WriteImpl
{code}
      assert(!need_log_sync && !need_log_dir_sync);
{code}
I have
{code} 
need_log_sync= true
need_log_dir_sync=true
{code}

these come from
{code}
  bool need_log_sync = !write_options.disableWAL && write_options.sync;
  bool need_log_dir_sync = need_log_sync && !log_dir_synced_;
{code}

It seems unlikely that MyRocks should operate with need_log_sync=off.  ",15,"Re {{concurrent_prepare}}: the assertion I am hitting is this one in DBImpl::WriteImpl
{code}
      assert(!need_log_sync && !need_log_dir_sync);
{code}
I have
{code} 
need_log_sync= true
need_log_dir_sync=true
{code}

these come from
{code}
  bool need_log_sync = !write_options.disableWAL && write_options.sync;
  bool need_log_dir_sync = need_log_sync && !log_dir_synced_;
{code}

It seems unlikely that MyRocks should operate with need_log_sync=off.  "
1838,MDEV-11934,MDEV,Sergei Petrunia,99673,2017-09-05 12:38:02," [^test-rocksdb-gcommit.tgz] Wrote a test program that uses RocksDB to perform:
* concurrent writes with commits
* concurrent ""Prepare( sync=true|false);  commit (sync=true|false)""

AWS ssd:

{noformat}
  sync_prepare=false
  sync_commit=true
  using_2pc=false
TH      trx     n_threads       time    tps
TR      1000    2       4.513832        443.082538
TR      1000    4       4.754442        841.318433
TR      1000    8       4.881946        1638.690906
TR      1000    16      5.181725        3087.774675
TR      1000    32      5.734200        5580.551382
TR      1000    64      5.817391        11001.495775
TR      1000    128     8.551488        14968.155234
TR      1000    256     12.689465       20174.215468
TR      1000    512     22.345159       22913.239996
{noformat}

{noformat}
  sync_prepare=true
  sync_commit=false
  using_2pc=true
TH      trx     n_threads       time    tps
TR      1000    1       2.109577        474.028568
TR      1000    2       4.606521        434.167139
TR      1000    4       5.010855        798.266883
TR      1000    8       6.948192        1151.378641
TR      1000    16      7.309022        2189.075393
TR      1000    32      8.835688        3621.676118
TR      1000    64      10.030643       6380.448559
TR      1000    128     12.595573       10162.300453
TR      1000    256     18.653547       13723.931357
TR      1000    512     29.878734       17135.933763
{noformat}

{noformat}
Rotating HDD:
  sync_prepare=true
  sync_commit=false
  using_2pc=true
TH      trx     n_threads       time    tps
TR      1000    1       31.522334       31.723539
TR      1000    2       63.834870       31.330839
TR      1000    4       76.781369       52.095972
TR      1000    8       110.673415      72.284749
TR      1000    16      116.457815      137.388804
TR      1000    32      98.671802       324.307446
TR      1000    64      125.448014      510.171488
TR      1000    128     123.296987      1038.143781
TR      1000    256     123.317320      2075.945219
TR      1000    512     155.731128      3287.717785
{noformat}

So, RocksDB itself seems to be perfectly capable of providing group commit for Prepare(sync=true)/Commit(sync=false) transactions.",16," [^test-rocksdb-gcommit.tgz] Wrote a test program that uses RocksDB to perform:
* concurrent writes with commits
* concurrent ""Prepare( sync=true|false);  commit (sync=true|false)""

AWS ssd:

{noformat}
  sync_prepare=false
  sync_commit=true
  using_2pc=false
TH      trx     n_threads       time    tps
TR      1000    2       4.513832        443.082538
TR      1000    4       4.754442        841.318433
TR      1000    8       4.881946        1638.690906
TR      1000    16      5.181725        3087.774675
TR      1000    32      5.734200        5580.551382
TR      1000    64      5.817391        11001.495775
TR      1000    128     8.551488        14968.155234
TR      1000    256     12.689465       20174.215468
TR      1000    512     22.345159       22913.239996
{noformat}

{noformat}
  sync_prepare=true
  sync_commit=false
  using_2pc=true
TH      trx     n_threads       time    tps
TR      1000    1       2.109577        474.028568
TR      1000    2       4.606521        434.167139
TR      1000    4       5.010855        798.266883
TR      1000    8       6.948192        1151.378641
TR      1000    16      7.309022        2189.075393
TR      1000    32      8.835688        3621.676118
TR      1000    64      10.030643       6380.448559
TR      1000    128     12.595573       10162.300453
TR      1000    256     18.653547       13723.931357
TR      1000    512     29.878734       17135.933763
{noformat}

{noformat}
Rotating HDD:
  sync_prepare=true
  sync_commit=false
  using_2pc=true
TH      trx     n_threads       time    tps
TR      1000    1       31.522334       31.723539
TR      1000    2       63.834870       31.330839
TR      1000    4       76.781369       52.095972
TR      1000    8       110.673415      72.284749
TR      1000    16      116.457815      137.388804
TR      1000    32      98.671802       324.307446
TR      1000    64      125.448014      510.171488
TR      1000    128     123.296987      1038.143781
TR      1000    256     123.317320      2075.945219
TR      1000    512     155.731128      3287.717785
{noformat}

So, RocksDB itself seems to be perfectly capable of providing group commit for Prepare(sync=true)/Commit(sync=false) transactions."
1839,MDEV-11934,MDEV,Sergei Petrunia,99701,2017-09-05 21:10:43,"Running the same parallel workload on fb-mysql and MariaDB, with and without binlog. Noting counter values:

fb/mysql,  no-binlog vs binlog:

{noformat}
@@ -40,8 +36,8 @@
-| rocksdb_memtable_total                | 44704   |
-| rocksdb_memtable_unflushed            | 44704   |
+| rocksdb_memtable_total                | 95096   |
+| rocksdb_memtable_unflushed            | 95096   |
@@ -59,7 +55,7 @@
-| rocksdb_bytes_written                 | 27561   |
+| rocksdb_bytes_written                 | 210849  |
@@ -79,7 +75,7 @@
-| rocksdb_number_keys_written           | 1005    |
+| rocksdb_number_keys_written           | 2005    |
@@ -97,13 +93,13 @@
-| rocksdb_wal_bytes                     | 27561   |
-| rocksdb_wal_group_syncs               | 0       |
+| rocksdb_wal_bytes                     | 149849  |
+| rocksdb_wal_group_syncs               | 200     |
 | rocksdb_wal_synced                    | 204     |
-| rocksdb_write_other                   | 800     |
-| rocksdb_write_self                    | 204     |
+| rocksdb_write_other                   | 26      |
+| rocksdb_write_self                    | 1978    |
 | rocksdb_write_timedout                | 0       |
-| rocksdb_write_wal                     | 2008    |
+| rocksdb_write_wal                     | 4008    |
{noformat}

MariaDB-gcommit-patch, no-binlog vs binlog:
{noformat}
@@ -28,8 +37,8 @@
-| Rocksdb_memtable_total                | 44584  |
-| Rocksdb_memtable_unflushed            | 44584  |
+| Rocksdb_memtable_total                | 95280  |
+| Rocksdb_memtable_unflushed            | 95280  |
@@ -47,7 +56,7 @@
-| Rocksdb_bytes_written                 | 27477  |
+| Rocksdb_bytes_written                 | 206333 |
@@ -65,7 +74,7 @@
-| Rocksdb_number_keys_written           | 1005   |
+| Rocksdb_number_keys_written           | 2005   |
@@ -83,12 +92,13 @@
-| Rocksdb_wal_bytes                     | 27477  |
+| Rocksdb_wal_bytes                     | 145333 |
 | Rocksdb_wal_group_syncs               | 0      |
-| Rocksdb_wal_synced                    | 197    |
-| Rocksdb_write_other                   | 807    |
-| Rocksdb_write_self                    | 197    |
+| Rocksdb_wal_synced                    | 518    |
+| Rocksdb_write_other                   | 569    |
+| Rocksdb_write_self                    | 1435   |
 | Rocksdb_write_timedout                | 0      |
-| Rocksdb_write_wal                     | 2008   |
+| Rocksdb_write_wal                     | 4008   |
{noformat}
",17,"Running the same parallel workload on fb-mysql and MariaDB, with and without binlog. Noting counter values:

fb/mysql,  no-binlog vs binlog:

{noformat}
@@ -40,8 +36,8 @@
-| rocksdb_memtable_total                | 44704   |
-| rocksdb_memtable_unflushed            | 44704   |
+| rocksdb_memtable_total                | 95096   |
+| rocksdb_memtable_unflushed            | 95096   |
@@ -59,7 +55,7 @@
-| rocksdb_bytes_written                 | 27561   |
+| rocksdb_bytes_written                 | 210849  |
@@ -79,7 +75,7 @@
-| rocksdb_number_keys_written           | 1005    |
+| rocksdb_number_keys_written           | 2005    |
@@ -97,13 +93,13 @@
-| rocksdb_wal_bytes                     | 27561   |
-| rocksdb_wal_group_syncs               | 0       |
+| rocksdb_wal_bytes                     | 149849  |
+| rocksdb_wal_group_syncs               | 200     |
 | rocksdb_wal_synced                    | 204     |
-| rocksdb_write_other                   | 800     |
-| rocksdb_write_self                    | 204     |
+| rocksdb_write_other                   | 26      |
+| rocksdb_write_self                    | 1978    |
 | rocksdb_write_timedout                | 0       |
-| rocksdb_write_wal                     | 2008    |
+| rocksdb_write_wal                     | 4008    |
{noformat}

MariaDB-gcommit-patch, no-binlog vs binlog:
{noformat}
@@ -28,8 +37,8 @@
-| Rocksdb_memtable_total                | 44584  |
-| Rocksdb_memtable_unflushed            | 44584  |
+| Rocksdb_memtable_total                | 95280  |
+| Rocksdb_memtable_unflushed            | 95280  |
@@ -47,7 +56,7 @@
-| Rocksdb_bytes_written                 | 27477  |
+| Rocksdb_bytes_written                 | 206333 |
@@ -65,7 +74,7 @@
-| Rocksdb_number_keys_written           | 1005   |
+| Rocksdb_number_keys_written           | 2005   |
@@ -83,12 +92,13 @@
-| Rocksdb_wal_bytes                     | 27477  |
+| Rocksdb_wal_bytes                     | 145333 |
 | Rocksdb_wal_group_syncs               | 0      |
-| Rocksdb_wal_synced                    | 197    |
-| Rocksdb_write_other                   | 807    |
-| Rocksdb_write_self                    | 197    |
+| Rocksdb_wal_synced                    | 518    |
+| Rocksdb_write_other                   | 569    |
+| Rocksdb_write_self                    | 1435   |
 | Rocksdb_write_timedout                | 0      |
-| Rocksdb_write_wal                     | 2008   |
+| Rocksdb_write_wal                     | 4008   |
{noformat}
"
1840,MDEV-11934,MDEV,Sergei Petrunia,99702,2017-09-05 21:18:38,"Most counter values are similar across MySQL and MariaDB.

* MariaDB has rocksdb_wal_group_syncs=0 in both cases, this is expected because there's no hton->flush() call.

* There is a difference in rocksdb_wal_synced:
MySQL has:
{noformat}
 | rocksdb_wal_synced                    | 204     |
{noformat}
MariaDB:
{noformat}
-| Rocksdb_wal_synced                    | 197    |
+| Rocksdb_wal_synced                    | 518    |
{noformat}

This agrees with the test execution time. 
Experiments with test-gcommit.cc show that WAL_FILE_SYNCED grows at a rate of ~30/sec (with total TPS being 50, 200 or 500 for 5, 20, and 50 threads in the run, respectively)

That is, Rocksdb_wal_synced is ""grouped commits"". 
It remains unclear why MariaDB+Binlog makes much more syncs than all other variants.",18,"Most counter values are similar across MySQL and MariaDB.

* MariaDB has rocksdb_wal_group_syncs=0 in both cases, this is expected because there's no hton->flush() call.

* There is a difference in rocksdb_wal_synced:
MySQL has:
{noformat}
 | rocksdb_wal_synced                    | 204     |
{noformat}
MariaDB:
{noformat}
-| Rocksdb_wal_synced                    | 197    |
+| Rocksdb_wal_synced                    | 518    |
{noformat}

This agrees with the test execution time. 
Experiments with test-gcommit.cc show that WAL_FILE_SYNCED grows at a rate of ~30/sec (with total TPS being 50, 200 or 500 for 5, 20, and 50 threads in the run, respectively)

That is, Rocksdb_wal_synced is ""grouped commits"". 
It remains unclear why MariaDB+Binlog makes much more syncs than all other variants."
1841,MDEV-11934,MDEV,Sergei Petrunia,99721,2017-09-06 13:44:43,"Added a printout in db_impl_write.cc, printing write_group.size right after 
this call:

{code:cpp}
  // Add to log and apply to memtable.  We can release the lock
  // during this phase since &w is currently responsible for logging
  // and protects against concurrent loggers and concurrent writes
  // into memtables

  last_batch_group_size_ =
      write_thread_.EnterAsBatchGroupLeader(&w, &write_group);
{code}

Trying with {{mysqlslap --concurrency=20}}.
With log_bin=off, I get:

{noformat}
  AAA write_group.size=15
  AAA write_group.size=5
  AAA write_group.size=15
  AAA write_group.size=15
  AAA write_group.size=5
  AAA write_group.size=15
  AAA write_group.size=5
  AAA write_group.size=15
  AAA write_group.size=9
  AAA write_group.size=11
  AAA write_group.size=9
  AAA write_group.size=17
{noformat}

with log_bin=ON, I get this:

{noformat}
  AAA write_group.size=1
  AAA write_group.size=4
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
{noformat}
",19,"Added a printout in db_impl_write.cc, printing write_group.size right after 
this call:

{code:cpp}
  // Add to log and apply to memtable.  We can release the lock
  // during this phase since &w is currently responsible for logging
  // and protects against concurrent loggers and concurrent writes
  // into memtables

  last_batch_group_size_ =
      write_thread_.EnterAsBatchGroupLeader(&w, &write_group);
{code}

Trying with {{mysqlslap --concurrency=20}}.
With log_bin=off, I get:

{noformat}
  AAA write_group.size=15
  AAA write_group.size=5
  AAA write_group.size=15
  AAA write_group.size=15
  AAA write_group.size=5
  AAA write_group.size=15
  AAA write_group.size=5
  AAA write_group.size=15
  AAA write_group.size=9
  AAA write_group.size=11
  AAA write_group.size=9
  AAA write_group.size=17
{noformat}

with log_bin=ON, I get this:

{noformat}
  AAA write_group.size=1
  AAA write_group.size=4
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
  AAA write_group.size=1
{noformat}
"
1842,MDEV-11934,MDEV,Sergei Petrunia,99780,2017-09-07 09:20:59,"Is there something that prevents Prepare() calls from grouping? I've used gdb and manually had two Prepare calls enter a group, and the leader flushed the WAL file for both:

Follower:
{noformat}
  Thread 12 (Thread 0x7fffe6b8e700 (LWP 31582)):
  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  #1  0x00007ffff629ad1c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
  #2  0x00007ffff49d9e73 in std::condition_variable::wait<rocksdb::WriteThread::BlockingAwaitState(rocksdb::WriteThread::Writer*, uint8_t)::<lambda()> >(std::unique_lock<std::mutex> &, rocksdb::WriteThread::<lambda()>) (this=0x7fffe6b8c080, __lock=..., __p=...) at /usr/include/c++/4.9/condition_variable:98
  #3  0x00007ffff49d803e in rocksdb::WriteThread::BlockingAwaitState (this=0x555557bb0360, w=0x7fffe6b8bff0, goal_mask=30 '\036') at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/write_thread.cc:46
  #4  0x00007ffff49d83d8 in rocksdb::WriteThread::AwaitState (this=0x555557bb0360, w=0x7fffe6b8bff0, goal_mask=30 '\036', ctx=0x7ffff50f59a0 <rocksdb::WriteThread::JoinBatchGroup(rocksdb::WriteThread::Writer*)::ctx>) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/write_thread.cc:166
  #5  0x00007ffff49d8c51 in rocksdb::WriteThread::JoinBatchGroup (this=0x555557bb0360, w=0x7fffe6b8bff0) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/write_thread.cc:299
  #6  0x00007ffff48d866a in rocksdb::DBImpl::WriteImpl (this=0x555557bafc20, write_options=..., my_batch=0x7fffb001a040, callback=0x0, log_used=0x7fffb0019e58, log_ref=0, disable_memtable=true) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:102
  #7  0x00007ffff4b0a7cc in rocksdb::TransactionImpl::Prepare (this=0x7fffb0019e50) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/utilities/transactions/transaction_impl.cc:195
  #8  0x00007ffff47ec302 in myrocks::Rdb_transaction_impl::prepare (this=0x7fffb0019a00, name=""\000\000\000\000\000\000\000\001\030\000MySQLXid\f\000\000\000\000\000\000\000\b\000\000\000\000\000\000"") at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2129
  #9  0x00007ffff47c1090 in myrocks::rocksdb_prepare (hton=0x555557b96e70, thd=0x7fffb0000b00, prepare_tx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2733
  #10 0x0000555555d70582 in prepare_or_error (ht=0x555557b96e70, thd=0x7fffb0000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #11 0x0000555555d70e2d in ha_commit_trans (thd=0x7fffb0000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #12 0x0000555555c5d5c7 in trans_commit_stmt (thd=0x7fffb0000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #13 0x0000555555b09c5b in mysql_execute_command (thd=0x7fffb0000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #14 0x0000555555b0e332 in mysql_parse (thd=0x7fffb0000b00, rawbuf=0x7fffb0011058 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", length=42, parser_state=0x7fffe6b8d250, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
  #15 0x0000555555afc33d in dispatch_command (command=COM_QUERY, thd=0x7fffb0000b00, packet=0x7fffb0008911 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", packet_length=42, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1812
  #16 0x0000555555afacb7 in do_command (thd=0x7fffb0000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1360
  #17 0x0000555555c47623 in do_handle_one_connection (connect=0x555557cf8ba0) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1354
  #18 0x0000555555c473b0 in handle_one_connection (arg=0x555557cf8ba0) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1260
  #19 0x00007ffff64fa6aa in start_thread (arg=0x7fffe6b8e700) at pthread_create.c:333
  #20 0x00007ffff5a02eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{noformat}

The leader:
{noformat}
  Thread 11 (Thread 0x7fffe6bd8700 (LWP 31575)):
  #0  rocksdb::DBImpl::WriteImpl (this=0x555557bafc20, write_options=..., my_batch=0x7fffac020a60, callback=0x0, log_used=0x7fffac020878, log_ref=0, disable_memtable=true) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:319
  #1  0x00007ffff4b0a7cc in rocksdb::TransactionImpl::Prepare (this=0x7fffac020870) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/utilities/transactions/transaction_impl.cc:195
  #2  0x00007ffff47ec302 in myrocks::Rdb_transaction_impl::prepare (this=0x7fffac01f260, name=""\000\000\000\000\000\000\000\001\030\000MySQLXid\f\000\000\000\000\000\000\000\a\000\000\000\000\000\000"") at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2129
  #3  0x00007ffff47c1090 in myrocks::rocksdb_prepare (hton=0x555557b96e70, thd=0x7fffac000b00, prepare_tx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2733
  #4  0x0000555555d70582 in prepare_or_error (ht=0x555557b96e70, thd=0x7fffac000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #5  0x0000555555d70e2d in ha_commit_trans (thd=0x7fffac000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #6  0x0000555555c5d5c7 in trans_commit_stmt (thd=0x7fffac000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #7  0x0000555555b09c5b in mysql_execute_command (thd=0x7fffac000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #8  0x0000555555b0e332 in mysql_parse (thd=0x7fffac000b00, rawbuf=0x7fffac011058 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", length=42, parser_state=0x7fffe6bd7250, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
  #9  0x0000555555afc33d in dispatch_command (command=COM_QUERY, thd=0x7fffac000b00, packet=0x7fffac008911 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", packet_length=42, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1812
  #10 0x0000555555afacb7 in do_command (thd=0x7fffac000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1360
  #11 0x0000555555c47623 in do_handle_one_connection (connect=0x555557d86e60) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1354
  #12 0x0000555555c473b0 in handle_one_connection (arg=0x555557d86e60) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1260
  #13 0x00007ffff64fa6aa in start_thread (arg=0x7fffe6bd8700) at pthread_create.c:333
  #14 0x00007ffff5a02eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{noformat}
",20,"Is there something that prevents Prepare() calls from grouping? I've used gdb and manually had two Prepare calls enter a group, and the leader flushed the WAL file for both:

Follower:
{noformat}
  Thread 12 (Thread 0x7fffe6b8e700 (LWP 31582)):
  #0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
  #1  0x00007ffff629ad1c in std::condition_variable::wait(std::unique_lock&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
  #2  0x00007ffff49d9e73 in std::condition_variable::wait >(std::unique_lock &, rocksdb::WriteThread::) (this=0x7fffe6b8c080, __lock=..., __p=...) at /usr/include/c++/4.9/condition_variable:98
  #3  0x00007ffff49d803e in rocksdb::WriteThread::BlockingAwaitState (this=0x555557bb0360, w=0x7fffe6b8bff0, goal_mask=30 '\036') at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/write_thread.cc:46
  #4  0x00007ffff49d83d8 in rocksdb::WriteThread::AwaitState (this=0x555557bb0360, w=0x7fffe6b8bff0, goal_mask=30 '\036', ctx=0x7ffff50f59a0 ) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/write_thread.cc:166
  #5  0x00007ffff49d8c51 in rocksdb::WriteThread::JoinBatchGroup (this=0x555557bb0360, w=0x7fffe6b8bff0) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/write_thread.cc:299
  #6  0x00007ffff48d866a in rocksdb::DBImpl::WriteImpl (this=0x555557bafc20, write_options=..., my_batch=0x7fffb001a040, callback=0x0, log_used=0x7fffb0019e58, log_ref=0, disable_memtable=true) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:102
  #7  0x00007ffff4b0a7cc in rocksdb::TransactionImpl::Prepare (this=0x7fffb0019e50) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/utilities/transactions/transaction_impl.cc:195
  #8  0x00007ffff47ec302 in myrocks::Rdb_transaction_impl::prepare (this=0x7fffb0019a00, name=""\000\000\000\000\000\000\000\001\030\000MySQLXid\f\000\000\000\000\000\000\000\b\000\000\000\000\000\000"") at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2129
  #9  0x00007ffff47c1090 in myrocks::rocksdb_prepare (hton=0x555557b96e70, thd=0x7fffb0000b00, prepare_tx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2733
  #10 0x0000555555d70582 in prepare_or_error (ht=0x555557b96e70, thd=0x7fffb0000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #11 0x0000555555d70e2d in ha_commit_trans (thd=0x7fffb0000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #12 0x0000555555c5d5c7 in trans_commit_stmt (thd=0x7fffb0000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #13 0x0000555555b09c5b in mysql_execute_command (thd=0x7fffb0000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #14 0x0000555555b0e332 in mysql_parse (thd=0x7fffb0000b00, rawbuf=0x7fffb0011058 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", length=42, parser_state=0x7fffe6b8d250, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
  #15 0x0000555555afc33d in dispatch_command (command=COM_QUERY, thd=0x7fffb0000b00, packet=0x7fffb0008911 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", packet_length=42, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1812
  #16 0x0000555555afacb7 in do_command (thd=0x7fffb0000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1360
  #17 0x0000555555c47623 in do_handle_one_connection (connect=0x555557cf8ba0) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1354
  #18 0x0000555555c473b0 in handle_one_connection (arg=0x555557cf8ba0) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1260
  #19 0x00007ffff64fa6aa in start_thread (arg=0x7fffe6b8e700) at pthread_create.c:333
  #20 0x00007ffff5a02eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{noformat}

The leader:
{noformat}
  Thread 11 (Thread 0x7fffe6bd8700 (LWP 31575)):
  #0  rocksdb::DBImpl::WriteImpl (this=0x555557bafc20, write_options=..., my_batch=0x7fffac020a60, callback=0x0, log_used=0x7fffac020878, log_ref=0, disable_memtable=true) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/db/db_impl_write.cc:319
  #1  0x00007ffff4b0a7cc in rocksdb::TransactionImpl::Prepare (this=0x7fffac020870) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/rocksdb/utilities/transactions/transaction_impl.cc:195
  #2  0x00007ffff47ec302 in myrocks::Rdb_transaction_impl::prepare (this=0x7fffac01f260, name=""\000\000\000\000\000\000\000\001\030\000MySQLXid\f\000\000\000\000\000\000\000\a\000\000\000\000\000\000"") at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2129
  #3  0x00007ffff47c1090 in myrocks::rocksdb_prepare (hton=0x555557b96e70, thd=0x7fffac000b00, prepare_tx=false) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:2733
  #4  0x0000555555d70582 in prepare_or_error (ht=0x555557b96e70, thd=0x7fffac000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1146
  #5  0x0000555555d70e2d in ha_commit_trans (thd=0x7fffac000b00, all=false) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:1425
  #6  0x0000555555c5d5c7 in trans_commit_stmt (thd=0x7fffac000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/transaction.cc:510
  #7  0x0000555555b09c5b in mysql_execute_command (thd=0x7fffac000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:6261
  #8  0x0000555555b0e332 in mysql_parse (thd=0x7fffac000b00, rawbuf=0x7fffac011058 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", length=42, parser_state=0x7fffe6bd7250, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:7886
  #9  0x0000555555afc33d in dispatch_command (command=COM_QUERY, thd=0x7fffac000b00, packet=0x7fffac008911 ""INSERT INTO t1 (id, value) VALUES(NULL, 1)"", packet_length=42, is_com_multi=false, is_next_command=false) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1812
  #10 0x0000555555afacb7 in do_command (thd=0x7fffac000b00) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_parse.cc:1360
  #11 0x0000555555c47623 in do_handle_one_connection (connect=0x555557d86e60) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1354
  #12 0x0000555555c473b0 in handle_one_connection (arg=0x555557d86e60) at /home/psergey/dev-git/10.2-mariarocks/sql/sql_connect.cc:1260
  #13 0x00007ffff64fa6aa in start_thread (arg=0x7fffe6bd8700) at pthread_create.c:333
  #14 0x00007ffff5a02eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{noformat}
"
1843,MDEV-11934,MDEV,Sergei Petrunia,99785,2017-09-07 11:27:45,"How about this explanation:

rocksdb_commit_ordered() is called for each transaction in a sequence (that is, SQL layer will not call it for transaction X+1 until the call for transaction X
has returned).

The code in rocksdb_commit_ordered does this:
{code}
  tx->set_sync(false);
  tx->commit_ordered_res= tx->commit();
{code}

the commit() call itself does not need a disk sync. However it gets into a  write group with Prepare() calls that do.

Since the group does a sync, the commit() call will be delayed until that happens. This gives a total run time of #commits * sync_time.
",21,"How about this explanation:

rocksdb_commit_ordered() is called for each transaction in a sequence (that is, SQL layer will not call it for transaction X+1 until the call for transaction X
has returned).

The code in rocksdb_commit_ordered does this:
{code}
  tx->set_sync(false);
  tx->commit_ordered_res= tx->commit();
{code}

the commit() call itself does not need a disk sync. However it gets into a  write group with Prepare() calls that do.

Since the group does a sync, the commit() call will be delayed until that happens. This gives a total run time of #commits * sync_time.
"
1844,MDEV-11934,MDEV,Sergei Petrunia,99786,2017-09-07 11:28:17,"EDIT how does that agree with
{noformat}
  AAA write_group.size=1
  AAA write_group.size=1
...
{noformat}
above?",22,"EDIT how does that agree with
{noformat}
  AAA write_group.size=1
  AAA write_group.size=1
...
{noformat}
above?"
1845,MDEV-11934,MDEV,Sergei Petrunia,99858,2017-09-08 14:36:53,"- Added logging for commit start and end timestamp
- Ran a concurrent workload (20 clients) on a box with a rotating HDD.
- total test run time 24.8 sec, time spent in {{commit}} calls is 25.6 sec.

- Distribution of the time spent in commit:
 !commit-time-histogram.png].png|thumbnail! ",23,"- Added logging for commit start and end timestamp
- Ran a concurrent workload (20 clients) on a box with a rotating HDD.
- total test run time 24.8 sec, time spent in {{commit}} calls is 25.6 sec.

- Distribution of the time spent in commit:
 !commit-time-histogram.png].png|thumbnail! "
1846,MDEV-11934,MDEV,Sergei Petrunia,99859,2017-09-08 14:37:19, !commit-time-histogram.png|thumbnail! ,24, !commit-time-histogram.png|thumbnail! 
1847,MDEV-11934,MDEV,Sergei Petrunia,100023,2017-09-11 17:52:39,"The current code (unfinished, with the issues described above) is at https://github.com/MariaDB/server/tree/bb-10.2-mdev11934",25,"The current code (unfinished, with the issues described above) is at URL"
1848,MDEV-11934,MDEV,Sergei Petrunia,100024,2017-09-11 17:58:46,"Email thread dedicated to this topic:
https://lists.launchpad.net/maria-developers/msg10862.html
https://groups.google.com/forum/#!topic/myrocks-dev/-IgsKABECug",26,"Email thread dedicated to this topic:
URL
URL"
1849,MDEV-11934,MDEV,Sergei Petrunia,100223,2017-09-14 22:56:02,"Found this in rocksdb/db/db_impl.h :
{code:cpp}

  WriteThread write_thread_;
  ..
  // The write thread when the writers have no memtable write. This will be used
  // in 2PC to batch the prepares separately from the serial commit.
  WriteThread nonmem_write_thread_;
{code}

but nonmem_write_thread_ does not seem to be used by the 2PC when the latter is used by MyRocks?

The above code was added in: https://github.com/facebook/rocksdb/commit/499ebb3ab5ea4207950fc95acf102b8f58add1c5
",27,"Found this in rocksdb/db/db_impl.h :
{code:cpp}

  WriteThread write_thread_;
  ..
  // The write thread when the writers have no memtable write. This will be used
  // in 2PC to batch the prepares separately from the serial commit.
  WriteThread nonmem_write_thread_;
{code}

but nonmem_write_thread_ does not seem to be used by the 2PC when the latter is used by MyRocks?

The above code was added in: URL
"
1850,MDEV-11934,MDEV,Sergei Petrunia,101625,2017-10-17 10:24:39,"Merged the patch with the current code.
Commit {{cd7fa0fd62da36f8c6bb88e29fd23fcfd85e3991}}. It is pushed into {{bb-10.2-mariarocks}} branch.",28,"Merged the patch with the current code.
Commit {{cd7fa0fd62da36f8c6bb88e29fd23fcfd85e3991}}. It is pushed into {{bb-10.2-mariarocks}} branch."
1851,MDEV-11934,MDEV,Sergei Petrunia,101628,2017-10-17 10:49:55,"Did some benchmarking
* Testcase [^psergey-test-scaling.test] 
* Results  [^oct17-benchmark.ods] 
* !oct17-benchmark-result-sshot.png|thumbnail! ",29,"Did some benchmarking
* Testcase [^psergey-test-scaling.test] 
* Results  [^oct17-benchmark.ods] 
* !oct17-benchmark-result-sshot.png|thumbnail! "
1852,MDEV-11934,MDEV,Sergei Petrunia,101629,2017-10-17 10:55:50,"Looking at the results
* MariaDB from September (before the latest MyRocks merge) gets no benefit from doing commits in multiple threads. 
* FB/MySQL and current MariaDB do get the benefit
* * MariaDB is actually faster than FB/MySQL. I am not sure why.
* * * It's a debug build, so if the slowdown was the CPU, it would be explainable
*  * * but the workload is disk-bound (I'm intentionally running this on a slow rotating disk) , so CPU speed should not have much effect.",30,"Looking at the results
* MariaDB from September (before the latest MyRocks merge) gets no benefit from doing commits in multiple threads. 
* FB/MySQL and current MariaDB do get the benefit
* * MariaDB is actually faster than FB/MySQL. I am not sure why.
* * * It's a debug build, so if the slowdown was the CPU, it would be explainable
*  * * but the workload is disk-bound (I'm intentionally running this on a slow rotating disk) , so CPU speed should not have much effect."
1853,MDEV-11934,MDEV,Sergei Petrunia,101635,2017-10-17 12:33:26,"Re-ran the benchmark while collecting more data:
MariaDB:
{noformat} TH CONCURRENCY QUERIES TIME QPS WSYNCS WSYNCED Binlog_bytes Binlog_commits Binlog_group_commits 
TR 4 5000 84.3612 59.2690 0 2500 1196120 5000 4720
TR 8 5000 43.4948 114.9563 0 1251 1201960 5000 2678
TR 16 5000 21.2343 235.4681 0 625 1201682 4992 1499
TR 32 5000 10.8093 462.5646 0 313 1202374 4992 854
TR 64 5000 5.4506 917.3302 0 157 1202630 4992 547
TR 128 5000 2.8370 1762.4251 0 80 1202902 4992 339
{noformat} 

FB/MySQL-5.6
{noformat}
TH CONCURRENCY QUERIES TIME QPS WSYNCS WSYNCED Binlog_bytes Binlog_fsync 
TR 4 5000 169.1061 29.5672 2500 2500 995000 2500
TR 8 5000 85.9797 58.1533 1250 1251 995000 1250
TR 16 5000 43.1937 115.7576 624 625 993408 624
TR 32 5000 21.4033 233.6088 312 313 993408 312
TR 64 5000 10.5582 473.5656 156 157 993408 156
TR 128 5000 5.6467 885.4729 79 80 993408 79
{noformat}",31,"Re-ran the benchmark while collecting more data:
MariaDB:
{noformat} TH CONCURRENCY QUERIES TIME QPS WSYNCS WSYNCED Binlog_bytes Binlog_commits Binlog_group_commits 
TR 4 5000 84.3612 59.2690 0 2500 1196120 5000 4720
TR 8 5000 43.4948 114.9563 0 1251 1201960 5000 2678
TR 16 5000 21.2343 235.4681 0 625 1201682 4992 1499
TR 32 5000 10.8093 462.5646 0 313 1202374 4992 854
TR 64 5000 5.4506 917.3302 0 157 1202630 4992 547
TR 128 5000 2.8370 1762.4251 0 80 1202902 4992 339
{noformat} 

FB/MySQL-5.6
{noformat}
TH CONCURRENCY QUERIES TIME QPS WSYNCS WSYNCED Binlog_bytes Binlog_fsync 
TR 4 5000 169.1061 29.5672 2500 2500 995000 2500
TR 8 5000 85.9797 58.1533 1250 1251 995000 1250
TR 16 5000 43.1937 115.7576 624 625 993408 624
TR 32 5000 21.4033 233.6088 312 313 993408 312
TR 64 5000 10.5582 473.5656 156 157 993408 156
TR 128 5000 5.6467 885.4729 79 80 993408 79
{noformat}"
1854,MDEV-11934,MDEV,Sergei Petrunia,101637,2017-10-17 12:46:51,File used for producing the above: [^psergey-test-scaling2.test] ,32,File used for producing the above: [^psergey-test-scaling2.test] 
1855,MDEV-11934,MDEV,Sergei Petrunia,101769,2017-10-23 09:51:35,MDEV-14103 is about testing for this task,33,MDEV-14103 is about testing for this task
1856,MDEV-11934,MDEV,Sergei Petrunia,102033,2017-10-27 14:41:07,KB page https://mariadb.com/kb/en/library/myrocks-and-group-commit-with-binary-log/,34,KB page URL
1857,MDEV-11934,MDEV,Sergei Petrunia,107622,2018-02-21 14:38:30,Closing as this has been fixed and testing found no issues.,35,Closing as this has been fixed and testing found no issues.
1858,MDEV-11953,MDEV,Oleksandr Byelkin,91468,2017-02-06 09:31:29,"The idea is for now do not touch SELECT_LEX in LEX, but make one function which ""allocate"" SELECT_LEX which will be aware about presence of pre-allocated SELECT_LEX.",1,"The idea is for now do not touch SELECT_LEX in LEX, but make one function which ""allocate"" SELECT_LEX which will be aware about presence of pre-allocated SELECT_LEX."
1859,MDEV-11953,MDEV,Oleksandr Byelkin,96819,2017-06-22 16:50:35,TODO: Need examples of pushing derived. (sql_derived.cc *pushdown*),2,TODO: Need examples of pushing derived. (sql_derived.cc *pushdown*)
1860,MDEV-11953,MDEV,Oleksandr Byelkin,96820,2017-06-22 16:51:45,"TODO: Reursive CTE (before opening check)

(According standard: UNION and left part of EXCEPT)",3,"TODO: Reursive CTE (before opening check)

(According standard: UNION and left part of EXCEPT)"
1861,MDEV-11953,MDEV,Oleksandr Byelkin,112023,2018-06-05 15:35:00,"Result of discussion:
1. INTO:
 1.1 standard variant with _SINGLE_ select and INTO after select list
 1.2 tail INTO after all SELECT query with warning about deprecating syntax
2. lock options: as is - belong to each SELECT
3. SELECT options with after parsing check
4. ORDER BY as in standard. It is already implemented syntactically but should be added execution of several tail ORDER/LIMIT constructions. ",4,"Result of discussion:
1. INTO:
 1.1 standard variant with _SINGLE_ select and INTO after select list
 1.2 tail INTO after all SELECT query with warning about deprecating syntax
2. lock options: as is - belong to each SELECT
3. SELECT options with after parsing check
4. ORDER BY as in standard. It is already implemented syntactically but should be added execution of several tail ORDER/LIMIT constructions. "
1862,MDEV-11994,MDEV,Vicențiu Ciorbaru,92139,2017-02-21 14:00:03,I would like to do it this time as well.,1,I would like to do it this time as well.
1863,MDEV-12007,MDEV,Alexander Barkov,91533,2017-02-07 09:33:56,Pushed into bb-10.2-compatibility,1,Pushed into bb-10.2-compatibility
1864,MDEV-12011,MDEV,Alexander Barkov,91661,2017-02-09 06:12:20,"h2. Oracle implementation detais:

Variables referencing the same cursor can be assigned to each other both by the := assignment and the default value assignment:
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT a FROM t1;
  rec1 c%ROWTYPE;
  rec2 c%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec2:= rec1;
  DECLARE
    rec3 c%ROWTYPE:=rec2;
  BEGIN
    DBMS_OUTPUT.PUT_LINE(rec1.a||','||rec2.a || ',' || rec3.a);
  END;
END;
/
CALL p1();
{code}
{noformat}
10,10,10
{noformat}


Variables referencing cursors with compatible components (i.e. with fields having the same field names and compatible types) can also be assigned to each other both by the := assignment and the default value assignment:
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
DROP TABLE t3;
CREATE TABLE t1 (a NUMERIC(10,1));
CREATE TABLE t2 (a FLOAT);
CREATE TABLE t3 (a INT);
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT a FROM t1;
  CURSOR c2 IS SELECT a FROM t2;
  rec1 c1%ROWTYPE;
  rec2 c2%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec2:= rec1;
  DECLARE
    CURSOR c3 IS SELECT a FROM t3;
    rec3 c3%ROWTYPE:=rec2;
  BEGIN
    DBMS_OUTPUT.PUT_LINE(rec1.a||','||rec2.a || ',' || rec3.a);
  END;
END;
/
CALL p1();
{code}
{noformat}
10,10,10
{noformat}


In case if the fields with the same names are on different positions, assignment is still possible and is done by name rather than the field ordinal number. I.e. two {{cursor%ROWTYPE}} variables can be assigned if they have the same set of fields of the same types, but the order of the fields in the record is not important. This is different from variables declared with an explicit type {{TYPE rec_t IS RECORD}} and variables declared as {{table%ROWTYPE}}, which requires the same order of equally named fields.
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
DROP TABLE t3;
CREATE TABLE t1 (a NUMERIC(10,1), b INT);
CREATE TABLE t2 (b INT, a FLOAT);
CREATE TABLE t3 (b INT, a INT);
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT * FROM t1;
  CURSOR c2 IS SELECT * FROM t2;
  rec1 c1%ROWTYPE;
  rec2 c2%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec1.b:=11;
  rec2:= rec1;
  DECLARE
    CURSOR c3 IS SELECT * FROM t3;
    rec3 c3%ROWTYPE:=rec2;
  BEGIN
    DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec2=('||rec2.a || ',' || rec2.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec3=('||rec3.a || ',' || rec3.b || ')');
  END;
END;
/
CALL p1();
{code}
{noformat}
rec1=(10,11)
rec2=(11,10)
rec3=(11,10)
{noformat}

Cursor %ROWTYPE variables and implicit {{RECORD}} variables with compatible structure are also mutually assignable, but only if the fields with the same names are in the same order:

{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT * FROM t1;
  rec1 c%ROWTYPE;
  TYPE rec_t IS RECORD (a INT, b VARCHAR(32));
  rec2 rec_t;
BEGIN
  rec1.a:=10;
  rec1.b:='b20';
  rec2:=rec1;
  rec1:=rec2;
  DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
  DBMS_OUTPUT.PUT_LINE('rec2=('||rec2.a || ',' || rec2.b || ')');
  DECLARE
    rec3 c%ROWTYPE:=rec2;
    rec4 rec_t:=rec1;
  BEGIN
    DBMS_OUTPUT.PUT_LINE('rec3=('||rec3.a || ',' || rec3.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec4=('||rec4.a || ',' || rec4.b || ')');
  END;
END;
/
CALL p1();
{code}
{noformat}
rec1=(10,b20)
rec2=(10,b20)
rec3=(10,b20)
rec4=(10,b20)
{noformat}

Cursor {{%ROWTYPE}} variables and table {{%ROWTYPE}} variables with compatible structure are also mutually assignable, but only if the fields with the same names are in the same order:

{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT * FROM t1;
  rec1 c1%ROWTYPE;
  rec2 t1%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec1.b:='b20';
  rec2:=rec1;
  rec1:=rec2;
  DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
  DBMS_OUTPUT.PUT_LINE('rec2=('||rec2.a || ',' || rec2.b || ')');
  DECLARE
    rec3 c1%ROWTYPE:=rec2;
    rec4 t1%ROWTYPE:=rec1;
  BEGIN
    DBMS_OUTPUT.PUT_LINE('rec3=('||rec3.a || ',' || rec3.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec4=('||rec4.a || ',' || rec4.b || ')');
  END;
END;
/
CALL p1();
{code}
{noformat}
rec1=(10,b20)
rec2=(10,b20)
rec3=(10,b20)
rec4=(10,b20)
{noformat}

{{FETCH}} into a cursor %ROWTYPE variable requires that the number of fields in this variable matches the number of fields in the cursor query. Field names in the cursor %ROWTYPE variable are not important. Assignment is done by ordinal positions.
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(21));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT a,b FROM t1;
  CURSOR c2 IS SELECT 11 AS c, 'd11' AS d FROM DUAL;
  rec1 c1%ROWTYPE;
BEGIN
  OPEN c2;
  FETCH c2 INTO rec1;
  CLOSE c2;
  DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
END;
/
CALL p1();
{code}
{noformat}
rec1=(11,d11)
{noformat}

h3. Duplicate column names in {{cursor%ROWTYPE}} variables
Oracle returns a compile time error if a cursor references by a {{cursor%ROWTYPE}} has duplicate column names.
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
  CURSOR cur1 IS SELECT 'x' AS x, 'y' AS x FROM DUAL;
  rec1 cur1%ROWTYPE;
BEGIN
  NULL;
END;
/
SHOW ERRORS;
{code}
{noformat}
LINE/COL ERROR
-------- -----------------------------------------------------------------
4/8	 PL/SQL: Item ignored
4/8	 PLS-00402: alias required in SELECT list of cursor to avoid
	 duplicate column names
{noformat}
",1,"h2. Oracle implementation detais:

Variables referencing the same cursor can be assigned to each other both by the := assignment and the default value assignment:
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT);
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT a FROM t1;
  rec1 c%ROWTYPE;
  rec2 c%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec2:= rec1;
  DECLARE
    rec3 c%ROWTYPE:=rec2;
  BEGIN
    DBMS_OUTPUT.PUT_LINE(rec1.a||','||rec2.a || ',' || rec3.a);
  END;
END;
/
CALL p1();
{code}
{noformat}
10,10,10
{noformat}


Variables referencing cursors with compatible components (i.e. with fields having the same field names and compatible types) can also be assigned to each other both by the := assignment and the default value assignment:
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
DROP TABLE t3;
CREATE TABLE t1 (a NUMERIC(10,1));
CREATE TABLE t2 (a FLOAT);
CREATE TABLE t3 (a INT);
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT a FROM t1;
  CURSOR c2 IS SELECT a FROM t2;
  rec1 c1%ROWTYPE;
  rec2 c2%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec2:= rec1;
  DECLARE
    CURSOR c3 IS SELECT a FROM t3;
    rec3 c3%ROWTYPE:=rec2;
  BEGIN
    DBMS_OUTPUT.PUT_LINE(rec1.a||','||rec2.a || ',' || rec3.a);
  END;
END;
/
CALL p1();
{code}
{noformat}
10,10,10
{noformat}


In case if the fields with the same names are on different positions, assignment is still possible and is done by name rather than the field ordinal number. I.e. two {{cursor%ROWTYPE}} variables can be assigned if they have the same set of fields of the same types, but the order of the fields in the record is not important. This is different from variables declared with an explicit type {{TYPE rec_t IS RECORD}} and variables declared as {{table%ROWTYPE}}, which requires the same order of equally named fields.
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
DROP TABLE t3;
CREATE TABLE t1 (a NUMERIC(10,1), b INT);
CREATE TABLE t2 (b INT, a FLOAT);
CREATE TABLE t3 (b INT, a INT);
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT * FROM t1;
  CURSOR c2 IS SELECT * FROM t2;
  rec1 c1%ROWTYPE;
  rec2 c2%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec1.b:=11;
  rec2:= rec1;
  DECLARE
    CURSOR c3 IS SELECT * FROM t3;
    rec3 c3%ROWTYPE:=rec2;
  BEGIN
    DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec2=('||rec2.a || ',' || rec2.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec3=('||rec3.a || ',' || rec3.b || ')');
  END;
END;
/
CALL p1();
{code}
{noformat}
rec1=(10,11)
rec2=(11,10)
rec3=(11,10)
{noformat}

Cursor %ROWTYPE variables and implicit {{RECORD}} variables with compatible structure are also mutually assignable, but only if the fields with the same names are in the same order:

{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c IS SELECT * FROM t1;
  rec1 c%ROWTYPE;
  TYPE rec_t IS RECORD (a INT, b VARCHAR(32));
  rec2 rec_t;
BEGIN
  rec1.a:=10;
  rec1.b:='b20';
  rec2:=rec1;
  rec1:=rec2;
  DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
  DBMS_OUTPUT.PUT_LINE('rec2=('||rec2.a || ',' || rec2.b || ')');
  DECLARE
    rec3 c%ROWTYPE:=rec2;
    rec4 rec_t:=rec1;
  BEGIN
    DBMS_OUTPUT.PUT_LINE('rec3=('||rec3.a || ',' || rec3.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec4=('||rec4.a || ',' || rec4.b || ')');
  END;
END;
/
CALL p1();
{code}
{noformat}
rec1=(10,b20)
rec2=(10,b20)
rec3=(10,b20)
rec4=(10,b20)
{noformat}

Cursor {{%ROWTYPE}} variables and table {{%ROWTYPE}} variables with compatible structure are also mutually assignable, but only if the fields with the same names are in the same order:

{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(32));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT * FROM t1;
  rec1 c1%ROWTYPE;
  rec2 t1%ROWTYPE;
BEGIN
  rec1.a:=10;
  rec1.b:='b20';
  rec2:=rec1;
  rec1:=rec2;
  DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
  DBMS_OUTPUT.PUT_LINE('rec2=('||rec2.a || ',' || rec2.b || ')');
  DECLARE
    rec3 c1%ROWTYPE:=rec2;
    rec4 t1%ROWTYPE:=rec1;
  BEGIN
    DBMS_OUTPUT.PUT_LINE('rec3=('||rec3.a || ',' || rec3.b || ')');
    DBMS_OUTPUT.PUT_LINE('rec4=('||rec4.a || ',' || rec4.b || ')');
  END;
END;
/
CALL p1();
{code}
{noformat}
rec1=(10,b20)
rec2=(10,b20)
rec3=(10,b20)
rec4=(10,b20)
{noformat}

{{FETCH}} into a cursor %ROWTYPE variable requires that the number of fields in this variable matches the number of fields in the cursor query. Field names in the cursor %ROWTYPE variable are not important. Assignment is done by ordinal positions.
{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
CREATE TABLE t1 (a INT, b VARCHAR(21));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  CURSOR c1 IS SELECT a,b FROM t1;
  CURSOR c2 IS SELECT 11 AS c, 'd11' AS d FROM DUAL;
  rec1 c1%ROWTYPE;
BEGIN
  OPEN c2;
  FETCH c2 INTO rec1;
  CLOSE c2;
  DBMS_OUTPUT.PUT_LINE('rec1=('||rec1.a || ',' || rec1.b || ')');
END;
/
CALL p1();
{code}
{noformat}
rec1=(11,d11)
{noformat}

h3. Duplicate column names in {{cursor%ROWTYPE}} variables
Oracle returns a compile time error if a cursor references by a {{cursor%ROWTYPE}} has duplicate column names.
{code:sql}
DROP PROCEDURE p1;
CREATE PROCEDURE p1
AS
  CURSOR cur1 IS SELECT 'x' AS x, 'y' AS x FROM DUAL;
  rec1 cur1%ROWTYPE;
BEGIN
  NULL;
END;
/
SHOW ERRORS;
{code}
{noformat}
LINE/COL ERROR
-------- -----------------------------------------------------------------
4/8	 PL/SQL: Item ignored
4/8	 PLS-00402: alias required in SELECT list of cursor to avoid
	 duplicate column names
{noformat}
"
1865,MDEV-12011,MDEV,Alexander Barkov,92857,2017-03-10 10:20:27,Pushed to bb-10.2-compatibility,2,Pushed to bb-10.2-compatibility
1866,MDEV-12011,MDEV,Michael Widenius,92887,2017-03-11 09:58:07,Review not done,3,Review not done
1867,MDEV-12011,MDEV,Michael Widenius,92888,2017-03-11 11:05:12,Now review done. Ok to push after review fixes.,4,Now review done. Ok to push after review fixes.
1868,MDEV-12011,MDEV,Alexander Barkov,93757,2017-04-04 06:41:18,"Addressed Monty's review suggestions.
Pushed a cleanup patch to bb-10.2-compatibility.",5,"Addressed Monty's review suggestions.
Pushed a cleanup patch to bb-10.2-compatibility."
1869,MDEV-12086,MDEV,Alexander Barkov,92080,2017-02-20 15:31:44,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1870,MDEV-12088,MDEV,Alexander Barkov,92082,2017-02-20 15:51:33,"Pushed into 10.3.0.
",1,"Pushed into 10.3.0.
"
1871,MDEV-12089,MDEV,Alexander Barkov,92079,2017-02-20 15:22:00,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1872,MDEV-12098,MDEV,Alexander Barkov,92856,2017-03-10 10:19:17,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1873,MDEV-12107,MDEV,Alexander Barkov,92161,2017-02-22 10:15:02,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1874,MDEV-12133,MDEV,Alexander Barkov,92300,2017-02-27 06:52:45,"h2. Oracle implementation details

{{FETCH}} assigns fields of {{table%ROWTYPE}} variables from left to right. Field names are not important.

{code}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a VARCHAR(10), b VARCHAR(10));
INSERT INTO t1 VALUES ('A','B');
CREATE TABLE t2 (x VARCHAR(10), y VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
  rec2 t2%ROWTYPE;
  CURSOR cur IS SELECT * FROM t1;
BEGIN
  OPEN cur;
  FETCH cur INTO rec1;
  CLOSE cur;
  DBMS_OUTPUT.PUT_LINE(rec1.a||' '||rec1.b);

  OPEN cur;
  FETCH cur INTO rec2;
  CLOSE cur;
  DBMS_OUTPUT.PUT_LINE(rec2.x||' '||rec2.y);
END;
/
CALL p1();
{code}
{noformat}
A B
A B
{noformat}



Two {{table%ROWTYPE}} variables are mutually assignable if they have the same set of equally named fields, but the order of the fields is not important. Strangely, fields are assigned by their ordinal position, from left to right:

{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a VARCHAR(10), b VARCHAR(10));
INSERT INTO t1 VALUES ('A','B');
CREATE TABLE t2 (b VARCHAR(10), a VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
  rec2 t2%ROWTYPE;
  CURSOR cur IS SELECT * FROM t1;
BEGIN
  OPEN cur;
  FETCH cur INTO rec1;
  CLOSE cur;
  DBMS_OUTPUT.PUT_LINE(rec1.a||' '||rec1.b);
  rec2:=rec1;
  DBMS_OUTPUT.PUT_LINE(rec2.a||' '||rec2.b);
END;
/
CALL p1();
{code}
{noformat}
A B
B A
{noformat}
",1,"h2. Oracle implementation details

{{FETCH}} assigns fields of {{table%ROWTYPE}} variables from left to right. Field names are not important.

{code}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a VARCHAR(10), b VARCHAR(10));
INSERT INTO t1 VALUES ('A','B');
CREATE TABLE t2 (x VARCHAR(10), y VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
  rec2 t2%ROWTYPE;
  CURSOR cur IS SELECT * FROM t1;
BEGIN
  OPEN cur;
  FETCH cur INTO rec1;
  CLOSE cur;
  DBMS_OUTPUT.PUT_LINE(rec1.a||' '||rec1.b);

  OPEN cur;
  FETCH cur INTO rec2;
  CLOSE cur;
  DBMS_OUTPUT.PUT_LINE(rec2.x||' '||rec2.y);
END;
/
CALL p1();
{code}
{noformat}
A B
A B
{noformat}



Two {{table%ROWTYPE}} variables are mutually assignable if they have the same set of equally named fields, but the order of the fields is not important. Strangely, fields are assigned by their ordinal position, from left to right:

{code:sql}
SET SERVEROUTPUT ON;
DROP TABLE t1;
DROP TABLE t2;
CREATE TABLE t1 (a VARCHAR(10), b VARCHAR(10));
INSERT INTO t1 VALUES ('A','B');
CREATE TABLE t2 (b VARCHAR(10), a VARCHAR(10));
DROP PROCEDURE p1;
CREATE PROCEDURE p1 AS
  rec1 t1%ROWTYPE;
  rec2 t2%ROWTYPE;
  CURSOR cur IS SELECT * FROM t1;
BEGIN
  OPEN cur;
  FETCH cur INTO rec1;
  CLOSE cur;
  DBMS_OUTPUT.PUT_LINE(rec1.a||' '||rec1.b);
  rec2:=rec1;
  DBMS_OUTPUT.PUT_LINE(rec2.a||' '||rec2.b);
END;
/
CALL p1();
{code}
{noformat}
A B
B A
{noformat}
"
1875,MDEV-12133,MDEV,Alexander Barkov,92858,2017-03-10 10:21:17,"Pushed to bb-10.2-compatibility
",2,"Pushed to bb-10.2-compatibility
"
1876,MDEV-12133,MDEV,Michael Widenius,92884,2017-03-11 08:48:14,Review not done,3,Review not done
1877,MDEV-12133,MDEV,Michael Widenius,92885,2017-03-11 08:49:26,"Review done, some minor changes needed + some function comments",4,"Review done, some minor changes needed + some function comments"
1878,MDEV-12133,MDEV,Alexander Barkov,93756,2017-04-04 06:38:45,"Addressed Monty's review suggestions.
Pushed a cleanup patch to bb-10.2-compatibility.",5,"Addressed Monty's review suggestions.
Pushed a cleanup patch to bb-10.2-compatibility."
1879,MDEV-12137,MDEV,Michael Widenius,93304,2017-03-22 11:20:55,"Suggestion of how to implement this:

- Instead of deleting the rows, first mark them deleted (by storing a record pointer to the be-deleted rows)
  and them delete them in a seconds pass.
- We already do this for secondary tables in our multi_delete code (see end of sql_delete.cc).
  Normally we do deletes directly in the first scan table, but in this case we shouldn't do that but instead
  treat the first table like any other table (mark first and then delete)
- There is a flag delete_while_scanning that looks like it already provides this functionality, don't know why this
  doesn't work in this case.
- It could be that the above delete is treated as a single delete table and this is why one gets the error. Detecting this case and changing this to multi-delete may solve this or at least part of this issue.",1,"Suggestion of how to implement this:

- Instead of deleting the rows, first mark them deleted (by storing a record pointer to the be-deleted rows)
  and them delete them in a seconds pass.
- We already do this for secondary tables in our multi_delete code (see end of sql_delete.cc).
  Normally we do deletes directly in the first scan table, but in this case we shouldn't do that but instead
  treat the first table like any other table (mark first and then delete)
- There is a flag delete_while_scanning that looks like it already provides this functionality, don't know why this
  doesn't work in this case.
- It could be that the above delete is treated as a single delete table and this is why one gets the error. Detecting this case and changing this to multi-delete may solve this or at least part of this issue."
1880,MDEV-12137,MDEV,Sergei Golubchik,120266,2018-12-07 16:41:44,"Please, report it as a new bug. This issue was fixed, and there is a [test|https://github.com/MariaDB/server/blob/mariadb-10.3.11/mysql-test/main/delete_use_source.test] proving that one can indeed use the same table as a source and a delete target, at least in all tested cases.",2,"Please, report it as a new bug. This issue was fixed, and there is a [test|URL proving that one can indeed use the same table as a source and a delete target, at least in all tested cases."
1881,MDEV-12137,MDEV,Sergei Golubchik,176811,2021-01-09 16:25:35,"FYI, this feature was only about *single-table DELETE*, multi-delete isn't fixed yet",3,"FYI, this feature was only about *single-table DELETE*, multi-delete isn't fixed yet"
1882,MDEV-12143,MDEV,Alexander Barkov,92321,2017-02-27 13:48:01,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1883,MDEV-12143,MDEV,Alexander Barkov,92455,2017-03-02 03:55:56,"Alvin, yes they are duplicates. I forgot that we already had the task and created a new one. Sorry for confusion.
",2,"Alvin, yes they are duplicates. I forgot that we already had the task and created a new one. Sorry for confusion.
"
1884,MDEV-12160,MDEV,Daniel Black,92664,2017-03-07 01:40:24,"Probably to help define what is it is, uses:
* Slave to master authentication
* Application to mariadb server authentication for web scripts etc

Rough thoughts:

Registration:
# Using modern authentication, the first use creates a registration with the server with the server and saves one side of the authentication (as a local file) and the server saves the other side (in the users table).
# The user is created without permissions
# A server administrator with existing permissions creates grants for the user
# ""Change Master TO"" could also use the same registration if PASSWORD was made optional (could help simplify the easiest part of MDEV-7502)
# Todo - optional MITM registration protection
( Assumptions: database servers are installed in network environments where the risk of a DoS though registration is low, however its not totally risk free and a post process to enable the registration is required )

Connection:
# The client connection will mmap the file used in registration and will use DH aspects of a key exchange to prevent MITM
# A shared secret will be provided to the client and stored in shared memory (mmap MAP_SHARED) with the other process by the client plugin using the same user (i.e. php-fpm, and equivalent for other languages)

Subsequent connections:
# use a HMAC based authentication from the shared secret in shared memory (for CPU speed in computation and less exchanges compared to DH)",1,"Probably to help define what is it is, uses:
* Slave to master authentication
* Application to mariadb server authentication for web scripts etc

Rough thoughts:

Registration:
# Using modern authentication, the first use creates a registration with the server with the server and saves one side of the authentication (as a local file) and the server saves the other side (in the users table).
# The user is created without permissions
# A server administrator with existing permissions creates grants for the user
# ""Change Master TO"" could also use the same registration if PASSWORD was made optional (could help simplify the easiest part of MDEV-7502)
# Todo - optional MITM registration protection
( Assumptions: database servers are installed in network environments where the risk of a DoS though registration is low, however its not totally risk free and a post process to enable the registration is required )

Connection:
# The client connection will mmap the file used in registration and will use DH aspects of a key exchange to prevent MITM
# A shared secret will be provided to the client and stored in shared memory (mmap MAP_SHARED) with the other process by the client plugin using the same user (i.e. php-fpm, and equivalent for other languages)

Subsequent connections:
# use a HMAC based authentication from the shared secret in shared memory (for CPU speed in computation and less exchanges compared to DH)"
1885,MDEV-12160,MDEV,Sergei Golubchik,92669,2017-03-07 08:46:28,"I'm currently just looking to replace SHA1-based password auth. So the new one should do the same — get the password from the client to the server, so that the server could compare it, without actually seeing or storing the password.

I'd rather avoid saving files on the server or client side, I think it complicates usage.

btw, my working prototype uses ed25519.",2,"I'm currently just looking to replace SHA1-based password auth. So the new one should do the same — get the password from the client to the server, so that the server could compare it, without actually seeing or storing the password.

I'd rather avoid saving files on the server or client side, I think it complicates usage.

btw, my working prototype uses ed25519."
1886,MDEV-12172,MDEV,Dinuka Nadeeshan,93270,2017-03-21 13:47:44,"hi,
I am undergraduate of computer engineering department, Faculty of Engineering, University of Peradeniya, Sri Lanka.
And I am interested in this project. I have lot of experience with MySQL also.

Thank you.",1,"hi,
I am undergraduate of computer engineering department, Faculty of Engineering, University of Peradeniya, Sri Lanka.
And I am interested in this project. I have lot of experience with MySQL also.

Thank you."
1887,MDEV-12172,MDEV,Igor Babaev,93562,2017-03-28 22:53:20,"Hi Dinuka,
It's great that you are interested in this task, but have you noticed that this task is coupled with the task mdev-12176?",2,"Hi Dinuka,
It's great that you are interested in this task, but have you noticed that this task is coupled with the task mdev-12176?"
1888,MDEV-12172,MDEV,Galina Shalygina,97145,2017-06-30 17:41:43,"*First evaluation results:*

( see the the list of commits here https://github.com/MariaDB/server/compare/10.3...shagalla:10.3-mdev12172 )

1. New structure Table Value Constructor (TVC) was added in grammar so it can be used in such ways:

{noformat}
values (1,2);
select 3,4 union values(1,2);
select * from t1 where (t1.a,t1.b) in (values (1,2),(2,3));
create view v1 as values (1,2);
{noformat}

2. TVC was made as a class in new file _sql_tvc.h_ (_class table\_value\_constr_). 
Each TVC instance contains its list of values and link on the results of the query where results of this TVC execution will be stored (_result_). 

*It was decided to start with the case when TVC is a part of UNION statement, so all forward statements are only for this case.*

3. Link on TVC instance is stored in _st\_select\_lex_ class. If TVC is recognised during parsing, new instance of _st\_select\_lex_ is initialized with non-zero link on TVC.

4. _sql_tvc.cc_ was created where _table\_value\_constr::prepare_ and _table\_value\_constr::exec_ are stored.
So TVC has different prepare and execution stages from _st\_select\_lex_ and doesn't have optimization stage.

5. On prepare stage TVCs values types and names of columns of future temporary table where TVC will be stored are searched. As SQL Standard says nothing about how columns should be named it was decided to name them as elements of the first group of values of this TVC.

6. On execution stage TVCs values are sent to _result_ using _send\_date_ method.

7. Now this example is working:

{noformat}
select 5,7 union values (1,2),(2,3);

+---+---+
| 5 | 7 |
+---+---+
| 5 | 7 |
| 1 | 2 |
| 2 | 3 |
+---+---+
{noformat}",3,"*First evaluation results:*

( see the the list of commits here URL )

1. New structure Table Value Constructor (TVC) was added in grammar so it can be used in such ways:

{noformat}
values (1,2);
select 3,4 union values(1,2);
select * from t1 where (t1.a,t1.b) in (values (1,2),(2,3));
create view v1 as values (1,2);
{noformat}

2. TVC was made as a class in new file _sql_tvc.h_ (_class table\_value\_constr_). 
Each TVC instance contains its list of values and link on the results of the query where results of this TVC execution will be stored (_result_). 

*It was decided to start with the case when TVC is a part of UNION statement, so all forward statements are only for this case.*

3. Link on TVC instance is stored in _st\_select\_lex_ class. If TVC is recognised during parsing, new instance of _st\_select\_lex_ is initialized with non-zero link on TVC.

4. _sql_tvc.cc_ was created where _table\_value\_constr::prepare_ and _table\_value\_constr::exec_ are stored.
So TVC has different prepare and execution stages from _st\_select\_lex_ and doesn't have optimization stage.

5. On prepare stage TVCs values types and names of columns of future temporary table where TVC will be stored are searched. As SQL Standard says nothing about how columns should be named it was decided to name them as elements of the first group of values of this TVC.

6. On execution stage TVCs values are sent to _result_ using _send\_date_ method.

7. Now this example is working:

{noformat}
select 5,7 union values (1,2),(2,3);

+---+---+
| 5 | 7 |
+---+---+
| 5 | 7 |
| 1 | 2 |
| 2 | 3 |
+---+---+
{noformat}"
1889,MDEV-12172,MDEV,Galina Shalygina,98004,2017-07-26 21:20:44,"*Second evaluation results:*

( see the the list of commits here https://github.com/MariaDB/server/compare/10.3...shagalla:10.3-mdev12172 )

Queries where TVC(s) are used are processed successfully.

TVCs can be used separately as in: 

{noformat}
values (1,2);

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
+---+---+
{noformat}

with UNION/UNION ALL:

{noformat}
values (1,2) union select 1,2;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
+---+---+
{noformat}

in derived tables:

{noformat}
select * from (values (1,2),(3,4)) as t2;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
| 3 | 4 |
+---+---+
{noformat}

views:

{noformat}
create view v1 as values (1,2),(3,4);
select * from v1;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
| 3 | 4 |
+---+---+
{noformat}

and non-recursive common table expressions:

{noformat}
with t2 as (values (1,2),(3,4)) select * from t2;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
| 3 | 4 |
{noformat}",4,"*Second evaluation results:*

( see the the list of commits here URL )

Queries where TVC(s) are used are processed successfully.

TVCs can be used separately as in: 

{noformat}
values (1,2);

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
+---+---+
{noformat}

with UNION/UNION ALL:

{noformat}
values (1,2) union select 1,2;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
+---+---+
{noformat}

in derived tables:

{noformat}
select * from (values (1,2),(3,4)) as t2;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
| 3 | 4 |
+---+---+
{noformat}

views:

{noformat}
create view v1 as values (1,2),(3,4);
select * from v1;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
| 3 | 4 |
+---+---+
{noformat}

and non-recursive common table expressions:

{noformat}
with t2 as (values (1,2),(3,4)) select * from t2;

+---+---+
| 1 | 2 |
+---+---+
| 1 | 2 |
| 3 | 4 |
{noformat}"
1890,MDEV-12176,MDEV,Galina Shalygina,99414,2017-08-29 14:04:47,"*Final evaluation results:*

( see the the list of commits here https://github.com/MariaDB/server/compare/10.3...shagalla:10.3-mdev12172 )

Optimization that transforms IN predicate into IN subquery that uses TVC was made.
It works this way:

{noformat}
SELECT ... WHERE (expr1, ...) [NOT] IN (value_list) ...;
=>
SELECT ... WHERE (expr1, ...) [NOT] IN (SELECT * FROM (VALUES value_list) AS tvc_name) ...;
{noformat}

Optimization works when IN predicate is used in WHERE clause of the query or when it is ON expression of some join. It also works when IN predicate is in derived tables, CTEs and subqueries.


*What's done:*

1. global variable _in\_subquery\_conversion\_threshold_ that controls optimization working
Optimization works only when the total number of scalar values used in IN predicate is more than this variable

2. transformer _Item\_func\_in::in\_predicate\_to\_in\_subs\_transformer_ that transforms IN predicate in IN subselect

3. creation of TVC specified by values from IN predicate

4. creation of new statement {noformat}SELECT * FROM (VALUES value_list) AS tvc_name{noformat}

5. new test file: _opt\_tvc.test_

To clone this project from github you need to do:

{noformat}
git clone https://github.com/shagalla/server 10.3-mdev12172
cd 10.3-mdev12172
git branch 10.3-mdev12172
git checkout 10.3-mdev12172
maria-git/10.3-mdev12172> git pull  https://github.com/shagalla/server 10.3-mdev12172
{noformat}
",1,"*Final evaluation results:*

( see the the list of commits here URL )

Optimization that transforms IN predicate into IN subquery that uses TVC was made.
It works this way:

{noformat}
SELECT ... WHERE (expr1, ...) [NOT] IN (value_list) ...;
=>
SELECT ... WHERE (expr1, ...) [NOT] IN (SELECT * FROM (VALUES value_list) AS tvc_name) ...;
{noformat}

Optimization works when IN predicate is used in WHERE clause of the query or when it is ON expression of some join. It also works when IN predicate is in derived tables, CTEs and subqueries.


*What's done:*

1. global variable _in\_subquery\_conversion\_threshold_ that controls optimization working
Optimization works only when the total number of scalar values used in IN predicate is more than this variable

2. transformer _Item\_func\_in::in\_predicate\_to\_in\_subs\_transformer_ that transforms IN predicate in IN subselect

3. creation of TVC specified by values from IN predicate

4. creation of new statement {noformat}SELECT * FROM (VALUES value_list) AS tvc_name{noformat}

5. new test file: _opt\_tvc.test_

To clone this project from github you need to do:

{noformat}
git clone URL 10.3-mdev12172
cd 10.3-mdev12172
git branch 10.3-mdev12172
git checkout 10.3-mdev12172
maria-git/10.3-mdev12172> git pull  URL 10.3-mdev12172
{noformat}
"
1891,MDEV-12176,MDEV,VAROQUI Stephane,117999,2018-10-16 15:44:28,Thanks Igor for that MDEV that's one of super generic performance improvement i'v seen so far ,2,Thanks Igor for that MDEV that's one of super generic performance improvement i'v seen so far 
1892,MDEV-12176,MDEV,Frank Sagurna,119554,2018-11-20 14:56:07,"Can i deactivate that ""optimization"", as it seems for me it degrades performance badly. 

MariaDB 10.2.19:
{noformat}
# Time: 181120 15:39:39
# User@Host: 
# Thread_id: 649859  Schema: leitsystem  QC_hit: No
# Query_time: *3*.009499  Lock_time: 0.001029  Rows_sent: 25  Rows_examined: 529865
# Rows_affected: 0
# Full_scan: No  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    SIMPLE  zr_te_value     range   PRIMARY,ts      PRIMARY 9       NULL    273462  264920.00       100.00  100.00  Using where; Using temporary; Using filesort
#
SET timestamp=1542724779;
SELECT CONCAT(""9906643000004NEXT"", LPAD(next_id_int, 16, ""0"")) AS next_id, ts - MOD(ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value WHERE next_id_int IN ('0000000000006447', '0000000000005272', ... )
AND ts >= 1542668700 AND ts < 1542755400 AND zr_art = 12 GROUP BY next_id, ts DIV 300 ORDER BY ts
{noformat}

MariaDB 10.3.10:
{noformat}
# Time: 181120 15:43:02
# User@Host:  
# Thread_id: 195  Schema: leitsystem  QC_hit: No
# Query_time: *191*.434622  Lock_time: 0.001057  Rows_sent: 25  Rows_examined: 210523423
# Rows_affected: 0  Bytes_sent: 1778
# Tmp_tables: 3  Tmp_disk_tables: 0  Tmp_table_sizes: 70491296
# Full_scan: Yes  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    PRIMARY <derived3>      ALL     NULL    NULL    NULL    NULL    1548    1548.00 100.00  100.00  Start temporary; Using temporary; Using filesort
# explain: 1    PRIMARY zr_te_value     ref     PRIMARY,ts      PRIMARY 5       const,tvc_0.0000000000006447    218     135824.89       100.00  0.13    Using where; End temporary
# explain: 3    DERIVED NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    No tables used
#
use leitsystem;
SET timestamp=1542724982;
SELECT CONCAT(""9906643000004NEXT"", LPAD(next_id_int, 16, ""0"")) AS next_id, ts - MOD(ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value WHERE next_id_int IN ('0000000000006447', '0000000000005272', ... )
AND ts >= 1542668700 AND ts < 1542755400 AND zr_art = 12 GROUP BY next_id, ts DIV 300 ORDER BY ts
{noformat}
",3,"Can i deactivate that ""optimization"", as it seems for me it degrades performance badly. 

MariaDB 10.2.19:
{noformat}
# Time: 181120 15:39:39
# User@Host: 
# Thread_id: 649859  Schema: leitsystem  QC_hit: No
# Query_time: *3*.009499  Lock_time: 0.001029  Rows_sent: 25  Rows_examined: 529865
# Rows_affected: 0
# Full_scan: No  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    SIMPLE  zr_te_value     range   PRIMARY,ts      PRIMARY 9       NULL    273462  264920.00       100.00  100.00  Using where; Using temporary; Using filesort
#
SET timestamp=1542724779;
SELECT CONCAT(""9906643000004NEXT"", LPAD(next_id_int, 16, ""0"")) AS next_id, ts - MOD(ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value WHERE next_id_int IN ('0000000000006447', '0000000000005272', ... )
AND ts >= 1542668700 AND ts < 1542755400 AND zr_art = 12 GROUP BY next_id, ts DIV 300 ORDER BY ts
{noformat}

MariaDB 10.3.10:
{noformat}
# Time: 181120 15:43:02
# User@Host:  
# Thread_id: 195  Schema: leitsystem  QC_hit: No
# Query_time: *191*.434622  Lock_time: 0.001057  Rows_sent: 25  Rows_examined: 210523423
# Rows_affected: 0  Bytes_sent: 1778
# Tmp_tables: 3  Tmp_disk_tables: 0  Tmp_table_sizes: 70491296
# Full_scan: Yes  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    PRIMARY       ALL     NULL    NULL    NULL    NULL    1548    1548.00 100.00  100.00  Start temporary; Using temporary; Using filesort
# explain: 1    PRIMARY zr_te_value     ref     PRIMARY,ts      PRIMARY 5       const,tvc_0.0000000000006447    218     135824.89       100.00  0.13    Using where; End temporary
# explain: 3    DERIVED NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    No tables used
#
use leitsystem;
SET timestamp=1542724982;
SELECT CONCAT(""9906643000004NEXT"", LPAD(next_id_int, 16, ""0"")) AS next_id, ts - MOD(ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value WHERE next_id_int IN ('0000000000006447', '0000000000005272', ... )
AND ts >= 1542668700 AND ts < 1542755400 AND zr_art = 12 GROUP BY next_id, ts DIV 300 ORDER BY ts
{noformat}
"
1893,MDEV-12176,MDEV,Igor Babaev,119587,2018-11-20 20:33:46,"Hi Frank,
What is the type of the next_id_int column?
",4,"Hi Frank,
What is the type of the next_id_int column?
"
1894,MDEV-12176,MDEV,Igor Babaev,119593,2018-11-20 21:31:08,"Frank
If you don't need this conversion use
set @@in_predicate_conversion_threshold=1000000;",5,"Frank
If you don't need this conversion use
set @@in_predicate_conversion_threshold=1000000;"
1895,MDEV-12176,MDEV,VAROQUI Stephane,119594,2018-11-20 22:14:25,"Can i suggest to experiment 

create  temporary table t(next_id_int  , key idx(next_id_int) ) engine =memory;
insert t select ('0000000000006447', '0000000000005272', ... );

SELECT CONCAT(""9906643000004NEXT"", LPAD(a.next_id_int, 16, ""0"")) AS next_id, a.ts - MOD(a.ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value a left join t ON a.next_id_int=t.next_id_int WHERE  t.next_id_int IS NULL AND  a.ts >= 1542668700 AND a.ts < 1542755400 AND a.zr_art = 12 GROUP BY a.next_id, a.ts DIV 300 ORDER BY a.ts;

it can help to check if the issue comes from icp shortcut, where filtering may take place after sending record or from full join cost ? 

 



",6,"Can i suggest to experiment 

create  temporary table t(next_id_int  , key idx(next_id_int) ) engine =memory;
insert t select ('0000000000006447', '0000000000005272', ... );

SELECT CONCAT(""9906643000004NEXT"", LPAD(a.next_id_int, 16, ""0"")) AS next_id, a.ts - MOD(a.ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value a left join t ON a.next_id_int=t.next_id_int WHERE  t.next_id_int IS NULL AND  a.ts >= 1542668700 AND a.ts < 1542755400 AND a.zr_art = 12 GROUP BY a.next_id, a.ts DIV 300 ORDER BY a.ts;

it can help to check if the issue comes from icp shortcut, where filtering may take place after sending record or from full join cost ? 

 



"
1896,MDEV-12176,MDEV,Frank Sagurna,119607,2018-11-21 08:21:10,"[~igor] the column type is int. I told the application developers long ago they should give me int at this, but as it never made problems i did not put force on that.
{noformat}
Server version: 10.3.10-MariaDB-1:10.3.10+maria~bionic-log mariadb.org binary distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> set @@in_predicate_conversion_threshold=1000000;
ERROR 1193 (HY000): Unknown system variable 'in_predicate_conversion_threshold'
MariaDB [(none)]>
{noformat}",7,"[~igor] the column type is int. I told the application developers long ago they should give me int at this, but as it never made problems i did not put force on that.
{noformat}
Server version: 10.3.10-MariaDB-1:10.3.10+maria~bionic-log mariadb.org binary distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]> set @@in_predicate_conversion_threshold=1000000;
ERROR 1193 (HY000): Unknown system variable 'in_predicate_conversion_threshold'
MariaDB [(none)]>
{noformat}"
1897,MDEV-12176,MDEV,Frank Sagurna,119612,2018-11-21 09:01:14,"[~stephane@skysql.com]:
I had to adapt it a little bit, but that was fast enough. 
But it gives me totally different row count. The original query gives me about 400k rows, this only about 80k rows (That are the numbers that phpmyadmin shows me):
{code:sql}
create temporary table t(next_id_int INT , key idx(next_id_int) ) engine =memory;
INSERT INTO t (next_id_int) VALUES (6447),(5272),...,(3104);

SELECT CONCAT(""9906643000004NEXT"", LPAD(a.next_id_int, 16, ""0"")) AS next_id, a.ts - MOD(a.ts, 300), ROUND(AVG(value), 3), AVG(quality) 
FROM leitsystem.zr_te_value a left join t ON a.next_id_int=t.next_id_int 
WHERE t.next_id_int IS NULL AND a.ts >= 1542668700 AND a.ts < 1542755400 AND a.zr_art = 12 GROUP BY a.next_id_int, a.ts DIV 300 ORDER BY a.ts;
{code}
{noformat}
# Time: 181121  9:48:07
# User@Host: 
# Thread_id: 525  Schema: leitsystem  QC_hit: No
# Query_time: 4.268082  Lock_time: 0.000173  Rows_sent: 25  Rows_examined: 1474207
# Rows_affected: 0  Bytes_sent: 1793
# Tmp_tables: 1  Tmp_disk_tables: 0  Tmp_table_sizes: 18162792
# Full_scan: No  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    SIMPLE  a       range   PRIMARY,ts      ts      4       NULL    2063432 981521.00       75.00   50.19   Using where; Using temporary; Using filesort
# explain: 1    SIMPLE  t       ref     idx     idx     5       leitsystem.a.next_id_int        2       0.84    100.00  100.00  Using where
#
use leitsystem;
SET timestamp=1542790087;
SELECT CONCAT(""9906643000004NEXT"", LPAD(a.next_id_int, 16, ""0"")) AS next_id, a.ts - MOD(a.ts, 300), ROUND(AVG(value), 3), AVG(quality)
FROM leitsystem.zr_te_value a left join t ON a.next_id_int=t.next_id_int
WHERE t.next_id_int IS NULL AND a.ts >= 1542668700 AND a.ts < 1542755400 AND a.zr_art = 12 GROUP BY a.next_id_int, a.ts DIV 300 ORDER BY a.ts LIMIT 0, 25;
{noformat}",8,"[~stephane@skysql.com]:
I had to adapt it a little bit, but that was fast enough. 
But it gives me totally different row count. The original query gives me about 400k rows, this only about 80k rows (That are the numbers that phpmyadmin shows me):
{code:sql}
create temporary table t(next_id_int INT , key idx(next_id_int) ) engine =memory;
INSERT INTO t (next_id_int) VALUES (6447),(5272),...,(3104);

SELECT CONCAT(""9906643000004NEXT"", LPAD(a.next_id_int, 16, ""0"")) AS next_id, a.ts - MOD(a.ts, 300), ROUND(AVG(value), 3), AVG(quality) 
FROM leitsystem.zr_te_value a left join t ON a.next_id_int=t.next_id_int 
WHERE t.next_id_int IS NULL AND a.ts >= 1542668700 AND a.ts < 1542755400 AND a.zr_art = 12 GROUP BY a.next_id_int, a.ts DIV 300 ORDER BY a.ts;
{code}
{noformat}
# Time: 181121  9:48:07
# User@Host: 
# Thread_id: 525  Schema: leitsystem  QC_hit: No
# Query_time: 4.268082  Lock_time: 0.000173  Rows_sent: 25  Rows_examined: 1474207
# Rows_affected: 0  Bytes_sent: 1793
# Tmp_tables: 1  Tmp_disk_tables: 0  Tmp_table_sizes: 18162792
# Full_scan: No  Full_join: No  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    SIMPLE  a       range   PRIMARY,ts      ts      4       NULL    2063432 981521.00       75.00   50.19   Using where; Using temporary; Using filesort
# explain: 1    SIMPLE  t       ref     idx     idx     5       leitsystem.a.next_id_int        2       0.84    100.00  100.00  Using where
#
use leitsystem;
SET timestamp=1542790087;
SELECT CONCAT(""9906643000004NEXT"", LPAD(a.next_id_int, 16, ""0"")) AS next_id, a.ts - MOD(a.ts, 300), ROUND(AVG(value), 3), AVG(quality)
FROM leitsystem.zr_te_value a left join t ON a.next_id_int=t.next_id_int
WHERE t.next_id_int IS NULL AND a.ts >= 1542668700 AND a.ts < 1542755400 AND a.zr_art = 12 GROUP BY a.next_id_int, a.ts DIV 300 ORDER BY a.ts LIMIT 0, 25;
{noformat}"
1898,MDEV-12176,MDEV,VAROQUI Stephane,119615,2018-11-21 09:41:21,"Tx Frank 
Who is correct that is the question, in the original query you get string IN ('0000xxxx')  witch looks wrong at first place if data type of next_id_int is INTEGER.  it looks like the optimization will build a temp table with the datatype of  NOT IN list : VARCHAR where in your table you get INTEGER, forcing conversion from int to string and not equivalent from string to int. 

1 <=> ""1""
1 <=> ""01""

""1"" <> ""01""
""1"" <=> ""1""
",9,"Tx Frank 
Who is correct that is the question, in the original query you get string IN ('0000xxxx')  witch looks wrong at first place if data type of next_id_int is INTEGER.  it looks like the optimization will build a temp table with the datatype of  NOT IN list : VARCHAR where in your table you get INTEGER, forcing conversion from int to string and not equivalent from string to int. 

1  ""1""
1  ""01""

""1"" <> ""01""
""1""  ""1""
"
1899,MDEV-12176,MDEV,Frank Sagurna,119616,2018-11-21 09:45:52,"@Stephane:
The INT conversion should not be the problem, as for both tests i used a list of INTs.",10,"@Stephane:
The INT conversion should not be the problem, as for both tests i used a list of INTs."
1900,MDEV-12176,MDEV,Igor Babaev,119656,2018-11-22 07:32:56,"Frank,
""The INT conversion should not be the problem, as for both tests i used a list of INTs.""
This is exactly the cause of the problem.
Anyway you can report it as a bug.",11,"Frank,
""The INT conversion should not be the problem, as for both tests i used a list of INTs.""
This is exactly the cause of the problem.
Anyway you can report it as a bug."
1901,MDEV-12176,MDEV,Frank Sagurna,119658,2018-11-22 08:23:34,"[~igor]: No, it is not:
{noformat}
# Time: 181122  9:20:42
# User@Host: 
# Thread_id: 82  Schema: leitsystem  QC_hit: No
# Query_time: 214.707173  Lock_time: 0.000781  Rows_sent: 25  Rows_examined: 211389323
# Rows_affected: 0  Bytes_sent: 1773
# Tmp_tables: 3  Tmp_disk_tables: 0  Tmp_table_sizes: 92843912
# Full_scan: Yes  Full_join: Yes  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    PRIMARY <subquery2>     ALL     distinct_key    NULL    NULL    NULL    1548    1548.00 100.00  100.00  Using temporary; Using filesort
# explain: 1    PRIMARY zr_te_value     ref     PRIMARY,ts      PRIMARY 5       const,tvc_0._col_1      218     136287.92       100.00  0.20    Using where
# explain: 2    MATERIALIZED    <derived3>      ALL     NULL    NULL    NULL    NULL    1548    1548.00 100.00  100.00
# explain: 3    DERIVED NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    No tables used
#
use leitsystem;
SET timestamp=1542874842;
SELECT CONCAT(""9906643000004NEXT"", LPAD(next_id_int, 16, ""0"")) AS next_id, ts - MOD(ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value WHERE next_id_in
t IN (6447,5272,...,3104) 
AND ts >= 1542668700 AND ts < 1542755400 AND zr_art = 12 GROUP BY next_id, ts DIV 300 ORDER BY ts ASC;
{noformat}",12,"[~igor]: No, it is not:
{noformat}
# Time: 181122  9:20:42
# User@Host: 
# Thread_id: 82  Schema: leitsystem  QC_hit: No
# Query_time: 214.707173  Lock_time: 0.000781  Rows_sent: 25  Rows_examined: 211389323
# Rows_affected: 0  Bytes_sent: 1773
# Tmp_tables: 3  Tmp_disk_tables: 0  Tmp_table_sizes: 92843912
# Full_scan: Yes  Full_join: Yes  Tmp_table: Yes  Tmp_table_on_disk: No
# Filesort: Yes  Filesort_on_disk: No  Merge_passes: 0  Priority_queue: Yes
#
# explain: id   select_type     table   type    possible_keys   key     key_len ref     rows    r_rows  filtered        r_filtered      Extra
# explain: 1    PRIMARY      ALL     distinct_key    NULL    NULL    NULL    1548    1548.00 100.00  100.00  Using temporary; Using filesort
# explain: 1    PRIMARY zr_te_value     ref     PRIMARY,ts      PRIMARY 5       const,tvc_0._col_1      218     136287.92       100.00  0.20    Using where
# explain: 2    MATERIALIZED          ALL     NULL    NULL    NULL    NULL    1548    1548.00 100.00  100.00
# explain: 3    DERIVED NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    No tables used
#
use leitsystem;
SET timestamp=1542874842;
SELECT CONCAT(""9906643000004NEXT"", LPAD(next_id_int, 16, ""0"")) AS next_id, ts - MOD(ts, 300), ROUND(AVG(value), 3), AVG(quality) FROM leitsystem.zr_te_value WHERE next_id_in
t IN (6447,5272,...,3104) 
AND ts >= 1542668700 AND ts < 1542755400 AND zr_art = 12 GROUP BY next_id, ts DIV 300 ORDER BY ts ASC;
{noformat}"
1902,MDEV-12176,MDEV,VAROQUI Stephane,119659,2018-11-22 08:48:18,"Hi Frank ! 

Can you report it as a regular bug a close task does not look the good place !  you can link that task as a reference ! 

/svar",13,"Hi Frank ! 

Can you report it as a regular bug a close task does not look the good place !  you can link that task as a reference ! 

/svar"
1903,MDEV-12176,MDEV,Frank Sagurna,119660,2018-11-22 09:00:43,"Should i open an additional bug for this one?

MariaDB [(none)]> set @@in_predicate_conversion_threshold=1000000;
ERROR 1193 (HY000): Unknown system variable 'in_predicate_conversion_threshold'
MariaDB [(none)]>",14,"Should i open an additional bug for this one?

MariaDB [(none)]> set @@in_predicate_conversion_threshold=1000000;
ERROR 1193 (HY000): Unknown system variable 'in_predicate_conversion_threshold'
MariaDB [(none)]>"
1904,MDEV-12176,MDEV,Sergei Petrunia,128130,2019-05-21 11:32:29,Apparently @@in_predicate_conversion_threshold is present in debug builds only.,15,Apparently @@in_predicate_conversion_threshold is present in debug builds only.
1905,MDEV-12176,MDEV,Sergei Petrunia,128131,2019-05-21 11:32:43,Added a documentation stub: https://mariadb.com/kb/en/library/conversion-of-big-in-predicates-into-subqueries/,16,Added a documentation stub: URL
1906,MDEV-12176,MDEV,Alexander Menk,138524,2019-11-23 14:35:01,"Related: MDEV-20871
@Frank: Did you ever create an additional bug for this one?",17,"Related: MDEV-20871
@Frank: Did you ever create an additional bug for this one?"
1907,MDEV-12176,MDEV,Frank Sagurna,138555,2019-11-25 08:27:47,"@Alexander: No i only created MDEV-17795, but there is MDEV-16871",18,"@Alexander: No i only created MDEV-17795, but there is MDEV-16871"
1908,MDEV-12179,MDEV,Marko Mäkelä,92814,2017-03-09 12:56:16,"MDEV-11657 suggests an alternative approach (with a much wider scope): storing the transaction state in a global table. This would require substantial changes to transactional storage engines, but it would avoid the need of using binlog’s XA 2PC mechanism for multi-engine transactions.
With MDEV-11657, persistent transactions would be registered in only one place, with GTID, XID and everything in a single place.",1,"MDEV-11657 suggests an alternative approach (with a much wider scope): storing the transaction state in a global table. This would require substantial changes to transactional storage engines, but it would avoid the need of using binlog’s XA 2PC mechanism for multi-engine transactions.
With MDEV-11657, persistent transactions would be registered in only one place, with GTID, XID and everything in a single place."
1909,MDEV-12179,MDEV,Kristian Nielsen,92819,2017-03-09 13:40:33,"Reading in the associated MDEV-11655:

  ""There will be a common redo log for and all crash-safe storage engines.""

The lack of a common redo log indeed is the source of endless amounts of
complexity and performance penalties, in replication and probably elsewhere
also. Resolving this would be very interesting, (and of much wider scope, as
you said).

The main point of per-engine mysql.gtid_slave_pos table is to allow
replication transactional commits with just one transactional log (if
--log-slave-updates=0). So if my understanding is correct, common redo log
would be a pre-requisite for MDEV-11657 to be a full alternative to this
feature?

It was interesting to see the MDEV-11657 and related designs, I have not
seen any mention of them before.
",2,"Reading in the associated MDEV-11655:

  ""There will be a common redo log for and all crash-safe storage engines.""

The lack of a common redo log indeed is the source of endless amounts of
complexity and performance penalties, in replication and probably elsewhere
also. Resolving this would be very interesting, (and of much wider scope, as
you said).

The main point of per-engine mysql.gtid_slave_pos table is to allow
replication transactional commits with just one transactional log (if
--log-slave-updates=0). So if my understanding is correct, common redo log
would be a pre-requisite for MDEV-11657 to be a full alternative to this
feature?

It was interesting to see the MDEV-11657 and related designs, I have not
seen any mention of them before.
"
1910,MDEV-12179,MDEV,Sergei Petrunia,96483,2017-06-12 18:18:12,"An interesting property of the current patch:

If one does 
{noformat}
 alter table mysql.gtid_slave_pos engine=innobase;
{noformat}
(suppose the above changes the storage engine of gtid_slave_pos from MyISAM to InnoDB)
then {{Transactions_gtid_foreign_engine}} counter will continue to be incremented as if ALTER TABLE didn't happen.  STOP SLAVE ; START SLAVE fixes the issue.",3,"An interesting property of the current patch:

If one does 
{noformat}
 alter table mysql.gtid_slave_pos engine=innobase;
{noformat}
(suppose the above changes the storage engine of gtid_slave_pos from MyISAM to InnoDB)
then {{Transactions_gtid_foreign_engine}} counter will continue to be incremented as if ALTER TABLE didn't happen.  STOP SLAVE ; START SLAVE fixes the issue."
1911,MDEV-12179,MDEV,Sergei Petrunia,96523,2017-06-13 15:53:35,"Another thing to investigate:  I have set 
{noformat}
gtid_pos_auto_engines='Aria,HEAP,TokuDB';
{noformat}

and I can observe {{gtid_slave_pos_TokuDB}} table to be created when a transaction involving a TokuDB table is replayed on the slave.

However, I dont observe a  {{gtid_slave_pos_Aria}} table to be created when a transaction with Aria table is replicated.  The Aria table does have {{TRANSACTIONAL=1}} flag.",4,"Another thing to investigate:  I have set 
{noformat}
gtid_pos_auto_engines='Aria,HEAP,TokuDB';
{noformat}

and I can observe {{gtid_slave_pos_TokuDB}} table to be created when a transaction involving a TokuDB table is replayed on the slave.

However, I dont observe a  {{gtid_slave_pos_Aria}} table to be created when a transaction with Aria table is replicated.  The Aria table does have {{TRANSACTIONAL=1}} flag."
1912,MDEV-12179,MDEV,Kristian Nielsen,96527,2017-06-13 16:57:07,"Aria (or HEAP, or MyISAM) is not transactional, in the sense that it does
not keep a transaction open.

So after an event has been executed by the slave, and the COMMIT event runs,
there is no open transaction involving the Aria storage engine. Therefore,
record_gtid() cannot choose an Aria table. Nor would there be any benefit,
since whatever is done, record_gtid() needs to commit a new transaction.

One could make the default mysql.gtid_slave_pos table use Aria, if desired.
",5,"Aria (or HEAP, or MyISAM) is not transactional, in the sense that it does
not keep a transaction open.

So after an event has been executed by the slave, and the COMMIT event runs,
there is no open transaction involving the Aria storage engine. Therefore,
record_gtid() cannot choose an Aria table. Nor would there be any benefit,
since whatever is done, record_gtid() needs to commit a new transaction.

One could make the default mysql.gtid_slave_pos table use Aria, if desired.
"
1913,MDEV-12179,MDEV,Sergei Petrunia,96944,2017-06-26 17:22:20,"[~knielsen], thanks for clarification",6,"[~knielsen], thanks for clarification"
1914,MDEV-12179,MDEV,Sergei Petrunia,96945,2017-06-26 17:23:27,"Ok I have no issues with the patch (Review Done), except the question of how to package this, which I have described here: https://lists.launchpad.net/maria-developers/msg10779.html .",7,"Ok I have no issues with the patch (Review Done), except the question of how to package this, which I have described here: URL ."
1915,MDEV-12179,MDEV,Sergei Petrunia,96946,2017-06-26 17:59:15,Merged the code to MariaDB 10.3 (not automatic but nothing complex): https://github.com/MariaDB/server/tree/bb-10.3-mdev12179,8,Merged the code to MariaDB 10.3 (not automatic but nothing complex): URL
1916,MDEV-12179,MDEV,Kristian Nielsen,97219,2017-07-03 13:10:52,"Pushed to 10.3.1.

For now, the default for --gtid-pos-auto-engines is empty. This can be easily changed if desired, just update the value in sql/mysqld.cc.",9,"Pushed to 10.3.1.

For now, the default for --gtid-pos-auto-engines is empty. This can be easily changed if desired, just update the value in sql/mysqld.cc."
1917,MDEV-12179,MDEV,Jean-François Gagné,107280,2018-02-13 18:32:59,"Leaving some links about this work here:
* http://kristiannielsen.livejournal.com/19223.html
* https://jfg-mysql.blogspot.com/2017/05/better-replication-with-many-storage-engine.html",10,"Leaving some links about this work here:
* URL
* URL"
1918,MDEV-12189,MDEV,Vicențiu Ciorbaru,92961,2017-03-13 23:03:58,I've tried to reproduce this hanging issue. It doesn't seem to want to happen on my end. I've disabled x86 in CMakeLists.txt in bb-10.2-mariarocks. Is that sufficient?,1,I've tried to reproduce this hanging issue. It doesn't seem to want to happen on my end. I've disabled x86 in CMakeLists.txt in bb-10.2-mariarocks. Is that sufficient?
1919,MDEV-12189,MDEV,Sergei Petrunia,92971,2017-03-14 08:48:15,Yes this is sufficient for now. We can open another issue if somebody is really interested in 32-bit builds. Closing this one.,2,Yes this is sufficient for now. We can open another issue if somebody is really interested in 32-bit builds. Closing this one.
1920,MDEV-12189,MDEV,Sergei Petrunia,92972,2017-03-14 08:48:45,"Fix pushed to bb-10.2-mariarocks tree
",3,"Fix pushed to bb-10.2-mariarocks tree
"
1921,MDEV-12199,MDEV,Alexey Botchkov,92853,2017-03-10 09:11:38,Ok to push.,1,Ok to push.
1922,MDEV-12199,MDEV,Alexander Barkov,92870,2017-03-10 12:14:16,"Pushed to bb-10.2-ext
",2,"Pushed to bb-10.2-ext
"
1923,MDEV-12209,MDEV,Alexander Barkov,92771,2017-03-08 19:58:20,Pushed into bb-10.2-compatibility,1,Pushed into bb-10.2-compatibility
1924,MDEV-12238,MDEV,Alexey Botchkov,93937,2017-04-08 20:38:10,ok to push.,1,ok to push.
1925,MDEV-12238,MDEV,Alexander Barkov,94019,2017-04-13 03:35:22,"Pushed to bb-10.2-ext and 10.3.0.
",2,"Pushed to bb-10.2-ext and 10.3.0.
"
1926,MDEV-12239,MDEV,Alexander Barkov,93199,2017-03-19 19:41:09,"Pushed to bb-10.2-ext
",1,"Pushed to bb-10.2-ext
"
1927,MDEV-12254,MDEV,Sergei Petrunia,92974,2017-03-14 09:59:09,"Running this to check tarbake packages:
{noformat}
PREFIX=http://hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499

FILES=""kvm-tarbake-jaunty-x86/mariadb-10.2.5.tar.gz \
kvm-bintar-centos5-amd64/mariadb-10.2.5-linux-x86_64.tar.gz \
kvm-bintar-centos5-x86/mariadb-10.2.5-linux-i686.tar.gz \
kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz \
kvm-bintar-trusty-x86/mariadb-10.2.5-linux-i686.tar.gz \
""
for  i in $FILES ;  do 
  wget ""$PREFIX/$i"";
  fname=`basename $i`
  tar tf $fname | grep rocksdb
  rm $fname
  echo '---------------------------'
done
{noformat}

I find that 

http://hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-tarbake-jaunty-x86/mariadb-10.2.5.tar.gz

* includes MyRocks and RocksDB sources and test suite.

http://hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz

* includes mariadb-10.2.5-linux-x86_64/lib/plugin/ha_rocksdb.so
* includes MyRocks testsuite in mariadb-10.2.5-linux-x86_64/mysql-test/plugin/rocksdb

Other packages do not seem to include anything related to MyRocks (which is expected for at least some of them).",1,"Running this to check tarbake packages:
{noformat}
PREFIX=URL

FILES=""kvm-tarbake-jaunty-x86/mariadb-10.2.5.tar.gz \
kvm-bintar-centos5-amd64/mariadb-10.2.5-linux-x86_64.tar.gz \
kvm-bintar-centos5-x86/mariadb-10.2.5-linux-i686.tar.gz \
kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz \
kvm-bintar-trusty-x86/mariadb-10.2.5-linux-i686.tar.gz \
""
for  i in $FILES ;  do 
  wget ""$PREFIX/$i"";
  fname=`basename $i`
  tar tf $fname | grep rocksdb
  rm $fname
  echo '---------------------------'
done
{noformat}

I find that 

URL

* includes MyRocks and RocksDB sources and test suite.

URL

* includes mariadb-10.2.5-linux-x86_64/lib/plugin/ha_rocksdb.so
* includes MyRocks testsuite in mariadb-10.2.5-linux-x86_64/mysql-test/plugin/rocksdb

Other packages do not seem to include anything related to MyRocks (which is expected for at least some of them)."
1928,MDEV-12254,MDEV,Sergei Petrunia,92975,2017-03-14 10:00:40,"Debian package doesn't include MyRocks :
{noformat}
find . -name '*deb'
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-connect_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-client_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-gssapi-client_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmysqlclient18_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-cracklib-password-check_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-server_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadb3_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-oqgraph_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-server-core-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-spider_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-test-data_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-common_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-server-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-client-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-mroonga_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-client-core-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-test_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadbd-dev_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadbd19_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mysql-common_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadbclient18_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-gssapi-server_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadb-dev_10.2.5+maria~xenial_i386.deb
{noformat}

{noformat}
psergey@hasky:~/tmp10$ find . -name '*deb' | while read a ; do less $a | grep rocksdb ; done 
psergey@hasky:~/tmp10$  
{noformat}
",2,"Debian package doesn't include MyRocks :
{noformat}
find . -name '*deb'
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-connect_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-client_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-gssapi-client_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmysqlclient18_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-cracklib-password-check_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-server_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadb3_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-oqgraph_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-server-core-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-spider_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-test-data_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-common_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-server-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-client-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-mroonga_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-client-core-10.2_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-test_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadbd-dev_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadbd19_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mysql-common_10.2.5+maria~xenial_all.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadbclient18_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/mariadb-plugin-gssapi-server_10.2.5+maria~xenial_i386.deb
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-deb-xenial-x86/debs/binary/libmariadb-dev_10.2.5+maria~xenial_i386.deb
{noformat}

{noformat}
psergey@hasky:~/tmp10$ find . -name '*deb' | while read a ; do less $a | grep rocksdb ; done 
psergey@hasky:~/tmp10$  
{noformat}
"
1929,MDEV-12254,MDEV,Sergei Petrunia,92976,2017-03-14 10:14:06,"{noformat}
psergey@hasky:~/tmp11$ find . -name '*.rpm' 
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-cassandra-engine.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-server.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-connect-engine.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-compat.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-shared.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-common.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-client.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-test.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-gssapi-server.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-devel.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-oqgraph-engine.rpm
{noformat}

{noformat}
find . -name '*.rpm' |  while read a ; do echo ""===========LOOKING INTO $a"" ;  less $a | grep rocksdb ; done  
{noformat}
shows
{noformat}
===========LOOKING INTO ./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-server.rpm
/usr/lib64/mysql/plugin/ha_rocksdb.so
...
===========LOOKING INTO ./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-test.rpm
/usr/share/mysql-test/plugin/rocksdb
/usr/share/mysql-test/plugin/rocksdb/rocksdb
... (lots of tests)
{noformat}

So, Fedora25 RPM package includes ha_rocksdb.so and the tests.
",3,"{noformat}
psergey@hasky:~/tmp11$ find . -name '*.rpm' 
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-cassandra-engine.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-server.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-connect-engine.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-compat.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-shared.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-common.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-client.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-test.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-gssapi-server.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-devel.rpm
./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-oqgraph-engine.rpm
{noformat}

{noformat}
find . -name '*.rpm' |  while read a ; do echo ""===========LOOKING INTO $a"" ;  less $a | grep rocksdb ; done  
{noformat}
shows
{noformat}
===========LOOKING INTO ./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-server.rpm
/usr/lib64/mysql/plugin/ha_rocksdb.so
...
===========LOOKING INTO ./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-test.rpm
/usr/share/mysql-test/plugin/rocksdb
/usr/share/mysql-test/plugin/rocksdb/rocksdb
... (lots of tests)
{noformat}

So, Fedora25 RPM package includes ha_rocksdb.so and the tests.
"
1930,MDEV-12254,MDEV,Sergei Petrunia,92977,2017-03-14 10:20:35,"Some of MyRocks tools are in  the ""client"" package:

{noformat}
===========LOOKING INTO ./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-client.rpm
/usr/bin/ldb
/usr/bin/mysql_ldb
/usr/bin/sst_dump
{noformat}
which I think is not entirely correct as these are tools for examining server's files.",4,"Some of MyRocks tools are in  the ""client"" package:

{noformat}
===========LOOKING INTO ./hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13499/kvm-rpm-fedora25-amd64/rpms/MariaDB-10.2.5-fedora25-x86_64-client.rpm
/usr/bin/ldb
/usr/bin/mysql_ldb
/usr/bin/sst_dump
{noformat}
which I think is not entirely correct as these are tools for examining server's files."
1931,MDEV-12254,MDEV,Sergei Petrunia,92978,2017-03-14 10:26:15,{{kvm-rpm-centos7-amd64}} and {{kvm-zyp-opensuse13-amd64}} have the same situation like Fedora25 RPMs.,5,{{kvm-rpm-centos7-amd64}} and {{kvm-zyp-opensuse13-amd64}} have the same situation like Fedora25 RPMs.
1932,MDEV-12254,MDEV,Sergei Petrunia,92980,2017-03-14 11:12:16,"Checked Windows builds, mariadb-10.2.5-win32.zip and mariadb-10.2.5-winx64.zip.  Both of them have the needed files:

{noformat}
mariadb-10.2.5-winx64/bin/ldb.exe
mariadb-10.2.5-winx64/bin/ldb.pdb
mariadb-10.2.5-winx64/bin/sst_dump.exe
mariadb-10.2.5-winx64/bin/sst_dump.pdb
mariadb-10.2.5-winx64/bin/mysql_ldb.exe
mariadb-10.2.5-winx64/bin/mysql_ldb.pdb
mariadb-10.2.5-winx64/lib/plugin/ha_rocksdb.dll
mariadb-10.2.5-winx64/lib/plugin/ha_rocksdb.pdb
mariadb-10.2.5-winx64/mysql-test/plugin/rocksdb/ *
{noformat}

I didn't check .msi.
",6,"Checked Windows builds, mariadb-10.2.5-win32.zip and mariadb-10.2.5-winx64.zip.  Both of them have the needed files:

{noformat}
mariadb-10.2.5-winx64/bin/ldb.exe
mariadb-10.2.5-winx64/bin/ldb.pdb
mariadb-10.2.5-winx64/bin/sst_dump.exe
mariadb-10.2.5-winx64/bin/sst_dump.pdb
mariadb-10.2.5-winx64/bin/mysql_ldb.exe
mariadb-10.2.5-winx64/bin/mysql_ldb.pdb
mariadb-10.2.5-winx64/lib/plugin/ha_rocksdb.dll
mariadb-10.2.5-winx64/lib/plugin/ha_rocksdb.pdb
mariadb-10.2.5-winx64/mysql-test/plugin/rocksdb/ *
{noformat}

I didn't check .msi.
"
1933,MDEV-12254,MDEV,Sergei Petrunia,92981,2017-03-14 11:21:41,"Questions open so far:
* Which compression libraries is ha_rocksdb.so linked with? (one way to do this is to start the server and look at {{@@rocksdb_supported_compression_types}})
* Should we have it the server RPM or move to a special RocksDB-SE package? (not sure about what the name should be)
* * Other storage engines are in their own RPM packages
* Why are server-side tools in the  *-client package? They should be in the ""server tools"", or ""storage engine"" package.

",7,"Questions open so far:
* Which compression libraries is ha_rocksdb.so linked with? (one way to do this is to start the server and look at {{@@rocksdb_supported_compression_types}})
* Should we have it the server RPM or move to a special RocksDB-SE package? (not sure about what the name should be)
* * Other storage engines are in their own RPM packages
* Why are server-side tools in the  *-client package? They should be in the ""server tools"", or ""storage engine"" package.

"
1934,MDEV-12254,MDEV,Sergei Petrunia,93135,2017-03-17 12:52:14,"Looking at the Windows x64 tarball, build 13577:
{noformat}
MariaDB [test]> show variables like '%rocksdb_supported_compression%';
+-------------------------------------+-------+
| Variable_name                       | Value |
+-------------------------------------+-------+
| rocksdb_supported_compression_types | Zlib  |
+-------------------------------------+-------+
1 row in set (0.00 sec)

MariaDB [test]> show variables like '%version_compile_os%';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| version_compile_os | Win64 |
+--------------------+-------+
1 row in set (0.00 sec)
{noformat}

Same with the MSI package.

",8,"Looking at the Windows x64 tarball, build 13577:
{noformat}
MariaDB [test]> show variables like '%rocksdb_supported_compression%';
+-------------------------------------+-------+
| Variable_name                       | Value |
+-------------------------------------+-------+
| rocksdb_supported_compression_types | Zlib  |
+-------------------------------------+-------+
1 row in set (0.00 sec)

MariaDB [test]> show variables like '%version_compile_os%';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| version_compile_os | Win64 |
+--------------------+-------+
1 row in set (0.00 sec)
{noformat}

Same with the MSI package.

"
1935,MDEV-12254,MDEV,Vicențiu Ciorbaru,93471,2017-03-25 23:03:43,"Debian packages seem to be in order:
ldd usr/lib/mysql/plugin/ha_rocksdb.so 
	linux-vdso.so.1 =>  (0x00007ffe8bb09000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fb2094ec000)
	libsnappy.so.1 => /usr/lib/x86_64-linux-gnu/libsnappy.so.1 (0x00007fb2092e4000)
	libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fb2090c9000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fb208d41000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fb208a38000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fb20866e000)
	/lib64/ld-linux-x86-64.so.2 (0x000055889521e000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fb208457000)

We just need to make sure that our builders have libsnappy and zlib installed.",9,"Debian packages seem to be in order:
ldd usr/lib/mysql/plugin/ha_rocksdb.so 
	linux-vdso.so.1 =>  (0x00007ffe8bb09000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fb2094ec000)
	libsnappy.so.1 => /usr/lib/x86_64-linux-gnu/libsnappy.so.1 (0x00007fb2092e4000)
	libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fb2090c9000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fb208d41000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fb208a38000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fb20866e000)
	/lib64/ld-linux-x86-64.so.2 (0x000055889521e000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fb208457000)

We just need to make sure that our builders have libsnappy and zlib installed."
1936,MDEV-12254,MDEV,Vicențiu Ciorbaru,93480,2017-03-26 16:23:55,RPM packages now add mysql_ldb and sst_dump into the rocksdb package. Dependencies are added correctly (if they exist).,10,RPM packages now add mysql_ldb and sst_dump into the rocksdb package. Dependencies are added correctly (if they exist).
1937,MDEV-12254,MDEV,Sergei Petrunia,93502,2017-03-27 11:20:49,"Looking at the bintars produced by a build from yesterday: http://hasky.askmonty.org/archive/bb-10.2-mariarocks/build-13696:

kvm-tarbake-jaunty-x86/mariadb-10.2.5.tar.gz
- Source code in storage/rocksdb

kvm-bintar-centos5-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- no MyRocks

kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- lib/plugin/ha_rocksdb.so
- mysql-test/plugin/rocksdb - has tests

kvm-bintar-quantal-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- no MyRocks

kvm-bintar-centos5-x86/mariadb-10.2.5-linux-i686.tar.gz
kvm-bintar-trusty-x86/mariadb-10.2.5-linux-i686.tar.gz
kvm-bintar-quantal-x86/mariadb-10.2.5-linux-i686.tar.gz
- No MyRocks (this is expected for 32-bit systems)

kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- Due to MDEV-12370 has a limited use (non MyRocks-specific issue)

mysqld is still linked to libsnappy.so :
{noformat}
ldd mariadb-10.2.5-linux-x86_64/bin/mysqld
        linux-vdso.so.1 =>  (0x00007ffed2bb4000)
        libsnappy.so.1 => /usr/lib/x86_64-linux-gnu/libsnappy.so.1 (0x00007f4209cfe000)
        libaio.so.1 => /lib/x86_64-linux-gnu/libaio.so.1 (0x00007f4209afc000)
        libnuma.so.1 => /usr/lib/x86_64-linux-gnu/libnuma.so.1 (0x00007f42098f0000)
        libcrypt.so.1 => /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007f42096b8000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f42094b4000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4209296000)
        libsystemd-daemon.so.0 => not found
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f4208f14000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4208c0a000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4208841000)
        /lib64/ld-linux-x86-64.so.2 (0x0000558bd800b000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f420862b000)
{noformat}

ha_rocksdb.so is not:

{noformat}
ldd mariadb-10.2.5-linux-x86_64/lib/plugin/ha_rocksdb.so 
        linux-vdso.so.1 =>  (0x00007ffdeffa6000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fd2f583c000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fd2f54ba000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fd2f51b0000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd2f4de7000)
        /lib64/ld-linux-x86-64.so.2 (0x000056524c44a000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fd2f4bd1000)
{noformat}

Looking at what ha_rocksdb.so supports I see:
{noformat}
MariaDB [test]> show variables like 'rocksdb%compre%';
+-------------------------------------+-------------+
| Variable_name                       | Value       |
+-------------------------------------+-------------+
| rocksdb_supported_compression_types | Snappy,Zlib |
+-------------------------------------+-------------+
{noformat}
so I assume that libsnappy is linked statically.",11,"Looking at the bintars produced by a build from yesterday: URL

kvm-tarbake-jaunty-x86/mariadb-10.2.5.tar.gz
- Source code in storage/rocksdb

kvm-bintar-centos5-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- no MyRocks

kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- lib/plugin/ha_rocksdb.so
- mysql-test/plugin/rocksdb - has tests

kvm-bintar-quantal-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- no MyRocks

kvm-bintar-centos5-x86/mariadb-10.2.5-linux-i686.tar.gz
kvm-bintar-trusty-x86/mariadb-10.2.5-linux-i686.tar.gz
kvm-bintar-quantal-x86/mariadb-10.2.5-linux-i686.tar.gz
- No MyRocks (this is expected for 32-bit systems)

kvm-bintar-trusty-amd64/mariadb-10.2.5-linux-x86_64.tar.gz
- Due to MDEV-12370 has a limited use (non MyRocks-specific issue)

mysqld is still linked to libsnappy.so :
{noformat}
ldd mariadb-10.2.5-linux-x86_64/bin/mysqld
        linux-vdso.so.1 =>  (0x00007ffed2bb4000)
        libsnappy.so.1 => /usr/lib/x86_64-linux-gnu/libsnappy.so.1 (0x00007f4209cfe000)
        libaio.so.1 => /lib/x86_64-linux-gnu/libaio.so.1 (0x00007f4209afc000)
        libnuma.so.1 => /usr/lib/x86_64-linux-gnu/libnuma.so.1 (0x00007f42098f0000)
        libcrypt.so.1 => /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007f42096b8000)
        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f42094b4000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4209296000)
        libsystemd-daemon.so.0 => not found
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f4208f14000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4208c0a000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4208841000)
        /lib64/ld-linux-x86-64.so.2 (0x0000558bd800b000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f420862b000)
{noformat}

ha_rocksdb.so is not:

{noformat}
ldd mariadb-10.2.5-linux-x86_64/lib/plugin/ha_rocksdb.so 
        linux-vdso.so.1 =>  (0x00007ffdeffa6000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fd2f583c000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fd2f54ba000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fd2f51b0000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd2f4de7000)
        /lib64/ld-linux-x86-64.so.2 (0x000056524c44a000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fd2f4bd1000)
{noformat}

Looking at what ha_rocksdb.so supports I see:
{noformat}
MariaDB [test]> show variables like 'rocksdb%compre%';
+-------------------------------------+-------------+
| Variable_name                       | Value       |
+-------------------------------------+-------------+
| rocksdb_supported_compression_types | Snappy,Zlib |
+-------------------------------------+-------------+
{noformat}
so I assume that libsnappy is linked statically."
1938,MDEV-12254,MDEV,Sergei Petrunia,93520,2017-03-27 16:09:40,"Using the same build as above, 
Checked {{mariadb-plugin-rocksdb_10.2.5+maria~trusty_amd64.deb}} :
{noformat}
ldd  ./usr/lib/mysql/plugin/ha_rocksdb.so
        linux-vdso.so.1 =>  (0x00007ffc46b0e000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f5d663b1000)
        libsnappy.so.1 => /usr/lib/libsnappy.so.1 (0x00007f5d661ab000)
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f5d65f90000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5d65c81000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5d65979000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5d655af000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f5d66c15000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5d65399000)
{noformat}
snappy and libz are there, which confirms [~cvicentiu]'s result.

Now, checking the MariaDB-10.2.5-fedora25-x86_64-rocksdb-engine.rpm
{noformat}
ldd ./usr/lib64/mysql/plugin/ha_rocksdb.so
./usr/lib64/mysql/plugin/ha_rocksdb.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by ./usr/lib64/mysql/plugin/ha_rocksdb.so)
./usr/lib64/mysql/plugin/ha_rocksdb.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./usr/lib64/mysql/plugin/ha_rocksdb.so)
        linux-vdso.so.1 =>  (0x00007ffc98fee000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f53a3d65000)
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f53a3b4a000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f53a383b000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f53a3533000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f53a3169000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f53a4604000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f53a2f53000)
{noformat}

It depends on libz but not on libsnappy? 
",12,"Using the same build as above, 
Checked {{mariadb-plugin-rocksdb_10.2.5+maria~trusty_amd64.deb}} :
{noformat}
ldd  ./usr/lib/mysql/plugin/ha_rocksdb.so
        linux-vdso.so.1 =>  (0x00007ffc46b0e000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f5d663b1000)
        libsnappy.so.1 => /usr/lib/libsnappy.so.1 (0x00007f5d661ab000)
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f5d65f90000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5d65c81000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5d65979000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5d655af000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f5d66c15000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5d65399000)
{noformat}
snappy and libz are there, which confirms [~cvicentiu]'s result.

Now, checking the MariaDB-10.2.5-fedora25-x86_64-rocksdb-engine.rpm
{noformat}
ldd ./usr/lib64/mysql/plugin/ha_rocksdb.so
./usr/lib64/mysql/plugin/ha_rocksdb.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by ./usr/lib64/mysql/plugin/ha_rocksdb.so)
./usr/lib64/mysql/plugin/ha_rocksdb.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./usr/lib64/mysql/plugin/ha_rocksdb.so)
        linux-vdso.so.1 =>  (0x00007ffc98fee000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f53a3d65000)
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f53a3b4a000)
        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f53a383b000)
        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f53a3533000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f53a3169000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f53a4604000)
        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f53a2f53000)
{noformat}

It depends on libz but not on libsnappy? 
"
1939,MDEV-12254,MDEV,Vicențiu Ciorbaru,93523,2017-03-27 16:57:30,"[~psergey] Libsnappy is not installed on that builder when the build was performed. When libsnappy gets installed (Daniel has been notified), it should work.",13,"[~psergey] Libsnappy is not installed on that builder when the build was performed. When libsnappy gets installed (Daniel has been notified), it should work."
1940,MDEV-12254,MDEV,Sergei Golubchik,95748,2017-05-24 16:36:35,"This is done by now, right?",14,"This is done by now, right?"
1941,MDEV-12266,MDEV,Marko Mäkelä,109030,2018-03-28 16:56:46,[bb-10.3-marko|https://github.com/MariaDB/server/commit/9536749232f2c74cf601b2161c2efe07534b3e4f] (23 commits),1,[bb-10.3-marko|URL (23 commits)
1942,MDEV-12266,MDEV,Thirunarayanan Balathandayuthapani,109082,2018-03-29 11:55:53,Patch looks OK,2,Patch looks OK
1943,MDEV-12279,MDEV,Sergei Petrunia,93080,2017-03-16 09:13:08,"
Some investigation: 
Debugging this statement:
{noformat}
MariaDB [test]>  CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';
{noformat}

{noformat}
  Breakpoint 1, myrocks::ha_rocksdb::create (this=0x7fffe886f088, name=0x7ffff7eecf50 ""./test/t1"", 
  table_arg=0x7ffff7eea720, create_info=0x7ffff7eed550) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:5410
(gdb) fini
  Run till exit from #0  myrocks::ha_rocksdb::create (this=0x7fffe886f088, name=0x7ffff7eecf50 ""./test/t1"", table_arg=0x7ffff7eea720, create_info=0x7ffff7eed550) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:5410
  0x0000555555d6bfdc in handler::ha_create (this=0x7fffe886f088, name=0x7ffff7eecf50 ""./test/t1"", form=0x7ffff7eea720, info_arg=0x7ffff7eed550) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:4368
  Value returned is $6 = 198
{noformat}

The 198 value is HA_ERR_ROCKSDB_TABLE_DATA_DIRECTORY_NOT_SUPPORTED.

Then, something odd happens.
Execution reaches ha_rocksdb::get_error_message which provides the right error
message

{noformat}
  Breakpoint 2, myrocks::ha_rocksdb::get_error_message (this=0x7fffe886f088, error=198, buf=0x7ffff7ee98d0) at 
  /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:4831
(gdb) next
(gdb) next
(gdb) p buf->c_ptr()
  $13 = 0x7fffe888f1b0 ""Specifying DATA DIRECTORY for an individual table is not supported.""
{noformat}

After which we end up in my_error:
{noformat}
  my_error (nr=1296, MyFlags=2048) at /home/psergey/dev-git/10.2-mariarocks/mysys/my_error.c:109
(gdb) print ebuff
  $18 = ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB\000\...""
{noformat}

... but the error is not passed over to the client.
Is this because there was another call before to my_error() made after ha_rocksdb::create but before ha_rocksdb::get_error_message() ? 

",1,"
Some investigation: 
Debugging this statement:
{noformat}
MariaDB [test]>  CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';
{noformat}

{noformat}
  Breakpoint 1, myrocks::ha_rocksdb::create (this=0x7fffe886f088, name=0x7ffff7eecf50 ""./test/t1"", 
  table_arg=0x7ffff7eea720, create_info=0x7ffff7eed550) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:5410
(gdb) fini
  Run till exit from #0  myrocks::ha_rocksdb::create (this=0x7fffe886f088, name=0x7ffff7eecf50 ""./test/t1"", table_arg=0x7ffff7eea720, create_info=0x7ffff7eed550) at /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:5410
  0x0000555555d6bfdc in handler::ha_create (this=0x7fffe886f088, name=0x7ffff7eecf50 ""./test/t1"", form=0x7ffff7eea720, info_arg=0x7ffff7eed550) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:4368
  Value returned is $6 = 198
{noformat}

The 198 value is HA_ERR_ROCKSDB_TABLE_DATA_DIRECTORY_NOT_SUPPORTED.

Then, something odd happens.
Execution reaches ha_rocksdb::get_error_message which provides the right error
message

{noformat}
  Breakpoint 2, myrocks::ha_rocksdb::get_error_message (this=0x7fffe886f088, error=198, buf=0x7ffff7ee98d0) at 
  /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/ha_rocksdb.cc:4831
(gdb) next
(gdb) next
(gdb) p buf->c_ptr()
  $13 = 0x7fffe888f1b0 ""Specifying DATA DIRECTORY for an individual table is not supported.""
{noformat}

After which we end up in my_error:
{noformat}
  my_error (nr=1296, MyFlags=2048) at /home/psergey/dev-git/10.2-mariarocks/mysys/my_error.c:109
(gdb) print ebuff
  $18 = ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB\000\...""
{noformat}

... but the error is not passed over to the client.
Is this because there was another call before to my_error() made after ha_rocksdb::create but before ha_rocksdb::get_error_message() ? 

"
1944,MDEV-12279,MDEV,Sergei Petrunia,93310,2017-03-22 14:24:36,"Debugging the same query
{noformat}
CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) 
ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';
{noformat}
query, stop in ha_create_table():

{noformat}
ha_rocksdb::create() = 198.
{noformat}

198 is MyRocks' internal HA_ERR_ROCKSDB_TABLE_DATA_DIRECTORY_NOT_SUPPORTED.

then it calls:

{noformat}
  my_error(ER_CANT_CREATE_TABLE, MYF(0), db, table_name, error);
{noformat}

ER_CANT_CREATE_TABLE is defined as:

{noformat}
   eng ""Can't create table %`s.%`s (errno: %M)""
{noformat}

for %M format my_error calls my_vsnprintf_ex, my_strerror.

my_strerror() checks if the error is in [HA_ERR_FIRST, HA_ERR_LAST] range (no), and then calls strerror_r() which returns ""Unknown error 198"".

Now, this already is weird.
Is ha_rocksdb::create() allowed to return custom storage-engine error codes?
It is apparent that error handling code doesn't expect that.
",2,"Debugging the same query
{noformat}
CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) 
ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data';
{noformat}
query, stop in ha_create_table():

{noformat}
ha_rocksdb::create() = 198.
{noformat}

198 is MyRocks' internal HA_ERR_ROCKSDB_TABLE_DATA_DIRECTORY_NOT_SUPPORTED.

then it calls:

{noformat}
  my_error(ER_CANT_CREATE_TABLE, MYF(0), db, table_name, error);
{noformat}

ER_CANT_CREATE_TABLE is defined as:

{noformat}
   eng ""Can't create table %`s.%`s (errno: %M)""
{noformat}

for %M format my_error calls my_vsnprintf_ex, my_strerror.

my_strerror() checks if the error is in [HA_ERR_FIRST, HA_ERR_LAST] range (no), and then calls strerror_r() which returns ""Unknown error 198"".

Now, this already is weird.
Is ha_rocksdb::create() allowed to return custom storage-engine error codes?
It is apparent that error handling code doesn't expect that.
"
1945,MDEV-12279,MDEV,Sergei Petrunia,93312,2017-03-22 14:40:10,"... But right after the my_error call, we have

{noformat}
    table.file->print_error(error, MYF(ME_JUST_WARNING));
{noformat}

{{error}} here is ha_create() return code, so now it is assumed that the error
is from storage-engine defined error codes after all?

Ok I step in there, get to the get_error_message() call,
see ha_rocksdb::get_error_message() produce the error message.

Then things get weird.  Execution arrives to line 3630:

{noformat}
	else
        {
=>        SET_FATAL_ERROR;
	  my_error(ER_GET_ERRMSG, errflag, error, str.c_ptr(), engine);
        }
      }
      else
        my_error(ER_GET_ERRNO, errflag, error, table_type());
      DBUG_VOID_RETURN;
    }
{noformat}

SET_FATAL_ERROR is defined as:

{noformat}
#define SET_FATAL_ERROR fatal_error=1
{noformat}
where fatal_error is a local variable.

Question: what's the point of calling SET_FATAL_ERROR if we then proceed to the DBUG_VOID_RETURN ?
",3,"... But right after the my_error call, we have

{noformat}
    table.file->print_error(error, MYF(ME_JUST_WARNING));
{noformat}

{{error}} here is ha_create() return code, so now it is assumed that the error
is from storage-engine defined error codes after all?

Ok I step in there, get to the get_error_message() call,
see ha_rocksdb::get_error_message() produce the error message.

Then things get weird.  Execution arrives to line 3630:

{noformat}
	else
        {
=>        SET_FATAL_ERROR;
	  my_error(ER_GET_ERRMSG, errflag, error, str.c_ptr(), engine);
        }
      }
      else
        my_error(ER_GET_ERRNO, errflag, error, table_type());
      DBUG_VOID_RETURN;
    }
{noformat}

SET_FATAL_ERROR is defined as:

{noformat}
#define SET_FATAL_ERROR fatal_error=1
{noformat}
where fatal_error is a local variable.

Question: what's the point of calling SET_FATAL_ERROR if we then proceed to the DBUG_VOID_RETURN ?
"
1946,MDEV-12279,MDEV,Sergei Petrunia,93314,2017-03-22 14:47:01,"... anyway, we get into the 
{noformat}
   my_error(ER_GET_ERRMSG, errflag, error, str.c_ptr(), engine);
{noformat} 
call,
which produces a 
{noformat} 
""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB\000
{noformat} 

The condition is eventually pushed into the list of statement' warnings:
{noformat} 
(gdb) wher
  #0  Warning_info::push_warning (this=0x7fffe881b288, thd=0x7fffe8816070, sql_errno=1296, sqlstate=0x5555565c67cd ""HY000"", level=Sql_condition::WARN_LEVEL_WARN, msg=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"") at /home/psergey/dev-git/10.2-mariarocks/sql/sql_error.cc:713
  #1  0x0000555555abdba1 in Diagnostics_area::push_warning (this=0x7fffe881b058, thd=0x7fffe8816070, sql_errno_arg=1296, sqlstate=0x5555565c67cd ""HY000"", level=Sql_condition::WARN_LEVEL_WARN, msg=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"") at /home/psergey/dev-git/10.2-mariarocks/sql/sql_error.h:849
  #2  0x0000555555aae103 in THD::raise_condition (this=0x7fffe8816070, sql_errno=1296, sqlstate=0x5555565c67cd ""HY000"", level=Sql_condition::WARN_LEVEL_WARN, msg=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"") at /home/psergey/dev-git/10.2-mariarocks/sql/sql_class.cc:1144
  #3  0x0000555555a1baed in my_message_sql (error=1296, str=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"", MyFlags=2048) at /home/psergey/dev-git/10.2-mariarocks/sql/mysqld.cc:3628
  #4  0x00005555564cdd9c in my_error (nr=1296, MyFlags=2048) at /home/psergey/dev-git/10.2-mariarocks/mysys/my_error.c:125
  #5  0x0000555555d6a72b in handler::print_error (this=0x7fffe8872088, error=198, errflag=2048) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:3631
{noformat}
",4,"... anyway, we get into the 
{noformat}
   my_error(ER_GET_ERRMSG, errflag, error, str.c_ptr(), engine);
{noformat} 
call,
which produces a 
{noformat} 
""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB\000
{noformat} 

The condition is eventually pushed into the list of statement' warnings:
{noformat} 
(gdb) wher
  #0  Warning_info::push_warning (this=0x7fffe881b288, thd=0x7fffe8816070, sql_errno=1296, sqlstate=0x5555565c67cd ""HY000"", level=Sql_condition::WARN_LEVEL_WARN, msg=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"") at /home/psergey/dev-git/10.2-mariarocks/sql/sql_error.cc:713
  #1  0x0000555555abdba1 in Diagnostics_area::push_warning (this=0x7fffe881b058, thd=0x7fffe8816070, sql_errno_arg=1296, sqlstate=0x5555565c67cd ""HY000"", level=Sql_condition::WARN_LEVEL_WARN, msg=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"") at /home/psergey/dev-git/10.2-mariarocks/sql/sql_error.h:849
  #2  0x0000555555aae103 in THD::raise_condition (this=0x7fffe8816070, sql_errno=1296, sqlstate=0x5555565c67cd ""HY000"", level=Sql_condition::WARN_LEVEL_WARN, msg=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"") at /home/psergey/dev-git/10.2-mariarocks/sql/sql_class.cc:1144
  #3  0x0000555555a1baed in my_message_sql (error=1296, str=0x7ffff7ee95a0 ""Got error 198 'Specifying DATA DIRECTORY for an individual table is not supported.' from ROCKSDB"", MyFlags=2048) at /home/psergey/dev-git/10.2-mariarocks/sql/mysqld.cc:3628
  #4  0x00005555564cdd9c in my_error (nr=1296, MyFlags=2048) at /home/psergey/dev-git/10.2-mariarocks/mysys/my_error.c:125
  #5  0x0000555555d6a72b in handler::print_error (this=0x7fffe8872088, error=198, errflag=2048) at /home/psergey/dev-git/10.2-mariarocks/sql/handler.cc:3631
{noformat}
"
1947,MDEV-12279,MDEV,Sergei Petrunia,93320,2017-03-22 16:34:50,"Discussed with [~serg].

Conclusions:
* return HA_WRONG_CREATE_OPTION as error
* also produce a warning with text",5,"Discussed with [~serg].

Conclusions:
* return HA_WRONG_CREATE_OPTION as error
* also produce a warning with text"
1948,MDEV-12288,MDEV,Marko Mäkelä,93129,2017-03-17 10:00:34,"I attached experimental patches to port this to MySQL 5.7.17 for performance evaluation. The second patch also depends on a patch that is attached to MDEV-11585.
In addition to these patches it could be useful to apply the patch attached to MDEV-12121.

Based on a quick test with MySQL 5.7.17 and these patches, the setting internal_tmp_disk_storage_engine=MyISAM should be used when testing the patches related to temporary tables. (MariaDB does not support InnoDB as the optimizer-internal storage engine, and MDEV-11487 removed the InnoDB implementation of such tables, leaving only the user-visible possibility to CREATE TEMPORARY TABLE…ENGINE=InnoDB.)",1,"I attached experimental patches to port this to MySQL 5.7.17 for performance evaluation. The second patch also depends on a patch that is attached to MDEV-11585.
In addition to these patches it could be useful to apply the patch attached to MDEV-12121.

Based on a quick test with MySQL 5.7.17 and these patches, the setting internal_tmp_disk_storage_engine=MyISAM should be used when testing the patches related to temporary tables. (MariaDB does not support InnoDB as the optimizer-internal storage engine, and MDEV-11487 removed the InnoDB implementation of such tables, leaving only the user-visible possibility to CREATE TEMPORARY TABLE…ENGINE=InnoDB.)"
1949,MDEV-12288,MDEV,Marko Mäkelä,94064,2017-04-18 14:28:56,"The following tasks are prerequisites for including the patch in a release:
* Support upgrade from the old undo log format. (Recover transactions from both insert_undo and update_undo logs.)
* Add consistency checks: Delete-marked records must never ever carry DB_TRX_ID=0 (except in secondary indexes, where DB_TRX_ID does not exist).",2,"The following tasks are prerequisites for including the patch in a release:
* Support upgrade from the old undo log format. (Recover transactions from both insert_undo and update_undo logs.)
* Add consistency checks: Delete-marked records must never ever carry DB_TRX_ID=0 (except in secondary indexes, where DB_TRX_ID does not exist)."
1950,MDEV-12288,MDEV,Marko Mäkelä,97364,2017-07-06 00:15:40,"[bb-10.3-marko|https://github.com/MariaDB/server/commit/9e4664d923d7521626a91523fe09269b223092b7] passes an upgrade test (see the commit message for details).

The only thing that remains to be done is redo logging for setting DB_TRX_ID,DB_ROLL_PTR on ROW_FORMAT=COMPRESSED pages. We probably have to introduce a new redo log record type for that.",3,"[bb-10.3-marko|URL passes an upgrade test (see the commit message for details).

The only thing that remains to be done is redo logging for setting DB_TRX_ID,DB_ROLL_PTR on ROW_FORMAT=COMPRESSED pages. We probably have to introduce a new redo log record type for that."
1951,MDEV-12288,MDEV,Jan Lindström,97382,2017-07-06 09:50:50,"Firstly, changes look correct but I have to say most of the critical changes are on code that is not familiar to me in great detail. Secondly, if upgrade tests pass, this code also works, there could be a new test case where both insert and update undo records are produced to persistent storage and then see that crash recovery works correctly (on different page sizes including compressed row format). Ok to push.",4,"Firstly, changes look correct but I have to say most of the critical changes are on code that is not familiar to me in great detail. Secondly, if upgrade tests pass, this code also works, there could be a new test case where both insert and update undo records are produced to persistent storage and then see that crash recovery works correctly (on different page sizes including compressed row format). Ok to push."
1952,MDEV-12288,MDEV,Marko Mäkelä,97396,2017-07-06 12:30:42,"Thank you for the [review|https://github.com/MariaDB/server/commit/9e4664d923d7521626a91523fe09269b223092b7]. I responded to your review comments, and will next implement the remaining redo log format changes.",5,"Thank you for the [review|URL I responded to your review comments, and will next implement the remaining redo log format changes."
1953,MDEV-12288,MDEV,Marko Mäkelä,97423,2017-07-07 00:00:47,"I implemented a new [redo log format version|https://github.com/MariaDB/server/commit/75f9b9e5a80d6381d093d26a7bd026730a67f6a8] and the [MLOG_ZIP_WRITE_TRX_ID record|https://github.com/MariaDB/server/commit/76a4c1f9242066fdc03dcb49af0e777f1a968404].
While testing the latter, I found out that we are not resetting the DB_TRX_ID as often as I would expect. Some further revision will be needed. Maybe the upgrade compatibility changes broke it, or maybe it was not fully working. It is hard to test this, because the DB_TRX_ID column is hidden from the SQL layer.

Nevertheless, the fields do get reset sometimes (during innodb_zip.bug56680 even for ROW_FORMAT=COMPRESSED tables). The implemented file format changes will allow the resetting to be improved later. So, I would push this now, before the file formats are frozen.",6,"I implemented a new [redo log format version|URL and the [MLOG_ZIP_WRITE_TRX_ID record|URL
While testing the latter, I found out that we are not resetting the DB_TRX_ID as often as I would expect. Some further revision will be needed. Maybe the upgrade compatibility changes broke it, or maybe it was not fully working. It is hard to test this, because the DB_TRX_ID column is hidden from the SQL layer.

Nevertheless, the fields do get reset sometimes (during innodb_zip.bug56680 even for ROW_FORMAT=COMPRESSED tables). The implemented file format changes will allow the resetting to be improved later. So, I would push this now, before the file formats are frozen."
1954,MDEV-12288,MDEV,Marko Mäkelä,97435,2017-07-07 14:51:04,"When testing the recovery of the added MLOG_ZIP_WRITE_TRX_ID record, I noticed that the system columns are not being reset in every case, such as soon after committing an INSERT.

I decided to push this nevertheless, so that we will have the necessary file format changes in place. The bug that the history is not always being reset can be fixed later when time permits.",7,"When testing the recovery of the added MLOG_ZIP_WRITE_TRX_ID record, I noticed that the system columns are not being reset in every case, such as soon after committing an INSERT.

I decided to push this nevertheless, so that we will have the necessary file format changes in place. The bug that the history is not always being reset can be fixed later when time permits."
1955,MDEV-12288,MDEV,Marko Mäkelä,99344,2017-08-28 08:28:24,"The resetting of the DB_TRX_ID column was fixed and regression tests added in
MDEV-13536 DB_TRX_ID is not actually being reset when the history is removed",8,"The resetting of the DB_TRX_ID column was fixed and regression tests added in
MDEV-13536 DB_TRX_ID is not actually being reset when the history is removed"
1956,MDEV-12288,MDEV,Marko Mäkelä,104012,2017-12-04 09:45:23,"[~svoj] noticed that the function lock_rec_convert_impl_to_expl() was unnecessarily looking up trx_id=0, and acquiring trx_sys->mutex when doing the futile lookup.
[The follow-up fix in 10.3.3|https://github.com/MariaDB/server/commit/b213f57dc3f9da93ce444805f7581d982bde9f75] fixes this omission. The [initial MDEV-12288 commit|https://github.com/MariaDB/server/commit/3c09f148f362a587ac3267c31fd17da5f71a0b11] already included a corresponding fast-path for the secondary index lock check in the function row_vers_impl_x_locked_low().",9,"[~svoj] noticed that the function lock_rec_convert_impl_to_expl() was unnecessarily looking up trx_id=0, and acquiring trx_sys->mutex when doing the futile lookup.
[The follow-up fix in 10.3.3|URL fixes this omission. The [initial MDEV-12288 commit|URL already included a corresponding fast-path for the secondary index lock check in the function row_vers_impl_x_locked_low()."
1957,MDEV-12288,MDEV,Marko Mäkelä,108036,2018-03-07 13:24:39,"For the record: Due to this change, InnoDB moved to a single persistent undo log. By design, this ought to fix the [upstream MySQL Bug #55283|https://bugs.mysql.com/bug.php?id=55283], which to my knowledge is still open. The bug should be present in all upstream InnoDB versions at least since MySQL 5.0, where the two-phase commit mechanism was introduced.",10,"For the record: Due to this change, InnoDB moved to a single persistent undo log. By design, this ought to fix the [upstream MySQL Bug #55283|URL which to my knowledge is still open. The bug should be present in all upstream InnoDB versions at least since MySQL 5.0, where the two-phase commit mechanism was introduced."
1958,MDEV-12291,MDEV,Alexander Barkov,93170,2017-03-18 13:40:18,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1959,MDEV-12314,MDEV,Alexander Barkov,93311,2017-03-22 14:32:34,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1960,MDEV-12328,MDEV,Rasmus Johansson,93441,2017-03-24 15:01:30,"Please review this simple addition for building the aws kms plugin for release builds:
https://github.com/MariaDB/server/pull/342",1,"Please review this simple addition for building the aws kms plugin for release builds:
URL"
1961,MDEV-12328,MDEV,Vladislav Vaintroub,93442,2017-03-24 15:07:47,Looks good. I merged the request,2,Looks good. I merged the request
1962,MDEV-12387,MDEV,Galina Shalygina,104950,2017-12-27 13:15:40,"Status for 27.12.2017

This project can be implemented in the similar way as MDEV-9197 and some of the procedures from MDEV-9197 can be reused.

The procedure for the push down conditions in the materialized IN-subqueries should work this way:

Let's look on the example:

{noformat}
select * from t1 
where (a>4) and (b<200) and (x>1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (c<10) 
     group by c)
;
{noformat}

1. Condition that can be pushed in the IN-subquery should be extracted. So it should be condition that is defined only with the fields from the left part of the IN subquery (fields (a,b))

_extracted\_cond_: 
{noformat}
(a>4) and (b<200)
{noformat}

2. Than this condition should be transformed into the condition that is defined only with the fields from the projection list from the right part of the IN-subquery. 
(as a=c and b=max(d))

_subq\_extracted\_cond_: 
{noformat}
(c>4) and (max(d)<200)
{noformat}

3. Than condition which depends only on the fields from the group by of the IN subquery should be extracted from _subq\_extracted\_cond_ (there procedures from MDEV-9197 can be reused)

_subq\_extracted\_cond\_for\_group\_fields_:
{noformat}
 (c>4)
{noformat}

4. Now _subq\_extracted\_cond\_for\_group\_fields_ can be pushed into the WHERE part of the IN subquery. Also subq\_extracted\_cond\_for\_group\_fields should be deleted from the _subq\_extracted\_cond_.

{noformat}
select * from t1 
where (a>4) and (b<200) and (x>1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (с>4) and (c<10) 
     group by c)
;
{noformat}

5. On the last stage _subq\_extracted\_cond_ can be pushed into the HAVING part of the IN subquery.

{noformat}
select * from t1 
where (a>4) and (b<200) and (x>1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (с>4) and (c<10)
     having (max(d)<200)
     group by c)
;
{noformat}

Now everything except the 2 step is done. Procedures for the transformations in the 2 step are also written but they should be corrected.
Also some tests are written.

There was a problem with the 2 step because in the MDEV-9197 all transformations were made after defining what condition can be pushed in the WHERE part of the materialized derived table/view (the 4 step there). There it should be made before (on the 2 step) and also all procedures should be rewritten in the way that they can work with the subquries.
",1,"Status for 27.12.2017

This project can be implemented in the similar way as MDEV-9197 and some of the procedures from MDEV-9197 can be reused.

The procedure for the push down conditions in the materialized IN-subqueries should work this way:

Let's look on the example:

{noformat}
select * from t1 
where (a>4) and (b1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (c<10) 
     group by c)
;
{noformat}

1. Condition that can be pushed in the IN-subquery should be extracted. So it should be condition that is defined only with the fields from the left part of the IN subquery (fields (a,b))

_extracted\_cond_: 
{noformat}
(a>4) and (b<200)
{noformat}

2. Than this condition should be transformed into the condition that is defined only with the fields from the projection list from the right part of the IN-subquery. 
(as a=c and b=max(d))

_subq\_extracted\_cond_: 
{noformat}
(c>4) and (max(d)<200)
{noformat}

3. Than condition which depends only on the fields from the group by of the IN subquery should be extracted from _subq\_extracted\_cond_ (there procedures from MDEV-9197 can be reused)

_subq\_extracted\_cond\_for\_group\_fields_:
{noformat}
 (c>4)
{noformat}

4. Now _subq\_extracted\_cond\_for\_group\_fields_ can be pushed into the WHERE part of the IN subquery. Also subq\_extracted\_cond\_for\_group\_fields should be deleted from the _subq\_extracted\_cond_.

{noformat}
select * from t1 
where (a>4) and (b1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (с>4) and (c<10) 
     group by c)
;
{noformat}

5. On the last stage _subq\_extracted\_cond_ can be pushed into the HAVING part of the IN subquery.

{noformat}
select * from t1 
where (a>4) and (b1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (с>4) and (c<10)
     having (max(d)<200)
     group by c)
;
{noformat}

Now everything except the 2 step is done. Procedures for the transformations in the 2 step are also written but they should be corrected.
Also some tests are written.

There was a problem with the 2 step because in the MDEV-9197 all transformations were made after defining what condition can be pushed in the WHERE part of the materialized derived table/view (the 4 step there). There it should be made before (on the 2 step) and also all procedures should be rewritten in the way that they can work with the subquries.
"
1963,MDEV-12387,MDEV,Galina Shalygina,105390,2018-01-09 02:37:59,"Status for 09.01.2018

It was found that the implementation scheme in the comment above is wrong. The procedure should work this way:

{noformat}
select * from t1 
where (a>4) and (b<200) and (x>1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (c<10) 
     group by c)
;
{noformat}

1 step remains the same. So we get:

_extracted\_cond:_

{noformat}
(a>4) and (b<200)
{noformat}

2. The condition which depends only on the fields from the group by of the IN subquery should be extracted from _extracted\_cond_

_extracted\_cond\_for\_group\_fields:_

{noformat}
(a>4)
{noformat}

3. Than _extracted\_cond\_for\_group\_fields_ should be trasformed into the condition that is defined only with the fields from the projection list from the right part of the IN-subquery (there a=c)

_subq\_extracted\_cond\_for\_group\_fields:_

{noformat}
 (c>4)
{noformat}

4. Now _subq\_extracted\_cond\_for\_group\_fields_ can be pushed into the WHERE part of the IN subquery. Also _extracted\_cond\_for\_group\_fields_ should be deleted from the _extracted\_cond_.

_extracted\_cond:_

{noformat}
(b<200)
{noformat}

5. Than _extracted\_cond_ should be trasformed into the condition that is defined only with the fields from the projection list from the right part of the IN-subquery (there b=max(d))

_subq\_extracted\_cond:_

{noformat}
(max(d)<200)
{noformat}

6. Now _subq\_extracted\_cond_ can be pushed into the HAVING part of the IN subquery. 

{noformat}
select * from t1 
where (a>4) and (b<200) and (x>1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (с>4) and (c<10)
     having (max(d)<200)
     group by c)
;
{noformat}

Now the working prototype is made. Btw it is needed to rewrite _setup\_jtbm\_semi\_joins_ procedure in the way that it will work in this case (to get equalities between left and right parts of IN subquery after pushdown is done). This remarks are done but needs to be tested. ",2,"Status for 09.01.2018

It was found that the implementation scheme in the comment above is wrong. The procedure should work this way:

{noformat}
select * from t1 
where (a>4) and (b1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (c<10) 
     group by c)
;
{noformat}

1 step remains the same. So we get:

_extracted\_cond:_

{noformat}
(a>4) and (b<200)
{noformat}

2. The condition which depends only on the fields from the group by of the IN subquery should be extracted from _extracted\_cond_

_extracted\_cond\_for\_group\_fields:_

{noformat}
(a>4)
{noformat}

3. Than _extracted\_cond\_for\_group\_fields_ should be trasformed into the condition that is defined only with the fields from the projection list from the right part of the IN-subquery (there a=c)

_subq\_extracted\_cond\_for\_group\_fields:_

{noformat}
 (c>4)
{noformat}

4. Now _subq\_extracted\_cond\_for\_group\_fields_ can be pushed into the WHERE part of the IN subquery. Also _extracted\_cond\_for\_group\_fields_ should be deleted from the _extracted\_cond_.

_extracted\_cond:_

{noformat}
(b<200)
{noformat}

5. Than _extracted\_cond_ should be trasformed into the condition that is defined only with the fields from the projection list from the right part of the IN-subquery (there b=max(d))

_subq\_extracted\_cond:_

{noformat}
(max(d)<200)
{noformat}

6. Now _subq\_extracted\_cond_ can be pushed into the HAVING part of the IN subquery. 

{noformat}
select * from t1 
where (a>4) and (b1)
  (a,b) in (
     select c,max(d) 
     from t2 
     where (с>4) and (c<10)
     having (max(d)<200)
     group by c)
;
{noformat}

Now the working prototype is made. Btw it is needed to rewrite _setup\_jtbm\_semi\_joins_ procedure in the way that it will work in this case (to get equalities between left and right parts of IN subquery after pushdown is done). This remarks are done but needs to be tested. "
1964,MDEV-12387,MDEV,Galina Shalygina,111275,2018-05-21 09:39:53,Pushed in 10.4,3,Pushed in 10.4
1965,MDEV-12387,MDEV,Sergei Petrunia,113054,2018-06-26 12:06:31,Documentation is being worked on here: https://mariadb.com/kb/en/library/condition-pushdown-into-in-subqueries/,4,Documentation is being worked on here: URL
1966,MDEV-12392,MDEV,Alexander Barkov,93594,2017-03-29 15:11:21,"Pushed to bb-10.2-ext
",1,"Pushed to bb-10.2-ext
"
1967,MDEV-12393,MDEV,Alexander Barkov,93595,2017-03-29 15:11:46,"Pushed to bb-10.2.ext
",1,"Pushed to bb-10.2.ext
"
1968,MDEV-12411,MDEV,Alexander Barkov,93783,2017-04-04 13:11:35,"Adressed Sergei's review suggestions. Pushed to bb-10.2-ext and 10.3.
",1,"Adressed Sergei's review suggestions. Pushed to bb-10.2-ext and 10.3.
"
1969,MDEV-12528,MDEV,Sergei Petrunia,94097,2017-04-19 13:42:59,"Not sure which one do they mean {{mysql-test/suite/engines}} or {{mysql-test/suite/storage_engine}}.  In either case, how does one get the default mysql-test-run run to include running that suite with MyRocks?

[~elenst], any idea?",1,"Not sure which one do they mean {{mysql-test/suite/engines}} or {{mysql-test/suite/storage_engine}}.  In either case, how does one get the default mysql-test-run run to include running that suite with MyRocks?

[~elenst], any idea?"
1970,MDEV-12528,MDEV,Elena Stepanova,94112,2017-04-19 16:11:14,"It's storage_engine. The other suite, engines, is very much non-agnostic, it's only suitable for InnoDB and MyISAM.
storage_engine is potentially suitable for much wider variety of engines, but the cost is that it takes an effort to adjust, and it's tough to maintain. 
I'll make the adjustments and will add the suite overlay to 10.2 tree. ",2,"It's storage_engine. The other suite, engines, is very much non-agnostic, it's only suitable for InnoDB and MyISAM.
storage_engine is potentially suitable for much wider variety of engines, but the cost is that it takes an effort to adjust, and it's tough to maintain. 
I'll make the adjustments and will add the suite overlay to 10.2 tree. "
1971,MDEV-12528,MDEV,Elena Stepanova,95753,2017-05-24 17:55:37,"First run (only storage_engine, no subsuites):

Completed: Failed 53/88 tests

Failing test(s): storage_engine-rocksdb.alter_tablespace storage_engine-rocksdb.autoinc_secondary storage_engine-rocksdb.cache_index storage_engine-rocksdb.checksum_table_live storage_engine-rocksdb.create_table storage_engine-rocksdb.delete_low_prio storage_engine-rocksdb.delete_with_keys storage_engine-rocksdb.foreign_keys storage_engine-rocksdb.fulltext_search storage_engine-rocksdb.handler storage_engine-rocksdb.index storage_engine-rocksdb.index_enable_disable storage_engine-rocksdb.index_key_block_size storage_engine-rocksdb.index_primary storage_engine-rocksdb.index_type_btree storage_engine-rocksdb.index_type_hash storage_engine-rocksdb.insert_delayed storage_engine-rocksdb.insert_high_prio storage_engine-rocksdb.insert_low_prio storage_engine-rocksdb.insert_with_keys storage_engine-rocksdb.lock storage_engine-rocksdb.lock_concurrent storage_engine-rocksdb.misc storage_engine-rocksdb.optimize_table storage_engine-rocksdb.repair_table storage_engine-rocksdb.replace storage_engine-rocksdb.select_high_prio storage_engine-rocksdb.show_engine storage_engine-rocksdb.show_table_status storage_engine-rocksdb.tbl_opt_data_dir storage_engine-rocksdb.tbl_opt_index_dir storage_engine-rocksdb.tbl_opt_insert_method storage_engine-rocksdb.tbl_opt_union storage_engine-rocksdb.tbl_temporary storage_engine-rocksdb.truncate_table storage_engine-rocksdb.type_binary_indexes storage_engine-rocksdb.type_bit_indexes storage_engine-rocksdb.type_blob_indexes storage_engine-rocksdb.type_char_indexes storage_engine-rocksdb.type_date_time_indexes storage_engine-rocksdb.type_enum_indexes storage_engine-rocksdb.type_fixed_indexes storage_engine-rocksdb.type_float_indexes storage_engine-rocksdb.type_int_indexes storage_engine-rocksdb.type_set_indexes storage_engine-rocksdb.type_spatial storage_engine-rocksdb.type_spatial_indexes storage_engine-rocksdb.type_text_indexes storage_engine-rocksdb.update_ignore storage_engine-rocksdb.update_low_prio storage_engine-rocksdb.update_multi storage_engine-rocksdb.update_with_keys storage_engine-rocksdb.vcol

Lots of tests fail with ""Unsupported collation"".",3,"First run (only storage_engine, no subsuites):

Completed: Failed 53/88 tests

Failing test(s): storage_engine-rocksdb.alter_tablespace storage_engine-rocksdb.autoinc_secondary storage_engine-rocksdb.cache_index storage_engine-rocksdb.checksum_table_live storage_engine-rocksdb.create_table storage_engine-rocksdb.delete_low_prio storage_engine-rocksdb.delete_with_keys storage_engine-rocksdb.foreign_keys storage_engine-rocksdb.fulltext_search storage_engine-rocksdb.handler storage_engine-rocksdb.index storage_engine-rocksdb.index_enable_disable storage_engine-rocksdb.index_key_block_size storage_engine-rocksdb.index_primary storage_engine-rocksdb.index_type_btree storage_engine-rocksdb.index_type_hash storage_engine-rocksdb.insert_delayed storage_engine-rocksdb.insert_high_prio storage_engine-rocksdb.insert_low_prio storage_engine-rocksdb.insert_with_keys storage_engine-rocksdb.lock storage_engine-rocksdb.lock_concurrent storage_engine-rocksdb.misc storage_engine-rocksdb.optimize_table storage_engine-rocksdb.repair_table storage_engine-rocksdb.replace storage_engine-rocksdb.select_high_prio storage_engine-rocksdb.show_engine storage_engine-rocksdb.show_table_status storage_engine-rocksdb.tbl_opt_data_dir storage_engine-rocksdb.tbl_opt_index_dir storage_engine-rocksdb.tbl_opt_insert_method storage_engine-rocksdb.tbl_opt_union storage_engine-rocksdb.tbl_temporary storage_engine-rocksdb.truncate_table storage_engine-rocksdb.type_binary_indexes storage_engine-rocksdb.type_bit_indexes storage_engine-rocksdb.type_blob_indexes storage_engine-rocksdb.type_char_indexes storage_engine-rocksdb.type_date_time_indexes storage_engine-rocksdb.type_enum_indexes storage_engine-rocksdb.type_fixed_indexes storage_engine-rocksdb.type_float_indexes storage_engine-rocksdb.type_int_indexes storage_engine-rocksdb.type_set_indexes storage_engine-rocksdb.type_spatial storage_engine-rocksdb.type_spatial_indexes storage_engine-rocksdb.type_text_indexes storage_engine-rocksdb.update_ignore storage_engine-rocksdb.update_low_prio storage_engine-rocksdb.update_multi storage_engine-rocksdb.update_with_keys storage_engine-rocksdb.vcol

Lots of tests fail with ""Unsupported collation""."
1972,MDEV-12528,MDEV,Elena Stepanova,96815,2017-06-22 15:46:36,"Got it running (all 3 suites together):
{noformat}
The servers were restarted 2 times
Spent 118.614 of 135 seconds executing testcases

Completed: All 77 tests were successful.
{noformat}

The following tests have been disabled on given reasons:
{noformat:title=storage_engine suite}
alter_tablespace     : Not supported
autoinc_secondary    : Not supported
create_table         : MDEV-12914 - Engine for temporary tables which are implicitly created as RocksDB is substituted silently
delete_low_prio      : Not supported
foreign_keys         : Not supported
fulltext_search      : Not supported
handler              : Not supported
index_enable_disable : Not supported
insert_delayed       : Not supported
insert_high_prio     : Not supported
insert_low_prio      : Not supported
lock                 : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
lock_concurrent      : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
optimize_table       : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
repair_table         : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
select_high_prio     : Not supported
show_table_status    : MDEV-13152 - Indeterministic row number in SHOW TABLE STATUS on RocksDB table
tbl_opt_data_dir     : Not supported
tbl_opt_index_dir    : Not supported
type_spatial         : Not supported
type_spatial_indexes : Not supported
update_low_prio      : Not supported
update_ignore        : MDEV-13151 - Indeterministic results of multi-table update on RocksDB tables
update_multi         : MDEV-13151 - Indeterministic results of multi-table update on RocksDB tables
vcol                 : Not supported
{noformat}
{noformat:title=storage_engine/parts subsuite}
alter_table    : MDEV-13153 - Assertion `global_status_var.global_memory_used == 0'
optimize_table : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
repair_table   : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
{noformat}
{noformat:title=storage_engine/trx subsuite}
cons_snapshot_serializable : Not supported
level_read_uncommitted     : Not supported
level_serializable         : Not supported
xa_recovery                : MDEV-13155 - XA recovery not supported for RocksDB
{noformat}",4,"Got it running (all 3 suites together):
{noformat}
The servers were restarted 2 times
Spent 118.614 of 135 seconds executing testcases

Completed: All 77 tests were successful.
{noformat}

The following tests have been disabled on given reasons:
{noformat:title=storage_engine suite}
alter_tablespace     : Not supported
autoinc_secondary    : Not supported
create_table         : MDEV-12914 - Engine for temporary tables which are implicitly created as RocksDB is substituted silently
delete_low_prio      : Not supported
foreign_keys         : Not supported
fulltext_search      : Not supported
handler              : Not supported
index_enable_disable : Not supported
insert_delayed       : Not supported
insert_high_prio     : Not supported
insert_low_prio      : Not supported
lock                 : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
lock_concurrent      : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
optimize_table       : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
repair_table         : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
select_high_prio     : Not supported
show_table_status    : MDEV-13152 - Indeterministic row number in SHOW TABLE STATUS on RocksDB table
tbl_opt_data_dir     : Not supported
tbl_opt_index_dir    : Not supported
type_spatial         : Not supported
type_spatial_indexes : Not supported
update_low_prio      : Not supported
update_ignore        : MDEV-13151 - Indeterministic results of multi-table update on RocksDB tables
update_multi         : MDEV-13151 - Indeterministic results of multi-table update on RocksDB tables
vcol                 : Not supported
{noformat}
{noformat:title=storage_engine/parts subsuite}
alter_table    : MDEV-13153 - Assertion `global_status_var.global_memory_used == 0'
optimize_table : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
repair_table   : MDEV-13148 - LOCK TABLE on RocksDB table fails with a bogus error message
{noformat}
{noformat:title=storage_engine/trx subsuite}
cons_snapshot_serializable : Not supported
level_read_uncommitted     : Not supported
level_serializable         : Not supported
xa_recovery                : MDEV-13155 - XA recovery not supported for RocksDB
{noformat}"
1973,MDEV-12528,MDEV,Elena Stepanova,96830,2017-06-22 21:29:55,"Overview of rdiffs and overlays for include files:

- {{define_engine.inc}}: 
_just engine name, no other changes_
- {{cleanup_engine.inc}}: 
_added restart after each test, whenever {{.rocksdb}} folder is not empty. It is to work around not being able to clean up column families_
- {{mask_engine.inc}}:
_extra substitution {{/ COLLATE\[= \]latin1_bin//}}_
- {{suite.opt}}:
_{{--ignore-db-dirs=.rocksdb --plugin-load=$HA_ROCKSDB_SO --binlog_format=ROW --collation-server=latin1_bin}}_
- {{parts/suite.opt}}:
_{{--ignore-db-dirs=.rocksdb --plugin-load=$HA_ROCKSDB_SO --binlog_format=ROW}}_
- {{trx/suite.opt}}:
_{{--ignore-db-dirs=.rocksdb --plugin-load=$HA_ROCKSDB_SO --binlog_format=ROW}}_

- {{cache_index.rdiff}}:
_the storage engine does not support {{assign_to_keycache}}/{{preload_keys}}_
- {{checksum_table_live.rdiff}}, {{parts/checksum_table.rdiff}}:
_{{CHECKSUM ... QUICK}}} returns {{NULL}}_
- {{index.rdiff}}, {{index_type_btree.rdiff}}:
_{{LSMTREE}} instead of {{BTREE}}, {{Unique index support is disabled when the table has no primary key}}_
- {{index_type_hash.rdiff}}:
_{{LSMTREE}} instead of {{HASH}}, {{Unique index support is disabled when the table has no primary key}}_
- {{misc.rdiff}}:
_{{Unknown storage engine 'InnoDB'}} on a query from {{INFORMATION_SCHEMA}}_
- {{parts/create_table.rdiff}}:
_different {{EXPLAIN PARTITIONS}} output_
- {{show_engine.rdiff}}:
_{{SHOW ENGINE RocksDB STATUS}} output_
- {{show_table_status.rdiff}}:
_different collation_
- {{tbl_opt_insert_method.rdiff}}, {{tbl_opt_union.rdiff}}:
_option is ignored_
- {{tbl_temporary.rdiff}}:
_{{Table storage engine 'ROCKSDB' does not support the create option 'TEMPORARY'}}_
- {{truncate_table.rdiff}}:
_{{HANDLER}} is not supported_
- {{trx/delete.rdiff}}, {{trx/insert.rdiff}}, {{trx/update.rdiff}}:
_{{MyRocks currently does not support ROLLBACK TO SAVEPOINT if modifying rows}}_
- {{trx/level_read_committed.rdiff}}:
_{{Only REPEATABLE READ isolation level is supported for START TRANSACTION WITH CONSISTENT SNAPSHOT in RocksDB Storage Engine}}_
- {{trx/level_repeatable_read.rdiff}}:
_no lock wait timeout on concurrent insert_
- {{type_binary_indexes.rdiff}}:
_{{Unique index support is disabled when the table has no primary key}}_
- {{index_key_block_size.rdiff}}, {{insert_with_keys.rdiff}}, {{replace.rdiff}}, {{type_bit_indexes.rdiff}}, {{type_blob_indexes.rdiff}}, {{type_char_indexes.rdiff}}, {{type_date_time_indexes.rdiff}}, {{type_enum_indexes.rdiff}}, {{type_fixed_indexes.rdiff}}, {{type_float_indexes.rdiff}}, {{type_int_indexes.rdiff}}, {{type_set_indexes.rdiff}}, {{type_text_indexes.rdiff}}, {{update_with_keys.rdiff}}:
_{{Unique index support is disabled when the table has no primary key}}_
- {{type_enum.rdiff}}, {{type_set.rdiff}}:
_different result apparently due to the binary collation_
",5,"Overview of rdiffs and overlays for include files:

- {{define_engine.inc}}: 
_just engine name, no other changes_
- {{cleanup_engine.inc}}: 
_added restart after each test, whenever {{.rocksdb}} folder is not empty. It is to work around not being able to clean up column families_
- {{mask_engine.inc}}:
_extra substitution {{/ COLLATE\[= \]latin1_bin//}}_
- {{suite.opt}}:
_{{--ignore-db-dirs=.rocksdb --plugin-load=$HA_ROCKSDB_SO --binlog_format=ROW --collation-server=latin1_bin}}_
- {{parts/suite.opt}}:
_{{--ignore-db-dirs=.rocksdb --plugin-load=$HA_ROCKSDB_SO --binlog_format=ROW}}_
- {{trx/suite.opt}}:
_{{--ignore-db-dirs=.rocksdb --plugin-load=$HA_ROCKSDB_SO --binlog_format=ROW}}_

- {{cache_index.rdiff}}:
_the storage engine does not support {{assign_to_keycache}}/{{preload_keys}}_
- {{checksum_table_live.rdiff}}, {{parts/checksum_table.rdiff}}:
_{{CHECKSUM ... QUICK}}} returns {{NULL}}_
- {{index.rdiff}}, {{index_type_btree.rdiff}}:
_{{LSMTREE}} instead of {{BTREE}}, {{Unique index support is disabled when the table has no primary key}}_
- {{index_type_hash.rdiff}}:
_{{LSMTREE}} instead of {{HASH}}, {{Unique index support is disabled when the table has no primary key}}_
- {{misc.rdiff}}:
_{{Unknown storage engine 'InnoDB'}} on a query from {{INFORMATION_SCHEMA}}_
- {{parts/create_table.rdiff}}:
_different {{EXPLAIN PARTITIONS}} output_
- {{show_engine.rdiff}}:
_{{SHOW ENGINE RocksDB STATUS}} output_
- {{show_table_status.rdiff}}:
_different collation_
- {{tbl_opt_insert_method.rdiff}}, {{tbl_opt_union.rdiff}}:
_option is ignored_
- {{tbl_temporary.rdiff}}:
_{{Table storage engine 'ROCKSDB' does not support the create option 'TEMPORARY'}}_
- {{truncate_table.rdiff}}:
_{{HANDLER}} is not supported_
- {{trx/delete.rdiff}}, {{trx/insert.rdiff}}, {{trx/update.rdiff}}:
_{{MyRocks currently does not support ROLLBACK TO SAVEPOINT if modifying rows}}_
- {{trx/level_read_committed.rdiff}}:
_{{Only REPEATABLE READ isolation level is supported for START TRANSACTION WITH CONSISTENT SNAPSHOT in RocksDB Storage Engine}}_
- {{trx/level_repeatable_read.rdiff}}:
_no lock wait timeout on concurrent insert_
- {{type_binary_indexes.rdiff}}:
_{{Unique index support is disabled when the table has no primary key}}_
- {{index_key_block_size.rdiff}}, {{insert_with_keys.rdiff}}, {{replace.rdiff}}, {{type_bit_indexes.rdiff}}, {{type_blob_indexes.rdiff}}, {{type_char_indexes.rdiff}}, {{type_date_time_indexes.rdiff}}, {{type_enum_indexes.rdiff}}, {{type_fixed_indexes.rdiff}}, {{type_float_indexes.rdiff}}, {{type_int_indexes.rdiff}}, {{type_set_indexes.rdiff}}, {{type_text_indexes.rdiff}}, {{update_with_keys.rdiff}}:
_{{Unique index support is disabled when the table has no primary key}}_
- {{type_enum.rdiff}}, {{type_set.rdiff}}:
_different result apparently due to the binary collation_
"
1974,MDEV-12533,MDEV,Alexander Barkov,94132,2017-04-20 04:31:49,"Pushed to bb-10.2-ext, 10.3, bb-10.2-compatibility.

",1,"Pushed to bb-10.2-ext, 10.3, bb-10.2-compatibility.

"
1975,MDEV-12542,MDEV,Alexey Botchkov,102154,2017-10-30 04:05:03,http://lists.askmonty.org/pipermail/commits/2017-October/011599.html,1,URL
1976,MDEV-12685,MDEV,Alexander Barkov,94880,2017-05-08 11:56:52,"Merged Jerome's pull request to bb-10.2-ext.
",1,"Merged Jerome's pull request to bb-10.2-ext.
"
1977,MDEV-12836,MDEV,Marko Mäkelä,95517,2017-05-19 08:15:40,"According to an interactive debugging session between myself and [~valerii], the SQL layer is delivering correct metadata (altered_table->found_next_number_field == NULL, and AUTO_INCREMENT_FLAG is not set on the field).

The fix should be in ha_innobase::check_if_supported_inplace_alter() and possibly ha_innobase::prepare_inplace_alter_table().

This should be a simple fix, because InnoDB does not store the AUTO_INCREMENT metadata in its own data dictionary. We will only have to update the in-memory data dictionary cache if this is the only change made in the ALTER TABLE statement.",1,"According to an interactive debugging session between myself and [~valerii], the SQL layer is delivering correct metadata (altered_table->found_next_number_field == NULL, and AUTO_INCREMENT_FLAG is not set on the field).

The fix should be in ha_innobase::check_if_supported_inplace_alter() and possibly ha_innobase::prepare_inplace_alter_table().

This should be a simple fix, because InnoDB does not store the AUTO_INCREMENT metadata in its own data dictionary. We will only have to update the in-memory data dictionary cache if this is the only change made in the ALTER TABLE statement."
1978,MDEV-12836,MDEV,Marko Mäkelä,105473,2018-01-10 13:52:48,"I tried the following variation of this:
{code:sql}
--source include/have_innodb.inc
create table t(id int auto_increment primary key) engine=InnoDB;
alter table t modify column id int, algorithm=inplace;
drop table t;
{code}
This will result in {{ha_alter_info->handler_flags = ALTER_COLUMN_DEFAULT | ALTER_COLUMN_TYPE}}.
To fix this, InnoDB would have to compare the types of existing columns when {{ALTER_COLUMN_TYPE}} is set. I think that this is feasible to fix in MariaDB 10.2, but not in earlier versions (theoretically it could be done already in 10.0).",2,"I tried the following variation of this:
{code:sql}
--source include/have_innodb.inc
create table t(id int auto_increment primary key) engine=InnoDB;
alter table t modify column id int, algorithm=inplace;
drop table t;
{code}
This will result in {{ha_alter_info->handler_flags = ALTER_COLUMN_DEFAULT | ALTER_COLUMN_TYPE}}.
To fix this, InnoDB would have to compare the types of existing columns when {{ALTER_COLUMN_TYPE}} is set. I think that this is feasible to fix in MariaDB 10.2, but not in earlier versions (theoretically it could be done already in 10.0)."
1979,MDEV-12836,MDEV,Julien Muchembled,105474,2018-01-10 14:46:24,What about TokuDB and RocksDB ? Is it possible to implement something generic ?,3,What about TokuDB and RocksDB ? Is it possible to implement something generic ?
1980,MDEV-12836,MDEV,Marko Mäkelä,106889,2018-02-06 13:56:14,"I think that the {{handler::inplace_alter_table()}} interface is already as generic as it gets. The storage engines that support {{AUTO_INCREMENT}} do need to be aware of the metadata changes. Theoretically, news flag could be introduced for the removal or addition of {{AUTO_INCREMENT}} attributes, but in the end each {{ALGORITHM=INPLACE}} capable storage engine would still have to do something about it.

This ticket will only cover the change for InnoDB.",4,"I think that the {{handler::inplace_alter_table()}} interface is already as generic as it gets. The storage engines that support {{AUTO_INCREMENT}} do need to be aware of the metadata changes. Theoretically, news flag could be introduced for the removal or addition of {{AUTO_INCREMENT}} attributes, but in the end each {{ALGORITHM=INPLACE}} capable storage engine would still have to do something about it.

This ticket will only cover the change for InnoDB."
1981,MDEV-12836,MDEV,Marko Mäkelä,120682,2018-12-12 14:50:07,"[~kevg], can you fix this? I think that it is more feasible to fix in 10.3, which introduced the {{ALGORITHM=INSTANT}} keyword. Or only 10.4, if the code is too different from 10.3.",5,"[~kevg], can you fix this? I think that it is more feasible to fix in 10.3, which introduced the {{ALGORITHM=INSTANT}} keyword. Or only 10.4, if the code is too different from 10.3."
1982,MDEV-12836,MDEV,Marko Mäkelä,122404,2019-01-25 09:43:42,The changes are mostly outside InnoDB. The InnoDB changes look mostly OK to me.,6,The changes are mostly outside InnoDB. The InnoDB changes look mostly OK to me.
1983,MDEV-12836,MDEV,Eugene Kosov,122924,2019-02-05 12:05:27,"According to [~serg] review this ticket not only about INSTANT operation which changes metadata only, but also about converting non-unique index to unique and generating/fixing AUTO_INCREMENT values like in this test case:

{code:sql}
  create table t1 (a int not null, index (a));
   insert t1 values (0),(0),(0);
   alter table t1 modify a int auto_increment;
   select * from t1;
{code}
",7,"According to [~serg] review this ticket not only about INSTANT operation which changes metadata only, but also about converting non-unique index to unique and generating/fixing AUTO_INCREMENT values like in this test case:

{code:sql}
  create table t1 (a int not null, index (a));
   insert t1 values (0),(0),(0);
   alter table t1 modify a int auto_increment;
   select * from t1;
{code}
"
1984,MDEV-12836,MDEV,Sergei Golubchik,123187,2019-02-12 14:15:33,"No, this issue is only about the INSTANT operation that changes metadata only — namely about removing AUTO_INCREMENT from a column.

Adding AUTO_INCREMENT is not metadata-only operation, and it's beyond the scope of this issue.",8,"No, this issue is only about the INSTANT operation that changes metadata only — namely about removing AUTO_INCREMENT from a column.

Adding AUTO_INCREMENT is not metadata-only operation, and it's beyond the scope of this issue."
1985,MDEV-12836,MDEV,Eugene Kosov,123198,2019-02-12 16:09:57,"I'm a bit confused now. What to do with https://github.com/MariaDB/server/pull/1125 ?
Adding {{AUTO_INCREMENT}} when data is not need to be changed can and thus should be {{INSTANT}}, right? No need to rebuild in that case.

If generating values along with adding {{AUTO_INCREMENT}} is a different task could you show me an issue? Or should I create it?",9,"I'm a bit confused now. What to do with URL ?
Adding {{AUTO_INCREMENT}} when data is not need to be changed can and thus should be {{INSTANT}}, right? No need to rebuild in that case.

If generating values along with adding {{AUTO_INCREMENT}} is a different task could you show me an issue? Or should I create it?"
1986,MDEV-12836,MDEV,Sergei Golubchik,123633,2019-02-20 17:42:35,"bq. What to do with https://github.com/MariaDB/server/pull/1125

Change it to be in scope of this MDEV. Only handle *removal* of {{AUTO_INCREMENT}}, not *adding*.

bq. Adding {{AUTO_INCREMENT}} when data is not need to be changed can and thus should be INSTANT, right?

Yes. But you need to *know* that the data does not need to be changed. It may be possible, but it's definitely more complex than ""simply do it INSTANT"" like for the removal of {{AUTO_INCREMENT}}. If you want to do it — sure, please, create an MDEV for INSTANT adding ot {{AUTO_INCREMENT}} and do it there. Make sure it corectly handles the test case from above.",10,"bq. What to do with URL

Change it to be in scope of this MDEV. Only handle *removal* of {{AUTO_INCREMENT}}, not *adding*.

bq. Adding {{AUTO_INCREMENT}} when data is not need to be changed can and thus should be INSTANT, right?

Yes. But you need to *know* that the data does not need to be changed. It may be possible, but it's definitely more complex than ""simply do it INSTANT"" like for the removal of {{AUTO_INCREMENT}}. If you want to do it — sure, please, create an MDEV for INSTANT adding ot {{AUTO_INCREMENT}} and do it there. Make sure it corectly handles the test case from above."
1987,MDEV-12836,MDEV,Eugene Kosov,123703,2019-02-21 20:56:19,"Thanks for clarifications.

??Yes. But you need to know that the data does not need to be changed. It may be possible, but it's definitely more complex than ""simply do it INSTANT"" like for the removal of AUTO_INCREMENT.??

I can see now it's problematic even when interesting index is unique:

{noformat}
create table t1 (a int not null, unique index (a));
insert t1 values (0),(1),(2);
alter table t1 modify a int auto_increment;
select * from t1;
drop table t1;
{noformat}

{noformat}
query 'alter table t1 modify a int auto_increment' failed: 1062: ALTER TABLE causes auto_increment resequencing, resulting in duplicate entry '1' for key 'a'
{noformat}",11,"Thanks for clarifications.

??Yes. But you need to know that the data does not need to be changed. It may be possible, but it's definitely more complex than ""simply do it INSTANT"" like for the removal of AUTO_INCREMENT.??

I can see now it's problematic even when interesting index is unique:

{noformat}
create table t1 (a int not null, unique index (a));
insert t1 values (0),(1),(2);
alter table t1 modify a int auto_increment;
select * from t1;
drop table t1;
{noformat}

{noformat}
query 'alter table t1 modify a int auto_increment' failed: 1062: ALTER TABLE causes auto_increment resequencing, resulting in duplicate entry '1' for key 'a'
{noformat}"
1988,MDEV-12894,MDEV,Sergei Golubchik,104022,2017-12-04 14:01:35,"Assorted notes:
* SQL Standard only has {{FOR SYSTEM_TIME}} with {{AS OF ...| FROM ... TO ...| BETWEEN ... AND ...}} and it can only directly follow a table name in the {{FROM}} clause. Does not apply to a view, does not apply to a query as a whole. Whenever {{FOR SYSTEM_TIME}} is missing, {{FOR SYSTEM_TIME AS OF CURRENT_TIMESTAMP}} is implied.
* SQL Server has {{FOR SYSTEM_TIME ALL}}
* DB2 supports {{FOR SYSTEM_TIME}} for views, if these views use system-versioned tables and do not use {{FOR SYSTEM_TIME}} inside.
* DB2 has a session setting like {{SET CURRENT TEMPORAL SYSTEM_TIME = current timestamp – 1 MONTH}}",1,"Assorted notes:
* SQL Standard only has {{FOR SYSTEM_TIME}} with {{AS OF ...| FROM ... TO ...| BETWEEN ... AND ...}} and it can only directly follow a table name in the {{FROM}} clause. Does not apply to a view, does not apply to a query as a whole. Whenever {{FOR SYSTEM_TIME}} is missing, {{FOR SYSTEM_TIME AS OF CURRENT_TIMESTAMP}} is implied.
* SQL Server has {{FOR SYSTEM_TIME ALL}}
* DB2 supports {{FOR SYSTEM_TIME}} for views, if these views use system-versioned tables and do not use {{FOR SYSTEM_TIME}} inside.
* DB2 has a session setting like {{SET CURRENT TEMPORAL SYSTEM_TIME = current timestamp – 1 MONTH}}"
1989,MDEV-12913,MDEV,Alexander Barkov,95903,2017-05-29 12:55:06,"Pushed to bb-10.2-compatibility
",1,"Pushed to bb-10.2-compatibility
"
1990,MDEV-12985,MDEV,Sergei Petrunia,96405,2017-06-09 09:32:35,"[[~dthompson], a question from me and [~varun]:  Does ColumnStore need just ""Ordered-set aggregates as window functions"", or it needs both ""Ordered-set aggregates as window functions"" and  ""Ordered-set aggregates"".   IIRC it supported both? ",1,"[[~dthompson], a question from me and [~varun]:  Does ColumnStore need just ""Ordered-set aggregates as window functions"", or it needs both ""Ordered-set aggregates as window functions"" and  ""Ordered-set aggregates"".   IIRC it supported both? "
1991,MDEV-12985,MDEV,David Thompson,96418,2017-06-09 20:31:53,"ColumnStore 1.0 supported these only as window functions (and i learned a new term!). If you can easily add as a regular aggregate that's nice to have but not required.

Loop in [~David.Hall] when you have a rough design as we will still need to reimplement the bottom end of the implementation on the columnstore side.",2,"ColumnStore 1.0 supported these only as window functions (and i learned a new term!). If you can easily add as a regular aggregate that's nice to have but not required.

Loop in [~David.Hall] when you have a rough design as we will still need to reimplement the bottom end of the implementation on the columnstore side."
1992,MDEV-12985,MDEV,Varun Gupta,96445,2017-06-11 12:38:09,"{noformat}
The grammar being:
	
<inverse distribution function> ::=
|<inverse distribution function type> <left paren> 
<inverse distribution function argument> <right paren><within group specification>					

<inverse distribution function argument> ::=
  <numeric value expression>					
 
<inverse distribution function type> ::=
   PERCENTILE_CONT					
 | PERCENTILE_DISC
					
<within group specification> ::=
  WITHIN GROUP <left paren> ORDER BY <sort specification> <right paren>
 
{noformat}

",3,"{noformat}
The grammar being:
	
 ::=
|  
 					

 ::=
  					
 
 ::=
   PERCENTILE_CONT					
 | PERCENTILE_DISC
					
 ::=
  WITHIN GROUP  ORDER BY  
 
{noformat}

"
1993,MDEV-12985,MDEV,Varun Gupta,96446,2017-06-11 12:40:38,"*Specification for the percentile functions*

{noformat}

For the <inverse distribution function>
a)  The <within group specification> shall contain a single <sort specification>
b)  The <inverse distribution function> shall not contain a <window function>, a 
     <set  function specification>, or a <query expression>
      c) Let DT be the declared type of the <value expression> simply contained in the 
          <sort specification>.
d)  DT shall be numeric or interval.
e)  The declared type of the result is
     Case:
      i)  If DT is numeric, then approximate numeric with implementation-defined precision.
     ii)  If DT is interval, then DT. 

{noformat}

",4,"*Specification for the percentile functions*

{noformat}

For the 
a)  The  shall contain a single 
b)  The  shall not contain a , a 
     , or a 
      c) Let DT be the declared type of the  simply contained in the 
          .
d)  DT shall be numeric or interval.
e)  The declared type of the result is
     Case:
      i)  If DT is numeric, then approximate numeric with implementation-defined precision.
     ii)  If DT is interval, then DT. 

{noformat}

"
1994,MDEV-12985,MDEV,Varun Gupta,96484,2017-06-12 18:19:21,"More specifications  for the inverse distribution function argument

{noformat}
a) Let NVE be the value of the <inverse distribution function argument> 
b) If NVE is the null value, then the result is the null value. 
c) If NVE is less than 0 (zero) or greater than 1 (one), then an exception condition is raised: data exception — numeric value out of range.
{noformat}
",5,"More specifications  for the inverse distribution function argument

{noformat}
a) Let NVE be the value of the  
b) If NVE is the null value, then the result is the null value. 
c) If NVE is less than 0 (zero) or greater than 1 (one), then an exception condition is raised: data exception — numeric value out of range.
{noformat}
"
1995,MDEV-12985,MDEV,Varun Gupta,96555,2017-06-14 16:07:50,"Computation for PERCENTILE_CONT

#   Get the number of rows in the partition, denoted by N 
#   RN = p*(N-1), where p denotes the argument to the PERCENTILE_CONT function
#   calculate the FRN(floor row number) and CRN(column row number  for the group( FRN= floor(RN) and CRN = ceil(RN))
#   look up rows FRN and CRN
#  If (CRN = FRN = RN) then the result is (value of expression from row at RN)
# Otherwise the result is
    (CRN - RN) * (value of expression for row at FRN) +
    (RN - FRN) * (value of expression for row at CRN)  

",6,"Computation for PERCENTILE_CONT

#   Get the number of rows in the partition, denoted by N 
#   RN = p*(N-1), where p denotes the argument to the PERCENTILE_CONT function
#   calculate the FRN(floor row number) and CRN(column row number  for the group( FRN= floor(RN) and CRN = ceil(RN))
#   look up rows FRN and CRN
#  If (CRN = FRN = RN) then the result is (value of expression from row at RN)
# Otherwise the result is
    (CRN - RN) * (value of expression for row at FRN) +
    (RN - FRN) * (value of expression for row at CRN)  

"
1996,MDEV-12985,MDEV,Varun Gupta,96556,2017-06-14 16:09:04,"Computation for PERCENTILE_DISC:

# Get the number of rows in the partition
# walk through the partition, in order, until we find the the first row with  CUME_DIST() > function_argument
# MEDIAN() = PERCENTILE_DISC(0.5",7,"Computation for PERCENTILE_DISC:

# Get the number of rows in the partition
# walk through the partition, in order, until we find the the first row with  CUME_DIST() > function_argument
# MEDIAN() = PERCENTILE_DISC(0.5"
1997,MDEV-12985,MDEV,Varun Gupta,100466,2017-09-20 12:21:49,"Datetime fields are not supported in the first iteration of  percentile functions, have created a seperate issue for it.(MDEV-13854). After MDEV-13854 , we would have datetime fields support in percentile functions",8,"Datetime fields are not supported in the first iteration of  percentile functions, have created a seperate issue for it.(MDEV-13854). After MDEV-13854 , we would have datetime fields support in percentile functions"
1998,MDEV-12985,MDEV,Vicențiu Ciorbaru,102128,2017-10-28 14:41:55,Minor coding style fixes. Rebase and merge into 10.3 once BB clears it.,9,Minor coding style fixes. Rebase and merge into 10.3 once BB clears it.
1999,MDEV-12985,MDEV,Ján Regeš,103875,2017-11-30 09:02:12,"@VicentiuCiorbaru - does it mean, that MEDIAN function will be in MariaDB 10.3?",10,"@VicentiuCiorbaru - does it mean, that MEDIAN function will be in MariaDB 10.3?"
2000,MDEV-13073,MDEV,Andrei Elkin,103802,2017-11-29 11:35:59,"Axel, hallo.

The latest code patch is in `bb-10.2-semisync_ali` branch. Monty suggests to
have 1 and 2 slave separate runs. Also to compare against 5.7.
I am all ears should you have any question.

Cheers,

Andrei
",1,"Axel, hallo.

The latest code patch is in `bb-10.2-semisync_ali` branch. Monty suggests to
have 1 and 2 slave separate runs. Also to compare against 5.7.
I am all ears should you have any question.

Cheers,

Andrei
"
2001,MDEV-13073,MDEV,Axel Schwenke,106071,2018-01-23 14:23:42,no idea why this is still assigned to me -> back to monty,2,no idea why this is still assigned to me -> back to monty
2002,MDEV-13073,MDEV,Michael Widenius,107357,2018-02-15 08:15:04,"This patch is already in 10.3 since November.

However, it needs to be benchmarked to ensure that we don't need to do any additional
work on it.
",3,"This patch is already in 10.3 since November.

However, it needs to be benchmarked to ensure that we don't need to do any additional
work on it.
"
2003,MDEV-13073,MDEV,Axel Schwenke,108961,2018-03-27 12:47:57,"I've rerun the benchmark for 10.3.5 and it is actually even faster than the branches I did before (probably due to latest InnoDB enhancements). Hence this can be shipped as is in 10.3.

{noformat}
# connection 03 -> bb-10.2-semisync_ali
# connection 07 -> bb-10.3-semisync
# connection 08 -> 10.3.5

# workload 01 -> OLTP(rw)

#thd    03      07      08
1       3938.6  3852.7  3869.5
2       7459.2  7261.3  7199.9
4       13119   12691   12879
8       25144   24791   24911
16      49818   48869   49342
32      96561   93955   94039
64      150180  142943  145888
128     170317  164725  164240
256     117872  115453  116282

# workload 02 -> insert

#thd    03      07      08
1       2758.2  2689.2  2589.0
2       4795.1  4714.3  4498.6
4       8233.8  8139.9  8078.8
8       15138   14820   14609
16      28524   28310   29013
32      47826   47659   49665
64      65291   63573   68569
128     66467   68718   70399
256     52182   55944   56501

# workload 03 -> index-update

#thd    03      07      08
1       2739.7  2625.6  2659.2
2       4715.1  4585.8  4478.5
4       8216.6  8207.0  7985.2
8       15207   15235   15387
16      30797   30867   31076
32      60730   60129   60990
64      96557   96185   97013
128     36013   109571  112897
256     36933   59615   46155

# workload 04 -> non-index-update

#thd    03      07      08
1       2628.6  2615.8  2633.5
2       4631.7  4559.3  4707.4
4       8132.6  8382.6  8563.9
8       15658   15538   15903
16      29896   29505   31163
32      51265   51247   58155
64      58164   58755   90635
128     57889   57987   90131
256     55673   55748   68406
{noformat}",4,"I've rerun the benchmark for 10.3.5 and it is actually even faster than the branches I did before (probably due to latest InnoDB enhancements). Hence this can be shipped as is in 10.3.

{noformat}
# connection 03 -> bb-10.2-semisync_ali
# connection 07 -> bb-10.3-semisync
# connection 08 -> 10.3.5

# workload 01 -> OLTP(rw)

#thd    03      07      08
1       3938.6  3852.7  3869.5
2       7459.2  7261.3  7199.9
4       13119   12691   12879
8       25144   24791   24911
16      49818   48869   49342
32      96561   93955   94039
64      150180  142943  145888
128     170317  164725  164240
256     117872  115453  116282

# workload 02 -> insert

#thd    03      07      08
1       2758.2  2689.2  2589.0
2       4795.1  4714.3  4498.6
4       8233.8  8139.9  8078.8
8       15138   14820   14609
16      28524   28310   29013
32      47826   47659   49665
64      65291   63573   68569
128     66467   68718   70399
256     52182   55944   56501

# workload 03 -> index-update

#thd    03      07      08
1       2739.7  2625.6  2659.2
2       4715.1  4585.8  4478.5
4       8216.6  8207.0  7985.2
8       15207   15235   15387
16      30797   30867   31076
32      60730   60129   60990
64      96557   96185   97013
128     36013   109571  112897
256     36933   59615   46155

# workload 04 -> non-index-update

#thd    03      07      08
1       2628.6  2615.8  2633.5
2       4631.7  4559.3  4707.4
4       8132.6  8382.6  8563.9
8       15658   15538   15903
16      29896   29505   31163
32      51265   51247   58155
64      58164   58755   90635
128     57889   57987   90131
256     55673   55748   68406
{noformat}"
2004,MDEV-13095,MDEV,Ranjan Ghosh,113189,2018-06-28 16:17:28,"Another use-case: I'm trying to write a script to automate restoring a database on our webserver which hosts multiple different websites - each of course with their own user/scheme. To restore the database properly, it needs to be wiped first. Unfortunately, after I deleted all tables, sometimes new website calls create new data/tables which subsequently prevent the restore operation. I dont want to stop the whole webserver because other websites should continue running. It would be really great if I could just LOCK the corresponding account to prevent new calls from coming in and wait until old calls have ceased. Then I knew I could easily restore the schema without any problems/interruptions and UNLOCK the account afterwards. I search if such a method is available and found that MySQL has it but not my much preferred MariaDB.",1,"Another use-case: I'm trying to write a script to automate restoring a database on our webserver which hosts multiple different websites - each of course with their own user/scheme. To restore the database properly, it needs to be wiped first. Unfortunately, after I deleted all tables, sometimes new website calls create new data/tables which subsequently prevent the restore operation. I dont want to stop the whole webserver because other websites should continue running. It would be really great if I could just LOCK the corresponding account to prevent new calls from coming in and wait until old calls have ceased. Then I knew I could easily restore the schema without any problems/interruptions and UNLOCK the account afterwards. I search if such a method is available and found that MySQL has it but not my much preferred MariaDB."
2005,MDEV-13095,MDEV,Teodor Mircea Ionita,118709,2018-11-05 13:34:37,"I would be interested in tackling this. For a start, what is the desired syntax for implementing this? I looked briefly into the SQL standard and there's no definition for that. Should we use the existing  ALTER USER method or add something new similar to LOCK TABLE?

Regarding the actual locking mechanism, are we looking at a new user flag or manipulate the hash as suggested on the mailing list (like linux does with passwd/shadow files)? The latter might not be a good approach due to the different hashing methods/libraries that are or might be used in the future, as Vicentiu has hinted.

PS: Let me know if these questions would be better directed at the maria-discuss list.",2,"I would be interested in tackling this. For a start, what is the desired syntax for implementing this? I looked briefly into the SQL standard and there's no definition for that. Should we use the existing  ALTER USER method or add something new similar to LOCK TABLE?

Regarding the actual locking mechanism, are we looking at a new user flag or manipulate the hash as suggested on the mailing list (like linux does with passwd/shadow files)? The latter might not be a good approach due to the different hashing methods/libraries that are or might be used in the future, as Vicentiu has hinted.

PS: Let me know if these questions would be better directed at the maria-discuss list."
2006,MDEV-13095,MDEV,Ranjan Ghosh,118712,2018-11-05 13:59:27,"I'm just a bystander, but I'd like to note that IMHO it might make sense to design this feature similar to what already exists in recent MySQL versions:

https://stackoverflow.com/questions/40604087/how-to-lock-a-users-account-in-mysql-5-7
https://dev.mysql.com/doc/mysql-security-excerpt/5.7/en/account-locking.html
",3,"I'm just a bystander, but I'd like to note that IMHO it might make sense to design this feature similar to what already exists in recent MySQL versions:

URL
URL
"
2007,MDEV-13122,MDEV,Sergei Petrunia,106153,2018-01-24 10:05:36,Note from [~wlad]: Alibaba has added TokuDB backup support into xtrabackup: https://github.com/alibaba/AliSQLBackup/commit/26573de135d115b100dbbfb9698274463ade5c8d . So we are not the first.,1,Note from [~wlad]: Alibaba has added TokuDB backup support into xtrabackup: URL . So we are not the first.
2008,MDEV-13122,MDEV,Vladislav Vaintroub,110762,2018-05-09 16:24:52,"We'll only support full backup in the first iteration, to incrementals or partials .
what needs to be done

During mariabackup --backup

when mariabackup --backup starts
check if rocksdb plugin is installed

*in the ""synch"" phase, under ""flush tables with readlock, lock binlog""
{noformat}
 if (rocksdb_present) {
   if (backup to directory) {
     execute SET GLOBAL rocksdb_create_checkpoint=<backup-dir>/#rocksdb
     // this will create rocksdb subdirectory in the backup dir, we're done
  }
  else  {// backup to  stdio with xbstream
     execute SET GLOBAL rocksdb_create_checkpoint=<temp-dir>/#rocksdb
     copy <temp-dir>/#rocksdb  recursively in xbstream format to stdout
     remove <temp-dir>/#rocksdb 
  }
}
{noformat}
Do we need a parameter for whether to backup rocksdb or not? 
Do we need a parameter for rocksdb temp-dir, in case backup is streaming?. It is  beneficial temp directory is on the same volume, checkpoint will create hardlinks rather than copy.


During  mariabackup copy-back
 * copy <backup-dir>/#rocksdb to the rocksdb-data-dir (in absense of this parameter to <target-dir>/#rocksdb)

That's it.
  ",2,"We'll only support full backup in the first iteration, to incrementals or partials .
what needs to be done

During mariabackup --backup

when mariabackup --backup starts
check if rocksdb plugin is installed

*in the ""synch"" phase, under ""flush tables with readlock, lock binlog""
{noformat}
 if (rocksdb_present) {
   if (backup to directory) {
     execute SET GLOBAL rocksdb_create_checkpoint=/#rocksdb
     // this will create rocksdb subdirectory in the backup dir, we're done
  }
  else  {// backup to  stdio with xbstream
     execute SET GLOBAL rocksdb_create_checkpoint=/#rocksdb
     copy /#rocksdb  recursively in xbstream format to stdout
     remove /#rocksdb 
  }
}
{noformat}
Do we need a parameter for whether to backup rocksdb or not? 
Do we need a parameter for rocksdb temp-dir, in case backup is streaming?. It is  beneficial temp directory is on the same volume, checkpoint will create hardlinks rather than copy.


During  mariabackup copy-back
 * copy /#rocksdb to the rocksdb-data-dir (in absense of this parameter to /#rocksdb)

That's it.
  "
2009,MDEV-13141,MDEV,Teodor Mircea Ionita,113887,2018-07-13 10:10:53,"I have used DockerLatentWorker successfully for all builders named **-docker* on the new *buildbot.mariadb.org* hosted at DigitalOcean:

http://buildbot.mariadb.org/#/builders

The master uses docker-py to communicate directly with the docker hosts *do-bbw1,2-docker*, residing on separate machines and properly executes builds as configured in the master config file using docker images specifically crafted for each target system. This system can be extended and improved with more logic for load balancing and automatic scaling further on.

I'm therefore marking this task as done and suggest using MDEV-8244 to track progress for the new buildbot installation instead.",1,"I have used DockerLatentWorker successfully for all builders named **-docker* on the new *buildbot.mariadb.org* hosted at DigitalOcean:

URL

The master uses docker-py to communicate directly with the docker hosts *do-bbw1,2-docker*, residing on separate machines and properly executes builds as configured in the master config file using docker images specifically crafted for each target system. This system can be extended and improved with more logic for load balancing and automatic scaling further on.

I'm therefore marking this task as done and suggest using MDEV-8244 to track progress for the new buildbot installation instead."
2010,MDEV-13292,MDEV,Alexander Barkov,98730,2017-08-15 13:40:42,"Pushed to bb-10.2-ext and 10.3.
",1,"Pushed to bb-10.2-ext and 10.3.
"
2011,MDEV-13298,MDEV,Alexander Barkov,97530,2017-07-12 08:03:21,"Pushed to bb-10.2-ext
",1,"Pushed to bb-10.2-ext
"
2012,MDEV-13302,MDEV,Alexander Barkov,97557,2017-07-12 18:55:47,"Pushed to bb-10.2-ext
",1,"Pushed to bb-10.2-ext
"
2013,MDEV-13342,MDEV,Alice Sherepa,97910,2017-07-24 12:46:55,"1)If we have compressed data types on master and compressed on slave, then we get wrong data on slave, for all BLOB and TEXT data types(TINY,...), works correct with VARCHAR and VARBINARY

The same if we have compressed on master, but not compressed on slave.
If we have compressed data on slave, but not compressed on master, then the results on slave are correct.

2) tables with partitions and compressed data types crash, when 
{code:sql}
select/update/delete ... where a=""..."";     # column a- compressed (varchar or varbinary)
{code}
no crash and correct result if we use compressed column with functions LIKE, NOT LIKE, LENGTH(), ...
example of stack trace {noformat}
Thread 1 (Thread 0x7fec02075700 (LWP 26424)):
#0  __pthread_kill (threadid=<optimized out>, signo=6) at ../sysdeps/unix/sysv/linux/pthread_kill.c:62
#1  0x0000556749cc8ea6 in my_write_core (sig=6) at /home/alice/git/10.3.sv/mysys/stacktrace.c:477
#2  0x0000556749553ca5 in handle_fatal_signal (sig=6) at /home/alice/git/10.3.sv/sql/signal_handler.cc:299
#3  <signal handler called>
#4  0x00007fec07dba428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#5  0x00007fec07dbc02a in __GI_abort () at abort.c:89
#6  0x00007fec07db2bd7 in __assert_fail_base (fmt=<optimized out>, assertion=assertion@entry=0x556749e85028 ""0"", file=file@entry=0x556749e84fe0 ""/home/alice/git/10.3.sv/sql/field.h"", line=line@entry=3213, function=function@entry=0x556749e8a840 <Field_varstring_compressed::get_key_image(unsigned char*, unsigned int, Field::imagetype)::__PRETTY_FUNCTION__> ""virtual uint Field_varstring_compressed::get_key_image(uchar*, uint, Field::imagetype)"") at assert.c:92
#7  0x00007fec07db2c82 in __GI___assert_fail (assertion=0x556749e85028 ""0"", file=0x556749e84fe0 ""/home/alice/git/10.3.sv/sql/field.h"", line=3213, function=0x556749e8a840 <Field_varstring_compressed::get_key_image(unsigned char*, unsigned int, Field::imagetype)::__PRETTY_FUNCTION__> ""virtual uint Field_varstring_compressed::get_key_image(uchar*, uint, Field::imagetype)"") at assert.c:101
#8  0x0000556749547aa9 in Field_varstring_compressed::get_key_image (this=0x7febf0036d40, buff=0x7febf0054f91 '\245' <repeats 200 times>..., length=1000, type_arg=Field::itRAW) at /home/alice/git/10.3.sv/sql/field.h:3213
#9  0x00005567496acdac in Item_bool_func::get_mm_leaf (this=0x7febf0015168, param=0x7fec02072a90, field=0x7febf0036d40, key_part=0x7febf00546f8, type=Item_func::EQ_FUNC, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/opt_range.cc:8123
#10 0x00005567496abdfc in Item_bool_func::get_mm_parts (this=0x7febf0015168, param=0x7fec02072a90, field=0x7febf0036d40, type=Item_func::EQ_FUNC, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/opt_range.cc:7841
#11 0x00005567492450f5 in Item_bool_func2_with_rev::get_func_mm_tree (this=0x7febf0015168, param=0x7fec02072a90, field=0x7febf0036d40, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/item_cmpfunc.h:442
#12 0x00005567496aaca0 in Item_bool_func::get_full_func_mm_tree (this=0x7febf0015168, param=0x7fec02072a90, field_item=0x7febf0014fd8, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/opt_range.cc:7520
#13 0x0000556749244e0c in Item_bool_func::get_full_func_mm_tree_for_args (this=0x7febf0015168, param=0x7fec02072a90, item=0x7febf0014fd8, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/item_cmpfunc.h:189
#14 0x0000556749245274 in Item_bool_func2_with_rev::get_mm_tree (this=0x7febf0015168, param=0x7fec02072a90, cond_ptr=0x7fec020729e8) at /home/alice/git/10.3.sv/sql/item_cmpfunc.h:470
#15 0x00005567496a12be in prune_partitions (thd=0x7febf0000b00, table=0x7febf003c7d0, pprune_cond=0x7febf0015168) at /home/alice/git/10.3.sv/sql/opt_range.cc:3483
#16 0x00005567496f3b44 in mysql_delete (thd=0x7febf0000b00, table_list=0x7febf00149c0, conds=0x7febf0015168, order_list=0x7febf0005020, limit=18446744073709551615, options=0, result=0x0) at /home/alice/git/10.3.sv/sql/sql_delete.cc:392
#17 0x00005567492b1e1e in mysql_execute_command (thd=0x7febf0000b00) at /home/alice/git/10.3.sv/sql/sql_parse.cc:4726
#18 0x00005567492bc290 in mysql_parse (thd=0x7febf0000b00, rawbuf=0x7febf00148d8 ""DELETE FROM t1 where a=\""a\"""", length=26, parser_state=0x7fec02074210, is_com_multi=false, is_next_command=false) at /home/alice/git/10.3.sv/sql/sql_parse.cc:7897
#19 0x00005567492a9d41 in dispatch_command (command=COM_QUERY, thd=0x7febf0000b00, packet=0x7febf0165e01 ""DELETE FROM t1 where a=\""a\"""", packet_length=26, is_com_multi=false, is_next_command=false) at /home/alice/git/10.3.sv/sql/sql_parse.cc:1814
#20 0x00005567492a86de in do_command (thd=0x7febf0000b00) at /home/alice/git/10.3.sv/sql/sql_parse.cc:1377
#21 0x00005567493f40e2 in do_handle_one_connection (connect=0x55674bdf78a0) at /home/alice/git/10.3.sv/sql/sql_connect.cc:1354
#22 0x00005567493f3e62 in handle_one_connection (arg=0x55674bdf78a0) at /home/alice/git/10.3.sv/sql/sql_connect.cc:1260
#23 0x000055674978ba52 in pfs_spawn_thread (arg=0x55674bdffc70) at /home/alice/git/10.3.sv/storage/perfschema/pfs.cc:1862
#24 0x00007fec089f76ba in start_thread (arg=0x7fec02075700) at pthread_create.c:333
#25 0x00007fec07e8c3dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{noformat}
 [^column_compression_parts.test]  [^column_compression_rpl.test ] ",1,"1)If we have compressed data types on master and compressed on slave, then we get wrong data on slave, for all BLOB and TEXT data types(TINY,...), works correct with VARCHAR and VARBINARY

The same if we have compressed on master, but not compressed on slave.
If we have compressed data on slave, but not compressed on master, then the results on slave are correct.

2) tables with partitions and compressed data types crash, when 
{code:sql}
select/update/delete ... where a=""..."";     # column a- compressed (varchar or varbinary)
{code}
no crash and correct result if we use compressed column with functions LIKE, NOT LIKE, LENGTH(), ...
example of stack trace {noformat}
Thread 1 (Thread 0x7fec02075700 (LWP 26424)):
#0  __pthread_kill (threadid=, signo=6) at ../sysdeps/unix/sysv/linux/pthread_kill.c:62
#1  0x0000556749cc8ea6 in my_write_core (sig=6) at /home/alice/git/10.3.sv/mysys/stacktrace.c:477
#2  0x0000556749553ca5 in handle_fatal_signal (sig=6) at /home/alice/git/10.3.sv/sql/signal_handler.cc:299
#3  
#4  0x00007fec07dba428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#5  0x00007fec07dbc02a in __GI_abort () at abort.c:89
#6  0x00007fec07db2bd7 in __assert_fail_base (fmt=, assertion=assertion@entry=0x556749e85028 ""0"", file=file@entry=0x556749e84fe0 ""/home/alice/git/10.3.sv/sql/field.h"", line=line@entry=3213, function=function@entry=0x556749e8a840  ""virtual uint Field_varstring_compressed::get_key_image(uchar*, uint, Field::imagetype)"") at assert.c:92
#7  0x00007fec07db2c82 in __GI___assert_fail (assertion=0x556749e85028 ""0"", file=0x556749e84fe0 ""/home/alice/git/10.3.sv/sql/field.h"", line=3213, function=0x556749e8a840  ""virtual uint Field_varstring_compressed::get_key_image(uchar*, uint, Field::imagetype)"") at assert.c:101
#8  0x0000556749547aa9 in Field_varstring_compressed::get_key_image (this=0x7febf0036d40, buff=0x7febf0054f91 '\245' ..., length=1000, type_arg=Field::itRAW) at /home/alice/git/10.3.sv/sql/field.h:3213
#9  0x00005567496acdac in Item_bool_func::get_mm_leaf (this=0x7febf0015168, param=0x7fec02072a90, field=0x7febf0036d40, key_part=0x7febf00546f8, type=Item_func::EQ_FUNC, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/opt_range.cc:8123
#10 0x00005567496abdfc in Item_bool_func::get_mm_parts (this=0x7febf0015168, param=0x7fec02072a90, field=0x7febf0036d40, type=Item_func::EQ_FUNC, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/opt_range.cc:7841
#11 0x00005567492450f5 in Item_bool_func2_with_rev::get_func_mm_tree (this=0x7febf0015168, param=0x7fec02072a90, field=0x7febf0036d40, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/item_cmpfunc.h:442
#12 0x00005567496aaca0 in Item_bool_func::get_full_func_mm_tree (this=0x7febf0015168, param=0x7fec02072a90, field_item=0x7febf0014fd8, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/opt_range.cc:7520
#13 0x0000556749244e0c in Item_bool_func::get_full_func_mm_tree_for_args (this=0x7febf0015168, param=0x7fec02072a90, item=0x7febf0014fd8, value=0x7febf00150d8) at /home/alice/git/10.3.sv/sql/item_cmpfunc.h:189
#14 0x0000556749245274 in Item_bool_func2_with_rev::get_mm_tree (this=0x7febf0015168, param=0x7fec02072a90, cond_ptr=0x7fec020729e8) at /home/alice/git/10.3.sv/sql/item_cmpfunc.h:470
#15 0x00005567496a12be in prune_partitions (thd=0x7febf0000b00, table=0x7febf003c7d0, pprune_cond=0x7febf0015168) at /home/alice/git/10.3.sv/sql/opt_range.cc:3483
#16 0x00005567496f3b44 in mysql_delete (thd=0x7febf0000b00, table_list=0x7febf00149c0, conds=0x7febf0015168, order_list=0x7febf0005020, limit=18446744073709551615, options=0, result=0x0) at /home/alice/git/10.3.sv/sql/sql_delete.cc:392
#17 0x00005567492b1e1e in mysql_execute_command (thd=0x7febf0000b00) at /home/alice/git/10.3.sv/sql/sql_parse.cc:4726
#18 0x00005567492bc290 in mysql_parse (thd=0x7febf0000b00, rawbuf=0x7febf00148d8 ""DELETE FROM t1 where a=\""a\"""", length=26, parser_state=0x7fec02074210, is_com_multi=false, is_next_command=false) at /home/alice/git/10.3.sv/sql/sql_parse.cc:7897
#19 0x00005567492a9d41 in dispatch_command (command=COM_QUERY, thd=0x7febf0000b00, packet=0x7febf0165e01 ""DELETE FROM t1 where a=\""a\"""", packet_length=26, is_com_multi=false, is_next_command=false) at /home/alice/git/10.3.sv/sql/sql_parse.cc:1814
#20 0x00005567492a86de in do_command (thd=0x7febf0000b00) at /home/alice/git/10.3.sv/sql/sql_parse.cc:1377
#21 0x00005567493f40e2 in do_handle_one_connection (connect=0x55674bdf78a0) at /home/alice/git/10.3.sv/sql/sql_connect.cc:1354
#22 0x00005567493f3e62 in handle_one_connection (arg=0x55674bdf78a0) at /home/alice/git/10.3.sv/sql/sql_connect.cc:1260
#23 0x000055674978ba52 in pfs_spawn_thread (arg=0x55674bdffc70) at /home/alice/git/10.3.sv/storage/perfschema/pfs.cc:1862
#24 0x00007fec089f76ba in start_thread (arg=0x7fec02075700) at pthread_create.c:333
#25 0x00007fec07e8c3dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{noformat}
 [^column_compression_parts.test]  [^column_compression_rpl.test ] "
2014,MDEV-13342,MDEV,Sergey Vojtovich,97950,2017-07-25 09:43:42,"Partitioning should be fixed now, please pull.
Another thing that may need extensive testing is optimiser when it has to create temporary table.",2,"Partitioning should be fixed now, please pull.
Another thing that may need extensive testing is optimiser when it has to create temporary table."
2015,MDEV-13342,MDEV,Sergey Vojtovich,97959,2017-07-25 11:52:52,In replication test please also make sure that data is stored in compressed form in binlog (whenever applicable).,3,In replication test please also make sure that data is stored in compressed form in binlog (whenever applicable).
2016,MDEV-13342,MDEV,Sergey Vojtovich,98019,2017-07-27 09:14:02,"Replication of BLOB COMPRESSED to BLOB COMPRESSED should be fixed now.

Replication of COMPRESSED<->UNCOMPRESSED is not yet solved. We have to extend binlog format to transfer COMPRESSED flag, which has to be discussed with [~monty].",4,"Replication of BLOB COMPRESSED to BLOB COMPRESSED should be fixed now.

Replication of COMPRESSEDUNCOMPRESSED is not yet solved. We have to extend binlog format to transfer COMPRESSED flag, which has to be discussed with [~monty]."
2017,MDEV-13342,MDEV,Alice Sherepa,98031,2017-07-27 11:39:11,"I added new test for replication, there is binlog printed at the end.

so, for now, (274ab73a89573554d07a6de1deb846ad47eb1004):
compressed to compressed works correct,
compressed to not compressed - tables on slave return wrong results
not compressed to compressed -- tables on slave return 0 (before worked for all types, except varchar and varbinary, error ""1259: ZLIB: Input data corrupted/ [^compr_rpl.test] ",5,"I added new test for replication, there is binlog printed at the end.

so, for now, (274ab73a89573554d07a6de1deb846ad47eb1004):
compressed to compressed works correct,
compressed to not compressed - tables on slave return wrong results
not compressed to compressed -- tables on slave return 0 (before worked for all types, except varchar and varbinary, error ""1259: ZLIB: Input data corrupted/ [^compr_rpl.test] "
2018,MDEV-13342,MDEV,Sergey Vojtovich,98034,2017-07-27 12:37:20,As I mentioned above not compressed to compressed is not ready yet. Otherwise this change is intentional and makes things inline with varchar.,6,As I mentioned above not compressed to compressed is not ready yet. Otherwise this change is intentional and makes things inline with varchar.
2019,MDEV-13342,MDEV,Sergey Vojtovich,98392,2017-08-08 13:10:58,"This change is now in bb-10.3-svoj: Fixed replication of compressed->uncompressed and uncompressed->compressed data. Depending on slave_type_conversions it is either rejected or converted properly.

To make it work currently, libmariadb has to be patched:
{noformat}
diff --git a/include/mariadb_com.h b/include/mariadb_com.h
index cdf7db8..31550cb 100644
--- a/include/mariadb_com.h
+++ b/include/mariadb_com.h
@@ -327,6 +327,8 @@ enum enum_field_types { MYSQL_TYPE_DECIMAL, MYSQL_TYPE_TINY,
                         MYSQL_TYPE_TIMESTAMP2,
                         MYSQL_TYPE_DATETIME2,
                         MYSQL_TYPE_TIME2,
+                        MYSQL_TYPE_BLOB_COMPRESSED= 140,
+                        MYSQL_TYPE_VARCHAR_COMPRESSED= 141,
                         /* --------------------------------------------- */
                         MYSQL_TYPE_JSON=245,
                         MYSQL_TYPE_NEWDECIMAL=246,
{noformat}",7,"This change is now in bb-10.3-svoj: Fixed replication of compressed->uncompressed and uncompressed->compressed data. Depending on slave_type_conversions it is either rejected or converted properly.

To make it work currently, libmariadb has to be patched:
{noformat}
diff --git a/include/mariadb_com.h b/include/mariadb_com.h
index cdf7db8..31550cb 100644
--- a/include/mariadb_com.h
+++ b/include/mariadb_com.h
@@ -327,6 +327,8 @@ enum enum_field_types { MYSQL_TYPE_DECIMAL, MYSQL_TYPE_TINY,
                         MYSQL_TYPE_TIMESTAMP2,
                         MYSQL_TYPE_DATETIME2,
                         MYSQL_TYPE_TIME2,
+                        MYSQL_TYPE_BLOB_COMPRESSED= 140,
+                        MYSQL_TYPE_VARCHAR_COMPRESSED= 141,
                         /* --------------------------------------------- */
                         MYSQL_TYPE_JSON=245,
                         MYSQL_TYPE_NEWDECIMAL=246,
{noformat}"
2020,MDEV-13342,MDEV,Elena Stepanova,98726,2017-08-15 12:33:00,"Ignore my previous comment (now removed), it belongs to a different task. ",8,"Ignore my previous comment (now removed), it belongs to a different task. "
2021,MDEV-13342,MDEV,Alice Sherepa,99024,2017-08-21 13:07:24,replication test failed for compresssed to not compressed part (Column 0 of table 'test.t1' cannot be converted from type 'tinyblob compressed' to type 'blob') in row replication type ,9,replication test failed for compresssed to not compressed part (Column 0 of table 'test.t1' cannot be converted from type 'tinyblob compressed' to type 'blob') in row replication type 
2022,MDEV-13342,MDEV,Sergey Vojtovich,99025,2017-08-21 13:25:13,Did you set slave_type_conversions?,10,Did you set slave_type_conversions?
2023,MDEV-13342,MDEV,Elena Stepanova,99031,2017-08-21 14:58:29,"[~alice], 
1) column_compression_parts is not stable, please try to record a result file and run the test several times with it, you'll see.
2) I think we should have both MyISAM and InnoDB in the test. Alternatively, we can have combinations based on engine, but it will require an rdiff file for the result, because there is a lot of engine-specific in the result output.

Replication test fails as [~alice] said above. [~svoj], {{setting slave_type_conversions}} to either {{ALL_NON_LOSSY}} or {{ALL_LOSSY}} doesn't seem to make a difference.",11,"[~alice], 
1) column_compression_parts is not stable, please try to record a result file and run the test several times with it, you'll see.
2) I think we should have both MyISAM and InnoDB in the test. Alternatively, we can have combinations based on engine, but it will require an rdiff file for the result, because there is a lot of engine-specific in the result output.

Replication test fails as [~alice] said above. [~svoj], {{setting slave_type_conversions}} to either {{ALL_NON_LOSSY}} or {{ALL_LOSSY}} doesn't seem to make a difference."
2024,MDEV-13342,MDEV,Sergey Vojtovich,99137,2017-08-23 10:47:32,"Back to the discussion about {code:sql}DELETE FROM t1 where a=(REPEAT('a',100));{code}.

It does compress indeed during partition pruning. I'd say it is performance bug. It shouldn't block initial version though.

In parts test data is spread differently depending on COMPRESSION flag setting (see output from I_S.PARTITIONS). I don't really like it.",12,"Back to the discussion about {code:sql}DELETE FROM t1 where a=(REPEAT('a',100));{code}.

It does compress indeed during partition pruning. I'd say it is performance bug. It shouldn't block initial version though.

In parts test data is spread differently depending on COMPRESSION flag setting (see output from I_S.PARTITIONS). I don't really like it."
2025,MDEV-13342,MDEV,Sergey Vojtovich,99138,2017-08-23 10:48:20,Also note that I pushed rebased version to bb-10.3-MDEV-11371. I suggest to switch to that branch unless it fails badly.,13,Also note that I pushed rebased version to bb-10.3-MDEV-11371. I suggest to switch to that branch unless it fails badly.
2026,MDEV-13342,MDEV,Sergey Vojtovich,99139,2017-08-23 11:05:14,"I confirm there's something wrong with replication. I'll try to get this fixed asap.

The fact that it says ""'tinyblob compressed' to type 'blob'"" is a bug as such. It should say ""'blob compressed' to type 'blob'"". Reported as MDEV-13629.

If I set --slave-type-conversions=ALL_NON_LOSSY it accepts blobs just fine, but then fails with ""varchar(10001) compressed' to type 'varchar(10000)"". That's something that I'll have to fix.",14,"I confirm there's something wrong with replication. I'll try to get this fixed asap.

The fact that it says ""'tinyblob compressed' to type 'blob'"" is a bug as such. It should say ""'blob compressed' to type 'blob'"". Reported as MDEV-13629.

If I set --slave-type-conversions=ALL_NON_LOSSY it accepts blobs just fine, but then fails with ""varchar(10001) compressed' to type 'varchar(10000)"". That's something that I'll have to fix."
2027,MDEV-13342,MDEV,Sergey Vojtovich,99247,2017-08-25 07:31:41,"Replication test should be extended to test tinyblob with data length 255 to tinyblob compressed with column_compression_threshold>255.
It should be an error.",15,"Replication test should be extended to test tinyblob with data length 255 to tinyblob compressed with column_compression_threshold>255.
It should be an error."
2028,MDEV-13342,MDEV,Sergey Vojtovich,99249,2017-08-25 07:42:00,Also VARCHAR(10000) to VARCHAR(9999) COMPRESSED should be an error.,16,Also VARCHAR(10000) to VARCHAR(9999) COMPRESSED should be an error.
2029,MDEV-13342,MDEV,Sergey Vojtovich,99250,2017-08-25 08:10:28,Replication should now be fixed in bb-10.3-MDEV-11371.,17,Replication should now be fixed in bb-10.3-MDEV-11371.
2030,MDEV-13342,MDEV,Sergey Vojtovich,99276,2017-08-25 12:20:47,Data distribution in partitioned tables is now fixed in bb-10.3-MDEV-11371.,18,Data distribution in partitioned tables is now fixed in bb-10.3-MDEV-11371.
2031,MDEV-13342,MDEV,Alice Sherepa,99394,2017-08-29 09:48:00,"replication does not work for the biggest value in blob and text (tiny, medium,..)  with all kinds (compr to compr, c->n c, n c->c). returns an error (e.g. tinyblob)
{noformat}
INSERT INTO t1(a)  VALUES(REPEAT('a',255))' failed: 1406: Data too long for column 'a' at row 1
{noformat}

",19,"replication does not work for the biggest value in blob and text (tiny, medium,..)  with all kinds (compr to compr, c->n c, n c->c). returns an error (e.g. tinyblob)
{noformat}
INSERT INTO t1(a)  VALUES(REPEAT('a',255))' failed: 1406: Data too long for column 'a' at row 1
{noformat}

"
2032,MDEV-13342,MDEV,Sergey Vojtovich,99395,2017-08-29 10:06:08,"It is a bug, it should only fail for nc->c.
I wonder how did you make it fail for c->c though?",20,"It is a bug, it should only fail for nc->c.
I wonder how did you make it fail for c->c though?"
2033,MDEV-13342,MDEV,Alice Sherepa,99399,2017-08-29 10:47:29,"it works for 254, but not 255. 
{noformat}
connection slave;
CREATE TABLE t1  (a tinyblob COMPRESSED);
connection master;
CREATE TABLE IF NOT EXISTS t1 (a tinyblob COMPRESSED);
INSERT INTO t1(a)  VALUES(REPEAT('a',255));
main.1dd 'mix'                           [ fail ]
        Test ended at 2017-08-29 12:45:34

CURRENT_TEST: main.1dd
mysqltest: At line 15: query 'INSERT INTO t1(a)  VALUES(REPEAT('a',255))' failed: 1406: Data too long for column 'a' at row 1
{noformat}
it indeed does not show an error with blobs while ALL_NON_LOSSY is set
with varchars there is  an error. In case with varchar(1000)->varchar(1000) compressed:
{noformat}
Last_Errno	1677
Last_Error	Column 0 of table 'test.t2' cannot be converted from type 'varchar(10001) compressed' to type 'varchar(9999) /*!100301 COMPRESS' 
{noformat}
",21,"it works for 254, but not 255. 
{noformat}
connection slave;
CREATE TABLE t1  (a tinyblob COMPRESSED);
connection master;
CREATE TABLE IF NOT EXISTS t1 (a tinyblob COMPRESSED);
INSERT INTO t1(a)  VALUES(REPEAT('a',255));
main.1dd 'mix'                           [ fail ]
        Test ended at 2017-08-29 12:45:34

CURRENT_TEST: main.1dd
mysqltest: At line 15: query 'INSERT INTO t1(a)  VALUES(REPEAT('a',255))' failed: 1406: Data too long for column 'a' at row 1
{noformat}
it indeed does not show an error with blobs while ALL_NON_LOSSY is set
with varchars there is  an error. In case with varchar(1000)->varchar(1000) compressed:
{noformat}
Last_Errno	1677
Last_Error	Column 0 of table 'test.t2' cannot be converted from type 'varchar(10001) compressed' to type 'varchar(9999) /*!100301 COMPRESS' 
{noformat}
"
2034,MDEV-13342,MDEV,Sergey Vojtovich,99403,2017-08-29 10:58:23,"It happens on master, which is kind of expected. Max data length for compressed blobs 1 byte shorter than for regular blobs. There's probably a way to fix it, but it was decided not to bother with this in first implementation.",22,"It happens on master, which is kind of expected. Max data length for compressed blobs 1 byte shorter than for regular blobs. There's probably a way to fix it, but it was decided not to bother with this in first implementation."
2035,MDEV-13342,MDEV,Sergey Vojtovich,99404,2017-08-29 11:00:05,"If you want to try 255 bytes compressed blob you should do something like this:
{code:sql}
SET column_compression_threshold=255;
INSERT INTO t1(a)  VALUES(REPEAT('a',254));
{code}",23,"If you want to try 255 bytes compressed blob you should do something like this:
{code:sql}
SET column_compression_threshold=255;
INSERT INTO t1(a)  VALUES(REPEAT('a',254));
{code}"
2036,MDEV-13342,MDEV,Sergey Vojtovich,99405,2017-08-29 11:03:04,Are you testing recent bb-10.3-MDEV-11371? VARCHAR should have been fixed there.,24,Are you testing recent bb-10.3-MDEV-11371? VARCHAR should have been fixed there.
2037,MDEV-13342,MDEV,Alice Sherepa,100563,2017-09-21 13:33:26, [^column_compression_parts.result]  [^column_compression_parts.test]  [^column_compression_rpl.inc]  [^column_compression_rpl.result]  [^column_compression_rpl.test] ,25, [^column_compression_parts.result]  [^column_compression_parts.test]  [^column_compression_rpl.inc]  [^column_compression_rpl.result]  [^column_compression_rpl.test] 
2038,MDEV-13369,MDEV,Igor Babaev,98267,2017-08-04 05:01:54,"A patch for this task that can be considered as a 'proof of concept' was sent for evaluation to Andrii.

This patch introduced two-phase optimization for some queries.
It also checked that the derived tables with outer references can be easily executed within the current architecture.",1,"A patch for this task that can be considered as a 'proof of concept' was sent for evaluation to Andrii.

This patch introduced two-phase optimization for some queries.
It also checked that the derived tables with outer references can be easily executed within the current architecture."
2039,MDEV-13369,MDEV,Igor Babaev,98570,2017-08-10 18:52:47,"Alvin,
Bar planned to add the patch to bb-10.2-compatibility  on August 4.
The comments of the patch mention mdev-13369.
I personally work with bb-10.2-ext and I did not push it there.",2,"Alvin,
Bar planned to add the patch to bb-10.2-compatibility  on August 4.
The comments of the patch mention mdev-13369.
I personally work with bb-10.2-ext and I did not push it there."
2040,MDEV-13369,MDEV,Igor Babaev,103447,2017-11-21 08:44:25,"Cost aware implementation of mdev-13369.

Design.

1.  Every materialized derived is checked for being potentially splittable:
a) the specification must contain only 1 select
b) if this select is with  GROUP BY then check that there is an index whose major components cover major components of the group by list (in any order)
c) if this select is with window functions then check that all window functions use only 1 partition;
there is an index whose major components cover some components of the partition.

If all the conditions above are satisfied then the materialized derived is considered as potentially splittable and a structure of the type SPLITTABLE_DERIVED_INFO  is attached
to the TABLE_LIST created for this derived.
All info about the indexes that potentially can be used for splitting is stored in this structure.

2. The join specifying a potentially splittable derived is subject to two-phase optimization. The first phase ends when the cheapest join order is found.

When the optimization of the main query asks for the optimization of the materialized derived tables only the first phase of the optimization is performed  for each derived that is optimized in two phases.
The check whether a join is subject to two phase optimization is performed when the first phase finishes.

3. Let v be a potentially derived used in the main query.
When  the function choose plan() called for the main query tries to expand the current partial join order with v1 using an index access that employs the index idx (v.a,v.b,…) the
following is checked. It is checked that some prefix of this index consists only of the fields that are selected into v directly from the fields of an underlying table (vt.a, vt.b,…) and vt.a,vt.b...are the major components of the indexes remembered in the SPLITTABLE_DERIVED_INFO structure created for v. If so, v can be effectively split.

4. Let assume that v can be accessed via a split technique after tables t1, t2 in a partial order for the main query. Let assume that the split is performed using the equality conditions v.a=f(t1.a), v(b)=g(t2.b). When in the partial join order t1,t2,v the table v is accessed the values of f(t1.a) and g(t2.b) are fixed (considered as constants). The equalities vt.a=f(t1.a), vt.b=g(t2.b) must be added to the WHERE condition of v to get the partition of v that is determined by the conditions v.a=f(t1.a), v(b)=g(t2.b). Now the best plan is built for v with extended WHERE that actually can produce any partition p from v. At this point the cost of this plan C(p) must be  evaluated . Let C(v) be the cost of producing rows of v. Let in(v) be the expected row count for the partial join t1,t2. If in(v)*C(p) < C(v) then accessing v with a splitting technique is more beneficial with this partial order.  

5. Adding conditions to existing one is not so cheap. Besides it's clear that when looking for the best plan for the main query the optimizer has not only to add conditions that can be used, but also has to remove the ones that are not valid for the current partial join order. For example when the partial order t1,v is evaluated the condition vt(b)=g(t2.b) must be removed from consideration.
With a certain technique we can add all usable equalities at once and then invalidate usage of those of them that are not valid in the current partial join order. The technique works as follows. When conditions the vt.a=f(t1.a), vt.b=g(t2.b) are built from the conditions v.a=f(t1.a), v(b)=g(t2.b) each of items f(t1.a), g(t2.b) is cloned and each clone is provided with  pointer to the parent. A clone is resolved against v, while the parent is resolved against the main query. Now if the partial join order is t1,v then vt.a=f(t1.a) can be used, while vt.b=g(t2.b) cannot because t2 is not in t1,v.

6. The array of extended KEYUSEs is built only once when the first usage of the split technique is evaluated.
The new variant of best_access_path() must take into account that some KEYUSEs can be invalidated for some partial join orders.  
",3,"Cost aware implementation of mdev-13369.

Design.

1.  Every materialized derived is checked for being potentially splittable:
a) the specification must contain only 1 select
b) if this select is with  GROUP BY then check that there is an index whose major components cover major components of the group by list (in any order)
c) if this select is with window functions then check that all window functions use only 1 partition;
there is an index whose major components cover some components of the partition.

If all the conditions above are satisfied then the materialized derived is considered as potentially splittable and a structure of the type SPLITTABLE_DERIVED_INFO  is attached
to the TABLE_LIST created for this derived.
All info about the indexes that potentially can be used for splitting is stored in this structure.

2. The join specifying a potentially splittable derived is subject to two-phase optimization. The first phase ends when the cheapest join order is found.

When the optimization of the main query asks for the optimization of the materialized derived tables only the first phase of the optimization is performed  for each derived that is optimized in two phases.
The check whether a join is subject to two phase optimization is performed when the first phase finishes.

3. Let v be a potentially derived used in the main query.
When  the function choose plan() called for the main query tries to expand the current partial join order with v1 using an index access that employs the index idx (v.a,v.b,…) the
following is checked. It is checked that some prefix of this index consists only of the fields that are selected into v directly from the fields of an underlying table (vt.a, vt.b,…) and vt.a,vt.b...are the major components of the indexes remembered in the SPLITTABLE_DERIVED_INFO structure created for v. If so, v can be effectively split.

4. Let assume that v can be accessed via a split technique after tables t1, t2 in a partial order for the main query. Let assume that the split is performed using the equality conditions v.a=f(t1.a), v(b)=g(t2.b). When in the partial join order t1,t2,v the table v is accessed the values of f(t1.a) and g(t2.b) are fixed (considered as constants). The equalities vt.a=f(t1.a), vt.b=g(t2.b) must be added to the WHERE condition of v to get the partition of v that is determined by the conditions v.a=f(t1.a), v(b)=g(t2.b). Now the best plan is built for v with extended WHERE that actually can produce any partition p from v. At this point the cost of this plan C(p) must be  evaluated . Let C(v) be the cost of producing rows of v. Let in(v) be the expected row count for the partial join t1,t2. If in(v)*C(p) < C(v) then accessing v with a splitting technique is more beneficial with this partial order.  

5. Adding conditions to existing one is not so cheap. Besides it's clear that when looking for the best plan for the main query the optimizer has not only to add conditions that can be used, but also has to remove the ones that are not valid for the current partial join order. For example when the partial order t1,v is evaluated the condition vt(b)=g(t2.b) must be removed from consideration.
With a certain technique we can add all usable equalities at once and then invalidate usage of those of them that are not valid in the current partial join order. The technique works as follows. When conditions the vt.a=f(t1.a), vt.b=g(t2.b) are built from the conditions v.a=f(t1.a), v(b)=g(t2.b) each of items f(t1.a), g(t2.b) is cloned and each clone is provided with  pointer to the parent. A clone is resolved against v, while the parent is resolved against the main query. Now if the partial join order is t1,v then vt.a=f(t1.a) can be used, while vt.b=g(t2.b) cannot because t2 is not in t1,v.

6. The array of extended KEYUSEs is built only once when the first usage of the split technique is evaluated.
The new variant of best_access_path() must take into account that some KEYUSEs can be invalidated for some partial join orders.  
"
2041,MDEV-13369,MDEV,Igor Babaev,105869,2018-01-19 03:55:55,A full cost-base solution was pushed into the 10.3 tree,4,A full cost-base solution was pushed into the 10.3 tree
2042,MDEV-13491,MDEV,Axel Schwenke,118760,2018-11-06 11:01:27,Blog posts were released by marketing,1,Blog posts were released by marketing
2043,MDEV-13528,MDEV,Alexander Barkov,98728,2017-08-15 13:25:25,"Pushed to bb-10.2-ext and 10.3.
",1,"Pushed to bb-10.2-ext and 10.3.
"
2044,MDEV-13533,MDEV,Alexander Barkov,98729,2017-08-15 13:26:53,Pushed to bb-10.2-ext and 10.3,1,Pushed to bb-10.2-ext and 10.3
2045,MDEV-13760,MDEV,Elena Stepanova,99809,2017-09-07 20:52:57,FYI [~greenman],1,FYI [~greenman]
2046,MDEV-13760,MDEV,Sergey Vojtovich,100051,2017-09-12 10:56:39,"I'm going to have a talk about column compression. When it is ready slides can be sync'ed to manual. Until then it is probably alright to miss some stuff, but what should be definitely documented is syntax changes, system variables, status variables, supported data types. It is all shortly explained in revision comment.",2,"I'm going to have a talk about column compression. When it is ready slides can be sync'ed to manual. Until then it is probably alright to miss some stuff, but what should be definitely documented is syntax changes, system variables, status variables, supported data types. It is all shortly explained in revision comment."
2047,MDEV-13760,MDEV,Ian Gilfillan,154029,2020-05-22 13:18:05,This was documented at https://mariadb.com/kb/en/storage-engine-independent-column-compression/,3,This was documented at URL
2048,MDEV-13761,MDEV,Elena Stepanova,99810,2017-09-07 21:00:13,"[~greenman], FYI.",1,"[~greenman], FYI."
2049,MDEV-13761,MDEV,Vladislav Vaintroub,106193,2018-01-24 20:51:45,https://mariadb.com/kb/en/library/proxy-protocol-support/,2,URL
2050,MDEV-13763,MDEV,Elena Stepanova,99813,2017-09-07 21:31:11,FYI [~greenman],1,FYI [~greenman]
2051,MDEV-13763,MDEV,Ian Gilfillan,100690,2017-09-25 11:06:58,Documented on the SHOW WARNINGS page,2,Documented on the SHOW WARNINGS page
2052,MDEV-13812,MDEV,Sergey Vojtovich,101708,2017-10-20 07:34:46,I already expressed my concerns re AT in MDEV-11386. In a nutshell I'm all for getting rid of it or at the very least having non-AT builds.,1,I already expressed my concerns re AT in MDEV-11386. In a nutshell I'm all for getting rid of it or at the very least having non-AT builds.
2053,MDEV-13812,MDEV,Sergei Golubchik,102258,2017-10-31 10:12:37,"ok, let's get rid of advance toolchain :)",2,"ok, let's get rid of advance toolchain :)"
2054,MDEV-13812,MDEV,Sergey Vojtovich,102259,2017-10-31 10:18:45,"[~dbart], your turn :)

Hopefully all we need to do is remove PATH setting on P8 builders.",3,"[~dbart], your turn :)

Hopefully all we need to do is remove PATH setting on P8 builders."
2055,MDEV-13812,MDEV,Daniel Bartholomew,105850,2018-01-18 18:14:44,"I've removed the PATH setting for the IBM P8 builders. So naturally they immediately started having build issues.

However, with that said, we're moving away from the IBM-provided builders and to kvm-based builders.

To that end, the kvm-rpm-centos73-ppc64 and kvm-rpm-centos73-ppc64le builders are now in place and taking over for p8-rhel7 and p8-rhel71 (respectively). These builders have actually been running for a while, but were in the 'experimental' category on buildbot because of some lingering AT dependencies with the 'shared' rpm that is used during building. I was able to get that fixed yesterday and so I've just now moved the builders out of 'experimental' and into 'main'.

I will next deactivate the p8-rhel7 and p8-rhel71 builders. After I'm done with that the only IBM P8 builders left will be p8-suse12 and p8-rhel6. So those two will have to be looked at to make sure they're able to build without AT, but we don't need to worry about the p8-rhel7 and p8-rhel71 builders.
",4,"I've removed the PATH setting for the IBM P8 builders. So naturally they immediately started having build issues.

However, with that said, we're moving away from the IBM-provided builders and to kvm-based builders.

To that end, the kvm-rpm-centos73-ppc64 and kvm-rpm-centos73-ppc64le builders are now in place and taking over for p8-rhel7 and p8-rhel71 (respectively). These builders have actually been running for a while, but were in the 'experimental' category on buildbot because of some lingering AT dependencies with the 'shared' rpm that is used during building. I was able to get that fixed yesterday and so I've just now moved the builders out of 'experimental' and into 'main'.

I will next deactivate the p8-rhel7 and p8-rhel71 builders. After I'm done with that the only IBM P8 builders left will be p8-suse12 and p8-rhel6. So those two will have to be looked at to make sure they're able to build without AT, but we don't need to worry about the p8-rhel7 and p8-rhel71 builders.
"
2056,MDEV-13812,MDEV,Daniel Black,105868,2018-01-19 03:43:02,"Now that you've migrated away from AT 659047b8207a1212adeaa80c23b22da929f22b1b (10.0 branch and up) can be reverted.

I thought docker for the builders was the goal? ( MDEV-12508 ) anyway either works.

To get debug symbol rpms (MDEV-4646), can you install a cmake-3.7.0+ version like MDEV-11258 and use this for the RPM production job. EPEL still only has cmake-3.6 I'm afraid.

Sorry I don't know if rhel6/suse12 have native compilers of sufficient version for p8 builds of mariadb and don't have any close by to test sorry.",5,"Now that you've migrated away from AT 659047b8207a1212adeaa80c23b22da929f22b1b (10.0 branch and up) can be reverted.

I thought docker for the builders was the goal? ( MDEV-12508 ) anyway either works.

To get debug symbol rpms (MDEV-4646), can you install a cmake-3.7.0+ version like MDEV-11258 and use this for the RPM production job. EPEL still only has cmake-3.6 I'm afraid.

Sorry I don't know if rhel6/suse12 have native compilers of sufficient version for p8 builds of mariadb and don't have any close by to test sorry."
2057,MDEV-13812,MDEV,Daniel Bartholomew,120915,2018-12-18 11:29:25,"Going through and closing old issues of mine that should have been closed a long time ago.

This task is done. All of our current ppc64 and ppc64le builds are made without the advanced toolchain (and have been for some time).",6,"Going through and closing old issues of mine that should have been closed a long time ago.

This task is done. All of our current ppc64 and ppc64le builds are made without the advanced toolchain (and have been for some time)."
2058,MDEV-13864,MDEV,Alexander Barkov,100651,2017-09-23 05:55:24,"Pushed to bb-10.2-ext and 10.3
",1,"Pushed to bb-10.2-ext and 10.3
"
2059,MDEV-13919,MDEV,Alexander Barkov,100863,2017-09-28 14:58:09,Pushed to bb-10.2-ext and 10.3,1,Pushed to bb-10.2-ext and 10.3
2060,MDEV-14013,MDEV,Alexander Barkov,101694,2017-10-19 10:02:49,"Reviewed a contributed patch from Jerome B.
Proposes a few small cleanups.
Pushed.
",1,"Reviewed a contributed patch from Jerome B.
Proposes a few small cleanups.
Pushed.
"
2061,MDEV-14013,MDEV,Anil,104610,2017-12-18 07:50:04,"Hi team,
I am getting the same issue when trying to set SQL_MODE as EMPTY_STRING_IS_NULL in Server version: 10.3.2-MariaDB MariaDB Server to compatible with ORACLE.

getting below error for your reference :

ERROR 1231 (42000): Variable 'sql_mode' can't be set to the value of 'EMPTY_STRING_IS_NULL'",2,"Hi team,
I am getting the same issue when trying to set SQL_MODE as EMPTY_STRING_IS_NULL in Server version: 10.3.2-MariaDB MariaDB Server to compatible with ORACLE.

getting below error for your reference :

ERROR 1231 (42000): Variable 'sql_mode' can't be set to the value of 'EMPTY_STRING_IS_NULL'"
2062,MDEV-14013,MDEV,Alexander Barkov,104611,2017-12-18 07:53:19,"Anil, this feature will be available in MariaDB-10.3.3.

",3,"Anil, this feature will be available in MariaDB-10.3.3.

"
2063,MDEV-14013,MDEV,Anil,104612,2017-12-18 07:55:16,"Please suggest how to fix if I want both the SQL_MODE active on MariaDB 10.3.2 server. 

SQL_MODE =STRICT_TRANS_TABLES, EMPTY_STRING_IS_NULL

Thanks in advance
Anil",4,"Please suggest how to fix if I want both the SQL_MODE active on MariaDB 10.3.2 server. 

SQL_MODE =STRICT_TRANS_TABLES, EMPTY_STRING_IS_NULL

Thanks in advance
Anil"
2064,MDEV-14013,MDEV,Anil,104613,2017-12-18 07:59:07,Thanks a lot Alexander Barkov  for quick update .... you have save my day .... Thanks again.,5,Thanks a lot Alexander Barkov  for quick update .... you have save my day .... Thanks again.
2065,MDEV-14013,MDEV,Mike Hawksfraert,115613,2018-08-24 10:22:36,"I am confused about this feature - is this supposed to work for LOAD DATA INFILE and mysqlimport loads as well? 

My version is 10.3.9-MariaDB-1:10.3.9+maria~xenial-log

I have a MyISAM table with an int(11) column user_id which has DEFAULT NULL defined.

When I insert records into this table using the INSERT INTO... command, using empty strings for user_id, I get the expected NULL for user_id.

But when I try to load CSV data which is empty on the user_id field (as in ""nothing between the commas""), the user_id for those rows is set to 0 (zero).
 
This happens for both LOAD DATA INFILE and mysqlimport.

Please advise.",6,"I am confused about this feature - is this supposed to work for LOAD DATA INFILE and mysqlimport loads as well? 

My version is 10.3.9-MariaDB-1:10.3.9+maria~xenial-log

I have a MyISAM table with an int(11) column user_id which has DEFAULT NULL defined.

When I insert records into this table using the INSERT INTO... command, using empty strings for user_id, I get the expected NULL for user_id.

But when I try to load CSV data which is empty on the user_id field (as in ""nothing between the commas""), the user_id for those rows is set to 0 (zero).
 
This happens for both LOAD DATA INFILE and mysqlimport.

Please advise."
2066,MDEV-14024,MDEV,Alexey Botchkov,109210,2018-04-02 23:36:01,"So that i removed the pcre/ subdirectory, added the pcre2/ subdirectory with the pcre2-10.31 sources, and then did these changes to the server code:
http://lists.askmonty.org/pipermail/commits/2018-April/012175.html",1,"So that i removed the pcre/ subdirectory, added the pcre2/ subdirectory with the pcre2-10.31 sources, and then did these changes to the server code:
URL"
2067,MDEV-14024,MDEV,Sergei Golubchik,109560,2018-04-11 10:48:27,"the first commit (remove pcre, add prce2) — did you do any changes to pcre2? or just unpacked the tarball?",2,"the first commit (remove pcre, add prce2) — did you do any changes to pcre2? or just unpacked the tarball?"
2068,MDEV-14024,MDEV,Alexey Botchkov,110033,2018-04-23 14:05:42,I just unpacked the tarball.,3,I just unpacked the tarball.
2069,MDEV-14024,MDEV,Alexey Botchkov,110034,2018-04-23 14:06:00,http://lists.askmonty.org/pipermail/commits/2018-April/012444.html,4,URL
2070,MDEV-14024,MDEV,Alexey Botchkov,113042,2018-06-26 10:45:20,"patch v2
http://lists.askmonty.org/pipermail/commits/2018-June/012656.html",5,"patch v2
URL"
2071,MDEV-14024,MDEV,Sergei Golubchik,119726,2018-11-23 13:10:59,review sent,6,review sent
2072,MDEV-14024,MDEV,Marko Mäkelä,133031,2019-08-26 09:16:02,"As noted in MDEV-20377, MemorySanitizer is finding several issues with the bundled PCRE1 (and according to [~serg] the ""{{libpcre3}}"" on my Debian system actually is PCRE1). Also, I remember AddressSanitizer flagging issues when using {{-DWITH_PCRE=bundled}}. Using {{-DWITH_PCRE=system}} could merely hide those issues from ASAN.

I feel that this issue is all but blocking the use of MemorySanitizer.",7,"As noted in MDEV-20377, MemorySanitizer is finding several issues with the bundled PCRE1 (and according to [~serg] the ""{{libpcre3}}"" on my Debian system actually is PCRE1). Also, I remember AddressSanitizer flagging issues when using {{-DWITH_PCRE=bundled}}. Using {{-DWITH_PCRE=system}} could merely hide those issues from ASAN.

I feel that this issue is all but blocking the use of MemorySanitizer."
2073,MDEV-14113,MDEV,Sergey Vojtovich,101827,2017-10-24 07:32:29,"The idea is generally fine, but I think we can have it always enabled. That is DISABLE_TCP_LINGER and cmake changes should not be needed.

Also we should not need tcp_linger variable. tcp_linger_timeout alone should be enough.

I'd avoid define TCP_LINGER_TIMEOUT. Just put default timeout directly into sys_var.",1,"The idea is generally fine, but I think we can have it always enabled. That is DISABLE_TCP_LINGER and cmake changes should not be needed.

Also we should not need tcp_linger variable. tcp_linger_timeout alone should be enough.

I'd avoid define TCP_LINGER_TIMEOUT. Just put default timeout directly into sys_var."
2074,MDEV-14113,MDEV,Vladislav Vaintroub,101907,2017-10-25 20:25:33,"https://www.nybek.com/blog/2015/03/05/cross-platform-testing-of-so_linger/ actually does not recommend setting SO_LINGER to  \{on,timeout > 0} cross-platform.
According to this article, only Windows would  abort the connection (send RST) if acknowledgement for sent data was not received after linger timeout, which is, I guess, the reason for this patch.

However, according to the same article , call to shutdown() before closesocket() on Windows would make closesocket() to return immediately, and connection closed in background,  as if setsockopt(SO_LINGER) was not called. 

I checked our code, and we do shutdown() before closesocket() ( this does not make much sense, but we have this since long time already). So, the questions are : 
* Is https://www.nybek.com/blog/2015/03/05/cross-platform-testing-of-so_linger/  correct wrt cross-platform SO_LINGER effects. If this is the case, perhaps we should make this setting avaialble only on Windows.

* with the current code, vio_close()  where shutdown() followed by closesocket(), does SO_LINGER setting have any effect?
 ",2,"URL actually does not recommend setting SO_LINGER to  \{on,timeout > 0} cross-platform.
According to this article, only Windows would  abort the connection (send RST) if acknowledgement for sent data was not received after linger timeout, which is, I guess, the reason for this patch.

However, according to the same article , call to shutdown() before closesocket() on Windows would make closesocket() to return immediately, and connection closed in background,  as if setsockopt(SO_LINGER) was not called. 

I checked our code, and we do shutdown() before closesocket() ( this does not make much sense, but we have this since long time already). So, the questions are : 
* Is URL  correct wrt cross-platform SO_LINGER effects. If this is the case, perhaps we should make this setting avaialble only on Windows.

* with the current code, vio_close()  where shutdown() followed by closesocket(), does SO_LINGER setting have any effect?
 "
2075,MDEV-14113,MDEV,Sepherosa Ziehau,103133,2017-11-14 02:24:44,"It's not only waiting for the sent data to be acknowledged, but also waiting for the connection's FIN being ACKed.

I don't know about how Windows handling shutdown(2) then close(2), but on all other systems, close(2) will obey the SO_LINGER settings, no matter whether the shutdown(2) was called or not.

As about shutdown(2) before close(2), I don't think the shutdown(2) immediately before the close(2) is necessary; removing it would save some syscall cost.  I'd suggest to remove it, no matter whether this patch could be in or not.

Generally speaking, I think SO_LINGER option is fine (the compile time option is unnecessary; all systems have this option).  But it will put the close(2) into sleep, so I'd suggest to default off for this option.  And don't call setsockopt(2), if this option is off, which introduces unnecessary syscall cost.

Thanks,
sephe",3,"It's not only waiting for the sent data to be acknowledged, but also waiting for the connection's FIN being ACKed.

I don't know about how Windows handling shutdown(2) then close(2), but on all other systems, close(2) will obey the SO_LINGER settings, no matter whether the shutdown(2) was called or not.

As about shutdown(2) before close(2), I don't think the shutdown(2) immediately before the close(2) is necessary; removing it would save some syscall cost.  I'd suggest to remove it, no matter whether this patch could be in or not.

Generally speaking, I think SO_LINGER option is fine (the compile time option is unnecessary; all systems have this option).  But it will put the close(2) into sleep, so I'd suggest to default off for this option.  And don't call setsockopt(2), if this option is off, which introduces unnecessary syscall cost.

Thanks,
sephe"
2076,MDEV-14113,MDEV,Vladislav Vaintroub,103178,2017-11-14 19:01:01,"[~Sepherosa Ziehau], thanks for commenting. Is this your patch?
As for cross-plattformness of SO_LINGER, all information I could find on the internet, is 
* in case of {on, 0} setting, connection is aborted with RST immediately (from here https://www.nybek.com/blog/2015/03/05/cross-platform-testing-of-so_linger/)
* in case of {on, > 0} setting, on close(), given some unsent data, connection blocks (assuming blocking socket) for the specified l_linger setting, however *only Windows* would  RST-abort connection after timeout, so the added effect cross-plattform is just waiting and blocking (from the same source)
* non-blocking sockets behave strangely, and with {on, > 0} setting, again, only on Windows the actual LINGER effect of aborting connection 
after specified timeout can be seen https://www.nybek.com/blog/2015/04/29/so_linger-on-non-blocking-sockets/#so_linger_on_20

The nybeck.com seems to have done quite a lot of research on the option, which if I summarize here, would be : except Windows, where things work as expected,  the SO_LINGER setting is really only useful either with timeout 0, or when sender wants to wait for specific time to ensure receiver gets the data, before closing connection.

timeout >= 0  on Windows and timeout = 0 on Unixes can be used to abort connection, so there are no connections in TIME_WAIT, so for that we can have the tcp_linger option, which I'd propose to make an ""int"" variable, in the  {-1, 65535} range ,  -1 meaning default ""off"". With that, 0 linger is allowed and can actually be used on Unixes, where other timeout does not make much sense.

Speaking of our client-server protocol,  the connection closure is normally initiated by the client, who sends COM_QUIT packet, closes the socket, and then server closes the socket as well. 

In other cases, server is the first to close socket.  I think there is at least one legitimate case, where server should abort the connection, rather than orderly close. That is, when server sends a large result set to the client, but the client is reading much slower than server is writing, so that socket buffers fill up,  and server's send() runs into timeout. Do you think we should abort the connection via SO_LINGER={on,0} in this case?

Can you give your thoughts on following: 

* is this correct, that, outside of Windows, linger timeout > 0 does not make much sense, because it does not abort  connection via RST?
* should the server abort connection if send() runs into timeout, disregarding tcp_linger settings.
",4,"[~Sepherosa Ziehau], thanks for commenting. Is this your patch?
As for cross-plattformness of SO_LINGER, all information I could find on the internet, is 
* in case of {on, 0} setting, connection is aborted with RST immediately (from here URL
* in case of {on, > 0} setting, on close(), given some unsent data, connection blocks (assuming blocking socket) for the specified l_linger setting, however *only Windows* would  RST-abort connection after timeout, so the added effect cross-plattform is just waiting and blocking (from the same source)
* non-blocking sockets behave strangely, and with {on, > 0} setting, again, only on Windows the actual LINGER effect of aborting connection 
after specified timeout can be seen URL

The nybeck.com seems to have done quite a lot of research on the option, which if I summarize here, would be : except Windows, where things work as expected,  the SO_LINGER setting is really only useful either with timeout 0, or when sender wants to wait for specific time to ensure receiver gets the data, before closing connection.

timeout >= 0  on Windows and timeout = 0 on Unixes can be used to abort connection, so there are no connections in TIME_WAIT, so for that we can have the tcp_linger option, which I'd propose to make an ""int"" variable, in the  {-1, 65535} range ,  -1 meaning default ""off"". With that, 0 linger is allowed and can actually be used on Unixes, where other timeout does not make much sense.

Speaking of our client-server protocol,  the connection closure is normally initiated by the client, who sends COM_QUIT packet, closes the socket, and then server closes the socket as well. 

In other cases, server is the first to close socket.  I think there is at least one legitimate case, where server should abort the connection, rather than orderly close. That is, when server sends a large result set to the client, but the client is reading much slower than server is writing, so that socket buffers fill up,  and server's send() runs into timeout. Do you think we should abort the connection via SO_LINGER={on,0} in this case?

Can you give your thoughts on following: 

* is this correct, that, outside of Windows, linger timeout > 0 does not make much sense, because it does not abort  connection via RST?
* should the server abort connection if send() runs into timeout, disregarding tcp_linger settings.
"
2077,MDEV-14113,MDEV,Sepherosa Ziehau,103191,2017-11-15 05:12:54,"> is this correct, that, outside of Windows, linger timeout > 0 does not make much sense, because it does not abort connection via RST?

This is correct.

> should the server abort connection if send() runs into timeout, disregarding tcp_linger settings.

This is correct too.  However, the retransmission timeout could be quite lengthy.

I am not the author.  I talked to the author about the background and the intention yesterday.  We lean toward TCP keepalive based solution instead of this one at the moment.  We will keep you updated on these.",5,"> is this correct, that, outside of Windows, linger timeout > 0 does not make much sense, because it does not abort connection via RST?

This is correct.

> should the server abort connection if send() runs into timeout, disregarding tcp_linger settings.

This is correct too.  However, the retransmission timeout could be quite lengthy.

I am not the author.  I talked to the author about the background and the intention yesterday.  We lean toward TCP keepalive based solution instead of this one at the moment.  We will keep you updated on these."
2078,MDEV-14113,MDEV,Vladislav Vaintroub,103199,2017-11-15 09:52:59,">> should the server abort connection if send() runs into timeout, disregarding tcp_linger settings.

>This is correct too. However, the retransmission timeout could be quite lengthy.
Not sure about the retransmission. We use socket timeouts 
there is a system variable net_write_timeout, defaulting to 60 seconds 
https://mariadb.com/kb/en/library/server-system-variables/#net_write_timeout

If server cannot send a packet within net_write_timeout seconds, connection is closed (should really be aborted as we just discussed)
",6,">> should the server abort connection if send() runs into timeout, disregarding tcp_linger settings.

>This is correct too. However, the retransmission timeout could be quite lengthy.
Not sure about the retransmission. We use socket timeouts 
there is a system variable net_write_timeout, defaulting to 60 seconds 
URL

If server cannot send a packet within net_write_timeout seconds, connection is closed (should really be aborted as we just discussed)
"
2079,MDEV-14113,MDEV,Vladislav Vaintroub,103220,2017-11-15 19:01:06,"speaking if TCP keepalive, we have an upcoming  patch for it as well
https://github.com/MariaDB/server/commit/44cfcce67f0c4a63989db406623bff2fe6faf40d",7,"speaking if TCP keepalive, we have an upcoming  patch for it as well
URL"
2080,MDEV-14113,MDEV,Sepherosa Ziehau,103229,2017-11-16 01:33:41,"As for the userland send timeout.  It's kinda different, an example:
1) The sending was not blocked in the last send(2).
2) The server close(2)'ed the socket after the last send(2) call.  The data in the last send(2) was still in the TCP send buffer.
3) If the client's ACK for the last piece of data was excessively dropped, or the extreme case that the client itself was crashed.
4) Then the socket's (and inpcb in the BSD world) destruction would solely rely on the TCP retransmission timeout; as I said, it could be quite lengthy.

Thanks,
sephe",8,"As for the userland send timeout.  It's kinda different, an example:
1) The sending was not blocked in the last send(2).
2) The server close(2)'ed the socket after the last send(2) call.  The data in the last send(2) was still in the TCP send buffer.
3) If the client's ACK for the last piece of data was excessively dropped, or the extreme case that the client itself was crashed.
4) Then the socket's (and inpcb in the BSD world) destruction would solely rely on the TCP retransmission timeout; as I said, it could be quite lengthy.

Thanks,
sephe"
2081,MDEV-14113,MDEV,Vladislav Vaintroub,104098,2017-12-05 17:08:25,"[~Sepherosa Ziehau]
 can you look at this patch to see if it will solve problems in most cases ?https://github.com/MariaDB/server/commit/8b09de139bf52777ba5e78821f3b64734bd22195

It would use abortive close, (no linger), if  connection ran into timeout. I think this is going to the most frequent cases, where server close connection first (usually client does it). In this case, server does not care about TCP  retransmission (as discussed, this would happen when writing large result sets and client application reads at slower pace than server writes)

There are some other cases where server would close the socket before client does it, e.g during connection on failed authentication. But  then, server sends an error message, and it does somewhat care that the client receives it.

The patch also cleans up socket timeout handling, so that Windows is consistent with another platforms (using nonblocking socket IO + waiting in poll/select on single socket, rather than setsockopt). This makes identifying timeouts easier.

I also removed the superficial shutdown() before closesocket() in places where it was used,  because it reportedly breaks SO_LINGER on Windows.

",9,"[~Sepherosa Ziehau]
 can you look at this patch to see if it will solve problems in most cases ?URL

It would use abortive close, (no linger), if  connection ran into timeout. I think this is going to the most frequent cases, where server close connection first (usually client does it). In this case, server does not care about TCP  retransmission (as discussed, this would happen when writing large result sets and client application reads at slower pace than server writes)

There are some other cases where server would close the socket before client does it, e.g during connection on failed authentication. But  then, server sends an error message, and it does somewhat care that the client receives it.

The patch also cleans up socket timeout handling, so that Windows is consistent with another platforms (using nonblocking socket IO + waiting in poll/select on single socket, rather than setsockopt). This makes identifying timeouts easier.

I also removed the superficial shutdown() before closesocket() in places where it was used,  because it reportedly breaks SO_LINGER on Windows.

"
2082,MDEV-14113,MDEV,Sepherosa Ziehau,104129,2017-12-06 03:50:22,This is definitely a step forward to the right direction!  The diff looks good.  Thank you very much!,10,This is definitely a step forward to the right direction!  The diff looks good.  Thank you very much!
2083,MDEV-14113,MDEV,Vladislav Vaintroub,104372,2017-12-12 14:30:06,I pushed  the mentioned patch into 10.3. Please let us know if there is anything else we should do WRT lingering.,11,I pushed  the mentioned patch into 10.3. Please let us know if there is anything else we should do WRT lingering.
2084,MDEV-14114,MDEV,Sergey Vojtovich,101829,2017-10-24 08:50:45,Note that default value was increased from IO_SIZE * 2 to IO_SIZE * 4. It would be nice if [~Elkin] could review this as well.,1,Note that default value was increased from IO_SIZE * 2 to IO_SIZE * 4. It would be nice if [~Elkin] could review this as well.
2085,MDEV-14114,MDEV,Jun Su,103076,2017-11-13 00:17:35,"Change looks good to me. One minor text change:
""binlog_init_cache_size"", ""The size of init malloc cache for the binary log."",
change to :
""binlog_init_cache_size"", ""The initial cache size for reading the binary log."",",2,"Change looks good to me. One minor text change:
""binlog_init_cache_size"", ""The size of init malloc cache for the binary log."",
change to :
""binlog_init_cache_size"", ""The initial cache size for reading the binary log."","
2086,MDEV-14114,MDEV,Vladislav Vaintroub,103078,2017-11-13 00:28:47,"[~junsu] the commit actually uses binlog_file_cache_size, as proposed by [~Elkin] ",3,"[~junsu] the commit actually uses binlog_file_cache_size, as proposed by [~Elkin] "
2087,MDEV-14114,MDEV,Jun Su,103095,2017-11-13 09:04:55,Contribution is contributed under BSD-new license.,4,Contribution is contributed under BSD-new license.
2088,MDEV-14135,MDEV,Elena Stepanova,102661,2017-11-05 00:06:00,"{{mysql-test/include/check-testcase.test}} will have a conflict due to MDEV-14029
{noformat}
let $datadir=`select @@datadir`;
<<<<<<< HEAD
list_files $datadir mysql_upgrade_info;
=======
list_files $datadir/test #sql*;
list_files $datadir/mysql #sql*;
>>>>>>> e691bef... MDEV-14029 Server does not remove #sql*.frm files after crash during ALTER TABLE
{noformat}
To resolve, add _all three_ {{list_files}} lines.
{noformat}
let $datadir=`select @@datadir`;
list_files $datadir mysql_upgrade_info;
list_files $datadir/test #sql*;
list_files $datadir/mysql #sql*;
{noformat}
",1,"{{mysql-test/include/check-testcase.test}} will have a conflict due to MDEV-14029
{noformat}
let $datadir=`select @@datadir`;
<<<<<<< HEAD
list_files $datadir mysql_upgrade_info;
=======
list_files $datadir/test #sql*;
list_files $datadir/mysql #sql*;
>>>>>>> e691bef... MDEV-14029 Server does not remove #sql*.frm files after crash during ALTER TABLE
{noformat}
To resolve, add _all three_ {{list_files}} lines.
{noformat}
let $datadir=`select @@datadir`;
list_files $datadir mysql_upgrade_info;
list_files $datadir/test #sql*;
list_files $datadir/mysql #sql*;
{noformat}
"
2089,MDEV-14286,MDEV,Elena Stepanova,102659,2017-11-04 22:03:00,"{noformat:title=bb-10.1-marko 1fb7ac18c417c8fbd95}
2017-11-05 00:02:20 7fd51f616b00  InnoDB: Assertion failure in thread 140553331239680 in file buf0buf.cc line 3555
InnoDB: Failing assertion: mode == BUF_GET_POSSIBLY_FREED || !fix_block->page.file_page_was_freed
InnoDB: We intentionally generate a memory trap.

#5  0x00007fd51d5aa3fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
#6  0x00007fd516a20cf9 in buf_page_get_gen (space=4, zip_size=0, offset=138, rw_latch=3, guess=0x0, mode=12, file=0x7fd516d01d38 ""/data/src/bb-10.1-marko/storage/innobase/row/row0import.cc"", line=2158, mtr=0x7fd51f614420, err=0x0) at /data/src/bb-10.1-marko/storage/innobase/buf/buf0buf.cc:3554
#7  0x00007fd516bd1876 in PageConverter::operator() (this=0x7fd51f614050, offset=2260992, block=0x7fd51f613370) at /data/src/bb-10.1-marko/storage/innobase/row/row0import.cc:2158
#8  0x00007fd516a98f3d in fil_iterate (iter=..., block=0x7fd51f613370, callback=...) at /data/src/bb-10.1-marko/storage/innobase/fil/fil0fil.cc:6512
#9  0x00007fd516a999ee in fil_tablespace_iterate (table=0x7fd515ebb8f8, n_io_buffers=64, callback=...) at /data/src/bb-10.1-marko/storage/innobase/fil/fil0fil.cc:6776
#10 0x00007fd516bd4ca9 in row_import_for_mysql (table=0x7fd515ebb8f8, prebuilt=0x7fd507583078) at /data/src/bb-10.1-marko/storage/innobase/row/row0import.cc:3604
#11 0x00007fd516af90a1 in ha_innodb::discard_or_import_tablespace (this=0x7fd5074b0088, discard=0 '\000') at /data/src/bb-10.1-marko/storage/innobase/handler/ha_innodb.cc:12412
#12 0x000055eedc944ec2 in handler::ha_discard_or_import_tablespace (this=0x7fd5074b0088, discard=0 '\000') at /data/src/bb-10.1-marko/sql/handler.cc:4189
#13 0x000055eedc7bf401 in mysql_discard_or_import_tablespace (thd=0x7fd513fc7070, table_list=0x7fd5074fa178, discard=false) at /data/src/bb-10.1-marko/sql/sql_table.cc:5666
#14 0x000055eedc83864d in Sql_cmd_discard_import_tablespace::execute (this=0x7fd5074fa770, thd=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_alter.cc:365
#15 0x000055eedc70917c in mysql_execute_command (thd=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:5680
#16 0x000055eedc70d42d in mysql_parse (thd=0x7fd513fc7070, rawbuf=0x7fd5074fa088 ""ALTER TABLE t1 IMPORT TABLESPACE"", length=32, parser_state=0x7fd51f6155e0) at /data/src/bb-10.1-marko/sql/sql_parse.cc:7326
#17 0x000055eedc6fc042 in dispatch_command (command=COM_QUERY, thd=0x7fd513fc7070, packet=0x7fd515ec9071 ""ALTER TABLE t1 IMPORT TABLESPACE"", packet_length=32) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1477
#18 0x000055eedc6fadc7 in do_command (thd=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1106
#19 0x000055eedc833a63 in do_handle_one_connection (thd_arg=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1349
#20 0x000055eedc8337c7 in handle_one_connection (arg=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1261
#21 0x000055eedcbecb98 in pfs_spawn_thread (arg=0x7fd515ebb8f0) at /data/src/bb-10.1-marko/storage/perfschema/pfs.cc:1861
#22 0x00007fd51f2a5494 in start_thread (arg=0x7fd51f616b00) at pthread_create.c:333
#23 0x00007fd51d65e93f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{code:sql|title=Run with --mysqld=--sequence}
--source include/have_innodb.inc

CREATE DATABASE db;
USE db;
--let $dir= `SELECT CONCAT(@@datadir,'/',DATABASE())`

CREATE TABLE t1 (pk INT PRIMARY KEY, f1 CHAR(255), UNIQUE(f1)) ENGINE=InnoDB;
INSERT INTO t1 SELECT seq, CONCAT('f1_',seq) FROM seq_1_to_10000;

FLUSH TABLE t1 FOR EXPORT;
--copy_file $dir/t1.ibd $dir/t1.ibd.backup
UNLOCK TABLES;

ALTER TABLE t1 DISCARD TABLESPACE;
--copy_file $dir/t1.ibd.backup $dir/t1.ibd
ALTER TABLE t1 IMPORT TABLESPACE;

# Cleanup
DROP TABLE t1;
{code}",1,"{noformat:title=bb-10.1-marko 1fb7ac18c417c8fbd95}
2017-11-05 00:02:20 7fd51f616b00  InnoDB: Assertion failure in thread 140553331239680 in file buf0buf.cc line 3555
InnoDB: Failing assertion: mode == BUF_GET_POSSIBLY_FREED || !fix_block->page.file_page_was_freed
InnoDB: We intentionally generate a memory trap.

#5  0x00007fd51d5aa3fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
#6  0x00007fd516a20cf9 in buf_page_get_gen (space=4, zip_size=0, offset=138, rw_latch=3, guess=0x0, mode=12, file=0x7fd516d01d38 ""/data/src/bb-10.1-marko/storage/innobase/row/row0import.cc"", line=2158, mtr=0x7fd51f614420, err=0x0) at /data/src/bb-10.1-marko/storage/innobase/buf/buf0buf.cc:3554
#7  0x00007fd516bd1876 in PageConverter::operator() (this=0x7fd51f614050, offset=2260992, block=0x7fd51f613370) at /data/src/bb-10.1-marko/storage/innobase/row/row0import.cc:2158
#8  0x00007fd516a98f3d in fil_iterate (iter=..., block=0x7fd51f613370, callback=...) at /data/src/bb-10.1-marko/storage/innobase/fil/fil0fil.cc:6512
#9  0x00007fd516a999ee in fil_tablespace_iterate (table=0x7fd515ebb8f8, n_io_buffers=64, callback=...) at /data/src/bb-10.1-marko/storage/innobase/fil/fil0fil.cc:6776
#10 0x00007fd516bd4ca9 in row_import_for_mysql (table=0x7fd515ebb8f8, prebuilt=0x7fd507583078) at /data/src/bb-10.1-marko/storage/innobase/row/row0import.cc:3604
#11 0x00007fd516af90a1 in ha_innodb::discard_or_import_tablespace (this=0x7fd5074b0088, discard=0 '\000') at /data/src/bb-10.1-marko/storage/innobase/handler/ha_innodb.cc:12412
#12 0x000055eedc944ec2 in handler::ha_discard_or_import_tablespace (this=0x7fd5074b0088, discard=0 '\000') at /data/src/bb-10.1-marko/sql/handler.cc:4189
#13 0x000055eedc7bf401 in mysql_discard_or_import_tablespace (thd=0x7fd513fc7070, table_list=0x7fd5074fa178, discard=false) at /data/src/bb-10.1-marko/sql/sql_table.cc:5666
#14 0x000055eedc83864d in Sql_cmd_discard_import_tablespace::execute (this=0x7fd5074fa770, thd=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_alter.cc:365
#15 0x000055eedc70917c in mysql_execute_command (thd=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:5680
#16 0x000055eedc70d42d in mysql_parse (thd=0x7fd513fc7070, rawbuf=0x7fd5074fa088 ""ALTER TABLE t1 IMPORT TABLESPACE"", length=32, parser_state=0x7fd51f6155e0) at /data/src/bb-10.1-marko/sql/sql_parse.cc:7326
#17 0x000055eedc6fc042 in dispatch_command (command=COM_QUERY, thd=0x7fd513fc7070, packet=0x7fd515ec9071 ""ALTER TABLE t1 IMPORT TABLESPACE"", packet_length=32) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1477
#18 0x000055eedc6fadc7 in do_command (thd=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1106
#19 0x000055eedc833a63 in do_handle_one_connection (thd_arg=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1349
#20 0x000055eedc8337c7 in handle_one_connection (arg=0x7fd513fc7070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1261
#21 0x000055eedcbecb98 in pfs_spawn_thread (arg=0x7fd515ebb8f0) at /data/src/bb-10.1-marko/storage/perfschema/pfs.cc:1861
#22 0x00007fd51f2a5494 in start_thread (arg=0x7fd51f616b00) at pthread_create.c:333
#23 0x00007fd51d65e93f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{code:sql|title=Run with --mysqld=--sequence}
--source include/have_innodb.inc

CREATE DATABASE db;
USE db;
--let $dir= `SELECT CONCAT(@@datadir,'/',DATABASE())`

CREATE TABLE t1 (pk INT PRIMARY KEY, f1 CHAR(255), UNIQUE(f1)) ENGINE=InnoDB;
INSERT INTO t1 SELECT seq, CONCAT('f1_',seq) FROM seq_1_to_10000;

FLUSH TABLE t1 FOR EXPORT;
--copy_file $dir/t1.ibd $dir/t1.ibd.backup
UNLOCK TABLES;

ALTER TABLE t1 DISCARD TABLESPACE;
--copy_file $dir/t1.ibd.backup $dir/t1.ibd
ALTER TABLE t1 IMPORT TABLESPACE;

# Cleanup
DROP TABLE t1;
{code}"
2090,MDEV-14286,MDEV,Marko Mäkelä,102720,2017-11-06 09:44:04,"I refactored the code so that instead of renaming BUF_PEEK_IF_IN_POOL (and slightly changing its semantics), a new mode BUF_EVICT_IF_IN_POOL is introduced.
This should avoid the debug assertion.

The assertion normally makes sense, because it tries to catch access to pages that were marked as free. In IMPORT, we are only accessing the pages in order to evict them from the buffer pool; we do not care about the page contents. Therefore the assertion failure was not a sign of real trouble.",2,"I refactored the code so that instead of renaming BUF_PEEK_IF_IN_POOL (and slightly changing its semantics), a new mode BUF_EVICT_IF_IN_POOL is introduced.
This should avoid the debug assertion.

The assertion normally makes sense, because it tries to catch access to pages that were marked as free. In IMPORT, we are only accessing the pages in order to evict them from the buffer pool; we do not care about the page contents. Therefore the assertion failure was not a sign of real trouble."
2091,MDEV-14286,MDEV,Elena Stepanova,102766,2017-11-06 17:31:16,"h2. Failures encountered during initial adjustments of the test, which are considered irrelevant to this bugfix and attributed to either misuse of the feature, or its general (known) instability

{noformat}
InnoDB: Number of bytes after aio 0 requested 16384
InnoDB: from file ./test/t28435.ibd
 InnoDB: Operation Linux aio to file /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc and at line 5648
2017-11-04 17:25:08 140429567706880 [ERROR] InnoDB: File ./test/t28435.ibd: 'Linux aio' returned OS error 0. Cannot continue operation
171104 17:25:08 [ERROR] mysqld got signal 6 ;

# 2017-11-04T17:25:18 [28346] Thread 1 (Thread 0x7fb84e7fc700 (LWP 28401)):
# 2017-11-04T17:25:18 [28346] #0  __pthread_kill (threadid=<optimized out>, signo=6) at ../sysdeps/unix/sysv/linux/pthread_kill.c:57
# 2017-11-04T17:25:18 [28346] #1  0x0000564e33e8bd7f in my_write_core (sig=6) at /data/src/bb-10.1-marko/mysys/stacktrace.c:477
# 2017-11-04T17:25:18 [28346] #2  0x0000564e3382abea in handle_fatal_signal (sig=6) at /data/src/bb-10.1-marko/sql/signal_handler.cc:296
# 2017-11-04T17:25:18 [28346] #3  <signal handler called>
# 2017-11-04T17:25:18 [28346] #4  0x00007fb86e2e8fcf in raise () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T17:25:18 [28346] #5  0x00007fb86e2ea3fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T17:25:18 [28346] #6  0x0000564e33bfdbbd in os_file_handle_error_cond_exit (name=0x7fb83fc16278 ""./test/t28435.ibd"", operation=0x564e340a16d1 ""Linux aio"", should_exit=1, on_error_silent=0, file=0x564e340a0390 ""/data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc"", line=5648) at /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc:915
# 2017-11-04T17:25:18 [28346] #7  0x0000564e33bfdc04 in os_file_handle_error (name=0x7fb83fc16278 ""./test/t28435.ibd"", operation=0x564e340a16d1 ""Linux aio"", file=0x564e340a0390 ""/data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc"", line=5648) at /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc:935
# 2017-11-04T17:25:18 [28346] #8  0x0000564e33c032e3 in os_aio_linux_handle (global_seg=3, message1=0x7fb84e7fbe40, message2=0x7fb84e7fbe48, type=0x7fb84e7fbe50, space_id=0x7fb84e7fbe58) at /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc:5648
# 2017-11-04T17:25:18 [28346] #9  0x0000564e33ddbc9b in fil_aio_wait (segment=3) at /data/src/bb-10.1-marko/storage/xtradb/fil/fil0fil.cc:6372
# 2017-11-04T17:25:18 [28346] #10 0x0000564e33ccbede in io_handler_thread (arg=0x564e351397d8 <n+24>) at /data/src/bb-10.1-marko/storage/xtradb/srv/srv0start.cc:585
# 2017-11-04T17:25:18 [28346] #11 0x00007fb86ffe5494 in start_thread (arg=0x7fb84e7fc700) at pthread_create.c:333
# 2017-11-04T17:25:18 [28346] #12 0x00007fb86e39e93f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{noformat}
2017-11-04 17:52:52 7f81603d6b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 17:52:52 140193642146560 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t29573.ibd
2017-11-04 17:52:52 140193642146560 [Note] InnoDB: Discarding tablespace of table ""test"".""t29573"": Tablespace not found
2017-11-04 17:52:52 140193606486784 [ERROR] InnoDB: Cannot delete tablespace 68 because it is not found in the tablespace memory cache.
2017-11-04 17:52:52 140193606486784 [Warning] InnoDB: Cannot delete tablespace 68 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 17:52:53 140193642449664 [ERROR] InnoDB: Cannot delete tablespace 69 because it is not found in the tablespace memory cache.
2017-11-04 17:52:53 140193642449664 [Warning] InnoDB: Cannot delete tablespace 69 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 17:52:53 140193606486784 [Note] InnoDB: Sync to disk
...
2017-11-04 17:52:54 140193642449664 [Note] InnoDB: Phase IV - Flush complete
2017-11-04 17:52:54 7f81603d6b00  InnoDB: Assertion failure in thread 140193642146560 in file log0log.cc line 2292
InnoDB: Failing assertion: oldest_lsn >= log_sys->next_checkpoint_lsn

# 2017-11-04T17:53:02 [29494] #5  0x00007f815e4123fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T17:53:02 [29494] #6  0x00005583101f193d in log_checkpoint (sync=1, write_always=1, safe_to_ignore=0) at /data/src/bb-10.1-marko/storage/xtradb/log/log0log.cc:2292
# 2017-11-04T17:53:02 [29494] #7  0x00005583101f1af6 in log_make_checkpoint_at (lsn=18446744073709551615, write_always=1) at /data/src/bb-10.1-marko/storage/xtradb/log/log0log.cc:2351
# 2017-11-04T17:53:02 [29494] #8  0x0000558310265627 in row_import_cleanup (prebuilt=0x7f812fcfb078, trx=0x7f812fcb0d78, err=DB_TABLESPACE_NOT_FOUND) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:2270
# 2017-11-04T17:53:02 [29494] #9  0x0000558310265705 in row_import_error (prebuilt=0x7f812fcfb078, trx=0x7f812fcb0d78, err=DB_TABLESPACE_NOT_FOUND) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:2298
# 2017-11-04T17:53:02 [29494] #10 0x000055831026856d in row_import_for_mysql (table=0x7f812fcc5ff8, prebuilt=0x7f812fcfb078) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:3594
# 2017-11-04T17:53:02 [29494] #11 0x0000558310186f27 in ha_innobase::discard_or_import_tablespace (this=0x7f812fc91888, discard=0 '\000') at /data/src/bb-10.1-marko/storage/xtradb/handler/ha_innodb.cc:12988
# 2017-11-04T17:53:02 [29494] #12 0x000055830fe4aec2 in handler::ha_discard_or_import_tablespace (this=0x7f812fc91888, discard=0 '\000') at /data/src/bb-10.1-marko/sql/handler.cc:4189
# 2017-11-04T17:53:02 [29494] #13 0x000055830fcc5401 in mysql_discard_or_import_tablespace (thd=0x7f813a777070, table_list=0x7f812fc221b0, discard=false) at /data/src/bb-10.1-marko/sql/sql_table.cc:5666
# 2017-11-04T17:53:02 [29494] #14 0x000055830fd3e64d in Sql_cmd_discard_import_tablespace::execute (this=0x7f812fc227a8, thd=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_alter.cc:365
# 2017-11-04T17:53:02 [29494] #15 0x000055830fc0f17c in mysql_execute_command (thd=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:5680
# 2017-11-04T17:53:02 [29494] #16 0x000055830fc1342d in mysql_parse (thd=0x7f813a777070, rawbuf=0x7f812fc22088 ""ALTER TABLE t29573 IMPORT TABLESPACE /* QNO 324 CON_ID 7 */"", length=59, parser_state=0x7f81603d5630) at /data/src/bb-10.1-marko/sql/sql_parse.cc:7326
# 2017-11-04T17:53:02 [29494] #17 0x000055830fc02042 in dispatch_command (command=COM_QUERY, thd=0x7f813a777070, packet=0x7f813a77d071 "" ALTER TABLE t29573 IMPORT TABLESPACE /* QNO 324 CON_ID 7 */ "", packet_length=61) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1477
# 2017-11-04T17:53:02 [29494] #18 0x000055830fc00dc7 in do_command (thd=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1106
# 2017-11-04T17:53:02 [29494] #19 0x000055830fd39a63 in do_handle_one_connection (thd_arg=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1349
# 2017-11-04T17:53:02 [29494] #20 0x000055830fd397c7 in handle_one_connection (arg=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1261
# 2017-11-04T17:53:02 [29494] #21 0x00007f816010d494 in start_thread (arg=0x7f81603d6b00) at pthread_create.c:333
# 2017-11-04T17:53:02 [29494] #22 0x00007f815e4c693f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{noformat}
2017-11-04 19:16:34 140404377053952 [ERROR] InnoDB: Cannot delete tablespace 25 because it is not found in the tablespace memory cache.
2017-11-04 19:16:34 140404377053952 [Warning] InnoDB: Cannot delete tablespace 25 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Phase I - Update all pages
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Discarding tablespace of table ""test"".""t7662"": Data structure corruption
2017-11-04 19:16:34 140404413319936 [ERROR] InnoDB: The table test/t7656 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413319936 [Warning] InnoDB: Trying to access missing tablespace 23.
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413016832 [ERROR] Got error 155 when reading table './test/t7658'
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404377053952 [ERROR] InnoDB: The table test/t7662 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:35 140404413016832 [ERROR] InnoDB: Cannot delete tablespace 14 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404413016832 [Warning] InnoDB: Cannot delete tablespace 14 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: Cannot delete tablespace 23 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404377053952 [ERROR] InnoDB: Cannot delete tablespace 25 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404377053952 [Warning] InnoDB: Cannot delete tablespace 25 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:35 140404413016832 [Note] InnoDB: Sync to disk
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: The table test/t7656 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: The table test/t7656 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:35 140404377053952 [Note] InnoDB: Sync to disk
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: Cannot delete tablespace 26 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404413319936 [Warning] InnoDB: Cannot delete tablespace 26 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:35 140404413016832 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:35 140404377053952 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:36 140404377053952 [Note] InnoDB: Phase I - Update all pages
2017-11-04 19:16:36 140404413016832 [Note] InnoDB: Phase I - Update all pages
2017-11-04 19:16:36 140404413016832 [Note] InnoDB: Sync to disk
2017-11-04 19:16:36 7fb27104eb00  InnoDB: Assertion failure in thread 140404377053952 in file rem0rec.ic line 998
InnoDB: Failing assertion: curr <= last

# 2017-11-04T19:16:43 [7580] #5  0x00007fb2712d63fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T19:16:43 [7580] #6  0x0000558b6c94056d in rec_offs_validate (rec=0x0, index=0x0, offsets=0x7fb27104c138) at /data/src/bb-10.1-marko/storage/xtradb/include/rem0rec.ic:998
# 2017-11-04T19:16:43 [7580] #7  0x0000558b6c9406ac in rec_offs_any_extern (offsets=0x7fb27104c138) at /data/src/bb-10.1-marko/storage/xtradb/include/rem0rec.ic:1086
# 2017-11-04T19:16:43 [7580] #8  0x0000558b6c9444b6 in PageConverter::adjust_cluster_index_blob_ref (this=0x7fb27104c0a0, rec=0x7fb241d90080 ""\200"", offsets=0x7fb27104c138) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1717
# 2017-11-04T19:16:43 [7580] #9  0x0000558b6c9445a5 in PageConverter::adjust_cluster_record (this=0x7fb27104c0a0, index=0x7fb241c33078, rec=0x7fb241d90080 ""\200"", offsets=0x7fb27104c138, deleted=false) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1768
# 2017-11-04T19:16:43 [7580] #10 0x0000558b6c94474e in PageConverter::update_records (this=0x7fb27104c0a0, block=0x7fb27104b1d0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1821
# 2017-11-04T19:16:43 [7580] #11 0x0000558b6c9449db in PageConverter::update_index_page (this=0x7fb27104c0a0, block=0x7fb27104b1d0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1914
# 2017-11-04T19:16:43 [7580] #12 0x0000558b6c944c91 in PageConverter::update_page (this=0x7fb27104c0a0, block=0x7fb27104b1d0, page_type=@0x7fb27104afb8: 17855) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1987
# 2017-11-04T19:16:43 [7580] #13 0x0000558b6c944fc8 in PageConverter::operator() (this=0x7fb27104c0a0, offset=163840, block=0x7fb27104b1d0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:2108
# 2017-11-04T19:16:43 [7580] #14 0x0000558b6cad2b42 in fil_iterate (iter=..., block=0x7fb27104b1d0, callback=...) at /data/src/bb-10.1-marko/storage/xtradb/fil/fil0fil.cc:6848
# 2017-11-04T19:16:43 [7580] #15 0x0000558b6cad3605 in fil_tablespace_iterate (table=0x7fb241cc55f8, n_io_buffers=64, callback=...) at /data/src/bb-10.1-marko/storage/xtradb/fil/fil0fil.cc:7112
# 2017-11-04T19:16:43 [7580] #16 0x0000558b6c9485f3 in row_import_for_mysql (table=0x7fb241cc55f8, prebuilt=0x7fb241c2b078) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:3609
# 2017-11-04T19:16:43 [7580] #17 0x0000558b6c866f27 in ha_innobase::discard_or_import_tablespace (this=0x7fb241c91088, discard=0 '\000') at /data/src/bb-10.1-marko/storage/xtradb/handler/ha_innodb.cc:12988
# 2017-11-04T19:16:43 [7580] #18 0x0000558b6c52aec2 in handler::ha_discard_or_import_tablespace (this=0x7fb241c91088, discard=0 '\000') at /data/src/bb-10.1-marko/sql/handler.cc:4189
# 2017-11-04T19:16:43 [7580] #19 0x0000558b6c3a5401 in mysql_discard_or_import_tablespace (thd=0x7fb24d78d070, table_list=0x7fb241c221b0, discard=false) at /data/src/bb-10.1-marko/sql/sql_table.cc:5666
# 2017-11-04T19:16:43 [7580] #20 0x0000558b6c41e64d in Sql_cmd_discard_import_tablespace::execute (this=0x7fb241c227a8, thd=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_alter.cc:365
# 2017-11-04T19:16:43 [7580] #21 0x0000558b6c2ef17c in mysql_execute_command (thd=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:5680
# 2017-11-04T19:16:43 [7580] #22 0x0000558b6c2f342d in mysql_parse (thd=0x7fb24d78d070, rawbuf=0x7fb241c22088 ""ALTER TABLE t7662 IMPORT TABLESPACE /* QNO 227 CON_ID 9 */"", length=58, parser_state=0x7fb27104d630) at /data/src/bb-10.1-marko/sql/sql_parse.cc:7326
# 2017-11-04T19:16:43 [7580] #23 0x0000558b6c2e2042 in dispatch_command (command=COM_QUERY, thd=0x7fb24d78d070, packet=0x7fb24d793071 "" ALTER TABLE t7662 IMPORT TABLESPACE /* QNO 227 CON_ID 9 */ "", packet_length=60) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1477
# 2017-11-04T19:16:43 [7580] #24 0x0000558b6c2e0dc7 in do_command (thd=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1106
# 2017-11-04T19:16:43 [7580] #25 0x0000558b6c419a63 in do_handle_one_connection (thd_arg=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1349
# 2017-11-04T19:16:43 [7580] #26 0x0000558b6c4197c7 in handle_one_connection (arg=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1261
# 2017-11-04T19:16:43 [7580] #27 0x00007fb272fd1494 in start_thread (arg=0x7fb27104eb00) at pthread_create.c:333
# 2017-11-04T19:16:43 [7580] #28 0x00007fb27138a93f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{noformat}
2017-11-04 21:01:35 7f1f92847b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:35 139773578869504 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t2.ibd
2017-11-04 21:01:35 139773578869504 [Note] InnoDB: Discarding tablespace of table ""test"".""t2"": Tablespace not found
2017-11-04 21:01:38 7f1f92891b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:38 139773579172608 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t1.ibd
2017-11-04 21:01:38 139773579172608 [Note] InnoDB: Discarding tablespace of table ""test"".""t1"": Tablespace not found
2017-11-04 21:01:38 7f1f8f561b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:38 139773525498624 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t6.ibd
2017-11-04 21:01:38 139773525498624 [Note] InnoDB: Discarding tablespace of table ""test"".""t6"": Tablespace not found
2017-11-04 21:01:38 7f1f8f5f5b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:39 139773526104832 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t4.ibd
2017-11-04 21:01:39 139773526104832 [Note] InnoDB: Discarding tablespace of table ""test"".""t4"": Tablespace not found
2017-11-04 21:01:40 7f1f8f5abb00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:40 139773525801728 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t5.ibd
2017-11-04 21:01:40 139773525801728 [Note] InnoDB: Discarding tablespace of table ""test"".""t5"": Tablespace not found
2017-11-04 21:01:40 7f1f927fdb00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:40 139773578566400 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t3.ibd
2017-11-04 21:01:40 139773578566400 [Note] InnoDB: Discarding tablespace of table ""test"".""t3"": Tablespace not found
2017-11-04 21:01:49 7f1f8f4cdb00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:49 139773524892416 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t8.ibd
2017-11-04 21:01:49 139773524892416 [Note] InnoDB: Discarding tablespace of table ""test"".""t8"": Tablespace not found
2017-11-04 21:01:49 7f1f68ffb700  InnoDB: Assertion failure in thread 139772882302720 in file btr0cur.cc line 792
InnoDB: Failing assertion: fil_page_get_type(page) == 17855

# 2017-11-04T21:01:52 [14021] #5  0x00007f1f908393fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T21:01:52 [14021] #6  0x0000556818b6e1c2 in btr_cur_search_to_nth_level (index=0x7f1f63859578, level=0, tuple=0x7f1f67852178, mode=4, latch_mode=2, cursor=0x7f1f68ffa650, has_search_latch=0, file=0x556818efc648 ""/d
ata/src/bb-10.1-marko/storage/xtradb/row/row0row.cc"", line=823, mtr=0x7f1f68ffa730) at /data/src/bb-10.1-marko/storage/xtradb/btr/btr0cur.cc:792
# 2017-11-04T21:01:52 [14021] #7  0x0000556818ad4d2c in btr_pcur_open_low (index=0x7f1f63859578, level=0, tuple=0x7f1f67852178, mode=4, latch_mode=8194, cursor=0x7f1f68ffa650, file=0x556818efc648 ""/data/src/bb-10.1-marko/stora
ge/xtradb/row/row0row.cc"", line=823, mtr=0x7f1f68ffa730) at /data/src/bb-10.1-marko/storage/xtradb/include/btr0pcur.ic:440
# 2017-11-04T21:01:52 [14021] #8  0x0000556818ad6bac in row_search_index_entry (index=0x7f1f63859578, entry=0x7f1f67852178, mode=8194, pcur=0x7f1f68ffa650, mtr=0x7f1f68ffa730) at /data/src/bb-10.1-marko/storage/xtradb/row/row0
row.cc:823
# 2017-11-04T21:01:52 [14021] #9  0x0000556818ad0758 in row_purge_remove_sec_if_poss_leaf (node=0x7f1f9030ea78, index=0x7f1f63859578, entry=0x7f1f67852178) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:452
# 2017-11-04T21:01:52 [14021] #10 0x0000556818ad09c7 in row_purge_remove_sec_if_poss (node=0x7f1f9030ea78, index=0x7f1f63859578, entry=0x7f1f67852178) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:533
# 2017-11-04T21:01:52 [14021] #11 0x0000556818ad0b29 in row_purge_del_mark (node=0x7f1f9030ea78) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:581
# 2017-11-04T21:01:52 [14021] #12 0x0000556818ad147a in row_purge_record_func (node=0x7f1f9030ea78, undo_rec=0x7f1f6781c088 ""\025\372\016"", thr=0x7f1f6ceaffc0, updated_extern=false) at /data/src/bb-10.1-marko/storage/xtradb/ro
w/row0purge.cc:860
# 2017-11-04T21:01:52 [14021] #13 0x0000556818ad16a6 in row_purge (node=0x7f1f9030ea78, undo_rec=0x7f1f6781c088 ""\025\372\016"", thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:916
# 2017-11-04T21:01:52 [14021] #14 0x0000556818ad194e in row_purge_step (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:996
# 2017-11-04T21:01:52 [14021] #15 0x0000556818a6f21b in que_thr_step (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/que/que0que.cc:1089
# 2017-11-04T21:01:52 [14021] #16 0x0000556818a6f41f in que_run_threads_low (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/que/que0que.cc:1151
# 2017-11-04T21:01:52 [14021] #17 0x0000556818a6f5ab in que_run_threads (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/que/que0que.cc:1192
# 2017-11-04T21:01:52 [14021] #18 0x0000556818b207d2 in trx_purge (n_purge_threads=1, batch_size=300, truncate=false) at /data/src/bb-10.1-marko/storage/xtradb/trx/trx0purge.cc:1254
# 2017-11-04T21:01:52 [14021] #19 0x0000556818b045ba in srv_do_purge (n_threads=1, n_total_purged=0x7f1f68ffaea0) at /data/src/bb-10.1-marko/storage/xtradb/srv/srv0srv.cc:3442
# 2017-11-04T21:01:52 [14021] #20 0x0000556818b04b5d in srv_purge_coordinator_thread (arg=0x0) at /data/src/bb-10.1-marko/storage/xtradb/srv/srv0srv.cc:3592
# 2017-11-04T21:01:52 [14021] #21 0x00007f1f92534494 in start_thread (arg=0x7f1f68ffb700) at pthread_create.c:333
{noformat}

",3,"h2. Failures encountered during initial adjustments of the test, which are considered irrelevant to this bugfix and attributed to either misuse of the feature, or its general (known) instability

{noformat}
InnoDB: Number of bytes after aio 0 requested 16384
InnoDB: from file ./test/t28435.ibd
 InnoDB: Operation Linux aio to file /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc and at line 5648
2017-11-04 17:25:08 140429567706880 [ERROR] InnoDB: File ./test/t28435.ibd: 'Linux aio' returned OS error 0. Cannot continue operation
171104 17:25:08 [ERROR] mysqld got signal 6 ;

# 2017-11-04T17:25:18 [28346] Thread 1 (Thread 0x7fb84e7fc700 (LWP 28401)):
# 2017-11-04T17:25:18 [28346] #0  __pthread_kill (threadid=, signo=6) at ../sysdeps/unix/sysv/linux/pthread_kill.c:57
# 2017-11-04T17:25:18 [28346] #1  0x0000564e33e8bd7f in my_write_core (sig=6) at /data/src/bb-10.1-marko/mysys/stacktrace.c:477
# 2017-11-04T17:25:18 [28346] #2  0x0000564e3382abea in handle_fatal_signal (sig=6) at /data/src/bb-10.1-marko/sql/signal_handler.cc:296
# 2017-11-04T17:25:18 [28346] #3  
# 2017-11-04T17:25:18 [28346] #4  0x00007fb86e2e8fcf in raise () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T17:25:18 [28346] #5  0x00007fb86e2ea3fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T17:25:18 [28346] #6  0x0000564e33bfdbbd in os_file_handle_error_cond_exit (name=0x7fb83fc16278 ""./test/t28435.ibd"", operation=0x564e340a16d1 ""Linux aio"", should_exit=1, on_error_silent=0, file=0x564e340a0390 ""/data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc"", line=5648) at /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc:915
# 2017-11-04T17:25:18 [28346] #7  0x0000564e33bfdc04 in os_file_handle_error (name=0x7fb83fc16278 ""./test/t28435.ibd"", operation=0x564e340a16d1 ""Linux aio"", file=0x564e340a0390 ""/data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc"", line=5648) at /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc:935
# 2017-11-04T17:25:18 [28346] #8  0x0000564e33c032e3 in os_aio_linux_handle (global_seg=3, message1=0x7fb84e7fbe40, message2=0x7fb84e7fbe48, type=0x7fb84e7fbe50, space_id=0x7fb84e7fbe58) at /data/src/bb-10.1-marko/storage/xtradb/os/os0file.cc:5648
# 2017-11-04T17:25:18 [28346] #9  0x0000564e33ddbc9b in fil_aio_wait (segment=3) at /data/src/bb-10.1-marko/storage/xtradb/fil/fil0fil.cc:6372
# 2017-11-04T17:25:18 [28346] #10 0x0000564e33ccbede in io_handler_thread (arg=0x564e351397d8 ) at /data/src/bb-10.1-marko/storage/xtradb/srv/srv0start.cc:585
# 2017-11-04T17:25:18 [28346] #11 0x00007fb86ffe5494 in start_thread (arg=0x7fb84e7fc700) at pthread_create.c:333
# 2017-11-04T17:25:18 [28346] #12 0x00007fb86e39e93f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{noformat}
2017-11-04 17:52:52 7f81603d6b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 17:52:52 140193642146560 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t29573.ibd
2017-11-04 17:52:52 140193642146560 [Note] InnoDB: Discarding tablespace of table ""test"".""t29573"": Tablespace not found
2017-11-04 17:52:52 140193606486784 [ERROR] InnoDB: Cannot delete tablespace 68 because it is not found in the tablespace memory cache.
2017-11-04 17:52:52 140193606486784 [Warning] InnoDB: Cannot delete tablespace 68 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 17:52:53 140193642449664 [ERROR] InnoDB: Cannot delete tablespace 69 because it is not found in the tablespace memory cache.
2017-11-04 17:52:53 140193642449664 [Warning] InnoDB: Cannot delete tablespace 69 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 17:52:53 140193606486784 [Note] InnoDB: Sync to disk
...
2017-11-04 17:52:54 140193642449664 [Note] InnoDB: Phase IV - Flush complete
2017-11-04 17:52:54 7f81603d6b00  InnoDB: Assertion failure in thread 140193642146560 in file log0log.cc line 2292
InnoDB: Failing assertion: oldest_lsn >= log_sys->next_checkpoint_lsn

# 2017-11-04T17:53:02 [29494] #5  0x00007f815e4123fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T17:53:02 [29494] #6  0x00005583101f193d in log_checkpoint (sync=1, write_always=1, safe_to_ignore=0) at /data/src/bb-10.1-marko/storage/xtradb/log/log0log.cc:2292
# 2017-11-04T17:53:02 [29494] #7  0x00005583101f1af6 in log_make_checkpoint_at (lsn=18446744073709551615, write_always=1) at /data/src/bb-10.1-marko/storage/xtradb/log/log0log.cc:2351
# 2017-11-04T17:53:02 [29494] #8  0x0000558310265627 in row_import_cleanup (prebuilt=0x7f812fcfb078, trx=0x7f812fcb0d78, err=DB_TABLESPACE_NOT_FOUND) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:2270
# 2017-11-04T17:53:02 [29494] #9  0x0000558310265705 in row_import_error (prebuilt=0x7f812fcfb078, trx=0x7f812fcb0d78, err=DB_TABLESPACE_NOT_FOUND) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:2298
# 2017-11-04T17:53:02 [29494] #10 0x000055831026856d in row_import_for_mysql (table=0x7f812fcc5ff8, prebuilt=0x7f812fcfb078) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:3594
# 2017-11-04T17:53:02 [29494] #11 0x0000558310186f27 in ha_innobase::discard_or_import_tablespace (this=0x7f812fc91888, discard=0 '\000') at /data/src/bb-10.1-marko/storage/xtradb/handler/ha_innodb.cc:12988
# 2017-11-04T17:53:02 [29494] #12 0x000055830fe4aec2 in handler::ha_discard_or_import_tablespace (this=0x7f812fc91888, discard=0 '\000') at /data/src/bb-10.1-marko/sql/handler.cc:4189
# 2017-11-04T17:53:02 [29494] #13 0x000055830fcc5401 in mysql_discard_or_import_tablespace (thd=0x7f813a777070, table_list=0x7f812fc221b0, discard=false) at /data/src/bb-10.1-marko/sql/sql_table.cc:5666
# 2017-11-04T17:53:02 [29494] #14 0x000055830fd3e64d in Sql_cmd_discard_import_tablespace::execute (this=0x7f812fc227a8, thd=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_alter.cc:365
# 2017-11-04T17:53:02 [29494] #15 0x000055830fc0f17c in mysql_execute_command (thd=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:5680
# 2017-11-04T17:53:02 [29494] #16 0x000055830fc1342d in mysql_parse (thd=0x7f813a777070, rawbuf=0x7f812fc22088 ""ALTER TABLE t29573 IMPORT TABLESPACE /* QNO 324 CON_ID 7 */"", length=59, parser_state=0x7f81603d5630) at /data/src/bb-10.1-marko/sql/sql_parse.cc:7326
# 2017-11-04T17:53:02 [29494] #17 0x000055830fc02042 in dispatch_command (command=COM_QUERY, thd=0x7f813a777070, packet=0x7f813a77d071 "" ALTER TABLE t29573 IMPORT TABLESPACE /* QNO 324 CON_ID 7 */ "", packet_length=61) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1477
# 2017-11-04T17:53:02 [29494] #18 0x000055830fc00dc7 in do_command (thd=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1106
# 2017-11-04T17:53:02 [29494] #19 0x000055830fd39a63 in do_handle_one_connection (thd_arg=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1349
# 2017-11-04T17:53:02 [29494] #20 0x000055830fd397c7 in handle_one_connection (arg=0x7f813a777070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1261
# 2017-11-04T17:53:02 [29494] #21 0x00007f816010d494 in start_thread (arg=0x7f81603d6b00) at pthread_create.c:333
# 2017-11-04T17:53:02 [29494] #22 0x00007f815e4c693f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{noformat}
2017-11-04 19:16:34 140404377053952 [ERROR] InnoDB: Cannot delete tablespace 25 because it is not found in the tablespace memory cache.
2017-11-04 19:16:34 140404377053952 [Warning] InnoDB: Cannot delete tablespace 25 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Phase I - Update all pages
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:34 140404377053952 [Note] InnoDB: Discarding tablespace of table ""test"".""t7662"": Data structure corruption
2017-11-04 19:16:34 140404413319936 [ERROR] InnoDB: The table test/t7656 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413319936 [Warning] InnoDB: Trying to access missing tablespace 23.
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413016832 [ERROR] Got error 155 when reading table './test/t7658'
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404413016832 [ERROR] InnoDB: The table test/t7658 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:34 140404377053952 [ERROR] InnoDB: The table test/t7662 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:35 140404413016832 [ERROR] InnoDB: Cannot delete tablespace 14 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404413016832 [Warning] InnoDB: Cannot delete tablespace 14 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: Cannot delete tablespace 23 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404377053952 [ERROR] InnoDB: Cannot delete tablespace 25 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404377053952 [Warning] InnoDB: Cannot delete tablespace 25 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:35 140404413016832 [Note] InnoDB: Sync to disk
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: The table test/t7656 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: The table test/t7656 doesn't have a corresponding tablespace, it was discarded.
2017-11-04 19:16:35 140404377053952 [Note] InnoDB: Sync to disk
2017-11-04 19:16:35 140404413319936 [ERROR] InnoDB: Cannot delete tablespace 26 because it is not found in the tablespace memory cache.
2017-11-04 19:16:35 140404413319936 [Warning] InnoDB: Cannot delete tablespace 26 in DISCARD TABLESPACE. Tablespace not found
2017-11-04 19:16:35 140404413016832 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:35 140404377053952 [Note] InnoDB: Sync to disk - done!
2017-11-04 19:16:36 140404377053952 [Note] InnoDB: Phase I - Update all pages
2017-11-04 19:16:36 140404413016832 [Note] InnoDB: Phase I - Update all pages
2017-11-04 19:16:36 140404413016832 [Note] InnoDB: Sync to disk
2017-11-04 19:16:36 7fb27104eb00  InnoDB: Assertion failure in thread 140404377053952 in file rem0rec.ic line 998
InnoDB: Failing assertion: curr <= last

# 2017-11-04T19:16:43 [7580] #5  0x00007fb2712d63fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T19:16:43 [7580] #6  0x0000558b6c94056d in rec_offs_validate (rec=0x0, index=0x0, offsets=0x7fb27104c138) at /data/src/bb-10.1-marko/storage/xtradb/include/rem0rec.ic:998
# 2017-11-04T19:16:43 [7580] #7  0x0000558b6c9406ac in rec_offs_any_extern (offsets=0x7fb27104c138) at /data/src/bb-10.1-marko/storage/xtradb/include/rem0rec.ic:1086
# 2017-11-04T19:16:43 [7580] #8  0x0000558b6c9444b6 in PageConverter::adjust_cluster_index_blob_ref (this=0x7fb27104c0a0, rec=0x7fb241d90080 ""\200"", offsets=0x7fb27104c138) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1717
# 2017-11-04T19:16:43 [7580] #9  0x0000558b6c9445a5 in PageConverter::adjust_cluster_record (this=0x7fb27104c0a0, index=0x7fb241c33078, rec=0x7fb241d90080 ""\200"", offsets=0x7fb27104c138, deleted=false) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1768
# 2017-11-04T19:16:43 [7580] #10 0x0000558b6c94474e in PageConverter::update_records (this=0x7fb27104c0a0, block=0x7fb27104b1d0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1821
# 2017-11-04T19:16:43 [7580] #11 0x0000558b6c9449db in PageConverter::update_index_page (this=0x7fb27104c0a0, block=0x7fb27104b1d0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1914
# 2017-11-04T19:16:43 [7580] #12 0x0000558b6c944c91 in PageConverter::update_page (this=0x7fb27104c0a0, block=0x7fb27104b1d0, page_type=@0x7fb27104afb8: 17855) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:1987
# 2017-11-04T19:16:43 [7580] #13 0x0000558b6c944fc8 in PageConverter::operator() (this=0x7fb27104c0a0, offset=163840, block=0x7fb27104b1d0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:2108
# 2017-11-04T19:16:43 [7580] #14 0x0000558b6cad2b42 in fil_iterate (iter=..., block=0x7fb27104b1d0, callback=...) at /data/src/bb-10.1-marko/storage/xtradb/fil/fil0fil.cc:6848
# 2017-11-04T19:16:43 [7580] #15 0x0000558b6cad3605 in fil_tablespace_iterate (table=0x7fb241cc55f8, n_io_buffers=64, callback=...) at /data/src/bb-10.1-marko/storage/xtradb/fil/fil0fil.cc:7112
# 2017-11-04T19:16:43 [7580] #16 0x0000558b6c9485f3 in row_import_for_mysql (table=0x7fb241cc55f8, prebuilt=0x7fb241c2b078) at /data/src/bb-10.1-marko/storage/xtradb/row/row0import.cc:3609
# 2017-11-04T19:16:43 [7580] #17 0x0000558b6c866f27 in ha_innobase::discard_or_import_tablespace (this=0x7fb241c91088, discard=0 '\000') at /data/src/bb-10.1-marko/storage/xtradb/handler/ha_innodb.cc:12988
# 2017-11-04T19:16:43 [7580] #18 0x0000558b6c52aec2 in handler::ha_discard_or_import_tablespace (this=0x7fb241c91088, discard=0 '\000') at /data/src/bb-10.1-marko/sql/handler.cc:4189
# 2017-11-04T19:16:43 [7580] #19 0x0000558b6c3a5401 in mysql_discard_or_import_tablespace (thd=0x7fb24d78d070, table_list=0x7fb241c221b0, discard=false) at /data/src/bb-10.1-marko/sql/sql_table.cc:5666
# 2017-11-04T19:16:43 [7580] #20 0x0000558b6c41e64d in Sql_cmd_discard_import_tablespace::execute (this=0x7fb241c227a8, thd=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_alter.cc:365
# 2017-11-04T19:16:43 [7580] #21 0x0000558b6c2ef17c in mysql_execute_command (thd=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:5680
# 2017-11-04T19:16:43 [7580] #22 0x0000558b6c2f342d in mysql_parse (thd=0x7fb24d78d070, rawbuf=0x7fb241c22088 ""ALTER TABLE t7662 IMPORT TABLESPACE /* QNO 227 CON_ID 9 */"", length=58, parser_state=0x7fb27104d630) at /data/src/bb-10.1-marko/sql/sql_parse.cc:7326
# 2017-11-04T19:16:43 [7580] #23 0x0000558b6c2e2042 in dispatch_command (command=COM_QUERY, thd=0x7fb24d78d070, packet=0x7fb24d793071 "" ALTER TABLE t7662 IMPORT TABLESPACE /* QNO 227 CON_ID 9 */ "", packet_length=60) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1477
# 2017-11-04T19:16:43 [7580] #24 0x0000558b6c2e0dc7 in do_command (thd=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_parse.cc:1106
# 2017-11-04T19:16:43 [7580] #25 0x0000558b6c419a63 in do_handle_one_connection (thd_arg=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1349
# 2017-11-04T19:16:43 [7580] #26 0x0000558b6c4197c7 in handle_one_connection (arg=0x7fb24d78d070) at /data/src/bb-10.1-marko/sql/sql_connect.cc:1261
# 2017-11-04T19:16:43 [7580] #27 0x00007fb272fd1494 in start_thread (arg=0x7fb27104eb00) at pthread_create.c:333
# 2017-11-04T19:16:43 [7580] #28 0x00007fb27138a93f in clone () from /lib/x86_64-linux-gnu/libc.so.6
{noformat}

{noformat}
2017-11-04 21:01:35 7f1f92847b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:35 139773578869504 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t2.ibd
2017-11-04 21:01:35 139773578869504 [Note] InnoDB: Discarding tablespace of table ""test"".""t2"": Tablespace not found
2017-11-04 21:01:38 7f1f92891b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:38 139773579172608 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t1.ibd
2017-11-04 21:01:38 139773579172608 [Note] InnoDB: Discarding tablespace of table ""test"".""t1"": Tablespace not found
2017-11-04 21:01:38 7f1f8f561b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:38 139773525498624 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t6.ibd
2017-11-04 21:01:38 139773525498624 [Note] InnoDB: Discarding tablespace of table ""test"".""t6"": Tablespace not found
2017-11-04 21:01:38 7f1f8f5f5b00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:39 139773526104832 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t4.ibd
2017-11-04 21:01:39 139773526104832 [Note] InnoDB: Discarding tablespace of table ""test"".""t4"": Tablespace not found
2017-11-04 21:01:40 7f1f8f5abb00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:40 139773525801728 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t5.ibd
2017-11-04 21:01:40 139773525801728 [Note] InnoDB: Discarding tablespace of table ""test"".""t5"": Tablespace not found
2017-11-04 21:01:40 7f1f927fdb00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:40 139773578566400 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t3.ibd
2017-11-04 21:01:40 139773578566400 [Note] InnoDB: Discarding tablespace of table ""test"".""t3"": Tablespace not found
2017-11-04 21:01:49 7f1f8f4cdb00  InnoDB: Operating system error number 2 in a file operation.
InnoDB: The error means the system cannot find the path specified.
2017-11-04 21:01:49 139773524892416 [ERROR] InnoDB: Trying to import a tablespace, but could not open the tablespace file ./test/t8.ibd
2017-11-04 21:01:49 139773524892416 [Note] InnoDB: Discarding tablespace of table ""test"".""t8"": Tablespace not found
2017-11-04 21:01:49 7f1f68ffb700  InnoDB: Assertion failure in thread 139772882302720 in file btr0cur.cc line 792
InnoDB: Failing assertion: fil_page_get_type(page) == 17855

# 2017-11-04T21:01:52 [14021] #5  0x00007f1f908393fa in abort () from /lib/x86_64-linux-gnu/libc.so.6
# 2017-11-04T21:01:52 [14021] #6  0x0000556818b6e1c2 in btr_cur_search_to_nth_level (index=0x7f1f63859578, level=0, tuple=0x7f1f67852178, mode=4, latch_mode=2, cursor=0x7f1f68ffa650, has_search_latch=0, file=0x556818efc648 ""/d
ata/src/bb-10.1-marko/storage/xtradb/row/row0row.cc"", line=823, mtr=0x7f1f68ffa730) at /data/src/bb-10.1-marko/storage/xtradb/btr/btr0cur.cc:792
# 2017-11-04T21:01:52 [14021] #7  0x0000556818ad4d2c in btr_pcur_open_low (index=0x7f1f63859578, level=0, tuple=0x7f1f67852178, mode=4, latch_mode=8194, cursor=0x7f1f68ffa650, file=0x556818efc648 ""/data/src/bb-10.1-marko/stora
ge/xtradb/row/row0row.cc"", line=823, mtr=0x7f1f68ffa730) at /data/src/bb-10.1-marko/storage/xtradb/include/btr0pcur.ic:440
# 2017-11-04T21:01:52 [14021] #8  0x0000556818ad6bac in row_search_index_entry (index=0x7f1f63859578, entry=0x7f1f67852178, mode=8194, pcur=0x7f1f68ffa650, mtr=0x7f1f68ffa730) at /data/src/bb-10.1-marko/storage/xtradb/row/row0
row.cc:823
# 2017-11-04T21:01:52 [14021] #9  0x0000556818ad0758 in row_purge_remove_sec_if_poss_leaf (node=0x7f1f9030ea78, index=0x7f1f63859578, entry=0x7f1f67852178) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:452
# 2017-11-04T21:01:52 [14021] #10 0x0000556818ad09c7 in row_purge_remove_sec_if_poss (node=0x7f1f9030ea78, index=0x7f1f63859578, entry=0x7f1f67852178) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:533
# 2017-11-04T21:01:52 [14021] #11 0x0000556818ad0b29 in row_purge_del_mark (node=0x7f1f9030ea78) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:581
# 2017-11-04T21:01:52 [14021] #12 0x0000556818ad147a in row_purge_record_func (node=0x7f1f9030ea78, undo_rec=0x7f1f6781c088 ""\025\372\016"", thr=0x7f1f6ceaffc0, updated_extern=false) at /data/src/bb-10.1-marko/storage/xtradb/ro
w/row0purge.cc:860
# 2017-11-04T21:01:52 [14021] #13 0x0000556818ad16a6 in row_purge (node=0x7f1f9030ea78, undo_rec=0x7f1f6781c088 ""\025\372\016"", thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:916
# 2017-11-04T21:01:52 [14021] #14 0x0000556818ad194e in row_purge_step (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/row/row0purge.cc:996
# 2017-11-04T21:01:52 [14021] #15 0x0000556818a6f21b in que_thr_step (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/que/que0que.cc:1089
# 2017-11-04T21:01:52 [14021] #16 0x0000556818a6f41f in que_run_threads_low (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/que/que0que.cc:1151
# 2017-11-04T21:01:52 [14021] #17 0x0000556818a6f5ab in que_run_threads (thr=0x7f1f6ceaffc0) at /data/src/bb-10.1-marko/storage/xtradb/que/que0que.cc:1192
# 2017-11-04T21:01:52 [14021] #18 0x0000556818b207d2 in trx_purge (n_purge_threads=1, batch_size=300, truncate=false) at /data/src/bb-10.1-marko/storage/xtradb/trx/trx0purge.cc:1254
# 2017-11-04T21:01:52 [14021] #19 0x0000556818b045ba in srv_do_purge (n_threads=1, n_total_purged=0x7f1f68ffaea0) at /data/src/bb-10.1-marko/storage/xtradb/srv/srv0srv.cc:3442
# 2017-11-04T21:01:52 [14021] #20 0x0000556818b04b5d in srv_purge_coordinator_thread (arg=0x0) at /data/src/bb-10.1-marko/storage/xtradb/srv/srv0srv.cc:3592
# 2017-11-04T21:01:52 [14021] #21 0x00007f1f92534494 in start_thread (arg=0x7f1f68ffb700) at pthread_create.c:333
{noformat}

"
2092,MDEV-14323,MDEV,Andrii Nikitin,102881,2017-11-08 09:43:20,"Attached files:
galera_2x3nodes.cnf to suite/galera_3nodes, 
my.* to suite/galera_3nodes/t

Then command `./mtr --suite=galera_3nodes my` works 
just sometimes leaves warnings 'Resource temporarily unavailable'.
This may need troubleshooting, but test may suppress that message as well as quick workaround if everything else is working properly


",1,"Attached files:
galera_2x3nodes.cnf to suite/galera_3nodes, 
my.* to suite/galera_3nodes/t

Then command `./mtr --suite=galera_3nodes my` works 
just sometimes leaves warnings 'Resource temporarily unavailable'.
This may need troubleshooting, but test may suppress that message as well as quick workaround if everything else is working properly


"
2093,MDEV-14533,MDEV,Johan Wikman,106801,2018-02-05 13:20:11,"Attached is a proof-of-concept implementation that has been built and tested with MariaDB 10.0.33.
 [^information_schema_disks.zip] ",1,"Attached is a proof-of-concept implementation that has been built and tested with MariaDB 10.0.33.
 [^information_schema_disks.zip] "
2094,MDEV-14533,MDEV,Alexey Botchkov,108658,2018-03-21 09:05:08,http://lists.askmonty.org/pipermail/commits/2018-March/012116.html,2,URL
2095,MDEV-14592,MDEV,Vicențiu Ciorbaru,108931,2018-03-26 18:39:46,Ok to push once extra tests are added and the coding style issue is resolved.,1,Ok to push once extra tests are added and the coding style issue is resolved.
2096,MDEV-14592,MDEV,Varun Gupta,109108,2018-03-30 07:11:03,Pushed to 10.3,2,Pushed to 10.3
2097,MDEV-14593,MDEV,Alexey Botchkov,104220,2017-12-07 20:17:42,"Patch proposal:
http://lists.askmonty.org/pipermail/commits/2017-December/011705.html",1,"Patch proposal:
URL"
2098,MDEV-14593,MDEV,Alexey Botchkov,104359,2017-12-12 11:10:37,"new patch
http://lists.askmonty.org/pipermail/commits/2017-December/011729.html",2,"new patch
URL"
2099,MDEV-14593,MDEV,Alexey Botchkov,104600,2017-12-17 22:23:26,http://lists.askmonty.org/pipermail/commits/2017-December/011735.html,3,URL
2100,MDEV-14638,MDEV,Sergey Vojtovich,104746,2017-12-20 11:10:56,"[~marko], please review top 3 patches here: https://github.com/MariaDB/server/commits/bb-10.3-MDEV14638",1,"[~marko], please review top 3 patches here: URL"
2101,MDEV-14638,MDEV,Sergey Vojtovich,104751,2017-12-20 12:23:16,"4 patches now: optimised one of find() calls.

Remaining find() calls worth optimising:
{noformat}
storage/innobase/lock/lock0lock.cc:             trx = trx_sys->rw_trx_hash.find(trx_id, true);
storage/innobase/row/row0vers.cc:       trx_t*  trx = trx_sys->rw_trx_hash.find(trx_id, true);
storage/innobase/row/row0vers.cc:               version_trx = trx_sys->rw_trx_hash.find(version_trx_id);
storage/innobase/btr/btr0cur.cc:            && !trx_sys->rw_trx_hash.find(row_get_rec_trx_id(rec, index,
{noformat}",2,"4 patches now: optimised one of find() calls.

Remaining find() calls worth optimising:
{noformat}
storage/innobase/lock/lock0lock.cc:             trx = trx_sys->rw_trx_hash.find(trx_id, true);
storage/innobase/row/row0vers.cc:       trx_t*  trx = trx_sys->rw_trx_hash.find(trx_id, true);
storage/innobase/row/row0vers.cc:               version_trx = trx_sys->rw_trx_hash.find(version_trx_id);
storage/innobase/btr/btr0cur.cc:            && !trx_sys->rw_trx_hash.find(row_get_rec_trx_id(rec, index,
{noformat}"
2102,MDEV-14638,MDEV,Sergey Vojtovich,104784,2017-12-20 19:26:30,"[~danblack], you might be interested in this cool thing as well.",3,"[~danblack], you might be interested in this cool thing as well."
2103,MDEV-14638,MDEV,Daniel Black,104787,2017-12-20 22:22:40,"Ooh, shiny, thanks [~svoj], I'll take a look.",4,"Ooh, shiny, thanks [~svoj], I'll take a look."
2104,MDEV-14638,MDEV,Sergey Vojtovich,104886,2017-12-23 08:05:00,"I saved tree with Marko's commits to https://github.com/MariaDB/server/commits/bb-10.3-MDEV14638-1
Also I integrated some of suggestions and did force push to https://github.com/MariaDB/server/commits/bb-10.3-MDEV14638",5,"I saved tree with Marko's commits to URL
Also I integrated some of suggestions and did force push to URL"
2105,MDEV-14638,MDEV,Marko Mäkelä,105429,2018-01-09 18:14:49,"Very good. Please address my review comments [1|https://github.com/mariadb/server/commit/60eb057b232055cf47137aaa3207a7a0f0d5660b], [2|https://github.com/mariadb/server/commit/9f72e91509457712ffb72a882d7b80d0bef29856] and submit revised commits for final approval.",6,Very good. Please address my review comments [1|URL [2|URL and submit revised commits for final approval.
2106,MDEV-14638,MDEV,Marko Mäkelä,105481,2018-01-10 15:42:59,"OK to push with [the fix-up|https://github.com/MariaDB/server/commit/0fac757af8a9f9e5134a6c11286f2564af139acf], after addressing my remaining minor comments.",7,OK to push with [the fix-up|URL after addressing my remaining minor comments.
2107,MDEV-14705,MDEV,Sergey Vojtovich,108319,2018-03-13 13:32:56,Overdue PR.,1,Overdue PR.
2108,MDEV-14705,MDEV,Marko Mäkelä,108383,2018-03-14 10:04:51,"Startup and shutdown are quite different. I would consider them separately.

h1. Startup
For startup, the biggest time consumers should be redo log apply and the buffer pool load. For the redo log apply, we cannot know in advance how much work there is to be done. MDEV-11027 introduced progress reporting for this, including a call to {{sd_notifyf()}}.

Undo recovery (rolling back incomplete transactions) takes place in a background thread. Starting with MDEV-12323, we do report progress for that as well via {{sd_notifyf()}}. If the binlog is used, then the {{tc_heuristic_recover}} option can come into play, internally initiating {{XA COMMIT}} or {{XA ROLLBACK}} operations in some thread. I do not know if that would block user connections from starting at all. Commit is fast in InnoDB, while the rollback speed is proportional to the number of modified rows and the number of indexes.

h1. Shutdown
In shutdown, there have been multiple hangs. Most recently, I fixed MDEV-15554.

There are different {{innodb_fast_shutdown}} modes. By default ({{innodb_fast_shutdown=1}}, a fast shutdown is done: all dirty pages are flushed to the buffer pool and a log checkpoint is created.

Some progress reporting for the flush phase would be nice. We know exactly how many dirty pages there are, so the progress measure would be linear.

There is also the crash-like {{innodb_fast_shutdown=2}} which is supposed to complete instantly.

The slowest shutdown is {{innodb_fast_shutdown=0}}. It will:
# Wait for all transactions to complete (starting with MDEV-12352, normal shutdown would abort the rollback of recovered transactions)
# Purge the history of all completed transactions (well, except when it fails to do so due to MDEV-11802)
# Empty the change buffer (formerly known as the insert buffer) by merging changes to secondary index leaf pages.

Only after these tasks, it becomes feasible to flush the dirty pages into the data files and to create the final redo log checkpoint.",2,"Startup and shutdown are quite different. I would consider them separately.

h1. Startup
For startup, the biggest time consumers should be redo log apply and the buffer pool load. For the redo log apply, we cannot know in advance how much work there is to be done. MDEV-11027 introduced progress reporting for this, including a call to {{sd_notifyf()}}.

Undo recovery (rolling back incomplete transactions) takes place in a background thread. Starting with MDEV-12323, we do report progress for that as well via {{sd_notifyf()}}. If the binlog is used, then the {{tc_heuristic_recover}} option can come into play, internally initiating {{XA COMMIT}} or {{XA ROLLBACK}} operations in some thread. I do not know if that would block user connections from starting at all. Commit is fast in InnoDB, while the rollback speed is proportional to the number of modified rows and the number of indexes.

h1. Shutdown
In shutdown, there have been multiple hangs. Most recently, I fixed MDEV-15554.

There are different {{innodb_fast_shutdown}} modes. By default ({{innodb_fast_shutdown=1}}, a fast shutdown is done: all dirty pages are flushed to the buffer pool and a log checkpoint is created.

Some progress reporting for the flush phase would be nice. We know exactly how many dirty pages there are, so the progress measure would be linear.

There is also the crash-like {{innodb_fast_shutdown=2}} which is supposed to complete instantly.

The slowest shutdown is {{innodb_fast_shutdown=0}}. It will:
# Wait for all transactions to complete (starting with MDEV-12352, normal shutdown would abort the rollback of recovered transactions)
# Purge the history of all completed transactions (well, except when it fails to do so due to MDEV-11802)
# Empty the change buffer (formerly known as the insert buffer) by merging changes to secondary index leaf pages.

Only after these tasks, it becomes feasible to flush the dirty pages into the data files and to create the final redo log checkpoint."
2109,MDEV-14705,MDEV,Daniel Black,108388,2018-03-14 10:52:03,"quick note - for the purpose of systemd timing startup start from process creation and ends just before accepting the first connection.
* startup time ends: https://github.com/MariaDB/server/blob/10.3/sql/mysqld.cc#L6680
* The end shutdown time starts: https://github.com/MariaDB/server/blob/10.3/sql/mysqld.cc#L6882 until the final exit.

On startup I haven't included the buffer pool load because as I understand it it continues in the background while the server is processing queries.

I have included the buffer pool save on shutdown too.

Exact time need not be known. Only that a loop iteration can progress within a estimated maximium time.",3,"quick note - for the purpose of systemd timing startup start from process creation and ends just before accepting the first connection.
* startup time ends: URL
* The end shutdown time starts: URL until the final exit.

On startup I haven't included the buffer pool load because as I understand it it continues in the background while the server is processing queries.

I have included the buffer pool save on shutdown too.

Exact time need not be known. Only that a loop iteration can progress within a estimated maximium time."
2110,MDEV-14705,MDEV,Rick Pizzi,108600,2018-03-20 12:45:32,@marko the startup has to consider Galera SST time  - can be hours on larger datasets,4,@marko the startup has to consider Galera SST time  - can be hours on larger datasets
2111,MDEV-14705,MDEV,Daniel Black,108643,2018-03-20 23:04:39,"[~rpizzi], SST scripts will be able to communicate just as easily as the server.  The tricky bit is identifying the lower bits of the SST and enacting on it. Like for rsync I was thinking of enabling a progress monitoring and getting a subfunction that takes any output on stdout and issues EXTEND_TIMEOUT=... at that point.

something like:
{code:shell}
if systemd-notify --booted && [ -n ""${NOTIFY_SOCKET}"" ]
then
  rsync_output_extendtimeout()
  {
    # some rate limiting / variable timeout progress
    while read input
   do
       systemd-notify EXTEND_TIMEMOUT=100000 
   done
  }
else
  rsync_output_extendtimeout() { [ 1 ]; }  > /dev/null
fi
...
     rsync --progress   |  rsync_output_extendtimeout
...
{code}
Edit: {{$NOTIFY_SOCKET}} is a unix socket or an abstract socket. Probably best just to use systemd-notify and check if {{$NOTIFY_SOCKET}} is set

Assistance in identifying other slow SST components and how to identify progress would be much appreciated.",5,"[~rpizzi], SST scripts will be able to communicate just as easily as the server.  The tricky bit is identifying the lower bits of the SST and enacting on it. Like for rsync I was thinking of enabling a progress monitoring and getting a subfunction that takes any output on stdout and issues EXTEND_TIMEOUT=... at that point.

something like:
{code:shell}
if systemd-notify --booted && [ -n ""${NOTIFY_SOCKET}"" ]
then
  rsync_output_extendtimeout()
  {
    # some rate limiting / variable timeout progress
    while read input
   do
       systemd-notify EXTEND_TIMEMOUT=100000 
   done
  }
else
  rsync_output_extendtimeout() { [ 1 ]; }  > /dev/null
fi
...
     rsync --progress   |  rsync_output_extendtimeout
...
{code}
Edit: {{$NOTIFY_SOCKET}} is a unix socket or an abstract socket. Probably best just to use systemd-notify and check if {{$NOTIFY_SOCKET}} is set

Assistance in identifying other slow SST components and how to identify progress would be much appreciated."
2112,MDEV-14705,MDEV,Rick Pizzi,108655,2018-03-21 08:26:55,"I believe the script wsrep_sst_xtrabackup-v2 (or mariabackup equivalent) is just spawned via fork/exec, in this case in the parent process, run the loop you have mentioned until the child process completes.",6,"I believe the script wsrep_sst_xtrabackup-v2 (or mariabackup equivalent) is just spawned via fork/exec, in this case in the parent process, run the loop you have mentioned until the child process completes."
2113,MDEV-14705,MDEV,Rick Pizzi,108657,2018-03-21 08:57:04,"Looks like this SST issue isn't new, just overlooked.
See item #8 of https://mariadb.com/kb/en/library/upgrading-from-mariadb-galera-cluster-100-to-mariadb-101/",7,"Looks like this SST issue isn't new, just overlooked.
See item #8 of URL"
2114,MDEV-14705,MDEV,Marko Mäkelä,109374,2018-04-06 07:27:36,"Some cleanup, and testing",8,"Some cleanup, and testing"
2115,MDEV-14705,MDEV,Marko Mäkelä,109376,2018-04-06 07:38:10,"[~rpizzi], I added the timeout extension to InnoDB and XtraDB. Please file a separate ticket for the Galera snapshot transfer, maybe linked to MDEV-11035 or some other ticket on {{innodb_disallow_writes}}.",9,"[~rpizzi], I added the timeout extension to InnoDB and XtraDB. Please file a separate ticket for the Galera snapshot transfer, maybe linked to MDEV-11035 or some other ticket on {{innodb_disallow_writes}}."
2116,MDEV-14705,MDEV,Marko Mäkelä,109440,2018-04-08 06:42:39,"[~rpizzi], [~fraggeln] in MDEV-11035 mentioned systemd timeout on startup, related to Galera rsync snapshot transfer failure. I wonder if this fix would address those problems, or if some further change is needed. (If yes, please file a separate ticket, as I suggested earlier.)",10,"[~rpizzi], [~fraggeln] in MDEV-11035 mentioned systemd timeout on startup, related to Galera rsync snapshot transfer failure. I wonder if this fix would address those problems, or if some further change is needed. (If yes, please file a separate ticket, as I suggested earlier.)"
2117,MDEV-14705,MDEV,Rick Pizzi,109585,2018-04-11 15:30:51,"""Please file a separate ticket for the Galera snapshot transfer""
already there.... https://jira.mariadb.org/browse/MDEV-15606",11,"""Please file a separate ticket for the Galera snapshot transfer""
already there.... URL"
2118,MDEV-14705,MDEV,Marko Mäkelä,147614,2020-03-25 07:13:11,It looks like the Galera issues were ultimately fixed in MDEV-17571.,12,It looks like the Galera issues were ultimately fixed in MDEV-17571.
2119,MDEV-14746,MDEV,Elena Stepanova,104878,2017-12-22 20:52:44,"It's probably more of a task than a bug, [~wlad], if you think so, please change it.",1,"It's probably more of a task than a bug, [~wlad], if you think so, please change it."
2120,MDEV-14756,MDEV,Marko Mäkelä,105910,2018-01-19 16:04:46,Great work! OK to push.,1,Great work! OK to push.
2121,MDEV-14756,MDEV,Sergey Vojtovich,105942,2018-01-20 12:15:59,"Pushed to 10.3:
{noformat}
commit ec32c050726bad8c0504c6b6b74a0fa3f8f8acbb
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Fri Jan 19 23:03:18 2018 +0400

    Get rid of trx->read_view pointer juggling

    trx->read_view|= 1 was done in a silly attempt to fix race condition
    where trx->read_view was closed without trx_sys.mutex lock by read-only
    trasnactions.

    This just made the problem less likely to happen. In fact there was race
    condition in const version of trx_get_read_view(): pointer may change to
    garbage any moment after MVCC::is_view_active(trx->read_view) check and
    before this function returns.

    This patch doesn't fix this race condition, but rather makes it's
    consequences less destructive.

commit 95070bf93977bf42b157819edea7573edaf1369e
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Fri Jan 19 21:42:33 2018 +0400

    MVCC simplifications

    Simplified away MVCC::get_oldest_view()
    Simplified away MVCC::get_view()
    Removed unused MVCC::view_release()

commit 90bf55673e63bf7c6633598abe52217e42516447
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Fri Jan 19 19:05:43 2018 +0400

    Misc trx_sys scalability fixes

    trx_erase_lists(): trx->read_view is owned by current thread and thus
    doesn't need trx_sys.mutex protection for reading it's value. Move
    trx->read_view check out of mutex

    trx_start_low(): moved assertion out of mutex.

    Call ReadView::creator_trx_id() directly: allows to inline this one-line
    method.

commit 64048bafe0fe5a7e73244929d6e4cae8eebb9a00
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Fri Jan 19 19:11:16 2018 +0400

    Removed purge_trx_id_age and purge_view_trx_id_age

    These were unused status variables available in debug builds only.
    Also removed trx_sys.rw_max_trx_id: not used anymore.

commit db5bb785f9b989b4ca6b1087b77a06b31b5ddf71
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Wed Jan 17 19:43:08 2018 +0400

    Allocate trx_sys.mvcc at link time

    trx_sys.mvcc was allocated dynamically for no good reason.

commit f8882cce93f9828ec4a5474134d893f8c68d28db
Author: Marko Mäkelä <marko.makela@mariadb.com>
Date:   Fri Dec 22 16:15:41 2017 +0200

    Replace trx_sys_t* trx_sys with trx_sys_t trx_sys

    There is only one transaction system object in InnoDB.
    Allocate the storage for it at link time, not at runtime.

    lock_rec_fetch_page(): Use the correct fetch mode BUF_GET.
    Pages may never be deallocated from a tablespace while
    record locks are pointing to them.

commit 7078203389b04e742de660d78c36034a3a4deb59
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Wed Dec 27 20:07:20 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Use atomic operations when accessing trx_sys_t::max_trx_id. We can't yet
    move trx_sys_t::get_new_trx_id() out of mutex because it must be updated
    atomically along with trx_sys_t::rw_trx_ids.

commit c6d2842d9a6f46c592d7dfe465bb2b647ebb4d19
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Wed Dec 27 16:23:53 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Remove rw_trx_list.

commit a447980ff3ba000968d89e0c0c16239addeaf438
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Wed Dec 27 15:38:23 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Let lock_print_info_all_transactions() iterate rw_trx_hash instead of
    rw_trx_list.

    When printing info of locks for transactions, InnoDB monitor doesn't
    attempt to read relevant page from disk anymore. The code was prone
    to race conditions.

    Note that TrxListIterator didn't work as advertised: it iterated
    rw_trx_list only.

commit 886af392d301dda720a1585a0e4e550c4d9cef69
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Wed Dec 27 14:24:34 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Let trx_rollback_recovered() iterate rw_trx_hash instead of rw_trx_list.

commit 02270b44d07b78336e0f0d6afe9934587281e056
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Sun Dec 24 21:23:10 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Let lock_validate_table_locks(), lock_rec_other_trx_holds_expl(),
    lock_table_locks_lookup(), trx_recover_for_mysql(), trx_get_trx_by_xid(),
    trx_roll_must_shutdown(), fetch_data_into_cache() iterate rw_trx_hash
    instead of rw_trx_list.

commit d8c0caad320827d4f92a769ece707e2f5d373b98
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Sun Dec 24 19:57:11 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Removed trx_sys_validate_trx_list(): with rw_trx_hash elements are not
    required to be ordered by transaction id. Transaction state is now guarded
    by asserts in rw_trx_hash_t.

commit 900b07908bf9dbd2c79c3a66fc471e6be4cf0d13
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Wed Dec 27 01:04:08 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Removed trx_sys_t::n_prepared_recovered_trx: never used.

    Removed trx_sys_t::n_prepared_trx: used only at shutdown, we can perfectly
    get this value from rw_trx_hash.

commit a0b385ea2b00734b3e06e217abaafd6f9e13f91e
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Tue Dec 26 23:53:38 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Determine minimum transaction id by iterating rw_trx_hash, not rw_trx_list.

    It is more expensive than previous implementation since it does linear
    search, especially if there're many concurrent transactions running. But in
    such case mutex is much bigger evil. And since it doesn't require
    trx_sys->mutex protection it scales better.

    For low concurrency performance difference is neglible.

commit 868c77df3ea0dfb5bd9263cf01df731ab147a8b3
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Thu Dec 21 17:20:14 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Replaced UT_LIST_GET_LEN(trx_sys->rw_trx_list) with
    trx_sys->rw_trx_hash.size().
    Moved freeing of trx objects at shutdown to rw_trx_hash destructor.
    Small clean-up in trx_rollback_recovered().

commit d09f14693406ea7612a7010917b39b895d77593f
Author: Sergey Vojtovich <svoj@mariadb.org>
Date:   Thu Dec 21 15:45:40 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Reduce divergence between trx_sys_t::rw_trx_hash and trx_sys_t::rw_trx_list
    by not adding recovered COMMITTED transactions to trx_sys_t::rw_trx_list.

    Such transactions are discarded immediately without creating trx object.

    This also required to split rollback and cleanup phases of recovery. To
    reflect these updates the following renames happened:
    trx_rollback_or_clean_all_recovered() -> trx_rollback_all_recovered()
    trx_rollback_or_clean_is_active -> trx_rollback_is_active
    trx_rollback_or_clean_recovered() -> trx_rollback_recovered()
    trx_cleanup_at_db_startup() -> trx_cleanup_recovered()

    Also removed a hack from lock_trx_release_locks(). Instead let recovery
    rollback thread to skip committed XA transactions.
{noformat}",2,"Pushed to 10.3:
{noformat}
commit ec32c050726bad8c0504c6b6b74a0fa3f8f8acbb
Author: Sergey Vojtovich 
Date:   Fri Jan 19 23:03:18 2018 +0400

    Get rid of trx->read_view pointer juggling

    trx->read_view|= 1 was done in a silly attempt to fix race condition
    where trx->read_view was closed without trx_sys.mutex lock by read-only
    trasnactions.

    This just made the problem less likely to happen. In fact there was race
    condition in const version of trx_get_read_view(): pointer may change to
    garbage any moment after MVCC::is_view_active(trx->read_view) check and
    before this function returns.

    This patch doesn't fix this race condition, but rather makes it's
    consequences less destructive.

commit 95070bf93977bf42b157819edea7573edaf1369e
Author: Sergey Vojtovich 
Date:   Fri Jan 19 21:42:33 2018 +0400

    MVCC simplifications

    Simplified away MVCC::get_oldest_view()
    Simplified away MVCC::get_view()
    Removed unused MVCC::view_release()

commit 90bf55673e63bf7c6633598abe52217e42516447
Author: Sergey Vojtovich 
Date:   Fri Jan 19 19:05:43 2018 +0400

    Misc trx_sys scalability fixes

    trx_erase_lists(): trx->read_view is owned by current thread and thus
    doesn't need trx_sys.mutex protection for reading it's value. Move
    trx->read_view check out of mutex

    trx_start_low(): moved assertion out of mutex.

    Call ReadView::creator_trx_id() directly: allows to inline this one-line
    method.

commit 64048bafe0fe5a7e73244929d6e4cae8eebb9a00
Author: Sergey Vojtovich 
Date:   Fri Jan 19 19:11:16 2018 +0400

    Removed purge_trx_id_age and purge_view_trx_id_age

    These were unused status variables available in debug builds only.
    Also removed trx_sys.rw_max_trx_id: not used anymore.

commit db5bb785f9b989b4ca6b1087b77a06b31b5ddf71
Author: Sergey Vojtovich 
Date:   Wed Jan 17 19:43:08 2018 +0400

    Allocate trx_sys.mvcc at link time

    trx_sys.mvcc was allocated dynamically for no good reason.

commit f8882cce93f9828ec4a5474134d893f8c68d28db
Author: Marko Mäkelä 
Date:   Fri Dec 22 16:15:41 2017 +0200

    Replace trx_sys_t* trx_sys with trx_sys_t trx_sys

    There is only one transaction system object in InnoDB.
    Allocate the storage for it at link time, not at runtime.

    lock_rec_fetch_page(): Use the correct fetch mode BUF_GET.
    Pages may never be deallocated from a tablespace while
    record locks are pointing to them.

commit 7078203389b04e742de660d78c36034a3a4deb59
Author: Sergey Vojtovich 
Date:   Wed Dec 27 20:07:20 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Use atomic operations when accessing trx_sys_t::max_trx_id. We can't yet
    move trx_sys_t::get_new_trx_id() out of mutex because it must be updated
    atomically along with trx_sys_t::rw_trx_ids.

commit c6d2842d9a6f46c592d7dfe465bb2b647ebb4d19
Author: Sergey Vojtovich 
Date:   Wed Dec 27 16:23:53 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Remove rw_trx_list.

commit a447980ff3ba000968d89e0c0c16239addeaf438
Author: Sergey Vojtovich 
Date:   Wed Dec 27 15:38:23 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Let lock_print_info_all_transactions() iterate rw_trx_hash instead of
    rw_trx_list.

    When printing info of locks for transactions, InnoDB monitor doesn't
    attempt to read relevant page from disk anymore. The code was prone
    to race conditions.

    Note that TrxListIterator didn't work as advertised: it iterated
    rw_trx_list only.

commit 886af392d301dda720a1585a0e4e550c4d9cef69
Author: Sergey Vojtovich 
Date:   Wed Dec 27 14:24:34 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Let trx_rollback_recovered() iterate rw_trx_hash instead of rw_trx_list.

commit 02270b44d07b78336e0f0d6afe9934587281e056
Author: Sergey Vojtovich 
Date:   Sun Dec 24 21:23:10 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Let lock_validate_table_locks(), lock_rec_other_trx_holds_expl(),
    lock_table_locks_lookup(), trx_recover_for_mysql(), trx_get_trx_by_xid(),
    trx_roll_must_shutdown(), fetch_data_into_cache() iterate rw_trx_hash
    instead of rw_trx_list.

commit d8c0caad320827d4f92a769ece707e2f5d373b98
Author: Sergey Vojtovich 
Date:   Sun Dec 24 19:57:11 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Removed trx_sys_validate_trx_list(): with rw_trx_hash elements are not
    required to be ordered by transaction id. Transaction state is now guarded
    by asserts in rw_trx_hash_t.

commit 900b07908bf9dbd2c79c3a66fc471e6be4cf0d13
Author: Sergey Vojtovich 
Date:   Wed Dec 27 01:04:08 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Removed trx_sys_t::n_prepared_recovered_trx: never used.

    Removed trx_sys_t::n_prepared_trx: used only at shutdown, we can perfectly
    get this value from rw_trx_hash.

commit a0b385ea2b00734b3e06e217abaafd6f9e13f91e
Author: Sergey Vojtovich 
Date:   Tue Dec 26 23:53:38 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Determine minimum transaction id by iterating rw_trx_hash, not rw_trx_list.

    It is more expensive than previous implementation since it does linear
    search, especially if there're many concurrent transactions running. But in
    such case mutex is much bigger evil. And since it doesn't require
    trx_sys->mutex protection it scales better.

    For low concurrency performance difference is neglible.

commit 868c77df3ea0dfb5bd9263cf01df731ab147a8b3
Author: Sergey Vojtovich 
Date:   Thu Dec 21 17:20:14 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Replaced UT_LIST_GET_LEN(trx_sys->rw_trx_list) with
    trx_sys->rw_trx_hash.size().
    Moved freeing of trx objects at shutdown to rw_trx_hash destructor.
    Small clean-up in trx_rollback_recovered().

commit d09f14693406ea7612a7010917b39b895d77593f
Author: Sergey Vojtovich 
Date:   Thu Dec 21 15:45:40 2017 +0400

    MDEV-14756 - Remove trx_sys_t::rw_trx_list

    Reduce divergence between trx_sys_t::rw_trx_hash and trx_sys_t::rw_trx_list
    by not adding recovered COMMITTED transactions to trx_sys_t::rw_trx_list.

    Such transactions are discarded immediately without creating trx object.

    This also required to split rollback and cleanup phases of recovery. To
    reflect these updates the following renames happened:
    trx_rollback_or_clean_all_recovered() -> trx_rollback_all_recovered()
    trx_rollback_or_clean_is_active -> trx_rollback_is_active
    trx_rollback_or_clean_recovered() -> trx_rollback_recovered()
    trx_cleanup_at_db_startup() -> trx_cleanup_recovered()

    Also removed a hack from lock_trx_release_locks(). Instead let recovery
    rollback thread to skip committed XA transactions.
{noformat}"
2122,MDEV-14912,MDEV,Axel Schwenke,106416,2018-01-29 14:57:13,Blog online at: https://mariadb.com/resources/blog/meltdown-vulnerability-impact-mariadb-server,1,Blog online at: URL
2123,MDEV-14958,MDEV,Marko Mäkelä,106178,2018-01-24 15:04:05,"As part of this work, I would also evaluate the following fixes that are in MySQL 8.0.4 but apparently not in 5.7:
it looks like this fix was skipped from 5.7, and is only in 8.0.4:
[Bug #26588537 : INNODB.INNODB_BUFFER_POOL_RESIZE_DEBUG FAILS WITH SIG 6|https://github.com/mysql/mysql-server/commit/e34c6b0dea2d8aa3b8e107762548ea71ad0dd1a3]
[BUG#26562371 INNODB: ASSERTION FAILURE: …!LOCK_REC_OTHER_TRX_HOLDS_EX|https://github.com/mysql/mysql-server/commit/c917825732af9feb5b463621dd4f251363c5080e]
[BUG #26818787: ASSERTION: DATA0DATA.IC:430:TUPLE|https://github.com/mysql/mysql-server/commit/ee606e62bbddd7ac3579b4a20ef8684fa7cd83fe]",1,"As part of this work, I would also evaluate the following fixes that are in MySQL 8.0.4 but apparently not in 5.7:
it looks like this fix was skipped from 5.7, and is only in 8.0.4:
[Bug #26588537 : INNODB.INNODB_BUFFER_POOL_RESIZE_DEBUG FAILS WITH SIG 6|URL
[BUG#26562371 INNODB: ASSERTION FAILURE: …!LOCK_REC_OTHER_TRX_HOLDS_EX|URL
[BUG #26818787: ASSERTION: DATA0DATA.IC:430:TUPLE|URL"
2124,MDEV-15053,MDEV,Marko Mäkelä,107047,2018-02-08 15:56:26,This was originally contributed by [~kastauyra] as [MySQL Bug #75534: Solve buffer pool mutex contention by splitting it|https://bugs.mysql.com/bug.php?id=75534].,1,This was originally contributed by [~kastauyra] as [MySQL Bug #75534: Solve buffer pool mutex contention by splitting it|URL
2125,MDEV-15053,MDEV,Laurynas Biveinis,107078,2018-02-09 07:50:02,"Make sure to review https://bugs.mysql.com/bug.php?id=85205, https://bugs.mysql.com/bug.php?id=85208. If your starting point is Oracle 8.0.3 commit, try to find follow-up fixes too, I only recall 946996a3767, e34c6b0dea2 from the top of my head.",2,"Make sure to review URL URL If your starting point is Oracle 8.0.3 commit, try to find follow-up fixes too, I only recall 946996a3767, e34c6b0dea2 from the top of my head."
2126,MDEV-15053,MDEV,Marko Mäkelä,107653,2018-02-21 20:07:30,"[~kastauyra], thank you for the helpful pointers!
I noticed that the [Oracle version|https://github.com/mysql/mysql-server/commit/2bcc00d11f21fe43ba3c0e0f81d3d9cec44c44a0] of  [^bug75534-2.patch] omitted the contributed test changes. [~thiru], can you please try to add them?
I just pushed a [test version of this|https://github.com/MariaDB/server/commit/7d136b737d48e2289ab12c6beab6f1d605e3f666]. I think that more work is needed for the initial commit, because I believe that we really need to use the atomic memory access primitives in order to work correctly, especially on ARM and POWER.",3,"[~kastauyra], thank you for the helpful pointers!
I noticed that the [Oracle version|URL of  [^bug75534-2.patch] omitted the contributed test changes. [~thiru], can you please try to add them?
I just pushed a [test version of this|URL I think that more work is needed for the initial commit, because I believe that we really need to use the atomic memory access primitives in order to work correctly, especially on ARM and POWER."
2127,MDEV-15053,MDEV,Marko Mäkelä,107654,2018-02-21 20:18:06,"[~thiru], a search of the MySQL 8.0 commit history reveals quite a few follow-up commits. Please cherry-pick those as well.
[3b1718b8150ea92166111798c5dc6a11a0e4bfeb|https://github.com/mysql/mysql-server/commit/3b1718b8150ea92166111798c5dc6a11a0e4bfeb] (with test case) 
[d887a49e84a618e150dfa90d3b977614dc0cb020|https://github.com/mysql/mysql-server/commit/d887a49e84a618e150dfa90d3b977614dc0cb020] (looks reasonable)
[bd914aebb3ec510784f364e5e71cd35bb8a0cad5|https://github.com/mysql/mysql-server/commit/bd914aebb3ec510784f364e5e71cd35bb8a0cad5] (with test case)
[65649a16ce9412f7d9286e7e6ad6c2de5769be25|https://github.com/mysql/mysql-server/commit/65649a16ce9412f7d9286e7e6ad6c2de5769be25] (looks questionable; let us skip it, and review the atomics instead)",4,"[~thiru], a search of the MySQL 8.0 commit history reveals quite a few follow-up commits. Please cherry-pick those as well.
[3b1718b8150ea92166111798c5dc6a11a0e4bfeb|URL (with test case) 
[d887a49e84a618e150dfa90d3b977614dc0cb020|URL (looks reasonable)
[bd914aebb3ec510784f364e5e71cd35bb8a0cad5|URL (with test case)
[65649a16ce9412f7d9286e7e6ad6c2de5769be25|URL (looks questionable; let us skip it, and review the atomics instead)"
2128,MDEV-15053,MDEV,Marko Mäkelä,107676,2018-02-22 09:17:00,"Let us use the new branch [bb-10.3-MDEV-15053|https://github.com/MariaDB/server/commit/24deb3737f60c8aea1781c4dd244322f0066b197] for this.
I cherry-picked the following:
[3b1718b8150ea92166111798c5dc6a11a0e4bfeb|https://github.com/mysql/mysql-server/commit/3b1718b8150ea92166111798c5dc6a11a0e4bfeb] (without test case or debug instrumentation)
[d887a49e84a618e150dfa90d3b977614dc0cb020|https://github.com/mysql/mysql-server/commit/d887a49e84a618e150dfa90d3b977614dc0cb020] (maybe we should test this while concurrently resizing the buffer pool)
I filed MDEV-15384 for a regression that MariaDB 10.2.2 got earlier, in an incorrect merge of InnoDB code from MySQL 5.7.9.

What remains to be done:
# Review and revise the use of atomics (use {{my_atomic_loadlint()}} or similar for reads)
# Check if the test cases of  [^bug75534-2.patch] can be imported.
# (for me): Squash your follow-up fixes the first commit, and review it.",5,"Let us use the new branch [bb-10.3-MDEV-15053|URL for this.
I cherry-picked the following:
[3b1718b8150ea92166111798c5dc6a11a0e4bfeb|URL (without test case or debug instrumentation)
[d887a49e84a618e150dfa90d3b977614dc0cb020|URL (maybe we should test this while concurrently resizing the buffer pool)
I filed MDEV-15384 for a regression that MariaDB 10.2.2 got earlier, in an incorrect merge of InnoDB code from MySQL 5.7.9.

What remains to be done:
# Review and revise the use of atomics (use {{my_atomic_loadlint()}} or similar for reads)
# Check if the test cases of  [^bug75534-2.patch] can be imported.
# (for me): Squash your follow-up fixes the first commit, and review it."
2129,MDEV-15053,MDEV,Marko Mäkelä,122141,2019-01-21 14:56:05,"In MySQL 8.0.14 there is a related bug fix:
[Bug#28556539 LOCK_ORDER: CYCLE INVOLVING BUF_POOL_FREE_LIST_MUTEX AND BUF_POOL_ZIP_FREE_MUTEX|https://github.com/mysql/mysql-server/commit/5ce335439e2fe2bda98c2292116f41959cfda8da]
The {{buf_pool->free_list_mutex}} was [introduced in MySQL 8.0.0|https://github.com/mysql/mysql-server/commit/2bcc00d11f21fe43ba3c0e0f81d3d9cec44c44a0].",6,"In MySQL 8.0.14 there is a related bug fix:
[Bug#28556539 LOCK_ORDER: CYCLE INVOLVING BUF_POOL_FREE_LIST_MUTEX AND BUF_POOL_ZIP_FREE_MUTEX|URL
The {{buf_pool->free_list_mutex}} was [introduced in MySQL 8.0.0|URL"
2130,MDEV-15053,MDEV,Marko Mäkelä,142952,2020-01-27 13:25:47,"The work is not only splitting {{buf_pool_t::mutex}} but also changing some data fields to rely on atomic memory operations instead of mutexes for controlling concurrent access.

The work so far was done on 10.3, where {{std::atomic}} is not available to InnoDB. Starting with 10.4, it is available and should be used. I think that we should try to use {{std::memory_order_relaxed}} where available.",7,"The work is not only splitting {{buf_pool_t::mutex}} but also changing some data fields to rely on atomic memory operations instead of mutexes for controlling concurrent access.

The work so far was done on 10.3, where {{std::atomic}} is not available to InnoDB. Starting with 10.4, it is available and should be used. I think that we should try to use {{std::memory_order_relaxed}} where available."
2131,MDEV-15053,MDEV,Marko Mäkelä,146894,2020-03-17 12:57:55,"This will require some more work and a careful review. At the moment, {{cmake -DWITH_ASAN}} is reporting heap-use-after-free in numerous tests.",8,"This will require some more work and a careful review. At the moment, {{cmake -DWITH_ASAN}} is reporting heap-use-after-free in numerous tests."
2132,MDEV-15053,MDEV,Matthias Leich,147487,2020-03-23 19:06:55,"Result of RQG testing on origin/bb-10.5-marko commit ab5fe285276ab2dc4d87fb59a31655db51706da5
The binaries build from this tree were neither better nor worse than actual 10.5.",9,"Result of RQG testing on origin/bb-10.5-marko commit ab5fe285276ab2dc4d87fb59a31655db51706da5
The binaries build from this tree were neither better nor worse than actual 10.5."
2133,MDEV-15053,MDEV,Matthias Leich,154400,2020-05-27 15:28:39,"Result of RQG testing on origin/bb-10.5-MDEV-15053-3 577ac7de0b26e595bc7bf678c25a65a0b4c92edc 2020-05-27T16:05:27+03:00
The binaries build from this tree were neither better nor worse than actual 10.5.",10,"Result of RQG testing on origin/bb-10.5-MDEV-15053-3 577ac7de0b26e595bc7bf678c25a65a0b4c92edc 2020-05-27T16:05:27+03:00
The binaries build from this tree were neither better nor worse than actual 10.5."
2134,MDEV-15053,MDEV,Marko Mäkelä,154733,2020-05-29 17:22:16,"[~kevg], after your review I still fixed one thing that probably caused the occasional hangs that [~axel] reproduced in {{buf_page_init_for_read()}}. I had wrongly released the {{buf_pool.page_hash}} latch before {{buf_page_t::set_io_fix(BUF_IO_READ)}} was called. This was a functional change from earlier.",11,"[~kevg], after your review I still fixed one thing that probably caused the occasional hangs that [~axel] reproduced in {{buf_page_init_for_read()}}. I had wrongly released the {{buf_pool.page_hash}} latch before {{buf_page_t::set_io_fix(BUF_IO_READ)}} was called. This was a functional change from earlier."
2135,MDEV-15053,MDEV,Marko Mäkelä,155464,2020-06-04 15:28:12,"The hang was very elusive, but finally I found the reason. Now that we removed {{buf_block_t::mutex}}, we must release {{buf_block_t::lock}} *before* invoking {{buf_block_t::unfix()}} or {{buf_block_t::io_unfix()}}. Otherwise, something bad will happen somewhere, causing a block in the {{buf_pool.free}} list to permanently remain in X-latched state, with {{block->lock.writer_thread==0}}. We only experienced the hang in a release build, and it never reproduced under {{rr record}}. In the end, I disabled {{buf_page_optimistic_get()}} and added debug assertions on {{buf_block_t::lock}} when the {{buf_pool.free}} list is modified. To be able to do that, I had to [fix a glitch in {{dict_check_sys_tables()}}|https://github.com/MariaDB/server/commit/c5883debd6ef440a037011c11873b396923e93c5].",12,"The hang was very elusive, but finally I found the reason. Now that we removed {{buf_block_t::mutex}}, we must release {{buf_block_t::lock}} *before* invoking {{buf_block_t::unfix()}} or {{buf_block_t::io_unfix()}}. Otherwise, something bad will happen somewhere, causing a block in the {{buf_pool.free}} list to permanently remain in X-latched state, with {{block->lock.writer_thread==0}}. We only experienced the hang in a release build, and it never reproduced under {{rr record}}. In the end, I disabled {{buf_page_optimistic_get()}} and added debug assertions on {{buf_block_t::lock}} when the {{buf_pool.free}} list is modified. To be able to do that, I had to [fix a glitch in {{dict_check_sys_tables()}}|URL"
2136,MDEV-15053,MDEV,Marko Mäkelä,155970,2020-06-09 15:08:33,"To reduce contention especially in read-only workloads, we increased {{svr_n_page_hash_locks}} from 16 to 64 and added {{LF_BACKOFF()}} to the spin loop in {{rw_lock_lock_word_decr()}}.",13,"To reduce contention especially in read-only workloads, we increased {{svr_n_page_hash_locks}} from 16 to 64 and added {{LF_BACKOFF()}} to the spin loop in {{rw_lock_lock_word_decr()}}."
2137,MDEV-15104,MDEV,Sergey Vojtovich,106371,2018-01-28 20:42:37,"[~marko], please review patches for this task: https://github.com/MariaDB/server/commits/bb-10.3-arm",1,"[~marko], please review patches for this task: URL"
2138,MDEV-15104,MDEV,Marko Mäkelä,106442,2018-01-29 19:07:47,"This looks OK, except for a possible race or starvation where we fail to update the {{max_trx_id}} in the {{TRX_SYS}} page before writing newer {{trx_id}} values to undo logs.

I think that a proper fix to that is to do the equivalent of {{SELECT MAX(trx_id) FROM undo_logs}} on startup, and only when there are no undo logs (not even to purge), read the value from the {{TRX_SYS}} page. (And only write to the {{TRX_SYS}} page when {{trx_undo_truncate_start()}} discards the last undo log.)",2,"This looks OK, except for a possible race or starvation where we fail to update the {{max_trx_id}} in the {{TRX_SYS}} page before writing newer {{trx_id}} values to undo logs.

I think that a proper fix to that is to do the equivalent of {{SELECT MAX(trx_id) FROM undo_logs}} on startup, and only when there are no undo logs (not even to purge), read the value from the {{TRX_SYS}} page. (And only write to the {{TRX_SYS}} page when {{trx_undo_truncate_start()}} discards the last undo log.)"
2139,MDEV-15104,MDEV,Marko Mäkelä,106596,2018-01-31 15:04:01,"Thank you! Very good work.
I posted some minor comments to the [MVCC snapshot optimization|https://github.com/MariaDB/server/commit/6c85b03c8462d5c4c53593f14f471e736bcd46f0], mainly suggesting additional comments around state transitions.",3,"Thank you! Very good work.
I posted some minor comments to the [MVCC snapshot optimization|URL mainly suggesting additional comments around state transitions."
2140,MDEV-15104,MDEV,Marko Mäkelä,106597,2018-01-31 15:05:07,"For the record, MDEV-15132 removes the race condition by removing the updates of the {{TRX_SYS}} page.",4,"For the record, MDEV-15132 removes the race condition by removing the updates of the {{TRX_SYS}} page."
2141,MDEV-15104,MDEV,zhai weixiang,107013,2018-02-08 06:09:22,"I got an assertion while running benchmark.(50% update-non-index, 50% select-on-pk), not sure if it relates to this change. 

ersion: '10.3.5-MariaDB-rds-dev'  socket: '/u01/maria/run/mysql.sock'  port: 3306  Source distribution
2018-02-08 13:55:24 269 [ERROR] [FATAL] InnoDB: Unknown error code 20: Required history data has been deleted
180208 13:55:24 [ERROR] mysqld got signal 6 ;

the latest commit is 029ab11cc883d486117f30900c787c4b2765b742






",5,"I got an assertion while running benchmark.(50% update-non-index, 50% select-on-pk), not sure if it relates to this change. 

ersion: '10.3.5-MariaDB-rds-dev'  socket: '/u01/maria/run/mysql.sock'  port: 3306  Source distribution
2018-02-08 13:55:24 269 [ERROR] [FATAL] InnoDB: Unknown error code 20: Required history data has been deleted
180208 13:55:24 [ERROR] mysqld got signal 6 ;

the latest commit is 029ab11cc883d486117f30900c787c4b2765b742






"
2142,MDEV-15104,MDEV,Marko Mäkelä,107016,2018-02-08 06:46:17,"[~zhaiwx1987], sorry, we forgot to update this ticket to inform of this problem that [~axel] found out when running benchmarks (actually to evaluate if MDEV-15058 could cause trouble). He got exactly the same type of errors, but only when running the benchmark for long enough. He isolated the problem to the very last scalability commit by [~svoj]:
{quote}
commit [bc7a1dc1fbd27e6064d3b40443fe242397668af7|https://github.com/MariaDB/server/commit/bc7a1dc1fbd27e6064d3b40443fe242397668af7]
Author: Sergey Vojtovich
Date:   Tue Jan 30 20:59:42 2018 +0400

    MDEV-15104 - Optimise MVCC snapshot
    
    With {{trx_sys_t::rw_trx_ids}} removal, MVCC snapshot overhead became
    slightly higher. That is instead of copying an array we now have to
    iterate {{LF_HASH}}. All this done under {{trx_sys.mutex}} protection.
    
    This patch moves MVCC snapshot out of {{trx_sys.mutex}}.
{quote}
Please revert this commit, or test with its parent. [~axel] ran benchmarks with is parent over last weekend, and did not report of any crashes.
[~svoj] has been working on a fix. I just filed MDEV-15246 to track this.",6,"[~zhaiwx1987], sorry, we forgot to update this ticket to inform of this problem that [~axel] found out when running benchmarks (actually to evaluate if MDEV-15058 could cause trouble). He got exactly the same type of errors, but only when running the benchmark for long enough. He isolated the problem to the very last scalability commit by [~svoj]:
{quote}
commit [bc7a1dc1fbd27e6064d3b40443fe242397668af7|URL
Author: Sergey Vojtovich
Date:   Tue Jan 30 20:59:42 2018 +0400

    MDEV-15104 - Optimise MVCC snapshot
    
    With {{trx_sys_t::rw_trx_ids}} removal, MVCC snapshot overhead became
    slightly higher. That is instead of copying an array we now have to
    iterate {{LF_HASH}}. All this done under {{trx_sys.mutex}} protection.
    
    This patch moves MVCC snapshot out of {{trx_sys.mutex}}.
{quote}
Please revert this commit, or test with its parent. [~axel] ran benchmarks with is parent over last weekend, and did not report of any crashes.
[~svoj] has been working on a fix. I just filed MDEV-15246 to track this."
2143,MDEV-15104,MDEV,zhai weixiang,107017,2018-02-08 07:04:26,"Hi, Marko, thank you for your reply,
I just also noticed that the code format of class ReadView is mixed with innodb style and server style, maybe that needs an adjustment ? ",7,"Hi, Marko, thank you for your reply,
I just also noticed that the code format of class ReadView is mixed with innodb style and server style, maybe that needs an adjustment ? "
2144,MDEV-15104,MDEV,Sergey Vojtovich,107018,2018-02-08 08:06:59,"[~zhaiwx1987], it was agreed to use server coding style in new InnoDB code.",8,"[~zhaiwx1987], it was agreed to use server coding style in new InnoDB code."
2145,MDEV-15241,MDEV,Varun Gupta,108683,2018-03-21 17:41:18,"MYSQL_ERRMSG_SIZE is the max length of the error message, part of the protocol specs",1,"MYSQL_ERRMSG_SIZE is the max length of the error message, part of the protocol specs"
2146,MDEV-15241,MDEV,Varun Gupta,108720,2018-03-22 07:17:55,"Patch
http://lists.askmonty.org/pipermail/commits/2018-March/012122.html",2,"Patch
URL"
2147,MDEV-15253,MDEV,Sergei Petrunia,107037,2018-02-08 13:31:37,"join_cache_level:
{noformat}
1 – flat (Block Nested Loop) BNL
2 – incremental BNL  -- *current default*
3 – flat Block Nested Loop Hash (BNLH)
4 – incremental BNLH  -- *new default*
5 – flat Batch Key Access (BKA)
6 – incremental BKA
7 – flat Batch Key Access Hash (BKAH)
8 – incremental BKAH
{noformat}
",1,"join_cache_level:
{noformat}
1 – flat (Block Nested Loop) BNL
2 – incremental BNL  -- *current default*
3 – flat Block Nested Loop Hash (BNLH)
4 – incremental BNLH  -- *new default*
5 – flat Batch Key Access (BKA)
6 – incremental BKA
7 – flat Batch Key Access Hash (BKAH)
8 – incremental BKAH
{noformat}
"
2148,MDEV-15253,MDEV,Sergei Petrunia,107038,2018-02-08 13:35:01,"optimizer_use_condition_selectivity:
{noformat}
1 Use selectivity of predicates as in MariaDB 5.5. -- *current default *
2 Use selectivity of all range predicates supported by indexes.
3 Use selectivity of all range predicates estimated without histogram.
4 Use selectivity of all range predicates estimated with histogram. -- *new default *
5 Additionally use selectivity of certain non-range predicates calculated on record sample.
{noformat}

Note that histograms are not collected automatically, so the value of {{4}} will have the same effect as {{3}} for those who never collected histograms.",2,"optimizer_use_condition_selectivity:
{noformat}
1 Use selectivity of predicates as in MariaDB 5.5. -- *current default *
2 Use selectivity of all range predicates supported by indexes.
3 Use selectivity of all range predicates estimated without histogram.
4 Use selectivity of all range predicates estimated with histogram. -- *new default *
5 Additionally use selectivity of certain non-range predicates calculated on record sample.
{noformat}

Note that histograms are not collected automatically, so the value of {{4}} will have the same effect as {{3}} for those who never collected histograms."
2149,MDEV-15253,MDEV,Sergei Petrunia,107040,2018-02-08 13:56:22,"TODO: As a second step, should {{histogram_size}} default be changed from {{0}} to {{255}} ?",3,"TODO: As a second step, should {{histogram_size}} default be changed from {{0}} to {{255}} ?"
2150,MDEV-15253,MDEV,Daniel Black,107076,2018-02-09 07:40:05,If {{optimizer_use_condition_selectivity}}=4 then it would seem a little odd to keep {{histogram_size}} at 0. 255 seems like a good default to cover a broad range of values in a column with little cost if this isn't the case.,4,If {{optimizer_use_condition_selectivity}}=4 then it would seem a little odd to keep {{histogram_size}} at 0. 255 seems like a good default to cover a broad range of values in a column with little cost if this isn't the case.
2151,MDEV-15253,MDEV,Varun Gupta,107412,2018-02-16 11:10:05,Milestone1: Increase the value for join_cache_level from 2 to 4,5,Milestone1: Increase the value for join_cache_level from 2 to 4
2152,MDEV-15253,MDEV,Daniel Black,107467,2018-02-18 08:36:43,[~varun] de7a3b23ba66e0946adde0906f1972e80b0e700a had the wrong MDEV ref.,6,[~varun] de7a3b23ba66e0946adde0906f1972e80b0e700a had the wrong MDEV ref.
2153,MDEV-15253,MDEV,Varun Gupta,111745,2018-05-30 17:04:45,Well  we are not going to change the value for join_cache_level. It will still remain to be 2. We are not having any improved performance gains here because there is no cost based approach for hash join and we unconditionally always prefer hash join.,7,Well  we are not going to change the value for join_cache_level. It will still remain to be 2. We are not having any improved performance gains here because there is no cost based approach for hash join and we unconditionally always prefer hash join.
2154,MDEV-15253,MDEV,Varun Gupta,116734,2018-09-17 17:30:25,The branch where all the changes is 10.4-selectivity4,8,The branch where all the changes is 10.4-selectivity4
2155,MDEV-15253,MDEV,Varun Gupta,116735,2018-09-17 17:34:11,"One issue we came across while changing the defaults that ANALYZE on a table would become quite slow , because it will also calculate the EITS at the same time.

According to the docs:

{noformat}
With engine-independent statistics:
If @@use_stat_tables='never' and PERSISTENT FOR isn't used as part of the ALTER TABLE statement, then only storage engine statistics will be updated and not engine-independent statistics.
For other values of @@use_stat_tables, both storage engine statistics and engine-independent statistics will be updated.
{noformat}
",9,"One issue we came across while changing the defaults that ANALYZE on a table would become quite slow , because it will also calculate the EITS at the same time.

According to the docs:

{noformat}
With engine-independent statistics:
If @@use_stat_tables='never' and PERSISTENT FOR isn't used as part of the ALTER TABLE statement, then only storage engine statistics will be updated and not engine-independent statistics.
For other values of @@use_stat_tables, both storage engine statistics and engine-independent statistics will be updated.
{noformat}
"
2156,MDEV-15253,MDEV,Varun Gupta,120513,2018-12-09 13:01:21,Pushed to 10.4 ,10,Pushed to 10.4 
2157,MDEV-15409,MDEV,Elena Stepanova,108212,2018-03-12 01:46:23,"I've set up *non-MTR* tests in buildbot, for now on deb packages with 10.1, 10.2, 10.3 *main* branches.

The tests are run amongst the installation/upgrade bunch. They use the resulting VM image from install test (which installs MariaDB server and whatever dependencies it pulls).
The logic is very basic:
- stop the ""default"" server (the one that's run by the service),
- create a minimal wsrep config and 3 custom configs for 3 nodes,
- copy the data directory created by the install test (which is basically empty, but fully bootstrapped and has a user for SST auth) for each of 3 nodes,
- start one node with {{wsrep-new-cluster}},
- create a table, insert a value,
- start two other nodes, one after another, using the configured SST method,
- make sure they joined the cluster and picked up the previously created table (SST worked),
- do something on one of the nodes and make sure other nodes picked up the change (runtime replication works).

The test is run 4 times, once for each of {{mariabackup}}, {{xtrabackup-v2}}, {{mysqldump}}, {{rsync}}.

Tests are shown in buildbot as {{galera-mariabackup}}, {{galera-rsync}} etc.

Error logs from all nodes and syslog are stored.

Notes:
- {{mysqldump}} has been disabled for 10.1 due to MDEV-15541	
- to start nodes, {{mysqld_safe}} is used. The initial idea was to use {{mysqld_multi}}, but it didn't work out, because mariabackup/xtrabackup SST methods don't work when the {{datadir}} is provided on the command line (which is what {{mysqld_multi}} would do). It can, however, work with {{defaults-extra-file}}, which is how it has been set up.
- {{xtrabackup-v2}} on Power is disabled, because xtrabackup itself is only available for amd64/i386.
- {{xtrabackup-v2}} on artful/i386 fails, because xtrabackup package is missing at Percona site; maybe needs to be disabled.

Tests seem to behave more or less as expected at the first glance, but probably some intermittent failures will happen and will need to be fixed.

Examples:
current 10.1 (passed, except for disabled mysqldump): http://buildbot.askmonty.org/buildbot/builders/kvm-deb-stretch-amd64/builds/2932
10.1, revision prior to MDEV-15254 bugfix (xtrabackup-v2 failed): https://internal.askmonty.org/buildbot/builders/kvm-deb-artful-amd64/builds/698
current 10.2 (passed): https://internal.askmonty.org/buildbot/builders/kvm-deb-stretch-amd64/builds/2931
current 10.3 (xtrabackup-v2 fails, says 10.3.6 format is not supported): http://buildbot.askmonty.org/buildbot/builders/kvm-deb-artful-amd64/builds/697 -- to be looked at, possibly it's not supposed to work and needs to be disabled

",1,"I've set up *non-MTR* tests in buildbot, for now on deb packages with 10.1, 10.2, 10.3 *main* branches.

The tests are run amongst the installation/upgrade bunch. They use the resulting VM image from install test (which installs MariaDB server and whatever dependencies it pulls).
The logic is very basic:
- stop the ""default"" server (the one that's run by the service),
- create a minimal wsrep config and 3 custom configs for 3 nodes,
- copy the data directory created by the install test (which is basically empty, but fully bootstrapped and has a user for SST auth) for each of 3 nodes,
- start one node with {{wsrep-new-cluster}},
- create a table, insert a value,
- start two other nodes, one after another, using the configured SST method,
- make sure they joined the cluster and picked up the previously created table (SST worked),
- do something on one of the nodes and make sure other nodes picked up the change (runtime replication works).

The test is run 4 times, once for each of {{mariabackup}}, {{xtrabackup-v2}}, {{mysqldump}}, {{rsync}}.

Tests are shown in buildbot as {{galera-mariabackup}}, {{galera-rsync}} etc.

Error logs from all nodes and syslog are stored.

Notes:
- {{mysqldump}} has been disabled for 10.1 due to MDEV-15541	
- to start nodes, {{mysqld_safe}} is used. The initial idea was to use {{mysqld_multi}}, but it didn't work out, because mariabackup/xtrabackup SST methods don't work when the {{datadir}} is provided on the command line (which is what {{mysqld_multi}} would do). It can, however, work with {{defaults-extra-file}}, which is how it has been set up.
- {{xtrabackup-v2}} on Power is disabled, because xtrabackup itself is only available for amd64/i386.
- {{xtrabackup-v2}} on artful/i386 fails, because xtrabackup package is missing at Percona site; maybe needs to be disabled.

Tests seem to behave more or less as expected at the first glance, but probably some intermittent failures will happen and will need to be fixed.

Examples:
current 10.1 (passed, except for disabled mysqldump): URL
10.1, revision prior to MDEV-15254 bugfix (xtrabackup-v2 failed): URL
current 10.2 (passed): URL
current 10.3 (xtrabackup-v2 fails, says 10.3.6 format is not supported): URL -- to be looked at, possibly it's not supposed to work and needs to be disabled

"
2158,MDEV-15409,MDEV,Daniel Black,108219,2018-03-12 05:26:42,"[~elenst] thanks for setting up all these tests.

xtrabackup-v2 and mariadbbackup sst failures with mysqld_multi:

As xtrabackup and mariadbbackup can take --datadir as an argument and the datadir is passed to the sst scripts there shouldn't be a reason these scripts are relying on a configuration file settings. [~wlad] commented here https://github.com/MariaDB/server/pull/554#issuecomment-359403975 that settings come directly from the server.

In theory it should also work with --defaults-group-suffix= too.

on 10.3 xtrabackup-v2 failure:

Its hitting the error:
https://github.com/percona/percona-xtrabackup/blob/e861671ab35dea0eeb6e7d96a1683c65f7445060/storage/innobase/log/log0recv.cc#L1391

however there's a lot more formats in mariadb 
https://github.com/MariaDB/server/blob/fe0e263e6d9c3330dbc8de4608fc62e8f4700a95/storage/innobase/log/log0recv.cc#L946..L952

I was working on some docker related tests. Can I get a link to the buildbot test source please?",2,"[~elenst] thanks for setting up all these tests.

xtrabackup-v2 and mariadbbackup sst failures with mysqld_multi:

As xtrabackup and mariadbbackup can take --datadir as an argument and the datadir is passed to the sst scripts there shouldn't be a reason these scripts are relying on a configuration file settings. [~wlad] commented here URL that settings come directly from the server.

In theory it should also work with --defaults-group-suffix= too.

on 10.3 xtrabackup-v2 failure:

Its hitting the error:
URL

however there's a lot more formats in mariadb 
URL

I was working on some docker related tests. Can I get a link to the buildbot test source please?"
2159,MDEV-15409,MDEV,Elena Stepanova,108238,2018-03-12 09:48:53,"{quote}
As xtrabackup and mariadbbackup can take --datadir as an argument and the datadir is passed to the sst scripts there shouldn't be a reason these scripts are relying on a configuration file settings
{quote}
The problem is not with xtrabackup and mariabackup, it's with *SST methods* xtrabackup-v2 and mariabackup. At some point (at the last {{move-back}} step in particular) they don't pass datadir over to innobackupex, it attempts to find it in default config files, and things get messed up.

{quote}
I was working on some docker related tests. Can I get a link to the buildbot test source please?
{quote}
There isn't much of a source, the text above basically describes it all, but anyway, everything is in buildbot's maria-master.cfg, {{def getDebGaleraStep}} (it will be in https://github.com/MariaDB/mariadb.org-tools/blob/master/buildbot/maria-master.cfg after it auto-commits next night). Please don't modify it directly even if it looks ugly to you. 
If you don't want to wait till it auto-commits, you can naturally see all steps in the buildbot logs.

Please also note that addition of these tests doesn't rule out the need of *MTR* tests. While of course ultimately it's MariaDB's responsibility to test the final code, somehow the rule that all contributions [must come with their own tests|https://mariadb.com/kb/en/library/contributing-code/] has been largely forgotten, there have been many patches to SST scripts without any tests whatsoever. It would be beneficial for the quality if they were tested more thoroughly before submission and followed guidelines upon submission, even if it would cause some decrease in the amount of patches.",3,"{quote}
As xtrabackup and mariadbbackup can take --datadir as an argument and the datadir is passed to the sst scripts there shouldn't be a reason these scripts are relying on a configuration file settings
{quote}
The problem is not with xtrabackup and mariabackup, it's with *SST methods* xtrabackup-v2 and mariabackup. At some point (at the last {{move-back}} step in particular) they don't pass datadir over to innobackupex, it attempts to find it in default config files, and things get messed up.

{quote}
I was working on some docker related tests. Can I get a link to the buildbot test source please?
{quote}
There isn't much of a source, the text above basically describes it all, but anyway, everything is in buildbot's maria-master.cfg, {{def getDebGaleraStep}} (it will be in URL after it auto-commits next night). Please don't modify it directly even if it looks ugly to you. 
If you don't want to wait till it auto-commits, you can naturally see all steps in the buildbot logs.

Please also note that addition of these tests doesn't rule out the need of *MTR* tests. While of course ultimately it's MariaDB's responsibility to test the final code, somehow the rule that all contributions [must come with their own tests|URL has been largely forgotten, there have been many patches to SST scripts without any tests whatsoever. It would be beneficial for the quality if they were tested more thoroughly before submission and followed guidelines upon submission, even if it would cause some decrease in the amount of patches."
2160,MDEV-15409,MDEV,Daniel Black,108288,2018-03-13 05:08:23,"Thanks for the details [~elenst]. I've no intention of modifying the buildbot config, just gaining ideas for testing. I apologize for my broken galera SST changes. Quick remedies especially after releases aren't good enough. They weren't tested properly, I'm really sorry, I lost patience with the state of SST MTR tests and hoped component based testing was sufficient. It wasn't. I'll try to fix some of the MTR tests for galera and I promise to do better next time. So sorry.",4,"Thanks for the details [~elenst]. I've no intention of modifying the buildbot config, just gaining ideas for testing. I apologize for my broken galera SST changes. Quick remedies especially after releases aren't good enough. They weren't tested properly, I'm really sorry, I lost patience with the state of SST MTR tests and hoped component based testing was sufficient. It wasn't. I'll try to fix some of the MTR tests for galera and I promise to do better next time. So sorry."
2161,MDEV-15409,MDEV,Sergei Golubchik,108465,2018-03-15 20:52:52,"# could you please check why galera_sst_mysqldump fails?
# do we have any tests for wsrep_sst_mariabackup? If not, perhaps they can be created based on galera_sst_xtrabackup* tests?",5,"# could you please check why galera_sst_mysqldump fails?
# do we have any tests for wsrep_sst_mariabackup? If not, perhaps they can be created based on galera_sst_xtrabackup* tests?"
2162,MDEV-15409,MDEV,Sachin Setiya,108556,2018-03-19 10:11:24,"It is failing because It gives 1205 error on each call of ""show status"" in wait_untill_connected_again.inc",6,"It is failing because It gives 1205 error on each call of ""show status"" in wait_untill_connected_again.inc"
2163,MDEV-15409,MDEV,Aurélien LEQUOY,108629,2018-03-20 17:41:33,"For me the problem is on client

have a look : https://jira.mariadb.org/browse/MDEV-15383?focusedCommentId=108624&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-108624


",7,"For me the problem is on client

have a look : URL


"
2164,MDEV-15409,MDEV,Aurélien LEQUOY,108630,2018-03-20 17:45:30,"the problem is not mariadb-Server, not on galera but on stupid client : libmariadbclient18                10.2.13+maria~stretch

https://jira.mariadb.org/browse/MDEV-15383?focusedCommentId=108624&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-108624

*You need to add a full test to test galera cluster WITH SST*, and don't push new version without try SST. I lost so many time with this, i remember on 10.0.x i spend one month to find tricky problem on xtrabackup (this time) and solve 95% of bug open on Percona.",8,"the problem is not mariadb-Server, not on galera but on stupid client : libmariadbclient18                10.2.13+maria~stretch

URL

*You need to add a full test to test galera cluster WITH SST*, and don't push new version without try SST. I lost so many time with this, i remember on 10.0.x i spend one month to find tricky problem on xtrabackup (this time) and solve 95% of bug open on Percona."
2165,MDEV-15473,MDEV,Ralf Gebhardt,109021,2018-03-28 14:45:55,"[~serg] If I understand this correctly, the server is crashing. From my point of view this should be a bug, do you agree?",1,"[~serg] If I understand this correctly, the server is crashing. From my point of view this should be a bug, do you agree?"
2166,MDEV-15473,MDEV,Sergei Golubchik,109052,2018-03-28 23:23:28,"No, this is not a bug. Everything works as designed. By design, a plugin is executed in the server address space, in the server process. So if the plugin crashes, it is expected that it will take the whole server with it.

It is possible, of course, to redesign the plugin architecture and execute plugins in a sandbox. But this will be by no means a bug fix.",2,"No, this is not a bug. Everything works as designed. By design, a plugin is executed in the server address space, in the server process. So if the plugin crashes, it is expected that it will take the whole server with it.

It is possible, of course, to redesign the plugin architecture and execute plugins in a sandbox. But this will be by no means a bug fix."
2167,MDEV-15473,MDEV,Ralf Gebhardt,109490,2018-04-09 13:44:46,"By getting MDEV-7032 done, the Server should not crash in this cases anymore",3,"By getting MDEV-7032 done, the Server should not crash in this cases anymore"
2168,MDEV-15473,MDEV,Alexey Botchkov,111928,2018-06-04 13:50:14,"http://lists.askmonty.org/pipermail/commits/2018-June/012595.html

Short description -
structurally i added the auth_pam_safe.so and auth_pam_tool modules.
The 'so' provides the same interface as the auth_pam.so, just is crash-safe. The auth_pam_tool is the 'sandbox' applicatin that does the PAM calls.
Part of the auth_pam.c was moved to the auth_pam_base.c to be included into auth_pam.c and auth_pam_toll.c.

I didn't add tests here intentionally - would like to agree the overall design first.",4,"URL

Short description -
structurally i added the auth_pam_safe.so and auth_pam_tool modules.
The 'so' provides the same interface as the auth_pam.so, just is crash-safe. The auth_pam_tool is the 'sandbox' applicatin that does the PAM calls.
Part of the auth_pam.c was moved to the auth_pam_base.c to be included into auth_pam.c and auth_pam_toll.c.

I didn't add tests here intentionally - would like to agree the overall design first."
2169,MDEV-15473,MDEV,Alexey Botchkov,113308,2018-07-01 09:00:50,http://lists.askmonty.org/pipermail/commits/2018-July/012669.html,5,URL
2170,MDEV-15473,MDEV,Alexey Botchkov,113432,2018-07-03 12:24:18,"Final patch.
http://lists.askmonty.org/pipermail/commits/2018-July/012672.html",6,"Final patch.
URL"
2171,MDEV-15473,MDEV,Sergei Golubchik,113441,2018-07-03 15:21:57,"still need to check that filesystem permissions on the new directory is set correctly

and minor cleanup in tests.",7,"still need to check that filesystem permissions on the new directory is set correctly

and minor cleanup in tests."
2172,MDEV-15473,MDEV,Alexey Botchkov,113534,2018-07-05 13:41:09,http://lists.askmonty.org/pipermail/commits/2018-July/012680.html,8,URL
2173,MDEV-15473,MDEV,Alexey Botchkov,113655,2018-07-09 09:58:20,http://lists.askmonty.org/pipermail/commits/2018-July/012691.html,9,URL
2174,MDEV-15473,MDEV,Alexey Botchkov,113698,2018-07-09 21:04:53,http://lists.askmonty.org/pipermail/commits/2018-July/012692.html,10,URL
2175,MDEV-15473,MDEV,Alexey Botchkov,113924,2018-07-14 19:44:03,http://lists.askmonty.org/pipermail/commits/2018-July/012698.html,11,URL
2176,MDEV-15501,MDEV,Vladislav Vaintroub,108020,2018-03-07 12:23:54,"proxy_protocol_networks are masks,  not IP addresses, so you definitely can add a new IP as long as it is subnet conforming, and do so without a restart.

You can always add any proxy, if the value is * (not recommended, obviously)
",1,"proxy_protocol_networks are masks,  not IP addresses, so you definitely can add a new IP as long as it is subnet conforming, and do so without a restart.

You can always add any proxy, if the value is * (not recommended, obviously)
"
2177,MDEV-15501,MDEV,Vladislav Vaintroub,108021,2018-03-07 12:26:38,You do not have to have a *big* subnet either. You can have a small subnet too,2,You do not have to have a *big* subnet either. You can have a small subnet too
2178,MDEV-15501,MDEV,dapeng huang,108023,2018-03-07 12:28:43,"In some situation, we may add more node for proxy cluster, and cannot determine which subnet of new node in advance, so need dynamic config",3,"In some situation, we may add more node for proxy cluster, and cannot determine which subnet of new node in advance, so need dynamic config"
2179,MDEV-20720,MDEV,Tim Soderstrom,87621,2016-10-19 19:02:43,"This sounds similar to what I ran into but it seemed a tad vague. I am running MariaDB 10.1.18. I have a 3 node Galera cluster and an async slave. GTID's are enabled and all nodes have 'log-bin' and 'log-slave-updates' and are using 0 for the domain (the default).

What I found was that all Galera nodes seem to be writing all data to their binary logs, but their GTIDs do not match. I can find things by the transaction-id across all the logs, but if I try to find things by GTID, results are inconsistent. This means I cannot merely re-point the slave server to another node because that node does not have the same GTID information as the current master and, thus, the slave does not no where to begin.

It sounds like this issue applies to this bug? I see the target is 10.2. If so, it would be ideal if it was reflected in the KB documentation for 10.1?",1,"This sounds similar to what I ran into but it seemed a tad vague. I am running MariaDB 10.1.18. I have a 3 node Galera cluster and an async slave. GTID's are enabled and all nodes have 'log-bin' and 'log-slave-updates' and are using 0 for the domain (the default).

What I found was that all Galera nodes seem to be writing all data to their binary logs, but their GTIDs do not match. I can find things by the transaction-id across all the logs, but if I try to find things by GTID, results are inconsistent. This means I cannot merely re-point the slave server to another node because that node does not have the same GTID information as the current master and, thus, the slave does not no where to begin.

It sounds like this issue applies to this bug? I see the target is 10.2. If so, it would be ideal if it was reflected in the KB documentation for 10.1?"
2180,MDEV-20720,MDEV,Andrew Garner,87623,2016-10-19 20:31:44,"Tim, I think you may be  running into MDEV-10944 (a 10.1.18 regression).  Although there are a myriad of other ways to get MariaDB GTIDs out of sync in a Galera cluster, even without that regression.",2,"Tim, I think you may be  running into MDEV-10944 (a 10.1.18 regression).  Although there are a myriad of other ways to get MariaDB GTIDs out of sync in a Galera cluster, even without that regression."
2181,MDEV-20720,MDEV,Tim Soderstrom,87624,2016-10-19 20:53:35,"Doh you are right, that sounds exactly like our problem. Bug search fail on my part - thank you for providing that!",3,"Doh you are right, that sounds exactly like our problem. Bug search fail on my part - thank you for providing that!"
2182,MDEV-20720,MDEV,Michaël de groot,90023,2016-12-28 11:40:08,"Hi!

I also noticed that the _initial_ GTID is not passed on with all SST methods. If I remember correctly, only rsync SST will sync it.

I think this implementation should make fixing that in SST unneeded, could you please confirm that?

Thanks,
Michaël",4,"Hi!

I also noticed that the _initial_ GTID is not passed on with all SST methods. If I remember correctly, only rsync SST will sync it.

I think this implementation should make fixing that in SST unneeded, could you please confirm that?

Thanks,
Michaël"
2183,MDEV-20720,MDEV,Michaël de groot,92941,2017-03-13 14:42:32,"[~sachin.setiya.007] can you please tell me why this issue is stalled? This is very important issue to fix. Right now it is very inconvenient to replicate 1 galera cluster to another, and with circular replication between 2 galera clusters it becomes a real pain.",5,"[~sachin.setiya.007] can you please tell me why this issue is stalled? This is very important issue to fix. Right now it is very inconvenient to replicate 1 galera cluster to another, and with circular replication between 2 galera clusters it becomes a real pain."
2184,MDEV-20720,MDEV,Geoff Montee,98289,2017-08-04 17:19:04,"I assume that ""Fix Version/s: 10.2"" is not accurate anymore. Since 10.2 is already GA, I assume this would go into MariaDB 10.3 at the earliest. Is that correct?
",6,"I assume that ""Fix Version/s: 10.2"" is not accurate anymore. Since 10.2 is already GA, I assume this would go into MariaDB 10.3 at the earliest. Is that correct?
"
2185,MDEV-20720,MDEV,Sachin Setiya,99906,2017-09-11 08:57:59,"I am occupied by galera merges and , galera bugs ,so I did not get the time to do this , I will again start working on  this hopefully in next week.",7,"I am occupied by galera merges and , galera bugs ,so I did not get the time to do this , I will again start working on  this hopefully in next week."
2186,MDEV-20720,MDEV,Sachin Setiya,101387,2017-10-10 12:25:59,http://lists.askmonty.org/pipermail/commits/2017-October/011552.html,8,URL
2187,MDEV-20720,MDEV,Michaël de groot,101403,2017-10-10 13:44:10,"Cool, very nice that this issue is finally getting done! Thank you [~sachin.setiya.007].

In the tests, please consider circular asynchronous replication between 2 or more Galera clusters:

Cluster 1: A <> B <> C
Cluster 2: D <> E <> F

All nodes have log_slave_updates enabled. Bidirectional asynchronous replication is between node A and node D. Writes originate from, for example, node B. 
Node D goes down. With this change, we should now be able to change the streams easily:
On node A: STOP SLAVE; CHANGE MASTER TO MASTER_HOST='e'; START SLAVE;
On node E: CHANGE MASTER TO MASTER_HOST='a', MASTER_USER='repl', MASTER_PASSWORD='insecure', MASTER_USE_GTID=slave_pos; START SLAVE;

Can you please make sure this scenario is tested?

[~sachin.setiya.007] maybe the implementation done here is not enough for this use case. How does node e recognize transactions that originated from node D? Maybe we need to set up ignore domain ID on the asynchronous replication stream?",9,"Cool, very nice that this issue is finally getting done! Thank you [~sachin.setiya.007].

In the tests, please consider circular asynchronous replication between 2 or more Galera clusters:

Cluster 1: A  C
Cluster 2: D  F

All nodes have log_slave_updates enabled. Bidirectional asynchronous replication is between node A and node D. Writes originate from, for example, node B. 
Node D goes down. With this change, we should now be able to change the streams easily:
On node A: STOP SLAVE; CHANGE MASTER TO MASTER_HOST='e'; START SLAVE;
On node E: CHANGE MASTER TO MASTER_HOST='a', MASTER_USER='repl', MASTER_PASSWORD='insecure', MASTER_USE_GTID=slave_pos; START SLAVE;

Can you please make sure this scenario is tested?

[~sachin.setiya.007] maybe the implementation done here is not enough for this use case. How does node e recognize transactions that originated from node D? Maybe we need to set up ignore domain ID on the asynchronous replication stream?"
2188,MDEV-20720,MDEV,Sachin Setiya,101485,2017-10-12 08:59:53,"Hi [~michaeldg],

Writing a mtr test case for this situation is bit difficult. But I will try to simulate this on vms

Regards
sachin ",10,"Hi [~michaeldg],

Writing a mtr test case for this situation is bit difficult. But I will try to simulate this on vms

Regards
sachin "
2189,MDEV-20720,MDEV,Andrei Elkin,101837,2017-10-24 13:58:39,"Sachin, hello.

Please check out a review mail I sent out.

Cheers,

Andrei.",11,"Sachin, hello.

Please check out a review mail I sent out.

Cheers,

Andrei."
2190,MDEV-20720,MDEV,Sachin Setiya,103705,2017-11-28 10:13:48,"Status Update:-

Actually test case for 2X3 node galera cluster has been created, But this tests fails because of 
issue with rpl_slave_state:hash.
More information of this bug [Problem|https://lists.launchpad.net/maria-developers/msg11005.html]",12,"Status Update:-

Actually test case for 2X3 node galera cluster has been created, But this tests fails because of 
issue with rpl_slave_state:hash.
More information of this bug [Problem|URL"
2191,MDEV-20720,MDEV,Sachin Setiya,103707,2017-11-28 10:16:07,Branch [Buildbot|http://buildbot.askmonty.org/buildbot/grid?category=main&branch=bb-10.1-10715],13,Branch [Buildbot|URL
2192,MDEV-20720,MDEV,Sachin Setiya,104002,2017-12-04 08:33:57,"Status Update:- All issue solved. 

So the problem was suppose A cluster like this 

A <--> B <--> C  (Galera Cluster 1)
|                
|              (Circular normal replication between A < -- > D (no galera))
|
D <--> E <--> F  (Galera Cluster 2)

So the event group arriving from B , C was applied 2 times on A (similarly for event group of E, F to D).
Reason being Galera event group does not contain GTID_LOG_EVENT , so say when A recieved an event group from B its rpl_slave_state::hash(gtid_slave_pos) is not updated, So when A gets the same event group from D(because of circular replication) It will apply this event again. If we set ignore_server_ids while setting circular replication this problem can be solved. 
A will ignore server id of B,C  And D will ignore server id of E , F. Replicate-same-sever-id shuould be turned off.",14,"Status Update:- All issue solved. 

So the problem was suppose A cluster like this 

A  B  C  (Galera Cluster 1)
|                
|              (Circular normal replication between A  D (no galera))
|
D  E  F  (Galera Cluster 2)

So the event group arriving from B , C was applied 2 times on A (similarly for event group of E, F to D).
Reason being Galera event group does not contain GTID_LOG_EVENT , so say when A recieved an event group from B its rpl_slave_state::hash(gtid_slave_pos) is not updated, So when A gets the same event group from D(because of circular replication) It will apply this event again. If we set ignore_server_ids while setting circular replication this problem can be solved. 
A will ignore server id of B,C  And D will ignore server id of E , F. Replicate-same-sever-id shuould be turned off."
2193,MDEV-20720,MDEV,Sachin Setiya,104288,2017-12-11 08:42:40,"There is one more constraint, In the case of master slave replication to gtid cluster. Or Gtid_cluster to gtid_cluster (async or may be circular replication ) , Cluster should have different domain id wrt to master or slave",15,"There is one more constraint, In the case of master slave replication to gtid cluster. Or Gtid_cluster to gtid_cluster (async or may be circular replication ) , Cluster should have different domain id wrt to master or slave"
2194,MDEV-20720,MDEV,Sachin Setiya,104917,2017-12-25 15:41:12,http://lists.askmonty.org/pipermail/commits/2017-December/011761.html,16,URL
2195,MDEV-20720,MDEV,Mark Stoute,109958,2018-04-20 14:51:15,"Thank you for this fix. 
I upgraded my production cluster to 10.1.32 via rolling-restart, and found GTIDs out of sync, and wsrep_provider_version is still behind (25.3.18(r3632)). Prod cluster was initially bootstrapped as v 10.1.18.

In a dev cluster where I bootstrapped the cluster from 10.1.32, GTIDs are in sync and wsrep_provider_version is higher (25.3.23(r3789)).

Is it true that in order to have my production cluster have GTIDs in sync, I will need to bootstrap with 10.1.32?",17,"Thank you for this fix. 
I upgraded my production cluster to 10.1.32 via rolling-restart, and found GTIDs out of sync, and wsrep_provider_version is still behind (25.3.18(r3632)). Prod cluster was initially bootstrapped as v 10.1.18.

In a dev cluster where I bootstrapped the cluster from 10.1.32, GTIDs are in sync and wsrep_provider_version is higher (25.3.23(r3789)).

Is it true that in order to have my production cluster have GTIDs in sync, I will need to bootstrap with 10.1.32?"
2196,MDEV-20720,MDEV,Sachin Setiya,110491,2018-05-03 14:20:08,"There is been some confusion, the gtid has been transferred between nodes only of the cluster is async slave , If we want to transfer gtid inside of write set that will be bugger change , and will involve changing galera code 
by change galera gtid to become same gtid format as of mariadb and use this gtid in commit instead of generating gtid.
",18,"There is been some confusion, the gtid has been transferred between nodes only of the cluster is async slave , If we want to transfer gtid inside of write set that will be bugger change , and will involve changing galera code 
by change galera gtid to become same gtid format as of mariadb and use this gtid in commit instead of generating gtid.
"
2197,MDEV-20720,MDEV,Arjen Lentz,113639,2018-07-09 02:54:11,"[~sachin.setiya.007] it would be ok if Galera just passed the MariaDB GTID around as-is (as an extra arbitrary field as part of a commit), so it will be stored in each binlog.  That would not require Galera to start using MariaDB GTIDs.  Just see them as separate: Galera GTID and MariaDB GTID.
The issue is that right now, what's happening with say [MDEV-14153] is just horrendous.",19,"[~sachin.setiya.007] it would be ok if Galera just passed the MariaDB GTID around as-is (as an extra arbitrary field as part of a commit), so it will be stored in each binlog.  That would not require Galera to start using MariaDB GTIDs.  Just see them as separate: Galera GTID and MariaDB GTID.
The issue is that right now, what's happening with say [MDEV-14153] is just horrendous."
2198,MDEV-20720,MDEV,Sachin Setiya,113769,2018-07-11 07:00:01,"Hi [~arjen] 

Actually that wont work , because lets mariadb server some how has to generate gtid in sync, lets say we have 3 node cluster with each node gtid 1-1-1 , and then we do simultaneous write on node 1 and node 2, So both will generate gtid 1-1-2 and this will be wrong sequence. So we need galera to manage gtid , since it is transaction coordinator not mariadb  ",20,"Hi [~arjen] 

Actually that wont work , because lets mariadb server some how has to generate gtid in sync, lets say we have 3 node cluster with each node gtid 1-1-1 , and then we do simultaneous write on node 1 and node 2, So both will generate gtid 1-1-2 and this will be wrong sequence. So we need galera to manage gtid , since it is transaction coordinator not mariadb  "
2199,MDEV-20720,MDEV,Arjen Lentz,113772,2018-07-11 07:56:06,"I'm sorry [~sachin.setiya.007] but that's just not correct.  Remember that GTID also works in an async replication and master-master configuration.
The format is S-D-# where S is the server-id (which should be unique in the cluster or replication environment), D for the replication domain (see the MariaDB docs, it tends to be 0 by default unless the application sets it to something else, and # for the # going up within that.
So for your example, you'd actually see something like 1-0-1 and 2-0-1 on the two different servers, which is a perfectly correct flow of things, and the next transactions written on the servers after that will be something like 1-0-2 and 2-0-2.
Hope this clarifies.",21,"I'm sorry [~sachin.setiya.007] but that's just not correct.  Remember that GTID also works in an async replication and master-master configuration.
The format is S-D-# where S is the server-id (which should be unique in the cluster or replication environment), D for the replication domain (see the MariaDB docs, it tends to be 0 by default unless the application sets it to something else, and # for the # going up within that.
So for your example, you'd actually see something like 1-0-1 and 2-0-1 on the two different servers, which is a perfectly correct flow of things, and the next transactions written on the servers after that will be something like 1-0-2 and 2-0-2.
Hope this clarifies."
2200,MDEV-20720,MDEV,Sachin Setiya,113773,2018-07-11 08:14:12,"When we have different domain id , then user ensures that the bingol events don't conflict each other , but this is not the case with galera, galera can handle conflicts , so I think within one cluster we should have one domain id, and this is what galera internally does , it has one uuid for one cluster ",22,"When we have different domain id , then user ensures that the bingol events don't conflict each other , but this is not the case with galera, galera can handle conflicts , so I think within one cluster we should have one domain id, and this is what galera internally does , it has one uuid for one cluster "
2201,MDEV-20720,MDEV,Daniel Black,113776,2018-07-11 08:57:38,Format is D-S-# and I'm fairly sure [~arjen] is talking about different server IDs on each galera node (despite a little dyslexia).,23,Format is D-S-# and I'm fairly sure [~arjen] is talking about different server IDs on each galera node (despite a little dyslexia).
2202,MDEV-20720,MDEV,Sachin Setiya,113777,2018-07-11 09:02:58,"[~danblack], right Format is D-S-X , Actually my first comment is slightly wrong , each node will have gtid 1(constant)-X(node server id) -Y(seq no ), server id will be different on each node , but still seq no will be wrt to domain id",24,"[~danblack], right Format is D-S-X , Actually my first comment is slightly wrong , each node will have gtid 1(constant)-X(node server id) -Y(seq no ), server id will be different on each node , but still seq no will be wrt to domain id"
2203,MDEV-20720,MDEV,Arjen Lentz,113778,2018-07-11 09:32:57,"yes thanks Dan - I had it right in a blogpost the other day.

[~sachin.setiya.007] The seq# component its own is not unique, it's the GTID as a whole that needs to be unique.
The UUID you're referring to is the Galera cluster identifier, which is indeed a single unique ID across the entire cluster - it never changes; this is how a node can see whether it belongs in a cluster or not.  If you bootstrap a new cluster, a new UUID is generated.",25,"yes thanks Dan - I had it right in a blogpost the other day.

[~sachin.setiya.007] The seq# component its own is not unique, it's the GTID as a whole that needs to be unique.
The UUID you're referring to is the Galera cluster identifier, which is indeed a single unique ID across the entire cluster - it never changes; this is how a node can see whether it belongs in a cluster or not.  If you bootstrap a new cluster, a new UUID is generated."
2204,MDEV-20720,MDEV,Sachin Setiya,113782,2018-07-11 10:22:14,"[~arjen], I never said that the sequence no is unique its own , it is unique with respect to domain id , For example 1-1-1 and 1-2-1 and conflicting gtid , However 1-1-1 and 2-1-1 and perfectly okay gtid  https://mariadb.com/kb/en/library/gtid/#the-domain-id",26,"[~arjen], I never said that the sequence no is unique its own , it is unique with respect to domain id , For example 1-1-1 and 1-2-1 and conflicting gtid , However 1-1-1 and 2-1-1 and perfectly okay gtid  URL"
2205,MDEV-20720,MDEV,Daniel Black,113822,2018-07-12 00:47:52,"GTIDs need to pass through the cluster. Consider this requirement:

* A DB connection occurs through a DB load balancer, at the end of the updating a user's profile transaction, the GTID is selected by the application
* The gtid is placed in the web session information for that user.
* The user in the next web fetches a new web page going though the load balancer to a different cluster member (or even async slave for that matter).
* Because galera transactions or async slaves aren't applied immediately, a query of the user's profile may retrieve an out of date version. To prevent this the DB application should be able to {code:sql}SELECT master_gtid_wait(@gtid, 0.1){code} to ensure it has the latest data that the user previously updated (it can deal with the timeout).

I'm sure I'm not the only one of the 19 voters and 31 watchers wanting this.

There should be no need for the application to consider that a *G*TID is anything but a global identifier.

Galera needs to ensure that the sequential visibility in applying each D-S pair (i.e 0-1-33 isn't visible when 0-1-22 isn't) so of course the gtid needs to be transferred in the writeset.
Galera should handle that 0-1-33 and 0-2-33 are unique transactions from different servers no matter what the replication process was taken to deliver them.

Each server has its own server-id and can be responsible for GTID generation without coordination. If the certification fails then the server skips a GTID value. The galera GTID has a different purpose so its needed to stay independent.

If that server is part of the cluster then the galera mechanism can ensure that can be applied without conflict however this is independent on what the GTID actually is.",27,"GTIDs need to pass through the cluster. Consider this requirement:

* A DB connection occurs through a DB load balancer, at the end of the updating a user's profile transaction, the GTID is selected by the application
* The gtid is placed in the web session information for that user.
* The user in the next web fetches a new web page going though the load balancer to a different cluster member (or even async slave for that matter).
* Because galera transactions or async slaves aren't applied immediately, a query of the user's profile may retrieve an out of date version. To prevent this the DB application should be able to {code:sql}SELECT master_gtid_wait(@gtid, 0.1){code} to ensure it has the latest data that the user previously updated (it can deal with the timeout).

I'm sure I'm not the only one of the 19 voters and 31 watchers wanting this.

There should be no need for the application to consider that a *G*TID is anything but a global identifier.

Galera needs to ensure that the sequential visibility in applying each D-S pair (i.e 0-1-33 isn't visible when 0-1-22 isn't) so of course the gtid needs to be transferred in the writeset.
Galera should handle that 0-1-33 and 0-2-33 are unique transactions from different servers no matter what the replication process was taken to deliver them.

Each server has its own server-id and can be responsible for GTID generation without coordination. If the certification fails then the server skips a GTID value. The galera GTID has a different purpose so its needed to stay independent.

If that server is part of the cluster then the galera mechanism can ensure that can be applied without conflict however this is independent on what the GTID actually is."
2206,MDEV-20720,MDEV,Valerii Kravchuk,115096,2018-08-10 09:07:51,"We should also consider the case of ALTER running on node by node in RSU mode. We should end up with consisetnt GTIDs in cluster after this, or invent some workaround (do not generate local GTIDs while in RSU mode, request to do everything with sql_log_bin=0?).",28,"We should also consider the case of ALTER running on node by node in RSU mode. We should end up with consisetnt GTIDs in cluster after this, or invent some workaround (do not generate local GTIDs while in RSU mode, request to do everything with sql_log_bin=0?)."
2207,MDEV-20720,MDEV,Kristian Nielsen,115114,2018-08-10 15:52:39,"RSU=rolling schema upgrade, perhaps?

If you want the ALTERs to replicate to async slaves not part of the cluster, the GTID way is to binlog the ALTER in a separate domain id (SET SESSION gtid_domain_id=xxx). This will make them independent of the normal binlog stream. Grab the @@last_gtid from the first node, and use it to set server_id / gtid_seq_no on the other nodes to get the same GTID on all nodes for the ALTER.

If you do not want the ALTERs to replicate async to slaves, SET SESSION sql_log_bin=0 is the way.",29,"RSU=rolling schema upgrade, perhaps?

If you want the ALTERs to replicate to async slaves not part of the cluster, the GTID way is to binlog the ALTER in a separate domain id (SET SESSION gtid_domain_id=xxx). This will make them independent of the normal binlog stream. Grab the @@last_gtid from the first node, and use it to set server_id / gtid_seq_no on the other nodes to get the same GTID on all nodes for the ALTER.

If you do not want the ALTERs to replicate async to slaves, SET SESSION sql_log_bin=0 is the way."
2208,MDEV-20720,MDEV,Sylvain ARBAUDIE,131027,2019-07-12 20:40:31,Would there be any issue using the galera seqno as the last part of the mariadb gtid ? apart for RSU DDL i mean ?,30,Would there be any issue using the galera seqno as the last part of the mariadb gtid ? apart for RSU DDL i mean ?
2209,MDEV-20720,MDEV,Teemu Ollakka,131037,2019-07-15 05:30:50,"There are at least two major issues which need to be resolved in order to use Galera seqno as part of the MariaDB GTID:
#  Occasionally Galera seqno is generated for a write set which do not commit a transaction, these include (but not limited to) write sets that fail certification and intermediate streaming replication fragments. In order to keep GTID sequences continuous, all of these events should be logged in binlog as dummy events, which could cause excessive clutter under certain workloads.
# Master-slave topology where Galera cluster acts as a slave: It is required that the original GTID from the master should be preserved in binlog events. However, as Galera will generate a write set/seqno for the applied transaction, there will be two GTIDs which should be persisted in binlog for each transaction. It is not clear how this could be handled to preserve compatibility with async master/slave replication.

",31,"There are at least two major issues which need to be resolved in order to use Galera seqno as part of the MariaDB GTID:
#  Occasionally Galera seqno is generated for a write set which do not commit a transaction, these include (but not limited to) write sets that fail certification and intermediate streaming replication fragments. In order to keep GTID sequences continuous, all of these events should be logged in binlog as dummy events, which could cause excessive clutter under certain workloads.
# Master-slave topology where Galera cluster acts as a slave: It is required that the original GTID from the master should be preserved in binlog events. However, as Galera will generate a write set/seqno for the applied transaction, there will be two GTIDs which should be persisted in binlog for each transaction. It is not clear how this could be handled to preserve compatibility with async master/slave replication.

"
2210,MDEV-20720,MDEV,Geoff Montee,135213,2019-10-02 18:56:16,"A feature like MDEV-20715 could also improve Galera's support for MariaDB GTIDs. Specifically, it could prevent each node from generating GTIDs for local transactions, which could make it easier for replication slaves to use any cluster node as master, without risking inconsistent GTIDs.",32,"A feature like MDEV-20715 could also improve Galera's support for MariaDB GTIDs. Specifically, it could prevent each node from generating GTIDs for local transactions, which could make it easier for replication slaves to use any cluster node as master, without risking inconsistent GTIDs."
2211,MDEV-20720,MDEV,Sachin Setiya,142500,2020-01-21 13:07:53,"Okay to push
",33,"Okay to push
"
2212,MDEV-20720,MDEV,Ian Gilfillan,148231,2020-03-29 23:21:46,"The pull request linked with this issue is still marked as open, although the task has been closed.",34,"The pull request linked with this issue is still marked as open, although the task has been closed."
2213,MDEV-3929,MDEV,Timour Katchaounov,28906,2012-12-17 15:37:19,"The feature is described here:
http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp

The relevant MySQL changesets are:

revno: 3690.68.37
revision-id: gopal.shankar@oracle.com-20120621025740-1mqjdhta9p5z54kq
parent: mysql-builder@oracle.com-20120620213112-ru1ze8ih8zdnmffh
committer: Gopal Shankar <gopal.shankar@oracle.com>
branch nick: mysql-trunk-wl6292-push
timestamp: Thu 2012-06-21 08:27:40 +0530
message:
  WL#6292 - Make TIMESTAMP columns nullable by default.

revno: 3899
revision-id: gopal.shankar@oracle.com-20120624093818-4zuroxvphp9199xa
parent: sergey.vojtovich@oracle.com-20120624070758-rhkxh0vqnlqusdo2
committer: Gopal Shankar <gopal.shankar@oracle.com>
branch nick: mysql-trunk-wl6292-postmergefixes
timestamp: Sun 2012-06-24 15:08:18 +0530
message:
  WL#6292 - Make TIMESTAMP columns nullable by default.

revno: 4111
revision-id: gopal.shankar@oracle.com-20120803135652-2lulve24dup1to60
parent: guilhem.bichot@oracle.com-20120803124053-psgnzlwyu3o77sw4
committer: Gopal Shankar <gopal.shankar@oracle.com>
branch nick: timestamp_makevisible_5.6
timestamp: Fri 2012-08-03 19:26:52 +0530
message:
  Bug#14409088 - MAKE --EXPLICIT_DEFAULTS_FOR_TIMESTAMP SETTING
                 VISIBLE TO USERS
",1,"The feature is described here:
URL

The relevant MySQL changesets are:

revno: 3690.68.37
revision-id: gopal.shankar@oracle.com-20120621025740-1mqjdhta9p5z54kq
parent: mysql-builder@oracle.com-20120620213112-ru1ze8ih8zdnmffh
committer: Gopal Shankar 
branch nick: mysql-trunk-wl6292-push
timestamp: Thu 2012-06-21 08:27:40 +0530
message:
  WL#6292 - Make TIMESTAMP columns nullable by default.

revno: 3899
revision-id: gopal.shankar@oracle.com-20120624093818-4zuroxvphp9199xa
parent: sergey.vojtovich@oracle.com-20120624070758-rhkxh0vqnlqusdo2
committer: Gopal Shankar 
branch nick: mysql-trunk-wl6292-postmergefixes
timestamp: Sun 2012-06-24 15:08:18 +0530
message:
  WL#6292 - Make TIMESTAMP columns nullable by default.

revno: 4111
revision-id: gopal.shankar@oracle.com-20120803135652-2lulve24dup1to60
parent: guilhem.bichot@oracle.com-20120803124053-psgnzlwyu3o77sw4
committer: Gopal Shankar 
branch nick: timestamp_makevisible_5.6
timestamp: Fri 2012-08-03 19:26:52 +0530
message:
  Bug#14409088 - MAKE --EXPLICIT_DEFAULTS_FOR_TIMESTAMP SETTING
                 VISIBLE TO USERS
"
2214,MDEV-3929,MDEV,Timour Katchaounov,28907,2012-12-17 17:05:54,"Analysis of the patches above. They change code in three main areas:

= the server:
- the default nullability of TIMESTAMP columns,
- the default assumption that the first TIMESTAMP should be autoupdated on INSERT/UPDATE
- non-NULLable TIMESTAMP columns without explicit default are treated as having no default

=> This is very easy to implement ~half a day

= replication:
""This variable is used by User thread and
  as well as by replication slave applier thread to apply relay_log.
  Slave applier thread enables/disables this option based on
  relay_log's from replication master versions. There is possibility of
  slave applier thread and User thread to have different setting for
  explicit_defaults_for_timestamp""

=> The change seems quite big, and I am not an expert. Kristian disliked the idea
of the patch. I don't know replication, so I cannot evaluate the quality of this new code.

= tests
Many tests have been adjusted to include the previously implicit TIMESTAMP
properties as explicit part of the create table statement. I don't understand for sure
why this was done, as the default is the old behavior. My guess is that in this way
the test cases would not have to be changed in the future, once the new behavior
becomes default.

=> Since there are many test files, and even more test results that need to be
verified, adapting/changing tests would take about 1/2 - 1 day.",2,"Analysis of the patches above. They change code in three main areas:

= the server:
- the default nullability of TIMESTAMP columns,
- the default assumption that the first TIMESTAMP should be autoupdated on INSERT/UPDATE
- non-NULLable TIMESTAMP columns without explicit default are treated as having no default

=> This is very easy to implement ~half a day

= replication:
""This variable is used by User thread and
  as well as by replication slave applier thread to apply relay_log.
  Slave applier thread enables/disables this option based on
  relay_log's from replication master versions. There is possibility of
  slave applier thread and User thread to have different setting for
  explicit_defaults_for_timestamp""

=> The change seems quite big, and I am not an expert. Kristian disliked the idea
of the patch. I don't know replication, so I cannot evaluate the quality of this new code.

= tests
Many tests have been adjusted to include the previously implicit TIMESTAMP
properties as explicit part of the create table statement. I don't understand for sure
why this was done, as the default is the old behavior. My guess is that in this way
the test cases would not have to be changed in the future, once the new behavior
becomes default.

=> Since there are many test files, and even more test results that need to be
verified, adapting/changing tests would take about 1/2 - 1 day."
2215,MDEV-3929,MDEV,Timour Katchaounov,28908,2012-12-17 17:12:00,"In an IRC discussion Kristian was against backporting this feature for the following reasons (edited IRC discussion):

<timour> From the code and comments, this is what I understand - 
<timour> during replication the slave checks this option on the master.
<timour> and adjusts the option on the slave so that it is compatible.

<knielsen> oh, that sounds broken :-(
<knielsen> because one can stop master, change the option, start master, and then slave can connect and read both old and new events from master (made with different value for the option)

<timour> this is the main goal of the patch - 
<timour> add a new system variable that:
<timour> - makes timestamp columns NULL by default (unlike now, they are NOT NULL)
<timour> - do not autoupdate the first timestamp column (now it is assumed to be auto-updated)
<timour> - also change non-NULL-able timestamp columns have no assumed default 0

<knielsen> any particular reason for us to merge that system variable? Sounds to me it adds more complexity for little gain?
<timour>  this variable deprecates old non-standard MySQL behavior with respect to timestamp fields.
<timour>  so if we don't merge it, we will be non-compatible in the next version.
<knielsen>  I'd say MySQL 5.7 (or whatever) will be non-compatible with older MySQL, while we will be compatible?
<knielsen> this deprecation sounds like a reasonable idea in theory, but in practice, it seems not to work so well, old applications start breaking when deprecated stuff is removed, even if it was deprecated for 10 years or whatever
<knielsen> so basically, this option, which is non-default, sounds like something I'd suggest not to be merged. But I didn't look closely, maybe I misunderstood
<timour> I personally think that making timestamp behave as other fields is a good thing.
<knielsen> yes, that would be good, but do you see a realistic way forward to change it without breaking too much existing applications?
<timour> the problem for me is that the main part of the code related to this variable is replication, and I cannot evaluate this change.
<timour> well, for now its off by default, so nothing should break
<knielsen> it's > 6k lines of code, so it's not something anyone can evaluate at a glance ..
<knielsen> yes, now it's off. But it seems to me we could never make it on by default?
<timour> the problem will come when it gets ON by default, then users can switch it OFF, this is the only way I can see.
<knielsen> deprecation usually works like this: there is some feature that does something odd, we want to remove it. So we make it give a deprecation warning for some years, then we remove it, assuming applications have been changed to use the recommended syntax/feature
<knielsen> even this is somewhat doubtfull in my mind ... apps don't change, is the experience
<knielsen> but it seems to me - this is something that *changes* behaviour? So there is no ""bad"" old syntax, rather the meaning of reasonable syntax is *changed*
<knielsen> so if I have an app, what should I do? How can I change my app so it works correctly with the deprecated feature, and also will work correctly with the new feature?
<knielsen> it seems I cannot, except to always specify TIMESTAMP explicitly as NOT NULL or NULL, as I need, and explicitly specifying a DEFAULT.
<knielsen> and then the option is in any case pointless, as apps anyway have to avoid any syntax affected by the option
<timour> you can change the app by not relying on the implicit default behavior - for each TIMESTAMP column, declare explicitly if its NULL or not, and if has a DEFAULT or not.
<knielsen> so the ""correct"" way to deprecate would seem to be to first deprecate using TIMESTAMP without explicit NULLability and default - and then make it an error - and then later re-add the ability with the new mechanism
<knielsen> which seems just too much bother
<knielsen> if we really want this, then better introduce TIMESTAMP2 or something like that with correct semantics, and retain old TIMESTAMP behaviour for compatibility
<knielsen> Just to be clear: I've no love for current TIMESTAMP complex/non-standard semantics, but I don't see adding some even more complex option semantics improves things ...
",3,"In an IRC discussion Kristian was against backporting this feature for the following reasons (edited IRC discussion):

 From the code and comments, this is what I understand - 
 during replication the slave checks this option on the master.
 and adjusts the option on the slave so that it is compatible.

 oh, that sounds broken :-(
 because one can stop master, change the option, start master, and then slave can connect and read both old and new events from master (made with different value for the option)

 this is the main goal of the patch - 
 add a new system variable that:
 - makes timestamp columns NULL by default (unlike now, they are NOT NULL)
 - do not autoupdate the first timestamp column (now it is assumed to be auto-updated)
 - also change non-NULL-able timestamp columns have no assumed default 0

 any particular reason for us to merge that system variable? Sounds to me it adds more complexity for little gain?
  this variable deprecates old non-standard MySQL behavior with respect to timestamp fields.
  so if we don't merge it, we will be non-compatible in the next version.
  I'd say MySQL 5.7 (or whatever) will be non-compatible with older MySQL, while we will be compatible?
 this deprecation sounds like a reasonable idea in theory, but in practice, it seems not to work so well, old applications start breaking when deprecated stuff is removed, even if it was deprecated for 10 years or whatever
 so basically, this option, which is non-default, sounds like something I'd suggest not to be merged. But I didn't look closely, maybe I misunderstood
 I personally think that making timestamp behave as other fields is a good thing.
 yes, that would be good, but do you see a realistic way forward to change it without breaking too much existing applications?
 the problem for me is that the main part of the code related to this variable is replication, and I cannot evaluate this change.
 well, for now its off by default, so nothing should break
 it's > 6k lines of code, so it's not something anyone can evaluate at a glance ..
 yes, now it's off. But it seems to me we could never make it on by default?
 the problem will come when it gets ON by default, then users can switch it OFF, this is the only way I can see.
 deprecation usually works like this: there is some feature that does something odd, we want to remove it. So we make it give a deprecation warning for some years, then we remove it, assuming applications have been changed to use the recommended syntax/feature
 even this is somewhat doubtfull in my mind ... apps don't change, is the experience
 but it seems to me - this is something that *changes* behaviour? So there is no ""bad"" old syntax, rather the meaning of reasonable syntax is *changed*
 so if I have an app, what should I do? How can I change my app so it works correctly with the deprecated feature, and also will work correctly with the new feature?
 it seems I cannot, except to always specify TIMESTAMP explicitly as NOT NULL or NULL, as I need, and explicitly specifying a DEFAULT.
 and then the option is in any case pointless, as apps anyway have to avoid any syntax affected by the option
 you can change the app by not relying on the implicit default behavior - for each TIMESTAMP column, declare explicitly if its NULL or not, and if has a DEFAULT or not.
 so the ""correct"" way to deprecate would seem to be to first deprecate using TIMESTAMP without explicit NULLability and default - and then make it an error - and then later re-add the ability with the new mechanism
 which seems just too much bother
 if we really want this, then better introduce TIMESTAMP2 or something like that with correct semantics, and retain old TIMESTAMP behaviour for compatibility
 Just to be clear: I've no love for current TIMESTAMP complex/non-standard semantics, but I don't see adding some even more complex option semantics improves things ...
"
2216,MDEV-3929,MDEV,Sergei Golubchik,28912,2012-12-17 23:22:06,"both me and monty were against this too. let's postpone this feature until 10.1.0, and then reconsider",4,"both me and monty were against this too. let's postpone this feature until 10.1.0, and then reconsider"
2217,MDEV-3929,MDEV,Timour Katchaounov,28927,2012-12-18 18:21:02,"Monty's suggestion is to implement the flag without changes to replication,
but instead move all dynamic decisions to the parser, so that the timestamp
fields are created according to this flag. That is, the different behavior is
recorded in the table definition, instead of checking every time at runtime.",5,"Monty's suggestion is to implement the flag without changes to replication,
but instead move all dynamic decisions to the parser, so that the timestamp
fields are created according to this flag. That is, the different behavior is
recorded in the table definition, instead of checking every time at runtime."
2218,MDEV-3929,MDEV,Elena Stepanova,30200,2013-02-23 17:47:28,"In case we ever decide to implement it, check the status of http://bugs.mysql.com/bug.php?id=68472 (just filed, I'm not sure whether it's going to be fixed or rejected as 'Not a bug').",6,"In case we ever decide to implement it, check the status of URL (just filed, I'm not sure whether it's going to be fixed or rejected as 'Not a bug')."
2219,MDEV-3929,MDEV,Elena Stepanova,69728,2015-04-02 19:01:14,See MDEV-5836 for the upstream revision/commit message.,7,See MDEV-5836 for the upstream revision/commit message.
2220,MDEV-3929,MDEV,Alexander Barkov,73322,2015-07-09 11:11:48,"Monty suggested the following regarding replication of explicit_defaults_for_timestamp:
{quote}
ok, the easy way to do this would be:
Change SELECT_DISTINCT from 1 to (1ULL << 39)
Add - OPTION_explicit_defaults_for_timestamp as 1
Add  OPTION_explicit_defaults_for_timestamp to OPTIONS_WRITTEN_TO_BIN_LOG
In  Query_log_event::do_apply_event set explicit_defaults_for_timestamp if OPTION_explicit_defaults_for_timestamp is set
That should help ensure:
- If you replicate from mariadb to mariadb, the slave will follow the master
- If you replicate from MySQL to MariaDB, you have to have the same setting for MySQL and MariaDB
Not much we can do for the second case, but the first case is the important one
{quote}

Also, Monty suggests to make the variable dynamic in the future.
",8,"Monty suggested the following regarding replication of explicit_defaults_for_timestamp:
{quote}
ok, the easy way to do this would be:
Change SELECT_DISTINCT from 1 to (1ULL << 39)
Add - OPTION_explicit_defaults_for_timestamp as 1
Add  OPTION_explicit_defaults_for_timestamp to OPTIONS_WRITTEN_TO_BIN_LOG
In  Query_log_event::do_apply_event set explicit_defaults_for_timestamp if OPTION_explicit_defaults_for_timestamp is set
That should help ensure:
- If you replicate from mariadb to mariadb, the slave will follow the master
- If you replicate from MySQL to MariaDB, you have to have the same setting for MySQL and MariaDB
Not much we can do for the second case, but the first case is the important one
{quote}

Also, Monty suggests to make the variable dynamic in the future.
"
2221,MDEV-3929,MDEV,Elena Stepanova,75730,2015-09-11 18:22:22,"Please note it is already deprecated in MySQL:
http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_explicit_defaults_for_timestamp
{quote}
explicit_defaults_for_timestamp is itself deprecated because its only purpose is to permit control over now-deprecated TIMESTAMP behaviors that will be removed in a future MySQL release. When that removal occurs, explicit_defaults_for_timestamp will have no purpose and will be removed as well.
{quote}
",9,"Please note it is already deprecated in MySQL:
URL
{quote}
explicit_defaults_for_timestamp is itself deprecated because its only purpose is to permit control over now-deprecated TIMESTAMP behaviors that will be removed in a future MySQL release. When that removal occurs, explicit_defaults_for_timestamp will have no purpose and will be removed as well.
{quote}
"
2222,MDEV-3944,MDEV,Arjen Lentz,31600,2013-04-29 05:18:17,"awesome, very happy to see this scheduled for fixing in 10.0.3",1,"awesome, very happy to see this scheduled for fixing in 10.0.3"
2223,MDEV-3944,MDEV,Oleksandr Byelkin,31770,2013-05-07 12:35:16,"In MariaDB there is no so much work should be done, but still derived tables of view should be processed correctly to build its field translation table and privilages should be read also.",2,"In MariaDB there is no so much work should be done, but still derived tables of view should be processed correctly to build its field translation table and privilages should be read also."
2224,MDEV-3944,MDEV,Emmanuel Galland,35305,2013-10-14 11:45:41,"Hello,
is there any hope for a fix in 10.0.5? (or at least 10.0.6?)
thx",3,"Hello,
is there any hope for a fix in 10.0.5? (or at least 10.0.6?)
thx"
2225,MDEV-3944,MDEV,Emmanuel Galland,70597,2015-05-02 01:29:02,"Hello,
is there anything new about this problematic behaviour ?",4,"Hello,
is there anything new about this problematic behaviour ?"
2226,MDEV-3944,MDEV,Oleksandr Byelkin,70604,2015-05-02 11:04:26,"It is more feature request, so bugs goes first, sorry.",5,"It is more feature request, so bugs goes first, sorry."
2227,MDEV-3944,MDEV,Sergei Golubchik,77760,2015-11-06 10:33:33,5.7 has it now,6,5.7 has it now
2228,MDEV-3944,MDEV,Oleksandr Byelkin,81638,2016-02-29 17:57:42,"revision-id: a6c0c01cec43731df3c58b4fdb68d3ac18e6c699 (mariadb-10.1.8-124-ga6c0c01)
parent(s): 0485328d030f4b742dac7b667e8ed099beb9e9f2
committer: Oleksandr Byelkin
timestamp: 2016-02-29 18:56:45 +0100
message:

MDEV-3944: Allow derived tables in VIEWS

---",7,"revision-id: a6c0c01cec43731df3c58b4fdb68d3ac18e6c699 (mariadb-10.1.8-124-ga6c0c01)
parent(s): 0485328d030f4b742dac7b667e8ed099beb9e9f2
committer: Oleksandr Byelkin
timestamp: 2016-02-29 18:56:45 +0100
message:

MDEV-3944: Allow derived tables in VIEWS

---"
2229,MDEV-3944,MDEV,Elena Stepanova,81639,2016-02-29 18:04:05,Tree: 10.2-MDEV-3944,8,Tree: 10.2-MDEV-3944
2230,MDEV-3944,MDEV,Sergei Petrunia,83677,2016-05-25 08:52:04,Ok to push.,9,Ok to push.
2231,MDEV-427,MDEV,AL13N,13781,2012-08-02 11:07:22,see also http://svnweb.mageia.org/packages/cauldron/mariadb/current/SOURCES/mysqld.service?view=log for mysqld.service script from mageia,1,see also URL for mysqld.service script from mageia
2232,MDEV-427,MDEV,Otto Kekäläinen,59306,2014-08-27 23:16:10,"More links do downstream versions:

OpenSUSE: https://build.opensuse.org/package/view_file/server:database/mariadb/mysql.service?expand=1

Fedora (uses two service files):
http://pkgs.fedoraproject.org/cgit/mariadb.git/tree/mysql.service.in
http://pkgs.fedoraproject.org/cgit/mariadb.git/tree/mysql-compat.service.in

Arch: https://projects.archlinux.org/svntogit/packages.git/tree/trunk/mariadb.service?h=packages/mariadb

These are all different. They all even use different service names: mysql, mysqld, mariadb and mysql+mysql-compat.

I'll do one for Debian once I figure out what the optimal service file should look like.",2,"More links do downstream versions:

OpenSUSE: URL

Fedora (uses two service files):
URL
URL

Arch: URL

These are all different. They all even use different service names: mysql, mysqld, mariadb and mysql+mysql-compat.

I'll do one for Debian once I figure out what the optimal service file should look like."
2233,MDEV-427,MDEV,David Strauss,59307,2014-08-28 00:07:29,It's possible to use one main service name and then configure an alias (in the units) as appropriate for each distro.,3,It's possible to use one main service name and then configure an alias (in the units) as appropriate for each distro.
2234,MDEV-427,MDEV,erkan yanar,59308,2014-08-28 00:17:48,"We should *not* waste time with broken examples.
i.e. the SUSE stuff:

{noformat}

[Service]
Type=forking
ExecStart=/usr/lib/mysql/rcmysql start
ExecStop=/usr/lib/mysql/rcmysql stop

{noformat}

This is broken. You use systemd to start the binary.
systemd takes care
To stop a SIGTERM is sufficient
Using mysqld_safe is also forbidden.
systemd takes care of restarting a service.


",4,"We should *not* waste time with broken examples.
i.e. the SUSE stuff:

{noformat}

[Service]
Type=forking
ExecStart=/usr/lib/mysql/rcmysql start
ExecStop=/usr/lib/mysql/rcmysql stop

{noformat}

This is broken. You use systemd to start the binary.
systemd takes care
To stop a SIGTERM is sufficient
Using mysqld_safe is also forbidden.
systemd takes care of restarting a service.


"
2235,MDEV-427,MDEV,erkan yanar,59314,2014-08-28 03:54:01,"I had an look at the mariadb package from fedora20 and changed it

{noformat}
[Unit]
Description=MariaDB database server
After=syslog.target
After=network.target

[Service]
# It is all default just for documentation
Type=simple
User=mysql
Group=mysql

# Removed that on. Should be done in the package
#ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n

# Changed from mysqld_safe to mysql
ExecStart=/usr/libexec/mysqld

# Makes no real sense to wait for the mainpid we already control it
#ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID

# Just defining the was how to kill mysqld.
KillMode=process
KillSignal=SIGTERM
# Thats me I don't want to see an automated SIGKILL ever
SendSIGKILL=no

# I hate timeouts. Most likely sysadmins use them and revert my SendSIGKILL
#TimeoutSec=300

# We restart mysqld if there is a failure (5s delay)
Restart=on-failure
RestartSec=5s

PrivateTmp=true

[Install]
# So we can use mysql and mariad as service
Alias=mysql.service
WantedBy=multi-user.target
{noformat}",5,"I had an look at the mariadb package from fedora20 and changed it

{noformat}
[Unit]
Description=MariaDB database server
After=syslog.target
After=network.target

[Service]
# It is all default just for documentation
Type=simple
User=mysql
Group=mysql

# Removed that on. Should be done in the package
#ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n

# Changed from mysqld_safe to mysql
ExecStart=/usr/libexec/mysqld

# Makes no real sense to wait for the mainpid we already control it
#ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID

# Just defining the was how to kill mysqld.
KillMode=process
KillSignal=SIGTERM
# Thats me I don't want to see an automated SIGKILL ever
SendSIGKILL=no

# I hate timeouts. Most likely sysadmins use them and revert my SendSIGKILL
#TimeoutSec=300

# We restart mysqld if there is a failure (5s delay)
Restart=on-failure
RestartSec=5s

PrivateTmp=true

[Install]
# So we can use mysql and mariad as service
Alias=mysql.service
WantedBy=multi-user.target
{noformat}"
2236,MDEV-427,MDEV,David Strauss,59321,2014-08-28 18:35:58,"""Makes no real sense to wait for the mainpid we already control it""

I disagree. With a Type=simple service, systemd considers the service fully running as soon as systemd starts running ExecStart. This is problematic if other services depend on MariaDB and need to run after its fully online.",6,"""Makes no real sense to wait for the mainpid we already control it""

I disagree. With a Type=simple service, systemd considers the service fully running as soon as systemd starts running ExecStart. This is problematic if other services depend on MariaDB and need to run after its fully online."
2237,MDEV-427,MDEV,AL13N,59323,2014-08-28 19:50:07,"for mageia, i would like to change this to not use mysqld_safe either...

anyway, i'm open to suggestions...",7,"for mageia, i would like to change this to not use mysqld_safe either...

anyway, i'm open to suggestions..."
2238,MDEV-427,MDEV,Christian McHugh,66054,2014-11-24 22:54:48,"In our environment we have about 1000 mariadb/mysql databases. It takes a few minutes for everything to fully come up and be ready to serve connections. The redhat service file includes a ExecStartPost=/usr/libexec/mysqld-wait-ready $MAINPID bit where that script loops with ""/usr/bin/mysqladmin --no-defaults --socket=""$socketfile"" --user=UNKNOWN_MYSQL_USER ping"" until it can successfully connect (not successfully login, just connect). Something like that will probably be required to ensure the service is really up.",8,"In our environment we have about 1000 mariadb/mysql databases. It takes a few minutes for everything to fully come up and be ready to serve connections. The redhat service file includes a ExecStartPost=/usr/libexec/mysqld-wait-ready $MAINPID bit where that script loops with ""/usr/bin/mysqladmin --no-defaults --socket=""$socketfile"" --user=UNKNOWN_MYSQL_USER ping"" until it can successfully connect (not successfully login, just connect). Something like that will probably be required to ensure the service is really up."
2239,MDEV-427,MDEV,Otto Kekäläinen,66077,2014-11-25 11:00:11,"[~mchugh19] The old init scripts have a section that waits for service to come up (and also to stop at shutdown/restart) which uses the mysqld_status command. See http://anonscm.debian.org/cgit/pkg-mysql/mariadb-10.0.git/tree/debian/mariadb-server-10.0.mysql.init

Others: http://0pointer.net/public/systemd-nluug-2014.pdf presents the systemd security features, which are encouraged (at least in Debian) to be used.",9,"[~mchugh19] The old init scripts have a section that waits for service to come up (and also to stop at shutdown/restart) which uses the mysqld_status command. See URL

Others: URL presents the systemd security features, which are encouraged (at least in Debian) to be used."
2240,MDEV-427,MDEV,erkan yanar,66418,2014-12-05 18:40:29,"Hmm,
Both do the same in the end.
Imho it is embarrassing to have *still* no systemd integration at all.
The whole world takes MariaDB into there distribution. systemd is going to be *the* PID 1 
and MariaDB is not able to get this done.",10,"Hmm,
Both do the same in the end.
Imho it is embarrassing to have *still* no systemd integration at all.
The whole world takes MariaDB into there distribution. systemd is going to be *the* PID 1 
and MariaDB is not able to get this done."
2241,MDEV-427,MDEV,Daniel Black,67063,2015-01-07 12:56:49,Avoided all deficiencies of type={simple|forking} by using Type=Notify in MDEV-5536. Thanks for the base service file [~erkules]. I've also seen a possibility of doing database install like sshd-keygen.service (https://bugzilla.redhat.com/show_bug.cgi?id=1066615#c7).,11,Avoided all deficiencies of type={simple|forking} by using Type=Notify in MDEV-5536. Thanks for the base service file [~erkules]. I've also seen a possibility of doing database install like sshd-keygen.service (URL
2242,MDEV-427,MDEV,erkan yanar,68513,2015-02-26 15:19:56,"Oh notify is indeed a very nice opportunity. Making it not that ""simple"" anymore :)

Making things more complex:
I would like to agree on how/when to initialise datadir.
Like Debian/Ubuntu on installation or on startup.",12,"Oh notify is indeed a very nice opportunity. Making it not that ""simple"" anymore :)

Making things more complex:
I would like to agree on how/when to initialise datadir.
Like Debian/Ubuntu on installation or on startup."
2243,MDEV-427,MDEV,erkan yanar,68514,2015-02-26 15:20:56,Btw: As with 10.1 and integrated Galera we need to take care of --wsrep-recover's job in mysqld_safe also :),13,Btw: As with 10.1 and integrated Galera we need to take care of --wsrep-recover's job in mysqld_safe also :)
2244,MDEV-427,MDEV,Daniel Black,69009,2015-03-11 23:27:29,"initialisations support-files/mariadb-systemd-start make it as late as possible, hopefully my.cnf files are populated by the time a user starts a service.

wsrep-recover -looks like its something that should be moved into mysqld rather than running mysqld, extracting output and running mysqld again.

https://github.com/MariaDB/server/pull/26 included systemd script",14,"initialisations support-files/mariadb-systemd-start make it as late as possible, hopefully my.cnf files are populated by the time a user starts a service.

wsrep-recover -looks like its something that should be moved into mysqld rather than running mysqld, extracting output and running mysqld again.

URL included systemd script"
2245,MDEV-427,MDEV,Nils Meyer,73536,2015-07-16 11:21:44,"This effectively prevents me from using MariaDB Galera in Debian Jessie since it's not possible to bootstrap the cluster without launching mysqld manually with the --wsrep-new-cluster, which will subsequently prevent managing the service through systemd. ",15,"This effectively prevents me from using MariaDB Galera in Debian Jessie since it's not possible to bootstrap the cluster without launching mysqld manually with the --wsrep-new-cluster, which will subsequently prevent managing the service through systemd. "
2246,MDEV-427,MDEV,Daniel Black,73572,2015-07-17 08:31:13,"[~nils.meyer] yeh, perhaps having a type=oneshot or different service for that specific purpose that isn't part of the default. I'll see what I can make up. Quite a few things to consider for that however.

I'll add them on top of the current pull request.

https://github.com/MariaDB/server/pull/83",16,"[~nils.meyer] yeh, perhaps having a type=oneshot or different service for that specific purpose that isn't part of the default. I'll see what I can make up. Quite a few things to consider for that however.

I'll add them on top of the current pull request.

URL"
2247,MDEV-427,MDEV,Daniel Black,73870,2015-07-24 02:57:48,[~nils.meyer] - MDEV-7752 - focuses on Centos7 however something might be applicable for you.,17,[~nils.meyer] - MDEV-7752 - focuses on Centos7 however something might be applicable for you.
2248,MDEV-427,MDEV,Julien Pivotto,73976,2015-07-28 16:02:08,would it be possible to ship that for 10.0 as well?,18,would it be possible to ship that for 10.0 as well?
2249,MDEV-427,MDEV,Sergei Golubchik,73981,2015-07-28 17:00:33,"We'd love to. But as it turned out, users don't like when we add new dependencies to our packages in a GA branch (yes, we've tried). And in these patches MariaDB is linked with {{libsystemd-daemon.so}}, so we'll have to add it as a dependency. We cannot do that. I see few options for 10.0:
- add the patch nevertheless, but build our packages without systemd. Users would need to compile MariaDB to get systemd support.
- bundle libsystemd-daemon sources with 10.0
- only implement a minimal systemd support in 10.0 that doesn't require changes in the server

What do you think we should do?

Either way, I think we'll add systemd support in 10.1 first and then, possibly, backport parts of it to 10.0
",19,"We'd love to. But as it turned out, users don't like when we add new dependencies to our packages in a GA branch (yes, we've tried). And in these patches MariaDB is linked with {{libsystemd-daemon.so}}, so we'll have to add it as a dependency. We cannot do that. I see few options for 10.0:
- add the patch nevertheless, but build our packages without systemd. Users would need to compile MariaDB to get systemd support.
- bundle libsystemd-daemon sources with 10.0
- only implement a minimal systemd support in 10.0 that doesn't require changes in the server

What do you think we should do?

Either way, I think we'll add systemd support in 10.1 first and then, possibly, backport parts of it to 10.0
"
2250,MDEV-427,MDEV,Guillaume Lefranc,73984,2015-07-28 17:06:16,"Sergei,

I would suggest the third choice, aka minimal systemd support. This is the less intrusive, and to be honest, most people just need minimal support.
Let's keep the systemd socket activation for 10.1 as it introduces major changes.

Regards,
Guillaume",20,"Sergei,

I would suggest the third choice, aka minimal systemd support. This is the less intrusive, and to be honest, most people just need minimal support.
Let's keep the systemd socket activation for 10.1 as it introduces major changes.

Regards,
Guillaume"
2251,MDEV-427,MDEV,Otto Kekäläinen,73987,2015-07-28 17:09:40,"I agree with [~serg] about being conservative in GA branches. Add well documented systemd scripts to 10.1 and downstream packagers can then choose themselves if they want to take the risk of backporting it somewhere, e.g. to new *OS/Linux distro* releases.",21,"I agree with [~serg] about being conservative in GA branches. Add well documented systemd scripts to 10.1 and downstream packagers can then choose themselves if they want to take the risk of backporting it somewhere, e.g. to new *OS/Linux distro* releases."
2252,MDEV-4537,MDEV,Jean Weisbuch,31948,2013-05-16 13:39:37,"Knowing that there is a bug on the 1.7.1 actually packaged (but not on the upstream latest version), it will crash if you press ""f"" to activate the ""Show a thread's full query"" option whith that error :
    Use of uninitialized value in hash element at ./innotop line 8361.",1,"Knowing that there is a bug on the 1.7.1 actually packaged (but not on the upstream latest version), it will crash if you press ""f"" to activate the ""Show a thread's full query"" option whith that error :
    Use of uninitialized value in hash element at ./innotop line 8361."
2253,MDEV-4537,MDEV,Colin Charles,31950,2013-05-16 14:16:49,"Hmm, how odd. I wonder why we ship innotop in debian/additions/innotop - thanks for the report",2,"Hmm, how odd. I wonder why we ship innotop in debian/additions/innotop - thanks for the report"
2254,MDEV-4537,MDEV,Sergei Golubchik,46712,2014-04-10 12:27:24,I'll fix it in 10.1 in my changes for debian packaging,3,I'll fix it in 10.1 in my changes for debian packaging
2255,MDEV-4537,MDEV,Otto Kekäläinen,73777,2015-07-21 16:53:44,"While at it, please move the innotop scripts out of the debian/ directory as they are more like a part of the main project and not a part of packaging.

Downstream Debian and Ubuntu completely replace debian/* contents so any innotop bugfixes there might never come to downstream, as downstream maintain their own packaging.",4,"While at it, please move the innotop scripts out of the debian/ directory as they are more like a part of the main project and not a part of packaging.

Downstream Debian and Ubuntu completely replace debian/* contents so any innotop bugfixes there might never come to downstream, as downstream maintain their own packaging."
2256,MDEV-4537,MDEV,Otto Kekäläinen,88720,2016-11-25 22:27:52,[~jb-boin] Do you plan on working on this? What might be the up-to-date status? I am happy to review pull requests on this topic.,5,[~jb-boin] Do you plan on working on this? What might be the up-to-date status? I am happy to review pull requests on this topic.
2257,MDEV-4537,MDEV,Jean Weisbuch,88728,2016-11-25 23:00:58,"What is the plan here : To remove innotop from the packages or to maintain it on the packages?

As far as i remember, there were nothing changed on the version shipped with MariaDB.",6,"What is the plan here : To remove innotop from the packages or to maintain it on the packages?

As far as i remember, there were nothing changed on the version shipped with MariaDB."
2258,MDEV-4537,MDEV,Otto Kekäläinen,88745,2016-11-26 00:13:59,"I don't know what the plan should be. I've personally never used innotop. Probably easiest to keep innotop as is, but update it (if needed).",7,"I don't know what the plan should be. I've personally never used innotop. Probably easiest to keep innotop as is, but update it (if needed)."
2259,MDEV-4537,MDEV,Jean Weisbuch,90958,2017-01-23 16:33:07,PR created on the 10.1 tree with the latest innotop version (i also updated the man file and the changelog) : https://github.com/MariaDB/server/pull/295,8,PR created on the 10.1 tree with the latest innotop version (i also updated the man file and the changelog) : URL
2260,MDEV-4608,MDEV,Sergei Golubchik,32294,2013-06-02 22:53:37,"[~colin], do you think we should start building packages for debian jessie?
When?

If yes — please, reassign to [~dbart] and set the priority accordingly.

Thanks!",1,"[~colin], do you think we should start building packages for debian jessie?
When?

If yes — please, reassign to [~dbart] and set the priority accordingly.

Thanks!"
2261,MDEV-4608,MDEV,Colin Charles,32299,2013-06-03 05:57:43,"Since we just started Debian 7 (wheezy) builds, and that was made stable May 4 2013, we may want to also make binaries for Debian testing (jessie). 

Pros? 
+ We will be building against the next release of Debian (if on a 2 year release cycle, we will be ready for 2015)
+ Pending inclusion into Debian, we will have testing repositories available for users as well 

Cons?
- Do we build for Debian unstable as well? (I don't think so)
- How many people are using Debian/testing (jessie) today?

As we have to start several VMs, and add to buildbot, it will take even longer for builds to come out. My only concern is user numbers of Debian/testing (jessie) now.

I hopped on over to Percona to see what they provide (http://www.percona.com/doc/percona-server/5.5/installation/apt_repo.html). It seems like its only current stable and obsolete stable releases that are offered.

I do not view this as high priority, but if there is a compelling argument to have Debian/testing builds available, we should definitely do it.",2,"Since we just started Debian 7 (wheezy) builds, and that was made stable May 4 2013, we may want to also make binaries for Debian testing (jessie). 

Pros? 
+ We will be building against the next release of Debian (if on a 2 year release cycle, we will be ready for 2015)
+ Pending inclusion into Debian, we will have testing repositories available for users as well 

Cons?
- Do we build for Debian unstable as well? (I don't think so)
- How many people are using Debian/testing (jessie) today?

As we have to start several VMs, and add to buildbot, it will take even longer for builds to come out. My only concern is user numbers of Debian/testing (jessie) now.

I hopped on over to Percona to see what they provide (URL It seems like its only current stable and obsolete stable releases that are offered.

I do not view this as high priority, but if there is a compelling argument to have Debian/testing builds available, we should definitely do it."
2262,MDEV-4608,MDEV,Timo R.,63715,2014-10-28 10:23:09,"Jessie is close to a release now, also, there are builds for sid.
Seems like this was just forgotten?",3,"Jessie is close to a release now, also, there are builds for sid.
Seems like this was just forgotten?"
2263,MDEV-4608,MDEV,Daniel Bartholomew,63801,2014-10-28 14:38:52,I will have builders for Jessie created very soon.,4,I will have builders for Jessie created very soon.
2264,MDEV-4608,MDEV,Timo R.,69473,2015-03-26 11:11:48,There still doesn't seem to be a repository for jessie.,5,There still doesn't seem to be a repository for jessie.
2265,MDEV-4608,MDEV,Sergei Golubchik,72061,2015-06-12 14:24:10,Jessie packages are now built ok,6,Jessie packages are now built ok
2266,MDEV-4646,MDEV,Sergei Petrunia,78728,2015-12-03 22:58:30,"Confirm. I've hit this while working on MDEV-9233.   There, we've got a core file. It's possible to load it, but debugging information is not available.

RPM-based repositories actually have a special ""debuginfo"" packages. When loading a core into gdb, it says:
{noformat}
Missing separate debuginfos, use: debuginfo-install MariaDB-server-10.0.22-1.el6.x86_64
{noformat}

The process to install debuginfo packages is as follows:
* Edit  /etc/yum.repos.d/CentOS-Debuginfo.repo (or what you have) and set {{enabled=1}} there.
* {{yum install yum-utils}}  - this is where {{debuginfo-install}} is.
* debuginfo-install $package_name

This gives:
 
{noformat}
# debuginfo-install MariaDB-server-10.0.22-1.el6.x86_64
Loaded plugins: fastestmirror, presto
Loading mirror speeds from cached hostfile
 * base: repos.dfw.quadranet.com
 * extras: mirror.thelinuxfix.com
 * updates: centos.chi.host-engine.com
Could not find debuginfo for main pkg: MariaDB-server-10.0.22-1.el6.x86_64
Could not find debuginfo pkg for dependency package libaio-0.3.107-10.el6.x86_64
Could not find debuginfo pkg for dependency package libaio-0.3.107-10.el6.x86_64
Could not find debuginfo pkg for dependency package libaio-0.3.107-10.el6.x86_64
Could not find debuginfo pkg for dependency package MariaDB-server-10.0.22-1.el6.x86_64
--> Running transaction check
---> Package gcc-debuginfo.x86_64 0:4.4.7-4.el6 will be installed
---> Package glibc-debuginfo.x86_64 0:2.12-1.132.el6 will be installed
--> Processing Dependency: glibc-debuginfo-common = 2.12-1.132.el6 for package: glibc-debuginfo-2.12-1.132.el6.x86_64
---> Package openssl-debuginfo.x86_64 0:1.0.1e-15.el6 will be installed
---> Package pam-debuginfo.x86_64 0:1.1.1-17.el6 will be installed
...
{noformat}

The key point is:
{noformat}
Could not find debuginfo for main pkg: MariaDB-server-10.0.22-1.el6.x86_64
{noformat}
I am looking at the repository as I got it configured by tool at mariadb.org and I don't see debuginfo packages there.
",1,"Confirm. I've hit this while working on MDEV-9233.   There, we've got a core file. It's possible to load it, but debugging information is not available.

RPM-based repositories actually have a special ""debuginfo"" packages. When loading a core into gdb, it says:
{noformat}
Missing separate debuginfos, use: debuginfo-install MariaDB-server-10.0.22-1.el6.x86_64
{noformat}

The process to install debuginfo packages is as follows:
* Edit  /etc/yum.repos.d/CentOS-Debuginfo.repo (or what you have) and set {{enabled=1}} there.
* {{yum install yum-utils}}  - this is where {{debuginfo-install}} is.
* debuginfo-install $package_name

This gives:
 
{noformat}
# debuginfo-install MariaDB-server-10.0.22-1.el6.x86_64
Loaded plugins: fastestmirror, presto
Loading mirror speeds from cached hostfile
 * base: repos.dfw.quadranet.com
 * extras: mirror.thelinuxfix.com
 * updates: centos.chi.host-engine.com
Could not find debuginfo for main pkg: MariaDB-server-10.0.22-1.el6.x86_64
Could not find debuginfo pkg for dependency package libaio-0.3.107-10.el6.x86_64
Could not find debuginfo pkg for dependency package libaio-0.3.107-10.el6.x86_64
Could not find debuginfo pkg for dependency package libaio-0.3.107-10.el6.x86_64
Could not find debuginfo pkg for dependency package MariaDB-server-10.0.22-1.el6.x86_64
--> Running transaction check
---> Package gcc-debuginfo.x86_64 0:4.4.7-4.el6 will be installed
---> Package glibc-debuginfo.x86_64 0:2.12-1.132.el6 will be installed
--> Processing Dependency: glibc-debuginfo-common = 2.12-1.132.el6 for package: glibc-debuginfo-2.12-1.132.el6.x86_64
---> Package openssl-debuginfo.x86_64 0:1.0.1e-15.el6 will be installed
---> Package pam-debuginfo.x86_64 0:1.1.1-17.el6 will be installed
...
{noformat}

The key point is:
{noformat}
Could not find debuginfo for main pkg: MariaDB-server-10.0.22-1.el6.x86_64
{noformat}
I am looking at the repository as I got it configured by tool at mariadb.org and I don't see debuginfo packages there.
"
2267,MDEV-4646,MDEV,Sergei Petrunia,78729,2015-12-03 23:00:51,"Btw, CentOS themselves provide debuginfo packages: Looking at http://debuginfo.centos.org/6/x86_64/  I see:
{noformat}
￼	mariadb55-mariadb-debuginfo-5.5.32-1.el6.centos.alt.x86_64.rpm	21-Dec-2013 13:31	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.35-1.1.el6.centos.alt.x86_64.rpm	26-Feb-2014 11:10	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.37-1.3.el6.centos.alt.x86_64.rpm	21-May-2014 17:23	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.37-9.el6.centos.alt.x86_64.rpm	25-Aug-2014 15:52	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.40-10.el6.centos.alt.x86_64.rpm	19-Nov-2014 15:08	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.41-12.el6.centos.alt.x86_64.rpm	06-Feb-2015 15:33	38M	 
{noformat}

which is good. Now, we need the same for packages from MariaDB.org.  For 10.0 and 10.1.

EDIT: Fedora hs them too:
http://dl.fedoraproject.org/pub/fedora/linux/development/rawhide/x86_64/debug/m/ has:
{noformat}
￼ mariadb-connector-c-debuginfo-2.1.0-1.fc24.x86_64.rpm                          2015-08-16 18:23  464K  
￼ mariadb-debuginfo-10.1.8-3.fc24.x86_64.rpm                                     2015-12-08 02:46   88M  
￼ mariadb-galera-debuginfo-10.0.17-5.fc23.x86_64.rpm                             2015-08-16 18:24   37M  
{noformat}
",2,"Btw, CentOS themselves provide debuginfo packages: Looking at URL  I see:
{noformat}
￼	mariadb55-mariadb-debuginfo-5.5.32-1.el6.centos.alt.x86_64.rpm	21-Dec-2013 13:31	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.35-1.1.el6.centos.alt.x86_64.rpm	26-Feb-2014 11:10	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.37-1.3.el6.centos.alt.x86_64.rpm	21-May-2014 17:23	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.37-9.el6.centos.alt.x86_64.rpm	25-Aug-2014 15:52	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.40-10.el6.centos.alt.x86_64.rpm	19-Nov-2014 15:08	38M	 
￼	mariadb55-mariadb-debuginfo-5.5.41-12.el6.centos.alt.x86_64.rpm	06-Feb-2015 15:33	38M	 
{noformat}

which is good. Now, we need the same for packages from MariaDB.org.  For 10.0 and 10.1.

EDIT: Fedora hs them too:
URL has:
{noformat}
￼ mariadb-connector-c-debuginfo-2.1.0-1.fc24.x86_64.rpm                          2015-08-16 18:23  464K  
￼ mariadb-debuginfo-10.1.8-3.fc24.x86_64.rpm                                     2015-12-08 02:46   88M  
￼ mariadb-galera-debuginfo-10.0.17-5.fc23.x86_64.rpm                             2015-08-16 18:24   37M  
{noformat}
"
2268,MDEV-4646,MDEV,Sergei Petrunia,78730,2015-12-03 23:08:59,"{noformat}
(gdb)  file /usr/sbin/mysqld
Reading symbols from /usr/sbin/mysqld...(no debugging symbols found)...done.

## OK

(gdb) core /path/to/core
...
Reading symbols from /lib64/libaio.so.1...(no debugging symbols found)...done.
Loaded symbols for /lib64/libaio.so.1
Reading symbols from /lib64/libz.so.1.2.3...Reading symbols from /usr/lib/debug/lib64/libz.so.1.2.3.debug...done.
done.
Loaded symbols for /lib64/libz.so.1.2.3
Reading symbols from /lib64/librt.so.1...Reading symbols from /usr/lib/debug/lib64/librt-2.12.so.debug...done.
done.
Loaded symbols for /lib64/librt.so.1
## Some libraries have debuginfo, some dont.
...

Reading symbols from /usr/lib64/mysql/plugin/ha_tokudb.so...done.
Loaded symbols for /usr/lib64/mysql/plugin/ha_tokudb.so

## But ha_tokudb.so in our packages DOES HAVE debug info! It's not in a
## separate package, though
{noformat}

Now let's see. traces in TokuDB show line numbers:

{noformat}
Core was generated by `/usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib6'.
...
Thread 152 (Thread 0x7f00511d8700 (LWP 112507)):
#0  0x000000332d00b63c in pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:245
#1  0x00007f00708ca7f3 in toku_cond_wait (kidv=Unhandled dwarf expression opcode 0xf3
)
    at /home/buildbot/buildbot/build/mariadb-10.0.22/storage/tokudb/ft-index/portability/toku_pthread.h:309
#2  kwait (kidv=Unhandled dwarf expression opcode 0xf3
) at /home/buildbot/buildbot/build/mariadb-10.0.22/storage/tokudb/ft-index/util/kibbutz.cc:157
#3  work_on_kibbutz (kidv=Unhandled dwarf expression opcode 0xf3
) at /home/buildbot/buildbot/build/mariadb-10.0.22/storage/tokudb/ft-index/util/kibbutz.cc:196
#4  0x000000332d007a51 in __nptl_deallocate_tsd (arg=0x7f00511d8700) at pthread_create.c:148
#5  start_thread (arg=0x7f00511d8700) at pthread_create.c:308
#6  0x000000332cce893d in ?? () from /lib64/libc.so.6
{noformat}

The rest of the server shows function names but not line numbers:

{noformat}
Thread 52 (Thread 0x7f00917f9700 (LWP 112432)):
#0  0x0000003ef1e00614 in ?? () from /lib64/libaio.so.1
#1  0x00000000008d6027 in os_aio_linux_handle(unsigned long, fil_node_t**, void**, unsigned long*, unsigned long*) ()
#2  0x00000000009f7915 in fil_aio_wait(unsigned long) ()
#3  0x00000000009443f5 in io_handler_thread ()
#4  0x000000332d007a51 in __nptl_deallocate_tsd (arg=0x7f00917f9700) at pthread_create.c:148
---Type <return> to continue, or q <return> to quit---
#5  start_thread (arg=0x7f00917f9700) at pthread_create.c:308
#6  0x000000332cce893d in ?? () from /lib64/libc.so.6
#7  0x0000000000000000 in ?? ()
{noformat}
",3,"{noformat}
(gdb)  file /usr/sbin/mysqld
Reading symbols from /usr/sbin/mysqld...(no debugging symbols found)...done.

## OK

(gdb) core /path/to/core
...
Reading symbols from /lib64/libaio.so.1...(no debugging symbols found)...done.
Loaded symbols for /lib64/libaio.so.1
Reading symbols from /lib64/libz.so.1.2.3...Reading symbols from /usr/lib/debug/lib64/libz.so.1.2.3.debug...done.
done.
Loaded symbols for /lib64/libz.so.1.2.3
Reading symbols from /lib64/librt.so.1...Reading symbols from /usr/lib/debug/lib64/librt-2.12.so.debug...done.
done.
Loaded symbols for /lib64/librt.so.1
## Some libraries have debuginfo, some dont.
...

Reading symbols from /usr/lib64/mysql/plugin/ha_tokudb.so...done.
Loaded symbols for /usr/lib64/mysql/plugin/ha_tokudb.so

## But ha_tokudb.so in our packages DOES HAVE debug info! It's not in a
## separate package, though
{noformat}

Now let's see. traces in TokuDB show line numbers:

{noformat}
Core was generated by `/usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib6'.
...
Thread 152 (Thread 0x7f00511d8700 (LWP 112507)):
#0  0x000000332d00b63c in pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:245
#1  0x00007f00708ca7f3 in toku_cond_wait (kidv=Unhandled dwarf expression opcode 0xf3
)
    at /home/buildbot/buildbot/build/mariadb-10.0.22/storage/tokudb/ft-index/portability/toku_pthread.h:309
#2  kwait (kidv=Unhandled dwarf expression opcode 0xf3
) at /home/buildbot/buildbot/build/mariadb-10.0.22/storage/tokudb/ft-index/util/kibbutz.cc:157
#3  work_on_kibbutz (kidv=Unhandled dwarf expression opcode 0xf3
) at /home/buildbot/buildbot/build/mariadb-10.0.22/storage/tokudb/ft-index/util/kibbutz.cc:196
#4  0x000000332d007a51 in __nptl_deallocate_tsd (arg=0x7f00511d8700) at pthread_create.c:148
#5  start_thread (arg=0x7f00511d8700) at pthread_create.c:308
#6  0x000000332cce893d in ?? () from /lib64/libc.so.6
{noformat}

The rest of the server shows function names but not line numbers:

{noformat}
Thread 52 (Thread 0x7f00917f9700 (LWP 112432)):
#0  0x0000003ef1e00614 in ?? () from /lib64/libaio.so.1
#1  0x00000000008d6027 in os_aio_linux_handle(unsigned long, fil_node_t**, void**, unsigned long*, unsigned long*) ()
#2  0x00000000009f7915 in fil_aio_wait(unsigned long) ()
#3  0x00000000009443f5 in io_handler_thread ()
#4  0x000000332d007a51 in __nptl_deallocate_tsd (arg=0x7f00917f9700) at pthread_create.c:148
---Type  to continue, or q  to quit---
#5  start_thread (arg=0x7f00917f9700) at pthread_create.c:308
#6  0x000000332cce893d in ?? () from /lib64/libc.so.6
#7  0x0000000000000000 in ?? ()
{noformat}
"
2269,MDEV-4646,MDEV,Sergei Petrunia,78731,2015-12-03 23:20:29,"I'll re-state the obvious just in case. This is not about stripped vs not-stripped binaries. All binaries are not stripped (and it's ok):
{noformat}
[root@ip-10-204-137-38 ~]# file /usr/lib64/mysql/plugin/ha_tokudb.so
/usr/lib64/mysql/plugin/ha_tokudb.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, not stripped
[root@ip-10-204-137-38 ~]# file /usr/sbin/mysqld
/usr/sbin/mysqld: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, not stripped
{noformat}

However, ha_toku has debug info:
{noformat}
[root@ip-10-204-137-38 ~]# nm -a /usr/lib64/mysql/plugin/ha_tokudb.so | grep '\.debug'
0000000000000000 N .debug_abbrev
0000000000000000 N .debug_aranges
0000000000000000 N .debug_info
0000000000000000 N .debug_line
0000000000000000 N .debug_loc
0000000000000000 N .debug_ranges
0000000000000000 N .debug_str
{noformat}

while mysqld hasn't:

{noformat}
[root@ip-10-204-137-38 ~]# nm -a /usr/sbin/mysqld | grep  '\.debug'
[root@ip-10-204-137-38 ~]# 
{noformat}
",4,"I'll re-state the obvious just in case. This is not about stripped vs not-stripped binaries. All binaries are not stripped (and it's ok):
{noformat}
[root@ip-10-204-137-38 ~]# file /usr/lib64/mysql/plugin/ha_tokudb.so
/usr/lib64/mysql/plugin/ha_tokudb.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, not stripped
[root@ip-10-204-137-38 ~]# file /usr/sbin/mysqld
/usr/sbin/mysqld: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, not stripped
{noformat}

However, ha_toku has debug info:
{noformat}
[root@ip-10-204-137-38 ~]# nm -a /usr/lib64/mysql/plugin/ha_tokudb.so | grep '\.debug'
0000000000000000 N .debug_abbrev
0000000000000000 N .debug_aranges
0000000000000000 N .debug_info
0000000000000000 N .debug_line
0000000000000000 N .debug_loc
0000000000000000 N .debug_ranges
0000000000000000 N .debug_str
{noformat}

while mysqld hasn't:

{noformat}
[root@ip-10-204-137-38 ~]# nm -a /usr/sbin/mysqld | grep  '\.debug'
[root@ip-10-204-137-38 ~]# 
{noformat}
"
2270,MDEV-4646,MDEV,Daniel Black,92408,2017-03-01 11:00:59,"Since CMake-3,7 - added to build infrastructure in MDEV-11258 has the added feature in CPackRPM that allows the creation of Debug info

https://cmake.org/cmake/help/v3.7/module/CPackRPM.html#variable:CPACK_RPM_DEBUGINFO_PACKAGE

Even with a text paste of a crash dump at least the addresses can be mapped to code without reporter intervention with the unpacked RPM and debug symbols:
{code:shell}
for pkg in  MariaDB-server-* ; do echo $pkg; rpm2cpio ${pkg} | (cd /tmp/m55;  cpio -idmv  ) ; done
gdb /tmp/m55/usr/sbin/mysqld
(gdb)  set debug-file-directory /tmp/m55//usr/lib/debug
(gdb) path /tmp/m55
(gdb) symbol-file  /tmp/m55//usr/sbin/mysqld
Reading symbols from /tmp/m55//usr/sbin/mysqld...Reading symbols from /tmp/m55/usr/lib/debug/usr/sbin/mysqld.debug...done.
done.
(gdb)   disassemble /m mysql_load 
Dump of assembler code for function mysql_load(THD*, sql_exchange*, TABLE_LIST*, List<Item>&, List<Item>&, List<Item>&, enum_duplicates, bool, bool):
110         ::end_io_cache(&cache);
   0x000000000077ae83 <+3443>:  mov    0x48(%rsp),%rax
   0x000000000077ae88 <+3448>:  lea    0x78(%rax),%rdi
   0x000000000077ae8c <+3452>:  callq  0x9a8220 <end_io_cache>
   0x000000000077b080 <+3952>:  mov    0x48(%rsp),%rax
   0x000000000077b085 <+3957>:  lea    0x78(%rax),%rdi
   0x000000000077b089 <+3961>:  callq  0x9a8220 <end_io_cache>

111         need_end_io_cache = 0;
   0x000000000077ae99 <+3465>:  movb   $0x0,0x203(%rsp)
   0x000000000077b096 <+3974>:  movb   $0x0,0x203(%rsp)

112       }
113       my_off_t file_length() { return cache.end_of_file; }
114       my_off_t position()    { return my_b_tell(&cache); }
   0x000000000077adac <+3228>:  mov    0x258(%rsp),%rax
   0x000000000077adba <+3242>:  mov    (%rax),%rsi
{code}",5,"Since CMake-3,7 - added to build infrastructure in MDEV-11258 has the added feature in CPackRPM that allows the creation of Debug info

URL

Even with a text paste of a crash dump at least the addresses can be mapped to code without reporter intervention with the unpacked RPM and debug symbols:
{code:shell}
for pkg in  MariaDB-server-* ; do echo $pkg; rpm2cpio ${pkg} | (cd /tmp/m55;  cpio -idmv  ) ; done
gdb /tmp/m55/usr/sbin/mysqld
(gdb)  set debug-file-directory /tmp/m55//usr/lib/debug
(gdb) path /tmp/m55
(gdb) symbol-file  /tmp/m55//usr/sbin/mysqld
Reading symbols from /tmp/m55//usr/sbin/mysqld...Reading symbols from /tmp/m55/usr/lib/debug/usr/sbin/mysqld.debug...done.
done.
(gdb)   disassemble /m mysql_load 
Dump of assembler code for function mysql_load(THD*, sql_exchange*, TABLE_LIST*, List&, List&, List&, enum_duplicates, bool, bool):
110         ::end_io_cache(&cache);
   0x000000000077ae83 :  mov    0x48(%rsp),%rax
   0x000000000077ae88 :  lea    0x78(%rax),%rdi
   0x000000000077ae8c :  callq  0x9a8220 
   0x000000000077b080 :  mov    0x48(%rsp),%rax
   0x000000000077b085 :  lea    0x78(%rax),%rdi
   0x000000000077b089 :  callq  0x9a8220 

111         need_end_io_cache = 0;
   0x000000000077ae99 :  movb   $0x0,0x203(%rsp)
   0x000000000077b096 :  movb   $0x0,0x203(%rsp)

112       }
113       my_off_t file_length() { return cache.end_of_file; }
114       my_off_t position()    { return my_b_tell(&cache); }
   0x000000000077adac :  mov    0x258(%rsp),%rax
   0x000000000077adba :  mov    (%rax),%rsi
{code}"
2271,MDEV-4646,MDEV,Sergei Golubchik,92412,2017-03-01 11:29:34,"Thanks, I'll check it out!",6,"Thanks, I'll check it out!"
2272,MDEV-4646,MDEV,Daniel Black,92893,2017-03-11 12:51:48,"With pull request and cmake-3.8.git-latest (required merged cmake [commit|https://gitlab.kitware.com/cmake/cmake/commit/5606622e61e7f6eba74326510cea8dd0128ab2aa] and [cmake merge request 620|https://gitlab.kitware.com/cmake/cmake/merge_requests/620] (not merged yet)):

{noformat}
 cmake /usr/src/mariadb-hopefully-this-is-a-really-really-long-path-big-enough-for-cmake-debug-package-builds/ -DRPM=fedora && make -j 10 package
...
ls -la *rpm
-rw-rw-r-- 1 dan dan  4578574 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-client.rpm
-rw-rw-r-- 1 dan dan    27170 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-common.rpm
-rw-rw-r-- 1 dan dan  1391578 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-devel.rpm
-rw-rw-r-- 1 dan dan 14907622 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-server.rpm
-rw-rw-r-- 1 dan dan  1038850 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-shared.rpm
-rw-rw-r-- 1 dan dan 20348250 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-test.rpm
-rw-rw-r-- 1 dan dan  3686154 Mar 24 21:52 MariaDB-client-debuginfo-5.5.55-1.fc25.x86_64.rpm
-rw-rw-r-- 1 dan dan 30874194 Mar 24 21:52 MariaDB-server-debuginfo-5.5.55-1.fc25.x86_64.rpm
-rw-rw-r-- 1 dan dan  2578694 Mar 24 21:52 MariaDB-test-debuginfo-5.5.55-1.fc25.x86_64.rpm
{noformat}

The package name isn't consistent however [CMake issue 16715|https://gitlab.kitware.com/cmake/cmake/issues/16715] refers it as a possible future release.

debug rpm contents:
{noformat}
 rm -rf /tmp/xx ; mkdir /tmp/xx ;for pkg in *rpm ; do echo PACKAGE: $pkg; rpm2cpio $pkg| (cd /tmp/xx;  cpio -idmv ) ; done2>&1  | tee /tmp/list-new.txt
...
PACKAGE: MariaDB-server-debuginfo-5.5.55-1.fc25.x86_64.rpm
./usr/lib/debug
./usr/lib/debug/.build-id
./usr/lib/debug/.build-id/01
./usr/lib/debug/.build-id/01/d91b7bdf07181be9147b18c5d62c4240f37e25
./usr/lib/debug/.build-id/01/d91b7bdf07181be9147b18c5d62c4240f37e25.debug
...
./usr/lib/debug/.build-id/fa/16afe4e85267197ca4b2d64d29ea1103f6733c.debug
./usr/lib/debug/.dwz
./usr/lib/debug/.dwz/MariaDB-server-5.5.55-1.fc25.x86_64
./usr/lib/debug/usr
./usr/lib/debug/usr/bin
./usr/lib/debug/usr/bin/aria_chk.debug
./usr/lib/debug/usr/bin/aria_dump_log.debug
./usr/lib/debug/usr/bin/aria_ftdump.debug
./usr/lib/debug/usr/bin/aria_pack.debug
.
...
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/client
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/client/client_priv.h
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/client/mysql_upgrade.c
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/extra
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/extra/innochecksum.c
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/extra/my_print_defaults.c
...
cd /tmp/xx
$ gdb usr/sbin/mysqld
..
Try: dnf --enablerepo='*debug*' install /usr/lib/debug/.build-id/3d/8222b0b73026809ab69161ec7459eba87cdd24.debug
Reading symbols from /tmp/xx/usr/sbin/mysqld...(no debugging symbols found)...done.
(no debugging symbols found)...done.
(gdb) file usr/lib/debug/.build-id/3d/8222b0b73026809ab69161ec7459eba87cdd24.debug
Reading symbols from usr/lib/debug/.build-id/3d/8222b0b73026809ab69161ec7459eba87cdd24.debug...done.
(gdb) list sql_kill
6922        only_kill_query     Should it kill the query or the connection
6923    */
6924
6925    static
6926    void sql_kill(THD *thd, ulong id, killed_state state)
6927    {
6928      uint error;
6929      if (!(error= kill_one_thread(thd, id, state)))
6930      {
6931        if ((!thd->killed))
{noformat}

So to make it work:
# requires latest cmake - make package generates tar.gz and untar that to /usr/local and set PATH=/usr/local/cmake-..../bin:$PATH
# Also requires the source/build directories to be very log. This is because the debug info path is edited in-place in the file during the build and is replaces with ./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/sql/..... - so the source/build path needs to be longer than this.",7,"With pull request and cmake-3.8.git-latest (required merged cmake [commit|URL and [cmake merge request 620|URL (not merged yet)):

{noformat}
 cmake /usr/src/mariadb-hopefully-this-is-a-really-really-long-path-big-enough-for-cmake-debug-package-builds/ -DRPM=fedora && make -j 10 package
...
ls -la *rpm
-rw-rw-r-- 1 dan dan  4578574 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-client.rpm
-rw-rw-r-- 1 dan dan    27170 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-common.rpm
-rw-rw-r-- 1 dan dan  1391578 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-devel.rpm
-rw-rw-r-- 1 dan dan 14907622 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-server.rpm
-rw-rw-r-- 1 dan dan  1038850 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-shared.rpm
-rw-rw-r-- 1 dan dan 20348250 Mar 24 21:52 MariaDB-5.5.55-fedora-x86_64-test.rpm
-rw-rw-r-- 1 dan dan  3686154 Mar 24 21:52 MariaDB-client-debuginfo-5.5.55-1.fc25.x86_64.rpm
-rw-rw-r-- 1 dan dan 30874194 Mar 24 21:52 MariaDB-server-debuginfo-5.5.55-1.fc25.x86_64.rpm
-rw-rw-r-- 1 dan dan  2578694 Mar 24 21:52 MariaDB-test-debuginfo-5.5.55-1.fc25.x86_64.rpm
{noformat}

The package name isn't consistent however [CMake issue 16715|URL refers it as a possible future release.

debug rpm contents:
{noformat}
 rm -rf /tmp/xx ; mkdir /tmp/xx ;for pkg in *rpm ; do echo PACKAGE: $pkg; rpm2cpio $pkg| (cd /tmp/xx;  cpio -idmv ) ; done2>&1  | tee /tmp/list-new.txt
...
PACKAGE: MariaDB-server-debuginfo-5.5.55-1.fc25.x86_64.rpm
./usr/lib/debug
./usr/lib/debug/.build-id
./usr/lib/debug/.build-id/01
./usr/lib/debug/.build-id/01/d91b7bdf07181be9147b18c5d62c4240f37e25
./usr/lib/debug/.build-id/01/d91b7bdf07181be9147b18c5d62c4240f37e25.debug
...
./usr/lib/debug/.build-id/fa/16afe4e85267197ca4b2d64d29ea1103f6733c.debug
./usr/lib/debug/.dwz
./usr/lib/debug/.dwz/MariaDB-server-5.5.55-1.fc25.x86_64
./usr/lib/debug/usr
./usr/lib/debug/usr/bin
./usr/lib/debug/usr/bin/aria_chk.debug
./usr/lib/debug/usr/bin/aria_dump_log.debug
./usr/lib/debug/usr/bin/aria_ftdump.debug
./usr/lib/debug/usr/bin/aria_pack.debug
.
...
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/client
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/client/client_priv.h
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/client/mysql_upgrade.c
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/extra
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/extra/innochecksum.c
./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/extra/my_print_defaults.c
...
cd /tmp/xx
$ gdb usr/sbin/mysqld
..
Try: dnf --enablerepo='*debug*' install /usr/lib/debug/.build-id/3d/8222b0b73026809ab69161ec7459eba87cdd24.debug
Reading symbols from /tmp/xx/usr/sbin/mysqld...(no debugging symbols found)...done.
(no debugging symbols found)...done.
(gdb) file usr/lib/debug/.build-id/3d/8222b0b73026809ab69161ec7459eba87cdd24.debug
Reading symbols from usr/lib/debug/.build-id/3d/8222b0b73026809ab69161ec7459eba87cdd24.debug...done.
(gdb) list sql_kill
6922        only_kill_query     Should it kill the query or the connection
6923    */
6924
6925    static
6926    void sql_kill(THD *thd, ulong id, killed_state state)
6927    {
6928      uint error;
6929      if (!(error= kill_one_thread(thd, id, state)))
6930      {
6931        if ((!thd->killed))
{noformat}

So to make it work:
# requires latest cmake - make package generates tar.gz and untar that to /usr/local and set PATH=/usr/local/cmake-..../bin:$PATH
# Also requires the source/build directories to be very log. This is because the debug info path is edited in-place in the file during the build and is replaces with ./usr/src/debug/MariaDB-5.5.55-fedora-x86_64/server/src_0/sql/..... - so the source/build path needs to be longer than this."
2273,MDEV-4646,MDEV,Daniel Black,94131,2017-04-20 00:37:19,"CMake-3.8.0 has been released containing all the fixes required to make this happen. Note quite the 3.7.0 required anticipated MDEV-11258. Still partially works with CMake-3.7.0 with the exception being the source code isn't saved in the package correctly. This can be worked around by checking out a source tree at the right release version.

For the build infrastructure it looks like a CMake-3.8.0 update will be required to generate proper debug packages for distribution.",8,"CMake-3.8.0 has been released containing all the fixes required to make this happen. Note quite the 3.7.0 required anticipated MDEV-11258. Still partially works with CMake-3.7.0 with the exception being the source code isn't saved in the package correctly. This can be worked around by checking out a source tree at the right release version.

For the build infrastructure it looks like a CMake-3.8.0 update will be required to generate proper debug packages for distribution."
2274,MDEV-4646,MDEV,Sergei Golubchik,96653,2017-06-19 14:04:39,"I've just push a one-liner to set {{CPACK_RPM_DEBUGINFO_PACKAGE}}. Although I suspect it won't do anything for our packages, until distributions start to ship cmake 3.7+.",9,"I've just push a one-liner to set {{CPACK_RPM_DEBUGINFO_PACKAGE}}. Although I suspect it won't do anything for our packages, until distributions start to ship cmake 3.7+."
2275,MDEV-4646,MDEV,Daniel Black,96677,2017-06-19 23:43:18,I think it will be good enough for mariadb package distribution to have debug symbol packages that can map to a source. The main reason for pushing this was so existing core dumps can be analysed without users to build debug versions or attempt to reproduce the bug. Developers can also use debug packages to analyse user core dumps. Fedora is still crafting its own spec file https://src.fedoraproject.org/cgit/rpms/mariadb.git/tree/ (and I'm not sure why _ maybe the rest of their build infrastrure depends on it),10,I think it will be good enough for mariadb package distribution to have debug symbol packages that can map to a source. The main reason for pushing this was so existing core dumps can be analysed without users to build debug versions or attempt to reproduce the bug. Developers can also use debug packages to analyse user core dumps. Fedora is still crafting its own spec file URL (and I'm not sure why _ maybe the rest of their build infrastrure depends on it)
2276,MDEV-4646,MDEV,Sergei Golubchik,96680,2017-06-20 06:47:04,"[~danblack], I fully agree with that. But if we build with non-standard manually installed cmake, we will get builds that users won't be able to reproduce (on a stock distro installation). That might still be fine. But then there's a good chance we break the build for older cmake versions and won't even notice it.

Perhaps we should switch to using manually installed new cmake almost everywhere, but keep the oldest supported cmake at least one one builder, just to make sure it still works...

Another thought — this {{RPM-DEFAULT}} patch. I've pushed it under assumption that it'll work for all new distributions, not for old one. If we switch to a new cmake on old distros, we'll have to document that ""mariadb rpm is named XXX for versions 5.5.x and below, 10.0.y and below, 10.1.z and below, and it's named YYY for newer 5.5, 10.0, 10.1 versions"". Of course, we can have this in the manual, but I think we'd better not to.",11,"[~danblack], I fully agree with that. But if we build with non-standard manually installed cmake, we will get builds that users won't be able to reproduce (on a stock distro installation). That might still be fine. But then there's a good chance we break the build for older cmake versions and won't even notice it.

Perhaps we should switch to using manually installed new cmake almost everywhere, but keep the oldest supported cmake at least one one builder, just to make sure it still works...

Another thought — this {{RPM-DEFAULT}} patch. I've pushed it under assumption that it'll work for all new distributions, not for old one. If we switch to a new cmake on old distros, we'll have to document that ""mariadb rpm is named XXX for versions 5.5.x and below, 10.0.y and below, 10.1.z and below, and it's named YYY for newer 5.5, 10.0, 10.1 versions"". Of course, we can have this in the manual, but I think we'd better not to."
2277,MDEV-4646,MDEV,Daniel Black,96975,2017-06-27 00:51:53,"[Fedora-25|https://apps.fedoraproject.org/packages/cmake] has cmake-3.8 up from 3.6 in FC24 so its gradually changing (as is [openSUSE|https://software.opensuse.org/package/cmake]).

An distro cmake on one builder sounds very sane.

The RPM-DEFAULT patch defers the package name to the rpmbuilder in the distribution. I don't imagine any distribution has that broken.

Do you think cmake/cpack_rpm.cmake CPACK_RPM_XXX_PACKAGE_OBSOLETES needs to refer to the old mariadb rpm name?

",12,"[Fedora-25|URL has cmake-3.8 up from 3.6 in FC24 so its gradually changing (as is [openSUSE|URL

An distro cmake on one builder sounds very sane.

The RPM-DEFAULT patch defers the package name to the rpmbuilder in the distribution. I don't imagine any distribution has that broken.

Do you think cmake/cpack_rpm.cmake CPACK_RPM_XXX_PACKAGE_OBSOLETES needs to refer to the old mariadb rpm name?

"
2278,MDEV-4646,MDEV,Sergei Golubchik,96983,2017-06-27 09:03:49,"No, I don't. I think the rpm package name was always MariaDB-server, MariaDB-client, etc. This didn't change. RPM-DEFAULT only changed file names, but MariaDB-server still means what it used to mean.",13,"No, I don't. I think the rpm package name was always MariaDB-server, MariaDB-client, etc. This didn't change. RPM-DEFAULT only changed file names, but MariaDB-server still means what it used to mean."
2279,MDEV-4646,MDEV,Daniel Black,97059,2017-06-28 12:29:27,"Good to know RPM names/filenames are distinguished.

I finally read your comments and MDEV-11258 correctly. You're saying a followup task is needed to use cmake-3.7+ on most builders (perhaps MDEV-12508). I agree.

On rpm documentation - seems that kb pages mentioning RPM could just mention the two forms of filename https://mariadb.com/kb/en/mariadb/about-the-mariadb-rpm-files/. Or like mentioned on https://mariadb.com/kb/en/mariadb/installing-mariadb-with-the-rpm-tool/ ""look similar to ..."". (or do a major cull of RPM documentation to mainly refer to yum repo based installs).",14,"Good to know RPM names/filenames are distinguished.

I finally read your comments and MDEV-11258 correctly. You're saying a followup task is needed to use cmake-3.7+ on most builders (perhaps MDEV-12508). I agree.

On rpm documentation - seems that kb pages mentioning RPM could just mention the two forms of filename URL Or like mentioned on URL ""look similar to ..."". (or do a major cull of RPM documentation to mainly refer to yum repo based installs)."
2280,MDEV-4682,MDEV,roberto spadim,32803,2013-06-19 20:55:47,"should use qc_plugin with MDEV-4682 support
this diff_10_0_3_qc_info.cc is the same sent to MDEV_4581 (with statistics support)",1,"should use qc_plugin with MDEV-4682 support
this diff_10_0_3_qc_info.cc is the same sent to MDEV_4581 (with statistics support)"
2281,MDEV-4682,MDEV,roberto spadim,32804,2013-06-19 21:23:32,use the same diff_10_0_3_qc_info.cc,2,use the same diff_10_0_3_qc_info.cc
2282,MDEV-4682,MDEV,roberto spadim,33124,2013-07-07 11:37:44,"sergey, should i put some #ifdef, and make statistic a option when compiling the source?",3,"sergey, should i put some #ifdef, and make statistic a option when compiling the source?"
2283,MDEV-4682,MDEV,roberto spadim,52064,2014-06-16 18:00:47,"hi guys, now i'm with time to code a bit more, could we consider a review of this MDEV?
i'm using the last code near to one year, and it's running quite ok at production server :) (ok i shouldn't use 10.0 at production that time, but it's running nice)

about informations, the statistic data, is nice to me, i could search queries that should be SQL_NO_CACHE and queries that could be SQL_CACHE very easy, and check if cache is doing a good job

thanks guys",4,"hi guys, now i'm with time to code a bit more, could we consider a review of this MDEV?
i'm using the last code near to one year, and it's running quite ok at production server :) (ok i shouldn't use 10.0 at production that time, but it's running nice)

about informations, the statistic data, is nice to me, i could search queries that should be SQL_NO_CACHE and queries that could be SQL_CACHE very easy, and check if cache is doing a good job

thanks guys"
2284,MDEV-4682,MDEV,roberto spadim,55333,2014-07-16 20:14:36,"hi, guys, i was thinking about using audit plugin to collect statistic about query cache, could it work? i'm confuse about query cache invalidation, if audit plugin could solve this problem or not, any idea if i will have problems? the 'bad' part of this today implementation patch, is the change of sql_cache.cc and sql_cache.h source code, it change structures of cache code at sql_cache.h",5,"hi, guys, i was thinking about using audit plugin to collect statistic about query cache, could it work? i'm confuse about query cache invalidation, if audit plugin could solve this problem or not, any idea if i will have problems? the 'bad' part of this today implementation patch, is the change of sql_cache.cc and sql_cache.h source code, it change structures of cache code at sql_cache.h"
2285,MDEV-4682,MDEV,roberto spadim,60901,2014-09-20 07:04:41,sending patchs via pull request at github,6,sending patchs via pull request at github
2286,MDEV-4682,MDEV,roberto spadim,60907,2014-09-20 23:54:40,"there's one TODO, use or not use statistics with query cache, since it increase memory use should be nice turn it off/on, at least at compile time, but should be nice if it could be done at runtime, for example, turning statistics on/off release all memory and queries and restart query cache again with a new 'table' (a block of memory) of statistics if it's on, if it's off don't use this 'table'",7,"there's one TODO, use or not use statistics with query cache, since it increase memory use should be nice turn it off/on, at least at compile time, but should be nice if it could be done at runtime, for example, turning statistics on/off release all memory and queries and restart query cache again with a new 'table' (a block of memory) of statistics if it's on, if it's off don't use this 'table'"
2287,MDEV-4682,MDEV,Oleksandr Byelkin,61356,2014-09-30 14:30:12,Before using try_lock() it is better to check of is_disabled() to avoid delay in case of changing QC state.,8,Before using try_lock() it is better to check of is_disabled() to avoid delay in case of changing QC state.
2288,MDEV-4682,MDEV,roberto spadim,61370,2014-09-30 19:33:57,nice i will include it now,9,nice i will include it now
2289,MDEV-4682,MDEV,roberto spadim,61385,2014-09-30 22:22:34,"updated: https://github.com/rspadim/server/commit/7bca392c82959b7e211822f988b95832f3e2d96a

qc->is_disabled() before all try_lock(thd)",10,"updated: URL

qc->is_disabled() before all try_lock(thd)"
2290,MDEV-4682,MDEV,roberto spadim,62502,2014-10-13 08:39:11,"i think we are ok to review :) 
i'm using in production without problems, at 10.0.13 and 10.1 (github) both tested, but 10.0.13 i didn't have bazaar, i just commited to github

must check if i done the right work with test files

one doubt is... query_id should be a number from 1......infinte, or there's a hash table value like a primary key or something like it? ",11,"i think we are ok to review :) 
i'm using in production without problems, at 10.0.13 and 10.1 (github) both tested, but 10.0.13 i didn't have bazaar, i just commited to github

must check if i done the right work with test files

one doubt is... query_id should be a number from 1......infinte, or there's a hash table value like a primary key or something like it? "
2291,MDEV-4682,MDEV,Daniel Black,69265,2015-03-19 01:22:43,Looks good. Seems documentation is ahead of code a bit this time - https://mariadb.com/kb/en/mariadb/query_cache_info-plugin/ probably needs a little more work to describe which tables are in which versions.,12,Looks good. Seems documentation is ahead of code a bit this time - URL probably needs a little more work to describe which tables are in which versions.
2292,MDEV-4682,MDEV,Oleksandr Byelkin,69341,2015-03-22 16:57:32,It should be mentioned that using this plugin (during fetching data from information schema) lock QC. So one who use it should be aware of it and do not make DoS on his server by frequent requests.,13,It should be mentioned that using this plugin (during fetching data from information schema) lock QC. So one who use it should be aware of it and do not make DoS on his server by frequent requests.
2293,MDEV-4682,MDEV,Daniel Black,71155,2015-05-17 09:35:16,"https://github.com/MariaDB/server/pull/67/files is now the basic Query Cache Info plugin with decoding of query cache flags and test cases that show most of the query cache flags changed by server variables.

Hoping it is suitable for 10.0 as well as 10.1

Edit: was incomplete. Now isn't.",14,"URL is now the basic Query Cache Info plugin with decoding of query cache flags and test cases that show most of the query cache flags changed by server variables.

Hoping it is suitable for 10.0 as well as 10.1

Edit: was incomplete. Now isn't."
2294,MDEV-4682,MDEV,roberto spadim,78811,2015-12-07 20:24:58,"nice, now just need some statistics, flags are ok",15,"nice, now just need some statistics, flags are ok"
2295,MDEV-4691,MDEV,David McLennan,41863,2014-02-20 19:26:13,"Would like to add support for this capability and for default plugins to be shipped with the server and client distributions.  We also need to consider JDBC, ODBC and .Net driver capabilities.

",1,"Would like to add support for this capability and for default plugins to be shipped with the server and client distributions.  We also need to consider JDBC, ODBC and .Net driver capabilities.

"
2296,MDEV-4691,MDEV,QIU Shuang,41871,2014-02-21 04:01:31,"@DavidMcLennan My initial release of the plugin could be found at https://code.launchpad.net/~qiush-summer/maria/krb_auth_plugin, and Vlad also gave a JDBC implementation http://bazaar.launchpad.net/~wlad-montyprogram/+junk/mariadb-java-client/revision/470. Unfortunately, we didn't consider the ODBC and .Net driver correspondents then. See anything else I can help.",2,"@DavidMcLennan My initial release of the plugin could be found at URL and Vlad also gave a JDBC implementation URL Unfortunately, we didn't consider the ODBC and .Net driver correspondents then. See anything else I can help."
2297,MDEV-4691,MDEV,Daniel Black,75055,2015-08-27 04:32:03,"Nice work [~qiushuang],

In CONJ-164 I've added a bit of a plugin framework. Should be easy enough to adapt the JDBC work there too. Vlad's client implementation seems compatible with http://docs.oracle.com/javase/6/docs/technotes/guides/security/jgss/lab/part5.html#SPNEGO (when SPNEGO_OID is used instead of KERBEROS_OID).

How hard would it be to wrap the server side in SPNEGO to be compatible with https://dev.mysql.com/doc/internals/en/spnego.html ?

[~elenst] can you add a patch label here?",3,"Nice work [~qiushuang],

In CONJ-164 I've added a bit of a plugin framework. Should be easy enough to adapt the JDBC work there too. Vlad's client implementation seems compatible with URL (when SPNEGO_OID is used instead of KERBEROS_OID).

How hard would it be to wrap the server side in SPNEGO to be compatible with URL ?

[~elenst] can you add a patch label here?"
2298,MDEV-4691,MDEV,Vladislav Vaintroub,75056,2015-08-27 06:23:26,"This can be easily modified to support Oracle/MySQL's Windows authentication. Easily however _on Windows only_ (since choice Kerberos vs Negotiate is defined by single parameter in SSPI), no idea what effort would be to support it Unix server and C-client side.

  But, if compatibility with Oracle/MySQL  is desired, SPNEGO plugin also should be called differently. Neither ""kerberos_client"" , nor SPNEGO,  but ""authentication_windows_client"" which is the name of Oracle/MySQL plugin ( they also only implement the server and C client  on Window only)

",4,"This can be easily modified to support Oracle/MySQL's Windows authentication. Easily however _on Windows only_ (since choice Kerberos vs Negotiate is defined by single parameter in SSPI), no idea what effort would be to support it Unix server and C-client side.

  But, if compatibility with Oracle/MySQL  is desired, SPNEGO plugin also should be called differently. Neither ""kerberos_client"" , nor SPNEGO,  but ""authentication_windows_client"" which is the name of Oracle/MySQL plugin ( they also only implement the server and C client  on Window only)

"
2299,MDEV-4691,MDEV,Vladislav Vaintroub,75057,2015-08-27 07:14:04,"@danblack : Also, there are some scary details in MySQL/Oracle windows authentication implementation for which I miss explanation, other than ""it is a historical bug, but we document it here"". Those should not be replicated QIUs kerberos plugin, I guess 

https://dev.mysql.com/doc/internals/en/windows-native-authentication.html . has this  or example: 

 Due to implementation details (again) the first packet sent from the client to the server is expected to be either

   - 254 bytes long max or

   - send the first 254 bytes first, appended by 1 byte with a magic value plus a 2nd packet with rest of the data 

",5,"@danblack : Also, there are some scary details in MySQL/Oracle windows authentication implementation for which I miss explanation, other than ""it is a historical bug, but we document it here"". Those should not be replicated QIUs kerberos plugin, I guess 

URL . has this  or example: 

 Due to implementation details (again) the first packet sent from the client to the server is expected to be either

   - 254 bytes long max or

   - send the first 254 bytes first, appended by 1 byte with a magic value plus a 2nd packet with rest of the data 

"
2300,MDEV-4691,MDEV,Daniel Black,75064,2015-08-27 12:53:08,"yeh, saw those odd document bits. Different plugin structure on the server side corresponding to a different name. Thanks for the info.",6,"yeh, saw those odd document bits. Different plugin structure on the server side corresponding to a different name. Thanks for the info."
2301,MDEV-4691,MDEV,Robbie Harwood,75092,2015-08-28 22:22:22,"Hello!  I have modified this code to support encryption as well, and opened https://github.com/MariaDB/server/pull/95 for it.",7,"Hello!  I have modified this code to support encryption as well, and opened URL for it."
2302,MDEV-4691,MDEV,Vladislav Vaintroub,75095,2015-08-29 05:45:40,"Robbie Harwood, nice work. But .. is there a very strong reason for Kerberos encryption, provided that server and most of the client libraries (including those written natively in Java, .NET, Python, Ruby etc) already support good encryption via SSL?  
Also when using Kerberos, people would likely (more often than not) want to just to authenticate for say SSO purposes not to encrypt, as  encryption is a big CPU hog.",8,"Robbie Harwood, nice work. But .. is there a very strong reason for Kerberos encryption, provided that server and most of the client libraries (including those written natively in Java, .NET, Python, Ruby etc) already support good encryption via SSL?  
Also when using Kerberos, people would likely (more often than not) want to just to authenticate for say SSO purposes not to encrypt, as  encryption is a big CPU hog."
2303,MDEV-4691,MDEV,Robbie Harwood,75353,2015-09-03 21:22:37,"If one uses Kerberos for authentication, there is no additional sysadmin effort to use it for encryption.  Kerberos does not need certificate management the way a TLS setup does.

Personally I doubt that the overhead of encryption is noticeable in the vast majority of cases, but if there is going to be a demand for authentication without encryption (which provides *zero* connection security on the wire since traffic it is not encrypted, which is why I don't understand the use case) then I can try to meet this demand.  I think that having this option invites sysadmins to inadvertently negate their security guarantees.

There's also a limit we are quickly approaching with the current plugin architecture to keep in mind - Kerberos authentication+encryption are best done as a plugin, but at the moment encryption plugins cannot be created.  This is changing, but the changes are not yet on the horizon is my understanding, and there is demand for Kerberos right now (there was even a GSOC for supporting it!).  I have been performing a similar task for postgres, where they for some time have had authentication without encryption; there is no pleasant way to add encryption later.",9,"If one uses Kerberos for authentication, there is no additional sysadmin effort to use it for encryption.  Kerberos does not need certificate management the way a TLS setup does.

Personally I doubt that the overhead of encryption is noticeable in the vast majority of cases, but if there is going to be a demand for authentication without encryption (which provides *zero* connection security on the wire since traffic it is not encrypted, which is why I don't understand the use case) then I can try to meet this demand.  I think that having this option invites sysadmins to inadvertently negate their security guarantees.

There's also a limit we are quickly approaching with the current plugin architecture to keep in mind - Kerberos authentication+encryption are best done as a plugin, but at the moment encryption plugins cannot be created.  This is changing, but the changes are not yet on the horizon is my understanding, and there is demand for Kerberos right now (there was even a GSOC for supporting it!).  I have been performing a similar task for postgres, where they for some time have had authentication without encryption; there is no pleasant way to add encryption later."
2304,MDEV-4691,MDEV,Vladislav Vaintroub,75354,2015-09-03 22:09:43,"The authentication is related yet separate thing from encryption. The use case I had in mind was  single sign on, where user does not have to type his password once more after OS/kerberos login. I do not know how much it improves security (certainly it does not on the wire), but it increases convenience considerably and removes the need of having yet one more password, just for accessing database. 

Apart from that, as  mentioned, there are different clients apart from C client,  that on different reasons cannot use C client.  There adding additional authentication is trivial, but they require a lot of work in case you tie it together with encryption.  You can investigate what it means to add kerberos encryption support to Maria JDBC,  while auth only  required maybe 20 lines of code :) 

So rather than rendering kerberos support useless outside of C client world, I believe it is  a better idea to just support authentication at current moment, and  discuss the architecture of pluggable encryption on the mailing list or so.  Pluggable encryption certainly has its merit. But, as you say,  in this case is mostly convenience (no need to self-sign SSL certificates)
",10,"The authentication is related yet separate thing from encryption. The use case I had in mind was  single sign on, where user does not have to type his password once more after OS/kerberos login. I do not know how much it improves security (certainly it does not on the wire), but it increases convenience considerably and removes the need of having yet one more password, just for accessing database. 

Apart from that, as  mentioned, there are different clients apart from C client,  that on different reasons cannot use C client.  There adding additional authentication is trivial, but they require a lot of work in case you tie it together with encryption.  You can investigate what it means to add kerberos encryption support to Maria JDBC,  while auth only  required maybe 20 lines of code :) 

So rather than rendering kerberos support useless outside of C client world, I believe it is  a better idea to just support authentication at current moment, and  discuss the architecture of pluggable encryption on the mailing list or so.  Pluggable encryption certainly has its merit. But, as you say,  in this case is mostly convenience (no need to self-sign SSL certificates)
"
2305,MDEV-4691,MDEV,Vladislav Vaintroub,75356,2015-09-03 23:06:07,"Apparently , there seems to be support for kerberos in SSL http://www.ietf.org/rfc/rfc2712.txt. ?I understand  no certificates are necessary on either side,  and the client (and server) identity could be extracted. Perhaps this can  be investigated further, this could  provide both encryption via SSL,  authentication via Kerberos , without certificates, and without need to  support more encryption in client libraries.",11,"Apparently , there seems to be support for kerberos in SSL URL ?I understand  no certificates are necessary on either side,  and the client (and server) identity could be extracted. Perhaps this can  be investigated further, this could  provide both encryption via SSL,  authentication via Kerberos , without certificates, and without need to  support more encryption in client libraries."
2306,MDEV-4691,MDEV,Robbie Harwood,75876,2015-09-15 23:23:22,"I do not believe 2712 is a realistic option or that it is currently supported in TLS.

If JDBC is going to be required for this to merge, can you point me to that codebase?",12,"I do not believe 2712 is a realistic option or that it is currently supported in TLS.

If JDBC is going to be required for this to merge, can you point me to that codebase?"
2307,MDEV-4691,MDEV,Vladislav Vaintroub,75877,2015-09-15 23:54:33,"JDBC driver is located here https://github.com/MariaDB/mariadb-connector-j

I have no authority about what is going to be required for merge (I am _currently_ not even a member of MariaDB team). I think bigger changes and ideas like yours should be discussed on the mailing list rather than in a bug database. 

So far I explained, why I think  bundling Kerberos authentication with encryption would make it  less valuable.  If this RFC2712 is bad or outdated, even then other changes that would make using SSL easier,  like for example  creating self-signed server certificate on the fly during server start, should solve the remaining concerns about encryption. I doubted there is a significant need of yet another encryption, but this is just my own opinion, based on some previous work on MySQL connectors. Neither Java nor NET (and I guess not many other languages) would offer a ""Kerberos stream"", yet  SSL support is pretty good  everywhere.",13,"JDBC driver is located here URL

I have no authority about what is going to be required for merge (I am _currently_ not even a member of MariaDB team). I think bigger changes and ideas like yours should be discussed on the mailing list rather than in a bug database. 

So far I explained, why I think  bundling Kerberos authentication with encryption would make it  less valuable.  If this RFC2712 is bad or outdated, even then other changes that would make using SSL easier,  like for example  creating self-signed server certificate on the fly during server start, should solve the remaining concerns about encryption. I doubted there is a significant need of yet another encryption, but this is just my own opinion, based on some previous work on MySQL connectors. Neither Java nor NET (and I guess not many other languages) would offer a ""Kerberos stream"", yet  SSL support is pretty good  everywhere."
2308,MDEV-4691,MDEV,Robbie Harwood,75880,2015-09-16 01:03:21,"If you'd like, I can propose a change on the mailing list, though there was discussion of them on the IRC channel beforehand.  Mostly, it takes me by surprise that anyone is opposed to these changes; they seem the natural addon to the GSOC work.

I am not sure what you mean by ""Kerberos stream"".",14,"If you'd like, I can propose a change on the mailing list, though there was discussion of them on the IRC channel beforehand.  Mostly, it takes me by surprise that anyone is opposed to these changes; they seem the natural addon to the GSOC work.

I am not sure what you mean by ""Kerberos stream""."
2309,MDEV-4691,MDEV,Vladislav Vaintroub,75882,2015-09-16 01:33:26,"Java, as well as  .NET and probably more languages and runtimes have notion of stream. Java and NET have transparent support for SSL , you read and write bytes without wrapping, in Java using SSLSocket.getInputStream() or SSLSocket.getOutputStream() . In .NET, there is SSLStream that wraps an unencrypted Stream. They are both used to implement SSL support for MySQL.

There is no analog for that stuff for Kerberos encryption, i.e no support out of the box.  I do not know why nobody opposed there changes on IRC, but mostly likely the reason would be that people you spoke to never worked on  drivers (Connectors in MySQL speak), other than C-based ones (which is, C API and ODBC). But I worked on them :)

",15,"Java, as well as  .NET and probably more languages and runtimes have notion of stream. Java and NET have transparent support for SSL , you read and write bytes without wrapping, in Java using SSLSocket.getInputStream() or SSLSocket.getOutputStream() . In .NET, there is SSLStream that wraps an unencrypted Stream. They are both used to implement SSL support for MySQL.

There is no analog for that stuff for Kerberos encryption, i.e no support out of the box.  I do not know why nobody opposed there changes on IRC, but mostly likely the reason would be that people you spoke to never worked on  drivers (Connectors in MySQL speak), other than C-based ones (which is, C API and ODBC). But I worked on them :)

"
2310,MDEV-4691,MDEV,Robbie Harwood,75884,2015-09-16 02:39:04,"That's true, however: creating such an interface is not difficult (done it before; will probably do it again), and I would imagine it's what these ""stream"" interfaces are doing under the hood anyway.  The GSSAPI interface is just set up to work with an existing flow control layer.",16,"That's true, however: creating such an interface is not difficult (done it before; will probably do it again), and I would imagine it's what these ""stream"" interfaces are doing under the hood anyway.  The GSSAPI interface is just set up to work with an existing flow control layer."
2311,MDEV-4691,MDEV,Vladislav Vaintroub,75885,2015-09-16 02:53:27,There is no GSSAPI for some of the environments we talk about.,17,There is no GSSAPI for some of the environments we talk about.
2312,MDEV-4691,MDEV,Robbie Harwood,75900,2015-09-16 19:13:55,"In those environments, one wouldn't be able to use GSSAPI authentication either, no?",18,"In those environments, one wouldn't be able to use GSSAPI authentication either, no?"
2313,MDEV-4691,MDEV,Vladislav Vaintroub,75902,2015-09-16 20:07:01,"No GSSAPI, but SSPI would do in some environments . Also in .NET for example, which does not support either SSPI nor GSSAPI natively, the closest things would be to reuse NegotateStream, which wraps SPNEGO packets .Which makes me think that (optionally?) supporting SPNEGO  OID would be good in this patch (also because of the other reasons , it would allow Windows auth outside of domain)",19,"No GSSAPI, but SSPI would do in some environments . Also in .NET for example, which does not support either SSPI nor GSSAPI natively, the closest things would be to reuse NegotateStream, which wraps SPNEGO packets .Which makes me think that (optionally?) supporting SPNEGO  OID would be good in this patch (also because of the other reasons , it would allow Windows auth outside of domain)"
2314,MDEV-4691,MDEV,Robbie Harwood,75903,2015-09-16 20:37:02,"Well, if we're talking Windows, GSSAPI is available there (MIT Kerberos for Windows, or alternately a shim library over SSPI exists).  QIU Shuang's original patch had SSPI support which I assume works; however, I can't test or code against that.",20,"Well, if we're talking Windows, GSSAPI is available there (MIT Kerberos for Windows, or alternately a shim library over SSPI exists).  QIU Shuang's original patch had SSPI support which I assume works; however, I can't test or code against that."
2315,MDEV-4691,MDEV,Vladislav Vaintroub,75905,2015-09-16 22:13:07,"His code works fine,I  tested it, and can code against that.  I actually asked QIU to use  SSPI, when I was mentoring his GSoc project, avoiding external dependencies is good , MariaDB/MySQL does not have any experience with shipping 3rd party dlls, the documentation MIT Kerberos on Windows is sparse, there is no 64 bit installer, the quality of them is unknown. His code does not have encryption, though it is possible with SSPI .
Nobody expects you to code on Windows.  People just need to be aware that their patches won't always  be taken ""as-is"", and could be heavily modified to  be shippable. 

Anyways, there is still a general question
1. Is TLS encryption with self-signed server certificate any worse than kerberos encryption? I know I bothered you with this one before, but have not got a ""yes"",""no"" or even ""maybe"" answer :)

Imagine for a second, TLS is super easy to use, and installer always creates some kind of self-signed certificate, and the server always starts with TLS support (I think I read that MySQL 5.7 installer does  that)
Authentication kerberos and encryption(optionally) TLS, would satisfy the needs?  What are the drawbacks in this scenario? the obvious benefits are less lines of code required to support GSSAPI encryption for all imaginable client libraries and also for the server. Also plugin can stay plugin,which is good.

2. Is SPNEGO support good in MIT Kerberos, if you are familiar with that? The examples are sparse on the internet? I thought perhaps it would be a good idea to make ""GSSAPI"" plugin, rather than Kerberos, with configurable OID, which is mostly useful for Windows (but also  .NET / Mono, where SPNEGO protocol is supported but plain Kerberos not  well)",21,"His code works fine,I  tested it, and can code against that.  I actually asked QIU to use  SSPI, when I was mentoring his GSoc project, avoiding external dependencies is good , MariaDB/MySQL does not have any experience with shipping 3rd party dlls, the documentation MIT Kerberos on Windows is sparse, there is no 64 bit installer, the quality of them is unknown. His code does not have encryption, though it is possible with SSPI .
Nobody expects you to code on Windows.  People just need to be aware that their patches won't always  be taken ""as-is"", and could be heavily modified to  be shippable. 

Anyways, there is still a general question
1. Is TLS encryption with self-signed server certificate any worse than kerberos encryption? I know I bothered you with this one before, but have not got a ""yes"",""no"" or even ""maybe"" answer :)

Imagine for a second, TLS is super easy to use, and installer always creates some kind of self-signed certificate, and the server always starts with TLS support (I think I read that MySQL 5.7 installer does  that)
Authentication kerberos and encryption(optionally) TLS, would satisfy the needs?  What are the drawbacks in this scenario? the obvious benefits are less lines of code required to support GSSAPI encryption for all imaginable client libraries and also for the server. Also plugin can stay plugin,which is good.

2. Is SPNEGO support good in MIT Kerberos, if you are familiar with that? The examples are sparse on the internet? I thought perhaps it would be a good idea to make ""GSSAPI"" plugin, rather than Kerberos, with configurable OID, which is mostly useful for Windows (but also  .NET / Mono, where SPNEGO protocol is supported but plain Kerberos not  well)"
2316,MDEV-4691,MDEV,Robbie Harwood,75978,2015-09-18 23:55:17,"From a historical perspective, Kerberos has a better track record with regards to security than SSL/TLS.  It's hard to use this figure for much, though, because the presence or lack thereof of known-and-resolved vulnerabilities doesn't indicate much about future vulnerability discoveries.  GSSAPI has had a core API (with I believe one exception, all that I'm calling here is core) and wire protocol that was defined in 1997 and has not changed since (rfc 2078) except for a slight update in 2000 (rfc 2743 + 2744).  By contrast, every version of TLS from SSL 1.0 through TLS 1.2 is either entirely insecure or requires deviation from the specification (client mitigations, disabling broken ciphers, etc.).

A self-signed certificate requires propagation of either the cert or the CA cert to each client that wishes to connect; if this is not performed, then *the encryption is worth very little* since an adversary can man-in-the-middle the connection without the client noticing.  This is not true of Kerberos encryption.  If there is a significant use case for GSSAPI authentication with TLS encryption, I'm willing to discuss adding support for operation in this mode, but I really do think GSSAPI encryption is strongly desirable as a feature (especially since it's not going to break anything else).

The plugin provided does not make any Kerberos calls; all are done through the GSSAPI, though I have not tested it with SPNEGO.  (This is also true of the original GSOC code, excluding the SSPI portions.)",22,"From a historical perspective, Kerberos has a better track record with regards to security than SSL/TLS.  It's hard to use this figure for much, though, because the presence or lack thereof of known-and-resolved vulnerabilities doesn't indicate much about future vulnerability discoveries.  GSSAPI has had a core API (with I believe one exception, all that I'm calling here is core) and wire protocol that was defined in 1997 and has not changed since (rfc 2078) except for a slight update in 2000 (rfc 2743 + 2744).  By contrast, every version of TLS from SSL 1.0 through TLS 1.2 is either entirely insecure or requires deviation from the specification (client mitigations, disabling broken ciphers, etc.).

A self-signed certificate requires propagation of either the cert or the CA cert to each client that wishes to connect; if this is not performed, then *the encryption is worth very little* since an adversary can man-in-the-middle the connection without the client noticing.  This is not true of Kerberos encryption.  If there is a significant use case for GSSAPI authentication with TLS encryption, I'm willing to discuss adding support for operation in this mode, but I really do think GSSAPI encryption is strongly desirable as a feature (especially since it's not going to break anything else).

The plugin provided does not make any Kerberos calls; all are done through the GSSAPI, though I have not tested it with SPNEGO.  (This is also true of the original GSOC code, excluding the SSPI portions.)"
2317,MDEV-4691,MDEV,Vladislav Vaintroub,79371,2015-12-25 23:49:13,"Serg, could you please review the corresponding commit 
https://github.com/MariaDB/server/commit/e6898d1044a435e3dfd37e5c73cf4b75af5dcade",23,"Serg, could you please review the corresponding commit 
URL"
2318,MDEV-4691,MDEV,Henning Kropp,81642,2016-02-29 22:17:37,"Hi, thank you for the patch. Could you please provide more details about how to configure the client correctly. For example where do I configure the SPN and the auth mech?",24,"Hi, thank you for the patch. Could you please provide more details about how to configure the client correctly. For example where do I configure the SPN and the auth mech?"
2319,MDEV-4691,MDEV,Robbie Harwood,81647,2016-03-01 00:20:13,"SPN is configured in the [server] section of /etc/my.cnf.d/server.cnf  (or similar) as kerberos_principal_name.  Mechanism is configured through the GSSAPI layer; typically it will be Kerberos5 and SPNEGO, though this may vary with your system.",25,"SPN is configured in the [server] section of /etc/my.cnf.d/server.cnf  (or similar) as kerberos_principal_name.  Mechanism is configured through the GSSAPI layer; typically it will be Kerberos5 and SPNEGO, though this may vary with your system."
2320,MDEV-4691,MDEV,Vladislav Vaintroub,81648,2016-03-01 00:25:44,"There is a documentation for the plugin, 
https://mariadb.com/kb/en/mariadb/gssapi-authentication-plugin/

with example of how to create service principal on Unix (on Windows, with active directory, the principal name  could be determined by the plugin startup code, so it is a lesser concern)

",26,"There is a documentation for the plugin, 
URL

with example of how to create service principal on Unix (on Windows, with active directory, the principal name  could be determined by the plugin startup code, so it is a lesser concern)

"
2321,MDEV-4691,MDEV,Henning Kropp,81651,2016-03-01 08:17:22,"Sorry, but this all is not very clear for me. I was able to follow the doc to created a GSSAPI user. But running {{mysql --plugin-dir=/usr/lib64/mysql/plugin/ -u maria-user}} is not working for me.

From my understanding the client would need to be configured to use GSSAPI as well. And typically it needs the SPN name, so it knows the service to authenticate against.

https://mariadb.com/kb/en/mariadb/gssapi-authentication-plugin/ is really thin concerning the client config. There is a list of properties, but where do I place them? Also the client certainly does not need the keytab of the server, but the docu tells you so. Could you please review?",27,"Sorry, but this all is not very clear for me. I was able to follow the doc to created a GSSAPI user. But running {{mysql --plugin-dir=/usr/lib64/mysql/plugin/ -u maria-user}} is not working for me.

From my understanding the client would need to be configured to use GSSAPI as well. And typically it needs the SPN name, so it knows the service to authenticate against.

URL is really thin concerning the client config. There is a list of properties, but where do I place them? Also the client certainly does not need the keytab of the server, but the docu tells you so. Could you please review?"
2322,MDEV-4691,MDEV,Henning Kropp,81653,2016-03-01 08:21:02,"Oh, I realized this might just be a formatting issue :/ ... I read the bullet points as being related to the client properties, but they are not, right? Sorry for the confusion....

Found my mistake, docu is also fine (except confusing formatting). Sorry for bothering you here.",28,"Oh, I realized this might just be a formatting issue :/ ... I read the bullet points as being related to the client properties, but they are not, right? Sorry for the confusion....

Found my mistake, docu is also fine (except confusing formatting). Sorry for bothering you here."
2323,MDEV-4691,MDEV,Vladislav Vaintroub,81655,2016-03-01 11:19:48,"The only thing client has to do is to login as GSSAPI user (e.g  kinit, or  Windows domain login), and point to the plugin-dir (sometimes default plugin dir works, but  often it does not ). client does not have any additional configuration, current plugin API does not allow it.

As for formatting, I do not see it as confusing, but maybe I miss something. I believe everyone can modify it, it is just a wiki, so you're welcome!",29,"The only thing client has to do is to login as GSSAPI user (e.g  kinit, or  Windows domain login), and point to the plugin-dir (sometimes default plugin dir works, but  often it does not ). client does not have any additional configuration, current plugin API does not allow it.

As for formatting, I do not see it as confusing, but maybe I miss something. I believe everyone can modify it, it is just a wiki, so you're welcome!"
2324,MDEV-4912,MDEV,roberto spadim,33977,2013-08-17 21:00:16,"We have some nice field type in PostgreSQL
THE MAIN PROBLEM ABOUT THIS PLUGIN IS A NOT STANDARD ABOUT FIELD TYPES... 
for example XML functions of MYSQL 5.7 want a VARCHAR/CHAR/TEXT/BLOB as parameter, and a XML type could make some incompatibility...

well more definition is need before add more considerations...",1,"We have some nice field type in PostgreSQL
THE MAIN PROBLEM ABOUT THIS PLUGIN IS A NOT STANDARD ABOUT FIELD TYPES... 
for example XML functions of MYSQL 5.7 want a VARCHAR/CHAR/TEXT/BLOB as parameter, and a XML type could make some incompatibility...

well more definition is need before add more considerations..."
2325,MDEV-4912,MDEV,Alexander Barkov,138207,2019-11-20 06:59:02,"There is no a separate patch for MDEV-4912. This task consists of all other tasks marked as blockers for MDEV-4912.
",2,"There is no a separate patch for MDEV-4912. This task consists of all other tasks marked as blockers for MDEV-4912.
"
2326,MDEV-505,MDEV,DELETEME,81843,2016-03-08 20:54:52,"Hello,

I implemented this here: https://github.com/MariaDB/server/pull/163

Best regards,
Dan Ungureanu",1,"Hello,

I implemented this here: URL

Best regards,
Dan Ungureanu"
2327,MDEV-505,MDEV,Sergey Vojtovich,83194,2016-05-04 08:01:44,"[~serg], I reviewed this patch and I have no strong opinion. The patch as such is fine. Though similar effect can be achived with MYSQL_PS1 or --prompt=""..."". Also there was concern that it won't be meaningful for multiple instances are running on same hosts.",2,"[~serg], I reviewed this patch and I have no strong opinion. The patch as such is fine. Though similar effect can be achived with MYSQL_PS1 or --prompt=""..."". Also there was concern that it won't be meaningful for multiple instances are running on same hosts."
2328,MDEV-505,MDEV,Sergei Golubchik,83300,2016-05-09 13:52:53,"I'm neutral about it too. {{MYSQL_PS1}} doesn't solve the case when one connects from the same host both to localhost and to remote hosts. For multiple instances one can use {{\H:\p}}.

I don't like it because it's a very limited solution. {{\h}} prints “localhost”, {{\H}} would print, say, “myhost.mynet”. And then another user will come and ask for “localhost (at myhost.mynet)”. And for {{\H:\p}} there can be many variations too.

But ok, as it certainly makes no sense to have a complex language just to specify a prompt, I'm fine with adding {{\H}}.

Just fix the patch not to do {{gethostbyname()}} every time when a prompt needs to be printed.",3,"I'm neutral about it too. {{MYSQL_PS1}} doesn't solve the case when one connects from the same host both to localhost and to remote hosts. For multiple instances one can use {{\H:\p}}.

I don't like it because it's a very limited solution. {{\h}} prints “localhost”, {{\H}} would print, say, “myhost.mynet”. And then another user will come and ask for “localhost (at myhost.mynet)”. And for {{\H:\p}} there can be many variations too.

But ok, as it certainly makes no sense to have a complex language just to specify a prompt, I'm fine with adding {{\H}}.

Just fix the patch not to do {{gethostbyname()}} every time when a prompt needs to be printed."
2329,MDEV-5535,MDEV,tem,40311,2014-01-20 18:34:51,any chance to be fixed in 5.x?,1,any chance to be fixed in 5.x?
2330,MDEV-5535,MDEV,Sergei Golubchik,40424,2014-01-27 16:41:12,"Sorry, no. Not even in 10.0 — what you're talking about is a fundamental limitation of the current MySQL and MariaDB codebase, it would require serious changes to lift it. We cannot do that in a GA version. Not even in RC or Beta, it still would be too risky.",2,"Sorry, no. Not even in 10.0 — what you're talking about is a fundamental limitation of the current MySQL and MariaDB codebase, it would require serious changes to lift it. We cannot do that in a GA version. Not even in RC or Beta, it still would be too risky."
2331,MDEV-5535,MDEV,Michael McClennen,66098,2014-11-26 00:22:01,"Can somebody please update the Mariadb documentation to mention this limitation?  At least the MySQL documentation <a href=""https://dev.mysql.com/doc/refman/5.7/en/temporary-table-problems.html"">is explicit about it</a>.

I have just added a vote for this issue: it is by far the MOST IMPORTANT issue for me in all of Mariadb.  This has been a problem for far too long.
",3,"Can somebody please update the Mariadb documentation to mention this limitation?  At least the MySQL documentation is explicit about it.

I have just added a vote for this issue: it is by far the MOST IMPORTANT issue for me in all of Mariadb.  This has been a problem for far too long.
"
2332,MDEV-5535,MDEV,kapil chhajer,68949,2015-03-11 09:12:31,"Hi,
I am Kapil Chhajer student from IIIT Hyderabad pursuing M.tech. CSE. I am comfortable with c, c++ and java. I want to work on this issue.

Please suggest me some related material that helps me to know more about it",4,"Hi,
I am Kapil Chhajer student from IIIT Hyderabad pursuing M.tech. CSE. I am comfortable with c, c++ and java. I want to work on this issue.

Please suggest me some related material that helps me to know more about it"
2333,MDEV-5535,MDEV,Sergei Golubchik,68989,2015-03-11 16:46:43,"Normal tables are opened like this:
# table definition is read, parsed into a TABLE_SHARE object
# TABLE_SHARE object is stored in the table definition cache
# a TABLE object is created from the TABLE_SHARE, and a table is opened in the storage engine

When the same table is opened again
# the TABLE_SHARE object is taken from the table definition cache
# a TABLE object is created from the TABLE_SHARE, and a table is opened in the storage engine

That is, there is always one TABLE_SHARE per table, but there can be many TABLE objects, if the table is opened many times, say, is joined to itself.

Temporary tables aren't cached in the table definition cache, and their TABLE_SHARE objects are, in a sense, part of the TABLE objects. That is, for temporary tables there can be only one TABLE per TABLE_SHARE. That's why temporary tables cannot be reopened.

The fix is to decouple TABLE_SHARE from TABLE for temporary tables and store them separately in two different lists in THD.",5,"Normal tables are opened like this:
# table definition is read, parsed into a TABLE_SHARE object
# TABLE_SHARE object is stored in the table definition cache
# a TABLE object is created from the TABLE_SHARE, and a table is opened in the storage engine

When the same table is opened again
# the TABLE_SHARE object is taken from the table definition cache
# a TABLE object is created from the TABLE_SHARE, and a table is opened in the storage engine

That is, there is always one TABLE_SHARE per table, but there can be many TABLE objects, if the table is opened many times, say, is joined to itself.

Temporary tables aren't cached in the table definition cache, and their TABLE_SHARE objects are, in a sense, part of the TABLE objects. That is, for temporary tables there can be only one TABLE per TABLE_SHARE. That's why temporary tables cannot be reopened.

The fix is to decouple TABLE_SHARE from TABLE for temporary tables and store them separately in two different lists in THD."
2334,MDEV-5535,MDEV,kapil chhajer,69061,2015-03-12 23:11:45,"Hi,
Thanks Sergei for replying. I got a copy of source code with the help of
this link https://code.launchpad.net/maria. I want to look
the source code So I will get whole idea.

please suggest me where to start.

On Wed, Mar 11, 2015 at 8:17 PM, Sergei Golubchik (JIRA) <

",6,"Hi,
Thanks Sergei for replying. I got a copy of source code with the help of
this link URL I want to look
the source code So I will get whole idea.

please suggest me where to start.

On Wed, Mar 11, 2015 at 8:17 PM, Sergei Golubchik (JIRA) <

"
2335,MDEV-5535,MDEV,Sergei Golubchik,69071,2015-03-13 10:27:09,"start from https://github.com/mariadb/server
then see my previous comment, it has enough pointers for you to start exploring the source code",7,"start from URL
then see my previous comment, it has enough pointers for you to start exploring the source code"
2336,MDEV-5535,MDEV,kapil chhajer,69080,2015-03-13 12:29:11,"Thanks sergei
",8,"Thanks sergei
"
2337,MDEV-5535,MDEV,Sergei Petrunia,78525,2015-11-26 00:33:08,"It looks like there are multiple other tasks that could benefit from this task.  These are:
* MDEV-6115: Window functions
* MDEV-8789: Common Table Expressions
* [No MDEV yet] Materialized view processing. Currently, if a materialized VIEW is used multiple times in the query, several temporary tables will be created and populated.",9,"It looks like there are multiple other tasks that could benefit from this task.  These are:
* MDEV-6115: Window functions
* MDEV-8789: Common Table Expressions
* [No MDEV yet] Materialized view processing. Currently, if a materialized VIEW is used multiple times in the query, several temporary tables will be created and populated."
2338,MDEV-5535,MDEV,Sergei Golubchik,78541,2015-11-26 16:08:38,"[~nirbhay_c], note the comment above. It means that one should be able to reopen *internal* temporary tables too.",10,"[~nirbhay_c], note the comment above. It means that one should be able to reopen *internal* temporary tables too."
2339,MDEV-5535,MDEV,Nirbhay Choubey,81834,2016-03-08 13:40:18,https://github.com/mariadb/server/tree/bb-10.2-mdev-5535,11,URL
2340,MDEV-5535,MDEV,Nirbhay Choubey,83460,2016-05-17 04:02:17,"Post-review patch :
http://lists.askmonty.org/pipermail/commits/2016-May/009365.html",12,"Post-review patch :
URL"
2341,MDEV-5535,MDEV,Nirbhay Choubey,83720,2016-05-27 04:27:54,"http://lists.askmonty.org/pipermail/commits/2016-May/009382.html
http://lists.askmonty.org/pipermail/commits/2016-May/009383.html",13,"URL
URL"
2342,MDEV-5535,MDEV,Nirbhay Choubey,84181,2016-06-10 23:19:39,"Branch: 10.2
Commits : 78d4276..b2ae32a",14,"Branch: 10.2
Commits : 78d4276..b2ae32a"
2343,MDEV-5536,MDEV,Honza Horak,41875,2014-02-21 10:28:50,"Just for anybody who is not willing to read the whole passionate discussion at (1), I'd like to point out the comment #62 there (2), which summarizes the main cons of using socket activation for a database server. I'd also add another comment, that according at least Fedora guidelines, socket-activated services need to be autostart (3), which is not something we wish from database server, and worth noting that other distros can have similar requirements.

That having in mind, I don't think it's worth trying to implement socket activation now, unless there is some special use case for such a feature.

(1) https://bugzilla.redhat.com/show_bug.cgi?id=714426
(2) https://bugzilla.redhat.com/show_bug.cgi?id=714426#c62
(3) https://fedoraproject.org/wiki/Packaging:Systemd#Socket_activation
",1,"Just for anybody who is not willing to read the whole passionate discussion at (1), I'd like to point out the comment #62 there (2), which summarizes the main cons of using socket activation for a database server. I'd also add another comment, that according at least Fedora guidelines, socket-activated services need to be autostart (3), which is not something we wish from database server, and worth noting that other distros can have similar requirements.

That having in mind, I don't think it's worth trying to implement socket activation now, unless there is some special use case for such a feature.

(1) URL
(2) URL
(3) URL
"
2344,MDEV-5536,MDEV,Sergei Golubchik,41878,2014-02-21 11:22:11,"Thanks, [~hhorak]! These are very important considerations that should be taken into account.",2,"Thanks, [~hhorak]! These are very important considerations that should be taken into account."
2345,MDEV-5536,MDEV,David Strauss,41880,2014-02-21 12:22:20,"> Just for anybody who is not willing to read the whole passionate discussion at (1), I'd like to point out the comment #62 there (2), which summarizes the main cons of using socket activation for a database server.

Those aren't _cons_ of using socket activation. At most, they're arguments against enjoying certain benefits, like on-demand startup. (We actually use on-demand startup for about 300,000 MariaDB instances; it's merely an issue of good hygiene during shutdown.) Any issues with on-demand startup are avoidable by simply starting the server at boot, which is the option of any administrator with or without socket activation.

Remember that ""socket activation"" merely means systemd opens the initial listener and passes it in; it does not mean the service must wait on the first client connection to start. I think people are getting too caught up in the name ""socket activation"" rather than its functional aspects.

> I'd also add another comment, that according at least Fedora guidelines, socket-activated services need to be autostart (3), which is not something we wish from database server

Fedora packaging guidelines, in general, require RPMs to not start services on installation or by default at boot. This is not a distinction for socket-activated services. If an administrator explicitly ""enables"" a socket-activated service unit, it will start at boot. This is the same behavior required today for the non-socket-activated MariaDB packages shipped by Fedora.

> and worth noting that other distros can have similar requirements.

Other distros, like Ubuntu and Debian (which are moving to systemd) actually do not have such a restriction on services auto-starting after package installation.

> That having in mind, I don't think it's worth trying to implement socket activation now, unless there is some special use case for such a feature.

The benefits for clean restarts, security isolation, privilege separation, race-condition avoidance, and support for the network features (which are in the current v209 release now) remain useful, even if you think on-demand startup isn't.",3,"> Just for anybody who is not willing to read the whole passionate discussion at (1), I'd like to point out the comment #62 there (2), which summarizes the main cons of using socket activation for a database server.

Those aren't _cons_ of using socket activation. At most, they're arguments against enjoying certain benefits, like on-demand startup. (We actually use on-demand startup for about 300,000 MariaDB instances; it's merely an issue of good hygiene during shutdown.) Any issues with on-demand startup are avoidable by simply starting the server at boot, which is the option of any administrator with or without socket activation.

Remember that ""socket activation"" merely means systemd opens the initial listener and passes it in; it does not mean the service must wait on the first client connection to start. I think people are getting too caught up in the name ""socket activation"" rather than its functional aspects.

> I'd also add another comment, that according at least Fedora guidelines, socket-activated services need to be autostart (3), which is not something we wish from database server

Fedora packaging guidelines, in general, require RPMs to not start services on installation or by default at boot. This is not a distinction for socket-activated services. If an administrator explicitly ""enables"" a socket-activated service unit, it will start at boot. This is the same behavior required today for the non-socket-activated MariaDB packages shipped by Fedora.

> and worth noting that other distros can have similar requirements.

Other distros, like Ubuntu and Debian (which are moving to systemd) actually do not have such a restriction on services auto-starting after package installation.

> That having in mind, I don't think it's worth trying to implement socket activation now, unless there is some special use case for such a feature.

The benefits for clean restarts, security isolation, privilege separation, race-condition avoidance, and support for the network features (which are in the current v209 release now) remain useful, even if you think on-demand startup isn't."
2346,MDEV-5536,MDEV,Honza Horak,41886,2014-02-21 13:10:41,"That sounds like you have been more successful than us, that's great. [~davidstrauss], can you share your service and socket unit files as well?",4,"That sounds like you have been more successful than us, that's great. [~davidstrauss], can you share your service and socket unit files as well?"
2347,MDEV-5536,MDEV,David Strauss,41894,2014-02-21 21:35:51,"> can you share your service and socket unit files as well?

We don't use socket activation with MariaDB yet, but we do something very similar. If anything, it takes longer:

 # Run a stub that tries to connect before serving a PHP-based web page.
 # Have the stub contact a service on the database host to relaunch the MariaDB instance if it's shut down.
 # The service waits until the Unix socket is created on disk.
 # The stub records into APC/APCu that the database is relaunched so the next hour of requests don't have to connect twice. (We never shut down instances that were launched under an hour ago.)
 # The PHP-based web page connects and continues execution as normal.

The remote CLI has a similar stub to relaunch the database on access. If MariaDB supported socket activation, we could remove most of this complexity. I've also written a socket activation proxy [1] that may be a better way than what we do today, but any proxy obfuscates the real client IP addresses, which breaks MariaDB/MySQL's automatic password intrusion prevention support.

Here's the my.cnf template [2] we use. You'll notice we disable fast shutdown in order to ensure fast(er) startup is possible.

[1] http://www.freedesktop.org/software/systemd/man/systemd-socket-proxyd.html
[2] https://gist.github.com/davidstrauss/9141741",5,"> can you share your service and socket unit files as well?

We don't use socket activation with MariaDB yet, but we do something very similar. If anything, it takes longer:

 # Run a stub that tries to connect before serving a PHP-based web page.
 # Have the stub contact a service on the database host to relaunch the MariaDB instance if it's shut down.
 # The service waits until the Unix socket is created on disk.
 # The stub records into APC/APCu that the database is relaunched so the next hour of requests don't have to connect twice. (We never shut down instances that were launched under an hour ago.)
 # The PHP-based web page connects and continues execution as normal.

The remote CLI has a similar stub to relaunch the database on access. If MariaDB supported socket activation, we could remove most of this complexity. I've also written a socket activation proxy [1] that may be a better way than what we do today, but any proxy obfuscates the real client IP addresses, which breaks MariaDB/MySQL's automatic password intrusion prevention support.

Here's the my.cnf template [2] we use. You'll notice we disable fast shutdown in order to ensure fast(er) startup is possible.

[1] URL
[2] URL"
2348,MDEV-5536,MDEV,Daniel Black,67003,2015-01-02 17:22:40,first draft.,6,first draft.
2349,MDEV-5536,MDEV,David Strauss,67052,2015-01-06 21:32:04,"Some feedback on the first draft of the patch:

 * It's unclear why the {{Also=mysqld.socket}} directive is in the service file. This should really be {{Requires=mysqld.socket}} (the {{Before=mysqld.service}} in the socket unit is implicit). It's not good practice to use {{Also=}} here because, then, manually starting the service using {{systemctl start mysqld.service}} won't work properly if the socket isn't already running.
 * Use of socket activation doesn't necessarily mean {{skip-networking}} is appropriate. For example, the instance running may be a slave and need to connect to a master. Replication is not compatible with {{skip-networking}}, [at least in MySQL|https://dev.mysql.com/doc/refman/5.5/en/faqs-replication.html#qandaitem-A-13-1-2].
 * Setting the I/O priority to real-time may not be safe. It's much safer for an administrator to drop the {{BlockIOWeight=}} of other control groups and leave/set MariaDB to {{BlockIOWeight=1000}}. This ensures that MariaDB gets very high priority access to I/O without potentially starving everything else (even admin tools).
 * {{Group=mysql}} is probably redundant. With {{User=mysql}} and an unspecified {{Group=}}, the group defaults to the default group of user {{mysql}}.
 * I assume the notify integration is working properly, but if you have issues, consider setting {{NotifyAccess=all}}, just in case the notification is coming from something systemd does not consider the main process.
 * It may be worth including {{PrivateNetwork=false}} (or documentation to that effect) because that option -- while seemingly good for a socket-activated MariaDB not using replication -- breaks {{Type=notify}} support, thereby mysteriously breaking the service.",7,"Some feedback on the first draft of the patch:

 * It's unclear why the {{Also=mysqld.socket}} directive is in the service file. This should really be {{Requires=mysqld.socket}} (the {{Before=mysqld.service}} in the socket unit is implicit). It's not good practice to use {{Also=}} here because, then, manually starting the service using {{systemctl start mysqld.service}} won't work properly if the socket isn't already running.
 * Use of socket activation doesn't necessarily mean {{skip-networking}} is appropriate. For example, the instance running may be a slave and need to connect to a master. Replication is not compatible with {{skip-networking}}, [at least in MySQL|URL
 * Setting the I/O priority to real-time may not be safe. It's much safer for an administrator to drop the {{BlockIOWeight=}} of other control groups and leave/set MariaDB to {{BlockIOWeight=1000}}. This ensures that MariaDB gets very high priority access to I/O without potentially starving everything else (even admin tools).
 * {{Group=mysql}} is probably redundant. With {{User=mysql}} and an unspecified {{Group=}}, the group defaults to the default group of user {{mysql}}.
 * I assume the notify integration is working properly, but if you have issues, consider setting {{NotifyAccess=all}}, just in case the notification is coming from something systemd does not consider the main process.
 * It may be worth including {{PrivateNetwork=false}} (or documentation to that effect) because that option -- while seemingly good for a socket-activated MariaDB not using replication -- breaks {{Type=notify}} support, thereby mysteriously breaking the service."
2350,MDEV-5536,MDEV,Daniel Black,67060,2015-01-07 11:17:45,"So here we have a completed patch that to support systemd socket activation (and notification).

Thanks for the feedback David. The reason for Also= is ""we just place an Also= line that makes sure that cups.path and cups.socket are automatically also enabled if the user asks to enable cups.service (they are enabled according to the [Install] sections in those unit files)."" as per http://0pointer.de/blog/projects/socket-activation2.html. This seems to be still current based on examples in distribution.

It has a compile option WITH_SYSTEMD for those distros/OSs that don't have systemd. Even with compiled with systemd the socket activation will have no effect unless invoked from systemd with socket activation configured.

I'm happy to write a wiki page when this gets accepted.

This invokes mysqld directly we reuse systemd options to perform the functions of mysqld_safe.

Using systemd notify (MDEV-5713) the systemd knows when the mysqld is running and taking connection and can start other dependent services.

Although socket activation isn't totally needed on production the WantedBy=multi-user.target means it runs on bootup like a normal service with the added start that if something early in boot needs it it starts earlier.

Adding $OPTIONS seems to be a standard way that end users can alter /etc/systemd/system/mariadb.service.d/XXX.conf to override locations or settings.

I've used a STATUS notification on some things that cause slow start up so sysadmins can see the progress.

There may be some improvements to the config as suggested by our distro maintainers. TODO package spec/deb rules don't install support-files/mariadb.{service|socket} - wasn't sure how.

Is exit status 1 from mysqld something that should cause systemd to auto restart mysqld?

I'm hoping a working patch and the most voted on issue topic (systemd) is enough to change the goal to 10.0.",8,"So here we have a completed patch that to support systemd socket activation (and notification).

Thanks for the feedback David. The reason for Also= is ""we just place an Also= line that makes sure that cups.path and cups.socket are automatically also enabled if the user asks to enable cups.service (they are enabled according to the [Install] sections in those unit files)."" as per URL This seems to be still current based on examples in distribution.

It has a compile option WITH_SYSTEMD for those distros/OSs that don't have systemd. Even with compiled with systemd the socket activation will have no effect unless invoked from systemd with socket activation configured.

I'm happy to write a wiki page when this gets accepted.

This invokes mysqld directly we reuse systemd options to perform the functions of mysqld_safe.

Using systemd notify (MDEV-5713) the systemd knows when the mysqld is running and taking connection and can start other dependent services.

Although socket activation isn't totally needed on production the WantedBy=multi-user.target means it runs on bootup like a normal service with the added start that if something early in boot needs it it starts earlier.

Adding $OPTIONS seems to be a standard way that end users can alter /etc/systemd/system/mariadb.service.d/XXX.conf to override locations or settings.

I've used a STATUS notification on some things that cause slow start up so sysadmins can see the progress.

There may be some improvements to the config as suggested by our distro maintainers. TODO package spec/deb rules don't install support-files/mariadb.{service|socket} - wasn't sure how.

Is exit status 1 from mysqld something that should cause systemd to auto restart mysqld?

I'm hoping a working patch and the most voted on issue topic (systemd) is enough to change the goal to 10.0."
2351,MDEV-5536,MDEV,David Strauss,67064,2015-01-07 15:44:29,"bq. The reason for Also= is ""we just place an Also= line that makes sure that cups.path and cups.socket are automatically also enabled if the user asks to enable cups.service (they are enabled according to the [Install] sections in those unit files)."" as per http://0pointer.de/blog/projects/socket-activation2.html. This seems to be still current based on examples in distribution.

That blog post is ancient by systemd documentation standards. The [current documentation|http://www.freedesktop.org/software/systemd/man/systemd.socket.html] states:

""Socket units will have a Before= dependency on the service which they trigger added implicitly. No implicit WantedBy= or RequiredBy= dependency from the socket to the service is added. This means that the service may be started without the socket, in which case it must be able to open sockets by itself. To prevent this, an explicit Requires= dependency may be added.""

bq. Is exit status 1 from mysqld something that should cause systemd to auto restart mysqld?

Yes, unless non-zero exit codes get explicitly listed in the service file as clean exits.",9,"bq. The reason for Also= is ""we just place an Also= line that makes sure that cups.path and cups.socket are automatically also enabled if the user asks to enable cups.service (they are enabled according to the [Install] sections in those unit files)."" as per URL This seems to be still current based on examples in distribution.

That blog post is ancient by systemd documentation standards. The [current documentation|URL states:

""Socket units will have a Before= dependency on the service which they trigger added implicitly. No implicit WantedBy= or RequiredBy= dependency from the socket to the service is added. This means that the service may be started without the socket, in which case it must be able to open sockets by itself. To prevent this, an explicit Requires= dependency may be added.""

bq. Is exit status 1 from mysqld something that should cause systemd to auto restart mysqld?

Yes, unless non-zero exit codes get explicitly listed in the service file as clean exits."
2352,MDEV-5536,MDEV,Daniel Black,67073,2015-01-07 23:07:56,"Ok. reworked with Requires. Removed MaxConnections as that is only for Accept=true.
",10,"Ok. reworked with Requires. Removed MaxConnections as that is only for Accept=true.
"
2353,MDEV-5536,MDEV,David Strauss,67074,2015-01-07 23:17:49,"Thanks for the changes. Just re-reviewed.

My only additional feedback: {{Requires=}} must be in the Unit section. Otherwise, systemd will ignore it and warn.",11,"Thanks for the changes. Just re-reviewed.

My only additional feedback: {{Requires=}} must be in the Unit section. Otherwise, systemd will ignore it and warn."
2354,MDEV-5536,MDEV,Daniel Black,67078,2015-01-08 05:08:06,"ack. done.

bq. Is exit status 1 from mysqld something that should cause systemd to auto restart mysqld?

The key word here is 'should'.  I looked it up and it seems exist status 1 is fatal config options to which a restart isn't' useful.

A such a restart isn't attempted on exit status 1:

-SuccessExitStatus=0,1
+RestartPreventExitStatus=1",12,"ack. done.

bq. Is exit status 1 from mysqld something that should cause systemd to auto restart mysqld?

The key word here is 'should'.  I looked it up and it seems exist status 1 is fatal config options to which a restart isn't' useful.

A such a restart isn't attempted on exit status 1:

-SuccessExitStatus=0,1
+RestartPreventExitStatus=1"
2355,MDEV-5536,MDEV,David Strauss,67118,2015-01-09 02:27:47,"> -SuccessExitStatus=0,1
> +RestartPreventExitStatus=1

Indeed, this is the right configuration.

I'll amend my comment: ""Yes, unless non-zero exit codes get explicitly listed in the service file as clean exits *or exits where a restart is useless*.""",13,"> -SuccessExitStatus=0,1
> +RestartPreventExitStatus=1

Indeed, this is the right configuration.

I'll amend my comment: ""Yes, unless non-zero exit codes get explicitly listed in the service file as clean exits *or exits where a restart is useless*."""
2356,MDEV-5536,MDEV,Daniel Black,67247,2015-01-14 15:54:42,"since last patch: corrected mariadb-systemd-start to do proper checks and use defaults if not set.

systemd didn't like mysqld doing a shutdown on the listening sockets so I disabled it for systemd sockets to make systemd happy.

Added dh-systemd to debian/ubuntu depends in control file and attempted to set dh_systemd_enable/dh_systemd_start as per debian policy (still could be errors there).

Added support-files/mariadb-socket-convert which takes a my.cnf config and generates a mariadb.socket.conf files based on the same network settings. Intended to help distro maintainers help users migrate across.

unresolved is, as far as I know, some of the distro policies and packaging:

Generally:

Does mysqld using dbus now for notify mean a selinux policy needs to be added? (seems like an easy policy for those that need it http://blog.siphos.be/2014/06/d-bus-and-selinux/).

Is a ExecStartPre=/usr/bin/mariadb-systemd-start the right way to create a database if not installed?

should ListenStream=3306 be [::1]:3306 by default?

should [mysqld_safe] bits get converted for distros like the network settings in mariadb-socket-convert (hoping raising LimitNOFile=16k eliminates most surprises)?

Debian: how is /etc/mysql/debian-start run (mysql-check/mysql_update) run? (may need a separate service that depends on mariadb.service - ExecStartPost may be too early)

rpm spec files need to set WITH_SYSTEMD=no for distros that don't use it.",14,"since last patch: corrected mariadb-systemd-start to do proper checks and use defaults if not set.

systemd didn't like mysqld doing a shutdown on the listening sockets so I disabled it for systemd sockets to make systemd happy.

Added dh-systemd to debian/ubuntu depends in control file and attempted to set dh_systemd_enable/dh_systemd_start as per debian policy (still could be errors there).

Added support-files/mariadb-socket-convert which takes a my.cnf config and generates a mariadb.socket.conf files based on the same network settings. Intended to help distro maintainers help users migrate across.

unresolved is, as far as I know, some of the distro policies and packaging:

Generally:

Does mysqld using dbus now for notify mean a selinux policy needs to be added? (seems like an easy policy for those that need it URL

Is a ExecStartPre=/usr/bin/mariadb-systemd-start the right way to create a database if not installed?

should ListenStream=3306 be [::1]:3306 by default?

should [mysqld_safe] bits get converted for distros like the network settings in mariadb-socket-convert (hoping raising LimitNOFile=16k eliminates most surprises)?

Debian: how is /etc/mysql/debian-start run (mysql-check/mysql_update) run? (may need a separate service that depends on mariadb.service - ExecStartPost may be too early)

rpm spec files need to set WITH_SYSTEMD=no for distros that don't use it."
2357,MDEV-5536,MDEV,David Strauss,67255,2015-01-14 21:35:24,"bq. Is a ExecStartPre=/usr/bin/mariadb-systemd-start the right way to create a database if not installed?

Yes. A separate service would be excessive and confusing.

bq. should ListenStream=3306 be [::1]:3306 by default?

ListenStream=3306 will result in a combination IPv4/IPv6 listener on port 3306 for all interfaces. My hunch is that [::1]:3306 will create a listener on 127.0.0.1:3306 and [::1]:3306, but I haven't tested this. I think most distros have MariaDB installing, by default, with only a localhost listener.

bq. should [mysqld_safe] bits get converted for distros like the network settings in mariadb-socket-convert (hoping raising LimitNOFile=16k eliminates most surprises)?

Do you have specific bits in mind?

bq. Debian: how is /etc/mysql/debian-start run (mysql-check/mysql_update) run? (may need a separate service that depends on mariadb.service - ExecStartPost may be too early)

It's possible to create a ""run after"" service using a combination of WantedBy= and After=. WantedBy= means, if enabled, it gets added into any transaction where the MariaDB service starts (but does not imply ordering). After= ensures that it waits for MariaDB to be ready. The advantage of it being a separate service is making it clearly optional to administrators.",15,"bq. Is a ExecStartPre=/usr/bin/mariadb-systemd-start the right way to create a database if not installed?

Yes. A separate service would be excessive and confusing.

bq. should ListenStream=3306 be [::1]:3306 by default?

ListenStream=3306 will result in a combination IPv4/IPv6 listener on port 3306 for all interfaces. My hunch is that [::1]:3306 will create a listener on 127.0.0.1:3306 and [::1]:3306, but I haven't tested this. I think most distros have MariaDB installing, by default, with only a localhost listener.

bq. should [mysqld_safe] bits get converted for distros like the network settings in mariadb-socket-convert (hoping raising LimitNOFile=16k eliminates most surprises)?

Do you have specific bits in mind?

bq. Debian: how is /etc/mysql/debian-start run (mysql-check/mysql_update) run? (may need a separate service that depends on mariadb.service - ExecStartPost may be too early)

It's possible to create a ""run after"" service using a combination of WantedBy= and After=. WantedBy= means, if enabled, it gets added into any transaction where the MariaDB service starts (but does not imply ordering). After= ensures that it waits for MariaDB to be ready. The advantage of it being a separate service is making it clearly optional to administrators."
2358,MDEV-5536,MDEV,Daniel Black,67259,2015-01-15 00:26:11,"bq. ListenStream=[::1]:3306 ...

only does ipv6. Changed to:
{noformat}
ListenStream=[::1]:3306
ListenStream=127.0.0.1:3306
BindToDevice=

Backlog=150
{noformat}

Reworked the mariadb-socket-convert to do all resolutions of a (hostname like localhost) as bind-address.

bq. [mysqld_safe]...Do you have specific bits in mind?

support-files/mariadb.service documents a way to manually convert all of the [mysqld_safe] to mariadb.service settings. As really only open-file-limit/LimitNOFILE and timezone really effect the running of the mariadb after and upgrade I've put a default 16K file limit to keep people out of trouble and maybe timezone isn't often set.

I'll only want to write any conversion script here if there is a real demand and value.

I think i've gone down the path of attempted distro maintenance here as far as I want to without any specific guidance from distro maintainers so I'll let them work out the rest. They have quite extensive policies that they are more familiar with after all :-)",16,"bq. ListenStream=[::1]:3306 ...

only does ipv6. Changed to:
{noformat}
ListenStream=[::1]:3306
ListenStream=127.0.0.1:3306
BindToDevice=

Backlog=150
{noformat}

Reworked the mariadb-socket-convert to do all resolutions of a (hostname like localhost) as bind-address.

bq. [mysqld_safe]...Do you have specific bits in mind?

support-files/mariadb.service documents a way to manually convert all of the [mysqld_safe] to mariadb.service settings. As really only open-file-limit/LimitNOFILE and timezone really effect the running of the mariadb after and upgrade I've put a default 16K file limit to keep people out of trouble and maybe timezone isn't often set.

I'll only want to write any conversion script here if there is a real demand and value.

I think i've gone down the path of attempted distro maintenance here as far as I want to without any specific guidance from distro maintainers so I'll let them work out the rest. They have quite extensive policies that they are more familiar with after all :-)"
2359,MDEV-5536,MDEV,David Strauss,67260,2015-01-15 00:35:41,"bq. only does ipv6. Changed

If you have two ListenStream= options, systemd will send in two separate socket file descriptors. Is that handled in the code?",17,"bq. only does ipv6. Changed

If you have two ListenStream= options, systemd will send in two separate socket file descriptors. Is that handled in the code?"
2360,MDEV-5536,MDEV,Daniel Black,67261,2015-01-15 00:46:15,"bq. If you have two ListenStream= options, systemd will send in two separate socket file descriptors. Is that handled in the code?

Actually 3 since the unix socket is a ListenStream too. Up to 10 are handled within the code which is just a #define with non-fatal errors if this is exceeded and written to mysql error log.

Reminded me the current code has extra_ip_sock(extra-port) has special handling. How important is it to maintain this?
{noformat}
    if (mysql_socket_getfd(sock) == mysql_socket_getfd(extra_ip_sock))
    {
      thd->extra_port= 1;
      thd->scheduler= extra_thread_scheduler;
    }
{noformat}",18,"bq. If you have two ListenStream= options, systemd will send in two separate socket file descriptors. Is that handled in the code?

Actually 3 since the unix socket is a ListenStream too. Up to 10 are handled within the code which is just a #define with non-fatal errors if this is exceeded and written to mysql error log.

Reminded me the current code has extra_ip_sock(extra-port) has special handling. How important is it to maintain this?
{noformat}
    if (mysql_socket_getfd(sock) == mysql_socket_getfd(extra_ip_sock))
    {
      thd->extra_port= 1;
      thd->scheduler= extra_thread_scheduler;
    }
{noformat}"
2361,MDEV-5536,MDEV,David Strauss,67262,2015-01-15 00:48:51,"bq. Reminded me the current code has extra_ip_sock(extra-port) has special handling. How important is it to maintain this?

I've never been a fan of the specialized handling of the IP versus the Unix socket. It would be great to have a generic pool of listeners; I see no downside.",19,"bq. Reminded me the current code has extra_ip_sock(extra-port) has special handling. How important is it to maintain this?

I've never been a fan of the specialized handling of the IP versus the Unix socket. It would be great to have a generic pool of listeners; I see no downside."
2362,MDEV-5536,MDEV,Daniel Black,67295,2015-01-16 12:05:51,"More fixes. I dare say this is getting pretty close.

Using systemd list fds on ip_sock, extra_ip_sock and unix_socket wasn't working as there needed to be a determination of the socket type. As systemd can specify multiple sockets in any order a proper map of socket type to file descriptor was made.

The performance schema interface was also missing which is now corrected. For the moment I've put a PS event type for systemd_unix, systemd_ipv6, systemd_ipv4 but can probably expand these to include path/host/port/numbers information if desired. Although the code for HAVE_POLL=no is there I couldn't get past MDEV-7473 which stalled on linux even with -DHAVE_SYSTEMD=no -DHAVE_POLL=no but hey, linux always has poll and systemd isn't ported to non-linux so we're safe for now.

The socket activation part already implements MDEV-6536 IPv6 bind address now because systemd handles the opening of sockets and with a little more config parsing some of these new structures this sets the basis a more extensive listening interface.

The main missing bit so far, a decent automated test case. Nothing non-root exists in systemd however it shouldn't be too hard to open a few sockets, set some env vars and exec another process.",20,"More fixes. I dare say this is getting pretty close.

Using systemd list fds on ip_sock, extra_ip_sock and unix_socket wasn't working as there needed to be a determination of the socket type. As systemd can specify multiple sockets in any order a proper map of socket type to file descriptor was made.

The performance schema interface was also missing which is now corrected. For the moment I've put a PS event type for systemd_unix, systemd_ipv6, systemd_ipv4 but can probably expand these to include path/host/port/numbers information if desired. Although the code for HAVE_POLL=no is there I couldn't get past MDEV-7473 which stalled on linux even with -DHAVE_SYSTEMD=no -DHAVE_POLL=no but hey, linux always has poll and systemd isn't ported to non-linux so we're safe for now.

The socket activation part already implements MDEV-6536 IPv6 bind address now because systemd handles the opening of sockets and with a little more config parsing some of these new structures this sets the basis a more extensive listening interface.

The main missing bit so far, a decent automated test case. Nothing non-root exists in systemd however it shouldn't be too hard to open a few sockets, set some env vars and exec another process."
2363,MDEV-5536,MDEV,Daniel Black,69007,2015-03-11 23:11:28,https://github.com/MariaDB/server/pull/26,21,URL
2364,MDEV-5536,MDEV,Daniel Black,73361,2015-07-11 08:51:36,Added https://github.com/MariaDB/server/pull/83 to merge to 10.1 incorporating review comments,22,Added URL to merge to 10.1 incorporating review comments
2365,MDEV-5536,MDEV,Sergey Vojtovich,77307,2015-10-28 10:16:11,Removing from 10.1 backlog and lowering priority until new pull request is submitted.,23,Removing from 10.1 backlog and lowering priority until new pull request is submitted.
2366,MDEV-5536,MDEV,Daniel Black,91136,2017-01-27 00:55:21,Generic systemd functionality has been incorporated for a while. As such I've stalled in getting the socket activation included. At this point I'd like to re-ask how much interest is there currently for socket activation? For what use cases? If there is an upper number of listening sockets what limit?,24,Generic systemd functionality has been incorporated for a while. As such I've stalled in getting the socket activation included. At this point I'd like to re-ask how much interest is there currently for socket activation? For what use cases? If there is an upper number of listening sockets what limit?
2367,MDEV-5536,MDEV,David Strauss,91139,2017-01-27 01:25:31,"> At this point I'd like to re-ask how much interest is there currently for socket activation? For what use cases?

Let's review the original list I posted on the issue.

> Cleaner restarts (the listener socket stays open persistently)

Still true unless MariaDB has added an nginx-style method of restarting/reloading, which itself uses socket inheritance (just not via systemd).

> Network namespace isolation, disallowing any network access beyond the inherited listener port (and connections accepted from it).

Still true, and network namespace isolation would have helped mitigate vulnerabilities like CVE-2016-6662 by not allowing malicious code to open new listeners on the host or initiate outbound connections.

> Lazy startup for densely hosted instances. (It's also possible with socket activation to start it eagerly, as usual.)

Still true, though admittedly an edge case. We still start MariaDB instances with our own on-demand logic, but socket activation would be cleaner.

> Running MariaDB on privileged ports without having to start it initially as root

Still true, but a very limited use case given the standard ports for MySQL/MariaDB.

> Non-racy startup for services (like a PHP site) that depend on connecting to MariaDB. Because systemd opens listener sockets early in boot, they're available even while MariaDB is starting

If MariaDB is now properly integrating with systemd via {{Type=notify}} or {{Type=forking}} with {{PIDFile=}} set, then this is no longer useful.

> Deeper integration into coming network support in future systemd releases

There have been a few things added to systemd's network and namespace support, including {{JoinsNamespaceOf=}}, which allows running a backend like memcached and having it accessible to MariaDB from its namespace. Nothing too groundbreaking, though.

> If there is an upper number of listening sockets what limit?

There are limits in the kernel on file descriptor counts, but they're very high (and consumed by MariaDB opening its own sockets just as much).

From the systemd side, admins and packagers can configure any number of sockets to be opened and passed into MariaDB via systemd. The configured sockets will appear to MariaDB with file descriptors numbered in a way corresponding to the listeners configured in {{mariadb.socket}}.",25,"> At this point I'd like to re-ask how much interest is there currently for socket activation? For what use cases?

Let's review the original list I posted on the issue.

> Cleaner restarts (the listener socket stays open persistently)

Still true unless MariaDB has added an nginx-style method of restarting/reloading, which itself uses socket inheritance (just not via systemd).

> Network namespace isolation, disallowing any network access beyond the inherited listener port (and connections accepted from it).

Still true, and network namespace isolation would have helped mitigate vulnerabilities like CVE-2016-6662 by not allowing malicious code to open new listeners on the host or initiate outbound connections.

> Lazy startup for densely hosted instances. (It's also possible with socket activation to start it eagerly, as usual.)

Still true, though admittedly an edge case. We still start MariaDB instances with our own on-demand logic, but socket activation would be cleaner.

> Running MariaDB on privileged ports without having to start it initially as root

Still true, but a very limited use case given the standard ports for MySQL/MariaDB.

> Non-racy startup for services (like a PHP site) that depend on connecting to MariaDB. Because systemd opens listener sockets early in boot, they're available even while MariaDB is starting

If MariaDB is now properly integrating with systemd via {{Type=notify}} or {{Type=forking}} with {{PIDFile=}} set, then this is no longer useful.

> Deeper integration into coming network support in future systemd releases

There have been a few things added to systemd's network and namespace support, including {{JoinsNamespaceOf=}}, which allows running a backend like memcached and having it accessible to MariaDB from its namespace. Nothing too groundbreaking, though.

> If there is an upper number of listening sockets what limit?

There are limits in the kernel on file descriptor counts, but they're very high (and consumed by MariaDB opening its own sockets just as much).

From the systemd side, admins and packagers can configure any number of sockets to be opened and passed into MariaDB via systemd. The configured sockets will appear to MariaDB with file descriptors numbered in a way corresponding to the listeners configured in {{mariadb.socket}}."
2368,MDEV-5536,MDEV,Daniel Black,91142,2017-01-27 04:53:49,"[~davidstrauss] Thanks for the feedback. I'll take this on board. Notes:

Cleaner restarts - not implemented in mariadb - http://nginx.org/en/docs/control.html - wow - quite involved, additionally so with a single process and Type=notify. sending file descriptors, exec(2) using shm for data provide partial solutions. Perhaps this should be a separate MDEV (if one doesn't exist).",26,"[~davidstrauss] Thanks for the feedback. I'll take this on board. Notes:

Cleaner restarts - not implemented in mariadb - URL - wow - quite involved, additionally so with a single process and Type=notify. sending file descriptors, exec(2) using shm for data provide partial solutions. Perhaps this should be a separate MDEV (if one doesn't exist)."
2369,MDEV-5536,MDEV,Daniel Black,182950,2021-03-19 02:46:47,"Please review:

https://github.com/MariaDB/server/pull/1783",27,"Please review:

URL"
2370,MDEV-5536,MDEV,Daniel Black,183897,2021-03-28 08:00:44,Thanks for your patience all. This will be in 10.6.,28,Thanks for your patience all. This will be in 10.6.
2371,MDEV-5536,MDEV,Daniel Black,183932,2021-03-28 23:29:48,"In this implementation and discussion there has been nothing mentioned on auto-deactivation. Is this a requirement in densely hosted instances? If so create a separate JIRA issue.

If something like a max_idle_execution time maybe. There are avenues for integration with [RuntimeMaxSec|https://www.freedesktop.org/software/systemd/man/systemd.service.html#RuntimeMaxSec=] and extending timeout of runtime, and/or purely terminating the listening execution loop if no new connections are happening or queries running.

But please, new issue and state the requirement first.

Documentation will be coming in the next week or so https://mariadb.com/kb/en/systemd/",29,"In this implementation and discussion there has been nothing mentioned on auto-deactivation. Is this a requirement in densely hosted instances? If so create a separate JIRA issue.

If something like a max_idle_execution time maybe. There are avenues for integration with [RuntimeMaxSec|URL and extending timeout of runtime, and/or purely terminating the listening execution loop if no new connections are happening or queries running.

But please, new issue and state the requirement first.

Documentation will be coming in the next week or so URL"
2372,MDEV-5536,MDEV,David Strauss,183938,2021-03-29 03:43:31,"{quote}In this implementation and discussion there has been nothing mentioned on auto-deactivation. Is this a requirement in densely hosted instances?{quote}

I wouldn't make it a requirement for this issue, even if it's an interesting capability. The socket activation should already mean that it's possible to shut down a MariaDB service that's about to get a new connection and have everything still work. A shutdown immediately followed by a connection attempt should cause systemd's own handling of the socket to enqueue the necessary work to re-launch the service. On the client side, the only thing that should be noticeable is a delay in the initial connection (while MariaDB starts).

After MDEV-5536, it should be possible to analyze MariaDB activity however one chooses and send ""systemctl stop something.service"" while being confident that they're not causing system breakage if they anticipated wrongly (and a new connection or query arrives after the analysis or decision to shut down).

So, self shutdown on idleness would be a complementary feature, but it's pretty separate. Socket activation is the main thing that unblocks the ""MariaDB farm"" use case because that part is what you can't easily do externally.

I agree that a separate issue is warranted.",30,"{quote}In this implementation and discussion there has been nothing mentioned on auto-deactivation. Is this a requirement in densely hosted instances?{quote}

I wouldn't make it a requirement for this issue, even if it's an interesting capability. The socket activation should already mean that it's possible to shut down a MariaDB service that's about to get a new connection and have everything still work. A shutdown immediately followed by a connection attempt should cause systemd's own handling of the socket to enqueue the necessary work to re-launch the service. On the client side, the only thing that should be noticeable is a delay in the initial connection (while MariaDB starts).

After MDEV-5536, it should be possible to analyze MariaDB activity however one chooses and send ""systemctl stop something.service"" while being confident that they're not causing system breakage if they anticipated wrongly (and a new connection or query arrives after the analysis or decision to shut down).

So, self shutdown on idleness would be a complementary feature, but it's pretty separate. Socket activation is the main thing that unblocks the ""MariaDB farm"" use case because that part is what you can't easily do externally.

I agree that a separate issue is warranted."
2373,MDEV-5536,MDEV,David Strauss,183940,2021-03-29 04:13:40,I've posted a separate issue for automatic shutdown when idle: MDEV-25282,31,I've posted a separate issue for automatic shutdown when idle: MDEV-25282
2374,MDEV-5713,MDEV,Sergei Golubchik,41879,2014-02-21 11:42:39,"[~hhorak], see also MDEV-5679. Would that help if mysqld would daemonize itself when it's ready to accept connections?",1,"[~hhorak], see also MDEV-5679. Would that help if mysqld would daemonize itself when it's ready to accept connections?"
2375,MDEV-5713,MDEV,Honza Horak,41887,2014-02-21 13:34:38,"Well, simply said from my POV, it would mean a lot of work more needed to be done.

IIUIC, it might help if we either:

1) stopped using mysqld_safe and ran mysqld directly in the systemd unit file (then using ""type=forking"" systemd would wait until the first process exists, which would be at a time the forked process would be already prepared; however, we'd lose some functionality in mysqld_safe)

2) added the notification mechanism into mysqld_safe, so mysqld_safe would send message as soon as the mysqld's first process finishes. Then we'd have to add a logic for monitoring the forked process, which is actually what systemd does, so we would basically duplicate the effort.

3) changed ""eval"" call to ""exec"" call in mysqld_safe, so we could use the parameter parsing done by mysqld_safe, but monitoring itself would be done by systemd. 

Getting rid of mysqld_safe monitoring (options 1 and 3) would mean that we'd have to use restarting mechanism from systemd to recover after daemon failure, which is not so easy now -- we simply cannot distinguish easily between daemon failure and desired stop (systemd decides based on exit codes, which would mean changing the exit codes and use them properly on many places).",2,"Well, simply said from my POV, it would mean a lot of work more needed to be done.

IIUIC, it might help if we either:

1) stopped using mysqld_safe and ran mysqld directly in the systemd unit file (then using ""type=forking"" systemd would wait until the first process exists, which would be at a time the forked process would be already prepared; however, we'd lose some functionality in mysqld_safe)

2) added the notification mechanism into mysqld_safe, so mysqld_safe would send message as soon as the mysqld's first process finishes. Then we'd have to add a logic for monitoring the forked process, which is actually what systemd does, so we would basically duplicate the effort.

3) changed ""eval"" call to ""exec"" call in mysqld_safe, so we could use the parameter parsing done by mysqld_safe, but monitoring itself would be done by systemd. 

Getting rid of mysqld_safe monitoring (options 1 and 3) would mean that we'd have to use restarting mechanism from systemd to recover after daemon failure, which is not so easy now -- we simply cannot distinguish easily between daemon failure and desired stop (systemd decides based on exit codes, which would mean changing the exit codes and use them properly on many places)."
2376,MDEV-5713,MDEV,Brian Evans,62516,2014-10-13 23:12:25,"Suggest a cmake option be added (default on if you want), so that this can be decided at build time.
This helps Gentoo Linux keep the server going if a user decides to switch between init systems.",3,"Suggest a cmake option be added (default on if you want), so that this can be decided at build time.
This helps Gentoo Linux keep the server going if a user decides to switch between init systems."
2377,MDEV-5713,MDEV,Daniel Black,66943,2014-12-27 14:17:15,"As requested by [~grknight] added a WITH_SYSTEMD option here.

Added packages deps to the debian/ubuntu.

Added bits to the support-files/mysql.spec for systemd however its not packaged until openSUSE_13 and RHEL7 (MDEV-6347).",4,"As requested by [~grknight] added a WITH_SYSTEMD option here.

Added packages deps to the debian/ubuntu.

Added bits to the support-files/mysql.spec for systemd however its not packaged until openSUSE_13 and RHEL7 (MDEV-6347)."
2378,MDEV-5713,MDEV,Daniel Black,67004,2015-01-02 17:26:52,patchwork continuing on MDEV-5536,5,patchwork continuing on MDEV-5536
2379,MDEV-5713,MDEV,Daniel Black,67005,2015-01-02 17:53:10,"needed:

locations in the code for global server status and presenting notification to users
http://0pointer.de/public/systemd-man/sd_notify.html
e.g.
galera could us RELOADING=1 when it goes to donor/accept and back to READY=1 when finished

some STATUS= messages when in innodb recovery.

Any other cases?",6,"needed:

locations in the code for global server status and presenting notification to users
URL
e.g.
galera could us RELOADING=1 when it goes to donor/accept and back to READY=1 when finished

some STATUS= messages when in innodb recovery.

Any other cases?"
2380,MDEV-5713,MDEV,Daniel Black,69010,2015-03-11 23:28:25,https://github.com/MariaDB/server/pull/26,7,URL
2381,MDEV-5800,MDEV,Richa Sehgal,69072,2015-03-13 10:40:41,"Hi,

I am Richa Sehgal currently pursuing Master’s at University of Illinois Urbana Champaign, USA. I did my undergraduate from Indian Institute of Technology Delhi (IIT-Delhi). I would like to take this up as my GSoC project.

Materialization gives us two things:
1. A name to the column which we can use in queries
2. A formal ""regular"" column which is stored and indexed in the regular fashion - Disadvantage: Extra memory requirements for the materialized column.

My initial thoughts on this project are the following:
We do need the name of the column which can be used to query. So maybe we can expose a command such as:

create virtual_index <name> on <column_name> <expression>

What this would do would run a regular query which evaluates expressions (like in WHERE clause) and the feed the result into the indexer. This index can then be stored in the regular fashion.
Am I approaching this in the right direction? Can you please point me to the next steps? 
Thanks
Richa
",1,"Hi,

I am Richa Sehgal currently pursuing Master’s at University of Illinois Urbana Champaign, USA. I did my undergraduate from Indian Institute of Technology Delhi (IIT-Delhi). I would like to take this up as my GSoC project.

Materialization gives us two things:
1. A name to the column which we can use in queries
2. A formal ""regular"" column which is stored and indexed in the regular fashion - Disadvantage: Extra memory requirements for the materialized column.

My initial thoughts on this project are the following:
We do need the name of the column which can be used to query. So maybe we can expose a command such as:

create virtual_index  on  

What this would do would run a regular query which evaluates expressions (like in WHERE clause) and the feed the result into the indexer. This index can then be stored in the regular fashion.
Am I approaching this in the right direction? Can you please point me to the next steps? 
Thanks
Richa
"
2382,MDEV-5800,MDEV,Sergei Golubchik,69082,2015-03-13 12:59:41,https://lists.launchpad.net/maria-developers/msg08303.html,2,URL
2383,MDEV-5800,MDEV,Axel Schwenke,76903,2015-10-19 11:56:07,"From a user right now on #maria: an index on a virtual column should be used not only when the virtual column is referenced by name, but also when the expression defining that column is used. That would mean that an index on a virtual column is equivalent to a functional index (MDEV-6017) on the expression defining that column.

Example:
{noformat}
CREATE TABLE t1 (c1 INT, c2 INT, c3 INT AS (c1+c2) VIRTUAL, INDEX (c3));
SELECT * FROM t1 WHERE c3=42;
SELECT * FROM t1 WHERE (c1+c2)=42;
{noformat}

The expectation is that both queries use the index on the virtual column. The first because the virtual column is referenced by name, the second because the  virtual column is referenced by the defining expression. The index on c3 would behave like a functional index on (c1+c2)",3,"From a user right now on #maria: an index on a virtual column should be used not only when the virtual column is referenced by name, but also when the expression defining that column is used. That would mean that an index on a virtual column is equivalent to a functional index (MDEV-6017) on the expression defining that column.

Example:
{noformat}
CREATE TABLE t1 (c1 INT, c2 INT, c3 INT AS (c1+c2) VIRTUAL, INDEX (c3));
SELECT * FROM t1 WHERE c3=42;
SELECT * FROM t1 WHERE (c1+c2)=42;
{noformat}

The expectation is that both queries use the index on the virtual column. The first because the virtual column is referenced by name, the second because the  virtual column is referenced by the defining expression. The index on c3 would behave like a functional index on (c1+c2)"
2384,MDEV-6066,MDEV,Sergei Golubchik,72326,2015-06-17 12:01:39,"note, this is not about blindly merging everything. see whether changes make sense, ask somebody in support or consulting, when in doubt, see blogs about these changes, etc.",1,"note, this is not about blindly merging everything. see whether changes make sense, ask somebody in support or consulting, when in doubt, see blogs about these changes, etc."
2385,MDEV-6066,MDEV,Oleksandr Byelkin,72948,2015-06-30 14:07:57,"Unexpectedly to the task attached following ones:

- added \-\-autoset- prefix for start-up options
- added unit test for my_getopt
- added unit test for \-\-autoset-
- Added ability to switch off QC without setting size to 0
- Fixed bug in query_cache_type checking procedure which expeted only numeric values when it is enum
- Fixed showing/setting functions which incorrectly detect type of the variable (no applied mask)",2,"Unexpectedly to the task attached following ones:

- added \-\-autoset- prefix for start-up options
- added unit test for my_getopt
- added unit test for \-\-autoset-
- Added ability to switch off QC without setting size to 0
- Fixed bug in query_cache_type checking procedure which expeted only numeric values when it is enum
- Fixed showing/setting functions which incorrectly detect type of the variable (no applied mask)"
2386,MDEV-6066,MDEV,Oleksandr Byelkin,73031,2015-07-03 19:21:25,"revision-id: 0f1cd7cfc871b458e2205a045ed602bdd2f77bea
parent(s): 302bf7c4664b904482ecc133476e822d497b114d
committer: Oleksandr Byelkin
branch nick: server
timestamp: 2015-07-03 18:17:15 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (part 1 (no hash serch))
",3,"revision-id: 0f1cd7cfc871b458e2205a045ed602bdd2f77bea
parent(s): 302bf7c4664b904482ecc133476e822d497b114d
committer: Oleksandr Byelkin
branch nick: server
timestamp: 2015-07-03 18:17:15 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (part 1 (no hash serch))
"
2387,MDEV-6066,MDEV,Elena Stepanova,73296,2015-07-08 02:37:30,"[~sanja], please document which changes you decided to merge and which not, and comment on those which you decided to skip to explain the decision. 
Meanwhile, please check MDEV-8429 -- whether you have already merged this one; if you decided before that it should be skipped, you might want to re-consider.",4,"[~sanja], please document which changes you decided to merge and which not, and comment on those which you decided to skip to explain the decision. 
Meanwhile, please check MDEV-8429 -- whether you have already merged this one; if you decided before that it should be skipped, you might want to re-consider."
2388,MDEV-6066,MDEV,Oleksandr Byelkin,73450,2015-07-14 01:13:02,"revision-id: 686761fbb0fc36e4fa781b68aba24af21599822d
parent(s): 302bf7c4664b904482ecc133476e822d497b114d
committer: Oleksandr Byelkin
branch nick: server
timestamp: 2015-07-14 00:11:25 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (part 1 (no hash serch) v2)",5,"revision-id: 686761fbb0fc36e4fa781b68aba24af21599822d
parent(s): 302bf7c4664b904482ecc133476e822d497b114d
committer: Oleksandr Byelkin
branch nick: server
timestamp: 2015-07-14 00:11:25 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (part 1 (no hash serch) v2)"
2389,MDEV-6066,MDEV,Oleksandr Byelkin,74537,2015-08-11 19:48:10,"postreview 2 patches:

revision-id: ba230c21adcfc6e6a6174ff25d8a8aebd1a70f0a (mariadb-10.1.6-12-gba230c2)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-08-10 21:45:11 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (autoset)

--autoset- command line prefix added

---

revision-id: 76d6ac448d18a9e97ce4be585d79607780e00a49 (mariadb-10.1.6-13-g76d6ac4)
parent(s): ba230c21adcfc6e6a6174ff25d8a8aebd1a70f0a
committer: Oleksandr Byelkin
timestamp: 2015-08-11 18:45:38 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (defaults changed, QC can be stopped with no-zero size)

---",6,"postreview 2 patches:

revision-id: ba230c21adcfc6e6a6174ff25d8a8aebd1a70f0a (mariadb-10.1.6-12-gba230c2)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-08-10 21:45:11 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (autoset)

--autoset- command line prefix added

---

revision-id: 76d6ac448d18a9e97ce4be585d79607780e00a49 (mariadb-10.1.6-13-g76d6ac4)
parent(s): ba230c21adcfc6e6a6174ff25d8a8aebd1a70f0a
committer: Oleksandr Byelkin
timestamp: 2015-08-11 18:45:38 +0200
message:

MDEV-6066: Merge new defaults from 5.6 and 5.7 (defaults changed, QC can be stopped with no-zero size)

---"
2390,MDEV-6076,MDEV,Jan Lindström,59401,2014-09-01 12:00:24,http://bugs.mysql.com/bug.php?id=21641,1,URL
2391,MDEV-6076,MDEV,Jan Lindström,66371,2014-12-02 16:43:10,"Current problem:

When we update a auto_increment value for a table t1 on one client and auto_increment value for a table t2 on second client, this leads lock wait.",2,"Current problem:

When we update a auto_increment value for a table t1 on one client and auto_increment value for a table t2 on second client, this leads lock wait."
2392,MDEV-6076,MDEV,Rick James,80946,2016-02-16 22:58:44,See also http://bugs.mysql.com/bug.php?id=199,3,See also URL
2393,MDEV-6076,MDEV,Marko Mäkelä,88958,2016-11-30 18:44:15,"MySQL 8.0.0 implements [WL#6204: InnoDB persistent max value for autoinc columns|http://dev.mysql.com/worklog/task/?id=6204]. The patch is complex, but one thing is better than in the contributed patch that I have seen.
The contributed patch would write the AUTO_INCREMENT value straight to the clustered index B-tree root page at the previously unused position PAGE_MAX_TRX_ID. This would introduce block->lock contention on the root page.
The Oracle patch would introduce a special buffering mechanism and a special redo log record type for writing the auto-increment values. On redo log checkpoint, the value would be written to a persistent table.

I think that we can combine the two ideas as follows:

# We will have two fields to hold the auto-increment value: dict_table_t::autoinc for the next available value, and dict_table_t::persisted for the last value that has been physically written to the data file.
# Whenever InnoDB is about to assign an AUTO_INCREMENT value, it will write a redo log record to ‘update’ the field in the root page, but without actually modifying the page and without adding the page to the flush list.
# Whenever any mini-transaction is modifying the root page, it will write dict_table_t::autoinc to the page and update dict_table_t::autoinc_persisted to this value.
# On redo log checkpoint, if dict_table_t::autoinc_persisted differs from dict_table_t::autoinc for any table, update the value to the root page. We might need some data structure that would allow log checkpoint to quickly find tables or pages with ‘stale’ metadata.

With the above approach, no changes should be necessary to crash recovery, backup tools and redo-log based replication. They would simply start scanning the redo log from some log checkpoint until the latest completed mini-transaction. If an auto-increment value was updated, there would be a redo log record for it. The last write would win; the value as of the latest completed mini-transaction would ultimately appear in the root page.

The same approach should be possible for any dynamic metadata that we could imagine: UPDATE_TIME, maximum LSN or TRX_ID in a table, number of delete-marked records in an index, number of purgeable records, whatever we can imagine. We would use some bytes within an InnoDB tablespace for persisting the value, and we would have 2 fields in a main-memory struct: the last redo-logged value and the last value written to the page.
We would write redo log records for updating this value, without actually updating the page or appending the page to the flush list. (If we happened to already be holding an X-latch on the page, then we could of course write it directly.)",4,"MySQL 8.0.0 implements [WL#6204: InnoDB persistent max value for autoinc columns|URL The patch is complex, but one thing is better than in the contributed patch that I have seen.
The contributed patch would write the AUTO_INCREMENT value straight to the clustered index B-tree root page at the previously unused position PAGE_MAX_TRX_ID. This would introduce block->lock contention on the root page.
The Oracle patch would introduce a special buffering mechanism and a special redo log record type for writing the auto-increment values. On redo log checkpoint, the value would be written to a persistent table.

I think that we can combine the two ideas as follows:

# We will have two fields to hold the auto-increment value: dict_table_t::autoinc for the next available value, and dict_table_t::persisted for the last value that has been physically written to the data file.
# Whenever InnoDB is about to assign an AUTO_INCREMENT value, it will write a redo log record to ‘update’ the field in the root page, but without actually modifying the page and without adding the page to the flush list.
# Whenever any mini-transaction is modifying the root page, it will write dict_table_t::autoinc to the page and update dict_table_t::autoinc_persisted to this value.
# On redo log checkpoint, if dict_table_t::autoinc_persisted differs from dict_table_t::autoinc for any table, update the value to the root page. We might need some data structure that would allow log checkpoint to quickly find tables or pages with ‘stale’ metadata.

With the above approach, no changes should be necessary to crash recovery, backup tools and redo-log based replication. They would simply start scanning the redo log from some log checkpoint until the latest completed mini-transaction. If an auto-increment value was updated, there would be a redo log record for it. The last write would win; the value as of the latest completed mini-transaction would ultimately appear in the root page.

The same approach should be possible for any dynamic metadata that we could imagine: UPDATE_TIME, maximum LSN or TRX_ID in a table, number of delete-marked records in an index, number of purgeable records, whatever we can imagine. We would use some bytes within an InnoDB tablespace for persisting the value, and we would have 2 fields in a main-memory struct: the last redo-logged value and the last value written to the page.
We would write redo log records for updating this value, without actually updating the page or appending the page to the flush list. (If we happened to already be holding an X-latch on the page, then we could of course write it directly.)"
2394,MDEV-6076,MDEV,Marko Mäkelä,88970,2016-12-01 04:53:01,"In other words, my idea is to split the main-memory representation of a buffer pool page into two parts: the base part, and a ‘shadow’ portion that is resides in some auxiliary data structure outside the page frame in the buffer pool. A correct implementation of the idea must fulfill the following invariants:
# Write-ahead logging (WAL): Any updates are first written to the redo log, then to the data files.
# Persistence across checkpoint: A checkpoint logically discards the head of the redo log. All changes that were present in the discarded portion of the redo log must be applied to the data files (alternatively, be appended to the redo log).
# Read observability: The ‘shadow’ portion must always be chosen instead of the base part.
# Write observability: When the page is being flushed from the buffer pool to the data file, the ‘shadow’ portion as it was at the flush LSN must be applied.
There is one more complication in the implementation: page eviction. If we update the ‘shadow’ data while the page is not in the buffer pool, we would have to ensure that when the page is eventually read to the buffer pool, the ‘shadow’ data will be applied to it. On redo log checkpoint, we would have to read the page into the buffer pool in order to write back the ‘shadow’ data. It would seem simplest to prevent the eviction of pages from the buffer pool when ‘shadow’ data is present.",5,"In other words, my idea is to split the main-memory representation of a buffer pool page into two parts: the base part, and a ‘shadow’ portion that is resides in some auxiliary data structure outside the page frame in the buffer pool. A correct implementation of the idea must fulfill the following invariants:
# Write-ahead logging (WAL): Any updates are first written to the redo log, then to the data files.
# Persistence across checkpoint: A checkpoint logically discards the head of the redo log. All changes that were present in the discarded portion of the redo log must be applied to the data files (alternatively, be appended to the redo log).
# Read observability: The ‘shadow’ portion must always be chosen instead of the base part.
# Write observability: When the page is being flushed from the buffer pool to the data file, the ‘shadow’ portion as it was at the flush LSN must be applied.
There is one more complication in the implementation: page eviction. If we update the ‘shadow’ data while the page is not in the buffer pool, we would have to ensure that when the page is eventually read to the buffer pool, the ‘shadow’ data will be applied to it. On redo log checkpoint, we would have to read the page into the buffer pool in order to write back the ‘shadow’ data. It would seem simplest to prevent the eviction of pages from the buffer pool when ‘shadow’ data is present."
2395,MDEV-6076,MDEV,Marko Mäkelä,88971,2016-12-01 05:37:14,"Premature optimization is the source of all evil. I think that we should run some benchmarks before starting to implement complicated logic.
The motivation for the complexity is that the clustered index B-tree root page latch is already a contended resource.
Maybe there is a simpler way to reduce the contention. In MySQL 5.7, [WL#6363: InnoDB: implement SX-lock for rw_lock|http://dev.mysql.com/worklog/task/?id=6363] introduced infrastructure that was first put into use in [WL#6326: InnoDB: fix index->lock contention|http://dev.mysql.com/worklog/task/?id=6326], for the dict_index_t::lock and the buf_block_t::lock of the root page. The SX lock mode allows concurrent readers (S lock) but not concurrent writers (X or SX lock). In WL#6326, updates of the page allocation lists BTR_SEG_TOP and BTR_SEG_LEAF are protected by SX latch, not blocking concurrent readers of the root page.

What if we updated the AUTO_INCREMENT value directly in the root page, but protected it with SX latch instead of X latch? What would the performance be in a workload that involves concurrent readers and writers and frequent page allocations? The test setup could be something like this:
{code:sh}
mysqld --skip-innodb-adaptive-hash-index --innodb-page-size=4k
{code}
The above startup parameters ensure that readers will actually traverse the index tree and that the B-tree will consist of multiple levels, increasing the contention on the root page latch. The initialization could be something like this:
{code:SQL}
CREATE TABLE t(a SERIAL, b TEXT) ENGINE=InnoDB;
INSERT INTO t(b) VALUES(REPEAT('abc',1234)),(REPEAT('abc',1234)),(REPEAT('abc',1234));
INSERT INTO t SELECT NULL,b FROM t;
INSERT INTO t SELECT NULL,b FROM t;
INSERT INTO t SELECT NULL,b FROM t;
INSERT INTO t SELECT NULL,b FROM t;
{code}
The workload would be something like this: A few readers executing
{code:SQL}
SELECT SQL_NO_CACHE LENGTH(b) FROM t WHERE a=5;
{code}
and one or more writers executing
{code:SQL}
-- this should only modify the auto-increment counter in the root page, but not allocate/free pages
BEGIN; INSERT INTO t SET b=''; ROLLBACK;
{code}
or (in another benchmark)
{code:SQL}
-- this causes access to BTR_SEG_LEAF (allocate&free the BLOB)
BEGIN; INSERT INTO t SET b=REPEAT('blob',12345); ROLLBACK;
{code}
or (yet another benchmark)
{code:SQL}
-- this should cause page splits&merges
BEGIN; INSERT INTO t(b) SELECT b FROM t; ROLLBACK;
{code}
With the benchmark, I would like to see if there is any noticeable performance regression from the simple patch (write the auto-increment counter directly to the root page, protected by SX-latch) with respect to the baseline (no persistent auto-increment counter).",6,"Premature optimization is the source of all evil. I think that we should run some benchmarks before starting to implement complicated logic.
The motivation for the complexity is that the clustered index B-tree root page latch is already a contended resource.
Maybe there is a simpler way to reduce the contention. In MySQL 5.7, [WL#6363: InnoDB: implement SX-lock for rw_lock|URL introduced infrastructure that was first put into use in [WL#6326: InnoDB: fix index->lock contention|URL for the dict_index_t::lock and the buf_block_t::lock of the root page. The SX lock mode allows concurrent readers (S lock) but not concurrent writers (X or SX lock). In WL#6326, updates of the page allocation lists BTR_SEG_TOP and BTR_SEG_LEAF are protected by SX latch, not blocking concurrent readers of the root page.

What if we updated the AUTO_INCREMENT value directly in the root page, but protected it with SX latch instead of X latch? What would the performance be in a workload that involves concurrent readers and writers and frequent page allocations? The test setup could be something like this:
{code:sh}
mysqld --skip-innodb-adaptive-hash-index --innodb-page-size=4k
{code}
The above startup parameters ensure that readers will actually traverse the index tree and that the B-tree will consist of multiple levels, increasing the contention on the root page latch. The initialization could be something like this:
{code:SQL}
CREATE TABLE t(a SERIAL, b TEXT) ENGINE=InnoDB;
INSERT INTO t(b) VALUES(REPEAT('abc',1234)),(REPEAT('abc',1234)),(REPEAT('abc',1234));
INSERT INTO t SELECT NULL,b FROM t;
INSERT INTO t SELECT NULL,b FROM t;
INSERT INTO t SELECT NULL,b FROM t;
INSERT INTO t SELECT NULL,b FROM t;
{code}
The workload would be something like this: A few readers executing
{code:SQL}
SELECT SQL_NO_CACHE LENGTH(b) FROM t WHERE a=5;
{code}
and one or more writers executing
{code:SQL}
-- this should only modify the auto-increment counter in the root page, but not allocate/free pages
BEGIN; INSERT INTO t SET b=''; ROLLBACK;
{code}
or (in another benchmark)
{code:SQL}
-- this causes access to BTR_SEG_LEAF (allocate&free the BLOB)
BEGIN; INSERT INTO t SET b=REPEAT('blob',12345); ROLLBACK;
{code}
or (yet another benchmark)
{code:SQL}
-- this should cause page splits&merges
BEGIN; INSERT INTO t(b) SELECT b FROM t; ROLLBACK;
{code}
With the benchmark, I would like to see if there is any noticeable performance regression from the simple patch (write the auto-increment counter directly to the root page, protected by SX-latch) with respect to the baseline (no persistent auto-increment counter)."
2396,MDEV-6076,MDEV,zhangyuan,88978,2016-12-01 10:39:48,"Use SX lock is a good idea!  But  so far we have not supported SX lock in AliSQL5.6. I made a sysbench test in AliSQL5.6, the result shows that the impact of x latch is so little.


{code:java}
./bin/sysbench
--test=insert.lua 
--num-threads=100  
--max-time=7200 
--max-requests
{code}


with different  oltp-tables-count
 
* one table


|| ||innodb_autoinc_persistent=OFF||	innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=1||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=10||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=100||
|TPS	|22199|22003|22069|22209|
|RT(ms)|2.25|2.27|2.26|2.25|

* 256 tables


|| ||innodb_autoinc_persistent=OFF||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=1||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=10||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=100||
|TPS	|24609|24444|24579|24605|
|RT(ms)|2.03|2.03|2.03|2.03|


*summary*
   innodb_autoinc_persistent=ON, innodb_autoinc_persistent_interval=1  performance degrade below %1。
   innodb_autoinc_persistent=ON, innodb_autoinc_persistent_interval=100  almost no effect。",7,"Use SX lock is a good idea!  But  so far we have not supported SX lock in AliSQL5.6. I made a sysbench test in AliSQL5.6, the result shows that the impact of x latch is so little.


{code:java}
./bin/sysbench
--test=insert.lua 
--num-threads=100  
--max-time=7200 
--max-requests
{code}


with different  oltp-tables-count
 
* one table


|| ||innodb_autoinc_persistent=OFF||	innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=1||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=10||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=100||
|TPS	|22199|22003|22069|22209|
|RT(ms)|2.25|2.27|2.26|2.25|

* 256 tables


|| ||innodb_autoinc_persistent=OFF||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=1||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=10||innodb_autoinc_persistent=ON
innodb_autoinc_persistent_interval=100||
|TPS	|24609|24444|24579|24605|
|RT(ms)|2.03|2.03|2.03|2.03|


*summary*
   innodb_autoinc_persistent=ON, innodb_autoinc_persistent_interval=1  performance degrade below %1。
   innodb_autoinc_persistent=ON, innodb_autoinc_persistent_interval=100  almost no effect。"
2397,MDEV-6076,MDEV,Marko Mäkelä,88992,2016-12-01 12:10:16,"Thanks, these results are encouraging.
Because my ‘shadow write’ idea would use the same file format, we could go with the simpler solution first, and then refine it later if performance problems arise. I would not introduce any configuration options, but instead make the AUTO_INCREMENT counter persistent, unconditionally.

Somewhat similar to the MySQL 8.0.0 implementation, I would persist the auto-increment counter in row_ins_clust_index_entry_low(), letting btr_cur_search_to_nth_level() acquire and hold the root page SX-latch or X-latch for us.",8,"Thanks, these results are encouraging.
Because my ‘shadow write’ idea would use the same file format, we could go with the simpler solution first, and then refine it later if performance problems arise. I would not introduce any configuration options, but instead make the AUTO_INCREMENT counter persistent, unconditionally.

Somewhat similar to the MySQL 8.0.0 implementation, I would persist the auto-increment counter in row_ins_clust_index_entry_low(), letting btr_cur_search_to_nth_level() acquire and hold the root page SX-latch or X-latch for us."
2398,MDEV-6076,MDEV,Marko Mäkelä,89445,2016-12-12 07:33:44,"The persistent AUTO_INCREMENT counter will introduce some changes to semantics. Some of them correspond to changes introduced in MySQL 8.0.0, while others are unique to the implementation in MariaDB. First the common changes:
# Normally, AUTO_INCREMENT counters are monotonically increasing.
# Assigned values will not be reused even after a transaction rollback.
# If the server is killed before committing a transaction for which an AUTO_INCREMENT value was assigned, it is possible (but not guaranteed) that the same value can be reassigned again after crash recovery.
# Like in MyISAM, UPDATE of an AUTO_INCREMENT column will move the AUTO_INCREMENT sequence if the updated value is larger.
# ALTER TABLE…AUTO_INCREMENT will not change the counter to a smaller value than what already exists in the column. (This is for compatibility with ALGORITHM=COPY.)
Finally, there is one possible deviation of what the final implementation (with atomic DDL operations using the Global Data Dictionary) will be in MySQL 8.0GA:
# ALTER TABLE…AUTO_INCREMENT will update the persistent auto-increment sequence before the transaction is committed. If the server is killed before the commit, it is possible (but not guaranteed) that the auto-increment sequence has already been reset.",9,"The persistent AUTO_INCREMENT counter will introduce some changes to semantics. Some of them correspond to changes introduced in MySQL 8.0.0, while others are unique to the implementation in MariaDB. First the common changes:
# Normally, AUTO_INCREMENT counters are monotonically increasing.
# Assigned values will not be reused even after a transaction rollback.
# If the server is killed before committing a transaction for which an AUTO_INCREMENT value was assigned, it is possible (but not guaranteed) that the same value can be reassigned again after crash recovery.
# Like in MyISAM, UPDATE of an AUTO_INCREMENT column will move the AUTO_INCREMENT sequence if the updated value is larger.
# ALTER TABLE…AUTO_INCREMENT will not change the counter to a smaller value than what already exists in the column. (This is for compatibility with ALGORITHM=COPY.)
Finally, there is one possible deviation of what the final implementation (with atomic DDL operations using the Global Data Dictionary) will be in MySQL 8.0GA:
# ALTER TABLE…AUTO_INCREMENT will update the persistent auto-increment sequence before the transaction is committed. If the server is killed before the commit, it is possible (but not guaranteed) that the auto-increment sequence has already been reset."
2399,MDEV-6076,MDEV,Marko Mäkelä,89561,2016-12-14 17:58:10,Please review the branch bb-10.2-mdev-6076.,10,Please review the branch bb-10.2-mdev-6076.
2400,MDEV-6076,MDEV,Marko Mäkelä,89563,2016-12-14 18:03:13,"mysql_upgrade will need some work to set the AUTO_INCREMENT attributes for InnoDB tables.

We may also want to file a follow-up task for removing the requirement of having an index on AUTO_INCREMENT columns.",11,"mysql_upgrade will need some work to set the AUTO_INCREMENT attributes for InnoDB tables.

We may also want to file a follow-up task for removing the requirement of having an index on AUTO_INCREMENT columns."
2401,MDEV-6076,MDEV,Marko Mäkelä,89579,2016-12-15 07:56:54,"I think that we can do upgrade from old data files transparently (without additions to mysql_upgrade) as follows:

If PAGE_ROOT_AUTO_INC is 0, the table must be either empty or in old format. If the table is nonempty and an index on AUTO_INCREMENT column exists, initialize dict_table_t::autoinc from MAX(auto_increment_column) as we used to. Else, effectively let the AUTO_INCREMENT sequence start from 1.

Note: I filed MDEV-11578 to remove the requirement for AUTO_INCREMENT columns to be indexed.",12,"I think that we can do upgrade from old data files transparently (without additions to mysql_upgrade) as follows:

If PAGE_ROOT_AUTO_INC is 0, the table must be either empty or in old format. If the table is nonempty and an index on AUTO_INCREMENT column exists, initialize dict_table_t::autoinc from MAX(auto_increment_column) as we used to. Else, effectively let the AUTO_INCREMENT sequence start from 1.

Note: I filed MDEV-11578 to remove the requirement for AUTO_INCREMENT columns to be indexed."
2402,MDEV-6076,MDEV,Marko Mäkelä,89581,2016-12-15 10:30:55,"I was unsure if PAGE_ROOT_AUTO_INC always was 0 on other than secondary index leaf pages. (Before MySQL 5.5, InnoDB had the bad habit of writing uninitialized garbage to unused data fields in pages.) Luckily it turns out that page_create() in mysql-3.23.49 initializes PAGE_MAX_TRX_ID to 0 whenever it is creating a B-tree page.

This means that the upgrade method that I implemented (if PAGE_ROOT_AUTO_INC is 0 and the root page is not empty, read MAX(auto_inc_column)) is sound.

I can only think of one possible glitch: Import a data file from a MDEV-6076 server to one that lacks MDEV-6076, then insert or update some rows using a larger value than what was persisted, and finally import back to a MDEV-6076 server. In this case, one extra statement would be needed to adjust the AUTO_INCREMENT value:
{code:sql}
ALTER TABLE t DISCARD TABLESPACE;
ALTER TABLE t IMPORT TABLESPACE;
ALTER TABLE t AUTO_INCREMENT=1;
{code}
The last statement would reset the persistent AUTO_INCREMENT sequence to the current maximum value. (MDEV-11578 may change the semantics.)",13,"I was unsure if PAGE_ROOT_AUTO_INC always was 0 on other than secondary index leaf pages. (Before MySQL 5.5, InnoDB had the bad habit of writing uninitialized garbage to unused data fields in pages.) Luckily it turns out that page_create() in mysql-3.23.49 initializes PAGE_MAX_TRX_ID to 0 whenever it is creating a B-tree page.

This means that the upgrade method that I implemented (if PAGE_ROOT_AUTO_INC is 0 and the root page is not empty, read MAX(auto_inc_column)) is sound.

I can only think of one possible glitch: Import a data file from a MDEV-6076 server to one that lacks MDEV-6076, then insert or update some rows using a larger value than what was persisted, and finally import back to a MDEV-6076 server. In this case, one extra statement would be needed to adjust the AUTO_INCREMENT value:
{code:sql}
ALTER TABLE t DISCARD TABLESPACE;
ALTER TABLE t IMPORT TABLESPACE;
ALTER TABLE t AUTO_INCREMENT=1;
{code}
The last statement would reset the persistent AUTO_INCREMENT sequence to the current maximum value. (MDEV-11578 may change the semantics.)"
2403,MDEV-6076,MDEV,zhangyuan,89591,2016-12-15 14:49:28,"
When delete all records from root page(not by truncate), root page will recreate by page_create_empty. PAGE_ROOT_AUTO_INC will be reset to 0.


{code:c}
diff --git a/storage/innobase/page/page0page.cc b/storage/innobase/page/page0page.cc
index d207371..af913bc 100644
--- a/storage/innobase/page/page0page.cc
+++ b/storage/innobase/page/page0page.cc
@@ -531,8 +531,7 @@
        same temp-table in parallel.
        max_trx_id is ignored for temp tables because it not required
        for MVCC. */
-       if (dict_index_is_sec_or_ibuf(index)
-           && !dict_table_is_temporary(index->table)
+       if (!dict_table_is_temporary(index->table)
            && page_is_leaf(page)) {
                max_trx_id = page_get_max_trx_id(page);
                ut_ad(max_trx_id);
{code}
",14,"
When delete all records from root page(not by truncate), root page will recreate by page_create_empty. PAGE_ROOT_AUTO_INC will be reset to 0.


{code:c}
diff --git a/storage/innobase/page/page0page.cc b/storage/innobase/page/page0page.cc
index d207371..af913bc 100644
--- a/storage/innobase/page/page0page.cc
+++ b/storage/innobase/page/page0page.cc
@@ -531,8 +531,7 @@
        same temp-table in parallel.
        max_trx_id is ignored for temp tables because it not required
        for MVCC. */
-       if (dict_index_is_sec_or_ibuf(index)
-           && !dict_table_is_temporary(index->table)
+       if (!dict_table_is_temporary(index->table)
            && page_is_leaf(page)) {
                max_trx_id = page_get_max_trx_id(page);
                ut_ad(max_trx_id);
{code}
"
2404,MDEV-6076,MDEV,Marko Mäkelä,89592,2016-12-15 15:04:26,"Thanks, Zhangyuan! Yes, page_create_empty() or its callers must be adjusted so that PAGE_ROOT_AUTO_INC will be preserved.
I will add a test for this. I made a similar omission in page reorganize, but that was caught by one test.",15,"Thanks, Zhangyuan! Yes, page_create_empty() or its callers must be adjusted so that PAGE_ROOT_AUTO_INC will be preserved.
I will add a test for this. I made a similar omission in page reorganize, but that was caught by one test."
2405,MDEV-6076,MDEV,Marko Mäkelä,89597,2016-12-15 16:51:14,"Some more code changes are needed to accommodate virtual columns (fix the test failures in --suite=vcol).
With the InnoDB virtual column implementation in MySQL 5.7, Field::field_index can no longer be passed to dict_table_get_nth_col(). We will have to calculate how many non-virtual columns precede the column.",16,"Some more code changes are needed to accommodate virtual columns (fix the test failures in --suite=vcol).
With the InnoDB virtual column implementation in MySQL 5.7, Field::field_index can no longer be passed to dict_table_get_nth_col(). We will have to calculate how many non-virtual columns precede the column."
2406,MDEV-6076,MDEV,Marko Mäkelä,89613,2016-12-15 20:12:54,"I pushed a fix to the conflict with MDEV-5800/WL#8114/WL#8149 (virtual columns).
The new function innodb_col_no() will determine the dict_table_t::cols[] offset of a Field.",17,"I pushed a fix to the conflict with MDEV-5800/WL#8114/WL#8149 (virtual columns).
The new function innodb_col_no() will determine the dict_table_t::cols[] offset of a Field."
2407,MDEV-6076,MDEV,Jan Lindström,89627,2016-12-16 07:55:23,This looks good now.,18,This looks good now.
2408,MDEV-6076,MDEV,Marko Mäkelä,89628,2016-12-16 08:35:27,"Thanks! I just pushed to bb-10.2-mdev-6076 rebased to latest 10.2, so that we avoid one test failure due to a bug in slow shutdown that was introduced in MDEV-5800 and fixed later.

The only code change since the review is the correction of a debug assertion that I had added late yesterday:
{code:diff}
diff --git a/storage/innobase/page/page0zip.cc b/storage/innobase/page/page0zip.cc
index 39eadda318f..798aeeed74f 100644
--- a/storage/innobase/page/page0zip.cc
+++ b/storage/innobase/page/page0zip.cc
@@ -4851,8 +4851,8 @@ page_zip_copy_recs(
 	} else {
 		/* The PAGE_MAX_TRX_ID must be nonzero on leaf pages
 		of secondary indexes, and 0 on others. */
-		ut_a(dict_table_is_temporary(index->table)
-		     || page_is_leaf(src) == !page_get_max_trx_id(src));
+		ut_ad(dict_table_is_temporary(index->table)
+		      || !page_is_leaf(src) == !page_get_max_trx_id(src));
 	}
 
 	/* Copy all fields of src_zip to page_zip, except the pointer
{code}
I also removed many restarts from the test innodb.autoinc_persist, testing more things across each restart.",19,"Thanks! I just pushed to bb-10.2-mdev-6076 rebased to latest 10.2, so that we avoid one test failure due to a bug in slow shutdown that was introduced in MDEV-5800 and fixed later.

The only code change since the review is the correction of a debug assertion that I had added late yesterday:
{code:diff}
diff --git a/storage/innobase/page/page0zip.cc b/storage/innobase/page/page0zip.cc
index 39eadda318f..798aeeed74f 100644
--- a/storage/innobase/page/page0zip.cc
+++ b/storage/innobase/page/page0zip.cc
@@ -4851,8 +4851,8 @@ page_zip_copy_recs(
 	} else {
 		/* The PAGE_MAX_TRX_ID must be nonzero on leaf pages
 		of secondary indexes, and 0 on others. */
-		ut_a(dict_table_is_temporary(index->table)
-		     || page_is_leaf(src) == !page_get_max_trx_id(src));
+		ut_ad(dict_table_is_temporary(index->table)
+		      || !page_is_leaf(src) == !page_get_max_trx_id(src));
 	}
 
 	/* Copy all fields of src_zip to page_zip, except the pointer
{code}
I also removed many restarts from the test innodb.autoinc_persist, testing more things across each restart."
2409,MDEV-6076,MDEV,Marko Mäkelä,90063,2016-12-29 13:31:12,"Now that 10.2.3 was released and 10.1 was merged to 10.2, I finally pushed this.",20,"Now that 10.2.3 was released and 10.1 was merged to 10.2, I finally pushed this."
2410,MDEV-6076,MDEV,Marko Mäkelä,94078,2017-04-19 05:22:36,"MDEV-12123 fixed a related bug. When using IMPORT TABLESPACE with data files on which IMPORT TABLESPACE has been used before the MDEV-12123 fix, the AUTO_INCREMENT value will be restored to a bogus value (something that was a transaction ID in the previous IMPORT).",21,"MDEV-12123 fixed a related bug. When using IMPORT TABLESPACE with data files on which IMPORT TABLESPACE has been used before the MDEV-12123 fix, the AUTO_INCREMENT value will be restored to a bogus value (something that was a transaction ID in the previous IMPORT)."
2411,MDEV-6080,MDEV,Sergei Petrunia,46814,2014-04-14 20:24:42,"Notes from irc discussion about adding EXPLAIN support:

For first step, EXPLAIN will show one line with extra=""executed by the storage engine"" for the whole SELECT.  (It would be nice to show the query plan that the storage engine is using, but fitting storage engine's query plans into EXPLAIN output form may be difficult. So, we're leaving this outside of scope of this MDEV).

Code-wise: 
- JOIN should have some data member that will mean that all join is pushed to the storage engine. 
- JOIN::save_explain_data_intern() should save this info in Explain_select object.  Please look at this code


{noformat}
int JOIN::save_explain_data_intern(Explain_query *output, bool need_tmp_table,
                                   bool need_order, bool distinct, 
                                   const char *message)
{
...
  if (message)
  {
    Explain_select *xpl_sel;
    explain_node= xpl_sel= new (output->mem_root) Explain_select;
    join->select_lex->set_explain_type(true);

    xpl_sel->select_id= join->select_lex->select_number; 
    xpl_sel->select_type= join->select_lex->type;
    xpl_sel->message= message;

    ...
{noformat}

There should be also  ""if (join_is_executed_by_storage_engine) ""   which should do the same as ""if (message) { ...}"" does - create an Explain_select object, and  xpl_sel->message=""Executed by storage engine"".  
This should be sufficient for [SHOW] EXPLAIN to work.",1,"Notes from irc discussion about adding EXPLAIN support:

For first step, EXPLAIN will show one line with extra=""executed by the storage engine"" for the whole SELECT.  (It would be nice to show the query plan that the storage engine is using, but fitting storage engine's query plans into EXPLAIN output form may be difficult. So, we're leaving this outside of scope of this MDEV).

Code-wise: 
- JOIN should have some data member that will mean that all join is pushed to the storage engine. 
- JOIN::save_explain_data_intern() should save this info in Explain_select object.  Please look at this code


{noformat}
int JOIN::save_explain_data_intern(Explain_query *output, bool need_tmp_table,
                                   bool need_order, bool distinct, 
                                   const char *message)
{
...
  if (message)
  {
    Explain_select *xpl_sel;
    explain_node= xpl_sel= new (output->mem_root) Explain_select;
    join->select_lex->set_explain_type(true);

    xpl_sel->select_id= join->select_lex->select_number; 
    xpl_sel->select_type= join->select_lex->type;
    xpl_sel->message= message;

    ...
{noformat}

There should be also  ""if (join_is_executed_by_storage_engine) ""   which should do the same as ""if (message) { ...}"" does - create an Explain_select object, and  xpl_sel->message=""Executed by storage engine"".  
This should be sufficient for [SHOW] EXPLAIN to work."
2412,MDEV-6080,MDEV,Michael Widenius,47805,2014-04-21 13:21:10,"The code is now pushed into
bzr+ssh://bazaar.launchpad.net/~maria-captains/maria/10.0-monty

The plan is to merge this with 10.1 as soon as this is properly reviewed.

If you have have any suggestions of how to extend or modify this interface or even suggestions for a better interface, please write your comments here!
",2,"The code is now pushed into
bzr+ssh://bazaar.launchpad.net/~maria-captains/maria/10.0-monty

The plan is to merge this with 10.1 as soon as this is properly reviewed.

If you have have any suggestions of how to extend or modify this interface or even suggestions for a better interface, please write your comments here!
"
2413,MDEV-6112,MDEV,Federico Razzoli,86629,2016-09-20 13:29:13,"Can this be disabled? Multiple triggers of the same type can be useful, but can also be chaotic and hard to debug...",1,"Can this be disabled? Multiple triggers of the same type can be useful, but can also be chaotic and hard to debug..."
2414,MDEV-6112,MDEV,Michael Widenius,86849,2016-09-27 09:53:25,ready for review and testing,2,ready for review and testing
2415,MDEV-6112,MDEV,Michael Widenius,86850,2016-09-27 09:58:54,"As it's completely optional to add a trigger for table, I don't see any reason to disable this.
We already have way too many startup options for MariaDB.
If one doesn't think a person is capable of using TRIGGER's, then one shouldn't give him the TRIGGER privilege.
",3,"As it's completely optional to add a trigger for table, I don't see any reason to disable this.
We already have way too many startup options for MariaDB.
If one doesn't think a person is capable of using TRIGGER's, then one shouldn't give him the TRIGGER privilege.
"
2416,MDEV-6112,MDEV,Federico Razzoli,86865,2016-09-27 23:56:02,"Almost every feature can be limited in some way, the strangest example is sql_safe_updates.
But I realized that my proposal would add an incompatibility with MySQL, so it isn't a good idea.",4,"Almost every feature can be limited in some way, the strangest example is sql_safe_updates.
But I realized that my proposal would add an incompatibility with MySQL, so it isn't a good idea."
2417,MDEV-6112,MDEV,Michael Widenius,86872,2016-09-28 14:09:09,"Another reason for not adding an option for this is that multiple triggers per table is standard ANSI SQL feature.

Optionally disabling all features in SQL that may be problematic for some users is task independent of this one.  For example, I think a lot of people would find it useful if DROP TABLE was disabled after they have dropped an important table ;)
 ",5,"Another reason for not adding an option for this is that multiple triggers per table is standard ANSI SQL feature.

Optionally disabling all features in SQL that may be problematic for some users is task independent of this one.  For example, I think a lot of people would find it useful if DROP TABLE was disabled after they have dropped an important table ;)
 "
2418,MDEV-6112,MDEV,Michael Widenius,87090,2016-10-04 22:25:35,Feature pushed into 10.2,6,Feature pushed into 10.2
2419,MDEV-6113,MDEV,Geoff Montee,69918,2015-04-13 20:40:00,"Will this merge include online truncation of InnoDB undo tablespaces? Would that be included in 10.1 or 10.2?

http://mysqlserverteam.com/online-truncate-of-innodb-undo-tablespaces/
",1,"Will this merge include online truncation of InnoDB undo tablespaces? Would that be included in 10.1 or 10.2?

URL
"
2420,MDEV-6113,MDEV,Sergey Vojtovich,77987,2015-11-13 13:50:07,Please note that 5.7.9 InnoDB is unstable on Power8. We'll likely have to fix it again.,2,Please note that 5.7.9 InnoDB is unstable on Power8. We'll likely have to fix it again.
2421,MDEV-6113,MDEV,Sergei Golubchik,78148,2015-11-17 17:01:28,Question: what to do with Boost.Geometry,3,Question: what to do with Boost.Geometry
2422,MDEV-6113,MDEV,Daniel Black,82887,2016-04-21 06:06:54,"[~svoj], I've started to look for them. https://github.com/linux-on-ibm-power/mysql-server/tree/5.7-POWER_FIXES has the ones I've need to make 5.7 stable though honestly most are ones you've found already and fixed.

http://bugs.mysql.com/bug.php?id=79378 might be one that applies though I haven't reproduced it on MariaDB yet.",4,"[~svoj], I've started to look for them. URL has the ones I've need to make 5.7 stable though honestly most are ones you've found already and fixed.

URL might be one that applies though I haven't reproduced it on MariaDB yet."
2423,MDEV-6113,MDEV,Sergey Vojtovich,82889,2016-04-21 07:07:50,"[~danblack], thanks for looking for them. Let's hope these fixes will be accepted by mysql.",5,"[~danblack], thanks for looking for them. Let's hope these fixes will be accepted by mysql."
2424,MDEV-6113,MDEV,Geoff Montee,84816,2016-07-08 16:25:16,"Will this merge include the fix for this upstream bug?

https://jira.mariadb.org/browse/MDEV-4697",6,"Will this merge include the fix for this upstream bug?

URL"
2425,MDEV-6113,MDEV,Michiel Hazelhof,85363,2016-08-03 10:16:35,"There seem to be quite a few improvements noted in: https://blogs.oracle.com/mysqlinnodb/entry/innodb_5_7_performance_improvements
Are all these planned to be included?",7,"There seem to be quite a few improvements noted in: URL
Are all these planned to be included?"
2426,MDEV-6113,MDEV,Jan Lindström,86337,2016-09-09 07:16:15,Attached server changes I have done while merged 5.7.9 and 5.7.14. ,8,Attached server changes I have done while merged 5.7.9 and 5.7.14. 
2427,MDEV-6113,MDEV,Sergei Golubchik,86471,2016-09-14 14:16:47,"[~GieltjE], most will be. All changes inside InnoDB will be included. If there will be optimizations that don't work without corresponding server changes — they might not work (we will merge some correspondent server changes, but not all).",9,"[~GieltjE], most will be. All changes inside InnoDB will be included. If there will be optimizations that don't work without corresponding server changes — they might not work (we will merge some correspondent server changes, but not all)."
2428,MDEV-6113,MDEV,Geoff Montee,89318,2016-12-07 16:52:20,"Did this merge include ""ALTER TABLE ... {DISCARD|IMPORT} PARTITION"" for InnoDB tables?

https://jira.mariadb.org/browse/MDEV-10568",10,"Did this merge include ""ALTER TABLE ... {DISCARD|IMPORT} PARTITION"" for InnoDB tables?

URL"
2429,MDEV-6114,MDEV,guo feng,78529,2015-11-26 09:09:57,Hope sys schema comes soon!,1,Hope sys schema comes soon!
2430,MDEV-6114,MDEV,Chris Calender,85500,2016-08-10 21:45:11,"Can this fix please be added to 10.1 instead of 10.2?

I don't think it should matter so long as only new tables are being added.

Many thanks for your consideration.",2,"Can this fix please be added to 10.1 instead of 10.2?

I don't think it should matter so long as only new tables are being added.

Many thanks for your consideration."
2431,MDEV-6114,MDEV,Sergei Golubchik,85562,2016-08-14 08:32:43,"No, it can not. ""Adding new P_S tables"" means doing *lots* of changes inside the server to all all the necessary instrumentation.",3,"No, it can not. ""Adding new P_S tables"" means doing *lots* of changes inside the server to all all the necessary instrumentation."
2432,MDEV-6114,MDEV,Jonathan Day,85676,2016-08-18 21:32:10,"I have a large prospect interested in tracking prepared statements.  I believe the  ""prepared_statements_instance"" table is needed in particular.  Thanks!
",4,"I have a large prospect interested in tracking prepared statements.  I believe the  ""prepared_statements_instance"" table is needed in particular.  Thanks!
"
2433,MDEV-6114,MDEV,Marko Mäkelä,122946,2019-02-05 13:54:01,"As part of doing this, I suggest considering the patch that is attached to MDEV-15798. It removes InnoDB {{rw_lock_list}}, the table {{information_schema.innodb_mutexes}} and the {{show engine innodb mutex}} output.",5,"As part of doing this, I suggest considering the patch that is attached to MDEV-15798. It removes InnoDB {{rw_lock_list}}, the table {{information_schema.innodb_mutexes}} and the {{show engine innodb mutex}} output."
2434,MDEV-6145,MDEV,Elena Stepanova,47905,2014-04-21 16:52:36,"From the precise box:

-- Running cmake version 2.8.7

(I believe gcc is also lower than needed, but it's not indicated in the buildbot log).
",1,"From the precise box:

-- Running cmake version 2.8.7

(I believe gcc is also lower than needed, but it's not indicated in the buildbot log).
"
2435,MDEV-6150,MDEV,Axel Schwenke,58633,2014-08-20 11:08:48,"<montywi> XL: I update the bb-fast-connect tree in git; This is after serg's review
<montywi> so I only need you to verify that things are same or better than normal 10.1 for connect tests and I can push
<XL> montywi: I see
<montywi> on my machine, having all threads in the thread cache gives a 5% speed increase; Having no thread cache was about the same
<montywi> this was when running 32 perl processes on the same machine to connect, do a query and disconnect
<montywi> doing this with a threaded benchmark should hopefully show a bigger speed increase",1," XL: I update the bb-fast-connect tree in git; This is after serg's review
 so I only need you to verify that things are same or better than normal 10.1 for connect tests and I can push
 montywi: I see
 on my machine, having all threads in the thread cache gives a 5% speed increase; Having no thread cache was about the same
 this was when running 32 perl processes on the same machine to connect, do a query and disconnect
 doing this with a threaded benchmark should hopefully show a bigger speed increase"
2436,MDEV-6150,MDEV,Michael Widenius,59415,2014-09-02 13:58:28,"Pushed to bb-fast-connect tree for testing.
",2,"Pushed to bb-fast-connect tree for testing.
"
2437,MDEV-6150,MDEV,Axel Schwenke,59419,2014-09-02 15:50:45,"Results from a short benchmark. I tested 3 scenarios

# standard sysbench OLTP readonly (1 connection per client thread that is reused)
# sysbench OLTP readonly, reconnection for each transaction
# simple workload: connect; select 1 from dual; disconnect

The last scenario shows advantages for the bb_fast_connect tree. The other however give better results with vanilla 10.1.

Benchmark was run on lizard2 (Intel, 32 cores, 64 threads). Huge thread pool was configured.",3,"Results from a short benchmark. I tested 3 scenarios

# standard sysbench OLTP readonly (1 connection per client thread that is reused)
# sysbench OLTP readonly, reconnection for each transaction
# simple workload: connect; select 1 from dual; disconnect

The last scenario shows advantages for the bb_fast_connect tree. The other however give better results with vanilla 10.1.

Benchmark was run on lizard2 (Intel, 32 cores, 64 threads). Huge thread pool was configured."
2438,MDEV-6150,MDEV,Michael Widenius,80634,2016-02-07 15:53:50,"Pushed to 10.2.
Speed up for connections are up to 40% when there is a lot of connections threads and the thread pool is 16 or higher.",4,"Pushed to 10.2.
Speed up for connections are up to 40% when there is a lot of connections threads and the thread pool is 16 or higher."
2439,MDEV-6150,MDEV,Axel Schwenke,81100,2016-02-19 13:47:18,Re-opened for benchmarking the effects of the change,5,Re-opened for benchmarking the effects of the change
2440,MDEV-6152,MDEV,Michael Widenius,75088,2015-08-28 13:14:27,Pushed into 10.1,1,Pushed into 10.1
2441,MDEV-6284,MDEV,Timofey Turenko,53709,2014-07-03 16:10:25,"Ubuntu 14.04 packages (MariaDB 5.5.37) contain libmysqld.*, but both libmysqld.so* and libmysqld.a are in 'libmariadbd-dev' package while in RPM (for example CentOS 6) libmysqld.a is in MariaDB-devel package and  libmysqld.so* are in MariaDB-server package.
As a result everything that is build under Ubuntu 14.04 depends on 'libmariadbd-dev' (on development package!)
Are we going to merge this packaging ito MariaDB?",1,"Ubuntu 14.04 packages (MariaDB 5.5.37) contain libmysqld.*, but both libmysqld.so* and libmysqld.a are in 'libmariadbd-dev' package while in RPM (for example CentOS 6) libmysqld.a is in MariaDB-devel package and  libmysqld.so* are in MariaDB-server package.
As a result everything that is build under Ubuntu 14.04 depends on 'libmariadbd-dev' (on development package!)
Are we going to merge this packaging ito MariaDB?"
2442,MDEV-6284,MDEV,Otto Kekäläinen,54406,2014-07-04 23:20:09,"About ""Fix Version: 10.0"" - please note that the merge request is for the 5.5 series only. I'll do a separate merge request for 10.0 and open a new JIRA issue for that, as the contents in debian/* for 10.0 is different enough to justify a separate issue. But before I push the 10.0 branch I'd like to see 5.5 merged first so we can learn from possible issues that arise in 5.5 work and fix not repeat them for 10.0.",2,"About ""Fix Version: 10.0"" - please note that the merge request is for the 5.5 series only. I'll do a separate merge request for 10.0 and open a new JIRA issue for that, as the contents in debian/* for 10.0 is different enough to justify a separate issue. But before I push the 10.0 branch I'd like to see 5.5 merged first so we can learn from possible issues that arise in 5.5 work and fix not repeat them for 10.0."
2443,MDEV-6284,MDEV,Otto Kekäläinen,67875,2015-02-06 11:15:53,"[~serg] This hasn't seen much progress because I've been busy with constantly updating and improving the Debian/Ubuntu packaging. Also the turnaround for request/review has been slow. What if we would meet and have a 1-2 day sprint to get this merge reviewed and pushed in one go?

What if we also changed the strategy, and not merge the latest packaging into 10.0 and 5.5 but rather only on 10.1 and keep downstream and upstream Debian packaging in sync from 10.1 onwards?",3,"[~serg] This hasn't seen much progress because I've been busy with constantly updating and improving the Debian/Ubuntu packaging. Also the turnaround for request/review has been slow. What if we would meet and have a 1-2 day sprint to get this merge reviewed and pushed in one go?

What if we also changed the strategy, and not merge the latest packaging into 10.0 and 5.5 but rather only on 10.1 and keep downstream and upstream Debian packaging in sync from 10.1 onwards?"
2444,MDEV-6284,MDEV,Otto Kekäläinen,73747,2015-07-21 09:24:58,[~serg] Note that current https://github.com/ottok/mariadb-10.0/compare/upstream...master.patch is 45488 lines long..,4,[~serg] Note that current URL is 45488 lines long..
2445,MDEV-6284,MDEV,Sergei Golubchik,73803,2015-07-21 22:46:59,Attached a complete review of the above: [^ottok.diff],5,Attached a complete review of the above: [^ottok.diff]
2446,MDEV-6284,MDEV,Otto Kekäläinen,73819,2015-07-22 08:21:29,"Recent changes done to downstream packaging based on Serg's review:

{noformat}
commit ada2c8c84271283eb0f604820d5994c9a057784e
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 22:40:16 2015 +0300

    Recover mysql-upgrade dir/link handlig wrongly removed in f7caa041db
    
    The preinst is supposed to save possible mysql datadir link
    and the postinst is supposed to recover it.
    
    Also unify namig of the variables that reference the data
    and log directories.

commit d190985e7ba7621c99f877a9451943c6e2ae5d54
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 22:02:54 2015 +0300

    mysqld config: rename key_buffer -> key_buffer_size
    
    Short name version is deprecated. Use full name instead.
    https://mariadb.com/kb/en/mariadb/myisam-system-variables/#key_buffer_size

commit d8fb0f21e7e835336b37d9d3533d7276fe7a99b3
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 21:12:18 2015 +0300

    Clean up old cruft from rules file after review by Sergei Golubchik
    
     * Don't set or use MYSQL_BUILD_CC as they were hard-coded anyway
     * Remove unnecessary comment about how make translates -j option to --jobserver
     * Assembler functions are long gone and references to them should be removed
     * C and C++ should have same optimization levels, that is -O3
     * BIG-JOINS are not used anywhere, remove them
     * Make compilation comment nicer (visible via ""show variables like 'version%';"")
     * Make -DDEB contain actual distro name, might be of later use and anyway better
       than just simple '1' (true) value
     * Not needed to have TAOCRYPT_OPT in CFLAGS, taocrypt is C++
     * Libwrap is disabled by default
     * INFO_BIN and INFO_SRC files are not used anymore, no need to install them.
     * Use INSTALL_MYSQLTESTDIR instead of manual mv line

commit a9bbb0c5dac55aefcad7131dc29a8b2a809cc3ed
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 18:23:25 2015 +0300

    Unified config file layout with upstream .cnf layout
    
    Now the packages mariadb-common, mariadb-client and mariadb-server each
    install their own cnf file. A system will not have extra cnf files around.
    
    Also the .cnf files list all possible stanzas that can be used to
    compartmentalize settings for different binaries.
    
    Role model was upstream
    https://github.com/MariaDB/server/tree/83ba48b7c670f6dba465325cafd808c91f551544/support-files/rpm

commit 59c96c74e098767b2e01c0fa6b2e70b8bce592cc
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 18:00:27 2015 +0300

    Simplyfy overkill utf8mb4 settings
    
    From https://dev.mysql.com/doc/refman/5.6/en/charset-connection.html
    
      A SET NAMES 'charset_name' statement is equivalent to these three statements:
    
      SET character_set_client = charset_name;
      SET character_set_results = charset_name;
      SET character_set_connection = charset_name;
    
    Therefore it is probably overkill to have 'SET NAMES'.
    
    Also using both - and _ in variable names in useless as they translate
    to the same variable.

commit 2f71f6834d93d9591cc187d6cce6e3dd7268bd33
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 17:46:24 2015 +0300

    Update upstream info in d/copyright

commit df02e679e29dabd1926f477c70d984cc168fc159
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 17:27:29 2015 +0300

    The test suite must depend on the exact MariaDB version as tests are version specific.
    
    Depending on latest or any newer version is wrong, as tests might not be compatible
    with some never program versions.

commit e3fe9539f010270aac5aa6e70a2c0e561c749df0
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 17:16:11 2015 +0300

    Wrap and sort

commit ed276550168e9933f9e804bcf5f10da0cf4fe648
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 16:24:30 2015 +0300

    Created package libmariadbd and ship libmysqld.so.18 in it.

commit 0ed651b0cd8bafed1d43be339d499cc0214e54e5
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 15:39:47 2015 +0300

    Add libdbd-mysql-perl as Depends instead of Suggests as libdbi-perl needs it.
    
    The package libdbi-perl is a dependency and it is useless unless libdbd-mysql-perl
    is also installed. Thus both should be in Depends. These lines are now also
    identical to those in the mysql-5.6 control file.

commit be79bfaf31b9839205cb34f6bb1244c2f9b36ef5
Author: Otto Kekäläinen <otto@seravo.fi>
Date:   Tue Jul 21 08:39:45 2015 +0300

    Updated d/changelog

{noformat}

(full log at https://github.com/ottok/mariadb-10.0/commits/master)

Preliminary branch for upstream merge is at https://github.com/MariaDB/server/compare/10.1...ottok:ok-debpkg",6,"Recent changes done to downstream packaging based on Serg's review:

{noformat}
commit ada2c8c84271283eb0f604820d5994c9a057784e
Author: Otto Kekäläinen 
Date:   Tue Jul 21 22:40:16 2015 +0300

    Recover mysql-upgrade dir/link handlig wrongly removed in f7caa041db
    
    The preinst is supposed to save possible mysql datadir link
    and the postinst is supposed to recover it.
    
    Also unify namig of the variables that reference the data
    and log directories.

commit d190985e7ba7621c99f877a9451943c6e2ae5d54
Author: Otto Kekäläinen 
Date:   Tue Jul 21 22:02:54 2015 +0300

    mysqld config: rename key_buffer -> key_buffer_size
    
    Short name version is deprecated. Use full name instead.
    URL

commit d8fb0f21e7e835336b37d9d3533d7276fe7a99b3
Author: Otto Kekäläinen 
Date:   Tue Jul 21 21:12:18 2015 +0300

    Clean up old cruft from rules file after review by Sergei Golubchik
    
     * Don't set or use MYSQL_BUILD_CC as they were hard-coded anyway
     * Remove unnecessary comment about how make translates -j option to --jobserver
     * Assembler functions are long gone and references to them should be removed
     * C and C++ should have same optimization levels, that is -O3
     * BIG-JOINS are not used anywhere, remove them
     * Make compilation comment nicer (visible via ""show variables like 'version%';"")
     * Make -DDEB contain actual distro name, might be of later use and anyway better
       than just simple '1' (true) value
     * Not needed to have TAOCRYPT_OPT in CFLAGS, taocrypt is C++
     * Libwrap is disabled by default
     * INFO_BIN and INFO_SRC files are not used anymore, no need to install them.
     * Use INSTALL_MYSQLTESTDIR instead of manual mv line

commit a9bbb0c5dac55aefcad7131dc29a8b2a809cc3ed
Author: Otto Kekäläinen 
Date:   Tue Jul 21 18:23:25 2015 +0300

    Unified config file layout with upstream .cnf layout
    
    Now the packages mariadb-common, mariadb-client and mariadb-server each
    install their own cnf file. A system will not have extra cnf files around.
    
    Also the .cnf files list all possible stanzas that can be used to
    compartmentalize settings for different binaries.
    
    Role model was upstream
    URL

commit 59c96c74e098767b2e01c0fa6b2e70b8bce592cc
Author: Otto Kekäläinen 
Date:   Tue Jul 21 18:00:27 2015 +0300

    Simplyfy overkill utf8mb4 settings
    
    From URL
    
      A SET NAMES 'charset_name' statement is equivalent to these three statements:
    
      SET character_set_client = charset_name;
      SET character_set_results = charset_name;
      SET character_set_connection = charset_name;
    
    Therefore it is probably overkill to have 'SET NAMES'.
    
    Also using both - and _ in variable names in useless as they translate
    to the same variable.

commit 2f71f6834d93d9591cc187d6cce6e3dd7268bd33
Author: Otto Kekäläinen 
Date:   Tue Jul 21 17:46:24 2015 +0300

    Update upstream info in d/copyright

commit df02e679e29dabd1926f477c70d984cc168fc159
Author: Otto Kekäläinen 
Date:   Tue Jul 21 17:27:29 2015 +0300

    The test suite must depend on the exact MariaDB version as tests are version specific.
    
    Depending on latest or any newer version is wrong, as tests might not be compatible
    with some never program versions.

commit e3fe9539f010270aac5aa6e70a2c0e561c749df0
Author: Otto Kekäläinen 
Date:   Tue Jul 21 17:16:11 2015 +0300

    Wrap and sort

commit ed276550168e9933f9e804bcf5f10da0cf4fe648
Author: Otto Kekäläinen 
Date:   Tue Jul 21 16:24:30 2015 +0300

    Created package libmariadbd and ship libmysqld.so.18 in it.

commit 0ed651b0cd8bafed1d43be339d499cc0214e54e5
Author: Otto Kekäläinen 
Date:   Tue Jul 21 15:39:47 2015 +0300

    Add libdbd-mysql-perl as Depends instead of Suggests as libdbi-perl needs it.
    
    The package libdbi-perl is a dependency and it is useless unless libdbd-mysql-perl
    is also installed. Thus both should be in Depends. These lines are now also
    identical to those in the mysql-5.6 control file.

commit be79bfaf31b9839205cb34f6bb1244c2f9b36ef5
Author: Otto Kekäläinen 
Date:   Tue Jul 21 08:39:45 2015 +0300

    Updated d/changelog

{noformat}

(full log at URL

Preliminary branch for upstream merge is at URL"
2447,MDEV-6284,MDEV,Otto Kekäläinen,75602,2015-09-09 09:49:22,"[~serg] Pull request filed at https://github.com/MariaDB/server/pull/97

It fixes at least MDEV-8667 and MDEV-6326 right now, and the new control file will prevent lots of future problems.

As the control file logic is now almost in sync with that of the official Debian version, this pull request will prevent problems people might have who try to upgrade from 10.0 in Debian/Ubuntu to 10.1 from mariadb.org repos.

Once this PR is accepted, I'll continue on the merge for other parts.",7,"[~serg] Pull request filed at URL

It fixes at least MDEV-8667 and MDEV-6326 right now, and the new control file will prevent lots of future problems.

As the control file logic is now almost in sync with that of the official Debian version, this pull request will prevent problems people might have who try to upgrade from 10.0 in Debian/Ubuntu to 10.1 from mariadb.org repos.

Once this PR is accepted, I'll continue on the merge for other parts."
2448,MDEV-6284,MDEV,Sergei Golubchik,76225,2015-09-25 11:26:16,merged,8,merged
2449,MDEV-6284,MDEV,Otto Kekäläinen,76242,2015-09-25 21:30:45,"[~serg][~ratzpo] I think I should continue to do another round of backporting fixes to 10.1 to close multiple issues linked above so that 10.1 would ship with less broken packaging. However the current 10.1 master head fails in multiple tests and I don't have a clean starting point: https://buildbot.askmonty.org/buildbot/grid?branch=ok-debpkg-10.1&category=main

None of those fails are due to Debian packaging. I'll wait until 10.1 master is all green before I start working on my branch.",9,"[~serg][~ratzpo] I think I should continue to do another round of backporting fixes to 10.1 to close multiple issues linked above so that 10.1 would ship with less broken packaging. However the current 10.1 master head fails in multiple tests and I don't have a clean starting point: URL

None of those fails are due to Debian packaging. I'll wait until 10.1 master is all green before I start working on my branch."
2450,MDEV-6284,MDEV,Daniel Black,85051,2016-07-20 04:24:17,https://buildbot.askmonty.org/buildbot/grid?branch=ok-debpkg-10.1&category=main green again.,10,URL green again.
2451,MDEV-6284,MDEV,Otto Kekäläinen,87292,2016-10-11 14:37:00,"I've been working on the next step for 5 days now, should soon be done. WIP at https://github.com/ottok/mariadb/commits/ok-debpkg-10.2",11,"I've been working on the next step for 5 days now, should soon be done. WIP at URL"
2452,MDEV-6284,MDEV,Otto Kekäläinen,87906,2016-10-30 12:20:00,"PR available at https://github.com/MariaDB/server/pull/251

There is still quite a lot to do to close this issue and all the issues linked to it, but this is a big milestone in that process.",12,"PR available at URL

There is still quite a lot to do to close this issue and all the issues linked to it, but this is a big milestone in that process."
2453,MDEV-6284,MDEV,Rasmus Johansson,94199,2017-04-21 11:34:42,Changing fixVersion. Everything that was supposed to be done for 10.2 has been done. The rest will go into 10.3.,13,Changing fixVersion. Everything that was supposed to be done for 10.2 has been done. The rest will go into 10.3.
2454,MDEV-6284,MDEV,Otto Kekäläinen,105545,2018-01-11 19:52:40,Work continues now on 10.3 branch by me and [~oerdnj],14,Work continues now on 10.3 branch by me and [~oerdnj]
2455,MDEV-6284,MDEV,Julien Fritsch,146779,2020-03-16 15:33:01,"[~otto], may I ask you if it's normal that this issue that is not closed is blocking so many issues that are closed?",15,"[~otto], may I ask you if it's normal that this issue that is not closed is blocking so many issues that are closed?"
2456,MDEV-6284,MDEV,Otto Kekäläinen,146799,2020-03-16 16:22:26,"[~julien.fritsch] Yes, this issue is still open. We need to fix e.g. https://jira.mariadb.org/browse/CONC-304 and https://jira.mariadb.org/browse/CONC-456 and then we are closer to closing this one. I expect this issue to be closed during the 10.5 cycle (assuming I get some help with a couple of things that should be changed upstream).",16,"[~julien.fritsch] Yes, this issue is still open. We need to fix e.g. URL and URL and then we are closer to closing this one. I expect this issue to be closed during the 10.5 cycle (assuming I get some help with a couple of things that should be changed upstream)."
2457,MDEV-6284,MDEV,Julien Fritsch,146803,2020-03-16 16:36:48,"[~otto] my question is more my question is more: how can these issues that have a link ""blocking by"" this one, can be closed it this one is not closed already?
 ",17,"[~otto] my question is more my question is more: how can these issues that have a link ""blocking by"" this one, can be closed it this one is not closed already?
 "
2458,MDEV-6284,MDEV,Otto Kekäläinen,146893,2020-03-17 12:33:12,"Issues marked ""blocked by"" are things that need to be fixed upstream before we can finalize the merge.

Issues marked ""blocks"" are bugs that are already fixed in Debian downstream and will automatically be fixed upstream at MariaDB.org when this issue is finalized.",18,"Issues marked ""blocked by"" are things that need to be fixed upstream before we can finalize the merge.

Issues marked ""blocks"" are bugs that are already fixed in Debian downstream and will automatically be fixed upstream at MariaDB.org when this issue is finalized."
2459,MDEV-6284,MDEV,Julien Fritsch,147398,2020-03-23 11:04:38,"Hi [~otto] would the ""upstream"" help then, maybe here to explain that indeed, this issue?",19,"Hi [~otto] would the ""upstream"" help then, maybe here to explain that indeed, this issue?"
2460,MDEV-6284,MDEV,Otto Kekäläinen,147420,2020-03-23 12:37:13,"I am both upstream and downstream. Currently things holding back this are my PRs pending to be reviewed/accepted and the issues that are still open that need help from somebody else. Most pressingly  https://jira.mariadb.org/browse/CONC-304 and https://jira.mariadb.org/browse/CONC-456  as stated above. I'll try to book 2h of time with Georg to get those CONC done, they are fairly simple path and filename changes, but just need to be done in a way that is backwards compatible.",20,"I am both upstream and downstream. Currently things holding back this are my PRs pending to be reviewed/accepted and the issues that are still open that need help from somebody else. Most pressingly  URL and URL  as stated above. I'll try to book 2h of time with Georg to get those CONC done, they are fairly simple path and filename changes, but just need to be done in a way that is backwards compatible."
2461,MDEV-6284,MDEV,Julien Fritsch,147422,2020-03-23 13:13:37,"OK, clear thank you",21,"OK, clear thank you"
2462,MDEV-6284,MDEV,Otto Kekäläinen,149629,2020-04-09 13:12:16,"Side notice: seems mtr depends on patch and libtool-bin, need to add that as a run-time dependency:

{noformat}
nano +2175 mysql-test-run.pl

sub executable_setup () {

  $exe_patch='patch' if `patch -v`;

  #
  # Check if libtool is available in this distribution/clone
  # we need it when valgrinding or debugging non installed binary
  # Otherwise valgrind will valgrind the libtool wrapper or bash
  # and gdb will not find the real executable to debug
  #
  if ( -x ""../libtool"")
  {
    $exe_libtool= ""../libtool"";
    if ($opt_valgrind or $glob_debugger or $opt_strace)
    {
      mtr_report(""Using \""$exe_libtool\"" when running valgrind, strace or debugger"");
    }
  }

{noformat}

Also, for all tests to run, they should be run as user 'mysql' with access to pam_auth_tool, and again with another user that is not in user.mysql by default.  Also some tests only run when mtr is invoked with --big-test, there is IPv6 available, there is a Cassandra back-end running etc. I wonder how big tests should be run to validate that there are no testable regressions in this MDEV once all patches are done (it is going to be huge).
",22,"Side notice: seems mtr depends on patch and libtool-bin, need to add that as a run-time dependency:

{noformat}
nano +2175 mysql-test-run.pl

sub executable_setup () {

  $exe_patch='patch' if `patch -v`;

  #
  # Check if libtool is available in this distribution/clone
  # we need it when valgrinding or debugging non installed binary
  # Otherwise valgrind will valgrind the libtool wrapper or bash
  # and gdb will not find the real executable to debug
  #
  if ( -x ""../libtool"")
  {
    $exe_libtool= ""../libtool"";
    if ($opt_valgrind or $glob_debugger or $opt_strace)
    {
      mtr_report(""Using \""$exe_libtool\"" when running valgrind, strace or debugger"");
    }
  }

{noformat}

Also, for all tests to run, they should be run as user 'mysql' with access to pam_auth_tool, and again with another user that is not in user.mysql by default.  Also some tests only run when mtr is invoked with --big-test, there is IPv6 available, there is a Cassandra back-end running etc. I wonder how big tests should be run to validate that there are no testable regressions in this MDEV once all patches are done (it is going to be huge).
"
2463,MDEV-6284,MDEV,Otto Kekäläinen,149630,2020-04-09 13:13:42,"This MDEV can be closed when all of these are green:

 !screenshot-1.png|thumbnail! 

https://salsa.debian.org/mariadb-team/mariadb-server/pipelines/124140
",23,"This MDEV can be closed when all of these are green:

 !screenshot-1.png|thumbnail! 

URL
"
2464,MDEV-6284,MDEV,Sergei Golubchik,149724,2020-04-10 10:00:34,"{{patch}} can be ""suggested"" or ""recommended"", because tests won't fail if it's not present, they'll be automatically skipped",24,"{{patch}} can be ""suggested"" or ""recommended"", because tests won't fail if it's not present, they'll be automatically skipped"
2465,MDEV-6284,MDEV,Otto Kekäläinen,153667,2020-05-19 15:52:22,"> Side notice: seems mtr depends on patch and libtool-bin, need to add that as a run-time dependency:

Tested libtool-bin, but it did not provide any extended traces, on the contrary the trace lost the filename paths:
 !screenshot-2.png|thumbnail! 

Tested by running mtr (without --force and without --parallel) and then issuing in another windows `killall -s 6 mariadbd` comparing baseline and situation after running `apt install libtool-bin`.

`patch` is already a suggests, no need to research it.",25,"> Side notice: seems mtr depends on patch and libtool-bin, need to add that as a run-time dependency:

Tested libtool-bin, but it did not provide any extended traces, on the contrary the trace lost the filename paths:
 !screenshot-2.png|thumbnail! 

Tested by running mtr (without --force and without --parallel) and then issuing in another windows `killall -s 6 mariadbd` comparing baseline and situation after running `apt install libtool-bin`.

`patch` is already a suggests, no need to research it."
2466,MDEV-6284,MDEV,Otto Kekäläinen,156603,2020-06-14 11:57:58,"All but https://jira.mariadb.org/browse/MDEV-5484 and merging of patches (https://jira.mariadb.org/browse/MDEV-14900) is done, but I deem this issue now closed enough.",26,"All but URL and merging of patches (URL is done, but I deem this issue now closed enough."
2467,MDEV-6697,MDEV,Jan Lindström,74007,2015-07-29 07:44:10,"Fix on 5.7 seems to me unsatisfactory :
{noformat}
mysql> create table t2(a int, constraint a foreign key a (a) references t1(a))engine=innodb;
ERROR 1215 (HY000): Cannot add foreign key constraint
mysql> show warnings\G
*************************** 1. row ***************************
  Level: Warning
   Code: 150
Message: Create table 'test/t2' with foreign key constraint failed. There is no index in the referenced table where the referenced columns appear as the first columns.
{noformat}

This is unclear when there is more than one foreign key on table test/t2.
",1,"Fix on 5.7 seems to me unsatisfactory :
{noformat}
mysql> create table t2(a int, constraint a foreign key a (a) references t1(a))engine=innodb;
ERROR 1215 (HY000): Cannot add foreign key constraint
mysql> show warnings\G
*************************** 1. row ***************************
  Level: Warning
   Code: 150
Message: Create table 'test/t2' with foreign key constraint failed. There is no index in the referenced table where the referenced columns appear as the first columns.
{noformat}

This is unclear when there is more than one foreign key on table test/t2.
"
2468,MDEV-6697,MDEV,Jan Lindström,74075,2015-07-30 15:34:12,"I can't change actual error messages on GA product. However, I can add additional or improved error messages to show warnings.",2,"I can't change actual error messages on GA product. However, I can add additional or improved error messages to show warnings."
2469,MDEV-6720,MDEV,Sergei Golubchik,78156,2015-11-17 21:24:42,and then we can remove few *.include files that do just that — switch connection with a message,1,and then we can remove few *.include files that do just that — switch connection with a message
2470,MDEV-6720,MDEV,Sergey Vojtovich,82282,2016-03-25 16:54:53,"[~serg], please review patch for this task. It is 95% complete, available in bb-10.2-MDEV-6720. I expect there're still tests to be stabilized (to be discovered by bb). I'll commit further changes on top of it and squash before pushing.",2,"[~serg], please review patch for this task. It is 95% complete, available in bb-10.2-MDEV-6720. I expect there're still tests to be stabilized (to be discovered by bb). I'll commit further changes on top of it and squash before pushing."
2471,MDEV-6720,MDEV,Sergei Golubchik,82371,2016-03-30 15:40:18,"You seem to be getting tasks that remove lots of code :)

This one: 64 insertions, 3414 deletions (not counting result files).",3,"You seem to be getting tasks that remove lots of code :)

This one: 64 insertions, 3414 deletions (not counting result files)."
2472,MDEV-6720,MDEV,Sergei Golubchik,82372,2016-03-30 16:19:29,"Ok to push, thanks!
One comment only: the change in rpl_begin_commit_rollback.test looks like a genuine bug in the test. Please, fix it, in a separate commit.
{code}
diff --git a/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test b/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
--- a/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
+++ b/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
@@ -133,8 +129,7 @@ source include/wait_for_slave_sql_to_start.inc;
 --echo # SAVEPOINT and ROLLBACK TO have the same problem in BUG#43263
 --echo # This was reported by BUG#50407
 connection master;
-echo [on master]
-SET SESSION AUTOCOMMIT=0;
+echo SET SESSION AUTOCOMMIT=0;
 let $binlog_start=query_get_value(SHOW MASTER STATUS, Position, 1);
 
 BEGIN;
{code}",4,"Ok to push, thanks!
One comment only: the change in rpl_begin_commit_rollback.test looks like a genuine bug in the test. Please, fix it, in a separate commit.
{code}
diff --git a/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test b/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
--- a/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
+++ b/mysql-test/suite/rpl/t/rpl_begin_commit_rollback.test
@@ -133,8 +129,7 @@ source include/wait_for_slave_sql_to_start.inc;
 --echo # SAVEPOINT and ROLLBACK TO have the same problem in BUG#43263
 --echo # This was reported by BUG#50407
 connection master;
-echo [on master]
-SET SESSION AUTOCOMMIT=0;
+echo SET SESSION AUTOCOMMIT=0;
 let $binlog_start=query_get_value(SHOW MASTER STATUS, Position, 1);
 
 BEGIN;
{code}"
2473,MDEV-6720,MDEV,Sergey Vojtovich,82373,2016-03-30 17:37:05,"Compared to previous tasks that removed lots of code, this one was disaster.

I also came across the same line in rpl_begin_commit_rollback.test, but I couldn't immediately find out if this test was actually supposed to set autocommit or not. I'll double check it.

Thanks!",5,"Compared to previous tasks that removed lots of code, this one was disaster.

I also came across the same line in rpl_begin_commit_rollback.test, but I couldn't immediately find out if this test was actually supposed to set autocommit or not. I'll double check it.

Thanks!"
2474,MDEV-6756,MDEV,Harry Higgs,60439,2014-09-19 00:07:40,"Following is a suggestion, provided by one of our senior developers, which could be used to accomplish this feature request. 

There is a code suggestion, made by Chrisian Hammers, 2008, back for MySQL 5.0.x,
documented in MySQL feature request:

    http://bugs.mysql.com/bug.php?id=33799.

However, I have a correction for the 'FIXME' line:

{noformat}
- thd_info->posix_thread_id = *((unsigned int *)(tmp->real_id + 72)); // FIXME this is from glibc-2.6.1/nptl/descr.h
+ thd_info->posix_thread_id = gettid(); // #include <sys/types.h>
{noformat}

The PID &/or Thread IDs are easily available, but only to the parent process and child thread or process.

A) From the parent process perspective, the child PID or thread_id is only made available during thread creation:

    The return code from fork(), is the PID of the child process. (Doc here http://pubs.opengroup.org/onlinepubs/007908799/xsh/fork.html)
    The first parameter from the POSIX function pthread_create(). (Doc here http://pubs.opengroup.org/onlinepubs/007908799/xsh/pthread_create.html)
    The sixth parameter from the Solaris function thr_create(). (Doc here http://docs.oracle.com/cd/E19253-01/816-5137/sthreads-69878/index.html)

B) From within the child Thread/Lwp (via pthread_create()), the thread_id is made available through:

    The return code from POSIX function, gettid().
    EX:

{noformat}
    #include <sys/types.h>
    pid_t gettid(void);
{noformat}

    The return code from syscall(SYS_gettid), where SYS_gettid is an enum.
    EX:

{noformat}
    #include <sys/types.h>
    #include <sys/syscall.h>
    #define gettid() syscall(SYS_gettid);
{noformat}

    The return code from Solaris function, thr_self().
    EX:

{noformat}
    #include <thread.h>
    thread_t thr_self();
{noformat}

C) From within the child process (via fork()), the PID is made available, at anytime through:

    The return code from getpid().
    EX:

{noformat}
    #include <unistd.h>
    pid_t getpid();
{noformat}

",1,"Following is a suggestion, provided by one of our senior developers, which could be used to accomplish this feature request. 

There is a code suggestion, made by Chrisian Hammers, 2008, back for MySQL 5.0.x,
documented in MySQL feature request:

    URL

However, I have a correction for the 'FIXME' line:

{noformat}
- thd_info->posix_thread_id = *((unsigned int *)(tmp->real_id + 72)); // FIXME this is from glibc-2.6.1/nptl/descr.h
+ thd_info->posix_thread_id = gettid(); // #include 
{noformat}

The PID &/or Thread IDs are easily available, but only to the parent process and child thread or process.

A) From the parent process perspective, the child PID or thread_id is only made available during thread creation:

    The return code from fork(), is the PID of the child process. (Doc here URL
    The first parameter from the POSIX function pthread_create(). (Doc here URL
    The sixth parameter from the Solaris function thr_create(). (Doc here URL

B) From within the child Thread/Lwp (via pthread_create()), the thread_id is made available through:

    The return code from POSIX function, gettid().
    EX:

{noformat}
    #include 
    pid_t gettid(void);
{noformat}

    The return code from syscall(SYS_gettid), where SYS_gettid is an enum.
    EX:

{noformat}
    #include 
    #include 
    #define gettid() syscall(SYS_gettid);
{noformat}

    The return code from Solaris function, thr_self().
    EX:

{noformat}
    #include 
    thread_t thr_self();
{noformat}

C) From within the child process (via fork()), the PID is made available, at anytime through:

    The return code from getpid().
    EX:

{noformat}
    #include 
    pid_t getpid();
{noformat}

"
2475,MDEV-6756,MDEV,Michael Widenius,61237,2014-09-24 01:13:12,"Hi!

> With that in mind, we would like to know if there is a way to map a
> linux child pid that's associated with the linux parent pid (ppid) of
> the mysqld process to the Id shown in the output of SHOW PROCESSLIST"".

To be able to help you, it would be useful to know how you plan to use
the linux child pid.

Is it to understand which thread, as given from 'ps -eLf', maps
to which connection id?

First, there is no stable association with a connection, which has a
THD associated with it, and a thread. If you are running with at
thread pool, then any thread may handle questions from any connection.

For the moment we store, and keep up the date, in each THD->real_id the
value of pthread_self().  This is the same value that is returned
from pthread_create(). This is however not the value returned from gettid().
This code is in sql_class.cc, see THD::store_globals().

It would be trivial to add into THD , the value of gettid() and show it in
information_schema.PROCESSLIST.

Would that help you in any way solving your problems.

I don't know if this is something that is useful for everyone,
especially as this would only work on Linux.

Regards,
Monty",2,"Hi!

> With that in mind, we would like to know if there is a way to map a
> linux child pid that's associated with the linux parent pid (ppid) of
> the mysqld process to the Id shown in the output of SHOW PROCESSLIST"".

To be able to help you, it would be useful to know how you plan to use
the linux child pid.

Is it to understand which thread, as given from 'ps -eLf', maps
to which connection id?

First, there is no stable association with a connection, which has a
THD associated with it, and a thread. If you are running with at
thread pool, then any thread may handle questions from any connection.

For the moment we store, and keep up the date, in each THD->real_id the
value of pthread_self().  This is the same value that is returned
from pthread_create(). This is however not the value returned from gettid().
This code is in sql_class.cc, see THD::store_globals().

It would be trivial to add into THD , the value of gettid() and show it in
information_schema.PROCESSLIST.

Would that help you in any way solving your problems.

I don't know if this is something that is useful for everyone,
especially as this would only work on Linux.

Regards,
Monty"
2476,MDEV-6756,MDEV,Harry Higgs,61424,2014-10-02 01:42:09,"Hi Monty, I apologize for not getting back to you sooner as I was on vacation last week.  Anyway, below may be a better explanation of what we are trying to accomplish and may provide you with answers to your questions.  This was provided from our senior developer who also provided the information above:

Senior Developer:

No, we are not using ‘thread pool’.   Our systems tend to be ‘batch’ processing orientated and establish many SQL queries that are individually treated as a ‘open cursor’ where the results are iterated across by the client application.

Yes, Please!  
Having the ‘gettid()’ value present in the information_schema.PROCESSLIST would greatly help achieve the association between a Unix LWP child of the mysqld (visible through the tools used by Sys Admins & DBAs) and the offending SQL query.   Once the offending SQL query is identified, the originating process can be tracked down.

Regarding the display of gettid() vs pthread_self(), in the info_schema.PROCESSLIST, this seems to be platform specific where the value displayed should correlate to the output of tools used by Sys Admins (unix:  ps -ef, htop, top, pgrep; Windows: taskmgr, plist).   However, IF you know of a means to translate the pthread_self() value into those reported by unix Sys Admin tools (ps -efL, htop, top),  then I think everyone would desire the POSIX standard value.

To facilitate our DBAs, our developers are required to embedded comments into the SQL queries that identify the originating process.  Ex:  “-- Program:$0  Host:$HOSTNAME  ProgPid:$$   Params:$*  DESC:Customer Cursor --\n  SELECT x,y,z FROM table WHERE filter = ?”  -- Where $$ is the unix pid of the process/script.

Ultimately, the problem we are experiencing is a random MySQL LWP thread that sustains 100% busy for many minutes, sometimes hours.  This is very visible from the unix tools ‘top’, ‘htop’ and ‘ps -efL’.  However, we have not been able to identify which SQL query to which the mysqld LWP thread is servicing.  During this situation, the info_schema.PROCESSLIST does not report any state or activity changes, thus we are unable to identify the offending SQL query.  Once the offending SQL query is identified, the affected processes can be identified and debugged.

Thanks, Harry",3,"Hi Monty, I apologize for not getting back to you sooner as I was on vacation last week.  Anyway, below may be a better explanation of what we are trying to accomplish and may provide you with answers to your questions.  This was provided from our senior developer who also provided the information above:

Senior Developer:

No, we are not using ‘thread pool’.   Our systems tend to be ‘batch’ processing orientated and establish many SQL queries that are individually treated as a ‘open cursor’ where the results are iterated across by the client application.

Yes, Please!  
Having the ‘gettid()’ value present in the information_schema.PROCESSLIST would greatly help achieve the association between a Unix LWP child of the mysqld (visible through the tools used by Sys Admins & DBAs) and the offending SQL query.   Once the offending SQL query is identified, the originating process can be tracked down.

Regarding the display of gettid() vs pthread_self(), in the info_schema.PROCESSLIST, this seems to be platform specific where the value displayed should correlate to the output of tools used by Sys Admins (unix:  ps -ef, htop, top, pgrep; Windows: taskmgr, plist).   However, IF you know of a means to translate the pthread_self() value into those reported by unix Sys Admin tools (ps -efL, htop, top),  then I think everyone would desire the POSIX standard value.

To facilitate our DBAs, our developers are required to embedded comments into the SQL queries that identify the originating process.  Ex:  “-- Program:$0  Host:$HOSTNAME  ProgPid:$$   Params:$*  DESC:Customer Cursor --\n  SELECT x,y,z FROM table WHERE filter = ?”  -- Where $$ is the unix pid of the process/script.

Ultimately, the problem we are experiencing is a random MySQL LWP thread that sustains 100% busy for many minutes, sometimes hours.  This is very visible from the unix tools ‘top’, ‘htop’ and ‘ps -efL’.  However, we have not been able to identify which SQL query to which the mysqld LWP thread is servicing.  During this situation, the info_schema.PROCESSLIST does not report any state or activity changes, thus we are unable to identify the offending SQL query.  Once the offending SQL query is identified, the affected processes can be identified and debugged.

Thanks, Harry"
2477,MDEV-6756,MDEV,Sergei Golubchik,62002,2014-10-09 22:58:49,Do you need it in 5.5 or 10.0?,4,Do you need it in 5.5 or 10.0?
2478,MDEV-6756,MDEV,Harry Higgs,62007,2014-10-09 23:32:57,"We would like it in both versions.  Currently we are on version 5.5.37 but at some point in the future we will upgrade to 10.0.  I just don't have a hard date as to when we will go to 10.0.

Thanks, Harry",5,"We would like it in both versions.  Currently we are on version 5.5.37 but at some point in the future we will upgrade to 10.0.  I just don't have a hard date as to when we will go to 10.0.

Thanks, Harry"
2479,MDEV-6756,MDEV,Michael Widenius,64203,2014-10-30 09:52:07,"Would it be acceptable for you to get a patch for 5.5.37 and have it in 10.0 ?
(This would mean that you have to compile MariaDB yourselves for 5.5).

The reason is that we don't want to do too many user visible changes to stable versions of MariaDB.
",6,"Would it be acceptable for you to get a patch for 5.5.37 and have it in 10.0 ?
(This would mean that you have to compile MariaDB yourselves for 5.5).

The reason is that we don't want to do too many user visible changes to stable versions of MariaDB.
"
2480,MDEV-6756,MDEV,John Dzilvelis,64400,2014-10-30 19:21:33,"Hi Monty, 

Harry and I talked about this a little earlier. I definitely agree with your point about visible changes on the 5.5.x versions.  I expect that we will begin studying a migration to the 10.x sometime in early 2015. 

As it turns out we are currently upgrading to Mariadb 5.5.40, so we'll only be on 5.5.37 for a couple more weeks in production. We are required to stay current to comply with our security practices.

We compile MariaDB for 5.5 now, so that is not a problem. However, whether you want to issue an additional patch for 5.5.40 is up to you. 

I just want you to know that I'll always need to stay current on the 5.5.x version and I don't know how ""portable"" the patch would be for future 5.5 versions. I doubt you would want to keep re-issuing patches for the same feature. We already have your patch for our SkySQL issue #8015 ( the slow shutdown issue we discussed a few months ago)  , and it's working fine on 5.5.40. 

Thank You, 

John Dzilvelis



",7,"Hi Monty, 

Harry and I talked about this a little earlier. I definitely agree with your point about visible changes on the 5.5.x versions.  I expect that we will begin studying a migration to the 10.x sometime in early 2015. 

As it turns out we are currently upgrading to Mariadb 5.5.40, so we'll only be on 5.5.37 for a couple more weeks in production. We are required to stay current to comply with our security practices.

We compile MariaDB for 5.5 now, so that is not a problem. However, whether you want to issue an additional patch for 5.5.40 is up to you. 

I just want you to know that I'll always need to stay current on the 5.5.x version and I don't know how ""portable"" the patch would be for future 5.5 versions. I doubt you would want to keep re-issuing patches for the same feature. We already have your patch for our SkySQL issue #8015 ( the slow shutdown issue we discussed a few months ago)  , and it's working fine on 5.5.40. 

Thank You, 

John Dzilvelis



"
2481,MDEV-6756,MDEV,Daniël van Eeden,73738,2015-07-20 23:28:56,"This relates to some work I did recently.
I wrote some UDF's:
* https://github.com/dveeden/udf_gettid Gets the Linux tid (task id, child of pid)
* https://github.com/dveeden/udf_pthread_name Set the pthread name (usable in top and ""ps -eLo pid,tid,comm | grep $(pgrep -x mysqld)"")
* https://github.com/dveeden/udf_cgroup move thread to cgroup

Note that tid and pthread id are related, but not equal. The pthread id is already shown in SHOW ENGINE INNODB STATUS.
Note that changed to the thread (name, cgroup) will stay effective after disconnect if the thread_cache is used.

Note that threads are already named on Windows.

Related MySQL bugs:
https://bugs.mysql.com/bug.php?id=70858",8,"This relates to some work I did recently.
I wrote some UDF's:
* URL Gets the Linux tid (task id, child of pid)
* URL Set the pthread name (usable in top and ""ps -eLo pid,tid,comm | grep $(pgrep -x mysqld)"")
* URL move thread to cgroup

Note that tid and pthread id are related, but not equal. The pthread id is already shown in SHOW ENGINE INNODB STATUS.
Note that changed to the thread (name, cgroup) will stay effective after disconnect if the thread_cache is used.

Note that threads are already named on Windows.

Related MySQL bugs:
URL"
2482,MDEV-6756,MDEV,Jean Weisbuch,73924,2015-07-25 19:25:53,"These UDFs seems interesting, especially the one to set the cgroup (if it could be used to limit resources per user/db, it would be really useful on a shared hosting environment for example).


It would be ideal to be able to add informations such as ""CPU Time"" or ""Memory usage"" of the process corresponding to the thread directly on _I\_S.PROCESSLIST_ which would be better than having to execute an external command to retrieve informations from ""/proc"" or use ""ps"".",9,"These UDFs seems interesting, especially the one to set the cgroup (if it could be used to limit resources per user/db, it would be really useful on a shared hosting environment for example).


It would be ideal to be able to add informations such as ""CPU Time"" or ""Memory usage"" of the process corresponding to the thread directly on _I\_S.PROCESSLIST_ which would be better than having to execute an external command to retrieve informations from ""/proc"" or use ""ps""."
2483,MDEV-6756,MDEV,Oleksandr Byelkin,74554,2015-08-12 19:57:24,"I found no way to see ID returned by gettid() (actually syscall()) in output of ps command (its TID for MariaDB threads is equal to PID) but the number clearly belong to the same numeration no doubts (found in experimenting)

getpid() works and allow to find the victim in the ps output.",10,"I found no way to see ID returned by gettid() (actually syscall()) in output of ps command (its TID for MariaDB threads is equal to PID) but the number clearly belong to the same numeration no doubts (found in experimenting)

getpid() works and allow to find the victim in the ps output."
2484,MDEV-6756,MDEV,Oleksandr Byelkin,74563,2015-08-13 00:12:07,"revision-id: 6811945eaf6a6e979d92739399b084bf4e00ae2d (mariadb-10.1.6-12-g6811945e)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-08-12 23:09:48 +0200
message:

MDEV-6756: map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST

Added tid & pid for Linux.

---",11,"revision-id: 6811945eaf6a6e979d92739399b084bf4e00ae2d (mariadb-10.1.6-12-g6811945e)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-08-12 23:09:48 +0200
message:

MDEV-6756: map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST

Added tid & pid for Linux.

---"
2485,MDEV-6756,MDEV,Oleksandr Byelkin,75811,2015-09-14 13:49:59,"revision-id: 7f7cbb132fa0a40687e4b25a9c9e163c93ec6d15 (mariadb-10.1.6-12-g7f7cbb1)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-09-14 12:47:57 +0200
message:

MDEV-6756: map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST

Added tid (thread ID) for system where it is present.

 ps -eL -o tid,pid,command

shows the thread on Linux

---",12,"revision-id: 7f7cbb132fa0a40687e4b25a9c9e163c93ec6d15 (mariadb-10.1.6-12-g7f7cbb1)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-09-14 12:47:57 +0200
message:

MDEV-6756: map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST

Added tid (thread ID) for system where it is present.

 ps -eL -o tid,pid,command

shows the thread on Linux

---"
2486,MDEV-6756,MDEV,Oleksandr Byelkin,75875,2015-09-15 22:51:04,"revision-id: a1f31cfa8d0f04cb166813786f7854db3dbcf715 (mariadb-10.1.6-12-ga1f31cf)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-09-15 21:50:28 +0200
message:

MDEV-6756: map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST

Added tid (thread ID) for system where it is present.

 ps -eL -o tid,pid,command

shows the thread on Linux

---",13,"revision-id: a1f31cfa8d0f04cb166813786f7854db3dbcf715 (mariadb-10.1.6-12-ga1f31cf)
parent(s): 86a3613d4e981c341e38291c9eeec5dc9f836fae
committer: Oleksandr Byelkin
timestamp: 2015-09-15 21:50:28 +0200
message:

MDEV-6756: map a linux pid (child pid) to a connection id shown in the output of SHOW PROCESSLIST

Added tid (thread ID) for system where it is present.

 ps -eL -o tid,pid,command

shows the thread on Linux

---"
2487,MDEV-6792,MDEV,Nirbhay Choubey,70565,2015-04-30 18:49:12,"* Fork https://github.com/codership/galera under MariaDB (/)
https://github.com/MariaDB/galera
* Create a branch of 3.x to be used to release galera packages (/)
mariadb-3.x

*List of platforms*
* Debian 6.0, Squeeze 
* Debian 7.0 Wheezy
* Debian 8.0 Jessie
* Debian Sid (Unstable)
* -Ubuntu 10.04 Lucid-
* Ubuntu 12.04 Precise
* -Ubuntu 12.10 Quantal-
* Ubuntu 14.04 Trusty
* Ubuntu 14.04 Trusty (Power 8, packages installed already)
* Ubuntu 14.10 Utopic
* Ubuntu 15.04 Vivid

* CentOS5
* CentOS6
* CentOS7
* Fedora-19 (galera-25.3.9-1.fc19.fc19.x86_64.rpm)
* Fedora-20 (galera-25.3.9-1.fc20.x86_64.rpm)
* RHEL-5 (not required, shares CentOS5 packages)
* SLES-11
* SLES-12 (x86-64 only)
* OpenSUSE13

Here are the extra packages needed:

*On Debian platforms*
* git
* scons
* check
* libboost-dev (any version but _1.46_)
* libboost-program-options-dev (any version but _1.46_)
* libssl-dev

*On RPM platforms*
* git
* scons
* check-devel
* boost-devel
* openssl-devel

*On SLES*
* scons
* check

Commands:
* wget http://softlayer-dal.dl.sourceforge.net/project/scons/scons/2.3.4/scons-2.3.4-1.noarch.rpm
* sudo rpm -ih scons-2.3.4-1.noarch.rpm

*On OpenSUSE13*
* scons
* check-devel
Command : sudo zypper install scons check-devel


On some older distros you might have to alter the sources.lst file to be able to pull the packages.

* -Quantal : sudo sed -i -e 's/us.archive.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list-
* -Lucid : sudo sed -i -e 's/dk.archive.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list-
* Squezze : Add deb http://ftp.de.debian.org/debian squeeze main (the default mirror did not work for me)",1,"* Fork URL under MariaDB (/)
URL
* Create a branch of 3.x to be used to release galera packages (/)
mariadb-3.x

*List of platforms*
* Debian 6.0, Squeeze 
* Debian 7.0 Wheezy
* Debian 8.0 Jessie
* Debian Sid (Unstable)
* -Ubuntu 10.04 Lucid-
* Ubuntu 12.04 Precise
* -Ubuntu 12.10 Quantal-
* Ubuntu 14.04 Trusty
* Ubuntu 14.04 Trusty (Power 8, packages installed already)
* Ubuntu 14.10 Utopic
* Ubuntu 15.04 Vivid

* CentOS5
* CentOS6
* CentOS7
* Fedora-19 (galera-25.3.9-1.fc19.fc19.x86_64.rpm)
* Fedora-20 (galera-25.3.9-1.fc20.x86_64.rpm)
* RHEL-5 (not required, shares CentOS5 packages)
* SLES-11
* SLES-12 (x86-64 only)
* OpenSUSE13

Here are the extra packages needed:

*On Debian platforms*
* git
* scons
* check
* libboost-dev (any version but _1.46_)
* libboost-program-options-dev (any version but _1.46_)
* libssl-dev

*On RPM platforms*
* git
* scons
* check-devel
* boost-devel
* openssl-devel

*On SLES*
* scons
* check

Commands:
* wget URL
* sudo rpm -ih scons-2.3.4-1.noarch.rpm

*On OpenSUSE13*
* scons
* check-devel
Command : sudo zypper install scons check-devel


On some older distros you might have to alter the sources.lst file to be able to pull the packages.

* -Quantal : sudo sed -i -e 's/us.archive.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list-
* -Lucid : sudo sed -i -e 's/dk.archive.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list-
* Squezze : Add deb URL squeeze main (the default mirror did not work for me)"
2488,MDEV-6792,MDEV,Elena Stepanova,70567,2015-04-30 18:57:28,"Please also update install images accordingly, otherwise we have a version mismatch, and our installation tests fail (it's already happening on 5.5 tree). 
",2,"Please also update install images accordingly, otherwise we have a version mismatch, and our installation tests fail (it's already happening on 5.5 tree). 
"
2489,MDEV-6792,MDEV,Nirbhay Choubey,72953,2015-06-30 17:42:40,"[~serg] / [~ratzpo] : Can you either grant me necessary permissions to clone repos under
MariaDB org or clone the galera repo ?
https://github.com/codership/galera.git
",3,"[~serg] / [~ratzpo] : Can you either grant me necessary permissions to clone repos under
MariaDB org or clone the galera repo ?
URL
"
2490,MDEV-6792,MDEV,Sergei Golubchik,73868,2015-07-24 00:53:46,done. https://github.com/MariaDB/galera,4,done. URL
2491,MDEV-6792,MDEV,Daniel Bartholomew,76245,2015-09-26 00:57:53,"Notes on Precise Build VMs

After updating all of the build VMs, I had to revert the Precise build VMs back to their previous state.
* MariaDB build script requires libboost-dev, but on precise that package points to libboost1.46-dev
* there's a libboost1.48-dev available, but when it is installed the libboost-dev package is removed
* ...so by installing libboost1.48-dev (as a dependency of libboost-program-options1.48-dev) on the precise builder, MariaDB builds break
* example: http://buildbot.askmonty.org/buildbot/builders/kvm-deb-precise-amd64/builds/4995/steps/compile/logs/stdio
* for now changes to the precise build VMs have be reverted",5,"Notes on Precise Build VMs

After updating all of the build VMs, I had to revert the Precise build VMs back to their previous state.
* MariaDB build script requires libboost-dev, but on precise that package points to libboost1.46-dev
* there's a libboost1.48-dev available, but when it is installed the libboost-dev package is removed
* ...so by installing libboost1.48-dev (as a dependency of libboost-program-options1.48-dev) on the precise builder, MariaDB builds break
* example: URL
* for now changes to the precise build VMs have be reverted"
2492,MDEV-6829,MDEV,Guillaume Lefranc,71225,2015-05-19 19:47:46,"There is an existing PXC 56 policy at this URL: https://github.com/gguillen/selinux_percona-pxc-56-cluster

If it works for PXC it should work for MariaDB. Have you tried it?",1,"There is an existing PXC 56 policy at this URL: URL

If it works for PXC it should work for MariaDB. Have you tried it?"
2493,MDEV-6829,MDEV,Nirbhay Choubey,72415,2015-06-18 17:06:17,https://github.com/MariaDB/server/commit/6050ab658696925f2a031b901eb398fff65fa92a,2,URL
2494,MDEV-6877,MDEV,Arjen Lentz,62587,2014-10-16 05:22:17,"This option was already flagged for porting by Kristian Nielsen in November 2013 on the MariaDB dev list, in response to an email from Sergey Vojtovic.
Ref: MDEV-5277",1,"This option was already flagged for porting by Kristian Nielsen in November 2013 on the MariaDB dev list, in response to an email from Sergey Vojtovic.
Ref: MDEV-5277"
2495,MDEV-6877,MDEV,Vicențiu Ciorbaru,72603,2015-06-22 11:03:30,"Can you please review the patch set found here?

[https://github.com/MariaDB/server/compare/10.1-MDEV-6877-binlog_row_image]

Thanks
Vicentiu",2,"Can you please review the patch set found here?

[URL

Thanks
Vicentiu"
2496,MDEV-6877,MDEV,Vicențiu Ciorbaru,72949,2015-06-30 15:18:58,"Implemented with:

[https://github.com/MariaDB/server/compare/768620ee5c71...1a3321b6496d]",3,"Implemented with:

[URL"
2497,MDEV-6877,MDEV,Daniel Black,74006,2015-07-29 05:16:05,"great work [~cvicentiu].

missing from documentation however https://mariadb.com/kb/en/mariadb/server-system-variables/",4,"great work [~cvicentiu].

missing from documentation however URL"
2498,MDEV-6877,MDEV,Daniel Black,74327,2015-08-06 16:10:57,[~greenman] - ^ new variable undocumented,5,[~greenman] - ^ new variable undocumented
2499,MDEV-6877,MDEV,Daniel Black,74895,2015-08-22 12:28:28,thanks [~greenman],6,thanks [~greenman]
2500,MDEV-7032,MDEV,Hartmut Holzgraefe,65071,2014-11-06 13:35:41,"> Perhaps this wrapper should check that it is invoked as mysql user…

This is an absolute must requirement IMHO to not expose /etc/shadow password information to a random user
(it is the reason for the UID checks within unix_chkpwd in the first place)

But all in all I think that pam_unix doesn't work as a general authentication mechanism by design and that no new tools to work around this should be distributed ...

If /etc/shadow based authentication is a must then explicitly adding user ""mysql"" the ""shadow"" group looks like a better approach as it requires explicit admin action ... problem right now though is that this automatically exposes
shadow contents to LOAD DATA INFILE. MySQL manual states that LOAD DATA INFILE can only read world-readable local files, but this seems to be a mixup - I can only find a check for that in the LOAD_FILE() function implementation which is a totally different beast than LOAD DATA INFILE ... (documentation bug report about that about to be written ...)
",1,"> Perhaps this wrapper should check that it is invoked as mysql user…

This is an absolute must requirement IMHO to not expose /etc/shadow password information to a random user
(it is the reason for the UID checks within unix_chkpwd in the first place)

But all in all I think that pam_unix doesn't work as a general authentication mechanism by design and that no new tools to work around this should be distributed ...

If /etc/shadow based authentication is a must then explicitly adding user ""mysql"" the ""shadow"" group looks like a better approach as it requires explicit admin action ... problem right now though is that this automatically exposes
shadow contents to LOAD DATA INFILE. MySQL manual states that LOAD DATA INFILE can only read world-readable local files, but this seems to be a mixup - I can only find a check for that in the LOAD_FILE() function implementation which is a totally different beast than LOAD DATA INFILE ... (documentation bug report about that about to be written ...)
"
2501,MDEV-7032,MDEV,Sergei Golubchik,65148,2014-11-08 11:10:25,{{/etc/shadow}} is just one use case. S/Key stores data in {{/etc/skeykeys}}. I've got complains about google-authenticator too. Many other PAM modules store data in {{$HOME}}. This setuid wrapper is the only solution I can think of that solves all these issues.,2,{{/etc/shadow}} is just one use case. S/Key stores data in {{/etc/skeykeys}}. I've got complains about google-authenticator too. Many other PAM modules store data in {{$HOME}}. This setuid wrapper is the only solution I can think of that solves all these issues.
2502,MDEV-7032,MDEV,Alexey Botchkov,113433,2018-07-03 12:26:01,"Fixed along with the MDEV-15473
",3,"Fixed along with the MDEV-15473
"
2503,MDEV-7032,MDEV,Christopher Halbersma,113689,2018-07-09 16:27:19,"> Fixed along with the MDEV-15473

Nice.",4,"> Fixed along with the MDEV-15473

Nice."
2504,MDEV-7145,MDEV,Phil Sweeney,72515,2015-06-21 02:46:13,"Would love this feature - I'm struggling with pt-slave-delay issues where it is behaving erratically, and obviously you have to run it as a separate process.  Plus, there are quirks with it such as that it will die after 10 failed attempts to connect, and it's not 'guaranteed' to keep things delayed as much as if it was built into the database.",1,"Would love this feature - I'm struggling with pt-slave-delay issues where it is behaving erratically, and obviously you have to run it as a separate process.  Plus, there are quirks with it such as that it will die after 10 failed attempts to connect, and it's not 'guaranteed' to keep things delayed as much as if it was built into the database."
2505,MDEV-7145,MDEV,Andrey Kashlak,75099,2015-08-29 22:47:17,"Please add this, it's very good for backups",2,"Please add this, it's very good for backups"
2506,MDEV-7145,MDEV,Nicolas Payart,76738,2015-10-13 18:15:05,Waiting for this feature too. It can be useful in a testing environment to detect failures on the client side because of asynchronous replication delay (which may occur on a master/slave production environment).,3,Waiting for this feature too. It can be useful in a testing environment to detect failures on the client side because of asynchronous replication delay (which may occur on a master/slave production environment).
2507,MDEV-7145,MDEV,"Su, Jun-Ming",86449,2016-09-14 07:45:21,"Waiting for this feature, and hope it in the 10.1.x series. It will help the users who use the replication as a buffer for preventing the wrong sql usage.",4,"Waiting for this feature, and hope it in the 10.1.x series. It will help the users who use the replication as a buffer for preventing the wrong sql usage."
2508,MDEV-7145,MDEV,Simon Wright,86458,2016-09-14 12:29:46,Waiting for this as well. Right now we're using pt-slave-delay which works but isn't stable; our sysadmin is yelling at me to switch to Percona. I would prefer to stay with MariaDB but data integrity trumps all other concerns.,5,Waiting for this as well. Right now we're using pt-slave-delay which works but isn't stable; our sysadmin is yelling at me to switch to Percona. I would prefer to stay with MariaDB but data integrity trumps all other concerns.
2509,MDEV-7145,MDEV,"Su, Jun-Ming",86469,2016-09-14 14:01:14,"Will this feature be in 10.1.x stable ga? Because 10.2 is the alpha release and not to use for production.
Thanks.",6,"Will this feature be in 10.1.x stable ga? Because 10.2 is the alpha release and not to use for production.
Thanks."
2510,MDEV-7145,MDEV,Phil Sweeney,87369,2016-10-13 10:47:51,"For those tracking this bug, [~knielsen] has begun work on this and posted an update on the developers mailing list:

https://lists.launchpad.net/maria-developers/msg09993.html

Thanks Kristian!",7,"For those tracking this bug, [~knielsen] has begun work on this and posted an update on the developers mailing list:

URL

Thanks Kristian!"
2511,MDEV-7145,MDEV,Kristian Nielsen,87437,2016-10-16 22:02:04,Pushed into 10.2.3,8,Pushed into 10.2.3
2512,MDEV-7331,MDEV,Sergey Vojtovich,81242,2016-02-24 12:33:21,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2513,MDEV-7331,MDEV,Sergey Vojtovich,81287,2016-02-25 10:40:42,"[~serg], please review updated patch.",2,"[~serg], please review updated patch."
2514,MDEV-7331,MDEV,Sergei Golubchik,81298,2016-02-25 15:59:29,ok to push,3,ok to push
2515,MDEV-7376,MDEV,Sergey Vojtovich,78234,2015-11-19 09:34:58,"[~serg], please review patch for this task. Did [~monty] agree with this clean-up?
Sorry for stealing it, I assume you don't have it implemented in your tree.",1,"[~serg], please review patch for this task. Did [~monty] agree with this clean-up?
Sorry for stealing it, I assume you don't have it implemented in your tree."
2516,MDEV-7376,MDEV,Sergei Golubchik,78466,2015-11-24 23:24:24,"I don't think I need to review it, push when you're think it's ok.
Yes, he did.
Sure, thanks for taking it.",2,"I don't think I need to review it, push when you're think it's ok.
Yes, he did.
Sure, thanks for taking it."
2517,MDEV-7376,MDEV,Michael Widenius,79862,2016-01-14 23:18:20,"Just a note:

A important distinction between pkill  and mysql_zap is that mysql_zap kills the server 'gently' first (with signal 15) and only if the server doesn't die in a limited time then tries -9.

To use pkill one must run it twice;  pkill --signal 15 mysqld  ; sleep(10) ; pkill -f --signal 9 pattern

So in the end, pkill is more cumbersome and slower to use than mysql_zap to kill the mysqld server.
",3,"Just a note:

A important distinction between pkill  and mysql_zap is that mysql_zap kills the server 'gently' first (with signal 15) and only if the server doesn't die in a limited time then tries -9.

To use pkill one must run it twice;  pkill --signal 15 mysqld  ; sleep(10) ; pkill -f --signal 9 pattern

So in the end, pkill is more cumbersome and slower to use than mysql_zap to kill the mysqld server.
"
2518,MDEV-7384,MDEV,Daniel Black,69772,2015-04-06 06:06:21,"https://github.com/MariaDB/server/pull/19 targeting 10.1

Happy to target 10.0 if you want.",1,"URL targeting 10.1

Happy to target 10.0 if you want."
2519,MDEV-7384,MDEV,Daniel Black,69774,2015-04-06 11:36:19,10.0 version https://github.com/MariaDB/server/pull/38,2,10.0 version URL
2520,MDEV-7384,MDEV,Vicențiu Ciorbaru,79200,2015-12-18 22:02:56,"Patch applied with tests in commit:

[3b9423fda2612a463e9f3af5750234ccf2667545|https://github.com/MariaDB/server/commit/3b9423fda2612a463e9f3af5750234ccf2667545]

Thanks for the contribution!",3,"Patch applied with tests in commit:

[3b9423fda2612a463e9f3af5750234ccf2667545|URL

Thanks for the contribution!"
2521,MDEV-7409,MDEV,Stoykov,67021,2015-01-05 14:45:33,That issue was created because I was not able to identify the MDEV task corresponding to http://askmonty.org/worklog/?tid=202 seen at https://mariadb.com/kb/en/progress-reporting/ ,1,That issue was created because I was not able to identify the MDEV task corresponding to URL seen at URL 
2522,MDEV-7409,MDEV,Sachin Setiya,90965,2017-01-23 17:36:43,http://lists.askmonty.org/pipermail/commits/2017-January/010488.html,2,URL
2523,MDEV-7409,MDEV,Sachin Setiya,91197,2017-01-30 11:37:10,Buildbot failure + some changes required in patch,3,Buildbot failure + some changes required in patch
2524,MDEV-7409,MDEV,Sachin Setiya,128460,2019-05-28 08:15:25,http://lists.askmonty.org/pipermail/commits/2019-May/013809.html,4,URL
2525,MDEV-7409,MDEV,Sachin Setiya,128668,2019-05-31 06:35:56,Removed memory leak http://lists.askmonty.org/pipermail/commits/2019-May/013826.html,5,Removed memory leak URL
2526,MDEV-7409,MDEV,Andrei Elkin,129292,2019-06-13 20:36:08,"Mailed my notes to suggest ansi-quoting and changes in the test part, specifically
replace simulations with the slave local trx as Rows_log_event applying blocker.",6,"Mailed my notes to suggest ansi-quoting and changes in the test part, specifically
replace simulations with the slave local trx as Rows_log_event applying blocker."
2527,MDEV-7409,MDEV,Sachin Setiya,178635,2021-01-29 10:47:38,"Hi [~ralf.gebhardt@mariadb.com], 

I just need to change the test cases. Then it should be okay to push.",7,"Hi [~ralf.gebhardt@mariadb.com], 

I just need to change the test cases. Then it should be okay to push."
2528,MDEV-7409,MDEV,Sachin Setiya,180638,2021-02-23 12:49:39,"I am getting a memory leak ,  regarding set_db() , Currently debugging that.",8,"I am getting a memory leak ,  regarding set_db() , Currently debugging that."
2529,MDEV-7409,MDEV,Sachin Setiya,180655,2021-02-23 14:26:25,Review branch bb-10.6-sachin,9,Review branch bb-10.6-sachin
2530,MDEV-7409,MDEV,Sujatha Sivakumar,180904,2021-02-25 13:49:55,Patch Looks fine.,10,Patch Looks fine.
2531,MDEV-7472,MDEV,Sergei Golubchik,67747,2015-02-03 19:51:07,"<bytee> serg: we do have bitmap changed page tracking in XtraDB
<bytee> serg: implemented MDEV-7472  makes sense so its user exposed
<bytee> esp when a feature is used by innobackupex
<bytee> see: FLUSH CHANGED_PAGE_BITMAPS - this statement can be used for synchronous bitmap write for immediate catch-up with the log checkpoint. This is used by innobackupex to make sure that XtraBackup indeed has all the required data it needs.",1," serg: we do have bitmap changed page tracking in XtraDB
 serg: implemented MDEV-7472  makes sense so its user exposed
 esp when a feature is used by innobackupex
 see: FLUSH CHANGED_PAGE_BITMAPS - this statement can be used for synchronous bitmap write for immediate catch-up with the log checkpoint. This is used by innobackupex to make sure that XtraBackup indeed has all the required data it needs."
2532,MDEV-7472,MDEV,Alex Boag-Munroe,69727,2015-04-02 18:20:10,"Any chance of this being implemented quicker than 10.2?

Percona's latest backup release now means incremental backups are not possible on MariaDB:

https://github.com/percona/percona-xtrabackup/pull/25/files

Their check returns true on MariaDB:
{code:sql}
SELECT COUNT(*) FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE 'INNODB_CHANGED_PAGES';
{code}
Resulting in:

Error executing 'FLUSH NO_WRITE_TO_BINLOG CHANGED_PAGE_BITMAPS': DBD::mysql::db do failed: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'CHANGED_PAGE_BITMAPS' at line 1 at /usr/bin/innobackupex line 3044.

During an incremental backup.",2,"Any chance of this being implemented quicker than 10.2?

Percona's latest backup release now means incremental backups are not possible on MariaDB:

URL

Their check returns true on MariaDB:
{code:sql}
SELECT COUNT(*) FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE 'INNODB_CHANGED_PAGES';
{code}
Resulting in:

Error executing 'FLUSH NO_WRITE_TO_BINLOG CHANGED_PAGE_BITMAPS': DBD::mysql::db do failed: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'CHANGED_PAGE_BITMAPS' at line 1 at /usr/bin/innobackupex line 3044.

During an incremental backup."
2533,MDEV-7472,MDEV,Sergei Golubchik,69760,2015-04-04 14:46:32,"This is unlikely. 10.1 is already beta and we generally don't add new statements after alpha.

We could add
{code:sql}
FLUSH NO_WRITE_TO_BINLOG INNODB_CHANGED_PAGES
{code}
statement though, because it doesn't change the parser — it's an internal change inside XtraDB (adding FLUSH support to the INFORMATION_SCHEMA.INNODB_CHANGED_PAGES table).

But I am not sure it'll help xtrabackup.",3,"This is unlikely. 10.1 is already beta and we generally don't add new statements after alpha.

We could add
{code:sql}
FLUSH NO_WRITE_TO_BINLOG INNODB_CHANGED_PAGES
{code}
statement though, because it doesn't change the parser — it's an internal change inside XtraDB (adding FLUSH support to the INFORMATION_SCHEMA.INNODB_CHANGED_PAGES table).

But I am not sure it'll help xtrabackup."
2534,MDEV-7472,MDEV,Sergei Golubchik,69761,2015-04-04 14:47:54,"Hmm. In fact, we could add INFORMATION_SCHEMA.CHANGED_PAGE_BITMAP table to XtraDB — and then FLUSH will work. Which is a pretty silly solution (dummy I_S table with no content), but it might work.",4,"Hmm. In fact, we could add INFORMATION_SCHEMA.CHANGED_PAGE_BITMAP table to XtraDB — and then FLUSH will work. Which is a pretty silly solution (dummy I_S table with no content), but it might work."
2535,MDEV-7472,MDEV,Jan Lindström,72643,2015-06-23 08:12:39,"Adding just IS table CHANGED_PAGE_BITMAPS is not enough 

{noformat}
FLUSH NO_WRITE_TO_BINLOG CHANGED_PAGE_BITMAPS;
ERROR 42000: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'CHANGED_PAGE_BITMAPS' at line 1
select * from information_schema.changed_page_bitmaps;
dummy
0
{noformat}",5,"Adding just IS table CHANGED_PAGE_BITMAPS is not enough 

{noformat}
FLUSH NO_WRITE_TO_BINLOG CHANGED_PAGE_BITMAPS;
ERROR 42000: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'CHANGED_PAGE_BITMAPS' at line 1
select * from information_schema.changed_page_bitmaps;
dummy
0
{noformat}"
2536,MDEV-7472,MDEV,Sergei Golubchik,72644,2015-06-23 08:19:32,"See https://mariadb.com/kb/en/mariadb/information-schema-plugins-show-and-flush-statements/

For I_S table to support FLUSH, it must implement a reset_table callback.",6,"See URL

For I_S table to support FLUSH, it must implement a reset_table callback."
2537,MDEV-7472,MDEV,Jan Lindström,72661,2015-06-23 14:47:27,"commit b7ff2f1b59c42171ff50fc682e8f723f0713d688
Author: Jan Lindström <jan.lindstrom@mariadb.com>
Date:   Tue Jun 23 14:36:24 2015 +0300

    MDEV-7472: Implementation of user statements for handling the xtradb changed page bitmaps
    
    Introduce a new dummy INFORMATION_SCHEMA.CHANGED_PAGE_BITMAPS table to XtraDB
    with reset_table callback to allow FLUSH NO_WRITE_TO_BINLOG CHANGED_PAGE_BITMAPS
    to be called from innobackupex.

",7,"commit b7ff2f1b59c42171ff50fc682e8f723f0713d688
Author: Jan Lindström 
Date:   Tue Jun 23 14:36:24 2015 +0300

    MDEV-7472: Implementation of user statements for handling the xtradb changed page bitmaps
    
    Introduce a new dummy INFORMATION_SCHEMA.CHANGED_PAGE_BITMAPS table to XtraDB
    with reset_table callback to allow FLUSH NO_WRITE_TO_BINLOG CHANGED_PAGE_BITMAPS
    to be called from innobackupex.

"
2538,MDEV-7472,MDEV,Joffrey MICHAIE,86040,2016-09-01 13:37:45,"Hi,

Sorry to re-open this, I see that the following feature has been implemented:
* FLUSH CHANGED_PAGE_BITMAPS
But what about the 2 following:
* RESET CHANGED_PAGE_BITMAPS
* PURGE CHANGED_PAGE_BITMAPS BEFORE <lsn>
?
I am especially interested in the last one, as it is used generally in post backup scripts to safely remove unneeded xdb files.

Regards,
Joffrey",8,"Hi,

Sorry to re-open this, I see that the following feature has been implemented:
* FLUSH CHANGED_PAGE_BITMAPS
But what about the 2 following:
* RESET CHANGED_PAGE_BITMAPS
* PURGE CHANGED_PAGE_BITMAPS BEFORE 
?
I am especially interested in the last one, as it is used generally in post backup scripts to safely remove unneeded xdb files.

Regards,
Joffrey"
2539,MDEV-7472,MDEV,Tomas Mozes,86307,2016-09-08 05:44:24,"As Joffrey, I'd also like ask what about RESET CHANGED_PAGE_BITMAPS and PURGE 
CHANGED_PAGE_BITMAPS BEFORE. It would really help us with the incremental backups in MariaDB.",9,"As Joffrey, I'd also like ask what about RESET CHANGED_PAGE_BITMAPS and PURGE 
CHANGED_PAGE_BITMAPS BEFORE. It would really help us with the incremental backups in MariaDB."
2540,MDEV-7472,MDEV,Joffrey MICHAIE,86388,2016-09-12 13:29:43,"Hi,

to avoid polluting this task, I have created https://jira.mariadb.org/browse/MDEV-10798
You are welcome to follow/vote on it.

Regards,
Joffrey",10,"Hi,

to avoid polluting this task, I have created URL
You are welcome to follow/vote on it.

Regards,
Joffrey"
2541,MDEV-7563,MDEV,Paul Weiss,74701,2015-08-17 05:56:07,This one is very high priority for me.,1,This one is very high priority for me.
2542,MDEV-7563,MDEV,Todd Allen,82767,2016-04-17 20:34:42,I would also very much like to see this implemented.,2,I would also very much like to see this implemented.
2543,MDEV-7563,MDEV,Laurent Langlois,83005,2016-04-25 17:53:50,"This issue if also of rather high priority for me.

We are in the process of migrating databases from Sybase to MariaDB and our database makes extensive use of Sybase ""rules.""  From what I know, it would seem that to implement the equivalent, 'check constraint' would be much easier to use (implement, maintain, etc.) than triggers.",3,"This issue if also of rather high priority for me.

We are in the process of migrating databases from Sybase to MariaDB and our database makes extensive use of Sybase ""rules.""  From what I know, it would seem that to implement the equivalent, 'check constraint' would be much easier to use (implement, maintain, etc.) than triggers."
2544,MDEV-7563,MDEV,ManasaTulluri,83162,2016-05-03 03:26:26,"Hi Dev Team,

Kindly let us know if you are implementing check constraints any time soon in the up coming releases.

Please let us know tentative date. We are migrating from Oracle to Mariadb and this will be very handy for us.

Thanks & Regards,
Manasa Tulluri.",4,"Hi Dev Team,

Kindly let us know if you are implementing check constraints any time soon in the up coming releases.

Please let us know tentative date. We are migrating from Oracle to Mariadb and this will be very handy for us.

Thanks & Regards,
Manasa Tulluri."
2545,MDEV-7563,MDEV,Sergei Golubchik,83164,2016-05-03 06:17:26,"Yes, there are plans to do it. But at the moment there's nothing final yet. Ask in a few weeks, please.",5,"Yes, there are plans to do it. But at the moment there's nothing final yet. Ask in a few weeks, please."
2546,MDEV-7563,MDEV,Michael Widenius,84361,2016-06-18 11:22:28,"This is now available in bb-10.2-default branch. Will be pushed into 10.2.1 shortly.
It's already documented at https://mariadb.com/kb/en/mariadb/create-table/",6,"This is now available in bb-10.2-default branch. Will be pushed into 10.2.1 shortly.
It's already documented at URL"
2547,MDEV-7635,MDEV,Daniel Black,68493,2015-02-26 11:06:48,"Morgan Tocker's defaults, for everything actually in MariaDB, seem quite sane to me. Ref 1 - bit dated.

Ref 2 interesting rant:

bad points:

* seems to oversimplify transactions by assuming the API are the same as the data
* character collations - rtfm and stop making assumptions.
* transaction isolation level - wrong reason to change and MySQL backed out of this change
* doesn't get that null isn't a value

on the valid good points:

* sql_mode - http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_sql-mode - invalid dates part of  STRICT_TRANS_TABLES - part of Tocker's changes
* utf8 - i think where there, +innodb_large_prefix will help with indexes as Tocker point out
* we have computed columns
* conditional index - i'm sure there is a feature request somewhere though probably low priority
* non-transaction DDL/slow alter table - fair point - hard problem - if their south/migration implementer didn't hate mysql could implement pt-online-schema-change in migrations but they won't (with probably good reason)",1,"Morgan Tocker's defaults, for everything actually in MariaDB, seem quite sane to me. Ref 1 - bit dated.

Ref 2 interesting rant:

bad points:

* seems to oversimplify transactions by assuming the API are the same as the data
* character collations - rtfm and stop making assumptions.
* transaction isolation level - wrong reason to change and MySQL backed out of this change
* doesn't get that null isn't a value

on the valid good points:

* sql_mode - URL - invalid dates part of  STRICT_TRANS_TABLES - part of Tocker's changes
* utf8 - i think where there, +innodb_large_prefix will help with indexes as Tocker point out
* we have computed columns
* conditional index - i'm sure there is a feature request somewhere though probably low priority
* non-transaction DDL/slow alter table - fair point - hard problem - if their south/migration implementer didn't hate mysql could implement pt-online-schema-change in migrations but they won't (with probably good reason)"
2548,MDEV-7635,MDEV,Daniel Black,75987,2015-09-20 07:55:59,"As pointed out on irc, sql_mode=STRICT_TRANS_TABLES would of helped prevent https://www.bugzilla.org/security/4.2.14/ .

A major release is the only time to change these defaults. 10.2 is a long way off.

So many of these values are already out of date addressing some would be a good start.

So to give this issue some concrete values (from mysql-5.7 changes):
* sql_mode = ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION
* innodb_large_prefix = yes
* log_slow_admin_statements ON
* log_slow_slave_statements	 ON
* long-query-time 2
* log-queries-not-using-indexes ON
* min-examined-row-limit 1000
* group_concat_max_len 1M
* max_allowed_packet 16M
* binlog_format ROW    (require for galera)
* innodb_autoinc_lock_mode 2  (required for galera anyway)
* innodb_checksum_algorithm crc32
* innodb_purge_threads 4
* innodb_strict_mode ON
* innodb_log_file_size 128M
* innodb_buffer_pool_dump_at_shutdown ON
* innodb_buffer_pool_load_at_startup ON
* innodb_buffer_pool_dump_pct 25 (Added in MDEV-8923)
* innodb_file_format Barracuda
* sync_binlog =1 ( or at most 10)
* slave_net_timeout 60
* performance schema - Enable events_statements_history and events_transactions_history consumers by default
* mysql client prompt=""\u@\h [\d] > ""


My additions:
* skip-name-resolve  (packages can enable on upgrade if host based usernames exist)
* binlog_annotate_row_events ON
* replicate_annotate_row_events ON
* innodb_stats_traditional FALSE
* use_stat_tables complementary
* histogram_size 255
* aria_recover  QUICK,BACKUP
* myisam-recover QUICK, BACKUP
* table_definition_cache autosize to max(2000, 400+table_open_cache / 2) like 5.7
* query_cache_strip_comments = ON

If label=order-by-optimisations are resolved:

* optimizer_switch	= mrr=on,mrr_sort_keys=on,optimize_join_buffer_size=on
* optimizer_use_condition_selectivity	=4
* join_cache_level = 8",2,"As pointed out on irc, sql_mode=STRICT_TRANS_TABLES would of helped prevent URL .

A major release is the only time to change these defaults. 10.2 is a long way off.

So many of these values are already out of date addressing some would be a good start.

So to give this issue some concrete values (from mysql-5.7 changes):
* sql_mode = ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION
* innodb_large_prefix = yes
* log_slow_admin_statements ON
* log_slow_slave_statements	 ON
* long-query-time 2
* log-queries-not-using-indexes ON
* min-examined-row-limit 1000
* group_concat_max_len 1M
* max_allowed_packet 16M
* binlog_format ROW    (require for galera)
* innodb_autoinc_lock_mode 2  (required for galera anyway)
* innodb_checksum_algorithm crc32
* innodb_purge_threads 4
* innodb_strict_mode ON
* innodb_log_file_size 128M
* innodb_buffer_pool_dump_at_shutdown ON
* innodb_buffer_pool_load_at_startup ON
* innodb_buffer_pool_dump_pct 25 (Added in MDEV-8923)
* innodb_file_format Barracuda
* sync_binlog =1 ( or at most 10)
* slave_net_timeout 60
* performance schema - Enable events_statements_history and events_transactions_history consumers by default
* mysql client prompt=""\u@\h [\d] > ""


My additions:
* skip-name-resolve  (packages can enable on upgrade if host based usernames exist)
* binlog_annotate_row_events ON
* replicate_annotate_row_events ON
* innodb_stats_traditional FALSE
* use_stat_tables complementary
* histogram_size 255
* aria_recover  QUICK,BACKUP
* myisam-recover QUICK, BACKUP
* table_definition_cache autosize to max(2000, 400+table_open_cache / 2) like 5.7
* query_cache_strip_comments = ON

If label=order-by-optimisations are resolved:

* optimizer_switch	= mrr=on,mrr_sort_keys=on,optimize_join_buffer_size=on
* optimizer_use_condition_selectivity	=4
* join_cache_level = 8"
2549,MDEV-7635,MDEV,Jan Steinman,80384,2016-01-27 05:13:32,"I would REALLY like to see *group_concat_max_len* default to something much higher than 1k, particularly since (as I note in [https://mariadb.atlassian.net/browse/MDEV-9474]) you apparently cannot set this value in either /etc/my.cnf nor as a command line argument to mysqld. Neither is this value stored in the binlog, which means potential synch problems with slaves ([https://mariadb.atlassian.net/browse/MDEV-8292])",3,"I would REALLY like to see *group_concat_max_len* default to something much higher than 1k, particularly since (as I note in [URL you apparently cannot set this value in either /etc/my.cnf nor as a command line argument to mysqld. Neither is this value stored in the binlog, which means potential synch problems with slaves ([URL"
2550,MDEV-7635,MDEV,Federico Razzoli,83528,2016-05-19 22:30:58,"I don't agree with --skip-name-resolve, because some people rely on hostnames.
For aria_recover and myisam_recover, I suggest FORCE,BACKUP (not just QUICK,BACKUP).

My proposals:
* disable query cache, it's not useful for common OLTP workloads
* lock_wait_timeout - ALTER TABLEs can be long, but having transactions waiting for 1 year is a potential disaster
* lower_case_table_names = 1 - default value is annoying and useless
* updatable_views_with_limit - having YES by default seems to me very dangerous

I also hope that this task will include more checks during startup. In a couple cases, inconsistent settings cause warnings, but more checks could be done, particularly when read_only = 1.",4,"I don't agree with --skip-name-resolve, because some people rely on hostnames.
For aria_recover and myisam_recover, I suggest FORCE,BACKUP (not just QUICK,BACKUP).

My proposals:
* disable query cache, it's not useful for common OLTP workloads
* lock_wait_timeout - ALTER TABLEs can be long, but having transactions waiting for 1 year is a potential disaster
* lower_case_table_names = 1 - default value is annoying and useless
* updatable_views_with_limit - having YES by default seems to me very dangerous

I also hope that this task will include more checks during startup. In a couple cases, inconsistent settings cause warnings, but more checks could be done, particularly when read_only = 1."
2551,MDEV-7635,MDEV,Richard Bensley,85807,2016-08-24 10:34:39,"Agree with FORCE,BACKUP for myisam recover. Out of the box MariaDB should be a reliable database to cater for the majority of installations and encourage best practices (the entire point of this ticket). Just because a server is ""on"" does not mean it's right.",5,"Agree with FORCE,BACKUP for myisam recover. Out of the box MariaDB should be a reliable database to cater for the majority of installations and encourage best practices (the entire point of this ticket). Just because a server is ""on"" does not mean it's right."
2552,MDEV-7635,MDEV,Sergei Golubchik,85998,2016-08-31 08:43:04,"My comments:
# {{innodb-*}}, {{slave_net_timeout}} — no opinion
# {{min_examined_row_limit}} — not quite. On the other hand, it goes directly against other suggested changes (that increase slow log verbosity), and after all a slow query is a slow query, no matter how many rows were examined. On the other hand, if {{log_queries_not_using_indexes}} is {{ON}}, then without {{min_examined_row_limit}} it'll log too much. A good idea might be to have a setting, similar to {{min_examined_row_limit}}, that only applies to {{log_queries_not_using_indexes}}. And practically it can be done by changing {{log_queries_not_using_indexes}} from {{ON}}/{{OFF}} boolean, to an integer, meaning “log queries not using indexes, if they examine at least that number of rows”. Because {{ON}} is boolean {{TRUE}}, that is 1, and {{OFF}} is 0, one can still use {{=ON}} and {{=OFF}} and they'll work as expected.
# {{query_cache_strip_comments}} — not sure. Query cache was supposed to be as low overhead as possible, stripping comments make it more expensive by default. Also, the query cache is most useful for an application using identical generated queries, and in that case all queries will typically have identical comments.
# {{default_tmp_storage_engine}}, {{updatable_views_with_limit}} — why?
# {{lower_case_table_names}} — not quite agree. May be 2? It'll give us standard-compliant case-insensitive behaviour, but will preserve user specified letter case.
# other suggested changes I agree with",6,"My comments:
# {{innodb-*}}, {{slave_net_timeout}} — no opinion
# {{min_examined_row_limit}} — not quite. On the other hand, it goes directly against other suggested changes (that increase slow log verbosity), and after all a slow query is a slow query, no matter how many rows were examined. On the other hand, if {{log_queries_not_using_indexes}} is {{ON}}, then without {{min_examined_row_limit}} it'll log too much. A good idea might be to have a setting, similar to {{min_examined_row_limit}}, that only applies to {{log_queries_not_using_indexes}}. And practically it can be done by changing {{log_queries_not_using_indexes}} from {{ON}}/{{OFF}} boolean, to an integer, meaning “log queries not using indexes, if they examine at least that number of rows”. Because {{ON}} is boolean {{TRUE}}, that is 1, and {{OFF}} is 0, one can still use {{=ON}} and {{=OFF}} and they'll work as expected.
# {{query_cache_strip_comments}} — not sure. Query cache was supposed to be as low overhead as possible, stripping comments make it more expensive by default. Also, the query cache is most useful for an application using identical generated queries, and in that case all queries will typically have identical comments.
# {{default_tmp_storage_engine}}, {{updatable_views_with_limit}} — why?
# {{lower_case_table_names}} — not quite agree. May be 2? It'll give us standard-compliant case-insensitive behaviour, but will preserve user specified letter case.
# other suggested changes I agree with"
2553,MDEV-7635,MDEV,Federico Razzoli,86025,2016-08-31 22:40:49,"updatable_views_with_limit: Because I can't think a case when running a DELETE or UPDATE against a view with LIMIT is reliable. Instead, I assume that usually it is a bug. In fact the number of affected rows depends on LIMIT and the number of rows in the base table. Even if you delete by id, the number of affected rows could be 0 or 1 depending if underlying data changed.

If the user is aware of this and considers this a good practice, he can still change updatable_views_with_limit.",7,"updatable_views_with_limit: Because I can't think a case when running a DELETE or UPDATE against a view with LIMIT is reliable. Instead, I assume that usually it is a bug. In fact the number of affected rows depends on LIMIT and the number of rows in the base table. Even if you delete by id, the number of affected rows could be 0 or 1 depending if underlying data changed.

If the user is aware of this and considers this a good practice, he can still change updatable_views_with_limit."
2554,MDEV-7635,MDEV,Daniel Black,86027,2016-08-31 23:18:14,"[~serg],

#  {{min_examined_row_limit}} - main purpose I saw was not logging too much. I'm happy with your ""log queries not using indexes, if they examine at least that number of rows"" variant.
# {{query_cache_strip_comments}} - quite right - happy to drop that one.

An alternate point of view for {{innodb_checksum_algorithm}}={{crc32}} is {{none}}. Just a gut feel but it seems that many forms storage media itself have forms of error detection/correction. The number of modern filesystems that supports data checksum/parity (based on wikipedia: filesystems) seems to include only ZFS/Btrfs/GPFS, which while generally not the greatest choice for databases, provide an indication of the low importance of this feature at a filesystem level. Its unclear if filesystems don't solve this because a) its a storage problem to solve or b) its a application level problem to solve. Provided there is sufficient bounds checking on the values read perhaps checksums on pages isn't needed? Happy to hear other opinions more researched than my simplified analysis here.",8,"[~serg],

#  {{min_examined_row_limit}} - main purpose I saw was not logging too much. I'm happy with your ""log queries not using indexes, if they examine at least that number of rows"" variant.
# {{query_cache_strip_comments}} - quite right - happy to drop that one.

An alternate point of view for {{innodb_checksum_algorithm}}={{crc32}} is {{none}}. Just a gut feel but it seems that many forms storage media itself have forms of error detection/correction. The number of modern filesystems that supports data checksum/parity (based on wikipedia: filesystems) seems to include only ZFS/Btrfs/GPFS, which while generally not the greatest choice for databases, provide an indication of the low importance of this feature at a filesystem level. Its unclear if filesystems don't solve this because a) its a storage problem to solve or b) its a application level problem to solve. Provided there is sufficient bounds checking on the values read perhaps checksums on pages isn't needed? Happy to hear other opinions more researched than my simplified analysis here."
2555,MDEV-7635,MDEV,Michiel Hazelhof,86036,2016-09-01 08:17:00,"Is there a big advantage to sql_mode = ONLY_FULL_GROUP_BY? If not I think this would cause more harm than good (especially for users migrating to MariaDB only to find out some default setting breaks their app).

Also I'd like to object against lower_case_table_names, case insensitive compares are slower and it doesn't encourage proper coding styles.

Regarding optimizer_use_condition_selectivity we should be extra cautious, we have been tried this setting and anything higher than 2 resulted in infinite hanging queries (only large queries on large tables, we have yet to debug this further).

On a semi related note, wouldn't it be better to stick to a default dash or underscore? The dash and underscore are now alternately used which neither promotes good standards nor makes it easier to read.",9,"Is there a big advantage to sql_mode = ONLY_FULL_GROUP_BY? If not I think this would cause more harm than good (especially for users migrating to MariaDB only to find out some default setting breaks their app).

Also I'd like to object against lower_case_table_names, case insensitive compares are slower and it doesn't encourage proper coding styles.

Regarding optimizer_use_condition_selectivity we should be extra cautious, we have been tried this setting and anything higher than 2 resulted in infinite hanging queries (only large queries on large tables, we have yet to debug this further).

On a semi related note, wouldn't it be better to stick to a default dash or underscore? The dash and underscore are now alternately used which neither promotes good standards nor makes it easier to read."
2556,MDEV-7635,MDEV,Daniel Black,87198,2016-10-08 10:01:55,"Noted in the MariaDB dev meetup Amsterdam 2016

h1. character set

default-set-server
collation-server = utf8mb4-general(?)-ci

default-character-set =  utf8mb4

h1. socket authentication

* in rpm and debs but not tarballs
* new installs only
* in mysql_secure_install to do it?

expose plugin name in error message for access denied to help people that do (user) mysql -u root

possibly too late?",10,"Noted in the MariaDB dev meetup Amsterdam 2016

h1. character set

default-set-server
collation-server = utf8mb4-general(?)-ci

default-character-set =  utf8mb4

h1. socket authentication

* in rpm and debs but not tarballs
* new installs only
* in mysql_secure_install to do it?

expose plugin name in error message for access denied to help people that do (user) mysql -u root

possibly too late?"
2557,MDEV-7635,MDEV,Elena Stepanova,87427,2016-10-15 17:49:51,"_My opinion on values that I saw in the description as of Oct 14._

h3. {color:red}Part 1: VERY STRONGLY DISAGREE{color}

_Note: if there is such thing as a QA veto, consider it such._

*{{lower_case_table_names=1,2}}*

I don't know how it's meant to work. With either value, all existing tables which have upper-case letters in their names will become unaccessible? 
For lower_case_table_names=2, currently server does not even allow to set it, switching to 0, as it says, ""to avoid future problems"". AI knows better this time. 
If there is a fix coming with this change which will certainly allow to handle existing tables, I'm ready to reconsider.

*use_stat_tables=COMPLEMENTARY + histogram_size=255 + optimizer_use_condition_selectivity=4*

These are awful defaults, highly dangerous, and will cause a lot of pain to most users without any gain whatsoever.

Table statistics and histograms are fine tools, but they are not for everyone, and cannot be used as defaults. They should be used as tuning, when people experience problems, and when their setup is thoroughly examined to make sure it won't make more harm than good. Basically, it's a prescription drug, not over-the-counter vitamins.

The idea of good defaults is either to make sure that the configuration works ""out-of-the-box"" for most users, or it simplifies runtime tuning (e.g. not all options can be changed without server startup). This configuration does neither of that. 

Let's consider different scenarios.

1) Users who do not run ANALYZE TABLE 
* gain: none, since there are no stats
* loss: 
** regressions (e.g. MDEV-9628, MDEV-6727, MDEV-9187, and probably many more);
** likely hidden problems, as the configuration with optimizer_use_condition_selectivity>1 but without the statistical data was considered a corner case with special treatment (there have been a bunch of bugs about this combination initially, and they were getting fixed, but the configuration was claimed not to be a priority because it's meaningless -- which in itself says a lot about proposed defaults).
** unnecessary extra logic (attempts to search for the statistics upon every query) which will possibly affect performance, even if only slightly. 

2) Users who run ANALYZE TABLE on their own reasons, on their own schedule
* gain: _possible_ performance improvement for _some_ queries in _some_ cases, when engine statistics is off
* loss: 
** since the users don't have their ANALYZE schedule adjusted specifically to EITS needs, they are likely to have wrong statistics in the tables most of the time, so plans can quite possibly get worse rather than better;
** ANALYZE with statistics has a huge problem with disk space (MDEV-6529). In his experiment, SPetrunia used a table with 1M rows and got disk consumption of 300M upon ANALYZE. Our users (not just biggest customers) already routinely have tables with billions rows. So, it can easily be hundreds of gigabytes. 
** ANALYZE with the new configuration will have disastrous performance. When it was taking less than a second before, it will easily take hours. E.g. on my machine, ANALYZE for dbt3 table lineitem (sf1, 6 mln rows) takes 0.15 sec without statistics, and 40 seconds with statistics. ANALYZE on a table with 600 mln rows will take over an hour, possibly much more. Existing maintenance jobs will all be broken, they simply won't fit into the scheduled window.
** ANALYZE is replicated, even if nobody needs statistics on the slave, and thus will make the slave lag dramatically. OM=>NS setups which are used for upgrades will be most affected.
** regressions (see scenario 1)

3) Users who run ANALYZE TABLE specifically to collect EITS data
* gain: none, since they already have it configured and used at their discretion
* loss: possibly same as in scenario 2, especially if they are smart enough to run ANALYZE with explicit PERSISTENT FOR ... when they need statistics to be recollected, and run ""normal"" ANALYZE otherwise.

*log_queries_not_using_indexes=1* 

It's a good tool, but a crazy default. 
It's only useful when somebody wants to make a special effort to analyze and fine-tune their workflow, they can always turn it on temporarily. 
Otherwise, in the form that it exists, most people don't need it. 
If the query is really slow, it will be reflected in the log anyway as a general slow query. If it's not slow, nobody cares -- it's fine to query 10-row tables without indexes, and people do it all the time, the log will be huge. Apparently it was meant to be controlled by min-examined-row-limit, but it's not suitable for that, see below.
Until this option can be properly configured, it mustn't become default. 
It's not surprising that MySQL introduced log_throttle_queries_not_using_indexes (which I also don't like though), and even with it, they were smart enough not to turn this logging ON. 

*min-examined-row-limit=N*

It's yet another tool for analyzing and fine-tuning a special workflow, it mustn't be used as a default. What if a simple DML is slow because it's blocked for no good reason? What if ANALYZE is slow -- it won't be logged at all, regardless the number of rows in the table? What if you forgot a long sleep somewhere deep in your query or function? All of these should go to the slow log, but with this option enabled, they won't.


h3. {color:blue}Part 2: DISAGREE{color}

*default_tmp_storage_engine=InnoDB*

Pointless nuissance. It works much better now with 'none' and temporary tables defaulting to @@default_storage_engine. Vast majority of users have default engine either InnoDB, or MyISAM, or Aria, and maybe in future RocksDB, and would want their tmp tables be of the same engine as normal tables, right? That's what 'none' does. It's hard to imagine that somebody would want default engine MyISAM, but default tmp engine InnoDB. Setting it by default will only make non-InnoDB users have to remember to modify it separately. Worse still, people who run without InnoDB on whatever reason will start getting failures on startup until they reconfigure the instance. 
Those who run by default with very special engines which can't be used for tmp tables already specify the engine for temporary tables explicitly anyway. 

*long_query_time=2*

I think it's excessive, it will create more noise than useful output, and will be all false-positives. Instances grow in size and concurrency faster than they grow in overall speed, many queries are bound to be longer than 2 seconds even if there is nothing wrong with them. Besides, any fluctuations in IO can make it breach the limit.
When somebody reaches the point in their performance tuning that queries longer than 2 seconds really bother them, I think they'll be happy to decrease the value. But it won't be the majority.

*updatable_views_with_limit=OFF*

I don't see any reason to tighten this setting. AFAIR we haven't had a single complaint from users regarding effects of updates which are controlled by this option. Making it forbidden by default will most likely cause some existing 3rd-party tools stop working properly (at least MySQL manual says that some GUI tools tend to generate such queries), and thus will lower the adoption without any good reason -- people will try installing the new version, see it doesn't work well on some obscure reason, and will switch back. 

*innodb_log_file_size =128M* 

According to MySQL manual, it's meaningless. ""Sensible values range from 4MB to 1/N-th of the size of the buffer pool, where N is the number of log files in the group"". Currently default buffer pool size is 128M, and there are 2 log files in the group. 

*query_cache_strip_comments=1*

Has it ever been performance-tested? I'm not sure it's very valuable, but rather suspicious. Yes, there is a specific scenario when comments are painful for query cache, but I think it's mostly a test scenario (e.g. when queries are numerated or otherwise get unique identifiers in the comments -- I myself use it a lot in tests). 


h3. {color:green}OTHER NOTES{color}

*lock_wait_timeout* 

I also vote for lowering the value to something that people can realistically wait for (even 1 day would be much better than 1 year). Currently there is no practical difference between ""locked"" and ""frozen forever"". 

*innodb_buffer_pool_dump_at_shutdown=ON and innodb_buffer_pool_load_at_startup=ON*

I'm not very happy about it, not because these values are wrong by themselves, but because of our assorted issues with startup/shutdown timeouts in scripts. I expect a considerable growth of complaints.

*sync_binlog=1*

It would be good to have and publish at least some numbers of our performance sacrifice. Is it more like 1% or more like 50%?

*binlog_format=MIXED* 

While I support this change, I think we must make a clear statement about it in a form of a blog, preferably before the GA release, to give people time to prepare to it. The change can be absolutely disastrous for setups with insufficient indexes. People need to either make sure they switch back to STATEMENT or add indexes *before* they get into a trouble and start loudly complaining about the regression MariaDB introduced. 

_Also, it should be noted that in some cases MBR is not just ""unsafe DML is logged as ROW, everything else STMT"". There are some subtleties related to temporary tables and MBR, there have always been some bugs around it. Unfortunately, we can't see MySQL bug reports anymore, but the fact that they switched right to RBR rather than MBR makes me a little nervous about MIXED as a new default. I'll try to find time to run some tests around it._

*log-warnings=2*

Fine, but please make sure InnoDB notifications go to a level higher than 2.

*table_definition_cache = autosized*

Have we actually merged the logic which autosizes it? Did anyone review it?

*innodb_stats_traditional=off*

Has it been performance-tested (more or less recently)?

",11,"_My opinion on values that I saw in the description as of Oct 14._

h3. {color:red}Part 1: VERY STRONGLY DISAGREE{color}

_Note: if there is such thing as a QA veto, consider it such._

*{{lower_case_table_names=1,2}}*

I don't know how it's meant to work. With either value, all existing tables which have upper-case letters in their names will become unaccessible? 
For lower_case_table_names=2, currently server does not even allow to set it, switching to 0, as it says, ""to avoid future problems"". AI knows better this time. 
If there is a fix coming with this change which will certainly allow to handle existing tables, I'm ready to reconsider.

*use_stat_tables=COMPLEMENTARY + histogram_size=255 + optimizer_use_condition_selectivity=4*

These are awful defaults, highly dangerous, and will cause a lot of pain to most users without any gain whatsoever.

Table statistics and histograms are fine tools, but they are not for everyone, and cannot be used as defaults. They should be used as tuning, when people experience problems, and when their setup is thoroughly examined to make sure it won't make more harm than good. Basically, it's a prescription drug, not over-the-counter vitamins.

The idea of good defaults is either to make sure that the configuration works ""out-of-the-box"" for most users, or it simplifies runtime tuning (e.g. not all options can be changed without server startup). This configuration does neither of that. 

Let's consider different scenarios.

1) Users who do not run ANALYZE TABLE 
* gain: none, since there are no stats
* loss: 
** regressions (e.g. MDEV-9628, MDEV-6727, MDEV-9187, and probably many more);
** likely hidden problems, as the configuration with optimizer_use_condition_selectivity>1 but without the statistical data was considered a corner case with special treatment (there have been a bunch of bugs about this combination initially, and they were getting fixed, but the configuration was claimed not to be a priority because it's meaningless -- which in itself says a lot about proposed defaults).
** unnecessary extra logic (attempts to search for the statistics upon every query) which will possibly affect performance, even if only slightly. 

2) Users who run ANALYZE TABLE on their own reasons, on their own schedule
* gain: _possible_ performance improvement for _some_ queries in _some_ cases, when engine statistics is off
* loss: 
** since the users don't have their ANALYZE schedule adjusted specifically to EITS needs, they are likely to have wrong statistics in the tables most of the time, so plans can quite possibly get worse rather than better;
** ANALYZE with statistics has a huge problem with disk space (MDEV-6529). In his experiment, SPetrunia used a table with 1M rows and got disk consumption of 300M upon ANALYZE. Our users (not just biggest customers) already routinely have tables with billions rows. So, it can easily be hundreds of gigabytes. 
** ANALYZE with the new configuration will have disastrous performance. When it was taking less than a second before, it will easily take hours. E.g. on my machine, ANALYZE for dbt3 table lineitem (sf1, 6 mln rows) takes 0.15 sec without statistics, and 40 seconds with statistics. ANALYZE on a table with 600 mln rows will take over an hour, possibly much more. Existing maintenance jobs will all be broken, they simply won't fit into the scheduled window.
** ANALYZE is replicated, even if nobody needs statistics on the slave, and thus will make the slave lag dramatically. OM=>NS setups which are used for upgrades will be most affected.
** regressions (see scenario 1)

3) Users who run ANALYZE TABLE specifically to collect EITS data
* gain: none, since they already have it configured and used at their discretion
* loss: possibly same as in scenario 2, especially if they are smart enough to run ANALYZE with explicit PERSISTENT FOR ... when they need statistics to be recollected, and run ""normal"" ANALYZE otherwise.

*log_queries_not_using_indexes=1* 

It's a good tool, but a crazy default. 
It's only useful when somebody wants to make a special effort to analyze and fine-tune their workflow, they can always turn it on temporarily. 
Otherwise, in the form that it exists, most people don't need it. 
If the query is really slow, it will be reflected in the log anyway as a general slow query. If it's not slow, nobody cares -- it's fine to query 10-row tables without indexes, and people do it all the time, the log will be huge. Apparently it was meant to be controlled by min-examined-row-limit, but it's not suitable for that, see below.
Until this option can be properly configured, it mustn't become default. 
It's not surprising that MySQL introduced log_throttle_queries_not_using_indexes (which I also don't like though), and even with it, they were smart enough not to turn this logging ON. 

*min-examined-row-limit=N*

It's yet another tool for analyzing and fine-tuning a special workflow, it mustn't be used as a default. What if a simple DML is slow because it's blocked for no good reason? What if ANALYZE is slow -- it won't be logged at all, regardless the number of rows in the table? What if you forgot a long sleep somewhere deep in your query or function? All of these should go to the slow log, but with this option enabled, they won't.


h3. {color:blue}Part 2: DISAGREE{color}

*default_tmp_storage_engine=InnoDB*

Pointless nuissance. It works much better now with 'none' and temporary tables defaulting to @@default_storage_engine. Vast majority of users have default engine either InnoDB, or MyISAM, or Aria, and maybe in future RocksDB, and would want their tmp tables be of the same engine as normal tables, right? That's what 'none' does. It's hard to imagine that somebody would want default engine MyISAM, but default tmp engine InnoDB. Setting it by default will only make non-InnoDB users have to remember to modify it separately. Worse still, people who run without InnoDB on whatever reason will start getting failures on startup until they reconfigure the instance. 
Those who run by default with very special engines which can't be used for tmp tables already specify the engine for temporary tables explicitly anyway. 

*long_query_time=2*

I think it's excessive, it will create more noise than useful output, and will be all false-positives. Instances grow in size and concurrency faster than they grow in overall speed, many queries are bound to be longer than 2 seconds even if there is nothing wrong with them. Besides, any fluctuations in IO can make it breach the limit.
When somebody reaches the point in their performance tuning that queries longer than 2 seconds really bother them, I think they'll be happy to decrease the value. But it won't be the majority.

*updatable_views_with_limit=OFF*

I don't see any reason to tighten this setting. AFAIR we haven't had a single complaint from users regarding effects of updates which are controlled by this option. Making it forbidden by default will most likely cause some existing 3rd-party tools stop working properly (at least MySQL manual says that some GUI tools tend to generate such queries), and thus will lower the adoption without any good reason -- people will try installing the new version, see it doesn't work well on some obscure reason, and will switch back. 

*innodb_log_file_size =128M* 

According to MySQL manual, it's meaningless. ""Sensible values range from 4MB to 1/N-th of the size of the buffer pool, where N is the number of log files in the group"". Currently default buffer pool size is 128M, and there are 2 log files in the group. 

*query_cache_strip_comments=1*

Has it ever been performance-tested? I'm not sure it's very valuable, but rather suspicious. Yes, there is a specific scenario when comments are painful for query cache, but I think it's mostly a test scenario (e.g. when queries are numerated or otherwise get unique identifiers in the comments -- I myself use it a lot in tests). 


h3. {color:green}OTHER NOTES{color}

*lock_wait_timeout* 

I also vote for lowering the value to something that people can realistically wait for (even 1 day would be much better than 1 year). Currently there is no practical difference between ""locked"" and ""frozen forever"". 

*innodb_buffer_pool_dump_at_shutdown=ON and innodb_buffer_pool_load_at_startup=ON*

I'm not very happy about it, not because these values are wrong by themselves, but because of our assorted issues with startup/shutdown timeouts in scripts. I expect a considerable growth of complaints.

*sync_binlog=1*

It would be good to have and publish at least some numbers of our performance sacrifice. Is it more like 1% or more like 50%?

*binlog_format=MIXED* 

While I support this change, I think we must make a clear statement about it in a form of a blog, preferably before the GA release, to give people time to prepare to it. The change can be absolutely disastrous for setups with insufficient indexes. People need to either make sure they switch back to STATEMENT or add indexes *before* they get into a trouble and start loudly complaining about the regression MariaDB introduced. 

_Also, it should be noted that in some cases MBR is not just ""unsafe DML is logged as ROW, everything else STMT"". There are some subtleties related to temporary tables and MBR, there have always been some bugs around it. Unfortunately, we can't see MySQL bug reports anymore, but the fact that they switched right to RBR rather than MBR makes me a little nervous about MIXED as a new default. I'll try to find time to run some tests around it._

*log-warnings=2*

Fine, but please make sure InnoDB notifications go to a level higher than 2.

*table_definition_cache = autosized*

Have we actually merged the logic which autosizes it? Did anyone review it?

*innodb_stats_traditional=off*

Has it been performance-tested (more or less recently)?

"
2558,MDEV-7635,MDEV,Daniel Black,87440,2016-10-17 01:39:41,"Thanks [~elenst], happy with QA Vetos - yes I'm happy with that idea.

log_queries_not_using_indexes, its meaning has changed to reflect a number of rows, commit d0064c6 - per https://lists.launchpad.net/maria-developers/msg09996.html I don't think its 100% right yet. min-examined-row-limit, partially agree that this is quite a brute force instrument - mentioned alternates in mailing list post.

binlog_format - I don't think a proper examination of pros/cons of MIXED/ROW has been done. Cases like galera need to be considered and the edge cases [~elenst] mentions. Also consider that package script detection of missing primary keys might be a good option to add a config file of binlog_format=MIXED(or keep STATEMENT) for those cases significantly affected by a ROW default. Other negatives or RBR - DML statements affecting large number of rows - can/do parallel slave options process each RBR component separately (they are in blocks of some size now anyway). innodb_force_primary_key=ON is appealing to me however apparently dragging users into modern SQL seems unpopular and letting them fail on edge cases is ok (hence sql_mode ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION).

A proper pros/cons of socket authentication could also be done rather than a 1/2 prepared 10mins at a dev meetup.

innodb_buffer_pool_size, also a horrible default because of the need to support those running on desktops or choosing to install on very small VMs. Is a packaging prompt to choose between ""dedicated DB"" and ""shared server"" (or a scale in-between) to configure between 80 and 20% of free RAM an option? Perhaps choose 50% of free RAM for server distros like RHEL/Centos. innodb_log_file_size could flow from this value. Making a better default for users with decent hardware and limited experience would be good.

skip-name-resolve could also be installed based on a packaging script if DNS usernames aren't being used. A warning based on creation should also exist if this option is enabled.

innodb_buffer_pool_dump_at_shutdown - agree timeout is an issue which is only slightly mitigated by innodb_buffer_pool_dump_pct. -I think closing MDEV-9202 without action was a mistake-.
edit: load is in background while active so really only shutdown time is important here.

log-slow-verbosity=query_plan,explain hasn't been mentioned and I think it would be good to log more details since its available.",12,"Thanks [~elenst], happy with QA Vetos - yes I'm happy with that idea.

log_queries_not_using_indexes, its meaning has changed to reflect a number of rows, commit d0064c6 - per URL I don't think its 100% right yet. min-examined-row-limit, partially agree that this is quite a brute force instrument - mentioned alternates in mailing list post.

binlog_format - I don't think a proper examination of pros/cons of MIXED/ROW has been done. Cases like galera need to be considered and the edge cases [~elenst] mentions. Also consider that package script detection of missing primary keys might be a good option to add a config file of binlog_format=MIXED(or keep STATEMENT) for those cases significantly affected by a ROW default. Other negatives or RBR - DML statements affecting large number of rows - can/do parallel slave options process each RBR component separately (they are in blocks of some size now anyway). innodb_force_primary_key=ON is appealing to me however apparently dragging users into modern SQL seems unpopular and letting them fail on edge cases is ok (hence sql_mode ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION).

A proper pros/cons of socket authentication could also be done rather than a 1/2 prepared 10mins at a dev meetup.

innodb_buffer_pool_size, also a horrible default because of the need to support those running on desktops or choosing to install on very small VMs. Is a packaging prompt to choose between ""dedicated DB"" and ""shared server"" (or a scale in-between) to configure between 80 and 20% of free RAM an option? Perhaps choose 50% of free RAM for server distros like RHEL/Centos. innodb_log_file_size could flow from this value. Making a better default for users with decent hardware and limited experience would be good.

skip-name-resolve could also be installed based on a packaging script if DNS usernames aren't being used. A warning based on creation should also exist if this option is enabled.

innodb_buffer_pool_dump_at_shutdown - agree timeout is an issue which is only slightly mitigated by innodb_buffer_pool_dump_pct. -I think closing MDEV-9202 without action was a mistake-.
edit: load is in background while active so really only shutdown time is important here.

log-slow-verbosity=query_plan,explain hasn't been mentioned and I think it would be good to log more details since its available."
2559,MDEV-7635,MDEV,Nirbhay Choubey,87503,2016-10-17 20:18:59,"I have updated the defaults list in the description to reflect arguments made by [~elenst] and [~danblack].
Also, based on the feedbacks, existing open issues and their current values in 5.7, I think its logical to keep
the current defaults for the following variables (no changes):

* innodb_stats_traditional
* histogram_size
* log_queries_not_using_indexes
* long_query_time
* lower_case_table_names
* min_examined_row_limit,
* optimizer_use_condition_selectivity
* updatable_views_with_limit
* use_stat_tables
* query_cache_strip_comments

There are still some variables, for which I am not clear about (and thus I have not changed them yet):

* innodb_log_file_size = (proposed) 128M : Based on the rule that [~elenst] pointed, should we change it to 64M or leave it to 48M?
* long_query_time = (proposed) 2 : I agree that the proposed value could be pretty low. Should we change it to 5secs or leave it to 10secs?
* table_definition_cache = (proposed) autosized : I believe 'autosized' is not implemented yet.
* log_slow_verbosity = query_plan,explain : Proposed by [~danblack]. @elena : any comments on this?
* sql_mode : 5.7 has following additional mode ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION
* optimizer_switch : The following switches are currently off in 10.2 while they are on in 5.7: engine_condition_pushdown=on,mrr=on,mrr_cost_based=on.
* Charsets: Should we make changes to the default client/server character sets as was discussed during the meetup?
",13,"I have updated the defaults list in the description to reflect arguments made by [~elenst] and [~danblack].
Also, based on the feedbacks, existing open issues and their current values in 5.7, I think its logical to keep
the current defaults for the following variables (no changes):

* innodb_stats_traditional
* histogram_size
* log_queries_not_using_indexes
* long_query_time
* lower_case_table_names
* min_examined_row_limit,
* optimizer_use_condition_selectivity
* updatable_views_with_limit
* use_stat_tables
* query_cache_strip_comments

There are still some variables, for which I am not clear about (and thus I have not changed them yet):

* innodb_log_file_size = (proposed) 128M : Based on the rule that [~elenst] pointed, should we change it to 64M or leave it to 48M?
* long_query_time = (proposed) 2 : I agree that the proposed value could be pretty low. Should we change it to 5secs or leave it to 10secs?
* table_definition_cache = (proposed) autosized : I believe 'autosized' is not implemented yet.
* log_slow_verbosity = query_plan,explain : Proposed by [~danblack]. @elena : any comments on this?
* sql_mode : 5.7 has following additional mode ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION
* optimizer_switch : The following switches are currently off in 10.2 while they are on in 5.7: engine_condition_pushdown=on,mrr=on,mrr_cost_based=on.
* Charsets: Should we make changes to the default client/server character sets as was discussed during the meetup?
"
2560,MDEV-7635,MDEV,Elena Stepanova,87504,2016-10-17 21:00:52,"{quote}
innodb_log_file_size = (proposed) 128M : Based on the rule that Elena Stepanova pointed, should we change it to 64M or leave it to 48M?
{quote}
IMO, just keep it the same as it is in MySQL. Increasing to 64M is not a big gain anyway, and even though resizing is not a critical problem anymore, there is no reason to do it when unnecessary. I'd leave it to [~jplindst] to decide. 

{quote}
long_query_time = (proposed) 2 : I agree that the proposed value could be pretty low. Should we change it to 5secs or leave it to 10secs?
{quote}
It doesn't matter much -- as long as the value is not too low; it's totally arbitrary anyway. I'd keep it to 10 seconds on one simple reason -- as soon as people upgrade and start getting their slow log growing rapidly, they'll assume it's a performance degradation, and before they figure out it's really not, there might be some FUD and/or rollbacks to previous versions. 

{quote}
log_slow_verbosity = query_plan,explain : Proposed by Daniel Black. @elena : any comments on this?
{quote}
I lean towards ""no"", but I don't have a strong enough opinion on the subject. 
There have been some bugs around {{verbosity=explain}}, e.g. MDEV-6439, although it's old, it might have already gone away (since it's about charsets which have changed a lot). Also, performance impact should be considered. 
In general, I don't quite agree we should have everything in the log just because it's available. As a tester, I'd love that, but it's not always in users' best interest. If a user has a query which shows up in the slow log every now and then, they can raise the verbosity at runtime and get the additional info. One case when they could actually benefit from the default is if the slow query was a one-time thing, but they are still desperate to find out what happened there (rare, but possible). 
{quote}
sql_mode : 5.7 has following additional mode ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION
{quote}
I ran MTR tests with ERROR_FOR_DIVISION_BY_ZERO upon Monty's request. It will require some test updates (see MDEV-10981), but it's doable, and I didn't see anything alarming during these tests.
ONLY_FULL_GROUP_BY -- I actually like it myself, but we have surprisingly many users who routinely run queries which don't satisfy the requirements, so there will be lots of angry screams. 
I have no opinion on the other values, but I think Monty does, please check with him so that we don't have to rollback at the last moment. 

{quote}
optimizer_switch : The following switches are currently off in 10.2 while they are on in 5.7: engine_condition_pushdown=on,mrr=on,mrr_cost_based=on.
{quote}
- engine_condition_pushdown is deprecated in MariaDB, so it certainly doesn't go to defaults.
- mrr has always seemed shaky to me in terms of quality, but I personally didn't do a lot of testing for it, only in early 5.3 -- it was implemented before my time and hasn't much changed since. If optimizer team really wants to turn it on and is willing to address possible bugs in a timely manner, I can run some tests for it. ",14,"{quote}
innodb_log_file_size = (proposed) 128M : Based on the rule that Elena Stepanova pointed, should we change it to 64M or leave it to 48M?
{quote}
IMO, just keep it the same as it is in MySQL. Increasing to 64M is not a big gain anyway, and even though resizing is not a critical problem anymore, there is no reason to do it when unnecessary. I'd leave it to [~jplindst] to decide. 

{quote}
long_query_time = (proposed) 2 : I agree that the proposed value could be pretty low. Should we change it to 5secs or leave it to 10secs?
{quote}
It doesn't matter much -- as long as the value is not too low; it's totally arbitrary anyway. I'd keep it to 10 seconds on one simple reason -- as soon as people upgrade and start getting their slow log growing rapidly, they'll assume it's a performance degradation, and before they figure out it's really not, there might be some FUD and/or rollbacks to previous versions. 

{quote}
log_slow_verbosity = query_plan,explain : Proposed by Daniel Black. @elena : any comments on this?
{quote}
I lean towards ""no"", but I don't have a strong enough opinion on the subject. 
There have been some bugs around {{verbosity=explain}}, e.g. MDEV-6439, although it's old, it might have already gone away (since it's about charsets which have changed a lot). Also, performance impact should be considered. 
In general, I don't quite agree we should have everything in the log just because it's available. As a tester, I'd love that, but it's not always in users' best interest. If a user has a query which shows up in the slow log every now and then, they can raise the verbosity at runtime and get the additional info. One case when they could actually benefit from the default is if the slow query was a one-time thing, but they are still desperate to find out what happened there (rare, but possible). 
{quote}
sql_mode : 5.7 has following additional mode ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION
{quote}
I ran MTR tests with ERROR_FOR_DIVISION_BY_ZERO upon Monty's request. It will require some test updates (see MDEV-10981), but it's doable, and I didn't see anything alarming during these tests.
ONLY_FULL_GROUP_BY -- I actually like it myself, but we have surprisingly many users who routinely run queries which don't satisfy the requirements, so there will be lots of angry screams. 
I have no opinion on the other values, but I think Monty does, please check with him so that we don't have to rollback at the last moment. 

{quote}
optimizer_switch : The following switches are currently off in 10.2 while they are on in 5.7: engine_condition_pushdown=on,mrr=on,mrr_cost_based=on.
{quote}
- engine_condition_pushdown is deprecated in MariaDB, so it certainly doesn't go to defaults.
- mrr has always seemed shaky to me in terms of quality, but I personally didn't do a lot of testing for it, only in early 5.3 -- it was implemented before my time and hasn't much changed since. If optimizer team really wants to turn it on and is willing to address possible bugs in a timely manner, I can run some tests for it. "
2561,MDEV-7635,MDEV,Nirbhay Choubey,87505,2016-10-17 21:40:23,"Also, with current set of defaults the following warning would pop up on server start :
{code}
2016-10-17 16:23:50 139906085902144 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --log_slow_queries is not set
{code}",15,"Also, with current set of defaults the following warning would pop up on server start :
{code}
2016-10-17 16:23:50 139906085902144 [Warning] options --log-slow-admin-statements, --log-queries-not-using-indexes and --log-slow-slave-statements have no effect if --log_slow_queries is not set
{code}"
2562,MDEV-7635,MDEV,Daniel Black,87507,2016-10-17 22:54:43,"Case for innodb_stats_traditional=false follows from MDEV-7084 where it was introduced. The ability of the optimizer to make really poor decisions based on too little information for large tables results can result in large table scans. The cost is that occasionally when innodb stats are pulled more pages are pulled to generate the statistics, (log2(rows)*innodb_stats_sample_pages), and this is significantly less impacting than the wrong query plan. This was extensively deployed at my previous employer across many clients resulting in no similar spurious queries suddenly doing a range/ALL scan on multimillion row tables because a more accurate sampling realised a consistent predictable query using an alternative index.

Rational for log_slow_verbosity = query_plan,explain. when a query has taken 2-10 seconds already the overhead of putting an extra 3-10 lines in log file is rather small. Disk space shouldn't be an issue for this amount and if they are going to do an explain anyway to debug it may as well include it at the time of the query.

bq. ONLY_FULL_GROUP_BY – I actually like it myself, but we have surprisingly many users who routinely run queries which don't satisfy the requirements, so there will be lots of angry screams.

Lets mitigate this with a clear notice in the release notes/blog, like binlog_format, long_query_time!=10 as it is impacting, but in a way that will help users generally, even if they need to change their query, indexes, or add a primary key. It is a major version upgrade, expecting users to read release notes and do basic testing I think is a reasonable expectation.

bq. I have no opinion on the other values, but I think Monty does, please check with him so that we don't have to rollback at the last moment. 

While I'm in favour of a QA veto utilizing the experience of multiple avenues of bug reports to gauge the relative stability of features, I don't think a Monty veto based on ""I wrote the code"" or whatever reason appropriately keeps with the ideals of a community rough consensus for technical decisions. There where a few hallway/pub comments at Percona Live / MariaDB dev meetup specifically around Monty vetos/code gatekeeper, which I defended (perhaps prematurely), and as such having a strong option and proclaiming ""for the good of the users"" shouldn't equate to veto because community raise changes for the same reason.",16,"Case for innodb_stats_traditional=false follows from MDEV-7084 where it was introduced. The ability of the optimizer to make really poor decisions based on too little information for large tables results can result in large table scans. The cost is that occasionally when innodb stats are pulled more pages are pulled to generate the statistics, (log2(rows)*innodb_stats_sample_pages), and this is significantly less impacting than the wrong query plan. This was extensively deployed at my previous employer across many clients resulting in no similar spurious queries suddenly doing a range/ALL scan on multimillion row tables because a more accurate sampling realised a consistent predictable query using an alternative index.

Rational for log_slow_verbosity = query_plan,explain. when a query has taken 2-10 seconds already the overhead of putting an extra 3-10 lines in log file is rather small. Disk space shouldn't be an issue for this amount and if they are going to do an explain anyway to debug it may as well include it at the time of the query.

bq. ONLY_FULL_GROUP_BY – I actually like it myself, but we have surprisingly many users who routinely run queries which don't satisfy the requirements, so there will be lots of angry screams.

Lets mitigate this with a clear notice in the release notes/blog, like binlog_format, long_query_time!=10 as it is impacting, but in a way that will help users generally, even if they need to change their query, indexes, or add a primary key. It is a major version upgrade, expecting users to read release notes and do basic testing I think is a reasonable expectation.

bq. I have no opinion on the other values, but I think Monty does, please check with him so that we don't have to rollback at the last moment. 

While I'm in favour of a QA veto utilizing the experience of multiple avenues of bug reports to gauge the relative stability of features, I don't think a Monty veto based on ""I wrote the code"" or whatever reason appropriately keeps with the ideals of a community rough consensus for technical decisions. There where a few hallway/pub comments at Percona Live / MariaDB dev meetup specifically around Monty vetos/code gatekeeper, which I defended (perhaps prematurely), and as such having a strong option and proclaiming ""for the good of the users"" shouldn't equate to veto because community raise changes for the same reason."
2563,MDEV-7635,MDEV,Elena Stepanova,87510,2016-10-17 23:38:47,"{quote}
I don't think a Monty veto based on ""I wrote the code"" or whatever reason appropriately keeps with the ideals of a community rough consensus for technical decisions.
{quote}
I wasn't talking about a veto. In this issue, opinion of all interested parties is considered. In regard to {{sql_mode}} nobody so far has expressed particularly strong opinion. It makes all sense to consult with and listen to the reasoning from somebody who a) does have an opinion; b) does actually speak to different users and customers; and c) does actually know the code, regardless of writing or not writing it. 

He does not watch issues which don't belong to him unless specifically asked to, so he might come up with his comments quite late, which will make things more complicated should others agree with his reasoning. ",17,"{quote}
I don't think a Monty veto based on ""I wrote the code"" or whatever reason appropriately keeps with the ideals of a community rough consensus for technical decisions.
{quote}
I wasn't talking about a veto. In this issue, opinion of all interested parties is considered. In regard to {{sql_mode}} nobody so far has expressed particularly strong opinion. It makes all sense to consult with and listen to the reasoning from somebody who a) does have an opinion; b) does actually speak to different users and customers; and c) does actually know the code, regardless of writing or not writing it. 

He does not watch issues which don't belong to him unless specifically asked to, so he might come up with his comments quite late, which will make things more complicated should others agree with his reasoning. "
2564,MDEV-7635,MDEV,Igor Babaev,87531,2016-10-18 17:35:09,"Nirbhay,

What is the exact reason not to have by default:
optimizer_use_condition_selectivity=4
?",18,"Nirbhay,

What is the exact reason not to have by default:
optimizer_use_condition_selectivity=4
?"
2565,MDEV-7635,MDEV,Nirbhay Choubey,87533,2016-10-18 17:59:23,"[~igor] Please check Elena's comments above. Also, we recently found a bug around it MDEV-11062.",19,"[~igor] Please check Elena's comments above. Also, we recently found a bug around it MDEV-11062."
2566,MDEV-7635,MDEV,Vladislav Vaintroub,87602,2016-10-19 00:06:28,"sql_mode = ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION

No zero in dates is really useful for connectors. dates with zeroes are nonsense especially any languages that have a type for dates and timestamps, so connectors go great length to map invalid dates to exception or to null or to special date in (.NET uses year 1601 if I recall correctly)

STRICT_TRANS_TABLES, NO_ENGINE_SUBSTITUTION - this one actually in use by the Windows installer since 5.2 (i.e sql_mode set to this in my.ini  during installation). I do not recall any complains, this seems very safe. NO_ENGINE_SUBSTITUTION a must in my opinion.

",20,"sql_mode = ONLY_FULL_GROUP_BY STRICT_TRANS_TABLES NO_ZERO_IN_DATE NO_ZERO_DATE ERROR_FOR_DIVISION_BY_ZERO NO_AUTO_CREATE_USER NO_ENGINE_SUBSTITUTION

No zero in dates is really useful for connectors. dates with zeroes are nonsense especially any languages that have a type for dates and timestamps, so connectors go great length to map invalid dates to exception or to null or to special date in (.NET uses year 1601 if I recall correctly)

STRICT_TRANS_TABLES, NO_ENGINE_SUBSTITUTION - this one actually in use by the Windows installer since 5.2 (i.e sql_mode set to this in my.ini  during installation). I do not recall any complains, this seems very safe. NO_ENGINE_SUBSTITUTION a must in my opinion.

"
2567,MDEV-7635,MDEV,Sergei Golubchik,87846,2016-10-27 21:52:44,"My comments based on what is in *bb-10.2-mdev7635* branch, as of
{noformat}
commit 6f36f28
Author: Nirbhay Choubey
Date:   Tue Oct 25 16:37:21 2016 -0400

    MDEV-7635: Fix expected warning regex (should not prefix process)
{noformat}

{code:diff}
--- a/mysql-test/suite/sys_vars/r/sysvars_aria.result
+++ b/mysql-test/suite/sys_vars/r/sysvars_aria.result
@@ -213,9 +213,9 @@ READ_ONLY	NO
 COMMAND_LINE_ARGUMENT	REQUIRED
 VARIABLE_NAME	ARIA_RECOVER_OPTIONS
 SESSION_VALUE	NULL
-GLOBAL_VALUE	NORMAL
+GLOBAL_VALUE	BACKUP,FORCE
{code}
I'd do BACKUP,QUICK. Unexpected multi-hour repair or automatic data loss
could be quite a WTF.
Same for MyISAM.
{code:diff}
--- a/mysql-test/suite/sys_vars/r/sysvars_innodb.result
+++ b/mysql-test/suite/sys_vars/r/sysvars_innodb.result
@@ -161,9 +161,9 @@ READ_ONLY	NO
 COMMAND_LINE_ARGUMENT	REQUIRED
 VARIABLE_NAME	INNODB_AUTOINC_LOCK_MODE
 SESSION_VALUE	NULL
-GLOBAL_VALUE	1
+GLOBAL_VALUE	2
{code}
This is unsafe in SBR, so I don't think we can change this now.
{code:diff}
--- a/mysql-test/suite/sys_vars/r/sysvars_server_embedded.result
+++ b/mysql-test/suite/sys_vars/r/sysvars_server_embedded.result
@@ -1746,10 +1746,10 @@ ENUM_VALUE_LIST	innodb,query_plan,explain
 READ_ONLY	NO
 COMMAND_LINE_ARGUMENT	REQUIRED
 VARIABLE_NAME	LOG_WARNINGS
-SESSION_VALUE	1
-GLOBAL_VALUE	1
+SESSION_VALUE	2
+GLOBAL_VALUE	2
 GLOBAL_VALUE_ORIGIN	COMPILE-TIME
-DEFAULT_VALUE	1
+DEFAULT_VALUE	2
{code}
I'm not sure it's a good idea, error log is contains too much noise as it is
{code:diff}
--- a/mysql-test/r/index_merge_innodb.result
+++ b/mysql-test/r/index_merge_innodb.result
@@ -106,6 +106,7 @@ insert into t1 (key1a, key1b, key2a, key2b, key3a, key3b)
 select key1a, key1b, key2a, key2b, key3a, key3b from t1;
 analyze table t1;
 Table	Op	Msg_type	Msg_text
+test.t1	analyze	status	Engine-independent statistics collected
{code}
where does that come from?
{code:diff}
--- a/mysql-test/r/statistics.result
+++ b/mysql-test/r/statistics.result
@@ -925,36 +925,36 @@ db_name	table_name	cardinality
 test	t1	40
 SELECT * FROM mysql.column_stats;
 db_name	table_name	column_name	min_value	max_value	nulls_ratio	avg_length	avg_frequency	hist_size	hist_type	histogram
-test	t1	c	aaaa	dddddddd	0.1250	6.6571	7.0000	0	NULL	NULL
-test	t1	e	0.01	0.112	0.2250	8.0000	6.2000	0	NULL	NULL
 test	t1	b	vvvvvvvvvvvvv	zzzzzzzzzzzzzzzzzz	0.2000	17.1250	6.4000	0	NULL	NULL
+test	t1	e	0.01	0.112	0.2250	8.0000	6.2000	0	NULL	NULL
+test	t1	c	aaaa	dddddddd	0.1250	6.6571	7.0000	0	NULL	NULL
{code}
where does that come from?
(same reason as above, I suspect)
",21,"My comments based on what is in *bb-10.2-mdev7635* branch, as of
{noformat}
commit 6f36f28
Author: Nirbhay Choubey
Date:   Tue Oct 25 16:37:21 2016 -0400

    MDEV-7635: Fix expected warning regex (should not prefix process)
{noformat}

{code:diff}
--- a/mysql-test/suite/sys_vars/r/sysvars_aria.result
+++ b/mysql-test/suite/sys_vars/r/sysvars_aria.result
@@ -213,9 +213,9 @@ READ_ONLY	NO
 COMMAND_LINE_ARGUMENT	REQUIRED
 VARIABLE_NAME	ARIA_RECOVER_OPTIONS
 SESSION_VALUE	NULL
-GLOBAL_VALUE	NORMAL
+GLOBAL_VALUE	BACKUP,FORCE
{code}
I'd do BACKUP,QUICK. Unexpected multi-hour repair or automatic data loss
could be quite a WTF.
Same for MyISAM.
{code:diff}
--- a/mysql-test/suite/sys_vars/r/sysvars_innodb.result
+++ b/mysql-test/suite/sys_vars/r/sysvars_innodb.result
@@ -161,9 +161,9 @@ READ_ONLY	NO
 COMMAND_LINE_ARGUMENT	REQUIRED
 VARIABLE_NAME	INNODB_AUTOINC_LOCK_MODE
 SESSION_VALUE	NULL
-GLOBAL_VALUE	1
+GLOBAL_VALUE	2
{code}
This is unsafe in SBR, so I don't think we can change this now.
{code:diff}
--- a/mysql-test/suite/sys_vars/r/sysvars_server_embedded.result
+++ b/mysql-test/suite/sys_vars/r/sysvars_server_embedded.result
@@ -1746,10 +1746,10 @@ ENUM_VALUE_LIST	innodb,query_plan,explain
 READ_ONLY	NO
 COMMAND_LINE_ARGUMENT	REQUIRED
 VARIABLE_NAME	LOG_WARNINGS
-SESSION_VALUE	1
-GLOBAL_VALUE	1
+SESSION_VALUE	2
+GLOBAL_VALUE	2
 GLOBAL_VALUE_ORIGIN	COMPILE-TIME
-DEFAULT_VALUE	1
+DEFAULT_VALUE	2
{code}
I'm not sure it's a good idea, error log is contains too much noise as it is
{code:diff}
--- a/mysql-test/r/index_merge_innodb.result
+++ b/mysql-test/r/index_merge_innodb.result
@@ -106,6 +106,7 @@ insert into t1 (key1a, key1b, key2a, key2b, key3a, key3b)
 select key1a, key1b, key2a, key2b, key3a, key3b from t1;
 analyze table t1;
 Table	Op	Msg_type	Msg_text
+test.t1	analyze	status	Engine-independent statistics collected
{code}
where does that come from?
{code:diff}
--- a/mysql-test/r/statistics.result
+++ b/mysql-test/r/statistics.result
@@ -925,36 +925,36 @@ db_name	table_name	cardinality
 test	t1	40
 SELECT * FROM mysql.column_stats;
 db_name	table_name	column_name	min_value	max_value	nulls_ratio	avg_length	avg_frequency	hist_size	hist_type	histogram
-test	t1	c	aaaa	dddddddd	0.1250	6.6571	7.0000	0	NULL	NULL
-test	t1	e	0.01	0.112	0.2250	8.0000	6.2000	0	NULL	NULL
 test	t1	b	vvvvvvvvvvvvv	zzzzzzzzzzzzzzzzzz	0.2000	17.1250	6.4000	0	NULL	NULL
+test	t1	e	0.01	0.112	0.2250	8.0000	6.2000	0	NULL	NULL
+test	t1	c	aaaa	dddddddd	0.1250	6.6571	7.0000	0	NULL	NULL
{code}
where does that come from?
(same reason as above, I suspect)
"
2568,MDEV-7635,MDEV,Nirbhay Choubey,87867,2016-10-28 13:43:15,"[~serg]:
{quote}
I'd do BACKUP,QUICK. Unexpected multi-hour repair or automatic data loss
could be quite a WTF.
Same for MyISAM.
{quote}
OK, I will update them.
 
{quote}
This is unsafe in SBR, so I don't think we can change this now.
{quote}
Good point. will revert it.

{quote}
I'm not sure it's a good idea, error log is contains too much noise as it is
{quote}
hmm.. I did not notice a lot of change in the test results after setting --log-warnings=2. But, the logs could get flooded with warnings like ""Deadlock found when trying to get lock"".

{quote}
where does that come from?
{quote}
oh.. its leftover from the commit where I had use_stat_tables enabled and later the test always skipped due to ""Requires: have_xtradb"" so it was never caught.

{quote}
where does that come from?
(same reason as above, I suspect)
{quote}
No, its due to the change in binlog_format. For some reason changing the binlog format caused the row order to change.",22,"[~serg]:
{quote}
I'd do BACKUP,QUICK. Unexpected multi-hour repair or automatic data loss
could be quite a WTF.
Same for MyISAM.
{quote}
OK, I will update them.
 
{quote}
This is unsafe in SBR, so I don't think we can change this now.
{quote}
Good point. will revert it.

{quote}
I'm not sure it's a good idea, error log is contains too much noise as it is
{quote}
hmm.. I did not notice a lot of change in the test results after setting --log-warnings=2. But, the logs could get flooded with warnings like ""Deadlock found when trying to get lock"".

{quote}
where does that come from?
{quote}
oh.. its leftover from the commit where I had use_stat_tables enabled and later the test always skipped due to ""Requires: have_xtradb"" so it was never caught.

{quote}
where does that come from?
(same reason as above, I suspect)
{quote}
No, its due to the change in binlog_format. For some reason changing the binlog format caused the row order to change."
2569,MDEV-7635,MDEV,Nirbhay Choubey,87907,2016-10-30 21:19:05,"@serg: Pushed the changes to bb-10.2-mdev7635 branch.
1) https://github.com/MariaDB/server/commit/c67e1e3e4fd40cb100ea3a9da1af53ec33a94817 (defaults changes)
2) https://github.com/MariaDB/server/commit/9912b1420d1212de971c3b953a54031f6a7982d0 (s/standards_compliant_cte/standard_compliant_cte)",23,"@serg: Pushed the changes to bb-10.2-mdev7635 branch.
1) URL (defaults changes)
2) URL (s/standards_compliant_cte/standard_compliant_cte)"
2570,MDEV-7635,MDEV,Daniel Black,89829,2016-12-21 23:42:09,"expire_log_days, 0 (aka infinity) is an exceptionally large number of days. With a default max_binlog_size of 1G and general storage size I suspect 20 may be a better default value.",24,"expire_log_days, 0 (aka infinity) is an exceptionally large number of days. With a default max_binlog_size of 1G and general storage size I suspect 20 may be a better default value."
2571,MDEV-7635,MDEV,Elena Stepanova,89837,2016-12-22 00:04:09,"I disagree, rather strongly. Keeping binary logs can be crucially important for various reasons. Removing them automatically by default can be a disaster, which the user will only find out when it's too late. Size does not matter here -- the activity can be low, the disk can be large, and any reasonable real-life setup will monitor the disk status anyway, and the admin will make the decision about binary logs if it comes to that. ",25,"I disagree, rather strongly. Keeping binary logs can be crucially important for various reasons. Removing them automatically by default can be a disaster, which the user will only find out when it's too late. Size does not matter here -- the activity can be low, the disk can be large, and any reasonable real-life setup will monitor the disk status anyway, and the admin will make the decision about binary logs if it comes to that. "
2572,MDEV-7635,MDEV,Daniel Black,89840,2016-12-22 00:42:25,Fair call. disk monitoring is a much better approach.,26,Fair call. disk monitoring is a much better approach.
2573,MDEV-7635,MDEV,Vladislav Vaintroub,89841,2016-12-22 00:51:18,"I propose (discussed with [~elenst] in the past while analyzing MDEV-11226)

thread_pool_max_threads=65535

Usually, there should not really be an upper bound to thread count by default, but DBAs who know their workload well,  might want set it. 
had a patch for this variable myself, but since you're already at it, and chance the same places, maybe it should go into this one.

",27,"I propose (discussed with [~elenst] in the past while analyzing MDEV-11226)

thread_pool_max_threads=65535

Usually, there should not really be an upper bound to thread count by default, but DBAs who know their workload well,  might want set it. 
had a patch for this variable myself, but since you're already at it, and chance the same places, maybe it should go into this one.

"
2574,MDEV-7635,MDEV,Elena Stepanova,91477,2017-02-06 12:50:35,"[~nirbhay_c], 

As requested, I've looked at [bb-10.2-mdev7635|https://github.com/MariaDB/server/tree/bb-10.2-mdev7635], diff between fd0479ce592e0b7c13d67b5deda62e9090956309 and the current e2d94b684a070b94519cd45e923a3c41267f66c4. 

There are a few suites that haven't been fixed. I've noticed
- mroonga/* (most important, since it's a part of the default set)
- jp (also runs in buildbot on fulltest)
- engines/*

I expect you're going to adjust them as well.

Also I have some doubts about innodb_storedproc* tests. Unlike in other tests, here you've kept the strict mode without {{ignore}} close, so that it didn't allow data to be loaded, which naturally further caused differences in result sets which were re-recorded. Unless you did it on purpose, maybe it makes sense to restore the test logic.

Apart from those, it looks mostly fine. I have some technical suggestions/fixes, all of which are pushed into bb-10.2-mdev7635.elenst, see
https://github.com/MariaDB/server/commits/bb-10.2-mdev7635.elenst

I intentionally created very itemized commits, so that it was clear which is doing what, and you could pick them up separately (they are mostly independent, except for maybe one or two).
You only need those that start with ""Additional test fixes for MDEV-7635 #N"". There are 13 of them now.
You don't need two others, one of which is to exclude mroonga from defaults (you don't need to merge it as it has to be fixed anyway), and one to disable a test which currently fails on 10.2 (you don't need to merge it either, it's already disabled in 10.2 and will be re-enabled when the problem is fixed).

Not related to tests directly, I still have some concerns about sync-binlog (see MDEV-11900), and about the new combination of slow-log options which ends up with a warning (see MDEV-11908), I don't think it makes any sense.",28,"[~nirbhay_c], 

As requested, I've looked at [bb-10.2-mdev7635|URL diff between fd0479ce592e0b7c13d67b5deda62e9090956309 and the current e2d94b684a070b94519cd45e923a3c41267f66c4. 

There are a few suites that haven't been fixed. I've noticed
- mroonga/* (most important, since it's a part of the default set)
- jp (also runs in buildbot on fulltest)
- engines/*

I expect you're going to adjust them as well.

Also I have some doubts about innodb_storedproc* tests. Unlike in other tests, here you've kept the strict mode without {{ignore}} close, so that it didn't allow data to be loaded, which naturally further caused differences in result sets which were re-recorded. Unless you did it on purpose, maybe it makes sense to restore the test logic.

Apart from those, it looks mostly fine. I have some technical suggestions/fixes, all of which are pushed into bb-10.2-mdev7635.elenst, see
URL

I intentionally created very itemized commits, so that it was clear which is doing what, and you could pick them up separately (they are mostly independent, except for maybe one or two).
You only need those that start with ""Additional test fixes for MDEV-7635 #N"". There are 13 of them now.
You don't need two others, one of which is to exclude mroonga from defaults (you don't need to merge it as it has to be fixed anyway), and one to disable a test which currently fails on 10.2 (you don't need to merge it either, it's already disabled in 10.2 and will be re-enabled when the problem is fixed).

Not related to tests directly, I still have some concerns about sync-binlog (see MDEV-11900), and about the new combination of slow-log options which ends up with a warning (see MDEV-11908), I don't think it makes any sense."
2575,MDEV-7635,MDEV,Nirbhay Choubey,91729,2017-02-10 12:14:55,"thread_pool_max_threads=65535
https://github.com/MariaDB/server/commit/def258061b884df91624562c27c818f9d1fa2d5c",29,"thread_pool_max_threads=65535
URL"
2576,MDEV-7660,MDEV,Sergey Vojtovich,83254,2016-05-06 11:22:32,"[~jplindst], could you review last patch in bb-10.2-mdev7660?

All tests except for handler.innodb succeeded. The latter failed because LOCK TABLE ... WRITE didn't protect table from HANDLER ... READ.

I need your help to complete this patch, specifically:
- fix LOCK TABLE ... WRITE to block concurrent HANDLER ... READ
- fix LOCK TABLE ... READ to block concurrent DML

Test for handler (should hang):
{noformat}
--source include/have_xtradb.inc

CREATE TABLE t1(a INT) ENGINE=InnoDB;
INSERT INTO t1 VALUES(1);

connect(con1, localhost, root);
HANDLER t1 OPEN;

connection default;
LOCK TABLE t1 WRITE;

connection con1;
HANDLER t1 READ NEXT;
disconnect con1;

connection default;
UNLOCK TABLES;
DROP TABLE t1;
{noformat}

Test for lock table read (should hang):
{noformat}
CREATE TABLE t1(a INT) ENGINE=InnoDB;
LOCK TABLE t1 READ;

connect(con1, localhost, root);
INSERT INTO t1 VALUES(1);
disconnect con1;

connection default;
UNLOCK TABLES;
DROP TABLE t1;
{noformat}",1,"[~jplindst], could you review last patch in bb-10.2-mdev7660?

All tests except for handler.innodb succeeded. The latter failed because LOCK TABLE ... WRITE didn't protect table from HANDLER ... READ.

I need your help to complete this patch, specifically:
- fix LOCK TABLE ... WRITE to block concurrent HANDLER ... READ
- fix LOCK TABLE ... READ to block concurrent DML

Test for handler (should hang):
{noformat}
--source include/have_xtradb.inc

CREATE TABLE t1(a INT) ENGINE=InnoDB;
INSERT INTO t1 VALUES(1);

connect(con1, localhost, root);
HANDLER t1 OPEN;

connection default;
LOCK TABLE t1 WRITE;

connection con1;
HANDLER t1 READ NEXT;
disconnect con1;

connection default;
UNLOCK TABLES;
DROP TABLE t1;
{noformat}

Test for lock table read (should hang):
{noformat}
CREATE TABLE t1(a INT) ENGINE=InnoDB;
LOCK TABLE t1 READ;

connect(con1, localhost, root);
INSERT INTO t1 VALUES(1);
disconnect con1;

connection default;
UNLOCK TABLES;
DROP TABLE t1;
{noformat}"
2577,MDEV-7660,MDEV,Sergey Vojtovich,83255,2016-05-06 11:41:16,"Apparently LOCK TABLE ... WRITE blocks concurrent DML at MDL level. HANDLER ... READ is not blocked because it doesn't upgrade MDL lock.

It likely means that neither LOCK TABLE ... WRITE nor LOCK TABLE ... READ is currently handled by InnoDB.",2,"Apparently LOCK TABLE ... WRITE blocks concurrent DML at MDL level. HANDLER ... READ is not blocked because it doesn't upgrade MDL lock.

It likely means that neither LOCK TABLE ... WRITE nor LOCK TABLE ... READ is currently handled by InnoDB."
2578,MDEV-7660,MDEV,Jan Lindström,83283,2016-05-09 05:17:54,"Remember that InnoDB table locks are transactional, see http://dev.mysql.com/doc/refman/5.7/en/lock-tables-and-transactions.html , I see correct 
{noformat}
dberr_t	error = row_lock_table_for_mysql(prebuilt, NULL, 0);
{noformat}

calls on ::external_lock(). I do not know about HANDLER ... xxx things.
",3,"Remember that InnoDB table locks are transactional, see URL , I see correct 
{noformat}
dberr_t	error = row_lock_table_for_mysql(prebuilt, NULL, 0);
{noformat}

calls on ::external_lock(). I do not know about HANDLER ... xxx things.
"
2579,MDEV-7660,MDEV,Sergey Vojtovich,83286,2016-05-09 07:38:56,"We can disregard LOCK TABLE ... WRITE as it is handled by MDL. We can also disregard HANDLER READ issue, since it is easily fixable at MDL level.

But what we should about LOCK TABLE ... READ? It doesn't block DML.",4,"We can disregard LOCK TABLE ... WRITE as it is handled by MDL. We can also disregard HANDLER READ issue, since it is easily fixable at MDL level.

But what we should about LOCK TABLE ... READ? It doesn't block DML."
2580,MDEV-7660,MDEV,Jan Lindström,83381,2016-05-12 06:26:32,Why LOCK TABLE .. READ can't be handled on MDL level ?,5,Why LOCK TABLE .. READ can't be handled on MDL level ?
2581,MDEV-7660,MDEV,Jan Lindström,83382,2016-05-12 06:26:47,Changes look ok.,6,Changes look ok.
2582,MDEV-7660,MDEV,Sergey Vojtovich,83383,2016-05-12 07:43:45,We discussed it in Berlin: we did want to let InnoDB handle LOCK TABLES. The reason is: LOCK TABLE ... READ requires substantial change to MDL.,7,We discussed it in Berlin: we did want to let InnoDB handle LOCK TABLES. The reason is: LOCK TABLE ... READ requires substantial change to MDL.
2583,MDEV-7660,MDEV,Jan Lindström,83385,2016-05-12 08:06:11,"ok, problem is that autocommit=1 at lock table read; i.e. correct way to use is:

{noformat}
--source include/have_innodb.inc

CREATE TABLE t1(a INT) ENGINE=InnoDB;
set autocommit=0;
LOCK TABLE t1 READ;
 
connect(con1, localhost, root);
INSERT INTO t1 VALUES(1);
disconnect con1;
 
connection default;
UNLOCK TABLES;
DROP TABLE t1;
{noformat}",8,"ok, problem is that autocommit=1 at lock table read; i.e. correct way to use is:

{noformat}
--source include/have_innodb.inc

CREATE TABLE t1(a INT) ENGINE=InnoDB;
set autocommit=0;
LOCK TABLE t1 READ;
 
connect(con1, localhost, root);
INSERT INTO t1 VALUES(1);
disconnect con1;
 
connection default;
UNLOCK TABLES;
DROP TABLE t1;
{noformat}"
2584,MDEV-7660,MDEV,Sergey Vojtovich,83387,2016-05-12 09:39:58,"[~serg], with my patch returining 0 from ha_innobase::lock_count():

* DDL works
* DML works
* LOCK TABLE ... WRITE is handled by MDL and works in all cases except for concurrent HANDLER READ
* LOCK TABLE ... READ works reasonably when autocommit=0
* *LOCK TABLE ... READ has no effect when autocommit=1*

Do you have any idea how can we fix the last point? Ignore it? Let LOCK TABLES start transaction?",9,"[~serg], with my patch returining 0 from ha_innobase::lock_count():

* DDL works
* DML works
* LOCK TABLE ... WRITE is handled by MDL and works in all cases except for concurrent HANDLER READ
* LOCK TABLE ... READ works reasonably when autocommit=0
* *LOCK TABLE ... READ has no effect when autocommit=1*

Do you have any idea how can we fix the last point? Ignore it? Let LOCK TABLES start transaction?"
2585,MDEV-7660,MDEV,Sergei Golubchik,83469,2016-05-17 12:51:06,"Let LOCK TABLE start a transaction. Just make sure, please, it's clearly documented.",10,"Let LOCK TABLE start a transaction. Just make sure, please, it's clearly documented."
2586,MDEV-7660,MDEV,Sergey Vojtovich,83492,2016-05-18 12:05:00,"[~jplindst], [~serg], should we start InnoDB internal transaction or call something like trans_begin()?",11,"[~jplindst], [~serg], should we start InnoDB internal transaction or call something like trans_begin()?"
2587,MDEV-7660,MDEV,Sergei Golubchik,83504,2016-05-18 16:20:15,"I think it's enough to disable auto-commit. Then if any innodb table was mentioned, it will open a transaction automatically.",12,"I think it's enough to disable auto-commit. Then if any innodb table was mentioned, it will open a transaction automatically."
2588,MDEV-7660,MDEV,Sergey Vojtovich,83641,2016-05-24 08:35:45,"[~serg], please review 4 patches in bb-10.2-mdev7660. Please note that I don't feel confident about the last patch, which disables autocommit. I can't foresee all possible side effects.",13,"[~serg], please review 4 patches in bb-10.2-mdev7660. Please note that I don't feel confident about the last patch, which disables autocommit. I can't foresee all possible side effects."
2589,MDEV-7660,MDEV,Sergey Vojtovich,84757,2016-07-05 08:57:13,"[~serg], please review my last comments, specifically what should we do about deadlock? Fix this particular thing on MDL level?",14,"[~serg], please review my last comments, specifically what should we do about deadlock? Fix this particular thing on MDL level?"
2590,MDEV-7660,MDEV,Sergei Golubchik,86066,2016-09-02 07:22:30,"Will deadlock timeout in 50 seconds?

One simple workaround would be to have an option, like, {{--innodb-locks-unsafe-for-handler}}, and only use this optimization if it's true.

Ideally, the server should be able to break InnoDB locks.",15,"Will deadlock timeout in 50 seconds?

One simple workaround would be to have an option, like, {{--innodb-locks-unsafe-for-handler}}, and only use this optimization if it's true.

Ideally, the server should be able to break InnoDB locks."
2591,MDEV-7660,MDEV,Sergey Vojtovich,87368,2016-10-13 10:40:17,"[~serg], please review latest patch at https://github.com/MariaDB/server/commit/c90bd38809582b27696a68058fbd57c57d6dbf6d",16,"[~serg], please review latest patch at URL"
2592,MDEV-7773,MDEV,Sergei Golubchik,81109,2016-02-19 17:40:13,"A possible approach is to have only one function that does everything — accumulates and returns the result. This is what  [HSQL|http://hsqldb.org/doc/guide/sqlroutines-chapt.html#src_aggregate_functions]  has:
{code:sql}
 CREATE AGGREGATE FUNCTION udavg(IN x INTEGER, IN flag BOOLEAN, INOUT acc BIGINT, INOUT counter INT)
...
{code}
here the first argument — {{x}} — is the actual argument of the aggregate function (called as {{SELECT udagv\(x) ...}}), the second — {{flag}} — tells where to accumulate or return the result, the last two — {{acc}} and {{counter}} — are the storage, their value is preserved between calls and automatically reset to NULL when a new group starts.

The function then does something like
{code:sql}
  IF flag THEN
    -- return the value
  ELSE
    -- update count and acc with the new value of x
  END IF;
{code}
",1,"A possible approach is to have only one function that does everything — accumulates and returns the result. This is what  [HSQL|URL  has:
{code:sql}
 CREATE AGGREGATE FUNCTION udavg(IN x INTEGER, IN flag BOOLEAN, INOUT acc BIGINT, INOUT counter INT)
...
{code}
here the first argument — {{x}} — is the actual argument of the aggregate function (called as {{SELECT udagv\(x) ...}}), the second — {{flag}} — tells where to accumulate or return the result, the last two — {{acc}} and {{counter}} — are the storage, their value is preserved between calls and automatically reset to NULL when a new group starts.

The function then does something like
{code:sql}
  IF flag THEN
    -- return the value
  ELSE
    -- update count and acc with the new value of x
  END IF;
{code}
"
2593,MDEV-7773,MDEV,Sergei Golubchik,81110,2016-02-19 17:50:21,"[PostgreSQL|http://www.postgresql.org/docs/8.3/static/sql-createaggregate.html] assembles an aggregate function out of two regular functions. The state, like in the HSQL case, is an INOUT argument for these functions. The syntax looks like
{code:sql}
CREATE FUNCTION norm1 (... ) ... ;
CREATE FUNCTION norm2 (... ) ... ;
CREATE AGGREGATE (...) ( SFUNC=norm1, FINALFUNC=norm2, ... )
{code}
it also allows to specify the initial value for the state and if the function is MIN or MAX (that is, can be answered by taking the first or last value from the index).",2,"[PostgreSQL|URL assembles an aggregate function out of two regular functions. The state, like in the HSQL case, is an INOUT argument for these functions. The syntax looks like
{code:sql}
CREATE FUNCTION norm1 (... ) ... ;
CREATE FUNCTION norm2 (... ) ... ;
CREATE AGGREGATE (...) ( SFUNC=norm1, FINALFUNC=norm2, ... )
{code}
it also allows to specify the initial value for the state and if the function is MIN or MAX (that is, can be answered by taking the first or last value from the index)."
2594,MDEV-7773,MDEV,Sergei Golubchik,81112,2016-02-19 17:58:39,"[Oracle|http://docs.oracle.com/cd/B19306_01/appdev.102/b14289/dciaggfns.htm] uses an object-oriented approach. An aggregate function is an object, the state is stored in the object, initialize/iterate/terminate/merge are the methods.
{code:sql}
create type SecondMaxImpl as object
(
  max NUMBER, -- highest value seen so far 
  secmax NUMBER, -- second highest value seen so far
  static function ODCIAggregateInitialize(sctx IN OUT SecondMaxImpl) return number,
  member function ODCIAggregateIterate(self IN OUT SecondMaxImpl, value IN number) return number,
  member function ODCIAggregateTerminate(self IN SecondMaxImpl, returnValue OUT number, flags IN number) return number,
  member function ODCIAggregateMerge(self IN OUT SecondMaxImpl, ctx2 IN SecondMaxImpl) return number
);
create type body SecondMaxImpl is 
static function ODCIAggregateInitialize(sctx IN OUT SecondMaxImpl) return number is 
begin
  sctx := SecondMaxImpl(0, 0);
  return ODCIConst.Success;
end;
-- define other methods accordingly...
CREATE FUNCTION SecondMax (input NUMBER) RETURN NUMBER 
PARALLEL_ENABLE AGGREGATE USING SecondMaxImpl;
{code}
",3,"[Oracle|URL uses an object-oriented approach. An aggregate function is an object, the state is stored in the object, initialize/iterate/terminate/merge are the methods.
{code:sql}
create type SecondMaxImpl as object
(
  max NUMBER, -- highest value seen so far 
  secmax NUMBER, -- second highest value seen so far
  static function ODCIAggregateInitialize(sctx IN OUT SecondMaxImpl) return number,
  member function ODCIAggregateIterate(self IN OUT SecondMaxImpl, value IN number) return number,
  member function ODCIAggregateTerminate(self IN SecondMaxImpl, returnValue OUT number, flags IN number) return number,
  member function ODCIAggregateMerge(self IN OUT SecondMaxImpl, ctx2 IN SecondMaxImpl) return number
);
create type body SecondMaxImpl is 
static function ODCIAggregateInitialize(sctx IN OUT SecondMaxImpl) return number is 
begin
  sctx := SecondMaxImpl(0, 0);
  return ODCIConst.Success;
end;
-- define other methods accordingly...
CREATE FUNCTION SecondMax (input NUMBER) RETURN NUMBER 
PARALLEL_ENABLE AGGREGATE USING SecondMaxImpl;
{code}
"
2595,MDEV-7773,MDEV,Sergei Golubchik,81113,2016-02-19 18:24:54,"Another approach is to use a pre-defined cursor that iterates the group. Like this:
{code:sql}
CREATE AGGREGATE FUNCTION avg(x DOUBLE) RETURNS DOUBLE
BEGIN
  DECLARE count INT DEFAULT 0;
  DECLARE sum DOUBLE DEFAULT 0;
  DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' RETURN sum/count;
  LOOP
    FETCH GROUP NEXT ROW;
    SET count:=count+1;
    SET sum:=sum+x;
  END LOOP;
END
{code}
Note that {{GROUP}} keyword is reserved, so this syntax can be unambiguously distinguished from {{FETCH cursor_name}}.
",4,"Another approach is to use a pre-defined cursor that iterates the group. Like this:
{code:sql}
CREATE AGGREGATE FUNCTION avg(x DOUBLE) RETURNS DOUBLE
BEGIN
  DECLARE count INT DEFAULT 0;
  DECLARE sum DOUBLE DEFAULT 0;
  DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' RETURN sum/count;
  LOOP
    FETCH GROUP NEXT ROW;
    SET count:=count+1;
    SET sum:=sum+x;
  END LOOP;
END
{code}
Note that {{GROUP}} keyword is reserved, so this syntax can be unambiguously distinguished from {{FETCH cursor_name}}.
"
2596,MDEV-7773,MDEV,Sergei Golubchik,81114,2016-02-19 18:28:06,"Another possibility is to use a complex {{CREATE FUNCTION}} with many blocks:
{code:sql}
CREATE AGGREGATE FUNCTION avg(x DOUBLE) RETURNS DOUBLE
  DECLARE count INT DEFAULT 0;  -- note, state in declarations before the first BEGIN
  DECLARE sum DOUBLE DEFAULT 0;
BEGIN                           -- this is called for every row in a group
  SET count:=count+1;
  SET sum:=sum+x;
END;
BEGIN                           -- this returns the result
  RETURN sum/count;
END;
{code}
but this is too confusing. {{BEGIN}}/{{END}} blocks need keywords or labels to specify what they do.
",5,"Another possibility is to use a complex {{CREATE FUNCTION}} with many blocks:
{code:sql}
CREATE AGGREGATE FUNCTION avg(x DOUBLE) RETURNS DOUBLE
  DECLARE count INT DEFAULT 0;  -- note, state in declarations before the first BEGIN
  DECLARE sum DOUBLE DEFAULT 0;
BEGIN                           -- this is called for every row in a group
  SET count:=count+1;
  SET sum:=sum+x;
END;
BEGIN                           -- this returns the result
  RETURN sum/count;
END;
{code}
but this is too confusing. {{BEGIN}}/{{END}} blocks need keywords or labels to specify what they do.
"
2597,MDEV-7773,MDEV,varun gupta,81613,2016-02-29 03:58:37,"Can you give more details on the argument x, what i understand that it is one of the attributes of the table, and we need to have an aggregate function for the attribute x",6,"Can you give more details on the argument x, what i understand that it is one of the attributes of the table, and we need to have an aggregate function for the attribute x"
2598,MDEV-7773,MDEV,Sergei Golubchik,81632,2016-02-29 15:25:26,"{{x}} is the function argument. Like in
{code:sql}
SELECT avg(val) FROM table1 GROUP BY grn;
{code}",7,"{{x}} is the function argument. Like in
{code:sql}
SELECT avg(val) FROM table1 GROUP BY grn;
{code}"
2599,MDEV-7773,MDEV,varun gupta,81637,2016-02-29 17:48:38,"Yeah  understand it is the argument to the function.
When we write 
SELECT avg(val) FROM table1 GROUP BY grn;
val is one of the columns of our table(attribute). Am I correct?",8,"Yeah  understand it is the argument to the function.
When we write 
SELECT avg(val) FROM table1 GROUP BY grn;
val is one of the columns of our table(attribute). Am I correct?"
2600,MDEV-7773,MDEV,Oleksandr Byelkin,82369,2016-03-30 14:46:25,val should be an arbitrary expression.,9,val should be an arbitrary expression.
2601,MDEV-7773,MDEV,Sergei Golubchik,82610,2016-04-08 21:19:38,"h4. Window function support

To work efficiently over windows, an aggregate function should be able to *remove* a row from a group. This can be implemented in all syntax variants discussed above. In all cases row-removal functionality is optional, it is an optimization, but a aggregate function can work over windows without it too.

{color:#f79232}Adding support for row removal is *not* part of this MDEV-7773, but it should be possible to add that functionality later, MDEV-7773 should be compatible with this future extension.{color}

h5. HSQL

The second argument {{IN}} becomes an {{ENUM(ADD,REMOVE,RESULT)}}

h5. PostgreSQL

The definition supports a new clause {{REMOVEFUNC=}}

h5. Oracle

The object gets a new method {{remove}}

h5. Cursor approach

A new syntax {{FETCH GROUP REMOVED ROW}}. A value is generated when both cursors (""next"" and ""removed"") have returned SQLSTATE 02000. _Note: This is not enough, in addition {{return}} needs not to return, but that would be too confusing, wouldn't it?_

h5. Many blocks approach

One more block, that is invoked to remove a row
",10,"h4. Window function support

To work efficiently over windows, an aggregate function should be able to *remove* a row from a group. This can be implemented in all syntax variants discussed above. In all cases row-removal functionality is optional, it is an optimization, but a aggregate function can work over windows without it too.

{color:#f79232}Adding support for row removal is *not* part of this MDEV-7773, but it should be possible to add that functionality later, MDEV-7773 should be compatible with this future extension.{color}

h5. HSQL

The second argument {{IN}} becomes an {{ENUM(ADD,REMOVE,RESULT)}}

h5. PostgreSQL

The definition supports a new clause {{REMOVEFUNC=}}

h5. Oracle

The object gets a new method {{remove}}

h5. Cursor approach

A new syntax {{FETCH GROUP REMOVED ROW}}. A value is generated when both cursors (""next"" and ""removed"") have returned SQLSTATE 02000. _Note: This is not enough, in addition {{return}} needs not to return, but that would be too confusing, wouldn't it?_

h5. Many blocks approach

One more block, that is invoked to remove a row
"
2602,MDEV-7773,MDEV,Sergei Golubchik,82612,2016-04-09 06:11:51,"The complete example of the {{AVGOW}} (average over window) function using the cursor approach:
{code:sql}
CREATE AGGREGATE FUNCTION avgow(x DOUBLE) RETURNS DOUBLE
BEGIN
  DECLARE count INT DEFAULT 0;
  DECLARE sum DOUBLE DEFAULT 0;
  DECLARE loop_done INT;
  DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET loop_done=1;
  LOOP
    SET loop_done=0;
    WHILE loop_done = 0
      FETCH GROUP REMOVED ROW;
      SET count:=count-1;
      SET sum:=sum-x;
    END WHILE;
    SET loop_done=0;
    WHILE loop_done = 0
      FETCH GROUP NEXT ROW;
      SET count:=count+1;
      SET sum:=sum+x;
    END WHILE;
    RETURN IF(count,sum/count,NULL);
  END LOOP;
END
{code}
It is more complex than necessary, but shows all the features. Note, that
* there are two cursors, not one (the presence of a second cursor makes this function “window-aware” and enables two-cursor optimization).
* after {{return}} execution continues! The function is restarted for every window partition, but it is not restarted for every frame inside a window.
* The function exhausts the {{REMOVED}} cursor first, then the {{NEXT}} cursor. This order is _required_. There is no logical reason for that, it is how our window functions work at the moment. This limitation can (and should) be removed, but let's do it as the next step.
* This function is universal, it works both for {{GROUP BY}} and for {{OVER WINDOW}}. In the former case the “frame” is equal to the whole group, so the function is restarted after every {{return}} (in other words, it has the usual semantics).
",11,"The complete example of the {{AVGOW}} (average over window) function using the cursor approach:
{code:sql}
CREATE AGGREGATE FUNCTION avgow(x DOUBLE) RETURNS DOUBLE
BEGIN
  DECLARE count INT DEFAULT 0;
  DECLARE sum DOUBLE DEFAULT 0;
  DECLARE loop_done INT;
  DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET loop_done=1;
  LOOP
    SET loop_done=0;
    WHILE loop_done = 0
      FETCH GROUP REMOVED ROW;
      SET count:=count-1;
      SET sum:=sum-x;
    END WHILE;
    SET loop_done=0;
    WHILE loop_done = 0
      FETCH GROUP NEXT ROW;
      SET count:=count+1;
      SET sum:=sum+x;
    END WHILE;
    RETURN IF(count,sum/count,NULL);
  END LOOP;
END
{code}
It is more complex than necessary, but shows all the features. Note, that
* there are two cursors, not one (the presence of a second cursor makes this function “window-aware” and enables two-cursor optimization).
* after {{return}} execution continues! The function is restarted for every window partition, but it is not restarted for every frame inside a window.
* The function exhausts the {{REMOVED}} cursor first, then the {{NEXT}} cursor. This order is _required_. There is no logical reason for that, it is how our window functions work at the moment. This limitation can (and should) be removed, but let's do it as the next step.
* This function is universal, it works both for {{GROUP BY}} and for {{OVER WINDOW}}. In the former case the “frame” is equal to the whole group, so the function is restarted after every {{return}} (in other words, it has the usual semantics).
"
2603,MDEV-7773,MDEV,Sergei Golubchik,83109,2016-04-29 06:48:20,"Another problem of the cursor approach — if we ever will support non-SQL stored procedures (like Antony Curtis [work|http://www.slideshare.net/AntonyTCurtis/perl-stored-procedures-for-my-sql-presentation-28149737]), this will unlikely to be able to use cursors. At least, not generally for all languages.",12,"Another problem of the cursor approach — if we ever will support non-SQL stored procedures (like Antony Curtis [work|URL this will unlikely to be able to use cursors. At least, not generally for all languages."
2604,MDEV-7773,MDEV,Oleksandr Byelkin,83110,2016-04-29 07:36:26,There should be ways to read cursors from any stored procedures (special functions or something like this) in any case we get control at that moment and do what we need. It is as difficult as for our procedures IMHO.,13,There should be ways to read cursors from any stored procedures (special functions or something like this) in any case we get control at that moment and do what we need. It is as difficult as for our procedures IMHO.
2605,MDEV-7773,MDEV,Alexander Barkov,83136,2016-05-01 05:28:50,"The proposed syntax looks fine for simple cases.

I'd propose to think towards another direction though:
implement aggregate functions as SQL classes!

Internally all aggregate functions are implemented as classes in the Item_sum hierarchy and they have various methods and members. To be able to support all features that a built-in aggregate function supports (like WINDOW functionality, DISTINCT, etc) we could implement stored functions as classes as well, using the SQL standard definitions for user-defined types.

See the ""<user-defined type definition>"" in the SQL standard.

A skeleton for a class definition would look like this:

{code:sql}
-- Define an aggregate function handler
-- Derive it from some top-level built-in abstract class MariaDBAggregateFunctionHandler
CREATE TYPE handler_avgow UNDER MariaDBAggregateFunctionHandler
AS
(
 count INT DEFAULT 0,  # member 1
 sum DOUBLE DEFAULT 0  # member 2
)
OVERRIDING METHOD val_real() RETURNS DOUBLE
OVERRIDING METHOD fix_length_and_dec() RETURNS BOOLEAN
OVERRIDING METHOD reset_and_add() RETURNS BOOLEAN
...
;
{code}
After a class is defined, we can create methods using the standard syntax:
(in SQL method cannot be defined ""inline"", so there will be separate statements for methods:
{code:sql}
-- Define aggregate handler methods
CREATE METHOD val_real() FOR handler_avgov
 BEGIN
   RETURN IF(count,sum/count,NULL);
 END;
 CREATE METHOD fix_length_and_dec() ...;
 ...
{code}

Now, when we have a class with its methods created with the standard syntax, we can create an aggregate function using non-standard syntax, for example like this:

{code:sql}
-- Define the function itself
CREATE AGGREGATE FUNCTION avgow(x DOUBLE) RETURNS DOUBLE
USING handler_avgrow;
{code}

For non-SQL definitions we could use some syntax (perhaps it already exists in the SQL standard, I saw something like EXTERNAL NAME '...', which looks very related) to load implementations from external shared libraries. This syntax could include <language clause>. 
How SQL and non-SQL definitions would relate:
- The CREATE TYPE statement will be the same for an SQL and non-SQL implementation. 
- The CREATE METHOD statement will be different for SQL and non-SQL!!! (explicit SQL routine definition vs external module reference)
- The CREATE AGGREGATE FUNCTION statement will be the same for SQL and non-SQL

Advantages of the class-based conception:
- A great step towards the SQL standard. This CREATE TYPE useful per se, to create user-defined data types.
- Splits the code logically. You don't have to care about the order of different SQL code pieces, like cursors in the above example. This code will reside in different methods, so the server will make sure to execute them in proper order. Also, you don't have to care about signals. All together this should give better performance, and better user-defined SQL code readability.
- SQL and Non-SQL have very close interface (only CREATE METHOD differs)
- Classes can be used to add SQL code for other user-defined purposes (not only for aggregate functions), e.g.:
-- Hybrid type functions, which need not only val_xxx() (RETURN in terms of SQL), but also fix_length_and_dec() to decide their return type depending on the parameters passed.
-- PROCEDURE in SQL (rewrite the SELECT data set into a different data set)
-- SQL code wrappers around pluggable modules for various server components (for example parser or optimizer), engines, and all other plugin types. A user defined parser could derive from the built-in parser to call the derived parse method then additionally store some statistics somewhere using SQL. An SQL wrapper for an engine could override some engine methods (using SQL) to disable certain components (e.g. there is a request to disable everything in ConnectSE, except ODBC). So there could be two instances of ConnectSE: the standard full-featured one with full functionality available for authorized users, and a derived ODBC-only one for non-authorized users.
",14,"The proposed syntax looks fine for simple cases.

I'd propose to think towards another direction though:
implement aggregate functions as SQL classes!

Internally all aggregate functions are implemented as classes in the Item_sum hierarchy and they have various methods and members. To be able to support all features that a built-in aggregate function supports (like WINDOW functionality, DISTINCT, etc) we could implement stored functions as classes as well, using the SQL standard definitions for user-defined types.

See the """" in the SQL standard.

A skeleton for a class definition would look like this:

{code:sql}
-- Define an aggregate function handler
-- Derive it from some top-level built-in abstract class MariaDBAggregateFunctionHandler
CREATE TYPE handler_avgow UNDER MariaDBAggregateFunctionHandler
AS
(
 count INT DEFAULT 0,  # member 1
 sum DOUBLE DEFAULT 0  # member 2
)
OVERRIDING METHOD val_real() RETURNS DOUBLE
OVERRIDING METHOD fix_length_and_dec() RETURNS BOOLEAN
OVERRIDING METHOD reset_and_add() RETURNS BOOLEAN
...
;
{code}
After a class is defined, we can create methods using the standard syntax:
(in SQL method cannot be defined ""inline"", so there will be separate statements for methods:
{code:sql}
-- Define aggregate handler methods
CREATE METHOD val_real() FOR handler_avgov
 BEGIN
   RETURN IF(count,sum/count,NULL);
 END;
 CREATE METHOD fix_length_and_dec() ...;
 ...
{code}

Now, when we have a class with its methods created with the standard syntax, we can create an aggregate function using non-standard syntax, for example like this:

{code:sql}
-- Define the function itself
CREATE AGGREGATE FUNCTION avgow(x DOUBLE) RETURNS DOUBLE
USING handler_avgrow;
{code}

For non-SQL definitions we could use some syntax (perhaps it already exists in the SQL standard, I saw something like EXTERNAL NAME '...', which looks very related) to load implementations from external shared libraries. This syntax could include . 
How SQL and non-SQL definitions would relate:
- The CREATE TYPE statement will be the same for an SQL and non-SQL implementation. 
- The CREATE METHOD statement will be different for SQL and non-SQL!!! (explicit SQL routine definition vs external module reference)
- The CREATE AGGREGATE FUNCTION statement will be the same for SQL and non-SQL

Advantages of the class-based conception:
- A great step towards the SQL standard. This CREATE TYPE useful per se, to create user-defined data types.
- Splits the code logically. You don't have to care about the order of different SQL code pieces, like cursors in the above example. This code will reside in different methods, so the server will make sure to execute them in proper order. Also, you don't have to care about signals. All together this should give better performance, and better user-defined SQL code readability.
- SQL and Non-SQL have very close interface (only CREATE METHOD differs)
- Classes can be used to add SQL code for other user-defined purposes (not only for aggregate functions), e.g.:
-- Hybrid type functions, which need not only val_xxx() (RETURN in terms of SQL), but also fix_length_and_dec() to decide their return type depending on the parameters passed.
-- PROCEDURE in SQL (rewrite the SELECT data set into a different data set)
-- SQL code wrappers around pluggable modules for various server components (for example parser or optimizer), engines, and all other plugin types. A user defined parser could derive from the built-in parser to call the derived parse method then additionally store some statistics somewhere using SQL. An SQL wrapper for an engine could override some engine methods (using SQL) to disable certain components (e.g. there is a request to disable everything in ConnectSE, except ODBC). So there could be two instances of ConnectSE: the standard full-featured one with full functionality available for authorized users, and a derived ODBC-only one for non-authorized users.
"
2606,MDEV-7773,MDEV,Sergei Golubchik,83243,2016-05-06 05:39:15,"This is very similar to Oracle approach (see above).

But this is way too verbose and complex in my opinion. It could be simplified
as:
{code:sql}
CREATE AGGREGATE FUNCTION avgow
AS
(
 count INT DEFAULT 0,  -- member 1
 sum DOUBLE DEFAULT 0  -- member 2
)
OVERRIDING METHOD val_real() RETURNS DOUBLE
OVERRIDING METHOD fix_length_and_dec() RETURNS BOOLEAN
OVERRIDING METHOD reset_and_add() RETURNS BOOLEAN
...
;
CREATE METHOD val_real() FOR avgow
 BEGIN
   RETURN IF(count,sum/count,NULL);
 END;
CREATE METHOD fix_length_and_dec() ...;
 ...
{code}
or even as
{code:sql}
CREATE AGGREGATE FUNCTION avgow
AS
(
 count INT DEFAULT 0,  -- member 1
 sum DOUBLE DEFAULT 0  -- member 2
);
CREATE METHOD val_real() FOR avgow
 BEGIN
   RETURN IF(count,sum/count,NULL);
 END;
CREATE METHOD fix_length_and_dec() ...;
 ...
{code}
But in certain cases it is more limited than cursor-based approach, it doesn't
allow one to implement a median or a percentile function, because you cannot
have temporary tables or arrays in the {{AS ( ... )}} clause. Still, both
approaches can be combined:
{code:sql}
CREATE AGGREGATE FUNCTION avgow(x DOUBLE) RETURNS DOUBLE
BEGIN
  -- ... aggreagate function body, as above
END;
CREATE METHOD fix_length_and_dec() ... FOR avgow...
 ...
{code}
And the beauty of this is that it perfectly applies to non-aggregate functions
too:
{code:sql}
CREATE FUNCTION foo(x DOUBLE) RETURNS DECIMAL(3,1)
BEGIN
  -- ...
END;
CREATE METHOD fix_length_and_dec() ... FOR foo...
 ...
{code}",15,"This is very similar to Oracle approach (see above).

But this is way too verbose and complex in my opinion. It could be simplified
as:
{code:sql}
CREATE AGGREGATE FUNCTION avgow
AS
(
 count INT DEFAULT 0,  -- member 1
 sum DOUBLE DEFAULT 0  -- member 2
)
OVERRIDING METHOD val_real() RETURNS DOUBLE
OVERRIDING METHOD fix_length_and_dec() RETURNS BOOLEAN
OVERRIDING METHOD reset_and_add() RETURNS BOOLEAN
...
;
CREATE METHOD val_real() FOR avgow
 BEGIN
   RETURN IF(count,sum/count,NULL);
 END;
CREATE METHOD fix_length_and_dec() ...;
 ...
{code}
or even as
{code:sql}
CREATE AGGREGATE FUNCTION avgow
AS
(
 count INT DEFAULT 0,  -- member 1
 sum DOUBLE DEFAULT 0  -- member 2
);
CREATE METHOD val_real() FOR avgow
 BEGIN
   RETURN IF(count,sum/count,NULL);
 END;
CREATE METHOD fix_length_and_dec() ...;
 ...
{code}
But in certain cases it is more limited than cursor-based approach, it doesn't
allow one to implement a median or a percentile function, because you cannot
have temporary tables or arrays in the {{AS ( ... )}} clause. Still, both
approaches can be combined:
{code:sql}
CREATE AGGREGATE FUNCTION avgow(x DOUBLE) RETURNS DOUBLE
BEGIN
  -- ... aggreagate function body, as above
END;
CREATE METHOD fix_length_and_dec() ... FOR avgow...
 ...
{code}
And the beauty of this is that it perfectly applies to non-aggregate functions
too:
{code:sql}
CREATE FUNCTION foo(x DOUBLE) RETURNS DECIMAL(3,1)
BEGIN
  -- ...
END;
CREATE METHOD fix_length_and_dec() ... FOR foo...
 ...
{code}"
2607,MDEV-7773,MDEV,Sergei Golubchik,83244,2016-05-06 05:43:21,"Hm, may be omitting the {{OVERRIDING}} clause is not a good idea — it might allow someone from a concurrent connection to invoke the function before it's fully defined. And that might be dangerous, if, for example, one of the overriding methods does some kind of the access checks.

But {{OVERRIDING}} can be made optional. In the above case one will specify {{OVERRIDING}}, but often one will be able to omit it.

Or, may be, it'll be simpler to stick to the standard here and require the {{OVERRIDING}} clause?",16,"Hm, may be omitting the {{OVERRIDING}} clause is not a good idea — it might allow someone from a concurrent connection to invoke the function before it's fully defined. And that might be dangerous, if, for example, one of the overriding methods does some kind of the access checks.

But {{OVERRIDING}} can be made optional. In the above case one will specify {{OVERRIDING}}, but often one will be able to omit it.

Or, may be, it'll be simpler to stick to the standard here and require the {{OVERRIDING}} clause?"
2608,MDEV-7773,MDEV,Alexander Barkov,83248,2016-05-06 06:57:40,"Good idea to build  CREATE TYPE inside CREATE [AGGREGATE] FUNCTION.
We should probably try to define method bodies inline (without having separate CREATE METHOD statements).
This would solve the problem with concurrent connections.

",17,"Good idea to build  CREATE TYPE inside CREATE [AGGREGATE] FUNCTION.
We should probably try to define method bodies inline (without having separate CREATE METHOD statements).
This would solve the problem with concurrent connections.

"
2609,MDEV-7773,MDEV,Vicențiu Ciorbaru,97464,2017-07-10 13:30:41,"The current test case seems to corrupt the procs table:

{code:sql}
delimiter |;

create aggregate function agg_sum(x INT) returns double
begin
declare z double default 0;
declare continue handler for not found return z;

loop
fetch group next row;
set z = z + x;
end loop;
end|

delimiter ;|

create table t1 (id int, salary int);

INSERT INTO t1 VALUES (1, 100), (2, 40), (3, 6);

show create function agg_sum;
alter function agg_sum aggregate none;
show create function agg_sum; 
{code}
Final query fails with:
{noformat}
# This fails with 'show create function agg_sum' failed: 1457: Failed to load routine test.agg_sum. The table mysql.proc is missing, corrupt, or contains bad data (internal code -6)
{noformat}",18,"The current test case seems to corrupt the procs table:

{code:sql}
delimiter |;

create aggregate function agg_sum(x INT) returns double
begin
declare z double default 0;
declare continue handler for not found return z;

loop
fetch group next row;
set z = z + x;
end loop;
end|

delimiter ;|

create table t1 (id int, salary int);

INSERT INTO t1 VALUES (1, 100), (2, 40), (3, 6);

show create function agg_sum;
alter function agg_sum aggregate none;
show create function agg_sum; 
{code}
Final query fails with:
{noformat}
# This fails with 'show create function agg_sum' failed: 1457: Failed to load routine test.agg_sum. The table mysql.proc is missing, corrupt, or contains bad data (internal code -6)
{noformat}"
2610,MDEV-7773,MDEV,Vicențiu Ciorbaru,102127,2017-10-28 14:41:00,"Just a few minor dead-code removal and history cleanup, then we can push.",19,"Just a few minor dead-code removal and history cleanup, then we can push."
2611,MDEV-7773,MDEV,Varun Gupta,103623,2017-11-26 18:29:25,"A final rebase was required, resolved the conflicts and have sent it to [~cvicentiu] for review.
Have pushed it to bb-10.3-varun for final testing",20,"A final rebase was required, resolved the conflicts and have sent it to [~cvicentiu] for review.
Have pushed it to bb-10.3-varun for final testing"
2612,MDEV-7773,MDEV,Elena Stepanova,103809,2017-11-29 13:16:36,"I haven't got anything of significance in a round of tests on bb-10.3-varun 7d8c5ee4039. 
Testing of correctness of results is limited due to numerous existing problems in this area, unrelated to the new functionality. I haven't got any functionality-specific crashes.
After fixing MDEV-14520 and addressing all reviews, please go ahead and push to 10.3. If reviews require significant changes in the code, please notify so that I re-run the tests.

Please make sure the functionality is documented in the KB, if not done yet. 

Among other things, I think it should be specifically mentioned in the documentation that unlike built-in functions, the custom ones don't provide automatic type recognition and act according to the types specified in the function itself. For a simple example, consider the following

{code:sql}
create or replace table t1 (i int, c char(8), ci char(8));
insert into t1 values (10,'foo','10'),(2,'bar','2');

delimiter $
create or replace aggregate function agg_max1(x INT) returns INT
begin
  declare m INT default NULL;
  declare continue handler for not found return m;
  loop
   fetch group next row;
   if (x is not null) and (m is null or m < x) then
      set m= x;
   end if;
  end loop;
end $

create or replace aggregate function agg_max2(x BLOB) returns BLOB
begin
  declare m BLOB default NULL;
  declare continue handler for not found return m;
  loop
   fetch group next row;
   if (x is not null) and (m is null or m < x) then
      set m= x;
   end if;
  end loop;
end $
delimiter ;
{code}

Here, column {{`i`}} is integer, column {{`ci`}} is text which contains integer values, and column {{`c`}} is text.
The in-built function {{MAX}} will treat each of them accordingly -- the result will be based on integer comparison for {{`i`}}, on text comparison for {{`ci`}} and {{`c`}}.
{code:sql}
MariaDB [test]> select max(i), max(ci), max(c) from t1;
+--------+---------+--------+
| max(i) | max(ci) | max(c) |
+--------+---------+--------+
|     10 | 2       | foo    |
+--------+---------+--------+
1 row in set (0.00 sec)
{code}

Definitions of functions {{agg_max1}} and {{agg_max2}} are identical, except that {{agg_max1}} works with integers and {{agg_max2}} with blobs.

On the same data, {{agg_max2}} will do text comparison for all columns, regardless their initial type
{code:sql}
MariaDB [test]> select agg_max2(i), agg_max2(ci), agg_max2(c) from t1;
+-------------+--------------+-------------+
| agg_max2(i) | agg_max2(ci) | agg_max2(c) |
+-------------+--------------+-------------+
| 2           | 2            | foo         |
+-------------+--------------+-------------+
1 row in set (0.00 sec)
{code}

Function {{agg_max1}} will do integer comparison for anything that looks like integer -- in this case, for {{`i`}} and for {{`ci`}}:
{code:sql}
MariaDB [test]> select agg_max1(i), agg_max1(ci) from t1;
+-------------+--------------+
| agg_max1(i) | agg_max1(ci) |
+-------------+--------------+
|          10 |           10 |
+-------------+--------------+
1 row in set (0.00 sec)
{code}

And for something that doesn't look like integer, it doesn't work at all:
{code:sql}
MariaDB [test]> set sql_mode= 'STRICT_ALL_TABLES';
Query OK, 0 rows affected (0.02 sec)

MariaDB [test]> select agg_max1(c) from t1;
ERROR 1366 (22007): Incorrect integer value: 'foo' for column 'x' at row 1
MariaDB [test]> set sql_mode= '';
Query OK, 0 rows affected (0.00 sec)

MariaDB [test]> select agg_max1(c) from t1;
+-------------+
| agg_max1(c) |
+-------------+
|           0 |
+-------------+
1 row in set, 3 warnings (0.00 sec)
{code}

Given that functions must define explicit types, I don't see a bug here, but it might be something users don't expect, so better to document it.",21,"I haven't got anything of significance in a round of tests on bb-10.3-varun 7d8c5ee4039. 
Testing of correctness of results is limited due to numerous existing problems in this area, unrelated to the new functionality. I haven't got any functionality-specific crashes.
After fixing MDEV-14520 and addressing all reviews, please go ahead and push to 10.3. If reviews require significant changes in the code, please notify so that I re-run the tests.

Please make sure the functionality is documented in the KB, if not done yet. 

Among other things, I think it should be specifically mentioned in the documentation that unlike built-in functions, the custom ones don't provide automatic type recognition and act according to the types specified in the function itself. For a simple example, consider the following

{code:sql}
create or replace table t1 (i int, c char(8), ci char(8));
insert into t1 values (10,'foo','10'),(2,'bar','2');

delimiter $
create or replace aggregate function agg_max1(x INT) returns INT
begin
  declare m INT default NULL;
  declare continue handler for not found return m;
  loop
   fetch group next row;
   if (x is not null) and (m is null or m < x) then
      set m= x;
   end if;
  end loop;
end $

create or replace aggregate function agg_max2(x BLOB) returns BLOB
begin
  declare m BLOB default NULL;
  declare continue handler for not found return m;
  loop
   fetch group next row;
   if (x is not null) and (m is null or m < x) then
      set m= x;
   end if;
  end loop;
end $
delimiter ;
{code}

Here, column {{`i`}} is integer, column {{`ci`}} is text which contains integer values, and column {{`c`}} is text.
The in-built function {{MAX}} will treat each of them accordingly -- the result will be based on integer comparison for {{`i`}}, on text comparison for {{`ci`}} and {{`c`}}.
{code:sql}
MariaDB [test]> select max(i), max(ci), max(c) from t1;
+--------+---------+--------+
| max(i) | max(ci) | max(c) |
+--------+---------+--------+
|     10 | 2       | foo    |
+--------+---------+--------+
1 row in set (0.00 sec)
{code}

Definitions of functions {{agg_max1}} and {{agg_max2}} are identical, except that {{agg_max1}} works with integers and {{agg_max2}} with blobs.

On the same data, {{agg_max2}} will do text comparison for all columns, regardless their initial type
{code:sql}
MariaDB [test]> select agg_max2(i), agg_max2(ci), agg_max2(c) from t1;
+-------------+--------------+-------------+
| agg_max2(i) | agg_max2(ci) | agg_max2(c) |
+-------------+--------------+-------------+
| 2           | 2            | foo         |
+-------------+--------------+-------------+
1 row in set (0.00 sec)
{code}

Function {{agg_max1}} will do integer comparison for anything that looks like integer -- in this case, for {{`i`}} and for {{`ci`}}:
{code:sql}
MariaDB [test]> select agg_max1(i), agg_max1(ci) from t1;
+-------------+--------------+
| agg_max1(i) | agg_max1(ci) |
+-------------+--------------+
|          10 |           10 |
+-------------+--------------+
1 row in set (0.00 sec)
{code}

And for something that doesn't look like integer, it doesn't work at all:
{code:sql}
MariaDB [test]> set sql_mode= 'STRICT_ALL_TABLES';
Query OK, 0 rows affected (0.02 sec)

MariaDB [test]> select agg_max1(c) from t1;
ERROR 1366 (22007): Incorrect integer value: 'foo' for column 'x' at row 1
MariaDB [test]> set sql_mode= '';
Query OK, 0 rows affected (0.00 sec)

MariaDB [test]> select agg_max1(c) from t1;
+-------------+
| agg_max1(c) |
+-------------+
|           0 |
+-------------+
1 row in set, 3 warnings (0.00 sec)
{code}

Given that functions must define explicit types, I don't see a bug here, but it might be something users don't expect, so better to document it."
2613,MDEV-7780,MDEV,Sergei Golubchik,77859,2015-11-10 13:10:41,"We can simply make @@version writable (from the command-line, not run-time)",1,"We can simply make @@version writable (from the command-line, not run-time)"
2614,MDEV-7780,MDEV,Sergey Vojtovich,78451,2015-11-24 17:58:37,"[~serg], please review patch for this task.",2,"[~serg], please review patch for this task."
2615,MDEV-7780,MDEV,Sergey Vojtovich,78465,2015-11-24 23:19:31,"[~serg], please disregard first patch, it's incorrect. Will send updated version ASAP.",3,"[~serg], please disregard first patch, it's incorrect. Will send updated version ASAP."
2616,MDEV-7780,MDEV,Sergey Vojtovich,78480,2015-11-25 11:59:20,"[~serg], patch updated.",4,"[~serg], patch updated."
2617,MDEV-7780,MDEV,Sergey Vojtovich,78531,2015-11-26 11:26:26,"[~serg], test case extended as requested.",5,"[~serg], test case extended as requested."
2618,MDEV-7780,MDEV,Sergey Vojtovich,78533,2015-11-26 12:07:46,"[~serg], test case updated.",6,"[~serg], test case updated."
2619,MDEV-7832,MDEV,Daniel Black,69773,2015-04-06 11:17:28,patch: https://github.com/MariaDB/server/pull/37,1,patch: URL
2620,MDEV-7832,MDEV,Sergei Golubchik,72493,2015-06-19 22:01:15,"[~svoj], could you, please, review this patch? In particular
* do we want this feature at all?
* approach that I used. it's kind of a hack. compare with the approach in the contributed patch. it's also kind of a hack.

Thanks!",2,"[~svoj], could you, please, review this patch? In particular
* do we want this feature at all?
* approach that I used. it's kind of a hack. compare with the approach in the contributed patch. it's also kind of a hack.

Thanks!"
2621,MDEV-7832,MDEV,Daniel Black,72509,2015-06-20 05:15:14,differentiating between temporary tables created and used by applications to more permanent tables is useful as each table type serves different functions.,3,differentiating between temporary tables created and used by applications to more permanent tables is useful as each table type serves different functions.
2622,MDEV-7832,MDEV,Sergey Vojtovich,72800,2015-06-26 18:16:54,Ok to push.,4,Ok to push.
2623,MDEV-7901,MDEV,Sergei Petrunia,69753,2015-04-04 00:50:46,"Removing analyze-stmt label . That label is for ANALYZE statement feature (https://mariadb.com/kb/en/mariadb/analyze-statement ), which has nothing to do with ""ANALYZE table"" statements.s",1,"Removing analyze-stmt label . That label is for ANALYZE statement feature (URL ), which has nothing to do with ""ANALYZE table"" statements.s"
2624,MDEV-7901,MDEV,Sergei Golubchik,69759,2015-04-04 14:19:42,"I don't see why {{ANALYZE TABLE}} that collects engine-independent statistics needs to take table locks at all. It could do a lock-less consistent read on InnoDB, just like a normal {{SELECT *}} does.",2,"I don't see why {{ANALYZE TABLE}} that collects engine-independent statistics needs to take table locks at all. It could do a lock-less consistent read on InnoDB, just like a normal {{SELECT *}} does."
2625,MDEV-7901,MDEV,Michael Widenius,73263,2015-07-07 15:21:33,"Agree with Sergei Golubchick; For InnoDB and XtraDB using lock-less select and even running in not-repeatable mode should allow analyze to run without any notable performance degredation.

When it comes to MyISAM and Aria, there is two possibilties:
- Run in ALLOW_READ_ALLOW_WRITE mode;  It doesn't matter if we miss a few rows while doing a scan.  It should not be hard to extend MyISAM and Aria to do this.
- Run things in blocks, 10000 rows at a time;  If there is another table waiting for the lock, unlock, relock and continue from the original position.
",3,"Agree with Sergei Golubchick; For InnoDB and XtraDB using lock-less select and even running in not-repeatable mode should allow analyze to run without any notable performance degredation.

When it comes to MyISAM and Aria, there is two possibilties:
- Run in ALLOW_READ_ALLOW_WRITE mode;  It doesn't matter if we miss a few rows while doing a scan.  It should not be hard to extend MyISAM and Aria to do this.
- Run things in blocks, 10000 rows at a time;  If there is another table waiting for the lock, unlock, relock and continue from the original position.
"
2626,MDEV-7901,MDEV,VAROQUI Stephane,74062,2015-07-30 11:41:47,"Would be nice to introduce ISET Independent Storage Engine Throttling API. We can upgrade EITS system tables via such plugin, but can also get an auto schedule backup by streaming chunk and Binlog Row Events between chunks. Some plugin can be use for maintenance like deleting rows based on on expiration date  and column name       ",4,"Would be nice to introduce ISET Independent Storage Engine Throttling API. We can upgrade EITS system tables via such plugin, but can also get an auto schedule backup by streaming chunk and Binlog Row Events between chunks. Some plugin can be use for maintenance like deleting rows based on on expiration date  and column name       "
2627,MDEV-7901,MDEV,Daniel Black,75209,2015-08-31 10:00:37,Was any progress made? Can this be pushed to a public branch so continuation work can be done?,5,Was any progress made? Can this be pushed to a public branch so continuation work can be done?
2628,MDEV-7901,MDEV,Oleksandr Byelkin,75210,2015-08-31 10:04:42,We found it too big changes to squeeze it just before RC release. ,6,We found it too big changes to squeeze it just before RC release. 
2629,MDEV-7901,MDEV,Daniel Black,75211,2015-08-31 10:20:24,"bq. We found it too big changes to squeeze it just before RC release.

Is that due to the MyISAM/Aria complexity [~monty] mentioned? If so can that part be put as a independent deferred request? After all, in a very large proportion of the time, innodb tables are the only ones that impact operations.",7,"bq. We found it too big changes to squeeze it just before RC release.

Is that due to the MyISAM/Aria complexity [~monty] mentioned? If so can that part be put as a independent deferred request? After all, in a very large proportion of the time, innodb tables are the only ones that impact operations."
2630,MDEV-7901,MDEV,Oleksandr Byelkin,75212,2015-08-31 10:53:00,Mostly because very probable delay of stable version due to fixing errors in this part.,8,Mostly because very probable delay of stable version due to fixing errors in this part.
2631,MDEV-7901,MDEV,Oleksandr Byelkin,85315,2016-08-01 17:26:19,done with innodb relaxed locks,9,done with innodb relaxed locks
2632,MDEV-7901,MDEV,Oleksandr Byelkin,85316,2016-08-01 17:26:39,"revision-id: fc42991720838e165ad448b6707602cded92faa4 (mariadb-10.2.1-9-gfc42991)
parent(s): 08683a726773f8cdf16a4a3dfb3920e5f7842481
committer: Oleksandr Byelkin
timestamp: 2016-08-01 19:25:45 +0200
message:

MDEV-7901: re-implement analyze table for low impact

Table before collecting engine independent statistics now is reopened in read mode,
InnoDB allow write operations in this case.

---",10,"revision-id: fc42991720838e165ad448b6707602cded92faa4 (mariadb-10.2.1-9-gfc42991)
parent(s): 08683a726773f8cdf16a4a3dfb3920e5f7842481
committer: Oleksandr Byelkin
timestamp: 2016-08-01 19:25:45 +0200
message:

MDEV-7901: re-implement analyze table for low impact

Table before collecting engine independent statistics now is reopened in read mode,
InnoDB allow write operations in this case.

---"
2633,MDEV-7901,MDEV,Daniel Black,85320,2016-08-01 22:18:52,Thank you so much [~sanja] :-),11,Thank you so much [~sanja] :-)
2634,MDEV-7901,MDEV,Oleksandr Byelkin,85373,2016-08-03 15:27:49,"revision-id: 810e5ae2eb8d7bdc1dc5549865dc9177360b8e62 (mariadb-10.2.1-9-g810e5ae)
parent(s): 08683a726773f8cdf16a4a3dfb3920e5f7842481
committer: Oleksandr Byelkin
timestamp: 2016-08-03 17:26:55 +0200
message:

MDEV-7901: re-implement analyze table for low impact

Table before collecting engine independent statistics now is reopened in read mode,
InnoDB allow write operations in this case.
",12,"revision-id: 810e5ae2eb8d7bdc1dc5549865dc9177360b8e62 (mariadb-10.2.1-9-g810e5ae)
parent(s): 08683a726773f8cdf16a4a3dfb3920e5f7842481
committer: Oleksandr Byelkin
timestamp: 2016-08-03 17:26:55 +0200
message:

MDEV-7901: re-implement analyze table for low impact

Table before collecting engine independent statistics now is reopened in read mode,
InnoDB allow write operations in this case.
"
2635,MDEV-7901,MDEV,Sergei Golubchik,85378,2016-08-03 18:05:12,ok to push,13,ok to push
2636,MDEV-7937,MDEV,Sergei Golubchik,71500,2015-05-28 19:21:31,"as discussed in emails, let's keep {{--ssl}} as is and fix {{CLIENT_SSL_VERIFY_SERVER_CERT}} instead.",1,"as discussed in emails, let's keep {{--ssl}} as is and fix {{CLIENT_SSL_VERIFY_SERVER_CERT}} instead."
2637,MDEV-7937,MDEV,Sergei Golubchik,71537,2015-05-29 23:05:47,"Another option would be to make {{CLIENT_SSL_VERIFY_SERVER_CERT}} enabled by default and make {{--ssl}} to be *required* if {{CLIENT_SSL_VERIFY_SERVER_CERT}} is enabled and *optional* if it is disabled. This might be easier to use than the previous suggestion.

Either way, the point is — without certificate checks the {{--ssl}} option doesn't guarantee anything, so requiring SSL that way does not make a lot of sense.",2,"Another option would be to make {{CLIENT_SSL_VERIFY_SERVER_CERT}} enabled by default and make {{--ssl}} to be *required* if {{CLIENT_SSL_VERIFY_SERVER_CERT}} is enabled and *optional* if it is disabled. This might be easier to use than the previous suggestion.

Either way, the point is — without certificate checks the {{--ssl}} option doesn't guarantee anything, so requiring SSL that way does not make a lot of sense."
2638,MDEV-7937,MDEV,Sergei Golubchik,71922,2015-06-09 14:49:29,ok to push with tests,3,ok to push with tests
2639,MDEV-7937,MDEV,Vicențiu Ciorbaru,71929,2015-06-09 16:09:37,"Fixed with:
[https://github.com/MariaDB/server/compare/56e2d8318bf3...be5035b4f4e4]",4,"Fixed with:
[URL"
2640,MDEV-7978,MDEV,Vicențiu Ciorbaru,80040,2016-01-19 13:45:00,"Hi Sergei,

I've updated the CREATE/ALTER USER syntax to work according to MySQL's. Can you please review the changes related to this MDEV? I've sent the patch on the mailing list.

I initially tried an approach that involved a bit of refactoring. After figuring out that it is not really worth it right now, I came up with the easier implementation just calling replace_user_table.",1,"Hi Sergei,

I've updated the CREATE/ALTER USER syntax to work according to MySQL's. Can you please review the changes related to this MDEV? I've sent the patch on the mailing list.

I initially tried an approach that involved a bit of refactoring. After figuring out that it is not really worth it right now, I came up with the easier implementation just calling replace_user_table."
2641,MDEV-7978,MDEV,Vicențiu Ciorbaru,81836,2016-03-08 15:05:31,"Implemented with:
[https://github.com/MariaDB/server/compare/1a3db0e...f12229f]",2,"Implemented with:
[URL"
2642,MDEV-8010,MDEV,Sergey Vojtovich,74523,2015-08-11 15:01:00,"[~monty], please review attached patch.",1,"[~monty], please review attached patch."
2643,MDEV-8010,MDEV,Michael Widenius,74575,2015-08-13 14:56:52,Review done per email. Please fix the very few issues and then ok to push,2,Review done per email. Please fix the very few issues and then ok to push
2644,MDEV-8010,MDEV,Michael Widenius,74576,2015-08-13 15:09:38,See email,3,See email
2645,MDEV-8091,MDEV,Vicențiu Ciorbaru,70614,2015-05-02 22:15:47,[~psergey][~igor][~sanja],1,[~psergey][~igor][~sanja]
2646,MDEV-8091,MDEV,Sergei Petrunia,78527,2015-11-26 00:48:33,"Overview of the solution sketch that was pushed into {{10.1-window}} branch:

JOIN::exec has a piece of code that detects that 
- the select uses one table
- all windowing functions have the same ORDER BY clause
- all windowing functions allow for streaming computation

if this is the case
- it runs filesort() to sort the source table in the required ordering
- then, end_send() has a code that calls {{func->advance_window()}} for 
  all window function items

then 

{noformat}
+void Item_window_func::advance_window() {
+  int changed = test_if_group_changed(partition_fields);
+
+  if (changed > -1) {
+    window_func->clear();
+  }
+  window_func->add();
+}
{noformat}

and this computes the window function. It is done on the fly.
",2,"Overview of the solution sketch that was pushed into {{10.1-window}} branch:

JOIN::exec has a piece of code that detects that 
- the select uses one table
- all windowing functions have the same ORDER BY clause
- all windowing functions allow for streaming computation

if this is the case
- it runs filesort() to sort the source table in the required ordering
- then, end_send() has a code that calls {{func->advance_window()}} for 
  all window function items

then 

{noformat}
+void Item_window_func::advance_window() {
+  int changed = test_if_group_changed(partition_fields);
+
+  if (changed > -1) {
+    window_func->clear();
+  }
+  window_func->add();
+}
{noformat}

and this computes the window function. It is done on the fly.
"
2647,MDEV-8091,MDEV,Juan Telleria,93848,2017-04-06 08:07:25,"¿Is possible to use Window Functions over columns which contain Aggregate Functions (For example: count(Column_Name)?

SELECT
     count(Column_Name) AS MyCount,
     PERCENT_RANK() OVER (MyCount)
FROM
     myTable;",3,"¿Is possible to use Window Functions over columns which contain Aggregate Functions (For example: count(Column_Name)?

SELECT
     count(Column_Name) AS MyCount,
     PERCENT_RANK() OVER (MyCount)
FROM
     myTable;"
2648,MDEV-8092,MDEV,Alexander Barkov,95856,2017-05-27 13:00:18,"This was done as a part of Monty's patch, fixing many members in many structured to LEX_CSTRING.
",1,"This was done as a part of Monty's patch, fixing many members in many structured to LEX_CSTRING.
"
2649,MDEV-8111,MDEV,Sergey Vojtovich,78241,2015-11-19 14:55:34,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2650,MDEV-8111,MDEV,Sergei Golubchik,78278,2015-11-20 13:35:11,"I don't think I need to review this. If you think it's ok to push, go ahead and push.",2,"I don't think I need to review this. If you think it's ok to push, go ahead and push."
2651,MDEV-8111,MDEV,Elena Stepanova,83908,2016-06-01 12:20:13,"Just to keep a note of it, since it's already been discussed on IRC and ruled out as unimportant (or rather harmless) -- Mroonga still has {{MY_PTHREAD_FASTMUTEX}}.
",3,"Just to keep a note of it, since it's already been discussed on IRC and ruled out as unimportant (or rather harmless) -- Mroonga still has {{MY_PTHREAD_FASTMUTEX}}.
"
2652,MDEV-8183,MDEV,VAROQUI Stephane,71222,2015-05-19 19:11:13,https://github.com/MariaDB/server/pull/69/commits,1,URL
2653,MDEV-8199,MDEV,Sergey Vojtovich,71286,2015-05-21 11:36:14,"[~psergey], please review patch for this task.",1,"[~psergey], please review patch for this task."
2654,MDEV-8199,MDEV,Sergey Vojtovich,71288,2015-05-21 12:10:21,There're two patches to review now.,2,There're two patches to review now.
2655,MDEV-8199,MDEV,Sergei Petrunia,72507,2015-06-20 02:02:06,Ok to push both patches.,3,Ok to push both patches.
2656,MDEV-8320,MDEV,Daniel Black,72250,2015-06-16 09:35:43,any hints where to start looking in the code to implement this appreciated.,1,any hints where to start looking in the code to implement this appreciated.
2657,MDEV-8320,MDEV,Sergei Golubchik,72335,2015-06-17 13:25:22,I'd look first at {{add_key_fields()}}. But perhaps I would've been wrong,2,I'd look first at {{add_key_fields()}}. But perhaps I would've been wrong
2658,MDEV-8320,MDEV,Sergei Petrunia,72352,2015-06-17 18:46:58,"{{add_key_fields()}} is an auxiliary function that used for constructing {{ref}} access plans (here, {{ref}} includes {{ref_or_null}}, or {{eq_ref}}).  ref access plans are based on equalities (or other conditions that can be reduced to equalities), such that the records can be read with handler->index_read()/handler->index_next_same() calls.

{{YEAR(date_col) = const}} specifies a range. So, the right way would be to use range optimizer to handle this.
",3,"{{add_key_fields()}} is an auxiliary function that used for constructing {{ref}} access plans (here, {{ref}} includes {{ref_or_null}}, or {{eq_ref}}).  ref access plans are based on equalities (or other conditions that can be reduced to equalities), such that the records can be read with handler->index_read()/handler->index_next_same() calls.

{{YEAR(date_col) = const}} specifies a range. So, the right way would be to use range optimizer to handle this.
"
2659,MDEV-8320,MDEV,Sergei Petrunia,72353,2015-06-17 18:50:52,"As for using range optimizer...  I'll debug a similar query and point places where changes are be made.

Let the query be:
{noformat}
explain select * from t100 where a between '2015-10-10' and '2015-10-12';
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
| id   | select_type | table | type  | possible_keys | key  | key_len | ref  | rows | Extra                 |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
|    1 | SIMPLE      | t100  | range | a             | a    | 4       | NULL |    3 | Using index condition |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
{noformat}


The first place is:
{noformat}
  #0  add_key_field (join=0x7fff5c0065a0, key_fields=0x7ffff7eb5b18, and_level=0, cond=0x7fff5c005cf8, field=0x7fff5c010a90, eq_func=false, value=0x7fff5c005e48, num_values=2, usable_tables=18446744073709551615, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4482
  #1  0x0000555555ab03be in add_key_equal_fields (join=0x7fff5c0065a0, key_fields=0x7ffff7eb5b18, and_level=0, cond=0x7fff5c005cf8, field_item=0x7fff5c005aa8, eq_func=false, val=0x7fff5c005e48, num_values=2, usable_tables=18446744073709551615, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4568
  #2  0x0000555555ab0a58 in Item_func_between::add_key_fields (this=0x7fff5c005cf8, join=0x7fff5c0065a0, key_fields=0x7ffff7eb5b18, and_level=0x7ffff7eb5b50, usable_tables=18446744073709551615, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4734
  #3  0x0000555555ab2524 in update_ref_and_keys (thd=0x55555aad3700, keyuse=0x7fff5c0068a0, join_tab=0x7fff5c006e18, tables=1, cond=0x7fff5c005cf8, normal_tables=18446744073709551615, select_lex=0x55555aad77b0, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:5302
  #4  0x0000555555aad6a1 in make_join_statistics (join=0x7fff5c0065a0, tables_list=..., keyuse_array=0x7fff5c0068a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:3615
  #5  0x0000555555aa5836 in JOIN::optimize_inner (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1341
  #6  0x0000555555aa478c in JOIN::optimize (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1023
{noformat}

This is the preparation stage. We collect in {{TABLE::const_keys}} a set of indexes for which range optimization is probably useful.   Note the frame with {{Item_func_between::add_key_fields}},  I guess Item_func_eq will need a similar function that will check if it is comparing a {{YEAR(key_col)}} with a constant
",4,"As for using range optimizer...  I'll debug a similar query and point places where changes are be made.

Let the query be:
{noformat}
explain select * from t100 where a between '2015-10-10' and '2015-10-12';
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
| id   | select_type | table | type  | possible_keys | key  | key_len | ref  | rows | Extra                 |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
|    1 | SIMPLE      | t100  | range | a             | a    | 4       | NULL |    3 | Using index condition |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
{noformat}


The first place is:
{noformat}
  #0  add_key_field (join=0x7fff5c0065a0, key_fields=0x7ffff7eb5b18, and_level=0, cond=0x7fff5c005cf8, field=0x7fff5c010a90, eq_func=false, value=0x7fff5c005e48, num_values=2, usable_tables=18446744073709551615, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4482
  #1  0x0000555555ab03be in add_key_equal_fields (join=0x7fff5c0065a0, key_fields=0x7ffff7eb5b18, and_level=0, cond=0x7fff5c005cf8, field_item=0x7fff5c005aa8, eq_func=false, val=0x7fff5c005e48, num_values=2, usable_tables=18446744073709551615, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4568
  #2  0x0000555555ab0a58 in Item_func_between::add_key_fields (this=0x7fff5c005cf8, join=0x7fff5c0065a0, key_fields=0x7ffff7eb5b18, and_level=0x7ffff7eb5b50, usable_tables=18446744073709551615, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4734
  #3  0x0000555555ab2524 in update_ref_and_keys (thd=0x55555aad3700, keyuse=0x7fff5c0068a0, join_tab=0x7fff5c006e18, tables=1, cond=0x7fff5c005cf8, normal_tables=18446744073709551615, select_lex=0x55555aad77b0, sargables=0x7ffff7eb5c88) at /home/psergey/dev-git/10.1/sql/sql_select.cc:5302
  #4  0x0000555555aad6a1 in make_join_statistics (join=0x7fff5c0065a0, tables_list=..., keyuse_array=0x7fff5c0068a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:3615
  #5  0x0000555555aa5836 in JOIN::optimize_inner (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1341
  #6  0x0000555555aa478c in JOIN::optimize (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1023
{noformat}

This is the preparation stage. We collect in {{TABLE::const_keys}} a set of indexes for which range optimization is probably useful.   Note the frame with {{Item_func_between::add_key_fields}},  I guess Item_func_eq will need a similar function that will check if it is comparing a {{YEAR(key_col)}} with a constant
"
2660,MDEV-8320,MDEV,Sergei Petrunia,72354,2015-06-17 18:57:07,"Construction of range objects happens here:

{noformat}
  #0  SEL_ARG::SEL_ARG (this=0x7fff5c014a60, f=0x7fff5c010a90, min_value_arg=0x7fff5c014a58 """", max_value_arg=0x7fff5c014a58 """") at /home/psergey/dev-git/10.1/sql/opt_range.cc:2479
  #1  0x0000555555988784 in get_mm_leaf (param=0x7ffff7eb3410, conf_func=0x7fff5c005cf8, field=0x7fff5c010a90, key_part=0x7fff5c0147b8, type=Item_func::GE_FUNC, value=0x7fff5c005bb8) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8598
  #2  0x00005555559878c9 in get_mm_parts (param=0x7ffff7eb3410, cond_func=0x7fff5c005cf8, field=0x7fff5c010a90, type=Item_func::GE_FUNC, value=0x7fff5c005bb8, cmp_type=TIME_RESULT) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8280
  #3  0x0000555555985b0b in get_func_mm_tree (param=0x7ffff7eb3410, cond_func=0x7fff5c005cf8, field=0x7fff5c010a90, value=0x0, cmp_type=TIME_RESULT, inv=false) at /home/psergey/dev-git/10.1/sql/opt_range.cc:7604
  #4  0x000055555598652d in get_full_func_mm_tree (param=0x7ffff7eb3410, cond_func=0x7fff5c005cf8, field_item=0x7fff5c005aa8, value=0x0, inv=false) at /home/psergey/dev-git/10.1/sql/opt_range.cc:7931
  #5  0x0000555555986de0 in Item_func_between::get_mm_tree (this=0x7fff5c005cf8, param=0x7ffff7eb3410, cond_ptr=0x7fff5c007eb0) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8117
  #6  0x000055555597c083 in SQL_SELECT::test_quick_select (this=0x7fff5c007ea8, thd=0x55555aad3700, keys_to_use=..., prev_tables=0, limit=18446744073709551615, force_quick_range=false, ordered_output=false, remove_false_parts_of_where=true) at /home/psergey/dev-git/10.1/sql/opt_range.cc:3133
  #7  0x0000555555aac9a4 in get_quick_record_count (thd=0x55555aad3700, select=0x7fff5c007ea8, table=0x7fff5c00f640, keys=0x7fff5c006f98, limit=18446744073709551615, in_outer_join=false) at /home/psergey/dev-git/10.1/sql/sql_select.cc:3362
  #8  0x0000555555aaed53 in make_join_statistics (join=0x7fff5c0065a0, tables_list=..., keyuse_array=0x7fff5c0068a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4004
  #9  0x0000555555aa5836 in JOIN::optimize_inner (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1341
  #10 0x0000555555aa478c in JOIN::optimize (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1023
{noformat}

Once SEL_ARG objects are there, the rest of things (getting records_in_range() estimates, handling of AND/OR, multi-component keys) will happen automatically.   The tricks is to create the right SEL_ARGs.",5,"Construction of range objects happens here:

{noformat}
  #0  SEL_ARG::SEL_ARG (this=0x7fff5c014a60, f=0x7fff5c010a90, min_value_arg=0x7fff5c014a58 """", max_value_arg=0x7fff5c014a58 """") at /home/psergey/dev-git/10.1/sql/opt_range.cc:2479
  #1  0x0000555555988784 in get_mm_leaf (param=0x7ffff7eb3410, conf_func=0x7fff5c005cf8, field=0x7fff5c010a90, key_part=0x7fff5c0147b8, type=Item_func::GE_FUNC, value=0x7fff5c005bb8) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8598
  #2  0x00005555559878c9 in get_mm_parts (param=0x7ffff7eb3410, cond_func=0x7fff5c005cf8, field=0x7fff5c010a90, type=Item_func::GE_FUNC, value=0x7fff5c005bb8, cmp_type=TIME_RESULT) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8280
  #3  0x0000555555985b0b in get_func_mm_tree (param=0x7ffff7eb3410, cond_func=0x7fff5c005cf8, field=0x7fff5c010a90, value=0x0, cmp_type=TIME_RESULT, inv=false) at /home/psergey/dev-git/10.1/sql/opt_range.cc:7604
  #4  0x000055555598652d in get_full_func_mm_tree (param=0x7ffff7eb3410, cond_func=0x7fff5c005cf8, field_item=0x7fff5c005aa8, value=0x0, inv=false) at /home/psergey/dev-git/10.1/sql/opt_range.cc:7931
  #5  0x0000555555986de0 in Item_func_between::get_mm_tree (this=0x7fff5c005cf8, param=0x7ffff7eb3410, cond_ptr=0x7fff5c007eb0) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8117
  #6  0x000055555597c083 in SQL_SELECT::test_quick_select (this=0x7fff5c007ea8, thd=0x55555aad3700, keys_to_use=..., prev_tables=0, limit=18446744073709551615, force_quick_range=false, ordered_output=false, remove_false_parts_of_where=true) at /home/psergey/dev-git/10.1/sql/opt_range.cc:3133
  #7  0x0000555555aac9a4 in get_quick_record_count (thd=0x55555aad3700, select=0x7fff5c007ea8, table=0x7fff5c00f640, keys=0x7fff5c006f98, limit=18446744073709551615, in_outer_join=false) at /home/psergey/dev-git/10.1/sql/sql_select.cc:3362
  #8  0x0000555555aaed53 in make_join_statistics (join=0x7fff5c0065a0, tables_list=..., keyuse_array=0x7fff5c0068a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4004
  #9  0x0000555555aa5836 in JOIN::optimize_inner (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1341
  #10 0x0000555555aa478c in JOIN::optimize (this=0x7fff5c0065a0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1023
{noformat}

Once SEL_ARG objects are there, the rest of things (getting records_in_range() estimates, handling of AND/OR, multi-component keys) will happen automatically.   The tricks is to create the right SEL_ARGs."
2661,MDEV-8320,MDEV,Sergei Petrunia,72356,2015-06-17 19:09:12,"Looking at where SEL_ARG object is made for a query with equality:
{noformat}
 explain select * from t100 where a= '2015-10-10' ;
{noformat}

{noformat}
  #0  SEL_ARG::SEL_ARG (this=0x7fff5c015cb0, f=0x7fff5c010a90, min_value_arg=0x7fff5c015ca8 """", max_value_arg=0x7fff5c015ca8 """") at /home/psergey/dev-git/10.1/sql/opt_range.cc:2479
  #1  0x0000555555988784 in get_mm_leaf (param=0x7ffff7eb3410, conf_func=0x7fff5c006d58, field=0x7fff5c010a90, key_part=0x7fff5c015a08, type=Item_func::EQ_FUNC, value=0x7fff5c005b88) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8598
  #2  0x00005555559878c9 in get_mm_parts (param=0x7ffff7eb3410, cond_func=0x7fff5c006d58, field=0x7fff5c010a90, type=Item_func::EQ_FUNC, value=0x7fff5c005b88, cmp_type=TIME_RESULT) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8280
  #3  0x00005555559872b3 in Item_equal::get_mm_tree (this=0x7fff5c006d58, param=0x7ffff7eb3410, cond_ptr=0x7fff5c008078) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8194
  #4  0x000055555597c083 in SQL_SELECT::test_quick_select (this=0x7fff5c008070, thd=0x55555aad3700, keys_to_use=..., prev_tables=0, limit=18446744073709551615, force_quick_range=false, ordered_output=false, remove_false_parts_of_where=true) at /home/psergey/dev-git/10.1/sql/opt_range.cc:3133
  #5  0x0000555555aac9a4 in get_quick_record_count (thd=0x55555aad3700, select=0x7fff5c008070, table=0x7fff5c00f640, keys=0x7fff5c007128, limit=18446744073709551615, in_outer_join=false) at /home/psergey/dev-git/10.1/sql/sql_select.cc:3362
  #6  0x0000555555aaed53 in make_join_statistics (join=0x7fff5c0064e0, tables_list=..., keyuse_array=0x7fff5c0067e0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4004
  #7  0x0000555555aa5836 in JOIN::optimize_inner (this=0x7fff5c0064e0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1341
  #8  0x0000555555aa478c in JOIN::optimize (this=0x7fff5c0064e0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1023
{noformat}

It looks like {{get_mm_leaf}} is the function which should have the logic that figures that
{noformat}
YEAR(key_col) = C1  -->   "" concat(C1, '-01-01') <=  key_col  <= concat(C1, '-12-31')  "" 
{noformat}
",6,"Looking at where SEL_ARG object is made for a query with equality:
{noformat}
 explain select * from t100 where a= '2015-10-10' ;
{noformat}

{noformat}
  #0  SEL_ARG::SEL_ARG (this=0x7fff5c015cb0, f=0x7fff5c010a90, min_value_arg=0x7fff5c015ca8 """", max_value_arg=0x7fff5c015ca8 """") at /home/psergey/dev-git/10.1/sql/opt_range.cc:2479
  #1  0x0000555555988784 in get_mm_leaf (param=0x7ffff7eb3410, conf_func=0x7fff5c006d58, field=0x7fff5c010a90, key_part=0x7fff5c015a08, type=Item_func::EQ_FUNC, value=0x7fff5c005b88) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8598
  #2  0x00005555559878c9 in get_mm_parts (param=0x7ffff7eb3410, cond_func=0x7fff5c006d58, field=0x7fff5c010a90, type=Item_func::EQ_FUNC, value=0x7fff5c005b88, cmp_type=TIME_RESULT) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8280
  #3  0x00005555559872b3 in Item_equal::get_mm_tree (this=0x7fff5c006d58, param=0x7ffff7eb3410, cond_ptr=0x7fff5c008078) at /home/psergey/dev-git/10.1/sql/opt_range.cc:8194
  #4  0x000055555597c083 in SQL_SELECT::test_quick_select (this=0x7fff5c008070, thd=0x55555aad3700, keys_to_use=..., prev_tables=0, limit=18446744073709551615, force_quick_range=false, ordered_output=false, remove_false_parts_of_where=true) at /home/psergey/dev-git/10.1/sql/opt_range.cc:3133
  #5  0x0000555555aac9a4 in get_quick_record_count (thd=0x55555aad3700, select=0x7fff5c008070, table=0x7fff5c00f640, keys=0x7fff5c007128, limit=18446744073709551615, in_outer_join=false) at /home/psergey/dev-git/10.1/sql/sql_select.cc:3362
  #6  0x0000555555aaed53 in make_join_statistics (join=0x7fff5c0064e0, tables_list=..., keyuse_array=0x7fff5c0067e0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:4004
  #7  0x0000555555aa5836 in JOIN::optimize_inner (this=0x7fff5c0064e0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1341
  #8  0x0000555555aa478c in JOIN::optimize (this=0x7fff5c0064e0) at /home/psergey/dev-git/10.1/sql/sql_select.cc:1023
{noformat}

It looks like {{get_mm_leaf}} is the function which should have the logic that figures that
{noformat}
YEAR(key_col) = C1  -->   "" concat(C1, '-01-01') <=  key_col  <= concat(C1, '-12-31')  "" 
{noformat}
"
2662,MDEV-8320,MDEV,Sergei Petrunia,72357,2015-06-17 19:11:21,"[~danblack], hopefully these are enough?  I hope I didn't miss anything.",7,"[~danblack], hopefully these are enough?  I hope I didn't miss anything."
2663,MDEV-8320,MDEV,Daniel Black,72373,2015-06-18 08:41:10,"great start, thanks [~psergei]!",8,"great start, thanks [~psergei]!"
2664,MDEV-8320,MDEV,Jiri Kavalik,73608,2015-07-17 22:58:42,"Thank you all for hints. I tried to implement this in https://github.com/jkavalik/server/commit/2a66fa9bde198bca78625210f632112586c818e7

No idea if it is the right way to do such a thing but it seems to work as I imagined. I did not yet manage to run complete test suite (had to put my laptop to sleep and it then crashed on timeout..) but I ran some (innodb, select, explain, type_date, type_datetime directly and then some 1200 from the entire suite before the sleep) without troubles.",9,"Thank you all for hints. I tried to implement this in URL

No idea if it is the right way to do such a thing but it seems to work as I imagined. I did not yet manage to run complete test suite (had to put my laptop to sleep and it then crashed on timeout..) but I ran some (innodb, select, explain, type_date, type_datetime directly and then some 1200 from the entire suite before the sleep) without troubles."
2665,MDEV-8320,MDEV,Sergei Petrunia,73733,2015-07-20 21:28:59,Commented on the patch in github.  Summary: found a number of issues which need to be resolved before this can be pushed.,10,Commented on the patch in github.  Summary: found a number of issues which need to be resolved before this can be pushed.
2666,MDEV-8320,MDEV,Jiri Kavalik,73762,2015-07-21 13:11:57,Thank you for the comments. I will try to work on those issues.,11,Thank you for the comments. I will try to work on those issues.
2667,MDEV-8320,MDEV,Daniel Black,73965,2015-07-28 10:26:28,"Minor addition, when these optimisations occur it would be good to see an addition text in the 'Extra' of the explain output like '{funct}(colref) optimised to range;'",12,"Minor addition, when these optimisations occur it would be good to see an addition text in the 'Extra' of the explain output like '{funct}(colref) optimised to range;'"
2668,MDEV-8320,MDEV,Sergei Petrunia,74555,2015-08-12 20:48:42,"Saw a customer case with conditions like this:

{noformat}
DATE(table.datetime_col) BETWEEN '2015-08-12' AND '2015-08-12'

DATE(table.date_col) BETWEEN '2015-08-12' AND '2015-08-12'
{noformat}

The columns have types {{DATE}} and {{DATETIME}}. 
",13,"Saw a customer case with conditions like this:

{noformat}
DATE(table.datetime_col) BETWEEN '2015-08-12' AND '2015-08-12'

DATE(table.date_col) BETWEEN '2015-08-12' AND '2015-08-12'
{noformat}

The columns have types {{DATE}} and {{DATETIME}}. 
"
2669,MDEV-8320,MDEV,Sergei Petrunia,84513,2016-06-23 21:01:57,"Saw another case where the real-world queries are using conditions in form:

{noformat}
DATE(date_col) = '2016-06-23'
{noformat}

Sometimes these conditions are OR-ed together.  The comparison is always with a constant.
It could be that date_col is actually a DATETIME.",14,"Saw another case where the real-world queries are using conditions in form:

{noformat}
DATE(date_col) = '2016-06-23'
{noformat}

Sometimes these conditions are OR-ed together.  The comparison is always with a constant.
It could be that date_col is actually a DATETIME."
2670,MDEV-8320,MDEV,Alexey Botchkov,86618,2016-09-20 09:37:32,"The patch v0. Makes DATE(date_col) <>= CONST_DATE sargable.
http://lists.askmonty.org/pipermail/commits/2016-September/009874.html",15,"The patch v0. Makes DATE(date_col) <>= CONST_DATE sargable.
URL"
2671,MDEV-8320,MDEV,Alexey Botchkov,86619,2016-09-20 09:37:50,http://lists.askmonty.org/pipermail/commits/2016-September/009874.html,16,URL
2672,MDEV-8320,MDEV,Michaël de groot,86726,2016-09-22 20:36:41,"When this is done, please don't forget to update this kb article: https://mariadb.com/kb/en/mariadb/building-the-best-index-for-a-given-select/",17,"When this is done, please don't forget to update this kb article: URL"
2673,MDEV-8320,MDEV,Alexey Botchkov,86870,2016-09-28 11:06:13,"Test added.
http://lists.askmonty.org/pipermail/commits/2016-September/009935.html",18,"Test added.
URL"
2674,MDEV-8320,MDEV,Michaël de groot,95937,2017-05-30 08:17:43,"This issue is in review for a while now, any status update?

Thanks!
",19,"This issue is in review for a while now, any status update?

Thanks!
"
2675,MDEV-8320,MDEV,Sergei Petrunia,103240,2017-11-16 05:23:22,"So the patch works when the WHERE clause has just the condition we are
targeting:

{noformat}
explain select * from t1 where date(a) < '2017-01-10' ;
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
| id   | select_type | table | type  | possible_keys | key  | key_len | ref  | rows | Extra                 |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
|    1 | SIMPLE      | t1    | range | a             | a    | 6       | NULL |   90 | Using index condition |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
{noformat}

but doesn't work with equality propagation:
{noformat}
explain select * from t1 where date(a1) < '2017-01-10' and a1=a;
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows  | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 10122 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
{noformat}

Actually, even irrelevant equalities make the patch not to work. Let's add
""b=b1"" to the first query and we get:
{noformat}
explain select * from t1 where date(a) < '2017-01-10' and b=b1;
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows  | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 10122 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
{noformat}

Another thing: the patch works only if DATE(col) is on the left side of the equality:
{noformat}
explain select * from t1 where date(a) = '2017-01-10';
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
| id   | select_type | table | type  | possible_keys | key  | key_len | ref  | rows | Extra                 |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
|    1 | SIMPLE      | t1    | range | a             | a    | 6       | NULL |   10 | Using index condition |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
{noformat}

{noformat}
explain select * from t1 where '2017-01-10'=date(a);
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows  | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 10122 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
{noformat}
",20,"So the patch works when the WHERE clause has just the condition we are
targeting:

{noformat}
explain select * from t1 where date(a) < '2017-01-10' ;
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
| id   | select_type | table | type  | possible_keys | key  | key_len | ref  | rows | Extra                 |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
|    1 | SIMPLE      | t1    | range | a             | a    | 6       | NULL |   90 | Using index condition |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
{noformat}

but doesn't work with equality propagation:
{noformat}
explain select * from t1 where date(a1) < '2017-01-10' and a1=a;
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows  | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 10122 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
{noformat}

Actually, even irrelevant equalities make the patch not to work. Let's add
""b=b1"" to the first query and we get:
{noformat}
explain select * from t1 where date(a) < '2017-01-10' and b=b1;
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows  | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 10122 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
{noformat}

Another thing: the patch works only if DATE(col) is on the left side of the equality:
{noformat}
explain select * from t1 where date(a) = '2017-01-10';
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
| id   | select_type | table | type  | possible_keys | key  | key_len | ref  | rows | Extra                 |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
|    1 | SIMPLE      | t1    | range | a             | a    | 6       | NULL |   10 | Using index condition |
+------+-------------+-------+-------+---------------+------+---------+------+------+-----------------------+
{noformat}

{noformat}
explain select * from t1 where '2017-01-10'=date(a);
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows  | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 10122 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+-------+-------------+
{noformat}
"
2676,MDEV-8320,MDEV,Sergei Petrunia,103243,2017-11-16 06:17:14,"Elaborating on Monty's ""do a rewrite at fix_fields"" proposal.

Let's consider YEAR(a), where a is a DATETIME column.

introduce functions:

{noformat}
YEAR_START(datetime_col) returns CONCAT(YEAR(datetime_col), '-01-01 00:00:00')
YEAR_END(datetime_col) returns CONCAT(YEAR(datetime_col), '-31-12 23:59:59 ...')
{noformat}
(add more precision if necessary)

then
{noformat}
YEAR(col) <= val  is the same as  col <= YEAR_END(val)
YEAR(col) < val   is the same as  col < YEAR_START(val)
{noformat}

{noformat}
YEAR(col) >= val  is the same as  col >= YEAR_START(val)
YEAR(col) > val   is the same as  col > YEAR_END(val)
{noformat}

{noformat}
YEAR(col) = val   is the same as  col>=YEAR_START(val) AND col<=YEAR_END(val)
{noformat}

{noformat}
YEAR(col) BETWEEN val1 AND val2 
{noformat}
is the same as 
{noformat}
YEAR(col) >= val2 AND YEAR(col) <= val2
{noformat}
which reduces it to the cases already handled above.

If we use the rewrite approach, equality propagation will work automatically.
",21,"Elaborating on Monty's ""do a rewrite at fix_fields"" proposal.

Let's consider YEAR(a), where a is a DATETIME column.

introduce functions:

{noformat}
YEAR_START(datetime_col) returns CONCAT(YEAR(datetime_col), '-01-01 00:00:00')
YEAR_END(datetime_col) returns CONCAT(YEAR(datetime_col), '-31-12 23:59:59 ...')
{noformat}
(add more precision if necessary)

then
{noformat}
YEAR(col) <= val  is the same as  col <= YEAR_END(val)
YEAR(col) < val   is the same as  col < YEAR_START(val)
{noformat}

{noformat}
YEAR(col) >= val  is the same as  col >= YEAR_START(val)
YEAR(col) > val   is the same as  col > YEAR_END(val)
{noformat}

{noformat}
YEAR(col) = val   is the same as  col>=YEAR_START(val) AND col<=YEAR_END(val)
{noformat}

{noformat}
YEAR(col) BETWEEN val1 AND val2 
{noformat}
is the same as 
{noformat}
YEAR(col) >= val2 AND YEAR(col) <= val2
{noformat}
which reduces it to the cases already handled above.

If we use the rewrite approach, equality propagation will work automatically.
"
2677,MDEV-8320,MDEV,Sergei Petrunia,103378,2017-11-20 07:48:20,I'm working on a patch to implement the new proposal.,22,I'm working on a patch to implement the new proposal.
2678,MDEV-8320,MDEV,Sergei Petrunia,103567,2017-11-23 14:47:02,"A patch: http://lists.askmonty.org/pipermail/commits/2017-November/011674.html
Still need to figure out if we need to take measures to account for leap seconds.",23,"A patch: URL
Still need to figure out if we need to take measures to account for leap seconds."
2679,MDEV-8320,MDEV,Sergei Petrunia,103568,2017-11-23 14:50:35,"Somehow, I am unable to observe this leap second.
I get Ubuntu 16.04.3 LTS, compile mysql-5.7, get the datadir from mysql-test/var/install.db, with non-empty mysql.time_zone* tables, and try to reproduce what is said at: https://dev.mysql.com/doc/refman/5.7/en/time-zone-leap-seconds.html.

The docs say:
{noformat}
mysql> -- values differ internally but display the same
mysql> SELECT a, ts, UNIX_TIMESTAMP(ts) FROM t1;
+------+---------------------+--------------------+
| a    | ts                  | UNIX_TIMESTAMP(ts) |
+------+---------------------+--------------------+
|    1 | 2008-12-31 23:59:59 |         1230767999 |
|    2 | 2008-12-31 23:59:59 |         1230768000 |
+------+---------------------+--------------------+
2 rows in set (0.00 sec)
{noformat}

while I get:
{noformat}
mysql> SELECT a, ts, UNIX_TIMESTAMP(ts) FROM t1;
+------+---------------------+--------------------+
| a    | ts                  | UNIX_TIMESTAMP(ts) |
+------+---------------------+--------------------+
|    1 | 2008-12-31 23:59:59 |         1230767999 |
|    2 | 2009-01-01 00:00:00 |         1230768000 |
+------+---------------------+--------------------+
2 rows in set (0.00 sec)

{noformat}
",24,"Somehow, I am unable to observe this leap second.
I get Ubuntu 16.04.3 LTS, compile mysql-5.7, get the datadir from mysql-test/var/install.db, with non-empty mysql.time_zone* tables, and try to reproduce what is said at: URL

The docs say:
{noformat}
mysql> -- values differ internally but display the same
mysql> SELECT a, ts, UNIX_TIMESTAMP(ts) FROM t1;
+------+---------------------+--------------------+
| a    | ts                  | UNIX_TIMESTAMP(ts) |
+------+---------------------+--------------------+
|    1 | 2008-12-31 23:59:59 |         1230767999 |
|    2 | 2008-12-31 23:59:59 |         1230768000 |
+------+---------------------+--------------------+
2 rows in set (0.00 sec)
{noformat}

while I get:
{noformat}
mysql> SELECT a, ts, UNIX_TIMESTAMP(ts) FROM t1;
+------+---------------------+--------------------+
| a    | ts                  | UNIX_TIMESTAMP(ts) |
+------+---------------------+--------------------+
|    1 | 2008-12-31 23:59:59 |         1230767999 |
|    2 | 2009-01-01 00:00:00 |         1230768000 |
+------+---------------------+--------------------+
2 rows in set (0.00 sec)

{noformat}
"
2680,MDEV-8320,MDEV,Sergei Petrunia,103603,2017-11-24 12:34:16,"Also discussed with [~bar].

Takeaways:
* one needs to have TZ data loaded into the server in order to observe the leap second. 
** Check mysql_tzinfo_to_sql, and --leap parameter. 
** Note that not all timezone files have leap second information (use ""file"" to see which do)
* DATE[TIME] type has no leap seconds. only TIMESTAMP does
* TIMESTAMP has open bugs like MDEV-13995
** It's better to not have this patch support TIMESTAMP for now. Let's do it just for DATE TIMEs.
** This means we dont have to support leap seconds.
** 
",25,"Also discussed with [~bar].

Takeaways:
* one needs to have TZ data loaded into the server in order to observe the leap second. 
** Check mysql_tzinfo_to_sql, and --leap parameter. 
** Note that not all timezone files have leap second information (use ""file"" to see which do)
* DATE[TIME] type has no leap seconds. only TIMESTAMP does
* TIMESTAMP has open bugs like MDEV-13995
** It's better to not have this patch support TIMESTAMP for now. Let's do it just for DATE TIMEs.
** This means we dont have to support leap seconds.
** 
"
2681,MDEV-8320,MDEV,Sergei Petrunia,103725,2017-11-28 13:35:39,"Adjusted the patch according to the above: http://lists.askmonty.org/pipermail/commits/2017-November/011680.html

A test patch to also make rewrite function a member of Item: http://lists.askmonty.org/pipermail/commits/2017-November/011681.html",26,"Adjusted the patch according to the above: URL

A test patch to also make rewrite function a member of Item: URL"
2682,MDEV-8320,MDEV,Sergei Petrunia,103726,2017-11-28 13:36:10,"[~bar], please review.",27,"[~bar], please review."
2683,MDEV-8320,MDEV,Oleg Smirnov,253630,2023-03-17 07:42:33,"There are test failures looking like
{code}
main.sargable_date_cond                  w3 [ fail ]
        Test ended at 2023-03-16 04:35:43

CURRENT_TEST: main.sargable_date_cond
--- /usr/share/mysql/mysql-test/main/sargable_date_cond.result	2023-03-16 01:36:33.000000000 -0400
+++ /dev/shm/var/3/log/sargable_date_cond.reject	2023-03-16 04:35:42.953159711 -0400
@@ -1919,6 +1919,8 @@
 timestampadd(hour, B.a, date_add('2016-01-01', interval A.a*8 day)),
 date_add('2016-01-01', interval A.a*7 day)
 from t1 A, t0 B;
+Warnings:
+Warning	1299	Invalid TIMESTAMP value in column 'a' at row 11
{code}
that occur only on some machines and not on others. Turns out it depends on whether the system time zone employs daylight saving time. 

If it does then calling timestampadd() can lead to execution of the following code
{code: title=my_time.cc}
  /*
    Fix that if we are in the non existing daylight saving time hour
    we move the start of the next real hour.

    This code doesn't handle such exotical thing as time-gaps whose length
    is more than one hour or non-integer (latter can theoretically happen
    if one of seconds will be removed due leap correction, or because of
    general time correction like it happened for Africa/Monrovia time zone
    in year 1972).
  */
  if (loop == 2 && t->hour != (uint) l_time->tm_hour)
  {
    int days= t->day - l_time->tm_mday;
    if (days < -1)
      days=1;					/* Month has wrapped */
    else if (days > 1)
      days= -1;
    diff=(3600L*(long) (days*24+((int) t->hour - (int) l_time->tm_hour))+
	  (long) (60*((int) t->minute - (int) l_time->tm_min)) +
          (long) ((int) t->second - (int) l_time->tm_sec));
    if (diff == 3600)
      tmp+=3600 - t->minute*60 - t->second;	/* Move to next hour */
    else if (diff == -3600)
      tmp-=t->minute*60 + t->second;		/* Move to previous hour */

    *error_code= ER_WARN_INVALID_TIMESTAMP;
  }
{code}",28,"There are test failures looking like
{code}
main.sargable_date_cond                  w3 [ fail ]
        Test ended at 2023-03-16 04:35:43

CURRENT_TEST: main.sargable_date_cond
--- /usr/share/mysql/mysql-test/main/sargable_date_cond.result	2023-03-16 01:36:33.000000000 -0400
+++ /dev/shm/var/3/log/sargable_date_cond.reject	2023-03-16 04:35:42.953159711 -0400
@@ -1919,6 +1919,8 @@
 timestampadd(hour, B.a, date_add('2016-01-01', interval A.a*8 day)),
 date_add('2016-01-01', interval A.a*7 day)
 from t1 A, t0 B;
+Warnings:
+Warning	1299	Invalid TIMESTAMP value in column 'a' at row 11
{code}
that occur only on some machines and not on others. Turns out it depends on whether the system time zone employs daylight saving time. 

If it does then calling timestampadd() can lead to execution of the following code
{code: title=my_time.cc}
  /*
    Fix that if we are in the non existing daylight saving time hour
    we move the start of the next real hour.

    This code doesn't handle such exotical thing as time-gaps whose length
    is more than one hour or non-integer (latter can theoretically happen
    if one of seconds will be removed due leap correction, or because of
    general time correction like it happened for Africa/Monrovia time zone
    in year 1972).
  */
  if (loop == 2 && t->hour != (uint) l_time->tm_hour)
  {
    int days= t->day - l_time->tm_mday;
    if (days < -1)
      days=1;					/* Month has wrapped */
    else if (days > 1)
      days= -1;
    diff=(3600L*(long) (days*24+((int) t->hour - (int) l_time->tm_hour))+
	  (long) (60*((int) t->minute - (int) l_time->tm_min)) +
          (long) ((int) t->second - (int) l_time->tm_sec));
    if (diff == 3600)
      tmp+=3600 - t->minute*60 - t->second;	/* Move to next hour */
    else if (diff == -3600)
      tmp-=t->minute*60 + t->second;		/* Move to previous hour */

    *error_code= ER_WARN_INVALID_TIMESTAMP;
  }
{code}"
2684,MDEV-8320,MDEV,Sergei Petrunia,253851,2023-03-20 08:59:56,https://github.com/MariaDB/server/tree/preview-11.1-mdev-8320,29,URL
2685,MDEV-8320,MDEV,Lena Startseva,255205,2023-04-04 09:38:10,"At the moment, the task has been tested with cases for the SELECT, a fix of  [#MDEV-30946]  is needed to continue testing",30,"At the moment, the task has been tested with cases for the SELECT, a fix of  [#MDEV-30946]  is needed to continue testing"
2686,MDEV-8320,MDEV,Lena Startseva,257057,2023-04-25 05:06:50,Testing done. Ok to push,31,Testing done. Ok to push
2687,MDEV-8320,MDEV,Oleg Smirnov,257152,2023-04-25 16:08:32,Pushed to 11.1,32,Pushed to 11.1
2688,MDEV-8348,MDEV,Oleksandr Byelkin,85811,2016-08-24 12:45:49,"DEFAULT looks ok.
There can not be DEFAULT partition for HASH and KEY partition types (If they can I can find why).",1,"DEFAULT looks ok.
There can not be DEFAULT partition for HASH and KEY partition types (If they can I can find why)."
2689,MDEV-8348,MDEV,Oleksandr Byelkin,85813,2016-08-24 13:04:16,VALUES LESS THAN (MAXVALUE) is effectively DEFAULT partition for RANGE partition so probably it has sens to make syntax sugar here.,2,VALUES LESS THAN (MAXVALUE) is effectively DEFAULT partition for RANGE partition so probably it has sens to make syntax sugar here.
2690,MDEV-8348,MDEV,Oleksandr Byelkin,85815,2016-08-24 13:15:49,"Actual LIST partitioning syntax in MariaDB is
{code:sql}
PARTITION BY LIST (partitioning_expression)
(
	PARTITION partition_name VALUES IN (value_list),
	[ PARTITION partition_name VALUES IN (value_list), ... ]
)
{code}

IMHO following syntax is what correspond to it:

{code:sql}
PARTITION BY LIST (partitioning_expression)
(
	PARTITION partition_name VALUES IN (value_list),
	[ PARTITION partition_name VALUES IN (value_list), ... ]
        [ PARTITION partition_name DEFAULT ]
)
{code}

""PARTITION  partition_name VALUES IN (DEFAULT)"" looks strange IMHO.",3,"Actual LIST partitioning syntax in MariaDB is
{code:sql}
PARTITION BY LIST (partitioning_expression)
(
	PARTITION partition_name VALUES IN (value_list),
	[ PARTITION partition_name VALUES IN (value_list), ... ]
)
{code}

IMHO following syntax is what correspond to it:

{code:sql}
PARTITION BY LIST (partitioning_expression)
(
	PARTITION partition_name VALUES IN (value_list),
	[ PARTITION partition_name VALUES IN (value_list), ... ]
        [ PARTITION partition_name DEFAULT ]
)
{code}

""PARTITION  partition_name VALUES IN (DEFAULT)"" looks strange IMHO."
2691,MDEV-8348,MDEV,Oleksandr Byelkin,85817,2016-08-24 15:34:46,"-For beginning prune_partitions() should always mark default partition as used.-

After discussing it with Sergei Petrunia we decided that with current approach it will be done automatically.",4,"-For beginning prune_partitions() should always mark default partition as used.-

After discussing it with Sergei Petrunia we decided that with current approach it will be done automatically."
2692,MDEV-8348,MDEV,Oleksandr Byelkin,85837,2016-08-25 15:18:04,"As far as several columns expression possible, we have to support
PARTITION p1 VALUES IN ((default, 2))
hopefully it will not conflict with default values of correspondent column in users understanding...
",5,"As far as several columns expression possible, we have to support
PARTITION p1 VALUES IN ((default, 2))
hopefully it will not conflict with default values of correspondent column in users understanding...
"
2693,MDEV-8348,MDEV,Oleksandr Byelkin,85855,2016-08-26 13:03:18,"Taking into account complications with deciding where to put record (2,3) when we have partitions (DEFAULT,3) and (2, DEFAULT) it will be better to have only one  partition which catch all missed records. So syntax will be following:
{code:sql}
PARTITION BY LIST (partitioning_expression)
(
	PARTITION partition_name VALUES IN (value_list),
	[ PARTITION partition_name VALUES IN (value_list), ... ]
        [ PARTITION partition_name DEFAULT ]
)
{code}
{code:sql}
PARTITION BY LIST COLUMNS(column_list)
(
	PARTITION partition_name VALUES IN ((value_list),(value_list),...),
	[ PARTITION partition_name VALUES IN ((value_list),(value_list), ...) ]
        [ PARTITION partition_name DEFAULT ]
)
{code}
",6,"Taking into account complications with deciding where to put record (2,3) when we have partitions (DEFAULT,3) and (2, DEFAULT) it will be better to have only one  partition which catch all missed records. So syntax will be following:
{code:sql}
PARTITION BY LIST (partitioning_expression)
(
	PARTITION partition_name VALUES IN (value_list),
	[ PARTITION partition_name VALUES IN (value_list), ... ]
        [ PARTITION partition_name DEFAULT ]
)
{code}
{code:sql}
PARTITION BY LIST COLUMNS(column_list)
(
	PARTITION partition_name VALUES IN ((value_list),(value_list),...),
	[ PARTITION partition_name VALUES IN ((value_list),(value_list), ...) ]
        [ PARTITION partition_name DEFAULT ]
)
{code}
"
2694,MDEV-8348,MDEV,Oleksandr Byelkin,86075,2016-09-02 13:15:00,"revision-id: 442a3d4766f9115bfd496dd4fafd38f7aeb0e273 (mariadb-10.2.1-52-g442a3d4)
parent(s): 1eb58ff3b8569d7dad1f5c180a5e55683e53d205
committer: Oleksandr Byelkin
timestamp: 2016-09-02 15:09:12 +0200
message:

MDEV-8348: Add catchall to all table partitioning for list partitions

DEFAULT partition support added to LIST and LIST COLUMN partitioning.
Partitions Prunning added for DEFAULT partititon.

---",7,"revision-id: 442a3d4766f9115bfd496dd4fafd38f7aeb0e273 (mariadb-10.2.1-52-g442a3d4)
parent(s): 1eb58ff3b8569d7dad1f5c180a5e55683e53d205
committer: Oleksandr Byelkin
timestamp: 2016-09-02 15:09:12 +0200
message:

MDEV-8348: Add catchall to all table partitioning for list partitions

DEFAULT partition support added to LIST and LIST COLUMN partitioning.
Partitions Prunning added for DEFAULT partititon.

---"
2695,MDEV-8348,MDEV,Oleksandr Byelkin,86163,2016-09-06 20:54:15,"revision-id: 06310902e5754fc2ead73d903267bb9fa85b85b2 (mariadb-10.2.1-52-g0631090)
parent(s): 1eb58ff3b8569d7dad1f5c180a5e55683e53d205
committer: Oleksandr Byelkin
timestamp: 2016-09-06 22:52:39 +0200
message:

MDEV-8348: Add catchall to all table partitioning for list partitions

DEFAULT partition support added to LIST and LIST COLUMN partitioning.
Partitions Prunning added for DEFAULT partititon.

---",8,"revision-id: 06310902e5754fc2ead73d903267bb9fa85b85b2 (mariadb-10.2.1-52-g0631090)
parent(s): 1eb58ff3b8569d7dad1f5c180a5e55683e53d205
committer: Oleksandr Byelkin
timestamp: 2016-09-06 22:52:39 +0200
message:

MDEV-8348: Add catchall to all table partitioning for list partitions

DEFAULT partition support added to LIST and LIST COLUMN partitioning.
Partitions Prunning added for DEFAULT partititon.

---"
2696,MDEV-8348,MDEV,Krishnadas,87045,2016-10-04 04:24:00,DBS test cases for MDEV-8348 [^MDEV-8348.zip] ,9,DBS test cases for MDEV-8348 [^MDEV-8348.zip] 
2697,MDEV-8491,MDEV,Elena Stepanova,73589,2015-07-17 14:49:20,"What if the shutdown was initiated by kill <pid>, which probably happens very often? ",1,"What if the shutdown was initiated by kill , which probably happens very often? "
2698,MDEV-8491,MDEV,Stoykov,73590,2015-07-17 14:54:55,"I don't ask for the impossible .. :D 
But the shutdown command seems to be omitted by the audit plugin. So from the security auditing point of view,  it will be a good addition/feature of MariaDB. ",2,"I don't ask for the impossible .. :D 
But the shutdown command seems to be omitted by the audit plugin. So from the security auditing point of view,  it will be a good addition/feature of MariaDB. "
2699,MDEV-8491,MDEV,Stoykov,73591,2015-07-17 15:08:58,"my bad, in fact the MariaDB  SHUTDOWN command is logged  via the audit plugin , but the mysqladmin SHUTDOWN command is not logged :
 5.5.44-MariaDB-log : 
{noformat}
mysql root@darkstar:[Fri Jul 17 14:10:01 2015][(none)]> INSTALL PLUGIN SERVER_AUDIT SONAME 'server_audit.so';
Query OK, 0 rows affected (0.06 sec)

mysql root@darkstar:[Fri Jul 17 14:10:12 2015][(none)]> set global server_audit_logging = 1;
Query OK, 0 rows affected (0.01 sec)
mysql root@darkstar:[Fri Jul 17 14:12:58 2015][(none)]> show global variables like '%audit%';
+-------------------------------+-----------------------+
| Variable_name                 | Value                 |
+-------------------------------+-----------------------+
| server_audit_events           |                       |
| server_audit_excl_users       |                       |
| server_audit_file_path        | server_audit.log      |
| server_audit_file_rotate_now  | OFF                   |
| server_audit_file_rotate_size | 1000000               |
| server_audit_file_rotations   | 9                     |
| server_audit_incl_users       |                       |
| server_audit_logging          | ON                    |
| server_audit_mode             | 0                     |
| server_audit_output_type      | file                  |
| server_audit_query_log_limit  | 1024                  |
| server_audit_syslog_facility  | LOG_USER              |
| server_audit_syslog_ident     | mysql-server_auditing |
| server_audit_syslog_info      |                       |
| server_audit_syslog_priority  | LOG_INFO              |
+-------------------------------+-----------------------+
15 rows in set (0.00 sec)

mysql root@darkstar:[Fri Jul 17 14:13:01 2015][(none)]> select version();
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)
ERROR: Can't connect to the server
root@darkstar:[Fri Jul 17 14:12:45][/var/lib/mysql]$ mysqladmin -p shutdown
Enter password: 
root@darkstar:[Fri Jul 17 14:13:10][/var/lib/mysql]$ tail server_audit.log 
20150717 14:10:42,darkstar,root,localhost,9,0,CONNECT,,,0
20150717 14:10:42,darkstar,root,localhost,9,219,QUERY,,'SHOW VARIABLES LIKE \'pid_file\'',0
20150717 14:10:42,darkstar,root,localhost,9,0,DISCONNECT,,,0
20150717 14:10:42,darkstar,root,localhost,8,0,DISCONNECT,,,0
20150717 14:12:58,darkstar,root,localhost,3,12,QUERY,,'set global server_audit_logging = 1',0
20150717 14:13:01,darkstar,root,localhost,3,13,QUERY,,'show global variables like \'%audit%\'',0
20150717 14:13:09,darkstar,root,localhost,4,0,CONNECT,,,0
20150717 14:13:09,darkstar,root,localhost,4,14,QUERY,,'SHOW VARIABLES LIKE \'pid_file\'',0
20150717 14:13:09,darkstar,root,localhost,4,0,DISCONNECT,,,0
20150717 14:13:09,darkstar,root,localhost,3,0,DISCONNECT,,,0
root@darkstar:[Fri Jul 17 14:13:14][/var/lib/mysql]$ mysqladmin -p shutdown


{noformat}",3,"my bad, in fact the MariaDB  SHUTDOWN command is logged  via the audit plugin , but the mysqladmin SHUTDOWN command is not logged :
 5.5.44-MariaDB-log : 
{noformat}
mysql root@darkstar:[Fri Jul 17 14:10:01 2015][(none)]> INSTALL PLUGIN SERVER_AUDIT SONAME 'server_audit.so';
Query OK, 0 rows affected (0.06 sec)

mysql root@darkstar:[Fri Jul 17 14:10:12 2015][(none)]> set global server_audit_logging = 1;
Query OK, 0 rows affected (0.01 sec)
mysql root@darkstar:[Fri Jul 17 14:12:58 2015][(none)]> show global variables like '%audit%';
+-------------------------------+-----------------------+
| Variable_name                 | Value                 |
+-------------------------------+-----------------------+
| server_audit_events           |                       |
| server_audit_excl_users       |                       |
| server_audit_file_path        | server_audit.log      |
| server_audit_file_rotate_now  | OFF                   |
| server_audit_file_rotate_size | 1000000               |
| server_audit_file_rotations   | 9                     |
| server_audit_incl_users       |                       |
| server_audit_logging          | ON                    |
| server_audit_mode             | 0                     |
| server_audit_output_type      | file                  |
| server_audit_query_log_limit  | 1024                  |
| server_audit_syslog_facility  | LOG_USER              |
| server_audit_syslog_ident     | mysql-server_auditing |
| server_audit_syslog_info      |                       |
| server_audit_syslog_priority  | LOG_INFO              |
+-------------------------------+-----------------------+
15 rows in set (0.00 sec)

mysql root@darkstar:[Fri Jul 17 14:13:01 2015][(none)]> select version();
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)
ERROR: Can't connect to the server
root@darkstar:[Fri Jul 17 14:12:45][/var/lib/mysql]$ mysqladmin -p shutdown
Enter password: 
root@darkstar:[Fri Jul 17 14:13:10][/var/lib/mysql]$ tail server_audit.log 
20150717 14:10:42,darkstar,root,localhost,9,0,CONNECT,,,0
20150717 14:10:42,darkstar,root,localhost,9,219,QUERY,,'SHOW VARIABLES LIKE \'pid_file\'',0
20150717 14:10:42,darkstar,root,localhost,9,0,DISCONNECT,,,0
20150717 14:10:42,darkstar,root,localhost,8,0,DISCONNECT,,,0
20150717 14:12:58,darkstar,root,localhost,3,12,QUERY,,'set global server_audit_logging = 1',0
20150717 14:13:01,darkstar,root,localhost,3,13,QUERY,,'show global variables like \'%audit%\'',0
20150717 14:13:09,darkstar,root,localhost,4,0,CONNECT,,,0
20150717 14:13:09,darkstar,root,localhost,4,14,QUERY,,'SHOW VARIABLES LIKE \'pid_file\'',0
20150717 14:13:09,darkstar,root,localhost,4,0,DISCONNECT,,,0
20150717 14:13:09,darkstar,root,localhost,3,0,DISCONNECT,,,0
root@darkstar:[Fri Jul 17 14:13:14][/var/lib/mysql]$ mysqladmin -p shutdown


{noformat}"
2700,MDEV-8491,MDEV,Sergey Vojtovich,78502,2015-11-25 16:13:06,"[~serg], please review patch for this task.",4,"[~serg], please review patch for this task."
2701,MDEV-8491,MDEV,Sergey Vojtovich,79360,2015-12-24 17:55:37,"[~serg], please review patch for this task.",5,"[~serg], please review patch for this task."
2702,MDEV-8542,MDEV,Sergey Vojtovich,78286,2015-11-20 15:38:25,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2703,MDEV-8646,MDEV,Sergei Petrunia,79313,2015-12-21 20:29:32,"The code was pushed into bb-10.1-mdev8646 branch.  The branch is based on 10.1, although the MDEV is targeted at 10.2

",1,"The code was pushed into bb-10.1-mdev8646 branch.  The branch is based on 10.1, although the MDEV is targeted at 10.2

"
2704,MDEV-8646,MDEV,Sergei Petrunia,79338,2015-12-23 00:30:12,"{noformat}
-2      DERIVED t1      ALL     NULL    NULL    NULL    NULL    2       100.00  Using temporary; Using filesort
+2      DERIVED t1      ALL     NULL    NULL    NULL    NULL    2       100.00  Using temporary
{noformat}

All of the test failures that miss ""Using filesort"" that I saw are for derived tables. 
No idea about the cause, so far.",2,"{noformat}
-2      DERIVED t1      ALL     NULL    NULL    NULL    NULL    2       100.00  Using temporary; Using filesort
+2      DERIVED t1      ALL     NULL    NULL    NULL    NULL    2       100.00  Using temporary
{noformat}

All of the test failures that miss ""Using filesort"" that I saw are for derived tables. 
No idea about the cause, so far."
2705,MDEV-8646,MDEV,Igor Babaev,79377,2015-12-26 05:25:07,"I fixed the above problem removing the following code from make_aggr_tables_info():

{noformat}
if (!only_const_tables() &&
    !join_tab[const_tables].table->sort.io_cache)
{
   /*
    If no IO cache exists for the first table then we are using an
    INDEX SCAN and no filesort. Thus we should not remove the sorted
     attribute on the INDEX SCAN.
   */
   skip_sort_order= true;
}
{noformat}
",3,"I fixed the above problem removing the following code from make_aggr_tables_info():

{noformat}
if (!only_const_tables() &&
    !join_tab[const_tables].table->sort.io_cache)
{
   /*
    If no IO cache exists for the first table then we are using an
    INDEX SCAN and no filesort. Thus we should not remove the sorted
     attribute on the INDEX SCAN.
   */
   skip_sort_order= true;
}
{noformat}
"
2706,MDEV-8646,MDEV,Sergei Petrunia,79398,2015-12-27 15:11:14,"Note: the 10.1 tree that the patch was based on already had failures in these tests: 

{noformat}
Failing test(s): main.system_mysql_db_fix40123 main.system_mysql_db_fix50030 main.system_mysql_db_fix50117 main.openssl_6975 main.events_1
{noformat}",4,"Note: the 10.1 tree that the patch was based on already had failures in these tests: 

{noformat}
Failing test(s): main.system_mysql_db_fix40123 main.system_mysql_db_fix50030 main.system_mysql_db_fix50117 main.openssl_6975 main.events_1
{noformat}"
2707,MDEV-8646,MDEV,Sergei Petrunia,79399,2015-12-27 15:39:47,"Spent more time reading the code.. 

Some observations:

- The patch is based on the revision of 10.1 that was before that I got {{ANALYZE statement}} feature to sort-of-work for GROUP/ORDER BY constructs.
- Because of the above, it is tempting to do the merge with current 10.1, but it is difficult to do that when so many test failures are present.  I guess we will need to fix the failures first and then do the merge. 
- MariaDB already has a subset of what this MDEV does in {{JOIN::pre_sort_join_tab}}.  This will need to be removed. 
",5,"Spent more time reading the code.. 

Some observations:

- The patch is based on the revision of 10.1 that was before that I got {{ANALYZE statement}} feature to sort-of-work for GROUP/ORDER BY constructs.
- Because of the above, it is tempting to do the merge with current 10.1, but it is difficult to do that when so many test failures are present.  I guess we will need to fix the failures first and then do the merge. 
- MariaDB already has a subset of what this MDEV does in {{JOIN::pre_sort_join_tab}}.  This will need to be removed. 
"
2708,MDEV-8646,MDEV,Sergei Petrunia,79400,2015-12-27 15:45:35,"[~igor], sorry I've missed your above comment re

{quote}
I fixed the above problem removing the following code from make_aggr_tables_info()
{quote}

I've just pushed a fix which I believe is a superset of your fix.",6,"[~igor], sorry I've missed your above comment re

{quote}
I fixed the above problem removing the following code from make_aggr_tables_info()
{quote}

I've just pushed a fix which I believe is a superset of your fix."
2709,MDEV-8646,MDEV,Sergei Petrunia,79402,2015-12-27 16:52:53,"Pushed a fix for {{group_by.test}} failure.  EXPLAIN in the old code showed wrong query plan. 

Whether the optimizer should run filesort in such cases is another question.  It does run it in current 10.1, pre-MDEV-8646 10.1, and 10.1-MDEV-8646.   I am not sure if MySQL has fixed this problem. If they did, I assume we don't want to port that fix within the scope of this MDEV.",7,"Pushed a fix for {{group_by.test}} failure.  EXPLAIN in the old code showed wrong query plan. 

Whether the optimizer should run filesort in such cases is another question.  It does run it in current 10.1, pre-MDEV-8646 10.1, and 10.1-MDEV-8646.   I am not sure if MySQL has fixed this problem. If they did, I assume we don't want to port that fix within the scope of this MDEV."
2710,MDEV-8646,MDEV,Sergei Petrunia,79403,2015-12-27 17:04:21,Pushed a fix for {{derived_opt.test}} There is another problem with {{Distinct;}} not being shown.,8,Pushed a fix for {{derived_opt.test}} There is another problem with {{Distinct;}} not being shown.
2711,MDEV-8646,MDEV,Igor Babaev,79507,2016-01-04 22:23:47,"Sergey,
I pushed several fixes on January 4 2016: pull them please into your local tree.",9,"Sergey,
I pushed several fixes on January 4 2016: pull them please into your local tree."
2712,MDEV-8646,MDEV,Sergei Petrunia,79731,2016-01-11 18:17:01,"Ok, currrent tree fails the following tests:

{noformat}
main.innodb_ext_key 
main.show_explain 
main.information_schema 
main.show_explain_non_select 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.union 
main.analyze_stmt_slow_query_log 
main.openssl_6975 
main.myisam 
main.limit 
main.limit_rows_examined 
main.func_group 
main.myisam_explain_non_select_all 
main.select_found 
main.select_jcl6 
main.subselect_cache 
main.subselect_mat 
main.subselect_mat_cost_bugs 
main.subselect_sj_mat 
main.subselect_no_exists_to_in 
main.subselect_sj_nonmerged 
main.user_var 
main.analyze_format_json 
main.analyze_stmt 
main.delete_returning 
main.derived_view 
main.distinct 
main.events_1
{noformat}

Some of these fail in the base tree also.",10,"Ok, currrent tree fails the following tests:

{noformat}
main.innodb_ext_key 
main.show_explain 
main.information_schema 
main.show_explain_non_select 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.union 
main.analyze_stmt_slow_query_log 
main.openssl_6975 
main.myisam 
main.limit 
main.limit_rows_examined 
main.func_group 
main.myisam_explain_non_select_all 
main.select_found 
main.select_jcl6 
main.subselect_cache 
main.subselect_mat 
main.subselect_mat_cost_bugs 
main.subselect_sj_mat 
main.subselect_no_exists_to_in 
main.subselect_sj_nonmerged 
main.user_var 
main.analyze_format_json 
main.analyze_stmt 
main.delete_returning 
main.derived_view 
main.distinct 
main.events_1
{noformat}

Some of these fail in the base tree also."
2713,MDEV-8646,MDEV,Sergei Petrunia,79732,2016-01-11 18:21:39,"I've investigated the crash in func_group.test.  This patch fixes it:

{noformat}
diff --git a/sql/sql_select.cc b/sql/sql_select.cc
index 1ee63cf..a16f719 100644
--- a/sql/sql_select.cc
+++ b/sql/sql_select.cc
@@ -19298,7 +19298,8 @@ enum_nested_loop_state
 	  {
 	    if (join->do_send_rows)
             {
-	      error=join->result->send_data(*fields) ? 1 : 0;
+	      //psergey-fix: error=join->result->send_data(*fields) ? 1 : 0;
+	      error=join->result->send_data(*fields);
               if (error < 0)
               {
                 /* Duplicate row, don't count */
{noformat}

git history shows that the line
{noformat}
	      error=join->result->send_data(*fields) ? 1 : 0;
{noformat}

was introduced in   
{noformat}
commit 33dc737f07d4a5c4d97a9f05953dc8f7c94f0f94
Author: Igor Babaev <igor@askmonty.org>
Date:   Mon Dec 21 10:27:34 2015 -0800

    MDEV-8646: Refactor the code for working tables
{noformat}

Both the original 10.1 tree and mysql-5.7 have just {{error = }}

The code below, {{if (error < 0)}} apparently does not make sense with the new code. [~igor], what do you think?",11,"I've investigated the crash in func_group.test.  This patch fixes it:

{noformat}
diff --git a/sql/sql_select.cc b/sql/sql_select.cc
index 1ee63cf..a16f719 100644
--- a/sql/sql_select.cc
+++ b/sql/sql_select.cc
@@ -19298,7 +19298,8 @@ enum_nested_loop_state
 	  {
 	    if (join->do_send_rows)
             {
-	      error=join->result->send_data(*fields) ? 1 : 0;
+	      //psergey-fix: error=join->result->send_data(*fields) ? 1 : 0;
+	      error=join->result->send_data(*fields);
               if (error < 0)
               {
                 /* Duplicate row, don't count */
{noformat}

git history shows that the line
{noformat}
	      error=join->result->send_data(*fields) ? 1 : 0;
{noformat}

was introduced in   
{noformat}
commit 33dc737f07d4a5c4d97a9f05953dc8f7c94f0f94
Author: Igor Babaev 
Date:   Mon Dec 21 10:27:34 2015 -0800

    MDEV-8646: Refactor the code for working tables
{noformat}

Both the original 10.1 tree and mysql-5.7 have just {{error = }}

The code below, {{if (error < 0)}} apparently does not make sense with the new code. [~igor], what do you think?"
2714,MDEV-8646,MDEV,Sergei Petrunia,79733,2016-01-11 18:26:00,"I'm also looking why union.test fails an assertion thd->status_var.memory_used != 0.
It fails in this line
{noformat}
EXPLAIN EXTENDED
SELECT * FROM t1 UNION SELECT * FROM t1
  ORDER BY MATCH(a) AGAINST ('+abc' IN BOOLEAN MODE);
{noformat}

And can be fixed as follows:

{noformat}
diff --git a/sql/sql_union.cc b/sql/sql_union.cc
index 65b63c4..f6e4d48 100644
--- a/sql/sql_union.cc
+++ b/sql/sql_union.cc
@@ -1024,9 +1024,9 @@ bool st_select_lex_unit::cleanup()
     JOIN *join;
     if ((join= fake_select_lex->join))
     {
-      join->tables_list= 0;
-      join->table_count= 0;
-      join->top_join_tab_count= 0;
+      //join->tables_list= 0;
+      //join->table_count= 0; //psergey: this is where the problem is!
+      //join->top_join_tab_count= 0;
     }
     error|= fake_select_lex->cleanup();
     /*
{noformat}

I don't have a lot of confidence in this change, but it seems not to introduce any new test failures. I'll continue looking at other failures.",12,"I'm also looking why union.test fails an assertion thd->status_var.memory_used != 0.
It fails in this line
{noformat}
EXPLAIN EXTENDED
SELECT * FROM t1 UNION SELECT * FROM t1
  ORDER BY MATCH(a) AGAINST ('+abc' IN BOOLEAN MODE);
{noformat}

And can be fixed as follows:

{noformat}
diff --git a/sql/sql_union.cc b/sql/sql_union.cc
index 65b63c4..f6e4d48 100644
--- a/sql/sql_union.cc
+++ b/sql/sql_union.cc
@@ -1024,9 +1024,9 @@ bool st_select_lex_unit::cleanup()
     JOIN *join;
     if ((join= fake_select_lex->join))
     {
-      join->tables_list= 0;
-      join->table_count= 0;
-      join->top_join_tab_count= 0;
+      //join->tables_list= 0;
+      //join->table_count= 0; //psergey: this is where the problem is!
+      //join->top_join_tab_count= 0;
     }
     error|= fake_select_lex->cleanup();
     /*
{noformat}

I don't have a lot of confidence in this change, but it seems not to introduce any new test failures. I'll continue looking at other failures."
2715,MDEV-8646,MDEV,Sergei Petrunia,79735,2016-01-11 18:55:21,"Looking at the crash in delete_returning.test

It happens in this query:
{noformat}
DELETE FROM t1 ORDER BY i1 RETURNING ( SELECT i2 FROM t2 );
{noformat}

We crash when executing the subquery in the RETURNING part. We crash,
because DELETE execution first destroys the join from here:

{noformat}
  Breakpoint 2, JOIN::cleanup (this=0x7fff980091d0, full=true) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:11725
  #1  0x0000555555a2f26b in JOIN::destroy (this=0x7fff980091d0) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:3216
  #2  0x0000555555abfd04 in st_select_lex::cleanup (this=0x7fff98007c70) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_union.cc:1165
  #3  0x0000555555abf88c in st_select_lex_unit::cleanup (this=0x7fff98007ff8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_union.cc:1020
  #4  0x0000555555a5f9f9 in free_underlaid_joins (thd=0x555557fd7950, select=0x555557fdbdc0) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:22851
  #5  0x0000555555d7b133 in mysql_delete (thd=0x555557fd7950, table_list=0x7fff98007528, conds=0x0, order_list=0x555557fdbff8, limit=18446744073709551615, options=0, result=0x7fff98008fd8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_delete.cc:494
{noformat}

but after that tries to execute it from here:

{noformat}
  #0  JOIN::exec (this=0x7fff980091d0) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:2987
  #1  0x0000555555cc0341 in subselect_single_select_engine::exec (this=0x7fff98008f70) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item_subselect.cc:3695
  #2  0x0000555555cb7097 in Item_subselect::exec (this=0x7fff98008e30) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item_subselect.cc:668
  #3  0x0000555555cb894d in Item_singlerow_subselect::val_int (this=0x7fff98008e30) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item_subselect.cc:1263
  #4  0x0000555555c38903 in Item::send (this=0x7fff98008e30, protocol=0x555557fd7ee0, buffer=0x7ffff4640a20) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item.cc:6404
  #5  0x0000555555939362 in Protocol::send_result_set_row (this=0x555557fd7ee0, row_items=0x555557fdbed8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/protocol.cc:906
  #6  0x00005555559aec51 in select_send::send_data (this=0x7fff98008fd8, items=...) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_class.cc:2771
  #7  0x0000555555d7b57e in mysql_delete (thd=0x555557fd7950, table_list=0x7fff98007528, conds=0x0, order_list=0x555557fdbff8, limit=18446744073709551615, options=0, result=0x7fff98008fd8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_delete.cc:562
{noformat}

I suppose, before MDEV-8646 code, the subquery was able to survive cleanups?
",13,"Looking at the crash in delete_returning.test

It happens in this query:
{noformat}
DELETE FROM t1 ORDER BY i1 RETURNING ( SELECT i2 FROM t2 );
{noformat}

We crash when executing the subquery in the RETURNING part. We crash,
because DELETE execution first destroys the join from here:

{noformat}
  Breakpoint 2, JOIN::cleanup (this=0x7fff980091d0, full=true) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:11725
  #1  0x0000555555a2f26b in JOIN::destroy (this=0x7fff980091d0) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:3216
  #2  0x0000555555abfd04 in st_select_lex::cleanup (this=0x7fff98007c70) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_union.cc:1165
  #3  0x0000555555abf88c in st_select_lex_unit::cleanup (this=0x7fff98007ff8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_union.cc:1020
  #4  0x0000555555a5f9f9 in free_underlaid_joins (thd=0x555557fd7950, select=0x555557fdbdc0) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:22851
  #5  0x0000555555d7b133 in mysql_delete (thd=0x555557fd7950, table_list=0x7fff98007528, conds=0x0, order_list=0x555557fdbff8, limit=18446744073709551615, options=0, result=0x7fff98008fd8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_delete.cc:494
{noformat}

but after that tries to execute it from here:

{noformat}
  #0  JOIN::exec (this=0x7fff980091d0) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_select.cc:2987
  #1  0x0000555555cc0341 in subselect_single_select_engine::exec (this=0x7fff98008f70) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item_subselect.cc:3695
  #2  0x0000555555cb7097 in Item_subselect::exec (this=0x7fff98008e30) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item_subselect.cc:668
  #3  0x0000555555cb894d in Item_singlerow_subselect::val_int (this=0x7fff98008e30) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item_subselect.cc:1263
  #4  0x0000555555c38903 in Item::send (this=0x7fff98008e30, protocol=0x555557fd7ee0, buffer=0x7ffff4640a20) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/item.cc:6404
  #5  0x0000555555939362 in Protocol::send_result_set_row (this=0x555557fd7ee0, row_items=0x555557fdbed8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/protocol.cc:906
  #6  0x00005555559aec51 in select_send::send_data (this=0x7fff98008fd8, items=...) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_class.cc:2771
  #7  0x0000555555d7b57e in mysql_delete (thd=0x555557fd7950, table_list=0x7fff98007528, conds=0x0, order_list=0x555557fdbff8, limit=18446744073709551615, options=0, result=0x7fff98008fd8) at /home/psergey/dev-git/10.1-mdev8646.fly/sql/sql_delete.cc:562
{noformat}

I suppose, before MDEV-8646 code, the subquery was able to survive cleanups?
"
2716,MDEV-8646,MDEV,Sergei Petrunia,80070,2016-01-20 15:01:38,"Pushed a (very simple) fix for {{analyze*.test}} and {{*json*.test}}. List of failing tests now:

There are now 25 tests failing:
{noformat}
main.innodb_ext_key
main.information_schema
main.union
main.show_explain
main.openssl_6975
main.myisam
main.show_explain_non_select
main.system_mysql_db_fix40123
main.system_mysql_db_fix50030
main.system_mysql_db_fix50117
main.limit
main.limit_rows_examined
main.func_group
main.select_found
main.user_var
main.delete_returning
main.distinct
main.events_1
main.derived_view
{noformat}
",14,"Pushed a (very simple) fix for {{analyze*.test}} and {{*json*.test}}. List of failing tests now:

There are now 25 tests failing:
{noformat}
main.innodb_ext_key
main.information_schema
main.union
main.show_explain
main.openssl_6975
main.myisam
main.show_explain_non_select
main.system_mysql_db_fix40123
main.system_mysql_db_fix50030
main.system_mysql_db_fix50117
main.limit
main.limit_rows_examined
main.func_group
main.select_found
main.user_var
main.delete_returning
main.distinct
main.events_1
main.derived_view
{noformat}
"
2717,MDEV-8646,MDEV,Sergei Petrunia,80400,2016-01-27 15:30:09,"Test failures yesterday:

{noformat}
<igor3> main.stat_tables_par_innodb 
main.show_explain 
main.show_explain_non_select 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.union 
main.delete_returning 
main.events_1
{noformat}

Test failures right now: ""Failed 12/882 tests"", 
{noformat}
main.show_explain 
main.show_explain_non_select 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.openssl_6975 
main.events_1
{noformat}
",15,"Test failures yesterday:

{noformat}
 main.stat_tables_par_innodb 
main.show_explain 
main.show_explain_non_select 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.union 
main.delete_returning 
main.events_1
{noformat}

Test failures right now: ""Failed 12/882 tests"", 
{noformat}
main.show_explain 
main.show_explain_non_select 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.openssl_6975 
main.events_1
{noformat}
"
2718,MDEV-8646,MDEV,Sergei Petrunia,80403,2016-01-27 15:57:02,"Pushed a fix for SHOW EXPLAIN tests. Now:

{noformat}
Completed: Failed 8/881 tests, 99.09% were successful.
{noformat}

{noformat}
Failing test(s): 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.openssl_6975 
main.events_1
{noformat}

That is, all remaining test failures are test failures that were present in the 10.1 tree that this tree was based on.",16,"Pushed a fix for SHOW EXPLAIN tests. Now:

{noformat}
Completed: Failed 8/881 tests, 99.09% were successful.
{noformat}

{noformat}
Failing test(s): 
main.system_mysql_db_fix40123 
main.system_mysql_db_fix50030 
main.system_mysql_db_fix50117 
main.openssl_6975 
main.events_1
{noformat}

That is, all remaining test failures are test failures that were present in the 10.1 tree that this tree was based on."
2719,MDEV-8646,MDEV,Igor Babaev,80743,2016-02-10 00:18:27,Today I pushed consolidated patch for this task into the 10.2-mdev8646 tree.,17,Today I pushed consolidated patch for this task into the 10.2-mdev8646 tree.
2720,MDEV-8646,MDEV,Sergei Petrunia,80752,2016-02-10 12:21:00,"Observations from looking at the buildbot
http://buildbot.askmonty.org/buildbot/grid?category=main&branch=bb-10.2-mdev8646

- show_explain failed on windows. the failure is not always repoducible

- maria.maria fails due to missing ""Distinct"" in EXPLAIN output (1), (2), other
  builds

- Some mroonga tests fail on on sol10-64, not sure what the cause is. (1)

- sequence.group_by Fails because we didn't merge ""Query pushdown"" code
  correctly. EXPLAIN changes from ""Storage engine handles GROUP BY"" to a
  regular plan. (Q: what does EXPLAIN FORMAT=JSON show for such queries?) (1),
  (2), other builds


(1) http://buildbot.askmonty.org/buildbot/builders/sol10-64/builds/4747/steps/test/logs/stdio
(2) http://buildbot.askmonty.org/buildbot/builders/labrador/builds/6605/steps/test/logs/stdio
",18,"Observations from looking at the buildbot
URL

- show_explain failed on windows. the failure is not always repoducible

- maria.maria fails due to missing ""Distinct"" in EXPLAIN output (1), (2), other
  builds

- Some mroonga tests fail on on sol10-64, not sure what the cause is. (1)

- sequence.group_by Fails because we didn't merge ""Query pushdown"" code
  correctly. EXPLAIN changes from ""Storage engine handles GROUP BY"" to a
  regular plan. (Q: what does EXPLAIN FORMAT=JSON show for such queries?) (1),
  (2), other builds


(1) URL
(2) URL
"
2721,MDEV-8646,MDEV,Sergei Petrunia,80803,2016-02-11 14:52:54,"Grepping in the current code for 'original_join_tab', I get:
{noformat}
sql_select.h|1179| JOIN_TAB *original_join_tab;
sql_select.h|1475| original_join_tab= 0;
sql_select.cc|11719| if (original_join_tab)
sql_select.cc|11722| join_tab= original_join_tab;
sql_select.cc|11723| original_join_tab= 0;
{noformat}

This doesn't make any sense (original_join_tab==NULL always, what's the point?)
It did make sense before the merge, because there was also this piece of code:
{noformat}
    /* Remember information about the original join */
    original_join_tab= join_tab;
    original_table_count= table_count;

    /* Set up one join tab to get sorting to work */
    const_tables= 0;
    table_count= 1;
    join_tab= (JOIN_TAB*) thd->calloc(sizeof(JOIN_TAB));
    join_tab[0].table= exec_tmp_table1;
{noformat}

We probably don't need this ""JOIN smashing"" in the post-MDEV-8626 codebase, but then we don't need JOIN::original_join_tab, either.
",19,"Grepping in the current code for 'original_join_tab', I get:
{noformat}
sql_select.h|1179| JOIN_TAB *original_join_tab;
sql_select.h|1475| original_join_tab= 0;
sql_select.cc|11719| if (original_join_tab)
sql_select.cc|11722| join_tab= original_join_tab;
sql_select.cc|11723| original_join_tab= 0;
{noformat}

This doesn't make any sense (original_join_tab==NULL always, what's the point?)
It did make sense before the merge, because there was also this piece of code:
{noformat}
    /* Remember information about the original join */
    original_join_tab= join_tab;
    original_table_count= table_count;

    /* Set up one join tab to get sorting to work */
    const_tables= 0;
    table_count= 1;
    join_tab= (JOIN_TAB*) thd->calloc(sizeof(JOIN_TAB));
    join_tab[0].table= exec_tmp_table1;
{noformat}

We probably don't need this ""JOIN smashing"" in the post-MDEV-8626 codebase, but then we don't need JOIN::original_join_tab, either.
"
2722,MDEV-8646,MDEV,Sergei Petrunia,80804,2016-02-11 15:01:19,"I get this warning:
{noformat}
|| /home/psergey/dev-git/10.2-mdev8646-igor/sql/sql_select.cc: In member function ‘bool JOIN::make_aggr_tables_info()’:
sql_select.cc|2085 col 8| warning: variable ‘materialize_join’ set but not used [-Wunused-but-set-variable]
{noformat}
can materialize_join be removed?",20,"I get this warning:
{noformat}
|| /home/psergey/dev-git/10.2-mdev8646-igor/sql/sql_select.cc: In member function ‘bool JOIN::make_aggr_tables_info()’:
sql_select.cc|2085 col 8| warning: variable ‘materialize_join’ set but not used [-Wunused-but-set-variable]
{noformat}
can materialize_join be removed?"
2723,MDEV-8646,MDEV,Sergei Petrunia,80805,2016-02-11 15:19:36,"Need to also figure out JOIN::do_select_call_count.

In 10.2-main, JOIN::exec_inner() can make multiple do_select calls for  one query execution, and it also sets curr_join->do_select_call_count= 0; at start.

After the merge
- JOIN::exec_inner has only one invocation of do_select().
- We don't reset do_select_call_count when starting join execution.

This is wrong.
",21,"Need to also figure out JOIN::do_select_call_count.

In 10.2-main, JOIN::exec_inner() can make multiple do_select calls for  one query execution, and it also sets curr_join->do_select_call_count= 0; at start.

After the merge
- JOIN::exec_inner has only one invocation of do_select().
- We don't reset do_select_call_count when starting join execution.

This is wrong.
"
2724,MDEV-8646,MDEV,Sergei Petrunia,80808,2016-02-11 16:43:59,"{noformat}
  /*
    For ""Using temporary+Using filesort"" queries, JOIN::join_tab can point to
    either: 
    1. array of join tabs describing how to run the select, or
    2. array of single join tab describing read from the temporary table.

    SHOW EXPLAIN code needs to read/show #1. This is why two next members are
    there for saving it.
  */
  JOIN_TAB *table_access_tabs;
  uint     top_table_access_tabs_count;
{noformat}
These members in class JOIN are probably not needed anymore",22,"{noformat}
  /*
    For ""Using temporary+Using filesort"" queries, JOIN::join_tab can point to
    either: 
    1. array of join tabs describing how to run the select, or
    2. array of single join tab describing read from the temporary table.

    SHOW EXPLAIN code needs to read/show #1. This is why two next members are
    there for saving it.
  */
  JOIN_TAB *table_access_tabs;
  uint     top_table_access_tabs_count;
{noformat}
These members in class JOIN are probably not needed anymore"
2725,MDEV-8646,MDEV,Sergei Petrunia,80811,2016-02-11 17:20:48,"GROUP BY pushdown code requires that a temporary table is created (although with do_not_open=1) even if the query itself doesn't need it.
Example:

{noformat}
  SELECT SUM(1) FROM seq_1_to_10;
{noformat}

This crashes if group_by_handler::table is not set.",23,"GROUP BY pushdown code requires that a temporary table is created (although with do_not_open=1) even if the query itself doesn't need it.
Example:

{noformat}
  SELECT SUM(1) FROM seq_1_to_10;
{noformat}

This crashes if group_by_handler::table is not set."
2726,MDEV-8646,MDEV,Sergei Petrunia,80869,2016-02-13 23:08:28,"Pushed a fix for {{maria.maria}}. 

Buildbot shows that valgrind tests crash on start: 
http://buildbot.askmonty.org/buildbot/builders/work-amd64-valgrind/builds/8348

sql/sql_base.cc:7907 is
{noformat}
  DBUG_PRINT(""enter"", (""ref_pointer_array: %p"", ref_pointer_array));
{noformat}

should be easy to fix",24,"Pushed a fix for {{maria.maria}}. 

Buildbot shows that valgrind tests crash on start: 
URL

sql/sql_base.cc:7907 is
{noformat}
  DBUG_PRINT(""enter"", (""ref_pointer_array: %p"", ref_pointer_array));
{noformat}

should be easy to fix"
2727,MDEV-8646,MDEV,Sergei Petrunia,80870,2016-02-13 23:16:46,Should be fixed now,25,Should be fixed now
2728,MDEV-8646,MDEV,Sergei Petrunia,81979,2016-03-15 21:14:00,Fixed {{sequence.group_by}} test.,26,Fixed {{sequence.group_by}} test.
2729,MDEV-8646,MDEV,Sergei Petrunia,82288,2016-03-27 14:49:30,"A remainder of the old code:
{noformat}
static bool
make_group_fields(JOIN *main_join, JOIN *curr_join)
{noformat}",27,"A remainder of the old code:
{noformat}
static bool
make_group_fields(JOIN *main_join, JOIN *curr_join)
{noformat}"
2730,MDEV-8654,MDEV,Sergey Vojtovich,78209,2015-11-18 18:18:19,"[~serg], please review patch for this task. Did [~monty] agree with this clean-up?",1,"[~serg], please review patch for this task. Did [~monty] agree with this clean-up?"
2731,MDEV-8654,MDEV,Sergei Golubchik,78467,2015-11-24 23:25:17,"I don't think I need to review it, push when you think it's ok.
Yes, he did.",2,"I don't think I need to review it, push when you think it's ok.
Yes, he did."
2732,MDEV-8713,MDEV,Elena Stepanova,75262,2015-09-01 14:55:27,"We already have a similar request, MDEV-8473. But that entry describes two separate issues -- a feature request for new options from 5.6, and an upstream bug https://bugs.mysql.com/bug.php?id=65812. So, I am going to convert this one, MDEV-8713, to a feature request, while MDEV-8473 will remain a bug report. ",1,"We already have a similar request, MDEV-8473. But that entry describes two separate issues -- a feature request for new options from 5.6, and an upstream bug URL So, I am going to convert this one, MDEV-8713, to a feature request, while MDEV-8473 will remain a bug report. "
2733,MDEV-8713,MDEV,Elena Stepanova,75263,2015-09-01 14:55:59,Please note that MDEV-8473 also requested --raw option.,2,Please note that MDEV-8473 also requested --raw option.
2734,MDEV-8713,MDEV,Elena Stepanova,75265,2015-09-01 15:00:57,"I've added 10.1 to the list of 'Fix version(s)' just in case it can somehow be added there, although I highly doubt that, please don't see it as a promise. ",3,"I've added 10.1 to the list of 'Fix version(s)' just in case it can somehow be added there, although I highly doubt that, please don't see it as a promise. "
2735,MDEV-8713,MDEV,Chris Calender,75297,2015-09-02 03:57:59,"Yes, thank you, the --raw option should also be included with this.

--raw

""By default, mysqlbinlog reads binary log files and writes events in text format. The --raw option tells mysqlbinlog to write them in their original binary format. Its use requires that --read-from-remote-server also be used because the files are requested from a server. mysqlbinlog writes one output file for each file read from the server. The --raw option can be used to make a backup of a server's binary log. With the --stop-never option, the backup is “live” because mysqlbinlog stays connected to the server. By default, output files are written in the current directory with the same names as the original log files. Output file names can be modified using the --result-file option. For more information, see Section 4.6.8.3, 'Using mysqlbinlog to Back Up Binary Log Files'. This option was added in MySQL 5.6.0.""

https://dev.mysql.com/doc/refman/5.6/en/mysqlbinlog.html#option_mysqlbinlog_raw",4,"Yes, thank you, the --raw option should also be included with this.

--raw

""By default, mysqlbinlog reads binary log files and writes events in text format. The --raw option tells mysqlbinlog to write them in their original binary format. Its use requires that --read-from-remote-server also be used because the files are requested from a server. mysqlbinlog writes one output file for each file read from the server. The --raw option can be used to make a backup of a server's binary log. With the --stop-never option, the backup is “live” because mysqlbinlog stays connected to the server. By default, output files are written in the current directory with the same names as the original log files. Output file names can be modified using the --result-file option. For more information, see Section 4.6.8.3, 'Using mysqlbinlog to Back Up Binary Log Files'. This option was added in MySQL 5.6.0.""

URL"
2736,MDEV-8713,MDEV,VAROQUI Stephane,78410,2015-11-23 16:49:16,Please note that semi sync  was added to this  feature in WebScaleSQL,5,Please note that semi sync  was added to this  feature in WebScaleSQL
2737,MDEV-8713,MDEV,Alexey Botchkov,78561,2015-11-27 14:51:53,"Patch proposal:
http://lists.askmonty.org/pipermail/commits/2015-November/008692.html",6,"Patch proposal:
URL"
2738,MDEV-8713,MDEV,Alexey Botchkov,81408,2016-02-26 12:50:43,"Patch proposal v2:
http://lists.askmonty.org/pipermail/commits/2016-February/009051.html",7,"Patch proposal v2:
URL"
2739,MDEV-8713,MDEV,Sergei Golubchik,81418,2016-02-26 17:41:09,ok to push,8,ok to push
2740,MDEV-8713,MDEV,Chris Calender,81759,2016-03-03 18:27:20,"Are the only real changes here to mysqlbinlong.cc?

If so, can this be backported to a GA version, say 10.1, and/or even 10.0?",9,"Are the only real changes here to mysqlbinlong.cc?

If so, can this be backported to a GA version, say 10.1, and/or even 10.0?"
2741,MDEV-8713,MDEV,Chris Calender,81760,2016-03-03 18:30:07,"Many thanks for adding this, btw! :)",10,"Many thanks for adding this, btw! :)"
2742,MDEV-8713,MDEV,Alexey Botchkov,81786,2016-03-07 08:14:44,"Chris,
from the technical point of view yes, it can be backported to 10.0.
",11,"Chris,
from the technical point of view yes, it can be backported to 10.0.
"
2743,MDEV-8715,MDEV,Sergey Vojtovich,78240,2015-11-19 14:53:44,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2744,MDEV-8715,MDEV,Sergei Golubchik,78272,2015-11-20 13:24:25,"ok to push.

this is all very straightforward, but the question is ­— does it bring any speedup or it only removes the function from the profile, by spreading the overhead over its numerous callers.",2,"ok to push.

this is all very straightforward, but the question is ­— does it bring any speedup or it only removes the function from the profile, by spreading the overhead over its numerous callers."
2745,MDEV-8715,MDEV,Sergey Vojtovich,78279,2015-11-20 13:43:51,"[~serg], the primary purpose of this task is to prevent usage of inefficient function. As for ""how much it was actually bad"":
- it called pthread_getspecific(), which can't be spread over it's callers
- IIRC according to profiler function call convention was over 50%
- the rest is definitely spread",3,"[~serg], the primary purpose of this task is to prevent usage of inefficient function. As for ""how much it was actually bad"":
- it called pthread_getspecific(), which can't be spread over it's callers
- IIRC according to profiler function call convention was over 50%
- the rest is definitely spread"
2746,MDEV-8716,MDEV,Sergey Vojtovich,78217,2015-11-18 21:55:38,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2747,MDEV-8716,MDEV,Sergei Golubchik,78274,2015-11-20 13:27:48,ok to push,2,ok to push
2748,MDEV-8717,MDEV,Sergey Vojtovich,78207,2015-11-18 17:52:11,"[~serg], please review patch for this bug.",1,"[~serg], please review patch for this bug."
2749,MDEV-8717,MDEV,Sergei Golubchik,78275,2015-11-20 13:29:10,ok to push,2,ok to push
2750,MDEV-8718,MDEV,Sergey Vojtovich,78216,2015-11-18 20:57:00,"[~serg], please review patch for this task. Note: I had to add 2 current_thd calls to avoid adding thd parameter to Item::val_str().",1,"[~serg], please review patch for this task. Note: I had to add 2 current_thd calls to avoid adding thd parameter to Item::val_str()."
2751,MDEV-8718,MDEV,Sergei Golubchik,78276,2015-11-20 13:33:13,ok to push,2,ok to push
2752,MDEV-8719,MDEV,Sergey Vojtovich,78206,2015-11-18 17:23:44,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2753,MDEV-8719,MDEV,Sergei Golubchik,78277,2015-11-20 13:33:24,ok to push,2,ok to push
2754,MDEV-8739,MDEV,Elena Stepanova,88784,2016-11-28 13:42:53,Re-opening because in fact the suites were removed again from the default suites a few days after adding -- they were way too unstable for the main tree. Please enable them in a staging/development tree first and make sure they're stable there.,1,Re-opening because in fact the suites were removed again from the default suites a few days after adding -- they were way too unstable for the main tree. Please enable them in a staging/development tree first and make sure they're stable there.
2755,MDEV-8739,MDEV,Elena Stepanova,93558,2017-03-28 15:41:43,Assigning to [~sachin.setiya.007] because he's been working on it recently.,2,Assigning to [~sachin.setiya.007] because he's been working on it recently.
2756,MDEV-8739,MDEV,Sachin Setiya,110423,2018-05-02 11:22:07,we added galera suite in defaults suite long time back,3,we added galera suite in defaults suite long time back
2757,MDEV-8842,MDEV,Alexey Botchkov,76507,2015-10-06 14:35:47,proposed patch: http://lists.askmonty.org/pipermail/commits/2015-October/008494.html,1,proposed patch: URL
2758,MDEV-8931,MDEV,Oleksandr Byelkin,82464,2016-04-03 18:03:04,"Changes are in the branch 10.2-MDEV-8931  or:


revision-id: 4c51152d9f43e271e17a2bc266f5887ce092c00f (mariadb-10.1.8-187-g4c51152)
parent(s): 69b5c4a4220c8fa98bbca8b3f935b2a3735e19ac
committer: Oleksandr Byelkin
timestamp: 2016-03-28 19:19:54 +0200
message:

MDEV-8931: (server part of) session state tracking

initial commit to test

---

revision-id: d4f228771adde02262061702950babd2539a99f5 (mariadb-10.1.8-188-gd4f2287)
parent(s): 4c51152d9f43e271e17a2bc266f5887ce092c00f
committer: Oleksandr Byelkin
timestamp: 2016-04-03 19:58:38 +0200
message:

MDEV-8931: (server part of) session state tracking

System variables tracking

---",1,"Changes are in the branch 10.2-MDEV-8931  or:


revision-id: 4c51152d9f43e271e17a2bc266f5887ce092c00f (mariadb-10.1.8-187-g4c51152)
parent(s): 69b5c4a4220c8fa98bbca8b3f935b2a3735e19ac
committer: Oleksandr Byelkin
timestamp: 2016-03-28 19:19:54 +0200
message:

MDEV-8931: (server part of) session state tracking

initial commit to test

---

revision-id: d4f228771adde02262061702950babd2539a99f5 (mariadb-10.1.8-188-gd4f2287)
parent(s): 4c51152d9f43e271e17a2bc266f5887ce092c00f
committer: Oleksandr Byelkin
timestamp: 2016-04-03 19:58:38 +0200
message:

MDEV-8931: (server part of) session state tracking

System variables tracking

---"
2759,MDEV-8931,MDEV,Oleksandr Byelkin,83462,2016-05-17 07:21:01,"Left tasks:
 1) make variable testable on unload with plugin
 2) SET STATEMENT workaround
 3) transactions info
",2,"Left tasks:
 1) make variable testable on unload with plugin
 2) SET STATEMENT workaround
 3) transactions info
"
2760,MDEV-8931,MDEV,Oleksandr Byelkin,83463,2016-05-17 07:22:10,"1) use variable bookmarks, add something like bookmarks to global plugin variables...",3,"1) use variable bookmarks, add something like bookmarks to global plugin variables..."
2761,MDEV-8931,MDEV,Oleksandr Byelkin,83711,2016-05-26 17:17:23,"revision-id: d2b496f035f0789ec3c2aa6d49d3c9ae41693e47 (mariadb-10.2.0-44-gd2b496f)
parent(s): 4cf74d8976c18e3a871ce3696f20ab6a08a77ba2
committer: Oleksandr Byelkin
timestamp: 2016-05-26 19:15:53 +0200
message:

MDEV-8931: (server part of) session state tracking

System variables tracking

---",4,"revision-id: d2b496f035f0789ec3c2aa6d49d3c9ae41693e47 (mariadb-10.2.0-44-gd2b496f)
parent(s): 4cf74d8976c18e3a871ce3696f20ab6a08a77ba2
committer: Oleksandr Byelkin
timestamp: 2016-05-26 19:15:53 +0200
message:

MDEV-8931: (server part of) session state tracking

System variables tracking

---"
2762,MDEV-8931,MDEV,Oleksandr Byelkin,83750,2016-05-28 07:07:07,2 commits are in 10.2-MDEV-8931,5,2 commits are in 10.2-MDEV-8931
2763,MDEV-8931,MDEV,Oleksandr Byelkin,83790,2016-05-30 19:26:10,Transaction tracking added and pushed (force),6,Transaction tracking added and pushed (force)
2764,MDEV-8931,MDEV,Oleksandr Byelkin,83791,2016-05-30 19:27:45,session_tracker_trx_state from 5.7 passes as is but can not be added while we have no client support.,7,session_tracker_trx_state from 5.7 passes as is but can not be added while we have no client support.
2765,MDEV-8931,MDEV,Sergei Golubchik,83829,2016-05-31 08:25:07,You can coordinate this with MDEV-9293 and [~wlad],8,You can coordinate this with MDEV-9293 and [~wlad]
2766,MDEV-8931,MDEV,Oleksandr Byelkin,85464,2016-08-09 13:51:39,"The postreview work also pushed

revision-id: db9822df31917c3d0162322a77bcd4433bfb705e (mariadb-10.2.1-15-gdb9822d)
parent(s): b5e0f70e04cec625c99760e5bfb28bb31bc972ed
committer: Oleksandr Byelkin
timestamp: 2016-08-09 15:49:30 +0200
message:

MDEV-8931: (server part of) session state tracking

Postreview fixes.
New MySQL tests fixes.

---",9,"The postreview work also pushed

revision-id: db9822df31917c3d0162322a77bcd4433bfb705e (mariadb-10.2.1-15-gdb9822d)
parent(s): b5e0f70e04cec625c99760e5bfb28bb31bc972ed
committer: Oleksandr Byelkin
timestamp: 2016-08-09 15:49:30 +0200
message:

MDEV-8931: (server part of) session state tracking

Postreview fixes.
New MySQL tests fixes.

---"
2767,MDEV-8931,MDEV,Oleksandr Byelkin,86017,2016-08-31 16:19:52,"-I do not close it to do not forget add tests when Connector/C will be added to server tree. But the patch is pushed.-

I think it will be better to create separate issue for this task.",10,"-I do not close it to do not forget add tests when Connector/C will be added to server tree. But the patch is pushed.-

I think it will be better to create separate issue for this task."
2768,MDEV-8971,MDEV,Nirbhay Choubey,78300,2015-11-21 02:10:46,A syntax error in scripts/wsrep_sst_common.sh needs to be fixed.,1,A syntax error in scripts/wsrep_sst_common.sh needs to be fixed.
2769,MDEV-8976,MDEV,Sergei Golubchik,77614,2015-11-02 12:17:33,I'd rather build and install libjudydebian1 on the power8 builder,1,I'd rather build and install libjudydebian1 on the power8 builder
2770,MDEV-8976,MDEV,Daniel Bartholomew,77881,2015-11-10 18:53:09,[~serg]: So you're saying we build the library from source on that box and then include it in our packages?,2,[~serg]: So you're saying we build the library from source on that box and then include it in our packages?
2771,MDEV-8976,MDEV,Kolbe Kegel,77893,2015-11-10 23:03:59,I think taking on responsibility for building and maintaining libjudy-dev on our own makes no sense. I vote that we skip OQGraph on POWER8 and remove the libjudy-dev dependency (MDEV-9111).,3,I think taking on responsibility for building and maintaining libjudy-dev on our own makes no sense. I vote that we skip OQGraph on POWER8 and remove the libjudy-dev dependency (MDEV-9111).
2772,MDEV-8976,MDEV,Daniel Black,77896,2015-11-11 01:55:51,"(untested) you could potentially install a libjudy-dev package from Debian to build leave end users having to do the same.

Hopefully ubuntu distro maintainers can make it happen.",4,"(untested) you could potentially install a libjudy-dev package from Debian to build leave end users having to do the same.

Hopefully ubuntu distro maintainers can make it happen."
2773,MDEV-8976,MDEV,Sergey Vojtovich,77915,2015-11-11 14:54:34,"[~danblack], it worked, thanks for your idea.

First successful build:
http://buildbot.askmonty.org/buildbot/builders/p8-trusty-deb/builds/648

Build fixed by installing libjudy-dev_1.0.5-4_ppc64el.deb, libjudydebian1_1.0.5-4_ppc64el.deb of Debian Jessie. However oqgraph package won't be functional without installing these packages, which should be acceptable.",5,"[~danblack], it worked, thanks for your idea.

First successful build:
URL

Build fixed by installing libjudy-dev_1.0.5-4_ppc64el.deb, libjudydebian1_1.0.5-4_ppc64el.deb of Debian Jessie. However oqgraph package won't be functional without installing these packages, which should be acceptable."
2774,MDEV-8980,MDEV,Sergei Golubchik,77050,2015-10-23 12:37:41,"[~Kentoku], please, see the above — I will try to merge the latest Mroonga and Spider into 10.0, so if you have something not pushed yet, you might want to push it now.

[~bertrandop], same, about Connect engine (Connect-10.0) — I'll soon merge it for 10.0.22 release.",1,"[~Kentoku], please, see the above — I will try to merge the latest Mroonga and Spider into 10.0, so if you have something not pushed yet, you might want to push it now.

[~bertrandop], same, about Connect engine (Connect-10.0) — I'll soon merge it for 10.0.22 release."
2775,MDEV-9022,MDEV,Sergei Golubchik,80341,2016-01-26 11:07:51,"Looks good to me. But I've seen in guthub that [~wlad] had comments.
Please push when you're both ok with the code.",1,"Looks good to me. But I've seen in guthub that [~wlad] had comments.
Please push when you're both ok with the code."
2776,MDEV-9058,MDEV,Oleksandr Byelkin,79644,2016-01-08 15:58:25,"I'll send you a dif also it can be reviewed on bb-sanja-10.2 on github, commits:

commit b92189b69d68a560240692d84155095607337ce5
Author: Oleksandr Byelkin <sanja@mariadb.com>
Date:   Fri Jan 8 12:55:34 2016 +0100

    Check ability to accept multi-results.

commit 3704e74a38d74cd33e6889c81f0abcf8e4515ddb
Author: Oleksandr Byelkin <sanja@mariadb.com>
Date:   Thu Jan 7 19:06:38 2016 +0100

    Last statement fix after talk to wlad.

commit 4474a9c0b5d657723bf01124c0689ad6de03fecf
Author: Oleksandr Byelkin <sanja@mariadb.com>
Date:   Thu Jan 7 16:00:02 2016 +0100

    MDEV-9058: protocol: COM_MULTI command (part 3)
    
    Support of ""previuousely used statement ID"".
    All IDs with highest bit ON reserved for special use.

commit 6c48449d90dc94bc58af0ef7adcf019a11014193
Author: Oleksandr Byelkin <sanja@mariadb.com>
Date:   Tue Jan 5 20:44:45 2016 +0100

    MDEV-9058: protocol: COM_MULTI command (part 2)
    
    simple COM_MULTI support (no prepared statements chain yet).

commit b78a53f18337c1c457b99e0ddb2da709c289fab2
Author: Oleksandr Byelkin <sanja@mariadb.com>
Date:   Thu Nov 26 11:21:56 2015 +0100

    MDEV-9058: protocol: COM_MULTI command (part 1)
    
    Adding a command from the end of avaliable commands numering space (255)

",1,"I'll send you a dif also it can be reviewed on bb-sanja-10.2 on github, commits:

commit b92189b69d68a560240692d84155095607337ce5
Author: Oleksandr Byelkin 
Date:   Fri Jan 8 12:55:34 2016 +0100

    Check ability to accept multi-results.

commit 3704e74a38d74cd33e6889c81f0abcf8e4515ddb
Author: Oleksandr Byelkin 
Date:   Thu Jan 7 19:06:38 2016 +0100

    Last statement fix after talk to wlad.

commit 4474a9c0b5d657723bf01124c0689ad6de03fecf
Author: Oleksandr Byelkin 
Date:   Thu Jan 7 16:00:02 2016 +0100

    MDEV-9058: protocol: COM_MULTI command (part 3)
    
    Support of ""previuousely used statement ID"".
    All IDs with highest bit ON reserved for special use.

commit 6c48449d90dc94bc58af0ef7adcf019a11014193
Author: Oleksandr Byelkin 
Date:   Tue Jan 5 20:44:45 2016 +0100

    MDEV-9058: protocol: COM_MULTI command (part 2)
    
    simple COM_MULTI support (no prepared statements chain yet).

commit b78a53f18337c1c457b99e0ddb2da709c289fab2
Author: Oleksandr Byelkin 
Date:   Thu Nov 26 11:21:56 2015 +0100

    MDEV-9058: protocol: COM_MULTI command (part 1)
    
    Adding a command from the end of avaliable commands numering space (255)

"
2777,MDEV-9058,MDEV,Oleksandr Byelkin,79823,2016-01-13 20:56:38,"Everything mentioned in the review answered on replies to review e-mail or fixed in this revisions:

revision-id: ec9b1f6f6d95d9a004d38dac4927fbd7512b7752 (mariadb-10.1.8-75-gec9b1f6)
parent(s): b92189b69d68a560240692d84155095607337ce5
committer: Oleksandr Byelkin
timestamp: 2016-01-13 19:36:00 +0100
message:

MDEV-9058: post-review changes.

---

revision-id: 65d0c9f8b8ea24ab6a5443bcde01880b081215c5 (mariadb-10.1.8-76-g65d0c9f)
parent(s): ec9b1f6f6d95d9a004d38dac4927fbd7512b7752
committer: Oleksandr Byelkin
timestamp: 2016-01-13 19:52:27 +0100
message:

MDEV-9058: usage of flags explained

---",2,"Everything mentioned in the review answered on replies to review e-mail or fixed in this revisions:

revision-id: ec9b1f6f6d95d9a004d38dac4927fbd7512b7752 (mariadb-10.1.8-75-gec9b1f6)
parent(s): b92189b69d68a560240692d84155095607337ce5
committer: Oleksandr Byelkin
timestamp: 2016-01-13 19:36:00 +0100
message:

MDEV-9058: post-review changes.

---

revision-id: 65d0c9f8b8ea24ab6a5443bcde01880b081215c5 (mariadb-10.1.8-76-g65d0c9f)
parent(s): ec9b1f6f6d95d9a004d38dac4927fbd7512b7752
committer: Oleksandr Byelkin
timestamp: 2016-01-13 19:52:27 +0100
message:

MDEV-9058: usage of flags explained

---"
2778,MDEV-9114,MDEV,Diego Dupin,84680,2016-06-29 19:16:47,"I know it may be late, but it would be good to reserve now a byte, for a future flag.
This flag would permit to ask for returning only one OKPacket. 
Its like insert multiple rows using a single SQL INSERT statement. (INSERT INTO table VALUES ('a'), ('b'), ('c') ) => only one OKPacket is returned to driver. 
Parsing those packet take a lot of time for batch.

In JDBC, batches (bulk operation) are done using a PreparedStatment object. You can indicate if  [resulting insert ids are needed or not|https://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#prepareStatement(java.lang.String,%20int)].
Parsing all returning OKPacket take a lot of time (nearly half the time when local). 

Reserve one byte now will permit to implement this in the future.
",1,"I know it may be late, but it would be good to reserve now a byte, for a future flag.
This flag would permit to ask for returning only one OKPacket. 
Its like insert multiple rows using a single SQL INSERT statement. (INSERT INTO table VALUES ('a'), ('b'), ('c') ) => only one OKPacket is returned to driver. 
Parsing those packet take a lot of time for batch.

In JDBC, batches (bulk operation) are done using a PreparedStatment object. You can indicate if  [resulting insert ids are needed or not|URL
Parsing all returning OKPacket take a lot of time (nearly half the time when local). 

Reserve one byte now will permit to implement this in the future.
"
2779,MDEV-9114,MDEV,Oleksandr Byelkin,84723,2016-07-02 14:24:59," I've checked and found that array binding generate only one OK for whole command (independently of number of parameters) and it is also true for ""not so optimized"" for bulk execution commands (checked on UPDATE also)",2," I've checked and found that array binding generate only one OK for whole command (independently of number of parameters) and it is also true for ""not so optimized"" for bulk execution commands (checked on UPDATE also)"
2780,MDEV-9114,MDEV,Oleksandr Byelkin,85565,2016-08-14 14:10:23,"Probably effort to tie default value during parameter binding was not correct. It is virtually impossible for non optimized command where we first set default, then bind...",3,"Probably effort to tie default value during parameter binding was not correct. It is virtually impossible for non optimized command where we first set default, then bind..."
2781,MDEV-9114,MDEV,Diego Dupin,86152,2016-09-06 14:38:19,"Following discussion : 
What would be great is that client indicate if he want all autoincrementIds or not. 

There is actually one byte flag to indicate to use cursor (description indicate cursor, there won't be cursor here)
This byte can be used to indicate that client want all autoincrementIds, like 0x01 = send all auto increment ids. 

Result could be an extended Okpacket replacing :
	
{noformat}
	int<lenenc>	last_insert_id	last insert-id
{noformat}

by 

{noformat}
	if (query is COM_STMT_BULK_EXECUTE response ) {
	  int(lenenc)	number of insert id 
	  n int(lenenc) insert id
	} else {
	  int(lenenc)	last insert id
	}
{noformat}

This would permit to handle for exemple java prepare batch  : [Connection.prepareStatement(String sql,int autoGeneratedKeys)|https://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#prepareStatement(java.lang.String,%20int)] with autoGeneratedKeys can have Statement.RETURN_GENERATED_KEYS or Statement.NO_GENERATED_KEYS.
the fastest possible way
",4,"Following discussion : 
What would be great is that client indicate if he want all autoincrementIds or not. 

There is actually one byte flag to indicate to use cursor (description indicate cursor, there won't be cursor here)
This byte can be used to indicate that client want all autoincrementIds, like 0x01 = send all auto increment ids. 

Result could be an extended Okpacket replacing :
	
{noformat}
	int	last_insert_id	last insert-id
{noformat}

by 

{noformat}
	if (query is COM_STMT_BULK_EXECUTE response ) {
	  int(lenenc)	number of insert id 
	  n int(lenenc) insert id
	} else {
	  int(lenenc)	last insert id
	}
{noformat}

This would permit to handle for exemple java prepare batch  : [Connection.prepareStatement(String sql,int autoGeneratedKeys)|URL with autoGeneratedKeys can have Statement.RETURN_GENERATED_KEYS or Statement.NO_GENERATED_KEYS.
the fastest possible way
"
2782,MDEV-9117,MDEV,Sergei Golubchik,78295,2015-11-20 20:40:36,"also redefine as
{code}
#define CLIENT_LONG_PASSWORD 0
#define CLIENT_MYSQL 1
{code}",1,"also redefine as
{code}
#define CLIENT_LONG_PASSWORD 0
#define CLIENT_MYSQL 1
{code}"
2783,MDEV-9117,MDEV,Oleksandr Byelkin,78302,2015-11-21 14:31:49,"I decide do not touch CLIENT_LONG_PASSWORD because it used by mysqlbinlog (for example) without 41 protocol

revision-id: 2606b8744404f352b44c85ee23d41b12cddc3470 (mariadb-10.1.8-51-g2606b87)
parent(s): 81e4ce5e31ba0753d7acfab28bc6c3d83bfad1c6
committer: Oleksandr Byelkin
timestamp: 2015-11-20 18:23:52 +0100
message:

MDEV-9117: Client Server capability negotiation for MariaDB specific functionality

New capability flags space.
Moved progress flag
fixed mysql_client_test.c

---",2,"I decide do not touch CLIENT_LONG_PASSWORD because it used by mysqlbinlog (for example) without 41 protocol

revision-id: 2606b8744404f352b44c85ee23d41b12cddc3470 (mariadb-10.1.8-51-g2606b87)
parent(s): 81e4ce5e31ba0753d7acfab28bc6c3d83bfad1c6
committer: Oleksandr Byelkin
timestamp: 2015-11-20 18:23:52 +0100
message:

MDEV-9117: Client Server capability negotiation for MariaDB specific functionality

New capability flags space.
Moved progress flag
fixed mysql_client_test.c

---"
2784,MDEV-9117,MDEV,Oleksandr Byelkin,78532,2015-11-26 11:57:41,"revision-id: cae19452eec1d3be47fa04759dd2dda6d061543e (mariadb-10.1.8-69-gcae1945)
parent(s): 55e67c3e344317c6f349f5391e5d117ec51ae062
committer: Oleksandr Byelkin
timestamp: 2015-11-25 18:09:39 +0100
message:

MDEV-9117: Client Server capability negotiation for MariaDB specific functionality

New capability flags space.
Removed old progress flag, added new one.

---",3,"revision-id: cae19452eec1d3be47fa04759dd2dda6d061543e (mariadb-10.1.8-69-gcae1945)
parent(s): 55e67c3e344317c6f349f5391e5d117ec51ae062
committer: Oleksandr Byelkin
timestamp: 2015-11-25 18:09:39 +0100
message:

MDEV-9117: Client Server capability negotiation for MariaDB specific functionality

New capability flags space.
Removed old progress flag, added new one.

---"
2785,MDEV-9117,MDEV,Oleksandr Byelkin,80431,2016-01-28 16:34:37,revision-id: 5dd6eb45c418f61c749a7884d7b3e13d62f3bfb9 (mariadb-10.1.8-113-g5dd6eb4),4,revision-id: 5dd6eb45c418f61c749a7884d7b3e13d62f3bfb9 (mariadb-10.1.8-113-g5dd6eb4)
2786,MDEV-9117,MDEV,Oleksandr Byelkin,80492,2016-02-02 00:10:21,"revision-id: 910576544ea45c5c0446b5da46b4ac0550621787 (mariadb-10.1.8-113-g9105765)
parent(s): 14d79a5e99f9992cabb7489203989ccdaacadb78
committer: Oleksandr Byelkin
timestamp: 2016-02-01 23:07:11 +0100
message:

MDEV-9117: Client Server capability negotiation for MariaDB specific functionality

New capability flags space.
Removed old progress flag, added new one.

---",5,"revision-id: 910576544ea45c5c0446b5da46b4ac0550621787 (mariadb-10.1.8-113-g9105765)
parent(s): 14d79a5e99f9992cabb7489203989ccdaacadb78
committer: Oleksandr Byelkin
timestamp: 2016-02-01 23:07:11 +0100
message:

MDEV-9117: Client Server capability negotiation for MariaDB specific functionality

New capability flags space.
Removed old progress flag, added new one.

---"
2787,MDEV-9117,MDEV,Sergei Golubchik,80497,2016-02-02 02:29:20,ok to push,6,ok to push
2788,MDEV-9118,MDEV,Oleksandr Byelkin,79157,2015-12-16 20:35:34,"I am not sure if BLOB should be ignored if it is explicitly mentioned in the fields list. Also I am not sure if warning should be issued.

revision-id: c9e76fec68476b7987cc6ce0a0a74d9d492854e0 (mariadb-10.1.9-20-gc9e76fe)
parent(s): 953d5680a3c050273a8f29253f7386984679f92b
committer: Oleksandr Byelkin
timestamp: 2015-12-16 19:33:41 +0100
message:

MDEV-9118 ANALYZE TABLE for Engine independent status fetchs blob/text columns without use

Do not include BLOB fields by default.

---",1,"I am not sure if BLOB should be ignored if it is explicitly mentioned in the fields list. Also I am not sure if warning should be issued.

revision-id: c9e76fec68476b7987cc6ce0a0a74d9d492854e0 (mariadb-10.1.9-20-gc9e76fe)
parent(s): 953d5680a3c050273a8f29253f7386984679f92b
committer: Oleksandr Byelkin
timestamp: 2015-12-16 19:33:41 +0100
message:

MDEV-9118 ANALYZE TABLE for Engine independent status fetchs blob/text columns without use

Do not include BLOB fields by default.

---"
2789,MDEV-9118,MDEV,Sergei Petrunia,79161,2015-12-16 21:21:37,"I think: if the blob column was explicitly mentioned, but we are not collecting statistics, a warning should be issued.

Now, to the question of what to do when ANALYZE explicitly mentions a blob column:
Looking at the patch:

{noformat}
@@ -1276,7 +1275,6 @@ test	t1	c	aaaa	dddddddd	0.1250	6.6571	7.0000	0	NULL	NULL
 test	t1	d	1989-03-12	1999-07-23	0.1500	3.0000	8.5000	0	NULL	NULL
 test	t1	e	0.01	0.112	0.2250	8.0000	6.2000	0	NULL	NULL
 test	t1	f	1	5	0.2000	1.0000	6.4000	0	NULL	NULL
-test	t1	b	NULL	NULL	0.2000	17.1250	NULL	NULL	NULL	NULL
{noformat}

column names are:
{noformat}
db_name	table_name	column_name	min_value	max_value	nulls_ratio	avg_length	avg_frequency	hist_size	hist_type	histogram
{noformat}

One can see that  {{min_value}} and {{max_value}} are not saved. {{nulls_ratio}} and {{avg_length}} are saved.
",2,"I think: if the blob column was explicitly mentioned, but we are not collecting statistics, a warning should be issued.

Now, to the question of what to do when ANALYZE explicitly mentions a blob column:
Looking at the patch:

{noformat}
@@ -1276,7 +1275,6 @@ test	t1	c	aaaa	dddddddd	0.1250	6.6571	7.0000	0	NULL	NULL
 test	t1	d	1989-03-12	1999-07-23	0.1500	3.0000	8.5000	0	NULL	NULL
 test	t1	e	0.01	0.112	0.2250	8.0000	6.2000	0	NULL	NULL
 test	t1	f	1	5	0.2000	1.0000	6.4000	0	NULL	NULL
-test	t1	b	NULL	NULL	0.2000	17.1250	NULL	NULL	NULL	NULL
{noformat}

column names are:
{noformat}
db_name	table_name	column_name	min_value	max_value	nulls_ratio	avg_length	avg_frequency	hist_size	hist_type	histogram
{noformat}

One can see that  {{min_value}} and {{max_value}} are not saved. {{nulls_ratio}} and {{avg_length}} are saved.
"
2790,MDEV-9118,MDEV,Sergei Petrunia,79164,2015-12-16 21:33:59,Trying to check whether {{nulls_ratio}} can be used...,3,Trying to check whether {{nulls_ratio}} can be used...
2791,MDEV-9118,MDEV,Sergei Petrunia,79182,2015-12-17 15:06:30,"Yes, it can be:

{noformat}
create table t1 (pk int primary key, a varchar(32), b blob); 
... # fill the table with data, 40% rows have a=b=NULL, other 60% are non-null values.
{noformat}

{noformat}
MariaDB [test]> explain extended select * from t1 where a is null;
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    40.00 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
{noformat}

{noformat}
MariaDB [test]> explain extended select * from t1 where b is null;
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    40.00 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
{noformat}

{noformat}
MariaDB [test]> explain extended select * from t1 where b is not null;
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    60.00 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
{noformat}

Debugging an {{ANALYZE TABLE t1 PERSISTENT FOR ALL}}, I see that column {{b}} doesn't have a Unique object.  So, does it make sense to collect some stats for blobs after all?",4,"Yes, it can be:

{noformat}
create table t1 (pk int primary key, a varchar(32), b blob); 
... # fill the table with data, 40% rows have a=b=NULL, other 60% are non-null values.
{noformat}

{noformat}
MariaDB [test]> explain extended select * from t1 where a is null;
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    40.00 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
{noformat}

{noformat}
MariaDB [test]> explain extended select * from t1 where b is null;
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    40.00 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
{noformat}

{noformat}
MariaDB [test]> explain extended select * from t1 where b is not null;
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
| id   | select_type | table | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
|    1 | SIMPLE      | t1    | ALL  | NULL          | NULL | NULL    | NULL | 1000 |    60.00 | Using where |
+------+-------------+-------+------+---------------+------+---------+------+------+----------+-------------+
{noformat}

Debugging an {{ANALYZE TABLE t1 PERSISTENT FOR ALL}}, I see that column {{b}} doesn't have a Unique object.  So, does it make sense to collect some stats for blobs after all?"
2792,MDEV-9118,MDEV,Sergei Petrunia,79183,2015-12-17 15:27:36,".. but if that is enabled, one can also try producing selectivities for other columns, as well.

[~sanja], I think that for now the code should never collect stats for a blob column.  if the column is specified explicitly, emit a warning  ""Engine-independent statistics are not collected for column %s"".",5,".. but if that is enabled, one can also try producing selectivities for other columns, as well.

[~sanja], I think that for now the code should never collect stats for a blob column.  if the column is specified explicitly, emit a warning  ""Engine-independent statistics are not collected for column %s""."
2793,MDEV-9118,MDEV,Oleksandr Byelkin,79195,2015-12-18 12:27:21,"revision-id: aa67fa247f9758b8361a79fb560ad44ef02397d3 (mariadb-10.1.9-20-gaa67fa2)
parent(s): 953d5680a3c050273a8f29253f7386984679f92b
committer: Oleksandr Byelkin
timestamp: 2015-12-18 11:26:20 +0100
message:

MDEV-9118 ANALYZE TABLE for Engine independent status fetchs blob/text columns without use

Do not include BLOB fields by default.

---",6,"revision-id: aa67fa247f9758b8361a79fb560ad44ef02397d3 (mariadb-10.1.9-20-gaa67fa2)
parent(s): 953d5680a3c050273a8f29253f7386984679f92b
committer: Oleksandr Byelkin
timestamp: 2015-12-18 11:26:20 +0100
message:

MDEV-9118 ANALYZE TABLE for Engine independent status fetchs blob/text columns without use

Do not include BLOB fields by default.

---"
2794,MDEV-9118,MDEV,Oleksandr Byelkin,79197,2015-12-18 13:25:08,"revision-id: 59fcd7ff2315d007045eb987da5f21abbea6f6f1 (mariadb-10.1.9-20-g59fcd7f)
parent(s): 953d5680a3c050273a8f29253f7386984679f92b
committer: Oleksandr Byelkin
timestamp: 2015-12-18 12:23:45 +0100
message:

MDEV-9118 ANALYZE TABLE for Engine independent status fetchs blob/text columns without use

Do not include BLOB fields by default.

---",7,"revision-id: 59fcd7ff2315d007045eb987da5f21abbea6f6f1 (mariadb-10.1.9-20-g59fcd7f)
parent(s): 953d5680a3c050273a8f29253f7386984679f92b
committer: Oleksandr Byelkin
timestamp: 2015-12-18 12:23:45 +0100
message:

MDEV-9118 ANALYZE TABLE for Engine independent status fetchs blob/text columns without use

Do not include BLOB fields by default.

---"
2795,MDEV-9118,MDEV,Sergei Petrunia,79328,2015-12-22 13:42:59,Review feedback sent over email.,8,Review feedback sent over email.
2796,MDEV-9143,MDEV,Alexey Botchkov,83846,2016-05-31 12:30:02,"First patch proposal:
http://lists.askmonty.org/pipermail/commits/2016-May/009406.html",1,"First patch proposal:
URL"
2797,MDEV-9143,MDEV,Alexey Botchkov,85141,2016-07-25 09:54:23,"patch v2.0
http://lists.askmonty.org/pipermail/commits/2016-July/009555.html",2,"patch v2.0
URL"
2798,MDEV-9143,MDEV,Alexey Botchkov,86407,2016-09-13 12:10:38,"patch v3.0
http://lists.askmonty.org/pipermail/commits/2016-September/009854.html",3,"patch v3.0
URL"
2799,MDEV-9143,MDEV,Alexey Botchkov,86975,2016-10-01 10:42:05,"unittest added.
http://lists.askmonty.org/pipermail/commits/2016-October/009959.html",4,"unittest added.
URL"
2800,MDEV-9143,MDEV,Alexey Botchkov,86976,2016-10-01 10:42:37,"See my reply to your review, and the new unittest commit.",5,"See my reply to your review, and the new unittest commit."
2801,MDEV-9143,MDEV,Alexey Botchkov,88342,2016-11-15 13:23:59,"Implementing the rest of the functions.
http://lists.askmonty.org/pipermail/commits/2016-November/010087.html",6,"Implementing the rest of the functions.
URL"
2802,MDEV-9143,MDEV,Steven WdV,99882,2017-09-09 19:58:17,"A have a question (sorry if this is the wrong place for it): why do the JSON functions not return compacted JSON?
For example: {code:sql}SELECT JSON_ARRAY(1,2,3){code} returns {{\[1, 2, 3]}} instead of {{\[1,2,3]}} (without spaces). Of course we can compact it with {{JSON_COMPACT}}, but it would probably more efficiënt to not put spaces in in the first place, since you rarely want loose JSON to appear in a database.",7,"A have a question (sorry if this is the wrong place for it): why do the JSON functions not return compacted JSON?
For example: {code:sql}SELECT JSON_ARRAY(1,2,3){code} returns {{\[1, 2, 3]}} instead of {{\[1,2,3]}} (without spaces). Of course we can compact it with {{JSON_COMPACT}}, but it would probably more efficiënt to not put spaces in in the first place, since you rarely want loose JSON to appear in a database."
2803,MDEV-9172,MDEV,Sergey Vojtovich,78554,2015-11-27 12:02:37,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2804,MDEV-9172,MDEV,Sergey Vojtovich,78640,2015-12-01 13:16:15,"A rough benchmark was performed on host running bb slave. In a nutshell: patched code seem to be 2-3% faster.

Benchmark:
{noformat}
SELECT NOW()+0 INTO @start_time;
disable_query_log;
disable_result_log;
let $i= 5000000;
while ($i)
{
  SELECT 1;
  dec $i;
}
enable_query_log;
enable_result_log;
SELECT NOW()-@start_time;
{noformat}

Benchmark results:
{noformat}
vanilla
-------
215, 220, 215, 218, 215

optimized
---------
212, 212, 211, 212, 212
{noformat}",2,"A rough benchmark was performed on host running bb slave. In a nutshell: patched code seem to be 2-3% faster.

Benchmark:
{noformat}
SELECT NOW()+0 INTO @start_time;
disable_query_log;
disable_result_log;
let $i= 5000000;
while ($i)
{
  SELECT 1;
  dec $i;
}
enable_query_log;
enable_result_log;
SELECT NOW()-@start_time;
{noformat}

Benchmark results:
{noformat}
vanilla
-------
215, 220, 215, 218, 215

optimized
---------
212, 212, 211, 212, 212
{noformat}"
2805,MDEV-9172,MDEV,Sergei Golubchik,79009,2015-12-09 14:50:49,ok to push,3,ok to push
2806,MDEV-9185,MDEV,Otto Kekäläinen,81640,2016-02-29 20:46:05,"@serg Are you going to merge this PR and then maybe mangle a debug version of it, and apply to all branches?",1,"@serg Are you going to merge this PR and then maybe mangle a debug version of it, and apply to all branches?"
2807,MDEV-9185,MDEV,Otto Kekäläinen,82619,2016-04-09 18:46:06,"As we have so much failing tests, I think this issue should be priority and help all developers and contibutors to have running and checking test results more deeply embedded in their workflows. Ping [~serg] [~ratzpo] [~monty]

Also, since I made this Travis integration 6 months Travis-ci.org has lifted their timeout limit from 50 minutes to 90 minutes, so there would be room for the tests to do more. Feel free to adapt this as much as you like.
",2,"As we have so much failing tests, I think this issue should be priority and help all developers and contibutors to have running and checking test results more deeply embedded in their workflows. Ping [~serg] [~ratzpo] [~monty]

Also, since I made this Travis integration 6 months Travis-ci.org has lifted their timeout limit from 50 minutes to 90 minutes, so there would be room for the tests to do more. Feel free to adapt this as much as you like.
"
2808,MDEV-9185,MDEV,Daniel Black,85752,2016-08-23 01:49:08,Step 1 & 2 from https://docs.travis-ci.com/user/getting-started/ are required to get travis actually building on commit/PR.,3,Step 1 & 2 from URL are required to get travis actually building on commit/PR.
2809,MDEV-9185,MDEV,Otto Kekäläinen,85753,2016-08-23 07:10:48,[~svoj] [~elenst] [~danblack] You can see at https://github.com/MariaDB/server/commits/10.2 the Travis-CI at work. Pressing on the red X you can view the Travis-CI log why it fails.,4,[~svoj] [~elenst] [~danblack] You can see at URL the Travis-CI at work. Pressing on the red X you can view the Travis-CI log why it fails.
2810,MDEV-9185,MDEV,Sergey Vojtovich,85761,2016-08-23 14:16:04,"So, is there still anything we should do?",5,"So, is there still anything we should do?"
2811,MDEV-9185,MDEV,Otto Kekäläinen,85763,2016-08-23 14:29:15,"Yes, please look into the build log if you can fix the failure?

The point of Travis-CI is to spot regerssions and point out the exact commit (in any branch, anywhere) where the regression occured. That function is not fulfilled if the test is already broken. The page https://travis-ci.org/MariaDB/server should be all green when contributors start branching off the 10.2 branch to do their new feature/fix.

Checkout https://travis-ci.org/MariaDB/mariadb-connector-j how it is all green. (Good work [~diego dupin]!)",6,"Yes, please look into the build log if you can fix the failure?

The point of Travis-CI is to spot regerssions and point out the exact commit (in any branch, anywhere) where the regression occured. That function is not fulfilled if the test is already broken. The page URL should be all green when contributors start branching off the 10.2 branch to do their new feature/fix.

Checkout URL how it is all green. (Good work [~diego dupin]!)"
2812,MDEV-9185,MDEV,Diego Dupin,85771,2016-08-23 16:11:16,"Since dec. 15, there is no IPv6 support on Travis (one of the remaining error is because of that)",7,"Since dec. 15, there is no IPv6 support on Travis (one of the remaining error is because of that)"
2813,MDEV-9185,MDEV,Daniel Black,85791,2016-08-23 22:22:20,my bad. was expecting to see it in 10.0 / 10.1 however it was only merged in 10.2. no problem.,8,my bad. was expecting to see it in 10.0 / 10.1 however it was only merged in 10.2. no problem.
2814,MDEV-9185,MDEV,Daniel Black,86127,2016-09-05 22:41:13,"https://github.com/MariaDB/server/pull/223 contains the IPv6 fix.

https://github.com/MariaDB/server/commit/3889b19f5414f689c8db98b81a19425629c98faa#diff-17368d0ec7d33cf4abea84c04d5be7b8 caused the IPv6 to return true if the sockaddr_in6 function existed.

The reworked implementation does a full bind to the port as it is used in the tests.

Test results of pull request, https://travis-ci.org/MariaDB/server/builds/157746279, shows IPv6 as being disabled on Travis.",9,"URL contains the IPv6 fix.

URL caused the IPv6 to return true if the sockaddr_in6 function existed.

The reworked implementation does a full bind to the port as it is used in the tests.

Test results of pull request, URL shows IPv6 as being disabled on Travis."
2815,MDEV-9185,MDEV,Sergey Vojtovich,86147,2016-09-06 12:52:11,IPV6 issue to be fixed by MDEV-10751,10,IPV6 issue to be fixed by MDEV-10751
2816,MDEV-9197,MDEV,Galina Shalygina,84347,2016-06-17 16:20:23,"+Detailed description of the task.+

The following optimization of the queries that use non­mergeable views and/or derived tables will be implemented.

A condition that depends only on the columns of the view/derived table is extracted from the
WHERE condition of the given query and pushed into the the subquery that produces the rows of the view/derived table.

Generally such condition can be pushed into the HAVING clause of the subquery.
But sometimes a new condition that depends only on grouping columns can be extracted from it and pushed into the WHERE clause of the subquery.

Let's consider a view defined through a grouping query:

_create view v as select a, avg(b) from t1 group by a;_

Let's use this view in a query:

_select * from v, t where v.a in (const1, const2) and t.a=v.a;_

Apparently this query could be very fast, especially if there were an index on _t1.a_.
Unfortunately now in MariaDB/MySQL the query is slow because any view defined by a grouping query is first materialized in a temporary table.

If before materialization we could push the condition _v.a in (const1, const2)_ into the view and materialized the result of the query:

_select a, avg(b) from t1 group by a having a in (const1, const2);_

materialization would become much faster.

At some conditions it makes sense to push _a in (const1, const2)_ even further into the WHERE clause:

_select a, avg(b) from t1 where a in (const1, const2) group by a;_

(E.g. when there is an index on _t1.a_)
The above is a simple case. In a general case we'll consider separable and non­separable conditions that can be pushed into materialized view.

1. Separable conditions

These are conjunctive conditions that depend only on view columns:
Example:

_select * from v,t where P1(v) AND P2(v,t);_

Here _P1(v)_ is a predicate depending only on the columns of _v_, while _P2(v,t)_ is a predicate depending on the columns of _v_ and _t_.

Apparently _P1(v)_ can be separated from the WHERE clause of the main query and moved to the HAVING clause of the view. (In some cases it can be moved to the WHERE clause of the view).

The case with separable conditions is pretty easy to implement.

2. Non­separable conditions

In a more general case there are no conjunctive conditions depending only on the columns of _v_, but still some restrictive condition depending only on _v_ can be extracted from the WHERE clause.
Example:

_select * from v,t_
  _where (P1(v) AND P2(v,t) OR P3(v) AND P4(v,t)) AND P(v,t);_

Here the condition _(P1(v) OR P3(v))_ is implied by the WHERE condition and can be pushed into the view. Yet it's not a conjunctive condition so it cannot be separated from the WHERE condition and moved into the HAVING condition of the view.

In this case we have to build clones of the items for _P1(v)_ and _P3(v)_.

3. Building item clones

In a general case building a clone for an item is quite a big task (E.g. when the item contains a subquery).
But for simple predicates and functions (like inequality, addition) it's not so difficult if we use copy constructors.

4. Using equalities

We could write the first query in the following equivalent form:

_select * from v, t where t.a in (const1, const2) and t.a=v.a;_

The condition _t.a in (const1, const2)_ can be pushed into the view if we take into account the conjunctive equality _t.a=v.a_.

When searching for pushable condition we should take into account existing equality predicates.
",1,"+Detailed description of the task.+

The following optimization of the queries that use non­mergeable views and/or derived tables will be implemented.

A condition that depends only on the columns of the view/derived table is extracted from the
WHERE condition of the given query and pushed into the the subquery that produces the rows of the view/derived table.

Generally such condition can be pushed into the HAVING clause of the subquery.
But sometimes a new condition that depends only on grouping columns can be extracted from it and pushed into the WHERE clause of the subquery.

Let's consider a view defined through a grouping query:

_create view v as select a, avg(b) from t1 group by a;_

Let's use this view in a query:

_select * from v, t where v.a in (const1, const2) and t.a=v.a;_

Apparently this query could be very fast, especially if there were an index on _t1.a_.
Unfortunately now in MariaDB/MySQL the query is slow because any view defined by a grouping query is first materialized in a temporary table.

If before materialization we could push the condition _v.a in (const1, const2)_ into the view and materialized the result of the query:

_select a, avg(b) from t1 group by a having a in (const1, const2);_

materialization would become much faster.

At some conditions it makes sense to push _a in (const1, const2)_ even further into the WHERE clause:

_select a, avg(b) from t1 where a in (const1, const2) group by a;_

(E.g. when there is an index on _t1.a_)
The above is a simple case. In a general case we'll consider separable and non­separable conditions that can be pushed into materialized view.

1. Separable conditions

These are conjunctive conditions that depend only on view columns:
Example:

_select * from v,t where P1(v) AND P2(v,t);_

Here _P1(v)_ is a predicate depending only on the columns of _v_, while _P2(v,t)_ is a predicate depending on the columns of _v_ and _t_.

Apparently _P1(v)_ can be separated from the WHERE clause of the main query and moved to the HAVING clause of the view. (In some cases it can be moved to the WHERE clause of the view).

The case with separable conditions is pretty easy to implement.

2. Non­separable conditions

In a more general case there are no conjunctive conditions depending only on the columns of _v_, but still some restrictive condition depending only on _v_ can be extracted from the WHERE clause.
Example:

_select * from v,t_
  _where (P1(v) AND P2(v,t) OR P3(v) AND P4(v,t)) AND P(v,t);_

Here the condition _(P1(v) OR P3(v))_ is implied by the WHERE condition and can be pushed into the view. Yet it's not a conjunctive condition so it cannot be separated from the WHERE condition and moved into the HAVING condition of the view.

In this case we have to build clones of the items for _P1(v)_ and _P3(v)_.

3. Building item clones

In a general case building a clone for an item is quite a big task (E.g. when the item contains a subquery).
But for simple predicates and functions (like inequality, addition) it's not so difficult if we use copy constructors.

4. Using equalities

We could write the first query in the following equivalent form:

_select * from v, t where t.a in (const1, const2) and t.a=v.a;_

The condition _t.a in (const1, const2)_ can be pushed into the view if we take into account the conjunctive equality _t.a=v.a_.

When searching for pushable condition we should take into account existing equality predicates.
"
2817,MDEV-9197,MDEV,Galina Shalygina,84557,2016-06-24 22:53:03,"The state of the development by June 24 2016:

( see the development tree here https://github.com/shagalla/server/tree/10.2-mdev9197 )

1. The build_clone method was implemented for simple exressions (items)
2. Conditions depended directly only on the columns of a materialized view now pushed into the having clause of the query specifying the view.
3. Conditions depended directly only on the groupinf columns of a materialized view now pushed into the where clause of the query specifying the view.

Table t1:

{noformat}
select * from t1;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    1 |   24 |    6 |
|    6 |    7 |   11 |
|    3 |    5 |   19 |
|    8 |    2 |    2 |
+------+------+------+
{noformat}


Table t2:

{noformat}
select * from t2;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    2 |    7 |   16 |
|    8 |    4 |    5 |
|    2 |    3 |   19 |
|   11 |    1 |    9 |
+------+------+------+
{noformat}


View v1:

{noformat}
create or replace view v1 as select a, round(avg(b)) b, max(c) c from t2 group by a;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    2 |    5 |   19 |
|    8 |    4 |    5 |
|   11 |    1 |    9 |
+------+------+------+
{noformat}


Queries:

1.
{noformat}
select * from v1, t1 where (t1.a>2) and (v1.b<3);
+------+------+------+------+------+------+
| a    | b    | c    | a    | b    | c    |
+------+------+------+------+------+------+
|   11 |    1 |    9 |    6 |    7 |   11 |
|   11 |    1 |    9 |    3 |    5 |   19 |
|   11 |    1 |    9 |    8 |    2 |    2 |
+------+------+------+------+------+------+
{noformat}


{noformat}
explain format=json select * from v1, t1 where (t1.a>2) and (v1.b<3);
| {
  ""query_block"": {
    ""select_id"": 1,
    ""table"": {
      ""table_name"": ""<derived2>"",
      ""access_type"": ""ALL"",
      ""rows"": 4,
      ""filtered"": 100,
      ""materialized"": {
        ""query_block"": {
          ""select_id"": 2,
          ""having_condition"": ""(b < 3)"",
          ""filesort"": {
            ""sort_key"": ""t2.a"",
            ""temporary_table"": {
              ""table"": {
                ""table_name"": ""t2"",
                ""access_type"": ""ALL"",
                ""rows"": 4,
                ""filtered"": 100
              }
            }
          }
        }
      }
    },
    ""block-nl-join"": {
      ""table"": {
        ""table_name"": ""t1"",
        ""access_type"": ""ALL"",
        ""rows"": 4,
        ""filtered"": 100,
        ""attached_condition"": ""(t1.a > 2)""
      },
      ""buffer_type"": ""flat"",
      ""buffer_size"": ""256Kb"",
      ""join_type"": ""BNL""
    }
  }
} |
{noformat}


2. 
{noformat}
select * from v1, t1 where (t1.a>2) and (v1.a<3);
+------+------+------+------+------+------+
| a    | b    | c    | a    | b    | c    |
+------+------+------+------+------+------+
|    2 |    5 |   19 |    6 |    7 |   11 |
|    2 |    5 |   19 |    3 |    5 |   19 |
|    2 |    5 |   19 |    8 |    2 |    2 |
+------+------+------+------+------+------+
{noformat}


{noformat}
explain format=json select * from v1, t1 where (t1.a>2) and (v1.a<3);
| {
  ""query_block"": {
    ""select_id"": 1,
    ""table"": {
      ""table_name"": ""<derived2>"",
      ""access_type"": ""ALL"",
      ""rows"": 4,
      ""filtered"": 100,
      ""materialized"": {
        ""query_block"": {
          ""select_id"": 2,
          ""filesort"": {
            ""sort_key"": ""t2.a"",
            ""temporary_table"": {
              ""table"": {
                ""table_name"": ""t2"",
                ""access_type"": ""ALL"",
                ""rows"": 4,
                ""filtered"": 100,
                ""attached_condition"": ""(t2.a < 3)""
              }
            }
          }
        }
      }
    },
    ""block-nl-join"": {
      ""table"": {
        ""table_name"": ""t1"",
        ""access_type"": ""ALL"",
        ""rows"": 4,
        ""filtered"": 100,
        ""attached_condition"": ""(t1.a > 2)""
      },
      ""buffer_type"": ""flat"",
      ""buffer_size"": ""256Kb"",
      ""join_type"": ""BNL""
    }
  }
} |
{noformat}",2,"The state of the development by June 24 2016:

( see the development tree here URL )

1. The build_clone method was implemented for simple exressions (items)
2. Conditions depended directly only on the columns of a materialized view now pushed into the having clause of the query specifying the view.
3. Conditions depended directly only on the groupinf columns of a materialized view now pushed into the where clause of the query specifying the view.

Table t1:

{noformat}
select * from t1;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    1 |   24 |    6 |
|    6 |    7 |   11 |
|    3 |    5 |   19 |
|    8 |    2 |    2 |
+------+------+------+
{noformat}


Table t2:

{noformat}
select * from t2;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    2 |    7 |   16 |
|    8 |    4 |    5 |
|    2 |    3 |   19 |
|   11 |    1 |    9 |
+------+------+------+
{noformat}


View v1:

{noformat}
create or replace view v1 as select a, round(avg(b)) b, max(c) c from t2 group by a;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    2 |    5 |   19 |
|    8 |    4 |    5 |
|   11 |    1 |    9 |
+------+------+------+
{noformat}


Queries:

1.
{noformat}
select * from v1, t1 where (t1.a>2) and (v1.b<3);
+------+------+------+------+------+------+
| a    | b    | c    | a    | b    | c    |
+------+------+------+------+------+------+
|   11 |    1 |    9 |    6 |    7 |   11 |
|   11 |    1 |    9 |    3 |    5 |   19 |
|   11 |    1 |    9 |    8 |    2 |    2 |
+------+------+------+------+------+------+
{noformat}


{noformat}
explain format=json select * from v1, t1 where (t1.a>2) and (v1.b<3);
| {
  ""query_block"": {
    ""select_id"": 1,
    ""table"": {
      ""table_name"": """",
      ""access_type"": ""ALL"",
      ""rows"": 4,
      ""filtered"": 100,
      ""materialized"": {
        ""query_block"": {
          ""select_id"": 2,
          ""having_condition"": ""(b < 3)"",
          ""filesort"": {
            ""sort_key"": ""t2.a"",
            ""temporary_table"": {
              ""table"": {
                ""table_name"": ""t2"",
                ""access_type"": ""ALL"",
                ""rows"": 4,
                ""filtered"": 100
              }
            }
          }
        }
      }
    },
    ""block-nl-join"": {
      ""table"": {
        ""table_name"": ""t1"",
        ""access_type"": ""ALL"",
        ""rows"": 4,
        ""filtered"": 100,
        ""attached_condition"": ""(t1.a > 2)""
      },
      ""buffer_type"": ""flat"",
      ""buffer_size"": ""256Kb"",
      ""join_type"": ""BNL""
    }
  }
} |
{noformat}


2. 
{noformat}
select * from v1, t1 where (t1.a>2) and (v1.a<3);
+------+------+------+------+------+------+
| a    | b    | c    | a    | b    | c    |
+------+------+------+------+------+------+
|    2 |    5 |   19 |    6 |    7 |   11 |
|    2 |    5 |   19 |    3 |    5 |   19 |
|    2 |    5 |   19 |    8 |    2 |    2 |
+------+------+------+------+------+------+
{noformat}


{noformat}
explain format=json select * from v1, t1 where (t1.a>2) and (v1.a<3);
| {
  ""query_block"": {
    ""select_id"": 1,
    ""table"": {
      ""table_name"": """",
      ""access_type"": ""ALL"",
      ""rows"": 4,
      ""filtered"": 100,
      ""materialized"": {
        ""query_block"": {
          ""select_id"": 2,
          ""filesort"": {
            ""sort_key"": ""t2.a"",
            ""temporary_table"": {
              ""table"": {
                ""table_name"": ""t2"",
                ""access_type"": ""ALL"",
                ""rows"": 4,
                ""filtered"": 100,
                ""attached_condition"": ""(t2.a < 3)""
              }
            }
          }
        }
      }
    },
    ""block-nl-join"": {
      ""table"": {
        ""table_name"": ""t1"",
        ""access_type"": ""ALL"",
        ""rows"": 4,
        ""filtered"": 100,
        ""attached_condition"": ""(t1.a > 2)""
      },
      ""buffer_type"": ""flat"",
      ""buffer_size"": ""256Kb"",
      ""join_type"": ""BNL""
    }
  }
} |
{noformat}"
2818,MDEV-9197,MDEV,Galina Shalygina,86356,2016-09-09 18:37:29,"+What's I've done for final evaluation:+

_-building item clones;_
  Methods: virtual build_clone, virtual get_copy.

_-pushing conditions into HAVING;_
  Methods:
    1. TABLE_LIST::check_pushable_cond_for_table:
      Mark subformulas of a condition unusable for the condition pushed into table.
    2. TABLE_LIST::build_pushable_cond_for_table:
      Build condition extractable from the given one depended only on this table.

_-pushing conditions into WHERE;_
  Methods:
    1. st_select_lex::check_cond_extraction_for_grouping_fields:
      For a condition check possibility of exraction a formula over grouping fields.
    2. st_select_lex::extract_cond_for_grouping_fields:
      Build condition extractable from the given one.

_-pushing conditions into embedded derived tables;_
_-using equalities to extract pushable conditions;_
_-implementation of the case when derived table with UNION is used;_
_-tests:  derived_cond_pushdown.test;_
_-comments to each new method;_

The code is ready to be merged into MariaDB 10.2.

*Here is the list of commits on github:*

https://github.com/MariaDB/server/compare/10.2...shagalla:10.2-mdev9197

*And here is the commit with the consolidated patch:*

https://github.com/MariaDB/server/compare/10.2...shagalla:10.2-mdev9197-cons",3,"+What's I've done for final evaluation:+

_-building item clones;_
  Methods: virtual build_clone, virtual get_copy.

_-pushing conditions into HAVING;_
  Methods:
    1. TABLE_LIST::check_pushable_cond_for_table:
      Mark subformulas of a condition unusable for the condition pushed into table.
    2. TABLE_LIST::build_pushable_cond_for_table:
      Build condition extractable from the given one depended only on this table.

_-pushing conditions into WHERE;_
  Methods:
    1. st_select_lex::check_cond_extraction_for_grouping_fields:
      For a condition check possibility of exraction a formula over grouping fields.
    2. st_select_lex::extract_cond_for_grouping_fields:
      Build condition extractable from the given one.

_-pushing conditions into embedded derived tables;_
_-using equalities to extract pushable conditions;_
_-implementation of the case when derived table with UNION is used;_
_-tests:  derived_cond_pushdown.test;_
_-comments to each new method;_

The code is ready to be merged into MariaDB 10.2.

*Here is the list of commits on github:*

URL

*And here is the commit with the consolidated patch:*

URL"
2819,MDEV-9197,MDEV,Galina Shalygina,86357,2016-09-09 18:45:11,The consolidated patch was submitted as a contribution for MariaDB server on 08-23-2016.,4,The consolidated patch was submitted as a contribution for MariaDB server on 08-23-2016.
2820,MDEV-9197,MDEV,Igor Babaev,86359,2016-09-09 21:06:06,The code for this task has been pushed into 10.2.2.,5,The code for this task has been pushed into 10.2.2.
2821,MDEV-9197,MDEV,Igor Babaev,86827,2016-09-26 16:46:35,The feature was pushed into the 10.2 tree and will appear in the 10.2.2 release.,6,The feature was pushed into the 10.2 tree and will appear in the 10.2.2 release.
2822,MDEV-9197,MDEV,Krishnadas,87046,2016-10-04 04:30:34,DBS test results  [^MDEV-9197.zip] ,7,DBS test results  [^MDEV-9197.zip] 
2823,MDEV-9252,MDEV,Phil Sweeney,79144,2015-12-16 11:26:02,Thanks for doing the TokuDB merge!,1,Thanks for doing the TokuDB merge!
2824,MDEV-9255,MDEV,Sergei Golubchik,91473,2017-02-06 12:17:30,"SQL standard {{INFORMATION_SCHEMA.COLUMNS}} table has {{IS_GENERATED}} and {{GENERATED_EXPRESSION}} columns. Other tables might have something too, this needs to be checked.",1,"SQL standard {{INFORMATION_SCHEMA.COLUMNS}} table has {{IS_GENERATED}} and {{GENERATED_EXPRESSION}} columns. Other tables might have something too, this needs to be checked."
2825,MDEV-9255,MDEV,Alexey Botchkov,93282,2017-03-21 22:01:34,"Patch proposal:
http://lists.askmonty.org/pipermail/commits/2017-March/010904.html",2,"Patch proposal:
URL"
2826,MDEV-9255,MDEV,Alexey Botchkov,93513,2017-03-27 14:05:03,http://lists.askmonty.org/pipermail/commits/2017-March/010929.html,3,URL
2827,MDEV-9267,MDEV,Vladislav Vaintroub,82527,2016-04-05 11:46:09,"Following tests currently fail :

main.mysql_locale_posix : (charset = auto ) http://buildbot.askmonty.org/buildbot/builders/kvm-bintar-centos5-amd64/builds/2532/steps/test/logs/stdio
main.openssl_1 : (different error msg) http://buildbot.askmonty.org/buildbot/builders/kvm-bintar-centos5-amd64/builds/2532/steps/test/logs/stdio
main.mysql_client_test(_nonblock) : http://buildbot.askmonty.org/buildbot/builders/kvm-bintar-quantal-x86/builds/3736/steps/test/logs/stdio
main.ssl-big(mysqltest failed provided no output ) :http://buildbot.askmonty.org/buildbot/builders/kvm-fulltest/builds/5636/steps/test_2/logs/stdio
funcs_1.myisam_views-big : http://buildbot.askmonty.org/buildbot/builders/kvm-fulltest/builds/5636/steps/test_2/logs/stdio
failed compile (solaris) : http://buildbot.askmonty.org/buildbot/builders/sol10-64/builds/4934/steps/compile/logs/stdio
main.userstat : http://buildbot.askmonty.org/buildbot/builders/win32-debug2/builds/9241/steps/test/logs/stdio
main.ssl_8k_key main.non_blocking_api main.openssl_1 main.ssl main.ssl_compress main.ssl_timeout : http://buildbot.askmonty.org/buildbot/builders/win32-debug-azure/builds/529/steps/test/logs/stdio",1,"Following tests currently fail :

main.mysql_locale_posix : (charset = auto ) URL
main.openssl_1 : (different error msg) URL
main.mysql_client_test(_nonblock) : URL
main.ssl-big(mysqltest failed provided no output ) :URL
funcs_1.myisam_views-big : URL
failed compile (solaris) : URL
main.userstat : URL
main.ssl_8k_key main.non_blocking_api main.openssl_1 main.ssl main.ssl_compress main.ssl_timeout : URL"
2828,MDEV-9267,MDEV,Vladislav Vaintroub,83471,2016-05-17 13:35:15,"Closing, since this part of bringing connector c to server is already done (in bb-10.2-connector-c-integ tree) Continuation of the connector c integraton is done in MDEV-9293",2,"Closing, since this part of bringing connector c to server is already done (in bb-10.2-connector-c-integ tree) Continuation of the connector c integraton is done in MDEV-9293"
2829,MDEV-9267,MDEV,Vladislav Vaintroub,83472,2016-05-17 13:36:32,implemented in in connector c integration tree.,3,implemented in in connector c integration tree.
2830,MDEV-9267,MDEV,Otto Kekäläinen,120539,2018-12-10 11:52:09,"For the record, in my opinion the correct solution to this would have been to add libmariadb3 into the debian/control file (where all other build dependencies are listed as well) instead of going with a bundled libmariadb3 as now done.

Or at least if using a bundled version of libmariadb3, then at least not publish the packages for it for others to consume, but rather let others consume the real original libmariadb3 built from MariaDB Connector C sources.

I am currently in the midst of thinking what to do in Debian official. I will probably not start publishing libmariadb3 from the server source package like upstream did.",4,"For the record, in my opinion the correct solution to this would have been to add libmariadb3 into the debian/control file (where all other build dependencies are listed as well) instead of going with a bundled libmariadb3 as now done.

Or at least if using a bundled version of libmariadb3, then at least not publish the packages for it for others to consume, but rather let others consume the real original libmariadb3 built from MariaDB Connector C sources.

I am currently in the midst of thinking what to do in Debian official. I will probably not start publishing libmariadb3 from the server source package like upstream did."
2831,MDEV-9438,MDEV,Alexey Botchkov,80715,2016-02-09 03:32:24,http://lists.askmonty.org/pipermail/commits/2016-February/008978.html,1,URL
2832,MDEV-9526,MDEV,Sergei Petrunia,80740,2016-02-09 21:57:02,"Some info about window frames:

There are two kinds, ROWS-based and RANGE-based.

ROWS-based frame counts physical rows. {{CURRENT ROW}} means the current row,  {{$n PRECEDING}} or {{$n FOLLOWING}} means {{$n}} preceding or following rows in the partition.

RANGE-based frame counts values. {{CURRENT ROW}} means current row, as well as all rows that sort together with the current row. ",1,"Some info about window frames:

There are two kinds, ROWS-based and RANGE-based.

ROWS-based frame counts physical rows. {{CURRENT ROW}} means the current row,  {{$n PRECEDING}} or {{$n FOLLOWING}} means {{$n}} preceding or following rows in the partition.

RANGE-based frame counts values. {{CURRENT ROW}} means current row, as well as all rows that sort together with the current row. "
2833,MDEV-9526,MDEV,Sergei Petrunia,81028,2016-02-17 18:06:31,"Working on getting COUNT() to work on ROWS-based frames.

COUNT() is a simple aggregate function that supports value removal.

Using this : https://gist.github.com/spetrunia/518c6ff06bd413ab202d for cursor cloning.
",2,"Working on getting COUNT() to work on ROWS-based frames.

COUNT() is a simple aggregate function that supports value removal.

Using this : URL for cursor cloning.
"
2834,MDEV-9526,MDEV,Sergei Petrunia,81029,2016-02-17 18:11:24,"On the call yesterday, it was discussed that we only need to check for partition bound when advancing the first cursor (this is always the  frame start cursor).
The question is, how to synchronize that with other cursors? If the cursor is ""CURRENT ROW"", it's trival. ""ROWS $n FOLLOWING"" is always $n rows ahead, 
""ROWS UNBOUNDED FOLLOWING"" immediately moves to the frame end and stays there.

Maybe, we could count number of rows in the partition. The first cursor that reaches partition end sets the number.  Other cursors can only check if #rows_seen_in_partition == total_rows_in_partition.",3,"On the call yesterday, it was discussed that we only need to check for partition bound when advancing the first cursor (this is always the  frame start cursor).
The question is, how to synchronize that with other cursors? If the cursor is ""CURRENT ROW"", it's trival. ""ROWS $n FOLLOWING"" is always $n rows ahead, 
""ROWS UNBOUNDED FOLLOWING"" immediately moves to the frame end and stays there.

Maybe, we could count number of rows in the partition. The first cursor that reaches partition end sets the number.  Other cursors can only check if #rows_seen_in_partition == total_rows_in_partition."
2835,MDEV-9651,MDEV,Sergey Vojtovich,81649,2016-03-01 06:26:44,"[~serg], please review patch for this task.",1,"[~serg], please review patch for this task."
2836,MDEV-9658,MDEV,Mark Callaghan,87131,2016-10-06 12:53:41,Does this mean it is coming to 10.2?,1,Does this mean it is coming to 10.2?
2837,MDEV-9658,MDEV,Sergei Petrunia,87152,2016-10-06 22:11:50,"[~mdcallag], yes, that is the plan.",2,"[~mdcallag], yes, that is the plan."
2838,MDEV-9658,MDEV,Richard Bensley,87183,2016-10-07 12:50:53,"Exciting!

For anyone else that is new to this thread; Callaghan and Co. have some myrocks [docs|https://github.com/facebook/mysql-5.6/wiki/Getting-Started-with-MyRocks] here:
https://github.com/facebook/mysql-5.6/wiki/Getting-Started-with-MyRocks,
and some [deep-dive-slides|http://www.slideshare.net/matsunobu/myrocks-deep-dive] from April.

MariaDB and TokuDB DBA's like myself are probably more interested in the hot backup tool!
https://github.com/facebook/mysql-5.6/wiki/Physical-backup-with-myrocks_hotbackup

Will rocks ever become compatible with the Online DDL implementation in MariaDB?",3,"Exciting!

For anyone else that is new to this thread; Callaghan and Co. have some myrocks [docs|URL here:
URL
and some [deep-dive-slides|URL from April.

MariaDB and TokuDB DBA's like myself are probably more interested in the hot backup tool!
URL

Will rocks ever become compatible with the Online DDL implementation in MariaDB?"
2839,MDEV-9658,MDEV,Sergei Golubchik,87205,2016-10-09 09:30:31,"Not all these required server changes might be needed for MariaDB. For example (I just picked one commit at random) per-database uuids were obviously created for MySQL GTID implementation, and might be not needed for MariaDB GTID implementation.",4,"Not all these required server changes might be needed for MariaDB. For example (I just picked one commit at random) per-database uuids were obviously created for MySQL GTID implementation, and might be not needed for MariaDB GTID implementation."
2840,MDEV-9658,MDEV,Kristian Nielsen,87221,2016-10-10 08:48:40,"Mail thread about support for optimistic parallel replication:

https://lists.launchpad.net/maria-developers/msg09989.html",5,"Mail thread about support for optimistic parallel replication:

URL"
2841,MDEV-9658,MDEV,Sergei Petrunia,87757,2016-10-25 14:48:36,"Status update: 
* the code in 10.2-mariarocks tree now compiles.
* a LOT of things related to replication and synchronization with binlog are still disabled (under #ifdef MARIAROCKS_NOT_YET) and are not expected to work yet.
* There are rocksdb MTR testcases that pass, but most testcases fail still",6,"Status update: 
* the code in 10.2-mariarocks tree now compiles.
* a LOT of things related to replication and synchronization with binlog are still disabled (under #ifdef MARIAROCKS_NOT_YET) and are not expected to work yet.
* There are rocksdb MTR testcases that pass, but most testcases fail still"
2842,MDEV-9658,MDEV,Sergei Petrunia,87760,2016-10-25 16:48:15,"With the 'rocksdb' test suite I currently get:
{noformat}
Completed: Failed 134/200 tests, 33.00% were successful.
{noformat}
",7,"With the 'rocksdb' test suite I currently get:
{noformat}
Completed: Failed 134/200 tests, 33.00% were successful.
{noformat}
"
2843,MDEV-9658,MDEV,Mark Callaghan,87761,2016-10-25 16:54:14,"What is the magic to compile it? Using ubuntu 16.04

cmake was:
prefix=$1
cmake \
      -DBUILD_CONFIG=mysql_release \
      -DCMAKE_BUILD_TYPE=RelWithDebInfo \
      -DCMAKE_INSTALL_PREFIX:PATH=$prefix \
      -DWITH_ZLIB=""bundled"" \
      -DMYSQL_DATADIR=""${prefix}/data"" \
      -DMYSQL_UNIX_ADDR=""${prefix}/var/mysql.sock"" \
      -DMYSQL_USER=""mysql"" \
      -DENABLED_LOCAL_INFILE=1 \
      -DMYSQL_MAINTAINER_MODE=1 \
      .
Error is:
./mariadb/mysys/charset.c: In function ‘get_charsets_dir’:
./mariadb/mysys/charset.c:509:22: error: ‘DEFAULT_CHARSET_HOME’ undeclared (first use in this function)
  is_prefix(sharedir, DEFAULT_CHARSET_HOME))
",8,"What is the magic to compile it? Using ubuntu 16.04

cmake was:
prefix=$1
cmake \
      -DBUILD_CONFIG=mysql_release \
      -DCMAKE_BUILD_TYPE=RelWithDebInfo \
      -DCMAKE_INSTALL_PREFIX:PATH=$prefix \
      -DWITH_ZLIB=""bundled"" \
      -DMYSQL_DATADIR=""${prefix}/data"" \
      -DMYSQL_UNIX_ADDR=""${prefix}/var/mysql.sock"" \
      -DMYSQL_USER=""mysql"" \
      -DENABLED_LOCAL_INFILE=1 \
      -DMYSQL_MAINTAINER_MODE=1 \
      .
Error is:
./mariadb/mysys/charset.c: In function ‘get_charsets_dir’:
./mariadb/mysys/charset.c:509:22: error: ‘DEFAULT_CHARSET_HOME’ undeclared (first use in this function)
  is_prefix(sharedir, DEFAULT_CHARSET_HOME))
"
2844,MDEV-9658,MDEV,Sergei Petrunia,87766,2016-10-25 20:24:40,"[~mdcallag], I wanted to note again that *most* of replication/binlog-related features are still disabled and do not work yet. Judging from the number of test failures, a number of other things are broken as well. 

Having said that, here are the steps I just used on Ubuntu 16.04.1 to build:
{noformat}
sudo apt-get update
sudo apt-get install g++ cmake libbz2-dev libaio-dev bison zlib1g-dev libsnappy-dev 
sudo apt-get install libgflags-dev libreadline6-dev libncurses5-dev libssl-dev liblz4-dev gdb git
git clone https://github.com/MariaDB/server.git mariadb-10.2
cd  mariadb-10.2
git checkout 10.2-mariarocks
git submodule init
git submodule update
cmake . -DCMAKE_BUILD_TYPE=Debug -DWITHOUT_MROONGA:bool=1 -DWITHOUT_TOKUDB:bool=1
make -j16
{noformat}
(-DWITHOUT_TOKUDB and -DWITHOUT_MROONGA are just to make the build faster)
A test that works:
{noformat}
cd mysql-test
./mysql-test-run --mysqld=--default-storage-engine=rocksdb --mysqld=--skip-innodb --mysqld=--default-tmp-storage-engine=MyISAM --mysqld=--rocksdb rocksdb.type_binary
{noformat}

Also tried  with {{-DCMAKE_BUILD_TYPE=RelWithDebInfo}} and it worked (I mean compilation finished and a few tests ran).",9,"[~mdcallag], I wanted to note again that *most* of replication/binlog-related features are still disabled and do not work yet. Judging from the number of test failures, a number of other things are broken as well. 

Having said that, here are the steps I just used on Ubuntu 16.04.1 to build:
{noformat}
sudo apt-get update
sudo apt-get install g++ cmake libbz2-dev libaio-dev bison zlib1g-dev libsnappy-dev 
sudo apt-get install libgflags-dev libreadline6-dev libncurses5-dev libssl-dev liblz4-dev gdb git
git clone URL mariadb-10.2
cd  mariadb-10.2
git checkout 10.2-mariarocks
git submodule init
git submodule update
cmake . -DCMAKE_BUILD_TYPE=Debug -DWITHOUT_MROONGA:bool=1 -DWITHOUT_TOKUDB:bool=1
make -j16
{noformat}
(-DWITHOUT_TOKUDB and -DWITHOUT_MROONGA are just to make the build faster)
A test that works:
{noformat}
cd mysql-test
./mysql-test-run --mysqld=--default-storage-engine=rocksdb --mysqld=--skip-innodb --mysqld=--default-tmp-storage-engine=MyISAM --mysqld=--rocksdb rocksdb.type_binary
{noformat}

Also tried  with {{-DCMAKE_BUILD_TYPE=RelWithDebInfo}} and it worked (I mean compilation finished and a few tests ran)."
2845,MDEV-9658,MDEV,Mark Callaghan,87776,2016-10-26 00:31:45,"Still trying the release build on ubuntu 16.04. Must be more dependencies. I get this failure:
{noformat}
cd /home/mdcallag/git/mariadb/client && /usr/bin/cmake -E cmake_link_script CMakeFiles/mysqltest.dir/link.txt --verbose=1
/usr/bin/c++    -pie -fPIC -Wl,-z,relro,-z,now -fstack-protector --param=ssp-buffer-size=4 -DWITH_INNODB_DISALLOW_WRITES -fno-rtti -O3 -g -static-libgcc -fno-omit-frame-pointer -fno-strict-aliasing -Wno-uninitialized -D_FORTIFY_SOURCE=2 -DDBUG_OFF  -Wl,--export-dynamic CMakeFiles/mysqltest.dir/mysqltest.cc.o  -o mysqltest  -lpthread ../libmariadb/libmariadb/libmariadbclient.a ../mysys/libmysys.a ../pcre/libpcre.a ../pcre/libpcreposix.a -lpthread -lc -lgnutls ../dbug/libdbug.a ../mysys_ssl/libmysys_ssl.a ../mysys/libmysys.a ../dbug/libdbug.a ../mysys_ssl/libmysys_ssl.a ../zlib/libzlib.a -ldl -lm ../strings/libstrings.a ../extra/yassl/libyassl.a ../extra/yassl/taocrypt/libtaocrypt.a ../pcre/libpcre.a -lpthread 
/usr/bin/ld: ../mysys/libmysys.a(stacktrace.c.o): undefined reference to symbol '__bss_start'
//usr/lib/x86_64-linux-gnu/libgmp.so.10: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
client/CMakeFiles/mysqltest.dir/build.make:110: recipe for target 'client/mysqltest' failed
{noformat}",10,"Still trying the release build on ubuntu 16.04. Must be more dependencies. I get this failure:
{noformat}
cd /home/mdcallag/git/mariadb/client && /usr/bin/cmake -E cmake_link_script CMakeFiles/mysqltest.dir/link.txt --verbose=1
/usr/bin/c++    -pie -fPIC -Wl,-z,relro,-z,now -fstack-protector --param=ssp-buffer-size=4 -DWITH_INNODB_DISALLOW_WRITES -fno-rtti -O3 -g -static-libgcc -fno-omit-frame-pointer -fno-strict-aliasing -Wno-uninitialized -D_FORTIFY_SOURCE=2 -DDBUG_OFF  -Wl,--export-dynamic CMakeFiles/mysqltest.dir/mysqltest.cc.o  -o mysqltest  -lpthread ../libmariadb/libmariadb/libmariadbclient.a ../mysys/libmysys.a ../pcre/libpcre.a ../pcre/libpcreposix.a -lpthread -lc -lgnutls ../dbug/libdbug.a ../mysys_ssl/libmysys_ssl.a ../mysys/libmysys.a ../dbug/libdbug.a ../mysys_ssl/libmysys_ssl.a ../zlib/libzlib.a -ldl -lm ../strings/libstrings.a ../extra/yassl/libyassl.a ../extra/yassl/taocrypt/libtaocrypt.a ../pcre/libpcre.a -lpthread 
/usr/bin/ld: ../mysys/libmysys.a(stacktrace.c.o): undefined reference to symbol '__bss_start'
//usr/lib/x86_64-linux-gnu/libgmp.so.10: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
client/CMakeFiles/mysqltest.dir/build.make:110: recipe for target 'client/mysqltest' failed
{noformat}"
2846,MDEV-9658,MDEV,Sergei Petrunia,87896,2016-10-29 12:24:32,"There is a number of similar test failures in
* type_varchar
* type_fixed_indexes
* type_enum
* type_date_time_indexes

etc. They all look like this failure in rocksdb.type_varchar:
{noformat}
  create table t (id int primary key, email varchar(100), KEY email_i (email(30)));
  insert into t values (1, 'abcabcabcabcabcabcabcabcabcabcabc  ');
  explain select 'email_i' as index_name, count(*) AS count from t force index(email_i);
  id	select_type	table	type	possible_keys	key	key_len	ref	rows	Extra
-  1	SIMPLE	t	index	NULL	email_i	33	NULL	#	Using index
+  1	SIMPLE	t	ALL	NULL	NULL	NULL	NULL	#	
{noformat}
Debugging it, I can see that in MariaDB:
{noformat}
(gdb) p table->field[0]->part_of_key
  $67 = {map = 1}
{noformat}

while in MySQL
{noformat}
(gdb) p table->field[0]->part_of_key
  $20 = {map = 3}
{noformat}

and the cause of that is this commit in MariaDB: b6b5f9fabe4866a8753e81e1f80593b645f35d8e

In particular the chunk of code in table.cc that starts with:
{noformat}
        /* 
          Do not extend the key that contains a component
          defined over the beginning of a field.
	*/ 
{noformat}

I'll need to check with Igor what was the exact reason for this logic.  MyRocks relies ALL keys having PK extension. (there is already one exception: SQL layer doesn't generate extensions for unique secondary indexes. MyRocks has code to produce index extension for its internal data structures).

",11,"There is a number of similar test failures in
* type_varchar
* type_fixed_indexes
* type_enum
* type_date_time_indexes

etc. They all look like this failure in rocksdb.type_varchar:
{noformat}
  create table t (id int primary key, email varchar(100), KEY email_i (email(30)));
  insert into t values (1, 'abcabcabcabcabcabcabcabcabcabcabc  ');
  explain select 'email_i' as index_name, count(*) AS count from t force index(email_i);
  id	select_type	table	type	possible_keys	key	key_len	ref	rows	Extra
-  1	SIMPLE	t	index	NULL	email_i	33	NULL	#	Using index
+  1	SIMPLE	t	ALL	NULL	NULL	NULL	NULL	#	
{noformat}
Debugging it, I can see that in MariaDB:
{noformat}
(gdb) p table->field[0]->part_of_key
  $67 = {map = 1}
{noformat}

while in MySQL
{noformat}
(gdb) p table->field[0]->part_of_key
  $20 = {map = 3}
{noformat}

and the cause of that is this commit in MariaDB: b6b5f9fabe4866a8753e81e1f80593b645f35d8e

In particular the chunk of code in table.cc that starts with:
{noformat}
        /* 
          Do not extend the key that contains a component
          defined over the beginning of a field.
	*/ 
{noformat}

I'll need to check with Igor what was the exact reason for this logic.  MyRocks relies ALL keys having PK extension. (there is already one exception: SQL layer doesn't generate extensions for unique secondary indexes. MyRocks has code to produce index extension for its internal data structures).

"
2847,MDEV-9658,MDEV,Sergei Petrunia,87899,2016-10-29 13:19:55," rocksdb.type_char_indexes fails like this:
{noformat}
 rocksdb.type_char_indexes
--- /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/mysql-test/rocksdb/r/type_char_indexes.result 2016-10-06 17:30:25.595958776 +0000
+++ /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/mysql-test/rocksdb/r/type_char_indexes.reject 2016-10-29 13:16:57.672889620 +0000
@@ -45,7 +45,7 @@
 INSERT INTO t1 (c,c20,v16,v128,pk) VALUES ('a','char1','varchar1a','varchar1b','1'),('a','char2','varchar2a','varchar2b','2'),('b','char3','varchar1a','varchar1b','3'),('c','char4','varchar3a','varchar3b','4');
 EXPLAIN SELECT SUBSTRING(v16,0,3) FROM t1 WHERE v16 LIKE 'varchar%';
 id     select_type     table   type    possible_keys   key     key_len ref     rows    Extra
-1      SIMPLE  t1      index   v16     v16     19      NULL    #       Using where; Using index
+1      SIMPLE  t1      index   v16     v16     21      NULL    #       Using where; Using index
 SELECT SUBSTRING(v16,7,3) FROM t1 WHERE v16 LIKE 'varchar%';
 SUBSTRING(v16,7,3)
 r1a
{noformat}. 
The reason - it's a bug in MariaDB,  MDEV-11172.  It has no visible harm effects, so I'll just update the .result file.",12," rocksdb.type_char_indexes fails like this:
{noformat}
 rocksdb.type_char_indexes
--- /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/mysql-test/rocksdb/r/type_char_indexes.result 2016-10-06 17:30:25.595958776 +0000
+++ /home/psergey/dev-git/10.2-mariarocks/storage/rocksdb/mysql-test/rocksdb/r/type_char_indexes.reject 2016-10-29 13:16:57.672889620 +0000
@@ -45,7 +45,7 @@
 INSERT INTO t1 (c,c20,v16,v128,pk) VALUES ('a','char1','varchar1a','varchar1b','1'),('a','char2','varchar2a','varchar2b','2'),('b','char3','varchar1a','varchar1b','3'),('c','char4','varchar3a','varchar3b','4');
 EXPLAIN SELECT SUBSTRING(v16,0,3) FROM t1 WHERE v16 LIKE 'varchar%';
 id     select_type     table   type    possible_keys   key     key_len ref     rows    Extra
-1      SIMPLE  t1      index   v16     v16     19      NULL    #       Using where; Using index
+1      SIMPLE  t1      index   v16     v16     21      NULL    #       Using where; Using index
 SELECT SUBSTRING(v16,7,3) FROM t1 WHERE v16 LIKE 'varchar%';
 SUBSTRING(v16,7,3)
 r1a
{noformat}. 
The reason - it's a bug in MariaDB,  MDEV-11172.  It has no visible harm effects, so I'll just update the .result file."
2848,MDEV-9658,MDEV,Sergei Petrunia,87918,2016-10-31 16:28:24,rocksdb.rocksdb_range test failure and similar ones - see MDEV-11194,13,rocksdb.rocksdb_range test failure and similar ones - see MDEV-11194
2849,MDEV-9658,MDEV,Sergei Petrunia,88136,2016-11-07 08:15:21,"Got more tests to work, currently an attempt to run {{rocksdb}} test suite gives:
{noformat}
Completed: Failed 85/200 tests, 57.50% were successful.
{noformat}
",14,"Got more tests to work, currently an attempt to run {{rocksdb}} test suite gives:
{noformat}
Completed: Failed 85/200 tests, 57.50% were successful.
{noformat}
"
2850,MDEV-9658,MDEV,Sergei Petrunia,88596,2016-11-22 14:21:53,"Current status with tests is:
{noformat}
Completed: Failed 65/199 tests, 67.34% were successful.
{noformat}
",15,"Current status with tests is:
{noformat}
Completed: Failed 65/199 tests, 67.34% were successful.
{noformat}
"
2851,MDEV-9658,MDEV,Sergei Petrunia,89241,2016-12-06 18:21:58,"Current status with tests is:
{noformat}
Completed: Failed 39/194 tests, 79.90% were successful.
{noformat}

which gives a speed of 26 tests/10 work days",16,"Current status with tests is:
{noformat}
Completed: Failed 39/194 tests, 79.90% were successful.
{noformat}

which gives a speed of 26 tests/10 work days"
2852,MDEV-9658,MDEV,Sergei Petrunia,89776,2016-12-20 18:43:24,"Current status is 
{noformat}
Completed: Failed 25/184 tests, 86.41% were successful.
{noformat}",17,"Current status is 
{noformat}
Completed: Failed 25/184 tests, 86.41% were successful.
{noformat}"
2853,MDEV-9658,MDEV,Sergei Petrunia,90122,2017-01-01 23:36:36,"Merged in the current MyRocks code.

This caused more tests to fail. Fixed some of these, but still have:
{noformat}
Completed: Failed 38/199 tests, 80.90% were successful.
{noformat}",18,"Merged in the current MyRocks code.

This caused more tests to fail. Fixed some of these, but still have:
{noformat}
Completed: Failed 38/199 tests, 80.90% were successful.
{noformat}"
2854,MDEV-9658,MDEV,Sergei Petrunia,90147,2017-01-02 20:32:18,"Got the number of failures back down to 
{noformat}
Completed: Failed 24/197 tests, 87.82% were successful.
{noformat}",19,"Got the number of failures back down to 
{noformat}
Completed: Failed 24/197 tests, 87.82% were successful.
{noformat}"
2855,MDEV-9658,MDEV,Sergei Petrunia,90188,2017-01-03 15:04:27,"Merged with the current MariaDB-10.2.  After test result update, now have:
{noformat}
Completed: Failed 23/197 tests, 88.32% were successful.
{noformat}",20,"Merged with the current MariaDB-10.2.  After test result update, now have:
{noformat}
Completed: Failed 23/197 tests, 88.32% were successful.
{noformat}"
2856,MDEV-9658,MDEV,Sergei Petrunia,90452,2017-01-10 10:27:58,"Fixed or disabled a few more tests. Current status:
{noformat}
Completed: Failed 17/195 tests, 91.28% were successful.                                                                                                                          
{noformat}
",21,"Fixed or disabled a few more tests. Current status:
{noformat}
Completed: Failed 17/195 tests, 91.28% were successful.                                                                                                                          
{noformat}
"
2857,MDEV-9658,MDEV,Sergei Petrunia,90731,2017-01-17 18:34:10,"From now on, will also run rocksdb_sys_vars test suite.

Current status is:
{noformat}
Completed: Failed 18/295 tests, 93.90% were successful.
{noformat}

Of 18 failing tests
- 17 failing are from {{rocksdb}}
- 1 test is from {{rocksdb_sys_vars}}

The tree is now in the buildbot:
https://github.com/MariaDB/server/commits/bb-10.2-mariarocks

A few builders: kvm-deb-trusty-amd64, work-amd64-valgrind build and run it.",22,"From now on, will also run rocksdb_sys_vars test suite.

Current status is:
{noformat}
Completed: Failed 18/295 tests, 93.90% were successful.
{noformat}

Of 18 failing tests
- 17 failing are from {{rocksdb}}
- 1 test is from {{rocksdb_sys_vars}}

The tree is now in the buildbot:
URL

A few builders: kvm-deb-trusty-amd64, work-amd64-valgrind build and run it."
2858,MDEV-9658,MDEV,Sergei Petrunia,90825,2017-01-19 13:32:49,Differences wrt stock MariaDB on the SQL layer: https://gist.github.com/spetrunia/e5d9fb8e94292ae7a7bc623a9c078111 ,23,Differences wrt stock MariaDB on the SQL layer: URL 
2859,MDEV-9658,MDEV,Elena Stepanova,91042,2017-01-24 16:51:53,"Setting to 10.2-ga because of the server changes. The plugin itself can come later, if necessary. ",24,"Setting to 10.2-ga because of the server changes. The plugin itself can come later, if necessary. "
2860,MDEV-9658,MDEV,Sergei Petrunia,91166,2017-01-27 21:01:58,"Reduced the amount of changes wrt stock MariaDB outside of {{storage/rocksdb}} directory.  The new diff: https://gist.github.com/spetrunia/c7665515f9c5e714dd8869701fdabcc0 .  
[~serg], Please take a look.",25,"Reduced the amount of changes wrt stock MariaDB outside of {{storage/rocksdb}} directory.  The new diff: URL .  
[~serg], Please take a look."
2861,MDEV-9658,MDEV,Sergei Petrunia,93018,2017-03-14 20:21:07,"Attempt to classify the new test failures after last merge:

h2. Timeout
rocksdb.drop_table3                      w4 [ fail ]  timeout after 900 seconds
rocksdb.cardinality                      w3 [ fail ]  timeout after 900 seconds
rocksdb.statistics                       w1 [ fail ]  timeout after 900 seconds
rocksdb.truncate_table3                  w4 [ fail ]  timeout after 900 seconds
rocksdb.records_in_range                 w1 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_checksums                w3 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter4                     w4 [ fail ]  timeout after 900 seconds
rocksdb.singledelete                     w2 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter_skip                 w1 [ fail ]  timeout after 900 seconds
rocksdb.drop_table                       w3 [ fail ]  timeout after 900 seconds
rocksdb.optimize_table                   w4 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_deadlock_stress_rc       w2 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter                      w1 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter2                     w3 [ fail ]  timeout after 900 seconds
rocksdb.blind_delete_without_tx_api 'row' w2 [ fail ]  timeout after 900 seconds
rocksdb.deadlock                         w4 [ fail ]  timeout after 900 seconds
rocksdb.drop_table2                      w2 [ fail ]  timeout after 900 seconds
rocksdb.commit_in_the_middle_ddl         w1 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_deadlock_stress_rr       w3 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_table_stats_sampling_pct_change w2 [ fail ]  timeout after 900 seconds

after rocksdb_deadlock_stress_rr on one machine and rocksdb_deadlock_stress_rc
on another, there are always loads of
{noformat}
2017-03-13 22:19:10 140500730528512 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
2017-03-13 22:19:10 140500731134720 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
2017-03-13 22:19:10 140500730831616 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
{noformat}
errors in  the log

h2. Deadlock

rocksdb.unique_sec                       w2 [ fail ]  Found warnings/errors in server log file!
rocksdb.rocksdb_locks                    w4 [ fail ]  Found warnings/errors in server log file!
rocksdb.unique_sec_rev_cf                w4 [ fail ]  Found warnings/errors in server log file!
rocksdb.issue111                         w4 [ fail ]  Found warnings/errors in server log file!
rocksdb.rocksdb_deadlock_detect_rr       w3 [ fail ]  Found warnings/errors in server log file!
rocksdb.hermitage                        w1 [ fail ]  Found warnings/errors in server log file!
rocksdb.level_repeatable_read            w4 [ fail ]  Found warnings/errors in server log file!
{noformat}
2017-03-14  0:19:17 139935554990848 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
{noformat}

h2. Crash in the client
rocksdb.rocksdb_range                    w3 [ fail ]
rocksdb.misc                             w3 [ fail ]
- Crash in the client?

h2. Different failures affecting single tests

rocksdb.rocksdb                          w3 [ fail ]  Found warnings/errors in server log file!
- ""Sort aborted"" after DELETE.

rocksdb.collation                        w2 [ fail ]
- Crash in sysvar update.

rocksdb.show_engine                      w2 [ fail ]
- SHOW ENGINE rocksdB TRANSACTION STATUS

rocksdb.blind_delete_without_tx_api 'mix' w2 [ fail ]
rocksdb.blind_delete_without_tx_api 'stmt' w4 [ fail ]
mysqltest: At line 23: query '$insert' failed: 1105: Can't execute updates on master with binlog_format != ROW.

h2. Misc failures

rocksdb.rpl_row_not_found 'row'          w4 [ fail ]
{noformat}
2017-03-14  0:20:26 140674317531008 [ERROR] /home/psergey/dev-git/10.2-mariarocks/sql/mysqld: Error while setting value 'SEMI_STRICT' to 'slave_exec_mode'
{noformat}
rocksdb.rpl_row_stats 'row'              w4 [ fail ]
(result file difference)

rocksdb.rpl_row_triggers 'row'           w4 [ fail ]
rocksdb.rpl_savepoint 'row'              w4 [ fail ]


rocksdb.trx_info_rpl 'row'               w4 [ fail ]
{noformat}
2017-03-14  0:21:04 140149053159296 [ERROR] /home/psergey/dev-git/10.2-mariarocks/sql/mysqld: unknown variable 'rpl_skip_tx_api=ON'
{noformat}
rocksdb.rocksdb_datadir                  w3 [ fail ]
{noformat}
2017-03-14  0:55:24 140347962312576 [ERROR] /home/psergey/dev-git/10.2-mariarocks/sql/mysqld: unknown variable 'rocksdb_datadir=/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/tmp/3/.rocksdb_datadir.test'
{noformat}

rocksdb.tbl_opt_data_index_dir           w1 [ fail ]
{noformat}
mysqltest: At line 14: query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno 1005: 'Can't create table `test`.`t1` (errno: 198 ""Unknown error 198"")', instead of 1296...
{noformat}
",26,"Attempt to classify the new test failures after last merge:

h2. Timeout
rocksdb.drop_table3                      w4 [ fail ]  timeout after 900 seconds
rocksdb.cardinality                      w3 [ fail ]  timeout after 900 seconds
rocksdb.statistics                       w1 [ fail ]  timeout after 900 seconds
rocksdb.truncate_table3                  w4 [ fail ]  timeout after 900 seconds
rocksdb.records_in_range                 w1 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_checksums                w3 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter4                     w4 [ fail ]  timeout after 900 seconds
rocksdb.singledelete                     w2 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter_skip                 w1 [ fail ]  timeout after 900 seconds
rocksdb.drop_table                       w3 [ fail ]  timeout after 900 seconds
rocksdb.optimize_table                   w4 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_deadlock_stress_rc       w2 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter                      w1 [ fail ]  timeout after 900 seconds
rocksdb.bloomfilter2                     w3 [ fail ]  timeout after 900 seconds
rocksdb.blind_delete_without_tx_api 'row' w2 [ fail ]  timeout after 900 seconds
rocksdb.deadlock                         w4 [ fail ]  timeout after 900 seconds
rocksdb.drop_table2                      w2 [ fail ]  timeout after 900 seconds
rocksdb.commit_in_the_middle_ddl         w1 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_deadlock_stress_rr       w3 [ fail ]  timeout after 900 seconds
rocksdb.rocksdb_table_stats_sampling_pct_change w2 [ fail ]  timeout after 900 seconds

after rocksdb_deadlock_stress_rr on one machine and rocksdb_deadlock_stress_rc
on another, there are always loads of
{noformat}
2017-03-13 22:19:10 140500730528512 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
2017-03-13 22:19:10 140500731134720 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
2017-03-13 22:19:10 140500730831616 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
{noformat}
errors in  the log

h2. Deadlock

rocksdb.unique_sec                       w2 [ fail ]  Found warnings/errors in server log file!
rocksdb.rocksdb_locks                    w4 [ fail ]  Found warnings/errors in server log file!
rocksdb.unique_sec_rev_cf                w4 [ fail ]  Found warnings/errors in server log file!
rocksdb.issue111                         w4 [ fail ]  Found warnings/errors in server log file!
rocksdb.rocksdb_deadlock_detect_rr       w3 [ fail ]  Found warnings/errors in server log file!
rocksdb.hermitage                        w1 [ fail ]  Found warnings/errors in server log file!
rocksdb.level_repeatable_read            w4 [ fail ]  Found warnings/errors in server log file!
{noformat}
2017-03-14  0:19:17 139935554990848 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
{noformat}

h2. Crash in the client
rocksdb.rocksdb_range                    w3 [ fail ]
rocksdb.misc                             w3 [ fail ]
- Crash in the client?

h2. Different failures affecting single tests

rocksdb.rocksdb                          w3 [ fail ]  Found warnings/errors in server log file!
- ""Sort aborted"" after DELETE.

rocksdb.collation                        w2 [ fail ]
- Crash in sysvar update.

rocksdb.show_engine                      w2 [ fail ]
- SHOW ENGINE rocksdB TRANSACTION STATUS

rocksdb.blind_delete_without_tx_api 'mix' w2 [ fail ]
rocksdb.blind_delete_without_tx_api 'stmt' w4 [ fail ]
mysqltest: At line 23: query '$insert' failed: 1105: Can't execute updates on master with binlog_format != ROW.

h2. Misc failures

rocksdb.rpl_row_not_found 'row'          w4 [ fail ]
{noformat}
2017-03-14  0:20:26 140674317531008 [ERROR] /home/psergey/dev-git/10.2-mariarocks/sql/mysqld: Error while setting value 'SEMI_STRICT' to 'slave_exec_mode'
{noformat}
rocksdb.rpl_row_stats 'row'              w4 [ fail ]
(result file difference)

rocksdb.rpl_row_triggers 'row'           w4 [ fail ]
rocksdb.rpl_savepoint 'row'              w4 [ fail ]


rocksdb.trx_info_rpl 'row'               w4 [ fail ]
{noformat}
2017-03-14  0:21:04 140149053159296 [ERROR] /home/psergey/dev-git/10.2-mariarocks/sql/mysqld: unknown variable 'rpl_skip_tx_api=ON'
{noformat}
rocksdb.rocksdb_datadir                  w3 [ fail ]
{noformat}
2017-03-14  0:55:24 140347962312576 [ERROR] /home/psergey/dev-git/10.2-mariarocks/sql/mysqld: unknown variable 'rocksdb_datadir=/home/psergey/dev-git/10.2-mariarocks/mysql-test/var/tmp/3/.rocksdb_datadir.test'
{noformat}

rocksdb.tbl_opt_data_index_dir           w1 [ fail ]
{noformat}
mysqltest: At line 14: query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno 1005: 'Can't create table `test`.`t1` (errno: 198 ""Unknown error 198"")', instead of 1296...
{noformat}
"
2862,MDEV-9658,MDEV,Sergei Petrunia,93019,2017-03-14 20:27:20,"error logs themselves:
* an aws instance https://gist.github.com/spetrunia/7bf36db2bf0ed4fbb8045ecd34a21358
* psergey-desktop: https://gist.github.com/spetrunia/0babf9e3a9f7b11487bb0ad78d641f9c",27,"error logs themselves:
* an aws instance URL
* psergey-desktop: URL"
2863,MDEV-9658,MDEV,Sergei Petrunia,93061,2017-03-15 20:28:19,"A lot of timeouts go away if I set @@rocksdb_flush_logs_at_trx_commit to 0 for the rocksdb test suite:

https://gist.github.com/spetrunia/d61bf3f92ca248cd57a3e87504daece0
https://gist.github.com/spetrunia/19eed7367eeb05b3500d062ca7e0e49e
",28,"A lot of timeouts go away if I set @@rocksdb_flush_logs_at_trx_commit to 0 for the rocksdb test suite:

URL
URL
"
2864,MDEV-9658,MDEV,Sergei Petrunia,93079,2017-03-16 08:27:30,"Current status : we are down to 14 test failures, of which 7 are ""something with deadlock detector"" and another 7 are individual issues.

MTR log: https://gist.github.com/spetrunia/e58537b1efa5094f62ecf5b7b6f60573

h2. Something with deadlock detector
  rocksdb.rocksdb_locks 
  rocksdb.rocksdb_deadlock_stress_rc 
  rocksdb.unique_sec 
  rocksdb.unique_sec_rev_cf 
  rocksdb.locking_issues 
  rocksdb.rocksdb_deadlock_detect_rr 
  rocksdb.rocksdb_deadlock_stress_rr 

Loads of errors in the log like this
{noformat}
2017-03-16  7:04:48 140240146146048 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
{noformat}

h2. Isolated failures

rocksdb.misc 
Dumb .result difference

rocksdb.collation 
{noformat}
mysqltest: At line 188: query 'SET GLOBAL rocksdb_strict_collation_exceptions=null' failed: 2013: Lost connection to MySQL server during query
{noformat}

rocksdb.tbl_opt_data_index_dir 
{noformat}
  query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno
{noformat}

rocksdb.rpl_row_stats 
  Counter increment difference 

rocksdb.rpl_savepoint
{noformat}
  query 'rollback to savepoint a' succeeded - should have failed with errno 1105...
{noformat}

rocksdb.blind_delete_without_tx_api  
- wait timeout, not always repeatable

rocksdb.rocksdb_datadir
{noformat}
/home/ubuntu/mariadb-10.2-dbg/sql/mysqld: unknown variable 'rocksdb_datadir=/home/ubuntu/mariadb-10.2-dbg/mysql-test/var/tmp/2/.rocksdb_datadir.test
{noformat}

",29,"Current status : we are down to 14 test failures, of which 7 are ""something with deadlock detector"" and another 7 are individual issues.

MTR log: URL

h2. Something with deadlock detector
  rocksdb.rocksdb_locks 
  rocksdb.rocksdb_deadlock_stress_rc 
  rocksdb.unique_sec 
  rocksdb.unique_sec_rev_cf 
  rocksdb.locking_issues 
  rocksdb.rocksdb_deadlock_detect_rr 
  rocksdb.rocksdb_deadlock_stress_rr 

Loads of errors in the log like this
{noformat}
2017-03-16  7:04:48 140240146146048 [ERROR] mysqld: Deadlock found when trying to get lock; try restarting transaction
{noformat}

h2. Isolated failures

rocksdb.misc 
Dumb .result difference

rocksdb.collation 
{noformat}
mysqltest: At line 188: query 'SET GLOBAL rocksdb_strict_collation_exceptions=null' failed: 2013: Lost connection to MySQL server during query
{noformat}

rocksdb.tbl_opt_data_index_dir 
{noformat}
  query 'CREATE TABLE t1 (a INT PRIMARY KEY, b CHAR(8)) ENGINE=rocksdb DATA DIRECTORY = '/foo/bar/data'' failed with wrong errno
{noformat}

rocksdb.rpl_row_stats 
  Counter increment difference 

rocksdb.rpl_savepoint
{noformat}
  query 'rollback to savepoint a' succeeded - should have failed with errno 1105...
{noformat}

rocksdb.blind_delete_without_tx_api  
- wait timeout, not always repeatable

rocksdb.rocksdb_datadir
{noformat}
/home/ubuntu/mariadb-10.2-dbg/sql/mysqld: unknown variable 'rocksdb_datadir=/home/ubuntu/mariadb-10.2-dbg/mysql-test/var/tmp/2/.rocksdb_datadir.test
{noformat}

"
2865,MDEV-9658,MDEV,Sergei Petrunia,93113,2017-03-16 18:47:33,"Down to -7- 4 test failures on the AWS instance: https://gist.github.com/spetrunia/8349449919264626a1f93dfb34be18a6
* rocksdb.collation 
* -rocksdb.misc- 
* -rocksdb.rocksdb_datadir-
* rocksdb.tbl_opt_data_index_dir
* -rocksdb.rpl_row_stats- 
* -rocksdb.rpl_savepoint- 
* -rocksdb.blind_delete_without_tx_api-
These are the ""Isolated failures"" mentioned above.


On psergey-desktop, debug build, I get 10 failures:  https://gist.github.com/spetrunia/d658ea3d195d420156feb8560a4d7b92 . Same as above plus three more.
",30,"Down to -7- 4 test failures on the AWS instance: URL
* rocksdb.collation 
* -rocksdb.misc- 
* -rocksdb.rocksdb_datadir-
* rocksdb.tbl_opt_data_index_dir
* -rocksdb.rpl_row_stats- 
* -rocksdb.rpl_savepoint- 
* -rocksdb.blind_delete_without_tx_api-
These are the ""Isolated failures"" mentioned above.


On psergey-desktop, debug build, I get 10 failures:  URL . Same as above plus three more.
"
2866,MDEV-9658,MDEV,Sergei Petrunia,93120,2017-03-16 22:24:29,"Latest status: -4- 3 test failures (6 in debug build):
https://gist.github.com/spetrunia/82a89d4bd35d1402145cef5ca11f1eda
https://gist.github.com/spetrunia/b19425d2bfd280fc29ee95151c9f2478",31,"Latest status: -4- 3 test failures (6 in debug build):
URL
URL"
2867,MDEV-9658,MDEV,Sergei Petrunia,93130,2017-03-17 10:43:03,"Latest status:
https://gist.github.com/spetrunia/ed35780f02121b5ee381c76fc6e765a3
https://gist.github.com/spetrunia/d3d7f715761026265f6d51ae8c65e188

-3- -2- 1 in release build: rocksdb.rocksdb -rocksdb.collation- -rocksdb.tbl_opt_data_index_dir-
-5- -4- 2 in debug build: rocksdb.rocksdb -rocksdb.collation- -rocksdb.tbl_opt_data_index_dir- rocksdb.persistent_cache (<- this is an upstream issue, see MDEV-1234)  rocksdb.unique_check

There is a new failure - rocksdb.rocksdb. It is not easily reproducible. It doesn't look very bad - it might be a race condition in the test.",32,"Latest status:
URL
URL

-3- -2- 1 in release build: rocksdb.rocksdb -rocksdb.collation- -rocksdb.tbl_opt_data_index_dir-
-5- -4- 2 in debug build: rocksdb.rocksdb -rocksdb.collation- -rocksdb.tbl_opt_data_index_dir- rocksdb.persistent_cache (<- this is an upstream issue, see MDEV-1234)  rocksdb.unique_check

There is a new failure - rocksdb.rocksdb. It is not easily reproducible. It doesn't look very bad - it might be a race condition in the test."
2868,MDEV-9658,MDEV,Sergei Petrunia,93331,2017-03-22 17:57:57,"Current status:
 (debug) https://gist.github.com/spetrunia/e9e08238479816c6c2e8bca9e4ec404e",33,"Current status:
 (debug) URL"
2869,MDEV-9658,MDEV,Oren Bissick,95749,2017-05-24 16:36:36,Will Galera be supported?,34,Will Galera be supported?
2870,MDEV-9658,MDEV,Sergei Petrunia,96186,2017-06-05 07:21:14,"[~obissick], currently there are no plans for Galera support.  Galera integrates tightly with the storage engine, and at the moment the only engine it has integration with is InnoDB/XtraDB.",35,"[~obissick], currently there are no plans for Galera support.  Galera integrates tightly with the storage engine, and at the moment the only engine it has integration with is InnoDB/XtraDB."
2871,MDEV-9658,MDEV,Sergei Petrunia,101656,2017-10-18 07:22:35,"Current status is:

* Upstream is merged up to
{noformat}
    commit 184a4a2d82f4f6f3cbcb1015bcdb32bebe73315c
    Author: Abhinav Sharma <abhinavsharma@fb.com>
    Date:   Thu Sep 14 11:40:08 2017 -0700
{noformat}

* Most of the features are in the 10.2 tree
* The candidate code for ""Group Commit with Binlog"" (MDEV-11934) feature is in {{bb-10.2-mariarocks}} tree.  

",36,"Current status is:

* Upstream is merged up to
{noformat}
    commit 184a4a2d82f4f6f3cbcb1015bcdb32bebe73315c
    Author: Abhinav Sharma 
    Date:   Thu Sep 14 11:40:08 2017 -0700
{noformat}

* Most of the features are in the 10.2 tree
* The candidate code for ""Group Commit with Binlog"" (MDEV-11934) feature is in {{bb-10.2-mariarocks}} tree.  

"
2872,MDEV-9658,MDEV,Sergei Petrunia,101741,2017-10-22 14:47:50,"Filed MDEV-14103, it is on [~elenst]'s plate now. ",37,"Filed MDEV-14103, it is on [~elenst]'s plate now. "
2873,MDEV-9658,MDEV,Elena Stepanova,101749,2017-10-22 20:56:59,"[~psergey],
The previous comment is ambiguous, could you please specify what exactly is on my plate -- MDEV-14103 or ""making MyRocks in MariaDB stable""?
I acknowledge receiving of MDEV-14103 (Testing for group commit in MyRocks), but it cannot possibly be the only obstacle between the current state and declaring MyRocks stable.
We have 7 open _buildbot_ bugs for RocksDB which need to be closed before declaring MyRocks stable, and quite a few other open bugs/tasks, priority of which needs to be revised, because it was initially set based on the fact that MyRocks was alpha/beta.",38,"[~psergey],
The previous comment is ambiguous, could you please specify what exactly is on my plate -- MDEV-14103 or ""making MyRocks in MariaDB stable""?
I acknowledge receiving of MDEV-14103 (Testing for group commit in MyRocks), but it cannot possibly be the only obstacle between the current state and declaring MyRocks stable.
We have 7 open _buildbot_ bugs for RocksDB which need to be closed before declaring MyRocks stable, and quite a few other open bugs/tasks, priority of which needs to be revised, because it was initially set based on the fact that MyRocks was alpha/beta."
2874,MDEV-9658,MDEV,Sergei Petrunia,101768,2017-10-23 09:50:29,"[~elenst], it is only MDEV-14103 that is on your plate. Sorry for the confusion.",39,"[~elenst], it is only MDEV-14103 that is on your plate. Sorry for the confusion."
2875,MDEV-9664,MDEV,Oleksandr Byelkin,81754,2016-03-03 16:44:10,"revision-id: a6c0c01cec43731df3c58b4fdb68d3ac18e6c699 (mariadb-10.1.8-124-ga6c0c01)
parent(s): 0485328d030f4b742dac7b667e8ed099beb9e9f2
committer: Oleksandr Byelkin
timestamp: 2016-02-29 18:56:45 +0100
message:

MDEV-3944: Allow derived tables in VIEWS

---

revision-id: f072d1672a067da09e7a5dc18d2c227992f0d672 (mariadb-10.1.8-125-gf072d16)
parent(s): a6c0c01cec43731df3c58b4fdb68d3ac18e6c699
committer: Oleksandr Byelkin
timestamp: 2016-03-03 17:42:15 +0100
message:

MDEV-9671 Wrong result upon select from a view with a FROM subquery

do not take marker OIN_TYPE_OUTER as a LEFT JOIN on print

---",1,"revision-id: a6c0c01cec43731df3c58b4fdb68d3ac18e6c699 (mariadb-10.1.8-124-ga6c0c01)
parent(s): 0485328d030f4b742dac7b667e8ed099beb9e9f2
committer: Oleksandr Byelkin
timestamp: 2016-02-29 18:56:45 +0100
message:

MDEV-3944: Allow derived tables in VIEWS

---

revision-id: f072d1672a067da09e7a5dc18d2c227992f0d672 (mariadb-10.1.8-125-gf072d16)
parent(s): a6c0c01cec43731df3c58b4fdb68d3ac18e6c699
committer: Oleksandr Byelkin
timestamp: 2016-03-03 17:42:15 +0100
message:

MDEV-9671 Wrong result upon select from a view with a FROM subquery

do not take marker OIN_TYPE_OUTER as a LEFT JOIN on print

---"
2876,MDEV-9664,MDEV,Elena Stepanova,81851,2016-03-09 12:47:27,"Ran several sets of optimizer tests with ExecuteAsView enabled on f072d1672a067da09e7a5dc18d2c227992f0d672 . 
There are a few known bugs which affect the tests (crashes and wrong results), but nothing that would be related to this change so far. 
Valgrind tests are queued. ",2,"Ran several sets of optimizer tests with ExecuteAsView enabled on f072d1672a067da09e7a5dc18d2c227992f0d672 . 
There are a few known bugs which affect the tests (crashes and wrong results), but nothing that would be related to this change so far. 
Valgrind tests are queued. "
2877,MDEV-9664,MDEV,Elena Stepanova,83197,2016-05-04 12:00:56,"The development task is still in review, tests might need to be repeated if there are any significant changes after the review.",3,"The development task is still in review, tests might need to be repeated if there are any significant changes after the review."
2878,MDEV-9665,MDEV,Sergei Golubchik,81967,2016-03-15 15:28:36,ok to push,1,ok to push
2879,MDEV-9676,MDEV,Sergei Petrunia,81943,2016-03-14 14:35:57,"A note about handling NULLs and ""PRECEDING|FOLLOWING n"". 

The standard says about Bound#1 and PRECEDING:
{quote}
If WFB1 specifies <window frame preceding> ...
If VSK (=current_row.sort_key) is not the null value, then...
If NULLS FIRST is specified or implied, then remove from WF all
rows R2 such that the value of SK in row R2 is the null value.
{quote}
That is, for ""RANGE BETWEEN n PRECEDING AND ..."", and non-null
current_row.sort_key, rows with NULL sort keys are not included.

This is achieved automatically when we compute 

{noformat}
current_row.sort_key - n_val
{noformat}

and compare that with current_cursor_row.sort_key. NULL is less than any
non-NULL value, so rows with NULL sort_key are never included.

what if current_row.sort_key IS NULL?  The standard has a passage about 

bq. If VSK is the null value and if NULLS LAST ...

but we are not using NULLS LAST. I interpret this as ""NULL values stay in the
frame when current_row.sort_key IS NULL"".

This happens automatically. We compute (NULL - n_val)= NULL, then we compare 
that to other rows, NULLs compare as equals during sorting, so all rows with
NULLs are included.",1,"A note about handling NULLs and ""PRECEDING|FOLLOWING n"". 

The standard says about Bound#1 and PRECEDING:
{quote}
If WFB1 specifies  ...
If VSK (=current_row.sort_key) is not the null value, then...
If NULLS FIRST is specified or implied, then remove from WF all
rows R2 such that the value of SK in row R2 is the null value.
{quote}
That is, for ""RANGE BETWEEN n PRECEDING AND ..."", and non-null
current_row.sort_key, rows with NULL sort keys are not included.

This is achieved automatically when we compute 

{noformat}
current_row.sort_key - n_val
{noformat}

and compare that with current_cursor_row.sort_key. NULL is less than any
non-NULL value, so rows with NULL sort_key are never included.

what if current_row.sort_key IS NULL?  The standard has a passage about 

bq. If VSK is the null value and if NULLS LAST ...

but we are not using NULLS LAST. I interpret this as ""NULL values stay in the
frame when current_row.sort_key IS NULL"".

This happens automatically. We compute (NULL - n_val)= NULL, then we compare 
that to other rows, NULLs compare as equals during sorting, so all rows with
NULLs are included."
2880,MDEV-9676,MDEV,Sergei Petrunia,81944,2016-03-14 14:41:03,"For Bound#1 and ""FOLLOWING n"" frame bound, it says:

{quote}
If VSK (=current_row.sort_key) is not the null value, then:
a) If NULLS FIRST is specified or implied, then remove from WF all
rows R2 such that the value of SK in row R2 is the null value.
{quote}

This again, is achieved automatically when we compute
{noformat}
current_row.sort_key + n_val
{noformat}

the result is a non-NULL value which is greater than NULL value, so NULLs are
excluded.
",2,"For Bound#1 and ""FOLLOWING n"" frame bound, it says:

{quote}
If VSK (=current_row.sort_key) is not the null value, then:
a) If NULLS FIRST is specified or implied, then remove from WF all
rows R2 such that the value of SK in row R2 is the null value.
{quote}

This again, is achieved automatically when we compute
{noformat}
current_row.sort_key + n_val
{noformat}

the result is a non-NULL value which is greater than NULL value, so NULLs are
excluded.
"
2881,MDEV-9676,MDEV,Sergei Petrunia,81945,2016-03-14 14:46:02,"
For window bound #2, it says:
{quote}
If WFB2 specifies <window frame preceding>...
 If VSK is the null value and if NULLS FIRST is specified or implied, then
remove from WF all rows R2 such that the value of SK in row R2 is not
the null value.
{quote}

We compute 
{noformat}
NULL - n_val
{noformat}

we get NULL as the range bound. This excludes all non-null rows from the frame.

The same is said for bound# and {{FOLLOWING n}}:
{quote}
 If VSK is the null value and if NULLS FIRST is specified or implied, then
remove from WF all rows R2 such that the value of SK in row R2 is not
the null value.
{quote}
",3,"
For window bound #2, it says:
{quote}
If WFB2 specifies ...
 If VSK is the null value and if NULLS FIRST is specified or implied, then
remove from WF all rows R2 such that the value of SK in row R2 is not
the null value.
{quote}

We compute 
{noformat}
NULL - n_val
{noformat}

we get NULL as the range bound. This excludes all non-null rows from the frame.

The same is said for bound# and {{FOLLOWING n}}:
{quote}
 If VSK is the null value and if NULLS FIRST is specified or implied, then
remove from WF all rows R2 such that the value of SK in row R2 is not
the null value.
{quote}
"
2882,MDEV-9676,MDEV,Sergei Petrunia,81953,2016-03-14 19:09:05,All expected functionality is in the feature tree.,4,All expected functionality is in the feature tree.
2883,MDEV-9695,MDEV,Sergei Petrunia,81909,2016-03-11 20:42:38,Fix pushed into the feature tree.,1,Fix pushed into the feature tree.
2884,MDEV-9711,MDEV,Krishnadas,87047,2016-10-04 04:35:32,DBS test cases  [^MDEV-9711.zip] ,1,DBS test cases  [^MDEV-9711.zip] 
2885,MDEV-9746,MDEV,Sergei Petrunia,82017,2016-03-16 12:30:11,"I like the two-cursor approach better.
* It does fewer update operations.  I can't prove it but looks like it's faster
* We will need window functions to use additional cursors anyway for LAG() and LEAD(). 
*  I don't think the approach of ""use the temp.table for temporary data during computations"" will be useful for any other window function",1,"I like the two-cursor approach better.
* It does fewer update operations.  I can't prove it but looks like it's faster
* We will need window functions to use additional cursors anyway for LAG() and LEAD(). 
*  I don't think the approach of ""use the temp.table for temporary data during computations"" will be useful for any other window function"
2886,MDEV-9758,MDEV,Sergey Vojtovich,83082,2016-04-28 11:08:14,"[~serg], this patch looks alright, but I'd like you opinion too. I believe we should shift fix version from 10.1 to 10.2. Also pay attention to my comments in PR#165.",1,"[~serg], this patch looks alright, but I'd like you opinion too. I believe we should shift fix version from 10.1 to 10.2. Also pay attention to my comments in PR#165."
2887,MDEV-9758,MDEV,Daniel Black,83140,2016-05-01 23:33:21,Thank you [~serg] and [~svoj]. Noted simplification fix and will incorporate concept in future patches.,2,Thank you [~serg] and [~svoj]. Noted simplification fix and will incorporate concept in future patches.
2888,MDEV-9780,MDEV,Sergei Petrunia,82597,2016-04-07 22:26:07,"Example from Peter:

{noformat}
create table t1 (s1 int, s2 char(5));
insert into t1 values (1,'a');
insert into t1 values (null,null);
insert into t1 values (1,null);
insert into t1 values (null,'a');
insert into t1 values (2,'b');
insert into t1 values (-1,'');
{noformat}

{noformat}
select *,
         row_number() over (order by s1)
         - row_number() over (order by s1) as X from t1;
{noformat}

This crashes, because ""split_sum_func"" process is not done correctly for window functions.
* The fact that split_sum_func is not invoked for the Item_func_minus is a trivial typo.
* But if I make it to be invoked, it does nothing
* What should Item::split_sum_func do when it is invoked for a Item_window_func that's an argument of an expression? Should we create an Item_direct_ref? Or just inject the item into the select list?

",1,"Example from Peter:

{noformat}
create table t1 (s1 int, s2 char(5));
insert into t1 values (1,'a');
insert into t1 values (null,null);
insert into t1 values (1,null);
insert into t1 values (null,'a');
insert into t1 values (2,'b');
insert into t1 values (-1,'');
{noformat}

{noformat}
select *,
         row_number() over (order by s1)
         - row_number() over (order by s1) as X from t1;
{noformat}

This crashes, because ""split_sum_func"" process is not done correctly for window functions.
* The fact that split_sum_func is not invoked for the Item_func_minus is a trivial typo.
* But if I make it to be invoked, it does nothing
* What should Item::split_sum_func do when it is invoked for a Item_window_func that's an argument of an expression? Should we create an Item_direct_ref? Or just inject the item into the select list?

"
2889,MDEV-9780,MDEV,Sergei Petrunia,82634,2016-04-10 17:47:23,The above is now resolved.,2,The above is now resolved.
2890,MDEV-9780,MDEV,Sergei Petrunia,82635,2016-04-10 17:48:06,The problem with DISTINCT being converted into GROUP BY is resolved,3,The problem with DISTINCT being converted into GROUP BY is resolved
2891,MDEV-9780,MDEV,Sergei Petrunia,82636,2016-04-10 17:49:55,"We seem to still have a problem with queries that have aggregates but do not have GROUP BY clause.

This is not critical because aggregages w/o GROUP BY means the query output is just one row. It is not very meaningful to compute window functions over a resultset that has just one row  (although this is allowed and should be fixed).",4,"We seem to still have a problem with queries that have aggregates but do not have GROUP BY clause.

This is not critical because aggregages w/o GROUP BY means the query output is just one row. It is not very meaningful to compute window functions over a resultset that has just one row  (although this is allowed and should be fixed)."
2892,MDEV-9780,MDEV,Sergei Petrunia,86873,2016-09-28 14:13:17,Marking as fixed (in 10.2.1 version),5,Marking as fixed (in 10.2.1 version)
2893,MDEV-9787,MDEV,Sergei Petrunia,82292,2016-03-27 17:13:26,"Consider the query without the window function: 

{noformat}
select 
  b,
  max(a) as MX 
from t1 
group by b 
having MX in (3,5,7)
{noformat}

Operations:
1. scan table t1 
   the obtained rows are used for group-by operation
   (the groups are interleaved)

2. call filesort 
2.1 filesort reads data from temp. table 
    and applies the ""MX in (3,5,6)"" condition

Now, let's add a window function into the select list.

* Window function should be computed after grouping
* but before the sorting.

The problem is that sorting step also checks the HAVING clause. 
According to the standard, HAVING must be checked before window functions are
computed.

Two possible options:
- Make window function code check HAVING. The problem is, every cursor will need to check HAVING clause.
- Create another temp.table which will only hold that have passed the HAVING. 
",1,"Consider the query without the window function: 

{noformat}
select 
  b,
  max(a) as MX 
from t1 
group by b 
having MX in (3,5,7)
{noformat}

Operations:
1. scan table t1 
   the obtained rows are used for group-by operation
   (the groups are interleaved)

2. call filesort 
2.1 filesort reads data from temp. table 
    and applies the ""MX in (3,5,6)"" condition

Now, let's add a window function into the select list.

* Window function should be computed after grouping
* but before the sorting.

The problem is that sorting step also checks the HAVING clause. 
According to the standard, HAVING must be checked before window functions are
computed.

Two possible options:
- Make window function code check HAVING. The problem is, every cursor will need to check HAVING clause.
- Create another temp.table which will only hold that have passed the HAVING. 
"
2894,MDEV-9787,MDEV,Sergei Petrunia,82314,2016-03-28 17:44:22,"Third option: so, subsequent sorting step also checks the HAVING clause.  Let all window functions' filesort() calls check the HAVING clause, too.

I'm not sure if this has the best performance but it's easiest to implement.",2,"Third option: so, subsequent sorting step also checks the HAVING clause.  Let all window functions' filesort() calls check the HAVING clause, too.

I'm not sure if this has the best performance but it's easiest to implement."
2895,MDEV-9787,MDEV,Sergei Petrunia,82638,2016-04-10 18:36:50,This was fixed by d146c2cedc88e6d8728584b83861bd9b677a44a3,3,This was fixed by d146c2cedc88e6d8728584b83861bd9b677a44a3
2896,MDEV-9792,MDEV,Alexey Botchkov,83048,2016-04-27 07:23:21,"FIxing patch:
http://lists.askmonty.org/pipermail/commits/2016-April/009321.html",1,"FIxing patch:
URL"
2897,MDEV-9857,MDEV,Sergey Vojtovich,83168,2016-05-03 08:45:08,"[~serg], I merged everything I found reasonable. Could you have another look at this PR?
Merged changes are here: https://github.com/MariaDB/server/commits/bb-10.2-mdev9857",1,"[~serg], I merged everything I found reasonable. Could you have another look at this PR?
Merged changes are here: URL"
2898,MDEV-9857,MDEV,Sergey Vojtovich,83937,2016-06-02 08:34:18,License confirmed. There's also PR#178 now.,2,License confirmed. There's also PR#178 now.
2899,MDEV-9857,MDEV,Sergei Golubchik,84040,2016-06-06 14:51:36,"Looks ok. The only comment is about
{code:c++}
 /*
   we want sizeof(LF_PINS) to be CPU_LEVEL1_DCACHE_LINESIZE * 2
   to avoid false sharing
 */
 char pad[CPU_LEVEL1_DCACHE_LINESIZE * 2 - sizeof(uint32) * 2
                                         - sizeof(LF_PINBOX*)
                                         - sizeof(void*)
                                         - sizeof(void*) * (LF_PINBOX_PINS + 1
{code}
This is not exactly true. We don't want {{sizeof(LF_PINS)}} to be {{CPU_LEVEL1_DCACHE_LINESIZE * 2}}, we want
{{sizeof(LF_PINS) % CPU_LEVEL1_DCACHE_LINESIZE == 0}}, but whether it'll be two cache lines, one cache line, three cache lines or ten cache lines — doesn't matter much. Preferably, as few as possible. With cache line size being 64, I had to use 64*2, because LF_PINS didn't fit in 64 bytes. But with cache line size being 128, LF_PINS will fit in one cache line, there's no need to allocate 256 bytes for it.

It'd be perfect if you could change it to use as few cache lines as possible.",3,"Looks ok. The only comment is about
{code:c++}
 /*
   we want sizeof(LF_PINS) to be CPU_LEVEL1_DCACHE_LINESIZE * 2
   to avoid false sharing
 */
 char pad[CPU_LEVEL1_DCACHE_LINESIZE * 2 - sizeof(uint32) * 2
                                         - sizeof(LF_PINBOX*)
                                         - sizeof(void*)
                                         - sizeof(void*) * (LF_PINBOX_PINS + 1
{code}
This is not exactly true. We don't want {{sizeof(LF_PINS)}} to be {{CPU_LEVEL1_DCACHE_LINESIZE * 2}}, we want
{{sizeof(LF_PINS) % CPU_LEVEL1_DCACHE_LINESIZE == 0}}, but whether it'll be two cache lines, one cache line, three cache lines or ten cache lines — doesn't matter much. Preferably, as few as possible. With cache line size being 64, I had to use 64*2, because LF_PINS didn't fit in 64 bytes. But with cache line size being 128, LF_PINS will fit in one cache line, there's no need to allocate 256 bytes for it.

It'd be perfect if you could change it to use as few cache lines as possible."
2900,MDEV-9857,MDEV,Daniel Black,84065,2016-06-07 01:52:46,"PR#178 removed padding as a mechanism  for alignment in favour of using compiler alignment combined with malloc alignment to keep these on different cache lines.

Test case to show the alignment of the structure member affects the structure size: https://gist.github.com/grooverdan/19a7c8d8ac0e680ba6fcf4668962b9c4

The original pad calculation that did include a mod - https://github.com/MariaDB/server/pull/169/files#diff-41202b17494e25e891bbfa27df165331R71
As the pad isn't actually referenced  in the code, additional size just means wasted memory.",4,"PR#178 removed padding as a mechanism  for alignment in favour of using compiler alignment combined with malloc alignment to keep these on different cache lines.

Test case to show the alignment of the structure member affects the structure size: URL

The original pad calculation that did include a mod - URL
As the pad isn't actually referenced  in the code, additional size just means wasted memory."
2901,MDEV-9857,MDEV,Sergey Vojtovich,84083,2016-06-07 09:03:12,"[~serg], remember our discussion re paddings:
{noformat}
svoj> > > > > +  struct st_locks
svoj> > > > > +  {
svoj> > > > > +    mysql_rwlock_t lock;
svoj> > > > > +    char pad[128];
serg> > > >
serg> > > > In these cases I usually do: pad[128 - sizeof(mysql_rwlock_t)];
svoj> > > Hmm... strange that I didn't define macro for this, I thought I did.
svoj> > >
svoj> > > Your suggestion guarantees that st_locks will be at least 128 bytes. But these
svoj> > > 128 bytes may be split across 2 cache lines. Or malloc returns pointer aligned
svoj> > > on cache line size?
serg> >
serg> > No, it doesn't. As far as I know. But I only care that different locks
serg> > go into different cache lines, they don't need to be aligned to 128 byte
serg> > boundary.
svoj> What about this:
svoj> sizeof(mysql_rwlock_t)= 64 (just rough estimation)
svoj> sizeof(pad[128 - sizeof(mysql_rwlock_t)])= 64
svoj> 
svoj> It gives:
svoj>  +-------------------------------+-------------+-------------------------------+
svoj>  | rw_lock_t 64b                 | padding 64b | rw_lock_t 64b                 |
svoj>  +---------------+---------------+-------------+---------------+---------------+
svoj>  Cache line 128b |               Cache line 128b               |Cache line 128b
svoj>  ----------------+---------------------------------------------+----------------
serg> Okay, indeed. You're right.
{noformat}

Your statetement is correct for aligned allocs. Minimal padding in this particular case is ""char pad\[CPU_LEVEL1_DCACHE_LINESIZE\]"". I'll fix it accordingly.",5,"[~serg], remember our discussion re paddings:
{noformat}
svoj> > > > > +  struct st_locks
svoj> > > > > +  {
svoj> > > > > +    mysql_rwlock_t lock;
svoj> > > > > +    char pad[128];
serg> > > >
serg> > > > In these cases I usually do: pad[128 - sizeof(mysql_rwlock_t)];
svoj> > > Hmm... strange that I didn't define macro for this, I thought I did.
svoj> > >
svoj> > > Your suggestion guarantees that st_locks will be at least 128 bytes. But these
svoj> > > 128 bytes may be split across 2 cache lines. Or malloc returns pointer aligned
svoj> > > on cache line size?
serg> >
serg> > No, it doesn't. As far as I know. But I only care that different locks
serg> > go into different cache lines, they don't need to be aligned to 128 byte
serg> > boundary.
svoj> What about this:
svoj> sizeof(mysql_rwlock_t)= 64 (just rough estimation)
svoj> sizeof(pad[128 - sizeof(mysql_rwlock_t)])= 64
svoj> 
svoj> It gives:
svoj>  +-------------------------------+-------------+-------------------------------+
svoj>  | rw_lock_t 64b                 | padding 64b | rw_lock_t 64b                 |
svoj>  +---------------+---------------+-------------+---------------+---------------+
svoj>  Cache line 128b |               Cache line 128b               |Cache line 128b
svoj>  ----------------+---------------------------------------------+----------------
serg> Okay, indeed. You're right.
{noformat}

Your statetement is correct for aligned allocs. Minimal padding in this particular case is ""char pad\[CPU_LEVEL1_DCACHE_LINESIZE\]"". I'll fix it accordingly."
2902,MDEV-9857,MDEV,Daniel Black,84687,2016-06-29 23:33:39,If you have aligned allocs and and aligned structure elements do you actually still need padding? (https://github.com/MariaDB/server/pull/185).,6,If you have aligned allocs and and aligned structure elements do you actually still need padding? (URL
2903,MDEV-9857,MDEV,Sergey Vojtovich,84689,2016-06-30 06:42:19,No padding for aligned structures is needed.,7,No padding for aligned structures is needed.
2904,MDEV-9872,MDEV,Sergey Vojtovich,84762,2016-07-05 14:06:41,"[~serg], please review patch for this task (b6d7d5878eecc7f0cb76f8f99a9989686ff9715c).",1,"[~serg], please review patch for this task (b6d7d5878eecc7f0cb76f8f99a9989686ff9715c)."
2905,MDEV-9872,MDEV,Sergey Vojtovich,84763,2016-07-05 14:09:56,upd: e252dae86392a2b50d647a380db36fb8b01bf405,2,upd: e252dae86392a2b50d647a380db36fb8b01bf405
2906,MDEV-9872,MDEV,Daniel Black,84773,2016-07-05 23:14:30,"[~svoj] thanks, sorry I didn't get back to this sooner. If you need a hand let me know.",3,"[~svoj] thanks, sorry I didn't get back to this sooner. If you need a hand let me know."
2907,MDEV-9872,MDEV,Sergey Vojtovich,84776,2016-07-06 07:16:49,"[~danblack], no problem. It'd be nice if you could review this patch and let me know if I missed something.",4,"[~danblack], no problem. It'd be nice if you could review this patch and let me know if I missed something."
2908,MDEV-9872,MDEV,Sergey Vojtovich,85356,2016-08-03 06:13:40,Waiting for feedback.,5,Waiting for feedback.
2909,MDEV-9872,MDEV,Sergei Golubchik,85367,2016-08-03 11:12:30,ok to push,6,ok to push
2910,MDEV-9872,MDEV,Sergey Vojtovich,85384,2016-08-04 13:20:43,"[~danblack], we're now getting failures like this:
http://buildbot.askmonty.org/buildbot/builders/p8-xenial-bintar/builds/88/steps/test/logs/stdio

I noticed that crc32-vpmsum sources differ in your PR. Did you also see these failures? Could you elaborate the difference?",7,"[~danblack], we're now getting failures like this:
URL

I noticed that crc32-vpmsum sources differ in your PR. Did you also see these failures? Could you elaborate the difference?"
2911,MDEV-9872,MDEV,Daniel Black,85395,2016-08-05 00:35:43,"The crux of it is innodb uses CRC32C and the rest of MySQL (my_checksum) uses CRC32-IEEE.

I'll patch in both implementations into the crc32-vpmsum library and then get my_checksum to point to the vpmsum crc32-ieee implementation

",8,"The crux of it is innodb uses CRC32C and the rest of MySQL (my_checksum) uses CRC32-IEEE.

I'll patch in both implementations into the crc32-vpmsum library and then get my_checksum to point to the vpmsum crc32-ieee implementation

"
2912,MDEV-9872,MDEV,Daniel Black,85396,2016-08-05 04:04:50,"https://github.com/MariaDB/server/pull/211 - test case for SQL CRC32 function

https://github.com/MariaDB/server/pull/210 - Add both versions of the CRC32 and adjust my_checksum to use the optimized version.",9,"URL - test case for SQL CRC32 function

URL - Add both versions of the CRC32 and adjust my_checksum to use the optimized version."
2913,MDEV-9942,MDEV,Phil Sweeney,83273,2016-05-07 13:53:34,"Looks like Percona fixed the issue blocking TokuDB merges:

https://tokutek.atlassian.net/browse/DB-962",1,"Looks like Percona fixed the issue blocking TokuDB merges:

URL"
2914,MDEV-9947,MDEV,Oleksandr Byelkin,83299,2016-05-09 13:29:50,"revision-id: 7e70658d1e88b4a255e3c28426f8259e5e596a85 (mariadb-10.2.0-20-g7e70658)
parent(s): c0a59b46be5be341bd6ffc9fe188a236ced46522
committer: Oleksandr Byelkin
timestamp: 2016-05-09 15:26:18 +0200
message:

MDEV-9947: COM_MULTI united response

---",1,"revision-id: 7e70658d1e88b4a255e3c28426f8259e5e596a85 (mariadb-10.2.0-20-g7e70658)
parent(s): c0a59b46be5be341bd6ffc9fe188a236ced46522
committer: Oleksandr Byelkin
timestamp: 2016-05-09 15:26:18 +0200
message:

MDEV-9947: COM_MULTI united response

---"
2915,MDEV-9947,MDEV,Oleksandr Byelkin,83366,2016-05-11 16:25:06,"So I made such test:

send in one COM_MULTI 50 following group of 3 statement:
{code}
insert into t1 values (1)
insert into t1 values (2)
delete from t1
{code}
then read response
and do it 10000 times

I run 5 times test with my patch then 5 times without and then 3 times with and without (just to be sure that result is stable):

With the patch (s):
{code}
26.956
26.743
26.659
26.708
26.597

26.671
26.213
26.501
------
26.631
{code}
without the patch (s):
{code}
38.014
39.698
38.269
39.738
39.620

39.558
39.310
39.779
------
39.24825
{code}


so it is 32% gain on 127.0.0.1 tcp connection.


Server and test was compiled without debug info (for server all 4 times, client was compiled only once).
Server started via ./mysql-test-run --start

client:
time ./unittest/libmariadb/features-10_2 -u root -d test -h 127.0.0.1  -P 16000

the test function is:
{code}
#define repeat1 50
#define repeat2 10000


static int com_multi_2(MYSQL *mysql)
{
  int rc;
  enum mariadb_com_multi status;


  rc= mysql_query(mysql, ""DROP TABLE IF EXISTS t1"");
  check_mysql_rc(rc, mysql);
  rc= mysql_query(mysql, ""create table t1 (a int)"");
  check_mysql_rc(rc, mysql);

  /* TEST COM_MULTI */

  for (uint i= 0; i < repeat2; i++)
  {
    status= MARIADB_COM_MULTI_BEGIN;
    if (mysql_options(mysql, MARIADB_OPT_COM_MULTI, &status))
    {
      diag(""COM_MULT not supported"");
      have_com_multi= 0;
      return SKIP;
    }

    for (uint j= 0; j < repeat1; j++)
    {
      rc= mysql_query(mysql, ""insert into t1 values (1)"");
      rc= mysql_query(mysql, ""insert into t1 values (2)"");
      rc= mysql_query(mysql, ""delete from t1;"");
    }

    status= MARIADB_COM_MULTI_END;
    rc= mysql_options(mysql, MARIADB_OPT_COM_MULTI, &status);

    for (uint j= 0; j < repeat1; j++)
    {
      /* 1 INSERT */
      check_mysql_rc(rc, mysql);
      /* 2 INSERT */
      rc= mysql_next_result(mysql);
      check_mysql_rc(rc, mysql);
      /* 3 DELETE */
      rc= mysql_next_result(mysql);
      check_mysql_rc(rc, mysql);

      rc= mysql_next_result(mysql);
      //printf(""%u\n"", (i+1)*repeat1 + j+1);
    }
    //putchar('.');

    rc= mysql_next_result(mysql);
    FAIL_UNLESS(rc == -1, ""more then 3*repeat1 results"");
  }

  /* TEST a simple query after COM_MULTI */

  rc= mysql_query(mysql, ""drop table t1"");

  /* question: how will result sets look like ? */
  diag(""error: %s"", mysql_error(mysql));

  return OK;
}
{code}",2,"So I made such test:

send in one COM_MULTI 50 following group of 3 statement:
{code}
insert into t1 values (1)
insert into t1 values (2)
delete from t1
{code}
then read response
and do it 10000 times

I run 5 times test with my patch then 5 times without and then 3 times with and without (just to be sure that result is stable):

With the patch (s):
{code}
26.956
26.743
26.659
26.708
26.597

26.671
26.213
26.501
------
26.631
{code}
without the patch (s):
{code}
38.014
39.698
38.269
39.738
39.620

39.558
39.310
39.779
------
39.24825
{code}


so it is 32% gain on 127.0.0.1 tcp connection.


Server and test was compiled without debug info (for server all 4 times, client was compiled only once).
Server started via ./mysql-test-run --start

client:
time ./unittest/libmariadb/features-10_2 -u root -d test -h 127.0.0.1  -P 16000

the test function is:
{code}
#define repeat1 50
#define repeat2 10000


static int com_multi_2(MYSQL *mysql)
{
  int rc;
  enum mariadb_com_multi status;


  rc= mysql_query(mysql, ""DROP TABLE IF EXISTS t1"");
  check_mysql_rc(rc, mysql);
  rc= mysql_query(mysql, ""create table t1 (a int)"");
  check_mysql_rc(rc, mysql);

  /* TEST COM_MULTI */

  for (uint i= 0; i < repeat2; i++)
  {
    status= MARIADB_COM_MULTI_BEGIN;
    if (mysql_options(mysql, MARIADB_OPT_COM_MULTI, &status))
    {
      diag(""COM_MULT not supported"");
      have_com_multi= 0;
      return SKIP;
    }

    for (uint j= 0; j < repeat1; j++)
    {
      rc= mysql_query(mysql, ""insert into t1 values (1)"");
      rc= mysql_query(mysql, ""insert into t1 values (2)"");
      rc= mysql_query(mysql, ""delete from t1;"");
    }

    status= MARIADB_COM_MULTI_END;
    rc= mysql_options(mysql, MARIADB_OPT_COM_MULTI, &status);

    for (uint j= 0; j < repeat1; j++)
    {
      /* 1 INSERT */
      check_mysql_rc(rc, mysql);
      /* 2 INSERT */
      rc= mysql_next_result(mysql);
      check_mysql_rc(rc, mysql);
      /* 3 DELETE */
      rc= mysql_next_result(mysql);
      check_mysql_rc(rc, mysql);

      rc= mysql_next_result(mysql);
      //printf(""%u\n"", (i+1)*repeat1 + j+1);
    }
    //putchar('.');

    rc= mysql_next_result(mysql);
    FAIL_UNLESS(rc == -1, ""more then 3*repeat1 results"");
  }

  /* TEST a simple query after COM_MULTI */

  rc= mysql_query(mysql, ""drop table t1"");

  /* question: how will result sets look like ? */
  diag(""error: %s"", mysql_error(mysql));

  return OK;
}
{code}"
2916,MDEV-9947,MDEV,Oleksandr Byelkin,83367,2016-05-11 16:26:53,there is problem in above test in case repeat1 = 100 (fixed in the connector by Georg),3,there is problem in above test in case repeat1 = 100 (fixed in the connector by Georg)
2917,MDEV-9947,MDEV,Oleksandr Byelkin,83432,2016-05-14 12:15:06,"revision-id: 8022548d2dd3ea4beef58a0613cc0f6a9d9e70be (mariadb-10.2.0-20-g8022548)
parent(s): c0a59b46be5be341bd6ffc9fe188a236ced46522
committer: Oleksandr Byelkin
timestamp: 2016-05-14 14:04:08 +0200
message:

MDEV-9947: COM_MULTI united response

---",4,"revision-id: 8022548d2dd3ea4beef58a0613cc0f6a9d9e70be (mariadb-10.2.0-20-g8022548)
parent(s): c0a59b46be5be341bd6ffc9fe188a236ced46522
committer: Oleksandr Byelkin
timestamp: 2016-05-14 14:04:08 +0200
message:

MDEV-9947: COM_MULTI united response

---"
2918,MDEV-9947,MDEV,Sergei Golubchik,83467,2016-05-17 11:03:46,ok to push,5,ok to push
2919,MXS-1004,MXS,Timofey Turenko,107186,2018-02-12 13:22:19,README added to BUILD/mdbci and maxscale-system-test/mdbci,1,README added to BUILD/mdbci and maxscale-system-test/mdbci
2920,MXS-1022,MXS,markus makela,88904,2016-11-30 08:36:35,"New monitors can be created at runtime. The created monitors are persisted to disk.  Due to the fact that most of the monitors have module specific parameters, a created monitor is initially in the stopped state. This allows the monitor to be configured properly before it is started.",1,"New monitors can be created at runtime. The created monitors are persisted to disk.  Due to the fact that most of the monitors have module specific parameters, a created monitor is initially in the stopped state. This allows the monitor to be configured properly before it is started."
2921,MXS-1023,MXS,markus makela,88903,2016-11-30 08:35:35,Listeners can now be created at runtime. New listeners are persisted to file when added.,1,Listeners can now be created at runtime. New listeners are persisted to file when added.
2922,MXS-1029,MXS,Massimiliano Pinto,89309,2016-12-07 13:15:47,"Checked how a big event should be encrypeted: as a whole buffer, no chunks",1,"Checked how a big event should be encrypeted: as a whole buffer, no chunks"
2923,MXS-1062,MXS,markus makela,90764,2017-01-18 10:05:34,Tests added and improved.,1,Tests added and improved.
2924,MXS-1063,MXS,markus makela,89716,2016-12-19 08:37:16,The changes to the client protocol routing capabilities allowed the tee filter to be simplified to a more asynchronous version. This allows the tee filter to ignore all results sent by the child session backends.,1,The changes to the client protocol routing capabilities allowed the tee filter to be simplified to a more asynchronous version. This allows the tee filter to ignore all results sent by the child session backends.
2925,MXS-1064,MXS,Massimiliano Pinto,89552,2016-12-14 15:00:00,"	Binlog Encryption is ON:
		Encryption Key File:      /home/mpinto/binlog/keys.txt
		Encryption Key Algorithm: aes_cbc
		Encryption Key length:    256 bits",1,"	Binlog Encryption is ON:
		Encryption Key File:      /home/mpinto/binlog/keys.txt
		Encryption Key Algorithm: aes_cbc
		Encryption Key length:    256 bits"
2926,MXS-1065,MXS,Massimiliano Pinto,90817,2017-01-19 10:39:16,"Implementation has started:


the source address can be:

source=192.168.122.1 (no wildcards)
source=192.168.122.%
source=192.168.%.%
source=192.%.%.%

Note:
- the wildcard char is then '%' only.
- source=% is not allowed
- source is only an IP address (possibly with wildcards)",1,"Implementation has started:


the source address can be:

source=192.168.122.1 (no wildcards)
source=192.168.122.%
source=192.168.%.%
source=192.%.%.%

Note:
- the wildcard char is then '%' only.
- source=% is not allowed
- source is only an IP address (possibly with wildcards)"
2927,MXS-1074,MXS,markus makela,90650,2017-01-16 14:12:01,The MaxScale core now validates and parses the configuration. The values can be accessed using type specific functions defined in {{include/maxscale/config.h}}.,1,The MaxScale core now validates and parses the configuration. The values can be accessed using type specific functions defined in {{include/maxscale/config.h}}.
2928,MXS-1075,MXS,Massimiliano Pinto,90582,2017-01-13 08:38:10,"MaxScale to Master registration with GTID implies registration process modification and possibly some checks at startup and configuration options


The slave registration phase to maxscale requires GTID handling:

the GTID must be extracted from current binlog file and possibly from old ones.

Kind of a database software is required, even i memory only, to store data whitout scanning binlog file all the time.

Tests from mariadb 10.0 and 10.1 should be done soon in order to check that everithing is ok before starting the implementation.

The provided estimation includes those  preliminary tests.",1,"MaxScale to Master registration with GTID implies registration process modification and possibly some checks at startup and configuration options


The slave registration phase to maxscale requires GTID handling:

the GTID must be extracted from current binlog file and possibly from old ones.

Kind of a database software is required, even i memory only, to store data whitout scanning binlog file all the time.

Tests from mariadb 10.0 and 10.1 should be done soon in order to check that everithing is ok before starting the implementation.

The provided estimation includes those  preliminary tests."
2929,MXS-1075,MXS,Massimiliano Pinto,91882,2017-02-15 09:02:34,"With the ""develop"" branch the binlog server reports now:


MySQL [(none)]> select @@gtid_current_pos;
+--------------------+
| @@gtid_current_pos |
+--------------------+
| 0-10116-36         |
+--------------------+
1 row in set (0.00 sec)

MySQL [(none)]> select @@global.gtid_current_pos;
+---------------------------+
| @@global.gtid_current_pos |
+---------------------------+
| 0-10116-36                |
+---------------------------+
1 row in set (0.00 sec)


and via maxadmin show services

	Last event from master:                      0x10, Transaction ID Event (2 Phase Commit)
	Last seen MariaDB GTID:                      0-10116-36
	Last binlog event timestamp:                 1487006091 (Mon Feb 13 18:14:51 2017)


Next steps are:

- Slave connecting to maxscale via GTID: gtids must be saved in oder to send related binlog events

- MaxScale connecting to master: as binlog server is a replication proxy and not an intermediate master with its own binlog files, some issues have to be managed.
More comments will follow on this.
",2,"With the ""develop"" branch the binlog server reports now:


MySQL [(none)]> select @@gtid_current_pos;
+--------------------+
| @@gtid_current_pos |
+--------------------+
| 0-10116-36         |
+--------------------+
1 row in set (0.00 sec)

MySQL [(none)]> select @@global.gtid_current_pos;
+---------------------------+
| @@global.gtid_current_pos |
+---------------------------+
| 0-10116-36                |
+---------------------------+
1 row in set (0.00 sec)


and via maxadmin show services

	Last event from master:                      0x10, Transaction ID Event (2 Phase Commit)
	Last seen MariaDB GTID:                      0-10116-36
	Last binlog event timestamp:                 1487006091 (Mon Feb 13 18:14:51 2017)


Next steps are:

- Slave connecting to maxscale via GTID: gtids must be saved in oder to send related binlog events

- MaxScale connecting to master: as binlog server is a replication proxy and not an intermediate master with its own binlog files, some issues have to be managed.
More comments will follow on this.
"
2930,MXS-1075,MXS,Massimiliano Pinto,92112,2017-02-21 09:24:55,"Just managed a basic hastable store for GTID being received.


First tests show that a MariaDB 10 Slave can register to maxscale this way:

set @@global.gtid_slave_pos='0-10116-78' // this is a GTID maxscale is aware of
change master to master_use_gtid=slave_pos;

start slave;

And after a few inserts into the master, replication follows

MariaDB [(none)]> select @@gtid_slave_pos;
+------------------+
| @@gtid_slave_pos |
+------------------+
| 0-10116-84       |
+------------------+

 This is the very first step.

More work needs to be done for transaction safety compatibility and later for a proper storage.",3,"Just managed a basic hastable store for GTID being received.


First tests show that a MariaDB 10 Slave can register to maxscale this way:

set @@global.gtid_slave_pos='0-10116-78' // this is a GTID maxscale is aware of
change master to master_use_gtid=slave_pos;

start slave;

And after a few inserts into the master, replication follows

MariaDB [(none)]> select @@gtid_slave_pos;
+------------------+
| @@gtid_slave_pos |
+------------------+
| 0-10116-84       |
+------------------+

 This is the very first step.

More work needs to be done for transaction safety compatibility and later for a proper storage."
2931,MXS-1075,MXS,Massimiliano Pinto,92362,2017-02-28 14:20:54,"Current work is in MXS-1075 branch


https://github.com/mariadb-corporation/MaxScale/tree/MXS-1075

",4,"Current work is in MXS-1075 branch


URL

"
2932,MXS-1078,MXS,markus makela,91526,2017-02-07 08:10:31,Rewrote the authentication plugins to use an SQLite database.,1,Rewrote the authentication plugins to use an SQLite database.
2933,MXS-1087,MXS,Johan Wikman,91154,2017-01-27 10:52:13,Tests for testing 16MB behaviour is still needed.,1,Tests for testing 16MB behaviour is still needed.
2934,MXS-1088,MXS,Esa Korhonen,92311,2017-02-27 11:39:04,"According to tests, the hint is parsed and used by at least readwritesplit router.
Test: 
// routes to master
SELECT LAST_INSERT_ID(), @@server_id;

// routes to slave
SELECT LAST_INSERT_ID(), @@server_id; 
-- maxscale route to slave

When using these commands in a client, make sure the hint is included in the same statement as the ""SELECT"". Some clients may automatically split them.",1,"According to tests, the hint is parsed and used by at least readwritesplit router.
Test: 
// routes to master
SELECT LAST_INSERT_ID(), @@server_id;

// routes to slave
SELECT LAST_INSERT_ID(), @@server_id; 
-- maxscale route to slave

When using these commands in a client, make sure the hint is included in the same statement as the ""SELECT"". Some clients may automatically split them."
2935,MXS-109,MXS,Guillaume Lefranc,70233,2015-04-23 11:34:51,"Hi Michael,

interesting points, I like the solution 2. which has two advantages, being configuration-less and elegant.
I'll check if I can implement it in the development branch.

Cheers
Guillaume",1,"Hi Michael,

interesting points, I like the solution 2. which has two advantages, being configuration-less and elegant.
I'll check if I can implement it in the development branch.

Cheers
Guillaume"
2936,MXS-109,MXS,Massimiliano Pinto,91365,2017-02-02 13:58:06,"[~tanj] [~michaeldg]

Current work is in https://github.com/mariadb-corporation/MaxScale/tree/MXS-109

Monitored nodes could be part of different cluster UUIDs: select only
the ones belonging to UUID with more joined nodes.

In case of different UUIDs, if the joined nodes number is less than (n_nodes
/ 2 ) + 1 don’t consider any node part of the cluster


MaxAdmin> show monitors

....
Galera Cluster UUID:    c08b6deb-e933-11e6-bf35-bb1d2a329b2e
Galera Cluster size:    4

maxscale.log

2017-02-02 11:00:07   info   : [galeramon] Galera cluster UUID is now c08b6deb-e933-11e6-bf35-bb1d2a329b2e with 4 members of 4 nodes

2 differents UUIDs

2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server1: UUID c08b6deb-e933-11e6-bf35-bb1d2a329b2e, joined nodes 2
2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server2: UUID fa7b0cab-e942-11e6-a67e-b669708bfe5a, joined nodes 2
2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server3: UUID c08b6deb-e933-11e6-bf35-bb1d2a329b2e, joined nodes 2
2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server4: UUID fa7b0cab-e942-11e6-a67e-b669708bfe5a, joined nodes 2
2017-02-02 12:30:40   error  : [galeramon] Galera cluster cannot be set with 2 members of 4: not enough nodes (3 at least)

MaxAdmin> show monitors
.....

Galera Cluster NOT set:	no member nodes
 ",2,"[~tanj] [~michaeldg]

Current work is in URL

Monitored nodes could be part of different cluster UUIDs: select only
the ones belonging to UUID with more joined nodes.

In case of different UUIDs, if the joined nodes number is less than (n_nodes
/ 2 ) + 1 don’t consider any node part of the cluster


MaxAdmin> show monitors

....
Galera Cluster UUID:    c08b6deb-e933-11e6-bf35-bb1d2a329b2e
Galera Cluster size:    4

maxscale.log

2017-02-02 11:00:07   info   : [galeramon] Galera cluster UUID is now c08b6deb-e933-11e6-bf35-bb1d2a329b2e with 4 members of 4 nodes

2 differents UUIDs

2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server1: UUID c08b6deb-e933-11e6-bf35-bb1d2a329b2e, joined nodes 2
2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server2: UUID fa7b0cab-e942-11e6-a67e-b669708bfe5a, joined nodes 2
2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server3: UUID c08b6deb-e933-11e6-bf35-bb1d2a329b2e, joined nodes 2
2017-02-02 12:30:40   debug  : [galeramon] Candidate cluster member server4: UUID fa7b0cab-e942-11e6-a67e-b669708bfe5a, joined nodes 2
2017-02-02 12:30:40   error  : [galeramon] Galera cluster cannot be set with 2 members of 4: not enough nodes (3 at least)

MaxAdmin> show monitors
.....

Galera Cluster NOT set:	no member nodes
 "
2937,MXS-109,MXS,Massimiliano Pinto,91366,2017-02-02 14:03:52,"[~tanj] [~michaeldg]

In case of 2 UUIDs if the user stops all the nodes of UUID_A (the good cluster), then nodes od UUID_B (the wrong one) will become part of the one Cluster MaxScale sees",3,"[~tanj] [~michaeldg]

In case of 2 UUIDs if the user stops all the nodes of UUID_A (the good cluster), then nodes od UUID_B (the wrong one) will become part of the one Cluster MaxScale sees"
2938,MXS-109,MXS,Michaël de groot,91593,2017-02-08 08:57:04,I think that is correct behaviour. Excellent work! :),4,I think that is correct behaviour. Excellent work! :)
2939,MXS-1110,MXS,Timofey Turenko,91790,2017-02-13 16:55:31,"does not work for TEE filter:

2017-01-10 02:28:18   error  : Closing client handler DCB, but it has no related service
2017-01-10 02:28:38   error  : [MySQLClient] Routing the query failed. Session will be closed.
2017-01-10 02:28:41   error  : Closing client handler DCB, but it has no related service",1,"does not work for TEE filter:

2017-01-10 02:28:18   error  : Closing client handler DCB, but it has no related service
2017-01-10 02:28:38   error  : [MySQLClient] Routing the query failed. Session will be closed.
2017-01-10 02:28:41   error  : Closing client handler DCB, but it has no related service"
2940,MXS-1113,MXS,Alex Boag-Munroe,97068,2017-06-28 15:44:23,"SchemaRouter is almost perfect for our need to shard over 2000 schemas, but lack of prepared statement support is an absolute show stopper, for hopefully obvious reasons.",1,"SchemaRouter is almost perfect for our need to shard over 2000 schemas, but lack of prepared statement support is an absolute show stopper, for hopefully obvious reasons."
2941,MXS-1121,MXS,markus makela,92114,2017-02-21 09:35:12,Added test for bulk insert with the new 10.2 array binding API.,1,Added test for bulk insert with the new 10.2 array binding API.
2942,MXS-1126,MXS,markus makela,91613,2017-02-08 13:40:34,Added upgrading to 2.1 document.,1,Added upgrading to 2.1 document.
2943,MXS-1140,MXS,Johan Wikman,95970,2017-05-31 07:17:57,"A concern for 2.2.
",1,"A concern for 2.2.
"
2944,MXS-1140,MXS,Dipti Joshi,99807,2017-09-07 16:50:54,Please see slides 5 through 8 of [this|https://docs.google.com/presentation/d/1NkiYSB82mN0LFmXP6voPiczFoBCVe1optndlE6Y4AIY/edit#slide=id.g21317f74fb_0_5] for the system setup.,2,Please see slides 5 through 8 of [this|URL for the system setup.
2945,MXS-1141,MXS,Massimiliano Pinto,91994,2017-02-17 09:32:58,Current code for -R $pos is in develop branch,1,Current code for -R $pos is in develop branch
2946,MXS-1142,MXS,Massimiliano Pinto,91961,2017-02-16 17:52:41,"options: -f -T 6311

2017-02-16 17:19:35   notice : [binlogrouter] >>> Position 6311 belongs to a transaction started at pos 6224.

2017-02-16 17:19:35   notice : [binlogrouter] === Replacing all events of Transaction at pos 6224 with IGNORABLE EVENT event type
2017-02-16 17:19:35   notice : [binlogrouter]  Replace event (GTID Event) at pos 6224 with an IGNORABLE EVENT 2017-02-16 17:19:36 notice : [binlogrouter]  Replace event (Table Map Event) at pos 6266 with an IGNORABLE EVENT
2017-02-16 17:19:36   notice : [binlogrouter]  Replace event (Write Rows Event (v1)) at pos 6311 with an IGNORABLE EVENT 2017-02-16 17:19:36 notice : [binlogrouter]  Replace event (Query Event) at pos 6349 with an IGNORABLE EVENT
",1,"options: -f -T 6311

2017-02-16 17:19:35   notice : [binlogrouter] >>> Position 6311 belongs to a transaction started at pos 6224.

2017-02-16 17:19:35   notice : [binlogrouter] === Replacing all events of Transaction at pos 6224 with IGNORABLE EVENT event type
2017-02-16 17:19:35   notice : [binlogrouter]  Replace event (GTID Event) at pos 6224 with an IGNORABLE EVENT 2017-02-16 17:19:36 notice : [binlogrouter]  Replace event (Table Map Event) at pos 6266 with an IGNORABLE EVENT
2017-02-16 17:19:36   notice : [binlogrouter]  Replace event (Write Rows Event (v1)) at pos 6311 with an IGNORABLE EVENT 2017-02-16 17:19:36 notice : [binlogrouter]  Replace event (Query Event) at pos 6349 with an IGNORABLE EVENT
"
2947,MXS-1156,MXS,Massimiliano Pinto,99466,2017-08-30 14:33:45,"[BinlogServer]
type=service
router=binlogrouter

master_retry_count=10
connect_retry=60
heartbeat=300

or

router_options=master_retry_count=10,connect_retry=60,heartbeat=300",1,"[BinlogServer]
type=service
router=binlogrouter

master_retry_count=10
connect_retry=60
heartbeat=300

or

router_options=master_retry_count=10,connect_retry=60,heartbeat=300"
2948,MXS-1156,MXS,Massimiliano Pinto,99467,2017-08-30 14:35:43,"maxadmin show services

....

	Number of connect retries:                 0
	Connect retry interval:                      60
	Connect retry count limit:              1000

",2,"maxadmin show services

....

	Number of connect retries:                 0
	Connect retry interval:                      60
	Connect retry count limit:              1000

"
2949,MXS-1156,MXS,Massimiliano Pinto,99468,2017-08-30 15:04:01,"MariaDB [(none)]> change master to master_connect_retry=60, master_heartbeat_period=300;

master_retry_count can onlt be changed in maxscale.cnf file.",3,"MariaDB [(none)]> change master to master_connect_retry=60, master_heartbeat_period=300;

master_retry_count can onlt be changed in maxscale.cnf file."
2950,MXS-1156,MXS,Massimiliano Pinto,99498,2017-08-31 09:03:00,Merged to develop with Documentation update.,4,Merged to develop with Documentation update.
2951,MXS-1177,MXS,Esa Korhonen,93717,2017-04-03 10:49:23,"The monitor parameter name to control this feature is ""backend_connect_attempts"".",1,"The monitor parameter name to control this feature is ""backend_connect_attempts""."
2952,MXS-1181,MXS,markus makela,93132,2017-03-17 11:45:47,Both client and backend connections now support IPv6 addresses.,1,Both client and backend connections now support IPv6 addresses.
2953,MXS-1193,MXS,Timofey Turenko,94359,2017-04-25 14:17:00,"10.2 is added to http://max-tst-01.mariadb.com:8089/view/test/job/run_test_matrix/ which is executed every week and 10.2 test setup is fixed (there was a problem with Galera setup, see https://github.com/OSLL/mdbci/pull/370",1,"10.2 is added to URL which is executed every week and 10.2 test setup is fixed (there was a problem with Galera setup, see URL"
2954,MXS-1194,MXS,Johan Wikman,94192,2017-04-21 09:39:58,"In general, the new functionality of 10.2 is an issue for the parser, which will not be able to parse e.g. selects using window functions properly.

However, in general, but for the database firewall, neither filters nor routers are affected.

The database firewall is affected in two ways:
* As the database firewall blocks all statements that can not be parsed completely, e.g. all selects using window functions will be blocked.
* The parser parses {{WITH}} statements correctly, but fails to report variables and functions referred to in the {{SELECT}} of the {{WITH}} clause. Consequently, it will not block a {{WITH}} statement where the {{WITH}} clause refers to forbidden columns.",1,"In general, the new functionality of 10.2 is an issue for the parser, which will not be able to parse e.g. selects using window functions properly.

However, in general, but for the database firewall, neither filters nor routers are affected.

The database firewall is affected in two ways:
* As the database firewall blocks all statements that can not be parsed completely, e.g. all selects using window functions will be blocked.
* The parser parses {{WITH}} statements correctly, but fails to report variables and functions referred to in the {{SELECT}} of the {{WITH}} clause. Consequently, it will not block a {{WITH}} statement where the {{WITH}} clause refers to forbidden columns."
2955,MXS-1209,MXS,Massimiliano Pinto,95249,2017-05-15 08:47:41,"MariaDB GTID Master registration:
creating missing binlog files (with 4 byes) between current one and the
filename coming from ROTATE_EVENT.

blr_slave_binlog_dump() is also checking possible empty files.",1,"MariaDB GTID Master registration:
creating missing binlog files (with 4 byes) between current one and the
filename coming from ROTATE_EVENT.

blr_slave_binlog_dump() is also checking possible empty files."
2956,MXS-1209,MXS,Massimiliano Pinto,95318,2017-05-16 09:28:41,"Tested with:


new Master:

- STOP SLAVE (it's no longer a slave server)
- SET @@read_only=OFF (if it was ON)
- Issue as many FLUSH LOGS as needed to have current maxscale binlog fie +1

MariaDB 10 Slaves can replicate without issues",2,"Tested with:


new Master:

- STOP SLAVE (it's no longer a slave server)
- SET @@read_only=OFF (if it was ON)
- Issue as many FLUSH LOGS as needed to have current maxscale binlog fie +1

MariaDB 10 Slaves can replicate without issues"
2957,MXS-1215,MXS,Esa Korhonen,95723,2017-05-24 11:29:49,Documentation will follow if code is accepted.,1,Documentation will follow if code is accepted.
2958,MXS-1225,MXS,Timofey Turenko,94442,2017-04-27 17:01:11,https://github.com/mariadb-corporation/build-scripts-vagrant/blob/mxs1225_build_manual/BUILD_TEST_DEBUG.md,1,URL
2959,MXS-1225,MXS,Timofey Turenko,94644,2017-05-03 05:53:23,https://github.com/mariadb-corporation/build-scripts-vagrant/blob/mxs1225_build_manual/BUILD_TEST_DEBUG.md,2,URL
2960,MXS-1226,MXS,Timofey Turenko,94486,2017-04-28 12:14:54,https://github.com/mariadb-corporation/build-scripts-vagrant/blob/mxs1226_release_manual/RELEASE.md,1,URL
2961,MXS-1226,MXS,Timofey Turenko,94643,2017-05-03 05:51:13,https://github.com/mariadb-corporation/build-scripts-vagrant/blob/mxs1226_release_manual/RELEASE.md,2,URL
2962,MXS-1229,MXS,markus makela,96267,2017-06-06 10:05:21,Added POST/PUT/PATCH support for relevant entry points.,1,Added POST/PUT/PATCH support for relevant entry points.
2963,MXS-1231,MXS,Timofey Turenko,94703,2017-05-04 12:04:11,"multiple servers can be run on the one machine:

cnf.example:

{code}
[mysqld1]
log-bin=mar-bin
binlog-format=row
max_long_data_size=1000000000
innodb_log_file_size=2000000000
slave-skip-errors=all
server_id=1
user            = mysql
pid-file        = /var/run/mysqld/mysqld1.pid
socket          = /var/run/mysqld/mysqld1.sock
port            = 3306
datadir         =  /data/mysql/mysql1

[mysqld2]
log-bin=mar-bin
binlog-format=row
max_long_data_size=1000000000
innodb_log_file_size=2000000000
slave-skip-errors=all
server_id=2
user            = mysql
pid-file        = /var/run/mysqld/mysqld2.pid
socket          = /var/run/mysqld/mysqld2.sock
port            = 3307
datadir         = /data/mysql/mysql2
{code}

{code}
sudo mysql_install_db --defaults-file=/home/vagrant/server1.cnf --user=mysql --datadir=/data/mysql/mysql1
sudo mysql_install_db --defaults-file=/home/vagrant/server1.cnf --user=mysql --datadir=/data/mysql/mysql2

sudo chown mysql:mysql -R /data
sudo mkdir /var/run/mysqld
sudo chown mysql:mysql /var/run/mysqld

sudo mysqld_multi  --defaults-file=/home/vagrant/server2.cnf  start
{code}


",1,"multiple servers can be run on the one machine:

cnf.example:

{code}
[mysqld1]
log-bin=mar-bin
binlog-format=row
max_long_data_size=1000000000
innodb_log_file_size=2000000000
slave-skip-errors=all
server_id=1
user            = mysql
pid-file        = /var/run/mysqld/mysqld1.pid
socket          = /var/run/mysqld/mysqld1.sock
port            = 3306
datadir         =  /data/mysql/mysql1

[mysqld2]
log-bin=mar-bin
binlog-format=row
max_long_data_size=1000000000
innodb_log_file_size=2000000000
slave-skip-errors=all
server_id=2
user            = mysql
pid-file        = /var/run/mysqld/mysqld2.pid
socket          = /var/run/mysqld/mysqld2.sock
port            = 3307
datadir         = /data/mysql/mysql2
{code}

{code}
sudo mysql_install_db --defaults-file=/home/vagrant/server1.cnf --user=mysql --datadir=/data/mysql/mysql1
sudo mysql_install_db --defaults-file=/home/vagrant/server1.cnf --user=mysql --datadir=/data/mysql/mysql2

sudo chown mysql:mysql -R /data
sudo mkdir /var/run/mysqld
sudo chown mysql:mysql /var/run/mysqld

sudo mysqld_multi  --defaults-file=/home/vagrant/server2.cnf  start
{code}


"
2964,MXS-1231,MXS,Timofey Turenko,94705,2017-05-04 12:15:12,"client connections:

sudo mysql --socket=/var/run/mysqld/mysqld1.sock
sudo mysql --socket=/var/run/mysqld/mysqld2.sock",2,"client connections:

sudo mysql --socket=/var/run/mysqld/mysqld1.sock
sudo mysql --socket=/var/run/mysqld/mysqld2.sock"
2965,MXS-1231,MXS,Timofey Turenko,95978,2017-05-31 09:16:13,"very simple: 4 MariaDB servers running on one machine, Maxscale and tests are also on this machine. 
Tests are limited by Master/Slave backend onlty",3,"very simple: 4 MariaDB servers running on one machine, Maxscale and tests are also on this machine. 
Tests are limited by Master/Slave backend onlty"
2966,MXS-1232,MXS,Timofey Turenko,95975,2017-05-31 09:13:30,https://github.com/mariadb-corporation/MaxScale/blob/2.1/maxscale-system-test/local_tests/start_multiple_mariadb.sh,1,URL
2967,MXS-1233,MXS,Timofey Turenko,95976,2017-05-31 09:14:05,integrated into TestConnection object of maxscale-system-tests,1,integrated into TestConnection object of maxscale-system-tests
2968,MXS-1236,MXS,markus makela,95459,2017-05-18 07:35:43,"Added a set of Node.js based tests to the MaxScale repository.
",1,"Added a set of Node.js based tests to the MaxScale repository.
"
2969,MXS-1245,MXS,markus makela,174342,2020-12-07 09:57:43,"Fixing this for single-target queries is relatively straightforward to fix. When a query would be routed to the target that we previously routed to, we can proceed with the routing even if we haven't received the result. The only real problem here can be the failure to write the query while ongoing queries are in progress. The simplest solution in this case is to close the session which would prevent any out-of-order errors from being created.

For multi-target queries this is a more complex issue. Simple session variable assignments can be executed even if a previous statement is in progress as this is only appended to the session command queue of each backend and then executed. The problem here is the tracking of which session commands have been replied to.

For direct execution of prepared statements, the problem is harder: MaxScale needs to be able to translate the external PS ID generated by Maxscale to an internal value which maps to different PS ID values for each backend connection. The mapping is unique to each connection and can change for each connection depending on when the connection is created and whether reconnections take place. This means that direct execution of prepared statements might make the prepared statement code much more complex.

Implementing the code required to handle single-target pipelining of queries would be the first milestone for this. The universal pipelining of queries would be the second one as pipelining of multi-target queries brings much more limited benefits than the pipelining of single-target queries (batch execution of statements).",1,"Fixing this for single-target queries is relatively straightforward to fix. When a query would be routed to the target that we previously routed to, we can proceed with the routing even if we haven't received the result. The only real problem here can be the failure to write the query while ongoing queries are in progress. The simplest solution in this case is to close the session which would prevent any out-of-order errors from being created.

For multi-target queries this is a more complex issue. Simple session variable assignments can be executed even if a previous statement is in progress as this is only appended to the session command queue of each backend and then executed. The problem here is the tracking of which session commands have been replied to.

For direct execution of prepared statements, the problem is harder: MaxScale needs to be able to translate the external PS ID generated by Maxscale to an internal value which maps to different PS ID values for each backend connection. The mapping is unique to each connection and can change for each connection depending on when the connection is created and whether reconnections take place. This means that direct execution of prepared statements might make the prepared statement code much more complex.

Implementing the code required to handle single-target pipelining of queries would be the first milestone for this. The universal pipelining of queries would be the second one as pipelining of multi-target queries brings much more limited benefits than the pipelining of single-target queries (batch execution of statements)."
2970,MXS-1252,MXS,markus makela,95460,2017-05-18 07:36:23,"Implemented the {{/maxscale/}} resource which exposes the core diagnostics.
",1,"Implemented the {{/maxscale/}} resource which exposes the core diagnostics.
"
2971,MXS-1266,MXS,Massimiliano Pinto,96281,2017-06-06 15:34:47,"Added binlog_structure=tree | flat option.

This allows storing binlog files in tree mode:

domain_id/server_id/file_name as thet come from master FAKE_ROTATE event at registration time.

With this new option the Slave replicates with GTID mode even if the new master has changed binlog filename or sequence.
With ""flat"" option value the new master should have next in sequence filename (with same prefix)
",1,"Added binlog_structure=tree | flat option.

This allows storing binlog files in tree mode:

domain_id/server_id/file_name as thet come from master FAKE_ROTATE event at registration time.

With this new option the Slave replicates with GTID mode even if the new master has changed binlog filename or sequence.
With ""flat"" option value the new master should have next in sequence filename (with same prefix)
"
2972,MXS-1267,MXS,markus makela,96458,2017-06-12 07:04:50,The tee filter now uses a less intrusive method of creating branching connections. This removes a number of broken concepts from MaxScale that were not robust enough to handle real workloads.,1,The tee filter now uses a less intrusive method of creating branching connections. This removes a number of broken concepts from MaxScale that were not robust enough to handle real workloads.
2973,MXS-1268,MXS,Esa Korhonen,95984,2017-05-31 10:34:35,Jira task CONC-257 created on the Connector-C board.,1,Jira task CONC-257 created on the Connector-C board.
2974,MXS-1268,MXS,Esa Korhonen,100742,2017-09-26 11:12:54,"Check https://jira.mariadb.org/browse/CONC-257
It's not marked as done but will probably be once 10.3 is out.",2,"Check URL
It's not marked as done but will probably be once 10.3 is out."
2975,MXS-1278,MXS,Dipti Joshi,101552,2017-10-13 11:25:20,Is there a new configuration flag added for this ?,1,Is there a new configuration flag added for this ?
2976,MXS-1278,MXS,Johan Wikman,101554,2017-10-13 12:18:34,"This is for turning on PL/SQL mode when a
{code}
set sql_mode=ORACLE;
{code}
statement is encountered.

Whether or not MaxScale _starts_ in Oracle mode is specified using the configuration parameter {{sql_mode}} as explained here:
https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Getting-Started/Configuration-Guide.md#sql_mode",2,"This is for turning on PL/SQL mode when a
{code}
set sql_mode=ORACLE;
{code}
statement is encountered.

Whether or not MaxScale _starts_ in Oracle mode is specified using the configuration parameter {{sql_mode}} as explained here:
URL"
2977,MXS-1300,MXS,markus makela,98020,2017-07-27 09:37:59,It is now possible to substitute MaxAdmin with MaxCtrl.,1,It is now possible to substitute MaxAdmin with MaxCtrl.
2978,MXS-1301,MXS,markus makela,97380,2017-07-06 09:33:06,"If the filter is configured with {{action=allow}} and a function type rule is defined, queries which do not use functions match the function rule.",1,"If the filter is configured with {{action=allow}} and a function type rule is defined, queries which do not use functions match the function rule."
2979,MXS-1301,MXS,Dipti Joshi,98351,2017-08-07 13:11:17,[~markus makela] Can you please give example configuration of dbfirewall filter(full) that blocks all functions on a column,2,[~markus makela] Can you please give example configuration of dbfirewall filter(full) that blocks all functions on a column
2980,MXS-1301,MXS,markus makela,98354,2017-08-07 14:19:09,"I did a quick test to figure out how the function whitelisting happens and I happened to spot a minor bug in the function type rule. Currently the syntax does not allow an empty set of function names to be given as a value for a function type rule. I've reported this in MXS-1345.

Meanwhile, the following rule file only allows the [name_const|https://mariadb.com/kb/en/mariadb/name_const/] function to be used with the {{name}} and {{address}} columns.

{code}
rule wl_columns deny columns name address
rule wl_function deny function name_const
users %@% match all rules wl_columns wl_function
{code}

Here's the configuration file for the rule file.

{code}
[fw]
type=filter
module=dbfwfilter
rules=/home/markusjm/build/rules
action=allow
{code}

A problematic fact about these types of rules is that the list of allowed columns is applied to all columns instead of a set of specified columns. This is partly due to the lack of expressiveness of the rule language of the dbfwfilter.",3,"I did a quick test to figure out how the function whitelisting happens and I happened to spot a minor bug in the function type rule. Currently the syntax does not allow an empty set of function names to be given as a value for a function type rule. I've reported this in MXS-1345.

Meanwhile, the following rule file only allows the [name_const|URL function to be used with the {{name}} and {{address}} columns.

{code}
rule wl_columns deny columns name address
rule wl_function deny function name_const
users %@% match all rules wl_columns wl_function
{code}

Here's the configuration file for the rule file.

{code}
[fw]
type=filter
module=dbfwfilter
rules=/home/markusjm/build/rules
action=allow
{code}

A problematic fact about these types of rules is that the list of allowed columns is applied to all columns instead of a set of specified columns. This is partly due to the lack of expressiveness of the rule language of the dbfwfilter."
2981,MXS-1301,MXS,Dipti Joshi,98359,2017-08-07 14:49:21,"[~markus makela] If the rule applies to function on call columns, that is a defect. A function should be specified on per column basis as well.  I have created MXS-1346 to report it.",4,"[~markus makela] If the rule applies to function on call columns, that is a defect. A function should be specified on per column basis as well.  I have created MXS-1346 to report it."
2982,MXS-1303,MXS,Esa Korhonen,98271,2017-08-04 08:47:51,"Tested using server branch ""bb-10.3-proxy-protocol"".",1,"Tested using server branch ""bb-10.3-proxy-protocol""."
2983,MXS-1305,MXS,Massimiliano Pinto,97634,2017-07-14 08:02:11,"Proper keyword is ""match"" instead of ""capture"":


            ""replace"": {
                ""column"": ""d_code"",
                ""match"": ""(?<=aaa).*(?=-12)|(?<=-12).*""
            },
            ""with"": {
                ""fill"": ""X#@""
            }
",1,"Proper keyword is ""match"" instead of ""capture"":


            ""replace"": {
                ""column"": ""d_code"",
                ""match"": ""(?<=aaa).*(?=-12)|(?<=-12).*""
            },
            ""with"": {
                ""fill"": ""X#@""
            }
"
2984,MXS-1305,MXS,Massimiliano Pinto,97650,2017-07-14 12:27:09,"in ""develop"" branch",2,"in ""develop"" branch"
2985,MXS-1305,MXS,Dipti Joshi,97823,2017-07-20 06:33:51, [~Massimiliano Pinto] Where can I find user documentation on this ?,3, [~Massimiliano Pinto] Where can I find user documentation on this ?
2986,MXS-1306,MXS,Massimiliano Pinto,97331,2017-07-05 13:41:04,"A new rule option has been added: ‘obfuscate’:
{
""obfuscate"": {
""column"": ""name""
}

-bash-4.1$ select id, name from test.masking""
+------+---------+
| id   | name    |
+------+---------+
|    1 | znffv   |
|    2 | erzb    |
| 1999 | crcr    |
|   99 | N-n&(Z |
+------+---------+

Currently a basic obfuscation routine is used.

Configuration:

    ""rules"": [
        {
            ""obfuscate"": {
                ""column"": ""name""
            }
        }, ...",1,"A new rule option has been added: ‘obfuscate’:
{
""obfuscate"": {
""column"": ""name""
}

-bash-4.1$ select id, name from test.masking""
+------+---------+
| id   | name    |
+------+---------+
|    1 | znffv   |
|    2 | erzb    |
| 1999 | crcr    |
|   99 | N-n&(Z |
+------+---------+

Currently a basic obfuscation routine is used.

Configuration:

    ""rules"": [
        {
            ""obfuscate"": {
                ""column"": ""name""
            }
        }, ..."
2987,MXS-1306,MXS,Massimiliano Pinto,97752,2017-07-18 09:23:13,"New output follows the new algorithm:

name = 'remo'

++------+
+| name |
++------+
+| $-~) |
++------+

Only printable ASCII chars in output, range 32 .. 126,  from a nonreversible obfuscation method.
",2,"New output follows the new algorithm:

name = 'remo'

++------+
+| name |
++------+
+| $-~) |
++------+

Only printable ASCII chars in output, range 32 .. 126,  from a nonreversible obfuscation method.
"
2988,MXS-1306,MXS,Massimiliano Pinto,97753,2017-07-18 09:24:09,"Nw obfuscation method, tests and code review",3,"Nw obfuscation method, tests and code review"
2989,MXS-1306,MXS,Dipti Joshi,97822,2017-07-20 06:33:39,[~Massimiliano Pinto] Where can I find user documentation on this ?,4,[~Massimiliano Pinto] Where can I find user documentation on this ?
2990,MXS-1306,MXS,Massimiliano Pinto,97824,2017-07-20 06:36:09,"Current documentation for partial masking and obfuscation is here:

https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Filters/Masking.md",5,"Current documentation for partial masking and obfuscation is here:

URL"
2991,MXS-1315,MXS,markus makela,108905,2018-03-26 11:25:05,Would make sense as a generic filter mechanism.,1,Would make sense as a generic filter mechanism.
2992,MXS-1315,MXS,Reiner Keller,113819,2018-07-11 19:27:33,"Yes, best like common host resolution used in MariaDB for core and all modules where needed.

I got e.g. with {{MariaDB MaxScale 2.2.11}} this error while trying to connect with IPv4 address:
{code}
ERROR 1045 (28000): Access denied for user 'user'@'::ffff:172.16.30.49' (using password: YES){code}

Maxscale shows me this error:
* while I use normally GRANT 'user'@'172.16.0.0/255.240.0.0';
{code}
error  : [MySQLAuth] Unrecognized IP-bytes in host/mask-combination. Merge incomplete: 172.16.0.0/255.240.0.0
{code}
* and for tests alreday simplified to 'user'@'172.16.%.%;
{code}
warning: [MySQLAuth] Read-Only-Service-testing: login attempt for user 'user'@[::ffff:172.16.30.49]:51204, authentication failed. User not found.
{code}

I used for testing masking read connects these modules:
* dbfwfilter
* masking
* (topfilter)
* (mariadbmon)

EDIT: seems also related to MXS-1827",2,"Yes, best like common host resolution used in MariaDB for core and all modules where needed.

I got e.g. with {{MariaDB MaxScale 2.2.11}} this error while trying to connect with IPv4 address:
{code}
ERROR 1045 (28000): Access denied for user 'user'@'::ffff:172.16.30.49' (using password: YES){code}

Maxscale shows me this error:
* while I use normally GRANT 'user'@'172.16.0.0/255.240.0.0';
{code}
error  : [MySQLAuth] Unrecognized IP-bytes in host/mask-combination. Merge incomplete: 172.16.0.0/255.240.0.0
{code}
* and for tests alreday simplified to 'user'@'172.16.%.%;
{code}
warning: [MySQLAuth] Read-Only-Service-testing: login attempt for user 'user'@[::ffff:172.16.30.49]:51204, authentication failed. User not found.
{code}

I used for testing masking read connects these modules:
* dbfwfilter
* masking
* (topfilter)
* (mariadbmon)

EDIT: seems also related to MXS-1827"
2993,MXS-1315,MXS,markus makela,115352,2018-08-16 06:10:53,[~Reiner030] I think your problem might be more about MXS-1827.,3,[~Reiner030] I think your problem might be more about MXS-1827.
2994,MXS-1317,MXS,markus makela,97630,2017-07-14 00:49:05,This is something we can definitely change for the next major version as the original default value of 10 seconds results is just too long in most cases. The estimated worst-case for the time it takes to detect a failure is around 15 seconds when the connector timeouts are taken into account. By changing the default value to one second we should end up with a worst-case time of about 4 seconds for unresponsive servers.,1,This is something we can definitely change for the next major version as the original default value of 10 seconds results is just too long in most cases. The estimated worst-case for the time it takes to detect a failure is around 15 seconds when the connector timeouts are taken into account. By changing the default value to one second we should end up with a worst-case time of about 4 seconds for unresponsive servers.
2995,MXS-1317,MXS,markus makela,98113,2017-07-31 03:07:11,The default monitor interval is now 2 seconds.,2,The default monitor interval is now 2 seconds.
2996,MXS-1331,MXS,markus makela,98442,2017-08-09 09:19:06,Added {{cluster diff}} and {{cluster sync}} commands and tests for them.,1,Added {{cluster diff}} and {{cluster sync}} commands and tests for them.
2997,MXS-1333,MXS,Valerii Kravchuk,98280,2017-08-04 11:49:55,Millisecond precision would be enough.,1,Millisecond precision would be enough.
2998,MXS-1343,MXS,Massimiliano Pinto,98505,2017-08-10 07:52:13,"Added a new option: ""slave_hostname""

The router can optionally identify itself to the master using a custom hostname.
The specified hostname can be seen in the master via
`SHOW SLAVE HOSTS` command.
The default is not to send any hostname string during registration.

slave_hostname=maxscale-blr-1.mydomain.net",1,"Added a new option: ""slave_hostname""

The router can optionally identify itself to the master using a custom hostname.
The specified hostname can be seen in the master via
`SHOW SLAVE HOSTS` command.
The default is not to send any hostname string during registration.

slave_hostname=maxscale-blr-1.mydomain.net"
2999,MXS-1344,MXS,Massimiliano Pinto,99134,2017-08-23 10:16:00,"MySQL monitor can now detect a replication setup with Binlog Server.


If 'master_id' is not set in Binlog Server configuration, then there is no need to add the server name to the monitor server list.

If is set this is the needed configuration:

Binlog server running on host 192.168.100.100

{{[BinlogServer]
type=service
router=binlogrouter
router_options=server-id=93,....,binlogdir=/x/y/z,master_id=2222}}
...
{{[BinlogServer_Listener]
type=listener
service=BinlogServer
protocol=MySQLClient
port=8808
address=192.168.100.100}}


and R/W split or readconnection router config with monitor is:

{{[MySQL Monitor]
type=monitor
module=mysqlmon
servers=server5,server2,server1,binlog_server
monitor_interval=10000
detect_replication_lag=true

[binlog_server]
type=server
address=192.168.100.100
port=8808
protocol=MySQLBackend
}}
...

{{[Read-Write-Service]
type=service
router=readwritesplit
servers=server5,server2,server1}}

....

{{[Read-Only-Service]
type=service
router=readconnroute
router_options=slave
servers=server5,server2,server1}}



There is no need to add 'binlog_server' to server list in traffic routers: only in the monitor.
",1,"MySQL monitor can now detect a replication setup with Binlog Server.


If 'master_id' is not set in Binlog Server configuration, then there is no need to add the server name to the monitor server list.

If is set this is the needed configuration:

Binlog server running on host 192.168.100.100

{{[BinlogServer]
type=service
router=binlogrouter
router_options=server-id=93,....,binlogdir=/x/y/z,master_id=2222}}
...
{{[BinlogServer_Listener]
type=listener
service=BinlogServer
protocol=MySQLClient
port=8808
address=192.168.100.100}}


and R/W split or readconnection router config with monitor is:

{{[MySQL Monitor]
type=monitor
module=mysqlmon
servers=server5,server2,server1,binlog_server
monitor_interval=10000
detect_replication_lag=true

[binlog_server]
type=server
address=192.168.100.100
port=8808
protocol=MySQLBackend
}}
...

{{[Read-Write-Service]
type=service
router=readwritesplit
servers=server5,server2,server1}}

....

{{[Read-Only-Service]
type=service
router=readconnroute
router_options=slave
servers=server5,server2,server1}}



There is no need to add 'binlog_server' to server list in traffic routers: only in the monitor.
"
3000,MXS-1344,MXS,Massimiliano Pinto,99135,2017-08-23 10:28:25,"MaxAdmin ""show servers"" output

Scenario (1): The 'master_id' is not set in binlog server.

- Master sees as slaves 1, 104 , 93 (including binlog server if it's being monitored)
- Binlog Server has only Running state and shows no slaves (no matter whether it's monitored or not)

=== MASTER ===
Server 0x65b680 (server5)
	Server:                              127.0.0.1
	Status:                              Master, Running
	Protocol:                            MySQLBackend
	Port:                                10124
	Server Version:                      10.1.24-MariaDB
	Node Id:                             10124
	Master Id:                           -1
	Slave Ids:                           1, 104 , 93 
	Repl Depth:                          0

=== BINLOG SERVER (if monitored some values are set) ===
Server 0x65c910 (binlog_server)
	Server:                              127.0.0.1
	Status:                              Running
	Protocol:                            MySQLBackend
	Port:                                8808
	Server Version:                      10.1.17-log
	Node Id:                             93
	Master Id:                           10124
	Slave Ids:                           
	Repl Depth:                          1

=== SLAVE 1 ===
Server 0x660210 (server1)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25235
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             104
	Master Id:                           10124
	Slave Ids:                           
	Repl Depth:                          1
	Slave delay:                         0
	Last Repl Heartbeat:                 Wed Aug 23 12:24:54 2017

=== SLAVE 2 ===
Server 0x65ef10 (server2)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25231
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             1
	Master Id:                           10124
	Slave Ids:                           
	Repl Depth:                          1
	Slave delay:                         0
	Last Repl Heartbeat:                 Wed Aug 23 12:24:54 2017

Scenario (2): The 'master_id' is set to 2222.

- router_options=server-id=93,...,master_id=2222
- Master sees as slaves only 93 binlog server which must be monitored.
- Binlog Server has 'Relay Master, Running' state and shows the slaves: 1, 104

The replication topology clearly shows 3 levels with Binlog Server as master of slaves.

==== MASTER ===
Server 0x65b690 (server5)
	Server:                              127.0.0.1
	Status:                              Master, Running
	Protocol:                            MySQLBackend
	Port:                                10124
	Server Version:                      10.1.24-MariaDB
	Node Id:                             10124
	Master Id:                           -1
	Slave Ids:                           2222
	Repl Depth:                          0
	Last Repl Heartbeat:                 Wed Aug 23 14:15:32 2017

=== BINLOG SERVER ===
Server 0x65c920 (binlog_server)
	Server:                              127.0.0.1
	Status:                              Relay Master, Running
	Protocol:                            MySQLBackend
	Port:                                8808
	Server Version:                      10.1.17-log
	Node Id:                             2222
	Master Id:                           10124
	Slave Ids:                           1, 104 
	Repl Depth:                          1

=== SLAVE 1 ===
Server 0x65ef20 (server2)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25231
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             1
	Master Id:                           2222
	Slave Ids:                           
	Repl Depth:                          2

=== SLAVE 2 ===
Server 0x660220 (server1)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25235
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             104
	Master Id:                           2222
	Slave Ids:                           
	Repl Depth:                          2
	Slave delay:                         0
	Last Repl Heartbeat:                 Wed Aug 23 14:15:22 2017


",2,"MaxAdmin ""show servers"" output

Scenario (1): The 'master_id' is not set in binlog server.

- Master sees as slaves 1, 104 , 93 (including binlog server if it's being monitored)
- Binlog Server has only Running state and shows no slaves (no matter whether it's monitored or not)

=== MASTER ===
Server 0x65b680 (server5)
	Server:                              127.0.0.1
	Status:                              Master, Running
	Protocol:                            MySQLBackend
	Port:                                10124
	Server Version:                      10.1.24-MariaDB
	Node Id:                             10124
	Master Id:                           -1
	Slave Ids:                           1, 104 , 93 
	Repl Depth:                          0

=== BINLOG SERVER (if monitored some values are set) ===
Server 0x65c910 (binlog_server)
	Server:                              127.0.0.1
	Status:                              Running
	Protocol:                            MySQLBackend
	Port:                                8808
	Server Version:                      10.1.17-log
	Node Id:                             93
	Master Id:                           10124
	Slave Ids:                           
	Repl Depth:                          1

=== SLAVE 1 ===
Server 0x660210 (server1)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25235
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             104
	Master Id:                           10124
	Slave Ids:                           
	Repl Depth:                          1
	Slave delay:                         0
	Last Repl Heartbeat:                 Wed Aug 23 12:24:54 2017

=== SLAVE 2 ===
Server 0x65ef10 (server2)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25231
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             1
	Master Id:                           10124
	Slave Ids:                           
	Repl Depth:                          1
	Slave delay:                         0
	Last Repl Heartbeat:                 Wed Aug 23 12:24:54 2017

Scenario (2): The 'master_id' is set to 2222.

- router_options=server-id=93,...,master_id=2222
- Master sees as slaves only 93 binlog server which must be monitored.
- Binlog Server has 'Relay Master, Running' state and shows the slaves: 1, 104

The replication topology clearly shows 3 levels with Binlog Server as master of slaves.

==== MASTER ===
Server 0x65b690 (server5)
	Server:                              127.0.0.1
	Status:                              Master, Running
	Protocol:                            MySQLBackend
	Port:                                10124
	Server Version:                      10.1.24-MariaDB
	Node Id:                             10124
	Master Id:                           -1
	Slave Ids:                           2222
	Repl Depth:                          0
	Last Repl Heartbeat:                 Wed Aug 23 14:15:32 2017

=== BINLOG SERVER ===
Server 0x65c920 (binlog_server)
	Server:                              127.0.0.1
	Status:                              Relay Master, Running
	Protocol:                            MySQLBackend
	Port:                                8808
	Server Version:                      10.1.17-log
	Node Id:                             2222
	Master Id:                           10124
	Slave Ids:                           1, 104 
	Repl Depth:                          1

=== SLAVE 1 ===
Server 0x65ef20 (server2)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25231
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             1
	Master Id:                           2222
	Slave Ids:                           
	Repl Depth:                          2

=== SLAVE 2 ===
Server 0x660220 (server1)
	Server:                              127.0.0.1
	Status:                              Slave, Running
	Protocol:                            MySQLBackend
	Port:                                25235
	Server Version:                      10.0.21-MariaDB-log
	Node Id:                             104
	Master Id:                           2222
	Slave Ids:                           
	Repl Depth:                          2
	Slave delay:                         0
	Last Repl Heartbeat:                 Wed Aug 23 14:15:22 2017


"
3001,MXS-1350,MXS,markus makela,145455,2020-03-02 08:46:40,Changed to a bug as it's more of a limitation than a plain task.,1,Changed to a bug as it's more of a limitation than a plain task.
3002,MXS-1350,MXS,Johan Wikman,197870,2021-08-30 05:52:08,"Changed to _New Feature_ as this is listed in the limitations.
",2,"Changed to _New Feature_ as this is listed in the limitations.
"
3003,MXS-1350,MXS,markus makela,203934,2021-10-28 13:23:42,"Treating {{XA START}} and {{XA END}} as normal transactions allows minimal changes to be done while still fixing the problem of queries in XA transactions not being routed to the correct types of servers. However, this does not change the problem that currently exists with the real transaction state on the databse and the perceived one in MaxScale: if one executes a {{START TRANSACTION}} inside of an XA transaction, MaxScale will think that a transaction is open even if it wasn't opened on the database due to the XA transaction already being open.",3,"Treating {{XA START}} and {{XA END}} as normal transactions allows minimal changes to be done while still fixing the problem of queries in XA transactions not being routed to the correct types of servers. However, this does not change the problem that currently exists with the real transaction state on the databse and the perceived one in MaxScale: if one executes a {{START TRANSACTION}} inside of an XA transaction, MaxScale will think that a transaction is open even if it wasn't opened on the database due to the XA transaction already being open."
3004,MXS-1354,MXS,markus makela,98992,2017-08-21 03:43:57,The admin interface users are now divided into two groups: basic and admin users. The basic users can execute operations that only read data whereas the admin users can perform all operations. This appears to be an adequate solution for the time being unless a more finer grained system is deemed necessary.,1,The admin interface users are now divided into two groups: basic and admin users. The basic users can execute operations that only read data whereas the admin users can perform all operations. This appears to be an adequate solution for the time being unless a more finer grained system is deemed necessary.
3005,MXS-1355,MXS,Timofey Turenko,99201,2017-08-24 10:13:50,merged to develop,1,merged to develop
3006,MXS-1386,MXS,markus makela,128467,2019-05-28 09:24:32,Partially done in 2.4 with the refactoring of session creation.,1,Partially done in 2.4 with the refactoring of session creation.
3007,MXS-1394,MXS,Kolbe Kegel,99741,2017-09-06 21:19:55,"Oops, this is not about using a new key, rather it's about using a newer hash algorithm to sign the repositories. I think this was enough to get reprepro to use the correct hashing algorithm:
{noformat}
$ cat >> ~/.gnupg/gpg.conf << EOF
# Prefer better digests for signing.
personal-digest-preferences SHA512 SHA384 SHA256 SHA224
EOF
{noformat}

If you're using something other than reprepro, you might need to do something else ...

I tested the repository generated this way on Debian 7 (wheezy) as well as Debian 9 (stretch), so it works even with older OSs.",1,"Oops, this is not about using a new key, rather it's about using a newer hash algorithm to sign the repositories. I think this was enough to get reprepro to use the correct hashing algorithm:
{noformat}
$ cat >> ~/.gnupg/gpg.conf << EOF
# Prefer better digests for signing.
personal-digest-preferences SHA512 SHA384 SHA256 SHA224
EOF
{noformat}

If you're using something other than reprepro, you might need to do something else ...

I tested the repository generated this way on Debian 7 (wheezy) as well as Debian 9 (stretch), so it works even with older OSs."
3008,MXS-1417,MXS,Oli Sennhauser,100231,2017-09-15 07:29:42,"removing a server as markus suggested does not help and shows the same ugly behaviour...

root@mariadb-10.2 [(none)] SQL> SELECT @@hostname, @@wsrep_node_name;
ERROR 2003 (HY000): Routing failed. Session is closed.
root@mariadb-10.2 [(none)] SQL> SELECT @@hostname, @@wsrep_node_name;
ERROR 2013 (HY000): Lost connection to MySQL server during query
root@mariadb-10.2 [(none)] SQL> SELECT @@hostname, @@wsrep_node_name;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    13622
Current database: *** NONE ***

",1,"removing a server as markus suggested does not help and shows the same ugly behaviour...

root@mariadb-10.2 [(none)] SQL> SELECT @@hostname, @@wsrep_node_name;
ERROR 2003 (HY000): Routing failed. Session is closed.
root@mariadb-10.2 [(none)] SQL> SELECT @@hostname, @@wsrep_node_name;
ERROR 2013 (HY000): Lost connection to MySQL server during query
root@mariadb-10.2 [(none)] SQL> SELECT @@hostname, @@wsrep_node_name;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    13622
Current database: *** NONE ***

"
3009,MXS-1417,MXS,markus makela,100287,2017-09-16 04:35:47,"The behavior described above is caused by MXS-1418 and will be fixed in 2.1.8. After MXS-1418 is fixed, this will be an usability improvement and fixing it in 2.2 sounds more reasonable.",2,"The behavior described above is caused by MXS-1418 and will be fixed in 2.1.8. After MXS-1418 is fixed, this will be an usability improvement and fixing it in 2.2 sounds more reasonable."
3010,MXS-1417,MXS,Brad Jorgensen,100639,2017-09-22 17:39:46,I am running maxscale 2.1.8 and this is still happening to me.,3,I am running maxscale 2.1.8 and this is still happening to me.
3011,MXS-1417,MXS,markus makela,100649,2017-09-23 05:26:21,[~bradjorgensen] do you mean that setting servers into maintenance closes all active connections? This is still expected as MXS-1418 was only about {{remove server}} command closing connections.,4,[~bradjorgensen] do you mean that setting servers into maintenance closes all active connections? This is still expected as MXS-1418 was only about {{remove server}} command closing connections.
3012,MXS-1417,MXS,markus makela,108903,2018-03-26 11:14:27,"This could be implemented as a MaxCtrl command: {{maxctrl drain server <name>}}

This command would remove the server from all services and wait until the connection count drops to zero. After this, it could set the server into maintenance mode and add it back to the services.",5,"This could be implemented as a MaxCtrl command: {{maxctrl drain server }}

This command would remove the server from all services and wait until the connection count drops to zero. After this, it could set the server into maintenance mode and add it back to the services."
3013,MXS-143,MXS,Timofey Turenko,71425,2015-05-26 09:59:02,"test:
- execute all existing tests with persistent connections on in the configuration (with different limit of connections)

",1,"test:
- execute all existing tests with persistent connections on in the configuration (with different limit of connections)

"
3014,MXS-143,MXS,Dipti Joshi,71426,2015-05-26 10:00:30,"[~tturenko] Preliminary Test Case

(1) Set the configuration parameter(xyz) to 0 - MaxScale Should not make any connection or report error
(2) Set the configuration parameter(xyz) to 1 - MaxScale should only make one connection to backend for such services 
(3) Set the configuration oaremter (xyz) to 10 - MaxScale should make up to 10 persitent connections to backend for such service",2,"[~tturenko] Preliminary Test Case

(1) Set the configuration parameter(xyz) to 0 - MaxScale Should not make any connection or report error
(2) Set the configuration parameter(xyz) to 1 - MaxScale should only make one connection to backend for such services 
(3) Set the configuration oaremter (xyz) to 10 - MaxScale should make up to 10 persitent connections to backend for such service"
3015,MXS-1430,MXS,Massimiliano Pinto,101087,2017-10-03 13:21:06,"Slides updated:
- obfuscate rule
- replace rule with match

Examples added",1,"Slides updated:
- obfuscate rule
- replace rule with match

Examples added"
3016,MXS-1438,MXS,markus makela,101801,2017-10-23 17:41:11,"Configure slave heartbeat to one second
{code:sql}
stop slave;
change master to master_heartbeat_period=1;
start slave;
{code}

Monitor that slave heartbeat count is not increasing
{code:sql}
show status like 'slave_received_heartbeats';
{code}",1,"Configure slave heartbeat to one second
{code:sql}
stop slave;
change master to master_heartbeat_period=1;
start slave;
{code}

Monitor that slave heartbeat count is not increasing
{code:sql}
show status like 'slave_received_heartbeats';
{code}"
3017,MXS-1441,MXS,Johan Wikman,101080,2017-10-03 11:01:51,"There is now three new configuration entries for the MySQL Monitor:
* {{switchover}}, a boolean using which the switchover behaviour can be enabled,
* {{switchover_timeout}}, an integer using which the switchover timeout can be enabled, and
* {{switchover_script}}, a string using which the switchover script can be defined.

If {{switchover_script}} is not specified, then the default script will be used.
",1,"There is now three new configuration entries for the MySQL Monitor:
* {{switchover}}, a boolean using which the switchover behaviour can be enabled,
* {{switchover_timeout}}, an integer using which the switchover timeout can be enabled, and
* {{switchover_script}}, a string using which the switchover script can be defined.

If {{switchover_script}} is not specified, then the default script will be used.
"
3018,MXS-1442,MXS,Johan Wikman,101081,2017-10-03 11:03:14,"As the switchover behaviour is exposed as a module command, it is implicitly available in the REST-API.

Mixed by MXS-1441.
",1,"As the switchover behaviour is exposed as a module command, it is implicitly available in the REST-API.

Mixed by MXS-1441.
"
3019,MXS-1443,MXS,Johan Wikman,101082,2017-10-03 11:11:12,"Implemented by MXS-1441
",1,"Implemented by MXS-1441
"
3020,MXS-1446,MXS,markus makela,100866,2017-09-28 16:59:31,"MaxScale now has a global {{passive}} option that enables the passive mode. In this mode no failover scripts are executed.

In addition to this, the launching of the failover script is now implemented but the actual command being executed is still a dummy command.",1,"MaxScale now has a global {{passive}} option that enables the passive mode. In this mode no failover scripts are executed.

In addition to this, the launching of the failover script is now implemented but the actual command being executed is still a dummy command."
3021,MXS-1467,MXS,markus makela,115159,2018-08-12 17:54:14,"Related MCOL issue: https://jira.mariadb.org/browse/MCOL-962
KB documentation on the functions: https://mariadb.com/kb/en/library/columnstore-utility-functions/",1,"Related MCOL issue: URL
KB documentation on the functions: URL"
3022,MXS-1474,MXS,Johan Wikman,101604,2017-10-16 12:51:55,"I'm thinking of adding the following kind of configuration parameter.
----
{{isolation}}
An enumeration option specifying how the cache should behave when transactions that are *not* explicitly _read-only_ are used:

* {{read_committed}}: The cache will return cached data until the first statement that modifies the database. Thereafter all statements are forwarded to the backend.
* {{repeatable_read}}: The cache will never return cached data, but all statements are always forwarded to the backend.

{code}
isolation=repeatable_read
{code}
Default is {{read_committed}}.
----

How does that sound?",1,"I'm thinking of adding the following kind of configuration parameter.
----
{{isolation}}
An enumeration option specifying how the cache should behave when transactions that are *not* explicitly _read-only_ are used:

* {{read_committed}}: The cache will return cached data until the first statement that modifies the database. Thereafter all statements are forwarded to the backend.
* {{repeatable_read}}: The cache will never return cached data, but all statements are always forwarded to the backend.

{code}
isolation=repeatable_read
{code}
Default is {{read_committed}}.
----

How does that sound?"
3023,MXS-1474,MXS,Johan Wikman,101757,2017-10-23 07:37:40,"This issue is in fact not limited to read/write transactions, but applies to read-only as well.

Suppose a transaction is started and something is selected and suppose the result was found from the cache. Then suppose the same thing is selected again, but this time the _ttl_ has been reached and the data is fetched from the server. There is no guarantee that the result will be identical. Thus, if _read_committed_ behaviour is required, then caching cannot be used during transactions.

So, I think I will still introduce that _isolation_  configuration parameter and if its value is set to _repeatable_read_ then no data will be returned from the cache during transactions. It will be populated though, so selects done with _autocommit_ being enabled will be served from the cache.
",2,"This issue is in fact not limited to read/write transactions, but applies to read-only as well.

Suppose a transaction is started and something is selected and suppose the result was found from the cache. Then suppose the same thing is selected again, but this time the _ttl_ has been reached and the data is fetched from the server. There is no guarantee that the result will be identical. Thus, if _read_committed_ behaviour is required, then caching cannot be used during transactions.

So, I think I will still introduce that _isolation_  configuration parameter and if its value is set to _repeatable_read_ then no data will be returned from the cache during transactions. It will be populated though, so selects done with _autocommit_ being enabled will be served from the cache.
"
3024,MXS-1474,MXS,Johan Wikman,102269,2017-10-31 14:16:20,"There is now a new configuration parameter {{cache_in_transactions}} that can take the following values:
* {{never}}: During transactions, nothing is ever returned from the cache. The cache is populated in explicitly read-only transactions. In not explcitly read-only transactions the cache is populated until the first non-SELECT statement.
* {{read_only_transactions}}: During explicitly read-only transactions, the cache is used and populated. In not explicitly read-only transactions the cache is populated until the first non-SELECT statement.
* {{all_transactions}}: During explicitly read-only transactions, the cache is used and populated. In not explicitly read-only transactions the cache is used and populated until the first non-SELECT statement.

With {{read_only_transactions}}, explicitly read only transactions behave as if the isolation level of the server were _read committed_. With {{all_transactions}} all transactions behave as if the isolation level of the server were _read committed_.",3,"There is now a new configuration parameter {{cache_in_transactions}} that can take the following values:
* {{never}}: During transactions, nothing is ever returned from the cache. The cache is populated in explicitly read-only transactions. In not explcitly read-only transactions the cache is populated until the first non-SELECT statement.
* {{read_only_transactions}}: During explicitly read-only transactions, the cache is used and populated. In not explicitly read-only transactions the cache is populated until the first non-SELECT statement.
* {{all_transactions}}: During explicitly read-only transactions, the cache is used and populated. In not explicitly read-only transactions the cache is used and populated until the first non-SELECT statement.

With {{read_only_transactions}}, explicitly read only transactions behave as if the isolation level of the server were _read committed_. With {{all_transactions}} all transactions behave as if the isolation level of the server were _read committed_."
3025,MXS-1475,MXS,Johan Wikman,108494,2018-03-16 12:41:39,"With the following user variables
* {{@maxscale.cache.populate}}
* {{@maxscale.cache.use}}
* {{@maxscale.cache.soft_ttl}}
* {{@maxscale.cache.hard_ttl}}
it is now possible for the client to control the caching behaviour.

For instance:
{code}
SET @maxscale.cache.soft_ttl=600;
SELECT a, b FROM unimportant;
SET @maxscale.cache.soft_tll=60;
SELECT c, d FROM critical;
{code}
",1,"With the following user variables
* {{@maxscale.cache.populate}}
* {{@maxscale.cache.use}}
* {{@maxscale.cache.soft_ttl}}
* {{@maxscale.cache.hard_ttl}}
it is now possible for the client to control the caching behaviour.

For instance:
{code}
SET @maxscale.cache.soft_ttl=600;
SELECT a, b FROM unimportant;
SET @maxscale.cache.soft_tll=60;
SELECT c, d FROM critical;
{code}
"
3026,MXS-1483,MXS,markus makela,135410,2019-10-07 13:20:47,"As the only operations that would benefit from this are mariadbmon operations, it makes more sense to implement this inside the module. MXS-2719 tracks this issue.",1,"As the only operations that would benefit from this are mariadbmon operations, it makes more sense to implement this inside the module. MXS-2719 tracks this issue."
3027,MXS-1487,MXS,Massimiliano Pinto,102622,2017-11-03 08:48:30,"Added a small fix to MySQL 5.7 slave connection:

MXS-1497 ",1,"Added a small fix to MySQL 5.7 slave connection:

MXS-1497 "
3028,MXS-1498,MXS,markus makela,103090,2017-11-13 08:21:54,The connector now supports packaging into DEB and RPM packages.,1,The connector now supports packaging into DEB and RPM packages.
3029,MXS-1505,MXS,markus makela,109514,2018-04-10 09:18:04,This was covered by MXS-1506 and no separate work was required.,1,This was covered by MXS-1506 and no separate work was required.
3030,MXS-1506,MXS,markus makela,109637,2018-04-13 05:44:07,The [{{delayed_retry}}|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Routers/ReadWriteSplit.md#delayed_retry] parameter controls whether delayed query retrying is enabled.,1,The [{{delayed_retry}}|URL parameter controls whether delayed query retrying is enabled.
3031,MXS-1507,MXS,markus makela,110409,2018-05-02 08:19:56,Enabling the [transaction_replay|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Routers/ReadWriteSplit.md#transaction_replay] now allows transactions to be transparently replayed on replacement servers.,1,Enabling the [transaction_replay|URL now allows transactions to be transparently replayed on replacement servers.
3032,MXS-151,MXS,Dipti Joshi,82196,2016-03-22 14:25:04,"[~tturenko], [~johan.wikman] Have we finished this task ?",1,"[~tturenko], [~johan.wikman] Have we finished this task ?"
3033,MXS-151,MXS,Timofey Turenko,82812,2016-04-19 04:33:57,test 'binlog_change_master' added,2,test 'binlog_change_master' added
3034,MXS-1515,MXS,Valerii Kravchuk,116965,2018-09-21 15:29:24,"If anyone cares, I've got the following results from sysbench 1.0.x /usr/share/sysbench/oltp_read_only.lua test with MariaDB 10.2.17 back end, MaxScale 2.2.14 on the same node and all connections for --host=127.0.0.1 to avoid network impact. Port 3308 is MariaDB direct connection, port 4006 is via readconnroute and port 4008 is via readwritesplit:

{noformat}
openxs@ao756:~/dbs/maria10.2$ sysbench --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run                                       sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            128856
        write:                           0
        other:                           18408
        total:                           147264
    transactions:                        9204   (306.78 per sec.)
    queries:                             147264 (4908.48 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0020s
    total number of events:              9204

Latency (ms):
         min:                                  2.60
         avg:                                  3.26
         max:                                 70.61
         95th percentile:                      4.03
         sum:                              29965.06

Threads fairness:
    events (avg/stddev):           9204.0000/0.00
    execution time (avg/stddev):   29.9651/0.00

--

openxs@ao756:~/dbs/maria10.2$ sysbench --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4006 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            83230
        write:                           0
        other:                           11890
        total:                           95120
    transactions:                        5945   (198.13 per sec.)
    queries:                             95120  (3170.08 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0055s
    total number of events:              5945

Latency (ms):
         min:                                  3.93
         avg:                                  5.04
         max:                                 16.39
         95th percentile:                      5.99
         sum:                              29973.73

Threads fairness:
    events (avg/stddev):           5945.0000/0.00
    execution time (avg/stddev):   29.9737/0.00

-- 

openxs@ao756:~/dbs/maria10.2$ sysbench --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            90104
        write:                           0
        other:                           12872
        total:                           102976
    transactions:                        6436   (214.50 per sec.)
    queries:                             102976 (3432.06 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0042s
    total number of events:              6436

Latency (ms):
         min:                                  3.86
         avg:                                  4.66
         max:                                 26.18
         95th percentile:                      5.28
         sum:                              29974.69

Threads fairness:
    events (avg/stddev):           6436.0000/0.00
    execution time (avg/stddev):   29.9747/0.00
{noformat}

I do not see any impact of script's --skip-trx=on parameter, while disabling prepared statements with --db-ps-mode=disable gave me the following:

{noformat}
openxs@ao756:~/dbs/maria10.2$ sysbench --db-ps-mode=disable --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            118888
        write:                           0
        other:                           16984
        total:                           135872
    transactions:                        8492   (283.05 per sec.)
    queries:                             135872 (4528.83 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0016s
    total number of events:              8492

Latency (ms):
         min:                                  2.83
         avg:                                  3.53
         max:                                 10.84
         95th percentile:                      4.41
         sum:                              29966.42

Threads fairness:
    events (avg/stddev):           8492.0000/0.00
    execution time (avg/stddev):   29.9664/0.00

--

openxs@ao756:~/dbs/maria10.2$ sysbench --db-ps-mode=disable --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4006 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            67718
        write:                           0
        other:                           9674
        total:                           77392
    transactions:                        4837   (161.20 per sec.)
    queries:                             77392  (2579.22 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0060s
    total number of events:              4837

Latency (ms):
         min:                                  4.44
         avg:                                  6.20
         max:                                 15.52
         95th percentile:                      7.30
         sum:                              29973.10

Threads fairness:
    events (avg/stddev):           4837.0000/0.00
    execution time (avg/stddev):   29.9731/0.00

--

openxs@ao756:~/dbs/maria10.2$ sysbench --db-ps-mode=disable --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            86128
        write:                           0
        other:                           12304
        total:                           98432
    transactions:                        6152   (205.03 per sec.)
    queries:                             98432  (3280.51 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0051s
    total number of events:              6152

Latency (ms):
         min:                                  4.07
         avg:                                  4.87
         max:                                 15.55
         95th percentile:                      5.57
         sum:                              29977.46

Threads fairness:
    events (avg/stddev):           6152.0000/0.00
    execution time (avg/stddev):   29.9775/0.00
{noformat}

Test was performed on slow netbook with 2 cores.",1,"If anyone cares, I've got the following results from sysbench 1.0.x /usr/share/sysbench/oltp_read_only.lua test with MariaDB 10.2.17 back end, MaxScale 2.2.14 on the same node and all connections for --host=127.0.0.1 to avoid network impact. Port 3308 is MariaDB direct connection, port 4006 is via readconnroute and port 4008 is via readwritesplit:

{noformat}
openxs@ao756:~/dbs/maria10.2$ sysbench --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run                                       sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            128856
        write:                           0
        other:                           18408
        total:                           147264
    transactions:                        9204   (306.78 per sec.)
    queries:                             147264 (4908.48 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0020s
    total number of events:              9204

Latency (ms):
         min:                                  2.60
         avg:                                  3.26
         max:                                 70.61
         95th percentile:                      4.03
         sum:                              29965.06

Threads fairness:
    events (avg/stddev):           9204.0000/0.00
    execution time (avg/stddev):   29.9651/0.00

--

openxs@ao756:~/dbs/maria10.2$ sysbench --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4006 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            83230
        write:                           0
        other:                           11890
        total:                           95120
    transactions:                        5945   (198.13 per sec.)
    queries:                             95120  (3170.08 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0055s
    total number of events:              5945

Latency (ms):
         min:                                  3.93
         avg:                                  5.04
         max:                                 16.39
         95th percentile:                      5.99
         sum:                              29973.73

Threads fairness:
    events (avg/stddev):           5945.0000/0.00
    execution time (avg/stddev):   29.9737/0.00

-- 

openxs@ao756:~/dbs/maria10.2$ sysbench --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            90104
        write:                           0
        other:                           12872
        total:                           102976
    transactions:                        6436   (214.50 per sec.)
    queries:                             102976 (3432.06 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0042s
    total number of events:              6436

Latency (ms):
         min:                                  3.86
         avg:                                  4.66
         max:                                 26.18
         95th percentile:                      5.28
         sum:                              29974.69

Threads fairness:
    events (avg/stddev):           6436.0000/0.00
    execution time (avg/stddev):   29.9747/0.00
{noformat}

I do not see any impact of script's --skip-trx=on parameter, while disabling prepared statements with --db-ps-mode=disable gave me the following:

{noformat}
openxs@ao756:~/dbs/maria10.2$ sysbench --db-ps-mode=disable --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            118888
        write:                           0
        other:                           16984
        total:                           135872
    transactions:                        8492   (283.05 per sec.)
    queries:                             135872 (4528.83 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0016s
    total number of events:              8492

Latency (ms):
         min:                                  2.83
         avg:                                  3.53
         max:                                 10.84
         95th percentile:                      4.41
         sum:                              29966.42

Threads fairness:
    events (avg/stddev):           8492.0000/0.00
    execution time (avg/stddev):   29.9664/0.00

--

openxs@ao756:~/dbs/maria10.2$ sysbench --db-ps-mode=disable --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4006 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            67718
        write:                           0
        other:                           9674
        total:                           77392
    transactions:                        4837   (161.20 per sec.)
    queries:                             77392  (2579.22 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0060s
    total number of events:              4837

Latency (ms):
         min:                                  4.44
         avg:                                  6.20
         max:                                 15.52
         95th percentile:                      7.30
         sum:                              29973.10

Threads fairness:
    events (avg/stddev):           4837.0000/0.00
    execution time (avg/stddev):   29.9731/0.00

--

openxs@ao756:~/dbs/maria10.2$ sysbench --db-ps-mode=disable --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=root --mysql-db=sbtest /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.1.0-2343e4b (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            86128
        write:                           0
        other:                           12304
        total:                           98432
    transactions:                        6152   (205.03 per sec.)
    queries:                             98432  (3280.51 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0051s
    total number of events:              6152

Latency (ms):
         min:                                  4.07
         avg:                                  4.87
         max:                                 15.55
         95th percentile:                      5.57
         sum:                              29977.46

Threads fairness:
    events (avg/stddev):           6152.0000/0.00
    execution time (avg/stddev):   29.9775/0.00
{noformat}

Test was performed on slow netbook with 2 cores."
3035,MXS-1515,MXS,markus makela,117099,2018-09-26 17:09:16,"Results of testing with a direct connection, HAProxy and MaxScale (readconnroute).

{code}
+------------------+
| Direct Connection|
+------------------+

[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=3000 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            430332
        write:                           0
        other:                           61476
        total:                           491808
    transactions:                        30738  (1024.54 per sec.)
    queries:                             491808 (16392.57 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0006s
    total number of events:              30738

Latency (ms):
         min:                                  0.51
         avg:                                  0.97
         max:                                  7.94
         95th percentile:                      1.21
         sum:                              29967.68

Threads fairness:
    events (avg/stddev):           30738.0000/0.00
    execution time (avg/stddev):   29.9677/0.00

+---------+
| HAProxy |
+---------+

[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4012 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            238686
        write:                           0
        other:                           34098
        total:                           272784
    transactions:                        17049  (568.24 per sec.)
    queries:                             272784 (9091.83 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0017s
    total number of events:              17049

Latency (ms):
         min:                                  0.83
         avg:                                  1.76
         max:                                 30.23
         95th percentile:                      2.61
         sum:                              29975.22

Threads fairness:
    events (avg/stddev):           17049.0000/0.00
    execution time (avg/stddev):   29.9752/0.00

+----------+
| MaxScale |
+----------+

[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            234948
        write:                           0
        other:                           33564
        total:                           268512
    transactions:                        16782  (559.36 per sec.)
    queries:                             268512 (8949.79 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0006s
    total number of events:              16782

Latency (ms):
         min:                                  0.97
         avg:                                  1.79
         max:                                  5.43
         95th percentile:                      2.76
         sum:                              29975.47

Threads fairness:
    events (avg/stddev):           16782.0000/0.00
    execution time (avg/stddev):   29.9755/0.00
{code}

HAProxy configuration:

{code}
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

defaults
    mode                    tcp
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

frontend fr_server1
        bind 0.0.0.0:4012
        default_backend server1

backend server1
        server srv1 127.0.0.1:3000 maxconn 2048
{code} 

maxscale.cnf

{code}
[maxscale]
threads=4

[server1]
type=server
address=127.0.0.1
port=3000
protocol=MariaDBBackend

[MariaDB-Monitor]
type=monitor
module=mariadbmon
servers=server1
user=maxuser
password=maxpwd
monitor_interval=10000

[Read-Connection-Router]
type=service
router=readconnroute
router_options=running
servers=server1
user=maxuser
password=maxpwd

[Read-Connection-Listener]
type=listener
service=Read-Connection-Router
protocol=MariaDBClient
port=4008
{code}
",2,"Results of testing with a direct connection, HAProxy and MaxScale (readconnroute).

{code}
+------------------+
| Direct Connection|
+------------------+

[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=3000 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            430332
        write:                           0
        other:                           61476
        total:                           491808
    transactions:                        30738  (1024.54 per sec.)
    queries:                             491808 (16392.57 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0006s
    total number of events:              30738

Latency (ms):
         min:                                  0.51
         avg:                                  0.97
         max:                                  7.94
         95th percentile:                      1.21
         sum:                              29967.68

Threads fairness:
    events (avg/stddev):           30738.0000/0.00
    execution time (avg/stddev):   29.9677/0.00

+---------+
| HAProxy |
+---------+

[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4012 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            238686
        write:                           0
        other:                           34098
        total:                           272784
    transactions:                        17049  (568.24 per sec.)
    queries:                             272784 (9091.83 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0017s
    total number of events:              17049

Latency (ms):
         min:                                  0.83
         avg:                                  1.76
         max:                                 30.23
         95th percentile:                      2.61
         sum:                              29975.22

Threads fairness:
    events (avg/stddev):           17049.0000/0.00
    execution time (avg/stddev):   29.9752/0.00

+----------+
| MaxScale |
+----------+

[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            234948
        write:                           0
        other:                           33564
        total:                           268512
    transactions:                        16782  (559.36 per sec.)
    queries:                             268512 (8949.79 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0006s
    total number of events:              16782

Latency (ms):
         min:                                  0.97
         avg:                                  1.79
         max:                                  5.43
         95th percentile:                      2.76
         sum:                              29975.47

Threads fairness:
    events (avg/stddev):           16782.0000/0.00
    execution time (avg/stddev):   29.9755/0.00
{code}

HAProxy configuration:

{code}
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

defaults
    mode                    tcp
    log                     global
    option                  URL
    option                  dontlognull
    option URL
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout URL    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout URL 10s
    timeout check           10s
    maxconn                 3000

frontend fr_server1
        bind 0.0.0.0:4012
        default_backend server1

backend server1
        server srv1 127.0.0.1:3000 maxconn 2048
{code} 

maxscale.cnf

{code}
[maxscale]
threads=4

[server1]
type=server
address=127.0.0.1
port=3000
protocol=MariaDBBackend

[MariaDB-Monitor]
type=monitor
module=mariadbmon
servers=server1
user=maxuser
password=maxpwd
monitor_interval=10000

[Read-Connection-Router]
type=service
router=readconnroute
router_options=running
servers=server1
user=maxuser
password=maxpwd

[Read-Connection-Listener]
type=listener
service=Read-Connection-Router
protocol=MariaDBClient
port=4008
{code}
"
3036,MXS-1515,MXS,markus makela,117117,2018-09-27 06:33:15,"Results with {{socat TCP4-LISTEN:4008,reuseaddr,fork,nodelay TCP4:127.0.0.1:3000,nodelay}}:

{code}
[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            284494
        write:                           0
        other:                           40642
        total:                           325136
    transactions:                        20321  (677.32 per sec.)
    queries:                             325136 (10837.11 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0008s
    total number of events:              20321

Latency (ms):
         min:                                  0.84
         avg:                                  1.48
         max:                                  7.23
         95th percentile:                      2.35
         sum:                              29976.13

Threads fairness:
    events (avg/stddev):           20321.0000/0.00
    execution time (avg/stddev):   29.9761/0.00
{code}

Compared to the result we get when we eliminate one TCP connection by replacing it with a UNIX domain socket connection.

{code}
[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-socket=/home/markusjm/build-2.2/rconn.sock --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            325038
        write:                           0
        other:                           46434
        total:                           371472
    transactions:                        23217  (773.85 per sec.)
    queries:                             371472 (12381.68 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0005s
    total number of events:              23217

Latency (ms):
         min:                                  0.72
         avg:                                  1.29
         max:                                  6.78
         95th percentile:                      1.89
         sum:                              29972.00

Threads fairness:
    events (avg/stddev):           23217.0000/0.00
    execution time (avg/stddev):   29.9720/0.00
{code}

The results above were achieved with the following {{maxscale.cnf}}.

{code}
[maxscale]
threads=4

[server1]
type=server
address=127.0.0.1
port=3000
protocol=MariaDBBackend

[MariaDB-Monitor]
type=monitor
module=mariadbmon
servers=server1
user=maxuser
password=maxpwd
monitor_interval=10000

[Read-Connection-Router]
type=service
router=readconnroute
router_options=running
servers=server1
user=maxuser
password=maxpwd

[Read-Connection-Listener]
type=listener
service=Read-Connection-Router
protocol=MariaDBClient
socket=/home/markusjm/build-2.2/rconn.sock
{code}",3,"Results with {{socat TCP4-LISTEN:4008,reuseaddr,fork,nodelay TCP4:127.0.0.1:3000,nodelay}}:

{code}
[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-host=127.0.0.1 --mysql-port=4008 --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            284494
        write:                           0
        other:                           40642
        total:                           325136
    transactions:                        20321  (677.32 per sec.)
    queries:                             325136 (10837.11 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0008s
    total number of events:              20321

Latency (ms):
         min:                                  0.84
         avg:                                  1.48
         max:                                  7.23
         95th percentile:                      2.35
         sum:                              29976.13

Threads fairness:
    events (avg/stddev):           20321.0000/0.00
    execution time (avg/stddev):   29.9761/0.00
{code}

Compared to the result we get when we eliminate one TCP connection by replacing it with a UNIX domain socket connection.

{code}
[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-socket=/home/markusjm/build-2.2/rconn.sock --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            325038
        write:                           0
        other:                           46434
        total:                           371472
    transactions:                        23217  (773.85 per sec.)
    queries:                             371472 (12381.68 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0005s
    total number of events:              23217

Latency (ms):
         min:                                  0.72
         avg:                                  1.29
         max:                                  6.78
         95th percentile:                      1.89
         sum:                              29972.00

Threads fairness:
    events (avg/stddev):           23217.0000/0.00
    execution time (avg/stddev):   29.9720/0.00
{code}

The results above were achieved with the following {{maxscale.cnf}}.

{code}
[maxscale]
threads=4

[server1]
type=server
address=127.0.0.1
port=3000
protocol=MariaDBBackend

[MariaDB-Monitor]
type=monitor
module=mariadbmon
servers=server1
user=maxuser
password=maxpwd
monitor_interval=10000

[Read-Connection-Router]
type=service
router=readconnroute
router_options=running
servers=server1
user=maxuser
password=maxpwd

[Read-Connection-Listener]
type=listener
service=Read-Connection-Router
protocol=MariaDBClient
socket=/home/markusjm/build-2.2/rconn.sock
{code}"
3037,MXS-1515,MXS,markus makela,117161,2018-09-27 19:05:38,"Results from {{socat UNIX-LISTEN:$PWD/mysql.sock,fork TCP4:127.0.0.1:3000,nodelay}}.

{code}
[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-socket=/home/markusjm/MaxScale/mysql.sock --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            317912
        write:                           0
        other:                           45416
        total:                           363328
    transactions:                        22708  (756.86 per sec.)
    queries:                             363328 (12109.82 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0013s
    total number of events:              22708

Latency (ms):
         min:                                  0.65
         avg:                                  1.32
         max:                                 67.20
         95th percentile:                      1.96
         sum:                              29974.78

Threads fairness:
    events (avg/stddev):           22708.0000/0.00
    execution time (avg/stddev):   29.9748/0.00
{code}",4,"Results from {{socat UNIX-LISTEN:$PWD/mysql.sock,fork TCP4:127.0.0.1:3000,nodelay}}.

{code}
[markusjm@monolith sysbench]$ sysbench --db-driver=mysql --table-size=1000000 --threads=1 --time=30 --mysql-socket=/home/markusjm/MaxScale/mysql.sock --mysql-user=maxuser --mysql-password=maxpwd --mysql-db=test /usr/share/sysbench/oltp_read_only.lua run
sysbench 1.0.12 (using system LuaJIT 2.1.0-beta3)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            317912
        write:                           0
        other:                           45416
        total:                           363328
    transactions:                        22708  (756.86 per sec.)
    queries:                             363328 (12109.82 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          30.0013s
    total number of events:              22708

Latency (ms):
         min:                                  0.65
         avg:                                  1.32
         max:                                 67.20
         95th percentile:                      1.96
         sum:                              29974.78

Threads fairness:
    events (avg/stddev):           22708.0000/0.00
    execution time (avg/stddev):   29.9748/0.00
{code}"
3038,MXS-152,MXS,Timofey Turenko,71140,2015-05-15 16:30:40,"two tasks:

- Adding support to promote a slave to new master without touching any slaves: STOP SLAVE, START SLAVE
- Adding support to promote a slave to new master without touching any slaves:CHANGE MASTER",1,"two tasks:

- Adding support to promote a slave to new master without touching any slaves: STOP SLAVE, START SLAVE
- Adding support to promote a slave to new master without touching any slaves:CHANGE MASTER"
3039,MXS-153,MXS,Dipti Joshi,82197,2016-03-22 14:25:25,"[~tturenko], [~johan.wikman] Have we finished this task ?",1,"[~tturenko], [~johan.wikman] Have we finished this task ?"
3040,MXS-153,MXS,Timofey Turenko,82813,2016-04-19 04:35:15,tested in the 'binlog_change_master' (master node is blocked by firewall),2,tested in the 'binlog_change_master' (master node is blocked by firewall)
3041,MXS-1550,MXS,markus makela,103762,2017-11-29 08:43:09,The latest network write/read is already tracked for the {{connection_timeout}} feature. It covers both read and write timeouts and controls how long a client can be idle before the connection is closed.,1,The latest network write/read is already tracked for the {{connection_timeout}} feature. It covers both read and write timeouts and controls how long a client can be idle before the connection is closed.
3042,MXS-1550,MXS,dapeng huang,103804,2017-11-29 11:44:46,"connection_timeout and net_write_timeout are different things,

The connection_timeout parameter is used to disconnect sessions to MariaDB
MaxScale that have been idle for too long. The session timeouts are disabled by
default.

The net_write_timeout is number of seconds to wait for a block to be written to a connection before aborting the write.",2,"connection_timeout and net_write_timeout are different things,

The connection_timeout parameter is used to disconnect sessions to MariaDB
MaxScale that have been idle for too long. The session timeouts are disabled by
default.

The net_write_timeout is number of seconds to wait for a block to be written to a connection before aborting the write."
3043,MXS-1550,MXS,markus makela,103807,2017-11-29 12:04:54,"Oh yes, I think we could add a similar mechanism into MaxScale. We already know if network writes are buffered so the only thing to add is to see if they have been buffered for a long time.

We could handle the read timeouts by comparing the last read and write times for a client socket. If a client sends a request the read timestamp will be updated. When data is written to the client socket, the write timestamp is updated. By evaluating {{write_timestamp - read_timestamp > net_read_timeout}}, we will know if a client request is taking a long time.",3,"Oh yes, I think we could add a similar mechanism into MaxScale. We already know if network writes are buffered so the only thing to add is to see if they have been buffered for a long time.

We could handle the read timeouts by comparing the last read and write times for a client socket. If a client sends a request the read timestamp will be updated. When data is written to the client socket, the write timestamp is updated. By evaluating {{write_timestamp - read_timestamp > net_read_timeout}}, we will know if a client request is taking a long time."
3044,MXS-1559,MXS,Johan Wikman,104628,2017-12-18 12:53:22,This is implemented by MXS-1560 and the actual testcase is {{mysqlmon_failover_manual2_2}}.,1,This is implemented by MXS-1560 and the actual testcase is {{mysqlmon_failover_manual2_2}}.
3045,MXS-1565,MXS,Esa Korhonen,104738,2017-12-20 08:34:46,"The scheme described above wont work with the way mysqlmonitor currently handles downed servers. When a slave goes down, it will lose the [Slave] status. If it comes back online when the master is down, the slave status is not restored, and failover cannot happen without at least one [Slave]. Even if it could, there would be a race, since the servers would not come online at the exact same moment. What may happen is that one slave comes online first, failover is performed on that one with no regards to the others.
    Instead, the following steps are taken:
    - 1 master, 3 slaves
    - stop maxscale so it does not autorejoin later on
    - stop & reset slave on servers 3 & 4
    - add data to server 4
    - restart maxscale, check that server 3 is rejoined but not server 4
    - manually set server 1 to replicate from server 4, creating a relay master
    - check that servers 2 & 3 are redirected, making server 1 just a slave
    - switchover master to server 1, check that it's the master
",1,"The scheme described above wont work with the way mysqlmonitor currently handles downed servers. When a slave goes down, it will lose the [Slave] status. If it comes back online when the master is down, the slave status is not restored, and failover cannot happen without at least one [Slave]. Even if it could, there would be a race, since the servers would not come online at the exact same moment. What may happen is that one slave comes online first, failover is performed on that one with no regards to the others.
    Instead, the following steps are taken:
    - 1 master, 3 slaves
    - stop maxscale so it does not autorejoin later on
    - stop & reset slave on servers 3 & 4
    - add data to server 4
    - restart maxscale, check that server 3 is rejoined but not server 4
    - manually set server 1 to replicate from server 4, creating a relay master
    - check that servers 2 & 3 are redirected, making server 1 just a slave
    - switchover master to server 1, check that it's the master
"
3046,MXS-1598,MXS,Dipti Joshi,116828,2018-09-19 15:07:43,Have we tested it [~esa.korhonen]?,1,Have we tested it [~esa.korhonen]?
3047,MXS-1599,MXS,Esa Korhonen,105727,2018-01-16 11:26:36,"Feature added, still needs documentation and test.",1,"Feature added, still needs documentation and test."
3048,MXS-1622,MXS,markus makela,107106,2018-02-09 20:40:17,Implemented as [{{servers_no_promotion}}|https://github.com/mariadb-corporation/MaxScale/blob/2.2/Documentation/Monitors/MariaDB-Monitor.md#servers_no_promotion].,1,Implemented as [{{servers_no_promotion}}|URL
3049,MXS-1632,MXS,markus makela,116795,2018-09-19 07:50:04,Added counts for read and write queries as well as the total sum partitioned by server to readwritesplit.,1,Added counts for read and write queries as well as the total sum partitioned by server to readwritesplit.
3050,MXS-1635,MXS,Johan Wikman,106590,2018-01-31 12:52:00,"[~kjoiner] Can the local address to be used be a global property of MaxScale or should it be possible to specify it separately for each server.

If the former is acceptable, then it can be provided in a maintenance release. If the latter is necessary then 2.3 is the earliest possible release.",1,"[~kjoiner] Can the local address to be used be a global property of MaxScale or should it be possible to specify it separately for each server.

If the former is acceptable, then it can be provided in a maintenance release. If the latter is necessary then 2.3 is the earliest possible release."
3051,MXS-1635,MXS,brian,106591,2018-01-31 12:54:12,"Hi Johan

The former would be good for our use case.

Thanks
Brian
",2,"Hi Johan

The former would be good for our use case.

Thanks
Brian
"
3052,MXS-1635,MXS,Johan Wikman,106709,2018-02-02 14:34:56,"There is now a global configuration item {{local_address}} using which the address/interface an outgoing socket should be bound to, can be specified.",3,"There is now a global configuration item {{local_address}} using which the address/interface an outgoing socket should be bound to, can be specified."
3053,MXS-1638,MXS,Geoff Montee,114223,2018-07-19 16:15:26,It doesn't look like the GitHub documentation changes have been synced to the KB yet. When do you expect that will happen?,1,It doesn't look like the GitHub documentation changes have been synced to the KB yet. When do you expect that will happen?
3054,MXS-1639,MXS,Johan Wikman,109857,2018-04-18 10:32:24,A limited version will be made available in 2.2.5.  A more versatile one in 2.3.,1,A limited version will be made available in 2.2.5.  A more versatile one in 2.3.
3055,MXS-1639,MXS,Richard Lane,109874,2018-04-18 16:09:15,"I assume that the ""limited"" version will still leave no windows where the repliation_ignore_tables would not be done before replication starts when a server goes Master?",2,"I assume that the ""limited"" version will still leave no windows where the repliation_ignore_tables would not be done before replication starts when a server goes Master?"
3056,MXS-1639,MXS,Johan Wikman,110213,2018-04-26 07:30:42,"_Limited_ meant only that we will not make the functionality very configurable or provide means for handling things intelligently in case some intermediate steps in the executed SQL statements fail.

Sorry for the long delay in the answer.
",3,"_Limited_ meant only that we will not make the functionality very configurable or provide means for handling things intelligently in case some intermediate steps in the executed SQL statements fail.

Sorry for the long delay in the answer.
"
3057,MXS-1656,MXS,Esa Korhonen,107023,2018-02-08 09:05:23,"Adding anything to _list servers_ is problematic for technical reasons as well as for formatting. Instead, for a quick fix, we will add gtid_current_pos to _show monitors_.",1,"Adding anything to _list servers_ is problematic for technical reasons as well as for formatting. Instead, for a quick fix, we will add gtid_current_pos to _show monitors_."
3058,MXS-1656,MXS,markus makela,107105,2018-02-09 20:37:07,This can be added to MaxCtrl quite easily. I'll assign this to myself and add it there.,2,This can be added to MaxCtrl quite easily. I'll assign this to myself and add it there.
3059,MXS-1656,MXS,Johan Wikman,107251,2018-02-13 13:51:01,"More complex than what initially thought, moving to 2.2.3.
",3,"More complex than what initially thought, moving to 2.2.3.
"
3060,MXS-1662,MXS,Johan Wikman,107236,2018-02-13 09:20:36,"In the case of maxadmin this would be straightforward, but not so in the case of maxctrl (the REST-API based replacement for maxadmin).",1,"In the case of maxadmin this would be straightforward, but not so in the case of maxctrl (the REST-API based replacement for maxadmin)."
3061,MXS-1662,MXS,Johan Wikman,107865,2018-03-05 08:20:05,"[~dshjoshi], could you have a look at this.

Giving access to maxadmin for specific Linux _group_ access would be doable. However, as maxadmin is being phased out that does not seem like time well spent.

However, making it possible to configure the REST-API of MaxScale to use Active Directory or LDAP for the authentication seems quite sensible and is something that probably would fit in the 2.3 timeframe.",2,"[~dshjoshi], could you have a look at this.

Giving access to maxadmin for specific Linux _group_ access would be doable. However, as maxadmin is being phased out that does not seem like time well spent.

However, making it possible to configure the REST-API of MaxScale to use Active Directory or LDAP for the authentication seems quite sensible and is something that probably would fit in the 2.3 timeframe."
3062,MXS-1662,MXS,Dipti Joshi,107876,2018-03-05 13:26:06,"Agree with you [~johan.wikman], we should support LDAP and AD for REST-API - Let us add it as an 2.3 item.",3,"Agree with you [~johan.wikman], we should support LDAP and AD for REST-API - Let us add it as an 2.3 item."
3063,MXS-1662,MXS,Dipti Joshi,111493,2018-05-25 09:49:14,[~johan.wikman]Let us get estimate for this,4,[~johan.wikman]Let us get estimate for this
3064,MXS-1662,MXS,Manjot Singh,114800,2018-08-02 22:36:22,@Dipti Do you have an update on this?,5,@Dipti Do you have an update on this?
3065,MXS-1662,MXS,Johan Wikman,115974,2018-09-04 13:00:24,"LDAP support for REST-API authorization is not entirelly trivial.

I think the basic use-case is that you, using maxctrl or the REST-API directly using curl, should be able to get your authorization provided from LDAP (or AD). The problem is that any back and forth communication between MaxScale and the client is not really possible since the protocol is HTTP.

What could be done is that MaxScale would simply take the provided username and password and use them for logging into an LDAP server to find out the rights of the user. The problem with that approach is that MaxScale would have access to the cleartext password of the user, which from a security perspective is not good. However, I suppose a security conscious organization could setup a dedicated LDAP server just for MaxScale, in which case the situation is security wise basically the same as it currently is.

The estimate is based upon this approach.",6,"LDAP support for REST-API authorization is not entirelly trivial.

I think the basic use-case is that you, using maxctrl or the REST-API directly using curl, should be able to get your authorization provided from LDAP (or AD). The problem is that any back and forth communication between MaxScale and the client is not really possible since the protocol is HTTP.

What could be done is that MaxScale would simply take the provided username and password and use them for logging into an LDAP server to find out the rights of the user. The problem with that approach is that MaxScale would have access to the cleartext password of the user, which from a security perspective is not good. However, I suppose a security conscious organization could setup a dedicated LDAP server just for MaxScale, in which case the situation is security wise basically the same as it currently is.

The estimate is based upon this approach."
3066,MXS-1662,MXS,Johan Wikman,116449,2018-09-11 07:34:45,"Seems like this is not that complex after all, so cut down on the estimate.
",7,"Seems like this is not that complex after all, so cut down on the estimate.
"
3067,MXS-1662,MXS,markus makela,121188,2018-12-27 12:47:27,Some prototype code for this exists on the MXS-1662 branch.,8,Some prototype code for this exists on the MXS-1662 branch.
3068,MXS-168,MXS,Johan Wikman,81831,2016-03-08 13:17:16,"Moved to 1.5 because the necessary functionality - exposing functions of a query - is not available.

Further, the regexp based filtering can to a certain extent be used for filtering out/in functions usage.",1,"Moved to 1.5 because the necessary functionality - exposing functions of a query - is not available.

Further, the regexp based filtering can to a certain extent be used for filtering out/in functions usage."
3069,MXS-168,MXS,Johan Wikman,83843,2016-05-31 11:47:38,Removed fix-version. Need to be decided if and when this feature will be included.,2,Removed fix-version. Need to be decided if and when this feature will be included.
3070,MXS-168,MXS,markus makela,90551,2017-01-12 06:22:10,The dbfwfilter now supports the {{function}} rule type which accepts a list of function names as its parameter.,3,The dbfwfilter now supports the {{function}} rule type which accepts a list of function names as its parameter.
3071,MXS-1685,MXS,markus makela,143397,2020-01-31 10:17:40,Added automatic fetching of the CREATE TABLE if the avrorouter is configured in the direct replication mode.,1,Added automatic fetching of the CREATE TABLE if the avrorouter is configured in the direct replication mode.
3072,MXS-1686,MXS,markus makela,143547,2020-02-03 11:08:02,The new {{kafkacdc}} router module streams data from MariaDB into a Kafka broker.,1,The new {{kafkacdc}} router module streams data from MariaDB into a Kafka broker.
3073,MXS-1686,MXS,wilsonlau,158157,2020-06-28 11:30:07,how to set kafkacdc GTID?,2,how to set kafkacdc GTID?
3074,MXS-1686,MXS,markus makela,158177,2020-06-29 04:19:37,The kafkacdc module starts replication from the beginning. This is an oversight and the GTID should be configurable like it is for other modules that rely on it.,3,The kafkacdc module starts replication from the beginning. This is an oversight and the GTID should be configurable like it is for other modules that rely on it.
3075,MXS-1702,MXS,markus makela,108549,2018-03-18 16:19:29,Rewrote the canonicalization code to use custom code instead of regular expressions. Rough performance measurements point towards a significant performance increase. ,1,Rewrote the canonicalization code to use custom code instead of regular expressions. Rough performance measurements point towards a significant performance increase. 
3076,MXS-1703,MXS,Esa Korhonen,110619,2018-05-07 07:44:08,"Not quite complete, but this is such a wide issue that it's time to mark it complete for now. ",1,"Not quite complete, but this is such a wide issue that it's time to mark it complete for now. "
3077,MXS-1712,MXS,Dipti Joshi,115590,2018-08-23 14:54:27,"The command would be called resetGTID to reflect the intention
",1,"The command would be called resetGTID to reflect the intention
"
3078,MXS-1720,MXS,markus makela,130514,2019-07-04 07:31:32,Is this intended to solve cases when replication lag is too large for efficient use of  {{causal_reads}}? Could [{{max_slave_replication_lag}}|https://mariadb.com/kb/en/mariadb-maxscale-23-readwritesplit/#max_slave_replication_lag] help in these cases?,1,Is this intended to solve cases when replication lag is too large for efficient use of  {{causal_reads}}? Could [{{max_slave_replication_lag}}|URL help in these cases?
3079,MXS-1720,MXS,markus makela,144747,2020-02-20 17:45:12,"Here are a few ways how the GTID extraction part could be implemented:
* Using monitors for GTID retrieval could work but it would be highly reliant on the monitoring interval. 
* The replication stream could be used to extract GTIDs but the overhead of sending everything through MaxScale would far outweight the benefit of faster GTID event delivery.
* Installing some sort of an agent application on the database node would allow an efficient and fast method of delivering GTIDs but this is the most cumbersome solution to apply as separate binaries would have to be installed and managed on each node.
* Writing a server-side plugin that exports GTID events to MaxScale would be a neater solution but it still faces some of the same problems that an agent application would.

Another way to do this would be to extend the causal_reads behavior with dedicated GTID tracking threads for each server. These threads would execute a MASTER_GTID_WAIT for each GTID on the master, effectively synchronizing the slave to a certain point. The GTID waiting could be done in batches to synchronize multiple transactions at the same time. This logical timestamp could then be used in place of the replication lag in readwritesplit.",2,"Here are a few ways how the GTID extraction part could be implemented:
* Using monitors for GTID retrieval could work but it would be highly reliant on the monitoring interval. 
* The replication stream could be used to extract GTIDs but the overhead of sending everything through MaxScale would far outweight the benefit of faster GTID event delivery.
* Installing some sort of an agent application on the database node would allow an efficient and fast method of delivering GTIDs but this is the most cumbersome solution to apply as separate binaries would have to be installed and managed on each node.
* Writing a server-side plugin that exports GTID events to MaxScale would be a neater solution but it still faces some of the same problems that an agent application would.

Another way to do this would be to extend the causal_reads behavior with dedicated GTID tracking threads for each server. These threads would execute a MASTER_GTID_WAIT for each GTID on the master, effectively synchronizing the slave to a certain point. The GTID waiting could be done in batches to synchronize multiple transactions at the same time. This logical timestamp could then be used in place of the replication lag in readwritesplit."
3080,MXS-1720,MXS,markus makela,145146,2020-02-26 15:48:18,Added a preliminary implementation that uses monitors to get the GTID positions.,3,Added a preliminary implementation that uses monitors to get the GTID positions.
3081,MXS-1726,MXS,Timofey Turenko,110412,2018-05-02 08:41:51,"final tests are done (with -02 and -03), regular jobs scheduled, work. Setup of new servers moved to separate JIRA items",1,"final tests are done (with -02 and -03), regular jobs scheduled, work. Setup of new servers moved to separate JIRA items"
3082,MXS-173,MXS,Johan Wikman,93711,2017-04-03 09:57:37,"The {{limit_queries}} functionality should be removed/deprecated in the database firewall filter. All other rules just match or not a statement, which subsequently cause the statement to be either allowed or blocked depending on how the filter is configures. The {{limit_queries}} rule is not really a rule but rule and an action combined into one; it makes no sense to use {{limit_queries}} and {{action=allow}}.",1,"The {{limit_queries}} functionality should be removed/deprecated in the database firewall filter. All other rules just match or not a statement, which subsequently cause the statement to be either allowed or blocked depending on how the filter is configures. The {{limit_queries}} rule is not really a rule but rule and an action combined into one; it makes no sense to use {{limit_queries}} and {{action=allow}}."
3083,MXS-1745,MXS,Esa Korhonen,110418,2018-05-02 09:37:49,The data is saved but not properly used yet.,1,The data is saved but not properly used yet.
3084,MXS-177,MXS,Dipti Joshi,71650,2015-06-02 19:15:35,MXS-122 makes persistent connections to backend database and only uses those connections. There can be upper and lower limit for these connections.,1,MXS-122 makes persistent connections to backend database and only uses those connections. There can be upper and lower limit for these connections.
3085,MXS-177,MXS,martin brampton,71671,2015-06-03 11:00:38,Who has reviewed the feasibility of this?,2,Who has reviewed the feasibility of this?
3086,MXS-177,MXS,Joffrey MICHAIE,71704,2015-06-03 20:51:12,"Hi, thanks for the answers !

The goal is not to refuse applications connections with an error, when the mysql/mariadb connections are fully used.
It is to put them in a backlog, until there is a free slot.

Example similar in other popular lb software:
Global maxcon = 4096
Per backend maxcon = 200

Hope this helps,
Joffrey",3,"Hi, thanks for the answers !

The goal is not to refuse applications connections with an error, when the mysql/mariadb connections are fully used.
It is to put them in a backlog, until there is a free slot.

Example similar in other popular lb software:
Global maxcon = 4096
Per backend maxcon = 200

Hope this helps,
Joffrey"
3087,MXS-177,MXS,martin brampton,71799,2015-06-05 10:26:55,"Thanks for the comment, Joffrey. I can see the attraction in principle, but have concerns over the practical implications in the software. Until the relevant router has been invoked (and there is an increasing number of different routers with different behaviours, plus we have the option of hierarchical routing where one router forwards to another router) MaxScale does not know which backend database server will be involved. It is therefore difficult (at the least) to know how to handle connection buffering prior to routing. Once routing has been involved, there is a quite complex set of data structures in play, and buffering these for later processing does not look straightforward. I am surprised that a feature of this complexity that has been nominated as minor priority would be allocated to a release prior to any evaluation (that I know of) of the volume of work required. The basis for the issue is that MaxScale is a lot more general than typical lb software.",4,"Thanks for the comment, Joffrey. I can see the attraction in principle, but have concerns over the practical implications in the software. Until the relevant router has been invoked (and there is an increasing number of different routers with different behaviours, plus we have the option of hierarchical routing where one router forwards to another router) MaxScale does not know which backend database server will be involved. It is therefore difficult (at the least) to know how to handle connection buffering prior to routing. Once routing has been involved, there is a quite complex set of data structures in play, and buffering these for later processing does not look straightforward. I am surprised that a feature of this complexity that has been nominated as minor priority would be allocated to a release prior to any evaluation (that I know of) of the volume of work required. The basis for the issue is that MaxScale is a lot more general than typical lb software."
3088,MXS-177,MXS,Joffrey MICHAIE,74893,2015-08-21 22:26:38,"Thanks Martin for the analysis.

Maybe it would be great to start with simple things, like limiting connections or implementing this throttling on the listener side ? 

Something like: 

[RW Split Listener]
type=listener
service=RW Split Router
protocol=MySQLClient
port=3306
max_connections=1000
max_extra/throttled_connections=200

Those new parameters would affect connections on the MaxScale backend directly (and indirectly on the MySQL/MariaDB Backends).
When 1000 connections app <--> maxscale are opened, new connections are still accepted (up to 200) but not sent to the router, until a slot frees up in the pool of 1000.

Currently, as far as I experience from the field, most of the maxscale users don't use complex routing/filtering, but simple round-robin or rw split.

What do you think ?

Joffrey",5,"Thanks Martin for the analysis.

Maybe it would be great to start with simple things, like limiting connections or implementing this throttling on the listener side ? 

Something like: 

[RW Split Listener]
type=listener
service=RW Split Router
protocol=MySQLClient
port=3306
max_connections=1000
max_extra/throttled_connections=200

Those new parameters would affect connections on the MaxScale backend directly (and indirectly on the MySQL/MariaDB Backends).
When 1000 connections app  maxscale are opened, new connections are still accepted (up to 200) but not sent to the router, until a slot frees up in the pool of 1000.

Currently, as far as I experience from the field, most of the maxscale users don't use complex routing/filtering, but simple round-robin or rw split.

What do you think ?

Joffrey"
3089,MXS-177,MXS,Johan Wikman,81680,2016-03-01 15:46:19,"Removing fix version of 1.4.
",6,"Removing fix version of 1.4.
"
3090,MXS-177,MXS,Dipti Joshi,82418,2016-03-31 13:55:31,Removed it from 1.5.0 for now. ,7,Removed it from 1.5.0 for now. 
3091,MXS-177,MXS,martin brampton,83824,2016-05-31 08:11:55,"Development has progressed, and there is now logic to handle the imposition of a limit on the number of connections to be accepted by MaxScale on a particular service. The logic is also in place to put a specified number of connections into a queue, after the limit has been reached. The logic does not exist to insert the queued connection requests back into active use. Given timescales, the current aim is to tidy up the limitation on number of connections, but to exclude queuing of connection requests for future development.",8,"Development has progressed, and there is now logic to handle the imposition of a limit on the number of connections to be accepted by MaxScale on a particular service. The logic is also in place to put a specified number of connections into a queue, after the limit has been reached. The logic does not exist to insert the queued connection requests back into active use. Given timescales, the current aim is to tidy up the limitation on number of connections, but to exclude queuing of connection requests for future development."
3092,MXS-177,MXS,martin brampton,83902,2016-06-01 10:07:22,"The limiting functionality is now operational, based on e.g. ""max_connections=25"" in the configuration of a service.  After the specified number of connections have been made, a request for a connection will receive the standard error reply, 1040 Too many connections.

NOTE: without the mysql library, there is no practical way to translate the error message into a local language, it is hard coded as English.",9,"The limiting functionality is now operational, based on e.g. ""max_connections=25"" in the configuration of a service.  After the specified number of connections have been made, a request for a connection will receive the standard error reply, 1040 Too many connections.

NOTE: without the mysql library, there is no practical way to translate the error message into a local language, it is hard coded as English."
3093,MXS-177,MXS,martin brampton,84240,2016-06-14 08:50:32,"Incorrect comparison fixed to stop limit being one higher than specified. The max_connections limit on a service is the functionality to be released in version 2.0. There is currently code under test that queues up requests beyond the connection limit and times them out after the specified limit (subject to the granularity of the housekeeper). Functionality for processing the queue as connections become available requires further work, although the principles are established.",10,"Incorrect comparison fixed to stop limit being one higher than specified. The max_connections limit on a service is the functionality to be released in version 2.0. There is currently code under test that queues up requests beyond the connection limit and times them out after the specified limit (subject to the granularity of the housekeeper). Functionality for processing the queue as connections become available requires further work, although the principles are established."
3094,MXS-1775,MXS,Esa Korhonen,113058,2018-06-26 12:29:50,"When autoselecting a new master during a failover or switchover, the disk space info is taken into account.",1,"When autoselecting a new master during a failover or switchover, the disk space info is taken into account."
3095,MXS-1780,MXS,Johan Wikman,118817,2018-11-07 13:13:46,"This has now been implemented so that the information is provided as part of the session information.
When querying for the session data, the output will contain the following:
{code}
$ curl admin:password@127.0.0.1:8989/v1/sessions/7
{
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/sessions/7""
    },
   ""data"": {
...
        ""attributes"": {
...
            ""queries"": [
                {
                    ""command"": ""COM_QUERY"",
                    ""statement"": ""select @@version_comment limit 1"",
                    ""received"": ""2018-11-07T11:28:22.133"",
                    ""completed"": ""2018-11-07T11:28:22.133"",
                    ""responses"": [
                        {
                            ""server"": ""Server2"",
                            ""duration"": 0
                        }
                    ]
                },
...
}
{code}
where
* {{command}} denotes the protocol command,
* {{statement}} is present if the command is {{COM_QUERY}},
* {{received}} shows the timestamp when MaxScale received the packet,
* {{completed}} is present and shows the timestamp, if MaxScale has received all responses from the servers (may be just one),
* {{responses}} is an array containing the individual server notes (may be just one), where {{server}} identifies the server and {{duration}} the time, expressed in milliseconds, it took before MaxScale received the response.

This information is available if {{retain_last_statements}} has been specified, either in the MaxScale global section or specifically for the service the session relates to.

The _maxctrl_ command {{show sessions}} shows the last statements of the sessions, but not the details.

The feature can be turned on and off at runtime using {{maxctrl alter service}}. E.g.
{code}
maxctrl -u admin -p password alter service RWS retain_last_statements 5
{code}
The command above will cause _new_ sessions related to the service RWS to retain the last 5 statements of the session. Existing sessions are not affected.",1,"This has now been implemented so that the information is provided as part of the session information.
When querying for the session data, the output will contain the following:
{code}
$ curl admin:password@127.0.0.1:8989/v1/sessions/7
{
    ""links"": {
        ""self"": ""URL
    },
   ""data"": {
...
        ""attributes"": {
...
            ""queries"": [
                {
                    ""command"": ""COM_QUERY"",
                    ""statement"": ""select @@version_comment limit 1"",
                    ""received"": ""2018-11-07T11:28:22.133"",
                    ""completed"": ""2018-11-07T11:28:22.133"",
                    ""responses"": [
                        {
                            ""server"": ""Server2"",
                            ""duration"": 0
                        }
                    ]
                },
...
}
{code}
where
* {{command}} denotes the protocol command,
* {{statement}} is present if the command is {{COM_QUERY}},
* {{received}} shows the timestamp when MaxScale received the packet,
* {{completed}} is present and shows the timestamp, if MaxScale has received all responses from the servers (may be just one),
* {{responses}} is an array containing the individual server notes (may be just one), where {{server}} identifies the server and {{duration}} the time, expressed in milliseconds, it took before MaxScale received the response.

This information is available if {{retain_last_statements}} has been specified, either in the MaxScale global section or specifically for the service the session relates to.

The _maxctrl_ command {{show sessions}} shows the last statements of the sessions, but not the details.

The feature can be turned on and off at runtime using {{maxctrl alter service}}. E.g.
{code}
maxctrl -u admin -p password alter service RWS retain_last_statements 5
{code}
The command above will cause _new_ sessions related to the service RWS to retain the last 5 statements of the session. Existing sessions are not affected."
3096,MXS-1782,MXS,markus makela,110874,2018-05-11 13:29:30,All required commands are now implemented in MaxCtrl. Any commands that do not have a MaxCtrl equivalent either are deprecated or should not be in the administrative API in the first place.,1,All required commands are now implemented in MaxCtrl. Any commands that do not have a MaxCtrl equivalent either are deprecated or should not be in the administrative API in the first place.
3097,MXS-1782,MXS,markus makela,110976,2018-05-15 06:16:23,The {{show persistent}} command is not in MaxCtrl as the persisted connections are not exposed to the REST API. Whether the detailed information about each persisted connection should be exposed by the REST API is not clear at this point in time. Exposing them as a part of the servers resource causes very large payloads to be generated if persisted connections are available.,2,The {{show persistent}} command is not in MaxCtrl as the persisted connections are not exposed to the REST API. Whether the detailed information about each persisted connection should be exposed by the REST API is not clear at this point in time. Exposing them as a part of the servers resource causes very large payloads to be generated if persisted connections are available.
3098,MXS-1789,MXS,Timofey Turenko,117156,2018-09-27 16:52:54,"two VM from Sofia are connected via VPN, their creads are loaded into max-tst-01 config",1,"two VM from Sofia are connected via VPN, their creads are loaded into max-tst-01 config"
3099,MXS-1790,MXS,Timofey Turenko,117956,2018-10-16 09:51:03,"performance test executed on -02 and -03, Sofia VMs and on local laptop. Results instability observed in all cases",1,"performance test executed on -02 and -03, Sofia VMs and on local laptop. Results instability observed in all cases"
3100,MXS-1791,MXS,Timofey Turenko,110035,2018-04-23 14:11:12,"final test done, daily performance test pipeline works",1,"final test done, daily performance test pipeline works"
3101,MXS-1792,MXS,Timofey Turenko,109949,2018-04-20 13:08:23,finally tested with real push,1,finally tested with real push
3102,MXS-1794,MXS,Timofey Turenko,110411,2018-05-02 08:39:46,"basic functions work, including matrix view for daily tests (with different number of threads)",1,"basic functions work, including matrix view for daily tests (with different number of threads)"
3103,MXS-1795,MXS,Timofey Turenko,109862,2018-04-18 10:58:25,merged,1,merged
3104,MXS-1797,MXS,Timofey Turenko,110414,2018-05-02 08:55:11,"remove separate 'upgrade test' jobs and *_alone jobs

temporary keep snapshot and core copying jobs (until final CI decision)",1,"remove separate 'upgrade test' jobs and *_alone jobs

temporary keep snapshot and core copying jobs (until final CI decision)"
3105,MXS-1810,MXS,markus makela,109897,2018-04-19 08:55:01,Added transaction checksums and verified that they work.,1,Added transaction checksums and verified that they work.
3106,MXS-1811,MXS,Niclas Antti,111489,2018-05-25 07:52:31,"Logging time here for work on scripting local test/docker, and scripts to generate config for running system test locally, and finally pushing files to vagrant testing.",1,"Logging time here for work on scripting local test/docker, and scripts to generate config for running system test locally, and finally pushing files to vagrant testing."
3107,MXS-1845,MXS,Esa Korhonen,113059,2018-06-26 12:33:03,"Stalling for now, as progress so far has been just on properly detecting the roles in an arbitrary replication graph. Once done, this issue can be handled.",1,"Stalling for now, as progress so far has been just on properly detecting the roles in an arbitrary replication graph. Once done, this issue can be handled."
3108,MXS-1845,MXS,Esa Korhonen,118894,2018-11-08 09:53:19,"Closing for now. Failover/switchover in arbitrary topologies should work. Requires proper testing, though. Rejoin is another issue, as the feature needs to be split into two.",2,"Closing for now. Failover/switchover in arbitrary topologies should work. Requires proper testing, though. Rejoin is another issue, as the feature needs to be split into two."
3109,MXS-1849,MXS,markus makela,111104,2018-05-16 12:34:05,"This can be done by expanding the functionality of the schemarouter. By performing a query similar to the following, all databases and tables on a server can be mapped.
{code:sql}
SELECT table_schema, table_name FROM information_schema.tables WHERE table_schema NOT IN ('information_schema', 'performance_schema', 'mysql');
{code}",1,"This can be done by expanding the functionality of the schemarouter. By performing a query similar to the following, all databases and tables on a server can be mapped.
{code:sql}
SELECT table_schema, table_name FROM information_schema.tables WHERE table_schema NOT IN ('information_schema', 'performance_schema', 'mysql');
{code}"
3110,MXS-185,MXS,Dipti Joshi,74221,2015-08-04 06:44:46,[~markus makela]Can you comment on this please ?,1,[~markus makela]Can you comment on this please ?
3111,MXS-185,MXS,markus makela,74233,2015-08-04 07:16:45,"Should be somewhat easy to implement as long as {{START TRANSACTION READ ONLY}} is used. If not, then it would require some other mechanism to detect read-only transactions, possibly with MaxScale's routing hints.",2,"Should be somewhat easy to implement as long as {{START TRANSACTION READ ONLY}} is used. If not, then it would require some other mechanism to detect read-only transactions, possibly with MaxScale's routing hints."
3112,MXS-185,MXS,Dipti Joshi,75856,2015-09-15 14:46:26,[~markus makela] Since the fixVersion for this feature is 1.4.0 - Please let us make sure we do not start this work until after 1.3.0 is released,3,[~markus makela] Since the fixVersion for this feature is 1.4.0 - Please let us make sure we do not start this work until after 1.3.0 is released
3113,MXS-185,MXS,Johan Wikman,83838,2016-05-31 11:11:36,Removed Fix-Version. Not clear at this point if/when this could be implemented.,4,Removed Fix-Version. Not clear at this point if/when this could be implemented.
3114,MXS-185,MXS,Massimiliano Pinto,91072,2017-01-25 16:39:13,START TRANSACTION READ ONLY is detected and all included statements and COMMIT or ROLLBACK are routed to the same slave server,5,START TRANSACTION READ ONLY is detected and all included statements and COMMIT or ROLLBACK are routed to the same slave server
3115,MXS-1855,MXS,Timofey Turenko,112320,2018-06-12 08:28:04,"- triggering by push tested
- run_test start tested
- build works
- BuildBot configuration structure is clean
- authorization/authentication: is easy via GitHub  ",1,"- triggering by push tested
- run_test start tested
- build works
- BuildBot configuration structure is clean
- authorization/authentication: is easy via GitHub  "
3116,MXS-1868,MXS,Timofey Turenko,117155,2018-09-27 16:49:13,mdbci AppImage deployed on all max-tst-xx.mariadb.com,1,mdbci AppImage deployed on all max-tst-xx.mariadb.com
3117,MXS-1870,MXS,Timofey Turenko,111226,2018-05-18 13:42:05,"Vagrant box changed again, increased memory solved initial problem ",1,"Vagrant box changed again, increased memory solved initial problem "
3118,MXS-1892,MXS,markus makela,126416,2019-04-16 21:25:20,Can probably be done for 2.5.,1,Can probably be done for 2.5.
3119,MXS-1892,MXS,markus makela,156088,2020-06-10 09:06:31,We need the support in the connector in order to test this in maxscale.,2,We need the support in the connector in order to test this in maxscale.
3120,MXS-1892,MXS,markus makela,167058,2020-09-28 12:06:51,This should be done whenever any other protocol extensions are done for 10.6.,3,This should be done whenever any other protocol extensions are done for 10.6.
3121,MXS-1895,MXS,Wagner Bianchi,139907,2019-12-12 12:06:38,"Hello Folks,

I see this feature request now as Router Improvements and I'm happy with that. However, when I opened this JIRA, I was working yet with the old MaxAdmin we can't consider anymore as it is about to be deprecated and removed from the MaxScale world, as per the docs. With the in mind, I would like to propose the same to be implemented for the below MaxCtrl output instead:
 maxctrl: show service readconn-service-master

{code:java}
┌─────────────────────┬─────────────────────────────────────────────────────────────────────────────────────┐
│ Service             │ readconn-service-master                                                             │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Router              │ readconnroute                                                                       │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ State               │ Started                                                                             │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Started At          │ Mon Oct 14 20:24:15 2019                                                            │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Current Connections │ 16                                                                                  │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Total Connections   │ 193735482                                                                           │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Servers             │ db2                                                                                 │
│                     │ db1                                                                                 │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Parameters          │ {                                                                                   │
│                     │     ""router_options"": ""master"",                                                     │
│                     │     ""user"": ""maxusr"",                                                               │
│                     │     ""password"": ""FF2856D24FBAC82A038DC04210E5C7884C60A27218DBC17D85C08E9391E85E13"", │
│                     │     ""passwd"": null,                                                                 │
│                     │     ""enable_root_user"": false,                                                      │
│                     │     ""max_retry_interval"": 3600,                                                     │
│                     │     ""max_connections"": 0,                                                           │
│                     │     ""connection_timeout"": 0,                                                        │
│                     │     ""auth_all_servers"": false,                                                      │
│                     │     ""strip_db_esc"": true,                                                           │
│                     │     ""localhost_match_wildcard_host"": true,                                          │
│                     │     ""version_string"": null,                                                         │
│                     │     ""weightby"": null,                                                               │
│                     │     ""log_auth_warnings"": true,                                                      │
│                     │     ""retry_on_failure"": true,                                                       │
│                     │     ""session_track_trx_state"": false,                                               │
│                     │     ""retain_last_statements"": -1,                                                   │
│                     │     ""session_trace"": 0                                                              │
│                     │ }                                                                                   │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Router Diagnostics  │ {                                                                                   │
│                     │     ""connections"": 193733865,                                                       │
│                     │     ""current_connections"": 16,                                                      │
│                     │     ""queries"": 1585574679                                                           │
│                     │ }                                                                                   │
└─────────────────────┴─────────────────────────────────────────────────────────────────────────────────────┘
{code}
Having individual numbers of routed transactions/queries per server on the Router Options will make it way more complete and able to give better information to who is analyzing traffic patterns.",1,"Hello Folks,

I see this feature request now as Router Improvements and I'm happy with that. However, when I opened this JIRA, I was working yet with the old MaxAdmin we can't consider anymore as it is about to be deprecated and removed from the MaxScale world, as per the docs. With the in mind, I would like to propose the same to be implemented for the below MaxCtrl output instead:
 maxctrl: show service readconn-service-master

{code:java}
┌─────────────────────┬─────────────────────────────────────────────────────────────────────────────────────┐
│ Service             │ readconn-service-master                                                             │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Router              │ readconnroute                                                                       │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ State               │ Started                                                                             │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Started At          │ Mon Oct 14 20:24:15 2019                                                            │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Current Connections │ 16                                                                                  │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Total Connections   │ 193735482                                                                           │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Servers             │ db2                                                                                 │
│                     │ db1                                                                                 │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Parameters          │ {                                                                                   │
│                     │     ""router_options"": ""master"",                                                     │
│                     │     ""user"": ""maxusr"",                                                               │
│                     │     ""password"": ""FF2856D24FBAC82A038DC04210E5C7884C60A27218DBC17D85C08E9391E85E13"", │
│                     │     ""passwd"": null,                                                                 │
│                     │     ""enable_root_user"": false,                                                      │
│                     │     ""max_retry_interval"": 3600,                                                     │
│                     │     ""max_connections"": 0,                                                           │
│                     │     ""connection_timeout"": 0,                                                        │
│                     │     ""auth_all_servers"": false,                                                      │
│                     │     ""strip_db_esc"": true,                                                           │
│                     │     ""localhost_match_wildcard_host"": true,                                          │
│                     │     ""version_string"": null,                                                         │
│                     │     ""weightby"": null,                                                               │
│                     │     ""log_auth_warnings"": true,                                                      │
│                     │     ""retry_on_failure"": true,                                                       │
│                     │     ""session_track_trx_state"": false,                                               │
│                     │     ""retain_last_statements"": -1,                                                   │
│                     │     ""session_trace"": 0                                                              │
│                     │ }                                                                                   │
├─────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┤
│ Router Diagnostics  │ {                                                                                   │
│                     │     ""connections"": 193733865,                                                       │
│                     │     ""current_connections"": 16,                                                      │
│                     │     ""queries"": 1585574679                                                           │
│                     │ }                                                                                   │
└─────────────────────┴─────────────────────────────────────────────────────────────────────────────────────┘
{code}
Having individual numbers of routed transactions/queries per server on the Router Options will make it way more complete and able to give better information to who is analyzing traffic patterns."
3122,MXS-192,MXS,Dipti Joshi,82198,2016-03-22 14:25:58,"[~tturenko], [~johan.wikman] Is this complete - can it be closed ?",1,"[~tturenko], [~johan.wikman] Is this complete - can it be closed ?"
3123,MXS-192,MXS,markus makela,87218,2016-10-10 07:40:05,"This tests lsyncd, not MaxScale.",2,"This tests lsyncd, not MaxScale."
3124,MXS-192,MXS,Dipti Joshi,87271,2016-10-11 02:46:05,[~markus makela] [~johan.wikman] [~tturenko]- The task here is to include this in MaxScale system test plan - even if it seems to be testing lsyncd - the impact is on MaxScale. If lsyncd is not configured properly - config changes on one MaxScale will not be propagated to other MaxScale and when failover occurs between MaxScale - the new active MaxScale will not behave like the failed MaxScale. I see this has been closed as Incomplete - what is the plan to complete it.,3,[~markus makela] [~johan.wikman] [~tturenko]- The task here is to include this in MaxScale system test plan - even if it seems to be testing lsyncd - the impact is on MaxScale. If lsyncd is not configured properly - config changes on one MaxScale will not be propagated to other MaxScale and when failover occurs between MaxScale - the new active MaxScale will not behave like the failed MaxScale. I see this has been closed as Incomplete - what is the plan to complete it.
3125,MXS-192,MXS,Johan Wikman,87275,2016-10-11 05:57:31,"Since lsyncd is configured by the end-user of MaxScale, what is the purpose of testing that we can configure lsyncd?
",4,"Since lsyncd is configured by the end-user of MaxScale, what is the purpose of testing that we can configure lsyncd?
"
3126,MXS-192,MXS,Dipti Joshi,87282,2016-10-11 10:40:28,"Here is the documentation we provide to configure lsynchd for maxscale configuration synching: https://github.com/mariadb-corporation/MaxScale/blob/2.0.1/Documentation/Tutorials/MaxScale-HA-with-lsyncd.md. The purpose of testing is to test integration of MaxScale and lsyncd as per this documentation. System test plan goal is to test MaxScale with the ecosystem of tools that we recommend to users and customers, rather then just MaxScale as standalone - hence the request.",5,"Here is the documentation we provide to configure lsynchd for maxscale configuration synching: URL The purpose of testing is to test integration of MaxScale and lsyncd as per this documentation. System test plan goal is to test MaxScale with the ecosystem of tools that we recommend to users and customers, rather then just MaxScale as standalone - hence the request."
3127,MXS-192,MXS,Johan Wikman,87283,2016-10-11 11:37:10,"Yes, that makes sense. And should be done once per release.
",6,"Yes, that makes sense. And should be done once per release.
"
3128,MXS-192,MXS,Timofey Turenko,90104,2016-12-30 18:57:28,tested manually: works as described in the documentation. The only additions: EPEL repo is needed to install lsyncd to CentOS/RHEL (but we can refer to some lsyncd installation guide) and user which is used by lsync to access nodes have to have write access to /etc/,7,tested manually: works as described in the documentation. The only additions: EPEL repo is needed to install lsyncd to CentOS/RHEL (but we can refer to some lsyncd installation guide) and user which is used by lsync to access nodes have to have write access to /etc/
3129,MXS-1923,MXS,markus makela,112425,2018-06-13 20:35:37,"Since MaxScale has no such server state nor does MaxAdmin show it, this should probably be filed as a new feature request rather than a bug.",1,"Since MaxScale has no such server state nor does MaxAdmin show it, this should probably be filed as a new feature request rather than a bug."
3130,MXS-1923,MXS,markus makela,112428,2018-06-13 20:39:22,"I changed this to a new feature request as it's a new state for the servers as well as a new concept inside MaxScale.
",2,"I changed this to a new feature request as it's a new state for the servers as well as a new concept inside MaxScale.
"
3131,MXS-1923,MXS,Johan Wikman,145467,2020-03-02 11:54:10,"Changed to 2.6 as we will not do this for 2.5.
",3,"Changed to 2.6 as we will not do this for 2.5.
"
3132,MXS-1929,MXS,markus makela,115502,2018-08-21 05:40:58,"Both services and filters can now be created and deleted via the REST API. Their relationships can also be altered at runtime which in practice means changing the filters of a running service.

Extended maxctrl to support the new commands.",1,"Both services and filters can now be created and deleted via the REST API. Their relationships can also be altered at runtime which in practice means changing the filters of a running service.

Extended maxctrl to support the new commands."
3133,MXS-194,MXS,Dipti Joshi,73351,2015-07-10 23:21:07,[~markus makela] Do we have estimate of the effort for this task ?,1,[~markus makela] Do we have estimate of the effort for this task ?
3134,MXS-194,MXS,Johan Wikman,81830,2016-03-08 13:15:53,"This functionality can be achieved using the regular expression based filter, so moving this for now to 1.5. But it needs to be assessed whether _specific_ functionality for dealing with DROP, CREATE, GRANT and USE really is needed.",2,"This functionality can be achieved using the regular expression based filter, so moving this for now to 1.5. But it needs to be assessed whether _specific_ functionality for dealing with DROP, CREATE, GRANT and USE really is needed."
3135,MXS-194,MXS,Dipti Joshi,81838,2016-03-08 15:20:33,"[~johan.wikman] Ability to block some one from doing DROP, CREATE, GRANT and USE is really needed. A hacker can use DROP to entirely drop a table, USE to access a database that the user should not be able to access, GRANT to get access to tables and database that the user should not be able to and CREATE to create a new table and then load data to over load the database. 
",3,"[~johan.wikman] Ability to block some one from doing DROP, CREATE, GRANT and USE is really needed. A hacker can use DROP to entirely drop a table, USE to access a database that the user should not be able to access, GRANT to get access to tables and database that the user should not be able to and CREATE to create a new table and then load data to over load the database. 
"
3136,MXS-1940,MXS,Johan Wikman,113332,2018-07-02 05:47:51,"From MaxScale 2.2.11 onwards, the cache is no longer marked as being experimental.

The following changes to the default behaviour has been made:
* The default thread model is now {{thread_specific}} (used to be {{shared}})
* The default select approach is now {{assumed_cacheable}} (used to be {{verify_cacheable}}
",1,"From MaxScale 2.2.11 onwards, the cache is no longer marked as being experimental.

The following changes to the default behaviour has been made:
* The default thread model is now {{thread_specific}} (used to be {{shared}})
* The default select approach is now {{assumed_cacheable}} (used to be {{verify_cacheable}}
"
3137,MXS-1954,MXS,Esa Korhonen,144065,2020-02-11 11:46:11,Only partially done.,1,Only partially done.
3138,MXS-1971,MXS,Timofey Turenko,117954,2018-10-16 09:49:00,Builders added - see branch https://github.com/mariadb-corporation/maxscale-buildbot/tree/temporal_perftest,1,Builders added - see branch URL
3139,MXS-1972,MXS,Timofey Turenko,115063,2018-08-09 10:23:39,"`destroy` builder added, all other cleaners are included as steps into other Builders",1,"`destroy` builder added, all other cleaners are included as steps into other Builders"
3140,MXS-1973,MXS,markus makela,113817,2018-07-11 18:31:25,"Should be relatively low-effort to implement. It'll be mainly about wrapping the {{dcb->remote}} into some function call that either returns the IP or the hostname depending on how MaxScale is configured. The value of {{dcb->remote}} cannot be changed to a hostname as it would cause functional changes to MaxScale, mainly authentication and how it behaves.",1,"Should be relatively low-effort to implement. It'll be mainly about wrapping the {{dcb->remote}} into some function call that either returns the IP or the hostname depending on how MaxScale is configured. The value of {{dcb->remote}} cannot be changed to a hostname as it would cause functional changes to MaxScale, mainly authentication and how it behaves."
3141,MXS-1976,MXS,markus makela,119179,2018-11-13 15:34:36,Updated the maxadmin documentation for 2.3 to suggest destroying the listener to immediately reject new connections.,1,Updated the maxadmin documentation for 2.3 to suggest destroying the listener to immediately reject new connections.
3142,MXS-1976,MXS,markus makela,119308,2018-11-15 08:32:56,Amended the 2.3 documentation with a note about {{destroy listener}} being an option.,2,Amended the 2.3 documentation with a note about {{destroy listener}} being an option.
3143,MXS-1995,MXS,Timofey Turenko,115185,2018-08-13 13:00:05,"- Chef does not support SLES/OpenSuse 15
- All SLES15 AWS images are HVM - no support from Vagrant side

probable solutions:
- use different provisioning tool (Ansible?)
- add SLES15 support to Chef",1,"- Chef does not support SLES/OpenSuse 15
- All SLES15 AWS images are HVM - no support from Vagrant side

probable solutions:
- use different provisioning tool (Ansible?)
- add SLES15 support to Chef"
3144,MXS-1995,MXS,Timofey Turenko,115363,2018-08-16 12:44:30,"A hack to install Chef from suse 42 is added - now it is possible to create OpenSUSE 15 VM with Chef installed for Maxscale build.

Experimental builds were done:
http://max-tst-01.mariadb.com/ci-repository/develop/mariadb-maxscale/opensuse/15/x86_64/
http://max-tst-01.mariadb.com/ci-repository/2.1/mariadb-maxscale/opensuse/15/x86_64/
http://max-tst-01.mariadb.com/ci-repository/2.2/mariadb-maxscale/opensuse/15/x86_64/",2,"A hack to install Chef from suse 42 is added - now it is possible to create OpenSUSE 15 VM with Chef installed for Maxscale build.

Experimental builds were done:
URL
URL
URL"
3145,MXS-1995,MXS,Timofey Turenko,115757,2018-08-29 09:34:41,"Please test installation process  in your environment. I've tested only with my local libvirt OpenSUSE 15 virtual machine and SLES15 AWS machine, but all thse machines can be a bit different from your environment. Packages are available from our CI repository: 

http://max-tst-01.mariadb.com/ci-repository/develop/mariadb-maxscale/opensuse/15/x86_64/
http://max-tst-01.mariadb.com/ci-repository/2.1/mariadb-maxscale/opensuse/15/x86_64/
http://max-tst-01.mariadb.com/ci-repository/2.2/mariadb-maxscale/opensuse/15/x86_64/",3,"Please test installation process  in your environment. I've tested only with my local libvirt OpenSUSE 15 virtual machine and SLES15 AWS machine, but all thse machines can be a bit different from your environment. Packages are available from our CI repository: 

URL
URL
URL"
3146,MXS-2012,MXS,Esa Korhonen,115631,2018-08-24 12:56:28,Replication lag is queried from the servers directly.,1,Replication lag is queried from the servers directly.
3147,MXS-2022,MXS,Axel Schwenke,117964,2018-10-16 10:22:36,"Results attached. MaxScale was built from source using the revision tagged ""maxscale-2.3.0"". Disabling the query classifier cache seems to have not much effect. But in general the cache is now very effective. No need to use the ""selects=assume_cacheable"" setting any more.",1,"Results attached. MaxScale was built from source using the revision tagged ""maxscale-2.3.0"". Disabling the query classifier cache seems to have not much effect. But in general the cache is now very effective. No need to use the ""selects=assume_cacheable"" setting any more."
3148,MXS-2029,MXS,Timofey Turenko,122196,2019-01-22 10:44:37,done by FRUCT https://github.com/mariadb-corporation/mdbci#mdbci-installation,1,done by FRUCT URL
3149,MXS-2030,MXS,Timofey Turenko,116521,2018-09-12 10:22:46,"notes for CentOS7 installation:

entOS7

sudo yum install git
sudo  yum update -y nss curl libcurl
git clone https://github.com/mariadb-corporation/mdbci.git
sudo yum groupinstall ""Virtualization Tools"" -y
sudo yum install libvirt libvirt-devel
sudo usermod -a -G libvirt ec2-user



put following:
polkit.addRule(function(action, subject) { if (action.id == ""org.libvirt.unix.manage"" && subject.user == ""ec2-user"") { return polkit.Result.YES; polkit.log(""action="" + action); polkit.log(""subject="" + subject); } });
into  /etc/polkit-1/rules.d/50-org.libvirt.unix.manage.rules


sudo yum install ruby
sudo gem install bundle
sudo bundle install --without development
sudo /usr/local/bin/bundle install --without development

curl -sSL https://rvm.io/mpapis.asc | gpg --import -
curl -L get.rvm.io | bash -s stable
source /etc/profile.d/rvm.sh
rvm install ""ruby-2.5""

gem install json-schema
gem install xdg
gem install ipaddress
gem install net-ssh
gem install net-scp
gem install nokogiri
gem install workers
sudo yum install git build-essential wget -y
sudo yum install python python-pip
sudo pip install --upgrade awscli

cd ~/mdbci
./scripts/setup_aws.sh

cd
wget https://releases.hashicorp.com/vagrant/2.1.4/vagrant_2.1.4_x86_64.rpm
sudo yum install vagrant_2.1.4_x86_64.rpm

git clone https://github.com/vagrant-libvirt/vagrant-libvirt.git
cd vagrant-libvirt
rake build
vagrant plugin install pkg/vagrant-libvirt*.gem
vagrant plugin install vagrant-aws vagrant-mutate vagrant-omnibus
",1,"notes for CentOS7 installation:

entOS7

sudo yum install git
sudo  yum update -y nss curl libcurl
git clone URL
sudo yum groupinstall ""Virtualization Tools"" -y
sudo yum install libvirt libvirt-devel
sudo usermod -a -G libvirt ec2-user



put following:
polkit.addRule(function(action, subject) { if (action.id == ""org.libvirt.unix.manage"" && subject.user == ""ec2-user"") { return polkit.Result.YES; polkit.log(""action="" + action); polkit.log(""subject="" + subject); } });
into  /etc/polkit-1/rules.d/50-org.libvirt.unix.manage.rules


sudo yum install ruby
sudo gem install bundle
sudo bundle install --without development
sudo /usr/local/bin/bundle install --without development

curl -sSL URL | gpg --import -
curl -L get.rvm.io | bash -s stable
source /etc/profile.d/rvm.sh
rvm install ""ruby-2.5""

gem install json-schema
gem install xdg
gem install ipaddress
gem install net-ssh
gem install net-scp
gem install nokogiri
gem install workers
sudo yum install git build-essential wget -y
sudo yum install python python-pip
sudo pip install --upgrade awscli

cd ~/mdbci
./scripts/setup_aws.sh

cd
wget URL
sudo yum install vagrant_2.1.4_x86_64.rpm

git clone URL
cd vagrant-libvirt
rake build
vagrant plugin install pkg/vagrant-libvirt*.gem
vagrant plugin install vagrant-aws vagrant-mutate vagrant-omnibus
"
3150,MXS-2030,MXS,Timofey Turenko,116522,2018-09-12 10:23:29,"preparing for building tests in CentOS:

sudo yum install cmake mariadb-test php
sudo yum install jansson-devel // not in the cmake list

        sudo yum -y install centos-release-scl || \
            sudo yum-config-manager --enable rhui-REGION-rhel-server-rhscl

        sudo yum -y install devtoolset-7-gcc-c++
        source /opt/rh/devtoolset-7/enable

        # Enable it by default
        echo ""source /opt/rh/devtoolset-7/enable"" >> ~/.bashrc
",2,"preparing for building tests in CentOS:

sudo yum install cmake mariadb-test php
sudo yum install jansson-devel // not in the cmake list

        sudo yum -y install centos-release-scl || \
            sudo yum-config-manager --enable rhui-REGION-rhel-server-rhscl

        sudo yum -y install devtoolset-7-gcc-c++
        source /opt/rh/devtoolset-7/enable

        # Enable it by default
        echo ""source /opt/rh/devtoolset-7/enable"" >> ~/.bashrc
"
3151,MXS-2030,MXS,Timofey Turenko,121185,2018-12-27 09:54:47,"Ubuntu Bionic, mdbci setup-dependencies command failed:

INFO: Invoking command: sudo virsh pool-info default
error: failed to get pool 'default'
error: Storage pool not found: no storage pool with matching name 'default'

bug reported to FRUCT",3,"Ubuntu Bionic, mdbci setup-dependencies command failed:

INFO: Invoking command: sudo virsh pool-info default
error: failed to get pool 'default'
error: Storage pool not found: no storage pool with matching name 'default'

bug reported to FRUCT"
3152,MXS-2030,MXS,Timofey Turenko,121641,2019-01-11 12:29:11,"Debian Stretch - works!
Ubuntu Bionic - works!",4,"Debian Stretch - works!
Ubuntu Bionic - works!"
3153,MXS-2030,MXS,Timofey Turenko,122193,2019-01-22 10:42:21,"Centos/RHEL/Fedora - works

Suse/Gentoo -  not supported

Amazon Linux - EPEL repos is needed",5,"Centos/RHEL/Fedora - works

Suse/Gentoo -  not supported

Amazon Linux - EPEL repos is needed"
3154,MXS-2030,MXS,Timofey Turenko,122194,2019-01-22 10:43:30,https://github.com/mariadb-corporation/mdbci#mdbci-installation,6,URL
3155,MXS-2030,MXS,Timofey Turenko,122195,2019-01-22 10:44:06,Mint 18/19 works,7,Mint 18/19 works
3156,MXS-2044,MXS,markus makela,121685,2019-01-12 22:57:04,I think we can use the {{log_type}} parameter and add {{stdout}} there. In addition to this it might be useful to also be able to log it in the maxscale.log.,1,I think we can use the {{log_type}} parameter and add {{stdout}} there. In addition to this it might be useful to also be able to log it in the maxscale.log.
3157,MXS-2051,MXS,markus makela,117064,2018-09-25 19:32:15,Removed from GitHub and KB.,1,Removed from GitHub and KB.
3158,MXS-2054,MXS,markus makela,117219,2018-10-01 10:20:03,"Possible configuration for a test case:
{code}
[maxscale]
threads=###threads###

[server1]
type=server
address=###node_server_IP_1###
port=###node_server_port_1###
protocol=MySQLBackend
weight=1

[server2]
type=server
address=###node_server_IP_2###
port=###node_server_port_2###
protocol=MySQLBackend
weight=1

[server3]
type=server
address=###node_server_IP_3###
port=###node_server_port_3###
protocol=MySQLBackend
weight=0

[server4]
type=server
address=###node_server_IP_4###
port=###node_server_port_4###
protocol=MySQLBackend
weight=0

[MySQL Monitor]
type=monitor
module=mysqlmon
# Note that server3 and server4 are not monitored
servers=server1,server2
user=maxskysql
password=skysql
monitor_interval=1000

[hybridizer]
type=filter
module=namedserverfilter
match03=test[.]t3
target03=server3
match04=test[.]t4
target04=server4

[RW Split Router]
type=service
router=readwritesplit
servers=server1,server2,server3,server4
user=maxskysql
password=skysql
filters=hybridizer
weightby=weight

[RW Split Listener]
type=listener
service=RW Split Router
protocol=MySQLClient
port=4006

[CLI]
type=service
router=cli

[CLI Listener]
type=listener
service=CLI
protocol=maxscaled
socket=default
{code}",1,"Possible configuration for a test case:
{code}
[maxscale]
threads=###threads###

[server1]
type=server
address=###node_server_IP_1###
port=###node_server_port_1###
protocol=MySQLBackend
weight=1

[server2]
type=server
address=###node_server_IP_2###
port=###node_server_port_2###
protocol=MySQLBackend
weight=1

[server3]
type=server
address=###node_server_IP_3###
port=###node_server_port_3###
protocol=MySQLBackend
weight=0

[server4]
type=server
address=###node_server_IP_4###
port=###node_server_port_4###
protocol=MySQLBackend
weight=0

[MySQL Monitor]
type=monitor
module=mysqlmon
# Note that server3 and server4 are not monitored
servers=server1,server2
user=maxskysql
password=skysql
monitor_interval=1000

[hybridizer]
type=filter
module=namedserverfilter
match03=test[.]t3
target03=server3
match04=test[.]t4
target04=server4

[RW Split Router]
type=service
router=readwritesplit
servers=server1,server2,server3,server4
user=maxskysql
password=skysql
filters=hybridizer
weightby=weight

[RW Split Listener]
type=listener
service=RW Split Router
protocol=MySQLClient
port=4006

[CLI]
type=service
router=cli

[CLI Listener]
type=listener
service=CLI
protocol=maxscaled
socket=default
{code}"
3159,MXS-2054,MXS,markus makela,117339,2018-10-03 09:49:09,Added a test case that emulates hybrid clusters being used with readwritesplit and namedserverfilter.,2,Added a test case that emulates hybrid clusters being used with readwritesplit and namedserverfilter.
3160,MXS-2057,MXS,Hartmut Holzgraefe,116731,2018-09-17 16:43:31,"Core dumps actually work just fine already, when starting maxscale from a shell where ""ulimit -c unlimited"" is set I am getting a core file in the /var/log/maxscale directory just fine when doing ""killall -6 maxscale""

Some modifications to the systemd service file may be needed to produce core dumps when maxscale is running under systemd control though.",1,"Core dumps actually work just fine already, when starting maxscale from a shell where ""ulimit -c unlimited"" is set I am getting a core file in the /var/log/maxscale directory just fine when doing ""killall -6 maxscale""

Some modifications to the systemd service file may be needed to produce core dumps when maxscale is running under systemd control though."
3161,MXS-2057,MXS,markus makela,116749,2018-09-18 05:25:28,"An idea for 3 would be to have MaxScale's communicate with each other via the REST API so that they would form a cluster. A manually assigned priority number would allow the user to tell the order of promotion which would also serve as the basis on which conflict resolution could be build.
",2,"An idea for 3 would be to have MaxScale's communicate with each other via the REST API so that they would form a cluster. A manually assigned priority number would allow the user to tell the order of promotion which would also serve as the basis on which conflict resolution could be build.
"
3162,MXS-2057,MXS,Johan Wikman,116864,2018-09-20 06:10:55,"There are two problems here:
# Detect when MaxScale is hung and kill it.
# When the MaxScale in the _active_ role has gone down, a MaxScale _not_ in the active role should be made _active_.

The first one can probably be handled with systemd's watchdog functionality.
The second can be handled with keepalived.",3,"There are two problems here:
# Detect when MaxScale is hung and kill it.
# When the MaxScale in the _active_ role has gone down, a MaxScale _not_ in the active role should be made _active_.

The first one can probably be handled with systemd's watchdog functionality.
The second can be handled with keepalived."
3163,MXS-2074,MXS,Johan Wikman,119447,2018-11-19 11:30:45,"We are likely to change the namedserverfilter into a router that is capable of routing incoming requests, depending on their characteristics, to different _services_. At the same time we will ensure that prepared statements are handled properly.",1,"We are likely to change the namedserverfilter into a router that is capable of routing incoming requests, depending on their characteristics, to different _services_. At the same time we will ensure that prepared statements are handled properly."
3164,MXS-2077,MXS,Johan Wikman,119236,2018-11-14 07:51:24,"{{maxctrl list sessions}} will now show more information about the client.

{{maxadmin list clients}} will not be altered, as it shows internal data of MaxScale that really shouldn't be exposed and as {{maxadmin}} on the whole is being phased out.

{code}
$ maxctrl list sessions
┌────┬────────┬───────────┬──────────────────────────┬────────┬──────────────────┐
│ Id │ User   │ Host      │ Connected                │ Idle   │ Service          │
├────┼────────┼───────────┼──────────────────────────┼────────┼──────────────────┤
│ 23 │ wikman │ localhost │ Tue Nov 13 15:52:18 2018 │ 5595.7 │ MaxAdmin-Service │
├────┼────────┼───────────┼──────────────────────────┼────────┼──────────────────┤
│ 24 │ johan  │ localhost │ Tue Nov 13 15:57:52 2018 │ 5915.9 │ RWS              │
└────┴────────┴───────────┴──────────────────────────┴────────┴──────────────────┘
{code}
",1,"{{maxctrl list sessions}} will now show more information about the client.

{{maxadmin list clients}} will not be altered, as it shows internal data of MaxScale that really shouldn't be exposed and as {{maxadmin}} on the whole is being phased out.

{code}
$ maxctrl list sessions
┌────┬────────┬───────────┬──────────────────────────┬────────┬──────────────────┐
│ Id │ User   │ Host      │ Connected                │ Idle   │ Service          │
├────┼────────┼───────────┼──────────────────────────┼────────┼──────────────────┤
│ 23 │ wikman │ localhost │ Tue Nov 13 15:52:18 2018 │ 5595.7 │ MaxAdmin-Service │
├────┼────────┼───────────┼──────────────────────────┼────────┼──────────────────┤
│ 24 │ johan  │ localhost │ Tue Nov 13 15:57:52 2018 │ 5915.9 │ RWS              │
└────┴────────┴───────────┴──────────────────────────┴────────┴──────────────────┘
{code}
"
3165,MXS-208,MXS,Timofey Turenko,72325,2015-06-17 11:48:40,"It can be done with existing setup. I consider just constantly running test (do not interrupt after 12 or 24 hours). 
We need to define the list of filters and their sequence in the configuration (testing of all combinations can  eat a lot of resources and do not give a good result).

Such testing can be started after merging at least reload config branch and clarification of binlog status. ",1,"It can be done with existing setup. I consider just constantly running test (do not interrupt after 12 or 24 hours). 
We need to define the list of filters and their sequence in the configuration (testing of all combinations can  eat a lot of resources and do not give a good result).

Such testing can be started after merging at least reload config branch and clarification of binlog status. "
3166,MXS-208,MXS,Dipti Joshi,74225,2015-08-04 06:54:33,[~tturenko] Did we do this testing in 1.2 ? What were the results ? If this task is complete - can this item be closed now ?,2,[~tturenko] Did we do this testing in 1.2 ? What were the results ? If this task is complete - can this item be closed now ?
3167,MXS-208,MXS,Timofey Turenko,80709,2016-02-08 22:51:40,"long_sysbench test added, it runs until stopped by user, logs are copied and published periodically ",3,"long_sysbench test added, it runs until stopped by user, logs are copied and published periodically "
3168,MXS-2105,MXS,markus makela,135686,2019-10-10 07:11:26,Added {{ssl_crl}} parameter for listeners.,1,Added {{ssl_crl}} parameter for listeners.
3169,MXS-2120,MXS,Esa Korhonen,118896,2018-11-08 09:55:27,"This is by design. Can be changed, but requires more than a simple bug fix.",1,"This is by design. Can be changed, but requires more than a simple bug fix."
3170,MXS-2120,MXS,Esa Korhonen,133545,2019-09-04 12:13:48,"Added in 2.4.0. Requires the ""enforce_simple_topology""-setting.",2,"Added in 2.4.0. Requires the ""enforce_simple_topology""-setting."
3171,MXS-2128,MXS,Timofey Turenko,120574,2018-12-10 22:15:18,"minimum patch is done, info sent to Axel for next step",1,"minimum patch is done, info sent to Axel for next step"
3172,MXS-213,MXS,Dipti Joshi,74222,2015-08-04 06:46:46,This is Phase 3 of binlog server,1,This is Phase 3 of binlog server
3173,MXS-213,MXS,Dipti Joshi,74579,2015-08-13 18:42:15,[~Massimiliano Pinto] Please add estimate,2,[~Massimiliano Pinto] Please add estimate
3174,MXS-213,MXS,VAROQUI Stephane,79624,2016-01-07 22:44:56,"What would look  nice is to get warning if semi-sync delay is not 3 times monitoring delay .

if this condition is true fetching  of semi-sync status should be done by the monitoring . If the last monitoring semy-sync status was sync than we know that we did no lost an event on the old master and he can become a candidate slave

This would enable failover to work as a switchover if replication is not delayed more than 3 times monitoring delay    ",3,"What would look  nice is to get warning if semi-sync delay is not 3 times monitoring delay .

if this condition is true fetching  of semi-sync status should be done by the monitoring . If the last monitoring semy-sync status was sync than we know that we did no lost an event on the old master and he can become a candidate slave

This would enable failover to work as a switchover if replication is not delayed more than 3 times monitoring delay    "
3175,MXS-213,MXS,Massimiliano Pinto,79631,2016-01-08 11:38:31,"Current work is in:

https://github.com/mariadb-corporation/MaxScale/tree/MXS-213

In maxscale.cnf just put semisync=1

In the Master Server:

MySQL>INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so'; 
MySQL> SET GLOBAL rpl_semi_sync_master_enabled = 1;

Check it:
MySQL> SHOW VARIABLES LIKE 'rpl_semi_sync_master_enabled';
",4,"Current work is in:

URL

In maxscale.cnf just put semisync=1

In the Master Server:

MySQL>INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so'; 
MySQL> SET GLOBAL rpl_semi_sync_master_enabled = 1;

Check it:
MySQL> SHOW VARIABLES LIKE 'rpl_semi_sync_master_enabled';
"
3176,MXS-213,MXS,Dipti Joshi,81837,2016-03-08 15:16:09,"[~Massimiliano Pinto][~tturenko] [~johan.wikman] Which jira item covers the QA test cases for this feature ?

Also this was originally planned for 1.5 , not 1.4",5,"[~Massimiliano Pinto][~tturenko] [~johan.wikman] Which jira item covers the QA test cases for this feature ?

Also this was originally planned for 1.5 , not 1.4"
3177,MXS-2145,MXS,markus makela,118781,2018-11-06 17:33:51,This could be implemented as a filter that injects a one-time SQL statement the first time a query is received.,1,This could be implemented as a filter that injects a one-time SQL statement the first time a query is received.
3178,MXS-2145,MXS,Dipti Joshi,118928,2018-11-08 18:50:41,"[~markus makela], [~johan.wikman] Can any of the existing filter be used to do so? or does this need to be done as listener parameter?",2,"[~markus makela], [~johan.wikman] Can any of the existing filter be used to do so? or does this need to be done as listener parameter?"
3179,MXS-2145,MXS,Johan Wikman,118953,2018-11-09 09:18:42,"Any existing filter could be modified to do this, but that does not seem like a good idea. A new filter called, say {{header}}, that, just like Markus writes above, would inject a one-time SQL statement the first time a query is received sounds like the right approach.",3,"Any existing filter could be modified to do this, but that does not seem like a good idea. A new filter called, say {{header}}, that, just like Markus writes above, would inject a one-time SQL statement the first time a query is received sounds like the right approach."
3180,MXS-2146,MXS,Dipti Joshi,118929,2018-11-08 18:54:44,"[~johan.wikman], [~markus makela] Let us have this done before 2.3.1 goes out",1,"[~johan.wikman], [~markus makela] Let us have this done before 2.3.1 goes out"
3181,MXS-2152,MXS,markus makela,127671,2019-05-13 09:17:39,Added preliminary support for GCOV builds.,1,Added preliminary support for GCOV builds.
3182,MXS-2160,MXS,Johan Wikman,119174,2018-11-13 14:37:53,"Based on the rather limited profiling there does not seem to be any major differences between MaxScale 2.3 and 2.3. However, there are a few issues that could be investigated further:
* {{clock_gettime}} is called a sufficient number of times so that the time it takes actually is visible in the profiling results. Most of the calles is related to load and statistics calculation, so it should perhaps be something that can be turned on/off.
* MaxScale always sniffs for {{set sql_mode=[default |oracle]}} and for the setting of variables related to client directed caching. Both of those could be optional.
",1,"Based on the rather limited profiling there does not seem to be any major differences between MaxScale 2.3 and 2.3. However, there are a few issues that could be investigated further:
* {{clock_gettime}} is called a sufficient number of times so that the time it takes actually is visible in the profiling results. Most of the calles is related to load and statistics calculation, so it should perhaps be something that can be turned on/off.
* MaxScale always sniffs for {{set sql_mode=[default |oracle]}} and for the setting of variables related to client directed caching. Both of those could be optional.
"
3183,MXS-2161,MXS,Niclas Antti,121768,2019-01-15 10:02:08,"High level design is complete. The implementation phases will produce more software design, and other documents.",1,"High level design is complete. The implementation phases will produce more software design, and other documents."
3184,MXS-2165,MXS,markus makela,121488,2019-01-08 12:13:58,The current design accommodates for MCOL-1662 as well as the lack of update and delete support in the ColumnStore API.,1,The current design accommodates for MCOL-1662 as well as the lack of update and delete support in the ColumnStore API.
3185,MXS-2195,MXS,Niclas Antti,122197,2019-01-22 11:02:22,price estimate and suggested system configuration writeup made.,1,price estimate and suggested system configuration writeup made.
3186,MXS-2197,MXS,markus makela,121687,2019-01-13 12:05:56,"Readwritesplit, readconnroute and schemarouter use the router template class. The routers that do not use it and have not been changed are:
* the deprecated modules (cli, debugcli)
* maxinfo
* binlogrouter
* hintrouter",1,"Readwritesplit, readconnroute and schemarouter use the router template class. The routers that do not use it and have not been changed are:
* the deprecated modules (cli, debugcli)
* maxinfo
* binlogrouter
* hintrouter"
3187,MXS-2197,MXS,markus makela,126511,2019-04-18 09:12:10,The rest are either deprecated or not worth the effort (better alternatives exist) to convert to use the router template.,2,The rest are either deprecated or not worth the effort (better alternatives exist) to convert to use the router template.
3188,MXS-2219,MXS,Johan Wikman,122135,2019-01-21 13:48:51,"First incarnation ready:
* Connects to bootstrap servers in turn until one that is in the quorum is found.
* Figures out dynamically what servers there are and creates corresponding new {{Server}} instances.
* Pings each server in the quorum once every second.
* Checks once per minute from the connected to server what the cluster state is and acts accordingly.",1,"First incarnation ready:
* Connects to bootstrap servers in turn until one that is in the quorum is found.
* Figures out dynamically what servers there are and creates corresponding new {{Server}} instances.
* Pings each server in the quorum once every second.
* Checks once per minute from the connected to server what the cluster state is and acts accordingly."
3189,MXS-2244,MXS,Niclas Antti,122187,2019-01-22 09:33:44,Closing in favor of a new epic jira with subtask.,1,Closing in favor of a new epic jira with subtask.
3190,MXS-2253,MXS,markus makela,121567,2019-01-10 04:56:29,"Another idea would be to add support for suffixed time types like we have for sizes:
{code}
monitor_interval=5s
monitor_interval=2000ms
monitor_interval=1m
{code}
",1,"Another idea would be to add support for suffixed time types like we have for sizes:
{code}
monitor_interval=5s
monitor_interval=2000ms
monitor_interval=1m
{code}
"
3191,MXS-2253,MXS,Johan Wikman,121573,2019-01-10 07:57:11,"Time suffixes is the way to go. That way it's completely unambiguous what the number means.
",2,"Time suffixes is the way to go. That way it's completely unambiguous what the number means.
"
3192,MXS-2253,MXS,Austin Rutherford,122947,2019-02-05 13:55:28,Looks good.  One item though - if they don't add a suffix they should IMO either error out or default to the same grain (seconds or milleseconds),3,Looks good.  One item though - if they don't add a suffix they should IMO either error out or default to the same grain (seconds or milleseconds)
3193,MXS-2253,MXS,Johan Wikman,123214,2019-02-13 08:42:08,"It's going to be like this;

* The suffixes {{h}}, {{m}}, {{s}} and {{ms}}  are introduced, using which a duration can be specified in hours, minutes, seconds or milliseconds, respectively.
* In MaxScale 2.4, if no explicit unit is specified, a value without a unit will be interpreted the way it is interpreted now (i.e. either as seconds or milliseconds), and a warning will be logged that a unit should be provided.
* _Not_ providing an explicit unit is deprecated in MaxScale 2.4.
* _Not_ providing an explicit unit will cause an error in some version after MaxScale 2.4.",4,"It's going to be like this;

* The suffixes {{h}}, {{m}}, {{s}} and {{ms}}  are introduced, using which a duration can be specified in hours, minutes, seconds or milliseconds, respectively.
* In MaxScale 2.4, if no explicit unit is specified, a value without a unit will be interpreted the way it is interpreted now (i.e. either as seconds or milliseconds), and a warning will be logged that a unit should be provided.
* _Not_ providing an explicit unit is deprecated in MaxScale 2.4.
* _Not_ providing an explicit unit will cause an error in some version after MaxScale 2.4."
3194,MXS-2260,MXS,markus makela,122916,2019-02-05 10:45:58,"Apart from {{curl}} what other sort of clients would you expect to find?

I think that a whole REST API tutorial might be a good way to get to know the API and how it behaves.",1,"Apart from {{curl}} what other sort of clients would you expect to find?

I think that a whole REST API tutorial might be a good way to get to know the API and how it behaves."
3195,MXS-2260,MXS,Geoff Montee,123174,2019-02-11 21:32:59,"It looks like there are a ton of REST clients, but I'm not sure which are popular beyond curl.

There's a list of some here:

https://github.com/marmelab/awesome-rest#querying",2,"It looks like there are a ton of REST clients, but I'm not sure which are popular beyond curl.

There's a list of some here:

URL"
3196,MXS-2273,MXS,Johan Wikman,122117,2019-01-21 11:15:30,"* When the server state bit {{BEING_DRAINED}} has been set, new connections must no longer be created to the server, but existing connections may continue to use it.
* The {{BEING_DRAINED}} bit is similar to the _maintenance_ bit, in that it can be set and unset manually by the end-user.
* Monitors may also set this bit if some operation exposed by them naturally require the bit to be set. For instance, the Clustrix monitor will turn on this bit when a soft failure is initiated via the monitor.
",1,"* When the server state bit {{BEING_DRAINED}} has been set, new connections must no longer be created to the server, but existing connections may continue to use it.
* The {{BEING_DRAINED}} bit is similar to the _maintenance_ bit, in that it can be set and unset manually by the end-user.
* Monitors may also set this bit if some operation exposed by them naturally require the bit to be set. For instance, the Clustrix monitor will turn on this bit when a soft failure is initiated via the monitor.
"
3197,MXS-23,MXS,Dipti Joshi,68899,2015-03-09 23:36:03,"This is comment history imported from bugzilla

+Comment 1 Markus Mäkelä 2014-12-04 12:18:53 UTC+
Added -f <filename> option.

+Comment 2 Markus Mäkelä 2014-12-04 12:31:28 UTC+
Rolled back the changes for now.

+Comment 3 Mark Riddoch 2015-02-13 10:40:03 UTC+
It was delibrately done like this to allow the script style behaviour

+Comment 4 Hartmut Holzgraefe 2015-02-13 10:47:47 UTC+
Can you elaborate on ""allow the script style behaviour""? It doesn't really make sense to me ...

I'm also missing comments on my ambiguity and security concerns ...",1,"This is comment history imported from bugzilla

+Comment 1 Markus Mäkelä 2014-12-04 12:18:53 UTC+
Added -f  option.

+Comment 2 Markus Mäkelä 2014-12-04 12:31:28 UTC+
Rolled back the changes for now.

+Comment 3 Mark Riddoch 2015-02-13 10:40:03 UTC+
It was delibrately done like this to allow the script style behaviour

+Comment 4 Hartmut Holzgraefe 2015-02-13 10:47:47 UTC+
Can you elaborate on ""allow the script style behaviour""? It doesn't really make sense to me ...

I'm also missing comments on my ambiguity and security concerns ..."
3198,MXS-23,MXS,markus makela,70902,2015-05-09 21:20:56,Should this be reviewed and possibly changed?,2,Should this be reviewed and possibly changed?
3199,MXS-2313,MXS,Dipti Joshi,122870,2019-02-04 16:01:44,[~johan.wikman] [~markus makela] What is the source of this request? Let us hold on to this one for now.,1,[~johan.wikman] [~markus makela] What is the source of this request? Let us hold on to this one for now.
3200,MXS-2313,MXS,markus makela,122871,2019-02-04 16:02:31,The parameter was deprecated in 2.3.,2,The parameter was deprecated in 2.3.
3201,MXS-2313,MXS,Johan Wikman,123713,2019-02-22 08:12:08,"This should be quite straightforward to support. Something like:
{code}
[Server1]
type=server
...
location=local

[Server2]
type=server
...
location=remote
{code}
And a {{remote}} server is _only_ used if a {{local}} is not available.",3,"This should be quite straightforward to support. Something like:
{code}
[Server1]
type=server
...
location=local

[Server2]
type=server
...
location=remote
{code}
And a {{remote}} server is _only_ used if a {{local}} is not available."
3202,MXS-2313,MXS,markus makela,124944,2019-03-18 11:23:59,Added the {{rank}} parameter with two values: {{primary}} and {{secondary}}. The default value is {{primary}}.,4,Added the {{rank}} parameter with two values: {{primary}} and {{secondary}}. The default value is {{primary}}.
3203,MXS-2338,MXS,Timofey Turenko,124941,2019-03-18 11:09:35,"alter_router/000/valgrind01.log:==25657==    definitely lost: 944 bytes in 23 blocks
alter_router/000/valgrind00.log:==25401==    definitely lost: 1,184 bytes in 20 blocks
avro/000/valgrind00.log:==30169==    definitely lost: 197,492 bytes in 4,396 blocks
binary_ps/000/valgrind00.log:==29828==    definitely lost: 728 bytes in 17 blocks
binary_ps_cursor/000/valgrind00.log:==30308==    definitely lost: 888 bytes in 19 blocks
binlog_change_master/000/valgrind01.log:==31962==    definitely lost: 1,632 bytes in 12 blocks
binlog_change_master_gtid/000/valgrind00.log:==32673==    definitely lost: 6,944 bytes in 8 blocks
binlog_incompl/000/valgrind00.log:==1494==    definitely lost: 3,688 bytes in 2 blocks
binlog_semisync_txs0_ss0/000/valgrind00.log:==5311==    definitely lost: 6,944 bytes in 8 blocks
binlog_semisync_txs0_ss1/000/valgrind00.log:==8324==    definitely lost: 6,944 bytes in 8 blocks
bug471/000/valgrind00.log:==8291==    definitely lost: 1,032 bytes in 24 blocks
bug473/000/valgrind00.log:==8746==    definitely lost: 1,038 bytes in 25 blocks
bug475/000/valgrind00.log:==9202==    definitely lost: 976 bytes in 22 blocks
bug519/000/valgrind00.log:==9685==    definitely lost: 888 bytes in 19 blocks
bug547/000/valgrind00.log:==10577==    definitely lost: 728 bytes in 17 blocks
bug571/000/valgrind00.log:==13488==    definitely lost: 1,032 bytes in 24 blocks
bug587/000/valgrind00.log:==14913==    definitely lost: 1,032 bytes in 24 blocks
bug587_1/000/valgrind00.log:==15368==    definitely lost: 1,032 bytes in 24 blocks
bug620/000/valgrind00.log:==16813==    definitely lost: 936 bytes in 20 blocks
bug654/000/valgrind00.log:==18273==    definitely lost: 752 bytes in 18 blocks
bug658/000/valgrind00.log:==19344==    definitely lost: 984 bytes in 22 blocks
bug662/000/valgrind01.log:==19932==    definitely lost: 752 bytes in 18 blocks
bug664/000/valgrind00.log:==20380==    definitely lost: 15,408 bytes in 26 blocks
bug705/000/valgrind01.log:==21549==    definitely lost: 15,032 bytes in 16 blocks
bug729/000/valgrind00.log:==22008==    definitely lost: 680 bytes in 15 blocks
bulk_insert/000/valgrind00.log:==22928==    definitely lost: 728 bytes in 17 blocks
cache_runtime_ttl/000/valgrind00.log:==28801==    definitely lost: 696 bytes in 13 blocks
ccrfilter_test/000/valgrind01.log:==23556==    definitely lost: 800 bytes in 20 blocks
ccrfilter_test/000/valgrind00.log:==23366==    definitely lost: 800 bytes in 20 blocks
check_backend/000/valgrind00.log:==4945==    definitely lost: 1,056 bytes in 23 blocks
connect_to_nonexisting_db/000/valgrind00.log:==25112==    definitely lost: 936 bytes in 20 blocks
Binary file different_size_binlog/000/valgrind01.log matches
different_size_binlog/000/valgrind00.log:==15175==    definitely lost: 6,968 bytes in 9 blocks
encrypted_passwords/000/valgrind00.log:==29115==    definitely lost: 15,032 bytes in 16 blocks
fwf/000/valgrind06.log:==8694==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind09.log:==9028==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind01.log:==8143==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind17.log:==9915==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind12.log:==9360==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind08.log:==8917==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind10.log:==9139==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind00.log:==8031==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind19.log:==10220==    definitely lost: 472 bytes in 14 blocks
fwf/000/valgrind16.log:==9802==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind07.log:==8805==    definitely lost: 4,136 bytes in 16 blocks
fwf2/000/valgrind02.log:==11110==    definitely lost: 4,136 bytes in 16 blocks
fwf2/000/valgrind03.log:==11302==    definitely lost: 4,136 bytes in 16 blocks
fwf_actions/000/valgrind00.log:==12917==    definitely lost: 504 bytes in 18 blocks
fwf_com_ping/000/valgrind00.log:==19314==    definitely lost: 3,832 bytes in 8 blocks
fwf_duplicate_rules/000/valgrind00.log:==11999==    definitely lost: 13,200 bytes in 3 blocks
fwf_logging/000/valgrind00.log:==13506==    definitely lost: 168 bytes in 6 blocks
fwf_prepared_stmt/000/valgrind00.log:==12427==    definitely lost: 472 bytes in 14 blocks
fwf_syntax/000/valgrind02.log:==15671==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind06.log:==16604==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind09.log:==17305==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind14.log:==18472==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind15.log:==18705==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind01.log:==15438==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind12.log:==18005==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind03.log:==15904==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind08.log:==17071==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind04.log:==16137==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind11.log:==17772==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind05.log:==16371==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind13.log:==18239==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind10.log:==17539==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind00.log:==15205==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind07.log:==16838==    definitely lost: 3,640 bytes in 1 blocks
galera_priority/000/valgrind00.log:==704==    definitely lost: 1,112 bytes in 18 blocks
keepalived_masterdown/000/valgrind02.log:==32760==    definitely lost: 15,216 bytes in 19 blocks
load_balancing_pers10/000/valgrind00.log:==21657==    definitely lost: 1,352 bytes in 26 blocks
Binary file local_address/000/valgrind01.log matches
Binary file local_address/000/valgrind00.log matches
mariadb_tests_hartmut/valgrind00.log:==23615==    definitely lost: 1,112 bytes in 18 blocks
masking_auto_firewall/000/valgrind00.log:==27580==    definitely lost: 584 bytes in 12 blocks
maxctrl_basic/000/valgrind00.log:==29865==    definitely lost: 11,856 bytes in 244 blocks
maxinfo_sql/000/valgrind00.log:==24240==    definitely lost: 1,772 bytes in 122 blocks
maxscale_process_user/000/valgrind00.log:==25007==    definitely lost: 15,032 bytes in 16 blocks
mm/000/valgrind00.log:==25471==    definitely lost: 7,752 bytes in 14 blocks
Binary file mxs1045/000/valgrind00.log matches
mxs1310_implicit_db/000/valgrind00.log:==22709==    definitely lost: 7,512 bytes in 8 blocks
mxs1323_stress/000/valgrind00.log:==23615==    definitely lost: 1,112 bytes in 18 blocks
mxs1418/000/valgrind00.log:==31899==    definitely lost: 1,056 bytes in 25 blocks
mxs1468/000/valgrind00.log:==2040==    definitely lost: 11,320 bytes in 14 blocks
mxs1476/000/valgrind00.log:==3726==    definitely lost: 880 bytes in 15 blocks
mxs1503_queued_sescmd/000/valgrind00.log:==3108==    definitely lost: 440 bytes in 8 blocks
Binary file mxs1506_no_master/000/valgrind00.log matches
mxs1507_migrate_trx/000/valgrind00.log:==27348==    definitely lost: 3,736 bytes in 102 blocks
mxs1509/000/valgrind00.log:==4090==    definitely lost: 7,584 bytes in 9 blocks
mxs1516/000/valgrind00.log:==4635==    definitely lost: 728 bytes in 17 blocks
mxs1542/000/valgrind00.log:==28650==    definitely lost: 4,752 bytes in 21 blocks
mxs1583_fwf/000/valgrind00.log:==19804==    definitely lost: 472 bytes in 14 blocks
mxs1585/000/valgrind00.log:==5838==    definitely lost: 1,448 bytes in 31 blocks
mxs1677_temp_table/000/valgrind00.log:==7673==    definitely lost: 728 bytes in 17 blocks
mxs1678_relay_master/000/valgrind00.log:==8152==    definitely lost: 15,056 bytes in 17 blocks
mxs1719/000/valgrind00.log:==19136==    definitely lost: 976 bytes in 18 blocks
Binary file mxs173_throttle_filter/000/valgrind00.log matches
mxs1773_failing_ldli/000/valgrind00.log:==10976==    definitely lost: 6,296 bytes in 25 blocks
mxs1778_causal_reads/000/valgrind00.log:==29517==    definitely lost: 488 bytes in 9 blocks
mxs1787_slave_reconnection/000/valgrind00.log:==12534==    definitely lost: 7,608 bytes in 46 blocks
mxs1824_double_cursor/000/valgrind00.log:==14042==    definitely lost: 7,424 bytes in 27 blocks
mxs1836_show_eventTimes/000/valgrind00.log:==8177==    definitely lost: 296 bytes in 7 blocks
mxs1873_large_sescmd/000/valgrind00.log:==15503==    definitely lost: 9,024 bytes in 30 blocks
Binary file mxs1889/000/valgrind00.log matches
mxs1896_load_data_infile/000/valgrind00.log:==15974==    definitely lost: 9,024 bytes in 31 blocks
mxs1926_killed_server/000/valgrind00.log:==11135==    definitely lost: 768 bytes in 15 blocks
Binary file mxs1958_insert_priv/000/valgrind00.log matches
mxs1980_blr_galera_server_ids/000/valgrind02.log:==6763==    definitely lost: 208 bytes in 5 blocks
mxs1980_blr_galera_server_ids/000/valgrind00.log:==6277==    definitely lost: 208 bytes in 5 blocks
mxs2106_avro_null/000/valgrind00.log:==24489==    definitely lost: 5,800 bytes in 29 blocks
mxs2167_extra_port/000/valgrind01.log:==31265==    definitely lost: 1,184 bytes in 19 blocks
mxs2187_multi_replay/000/valgrind00.log:==28659==    definitely lost: 1,040 bytes in 16 blocks
mxs2295_change_user_loop/000/valgrind00.log:==26546==    definitely lost: 3,080 bytes in 10 blocks
mxs2326_hint_clone/000/valgrind00.log:==27873==    definitely lost: 3,192 bytes in 13 blocks
mxs359_error_on_write/000/valgrind00.log:==29149==    definitely lost: 1,040 bytes in 17 blocks
mxs359_master_switch/000/valgrind00.log:==27404==    definitely lost: 1,136 bytes in 19 blocks
mxs359_read_only/000/valgrind00.log:==28311==    definitely lost: 1,136 bytes in 19 blocks
mxs431/000/valgrind00.log:==32302==    definitely lost: 14,840 bytes in 12 blocks
mxs548_short_session_change_user/000/valgrind00.log:==32753==    definitely lost: 8,008 bytes in 19 blocks
mxs559_block_master/000/valgrind00.log:==889==    definitely lost: 1,448 bytes in 30 blocks
mxs564_big_dump/000/valgrind00.log:==3121==    definitely lost: 1,472 bytes in 31 blocks
mxs701_binlog_filter/000/valgrind01.log:==23037==    definitely lost: 328 bytes in 10 blocks
mxs701_binlog_filter/000/valgrind00.log:==22641==    definitely lost: 7,040 bytes in 12 blocks
mxs716/000/valgrind00.log:==5268==    definitely lost: 1,256 bytes in 24 blocks
mxs729_maxadmin/000/valgrind00.log:==6583==    definitely lost: 15,032 bytes in 16 blocks
mxs827_write_timeout/000/valgrind00.log:==9423==    definitely lost: 888 bytes in 19 blocks
mxs874_slave_recovery/000/valgrind00.log:==10796==    definitely lost: 936 bytes in 21 blocks
mxs922_scaling/000/valgrind00.log:==20098==    definitely lost: 29,912 bytes in 26 blocks
mxs951_utfmb4/000/valgrind01.log:==29389==    definitely lost: 728 bytes in 17 blocks
mxs951_utfmb4/000/valgrind00.log:==29254==    definitely lost: 15,032 bytes in 16 blocks
mysqlmon_external_master/000/valgrind00.log:==18728==    definitely lost: 1,136 bytes in 19 blocks
mysqlmon_failover_auto/000/valgrind01.log:==3187==    definitely lost: 1,328 bytes in 25 blocks
mysqlmon_failover_manual/000/valgrind02.log:==5553==    definitely lost: 15,032 bytes in 16 blocks
mysqlmon_failover_manual/000/valgrind00.log:==4111==    definitely lost: 1,376 bytes in 27 blocks
mysqlmon_failover_manual2_2/000/valgrind00.log:==7205==    definitely lost: 912 bytes in 20 blocks
mysqlmon_failover_no_slaves/000/valgrind00.log:==9617==    definitely lost: 1,096 bytes in 22 blocks
mysqlmon_failover_rejoin_old_slave/000/valgrind00.log:==14963==    definitely lost: 984 bytes in 22 blocks
mysqlmon_fail_switch_events/000/valgrind00.log:==19621==    definitely lost: 1,232 bytes in 24 blocks
mysqlmon_rejoin_bad2/000/valgrind00.log:==11903==    definitely lost: 1,352 bytes in 26 blocks
mysqlmon_rejoin_good/000/valgrind00.log:==10211==    definitely lost: 1,376 bytes in 27 blocks
mysqlmon_reset_replication/000/valgrind01.log:==20729==    definitely lost: 1,144 bytes in 23 blocks
mysqlmon_switchover_bad_master/000/valgrind00.log:==8627==    definitely lost: 1,008 bytes in 23 blocks
mysqlmon_switchover_stress/000/valgrind00.log:==17977==    definitely lost: 1,352 bytes in 27 blocks
namedserverfilter_test/000/valgrind00.log:==17012==    definitely lost: 10,032 bytes in 28 blocks
pers_02/000/valgrind00.log:==18421==    definitely lost: 9,096 bytes in 37 blocks
proxy_protocol/000/valgrind00.log:==29844==    definitely lost: 1,304 bytes in 24 blocks
readconnrouter_slave/000/valgrind00.log:==19879==    definitely lost: 21,880 bytes in 29 blocks
rwsplit_conn_num/000/valgrind00.log:==22060==    definitely lost: 4,264 bytes in 30 blocks
rwsplit_multi_stmt/000/valgrind00.log:==24046==    definitely lost: 15,896 bytes in 28 blocks
rwsplit_readonly_stress/000/valgrind00.log:==23595==    definitely lost: 6,288 bytes in 43 blocks
sanity_check/000/valgrind00.log:==6364==    definitely lost: 1,192 bytes in 25 blocks
schemarouter_duplicate/000/valgrind00.log:==24624==    definitely lost: 18,608 bytes in 12 blocks
session_limits/000/valgrind00.log:==27232==    definitely lost: 7,800 bytes in 32 blocks
setup_binlog/000/valgrind00.log:==19763==    definitely lost: 6,944 bytes in 8 blocks
setup_binlog_gtid/000/valgrind00.log:==21369==    definitely lost: 6,944 bytes in 8 blocks
short_sessions/000/valgrind00.log:==28697==    definitely lost: 8,112 bytes in 35 blocks
",1,"alter_router/000/valgrind01.log:==25657==    definitely lost: 944 bytes in 23 blocks
alter_router/000/valgrind00.log:==25401==    definitely lost: 1,184 bytes in 20 blocks
avro/000/valgrind00.log:==30169==    definitely lost: 197,492 bytes in 4,396 blocks
binary_ps/000/valgrind00.log:==29828==    definitely lost: 728 bytes in 17 blocks
binary_ps_cursor/000/valgrind00.log:==30308==    definitely lost: 888 bytes in 19 blocks
binlog_change_master/000/valgrind01.log:==31962==    definitely lost: 1,632 bytes in 12 blocks
binlog_change_master_gtid/000/valgrind00.log:==32673==    definitely lost: 6,944 bytes in 8 blocks
binlog_incompl/000/valgrind00.log:==1494==    definitely lost: 3,688 bytes in 2 blocks
binlog_semisync_txs0_ss0/000/valgrind00.log:==5311==    definitely lost: 6,944 bytes in 8 blocks
binlog_semisync_txs0_ss1/000/valgrind00.log:==8324==    definitely lost: 6,944 bytes in 8 blocks
bug471/000/valgrind00.log:==8291==    definitely lost: 1,032 bytes in 24 blocks
bug473/000/valgrind00.log:==8746==    definitely lost: 1,038 bytes in 25 blocks
bug475/000/valgrind00.log:==9202==    definitely lost: 976 bytes in 22 blocks
bug519/000/valgrind00.log:==9685==    definitely lost: 888 bytes in 19 blocks
bug547/000/valgrind00.log:==10577==    definitely lost: 728 bytes in 17 blocks
bug571/000/valgrind00.log:==13488==    definitely lost: 1,032 bytes in 24 blocks
bug587/000/valgrind00.log:==14913==    definitely lost: 1,032 bytes in 24 blocks
bug587_1/000/valgrind00.log:==15368==    definitely lost: 1,032 bytes in 24 blocks
bug620/000/valgrind00.log:==16813==    definitely lost: 936 bytes in 20 blocks
bug654/000/valgrind00.log:==18273==    definitely lost: 752 bytes in 18 blocks
bug658/000/valgrind00.log:==19344==    definitely lost: 984 bytes in 22 blocks
bug662/000/valgrind01.log:==19932==    definitely lost: 752 bytes in 18 blocks
bug664/000/valgrind00.log:==20380==    definitely lost: 15,408 bytes in 26 blocks
bug705/000/valgrind01.log:==21549==    definitely lost: 15,032 bytes in 16 blocks
bug729/000/valgrind00.log:==22008==    definitely lost: 680 bytes in 15 blocks
bulk_insert/000/valgrind00.log:==22928==    definitely lost: 728 bytes in 17 blocks
cache_runtime_ttl/000/valgrind00.log:==28801==    definitely lost: 696 bytes in 13 blocks
ccrfilter_test/000/valgrind01.log:==23556==    definitely lost: 800 bytes in 20 blocks
ccrfilter_test/000/valgrind00.log:==23366==    definitely lost: 800 bytes in 20 blocks
check_backend/000/valgrind00.log:==4945==    definitely lost: 1,056 bytes in 23 blocks
connect_to_nonexisting_db/000/valgrind00.log:==25112==    definitely lost: 936 bytes in 20 blocks
Binary file different_size_binlog/000/valgrind01.log matches
different_size_binlog/000/valgrind00.log:==15175==    definitely lost: 6,968 bytes in 9 blocks
encrypted_passwords/000/valgrind00.log:==29115==    definitely lost: 15,032 bytes in 16 blocks
fwf/000/valgrind06.log:==8694==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind09.log:==9028==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind01.log:==8143==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind17.log:==9915==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind12.log:==9360==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind08.log:==8917==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind10.log:==9139==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind00.log:==8031==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind19.log:==10220==    definitely lost: 472 bytes in 14 blocks
fwf/000/valgrind16.log:==9802==    definitely lost: 4,136 bytes in 16 blocks
fwf/000/valgrind07.log:==8805==    definitely lost: 4,136 bytes in 16 blocks
fwf2/000/valgrind02.log:==11110==    definitely lost: 4,136 bytes in 16 blocks
fwf2/000/valgrind03.log:==11302==    definitely lost: 4,136 bytes in 16 blocks
fwf_actions/000/valgrind00.log:==12917==    definitely lost: 504 bytes in 18 blocks
fwf_com_ping/000/valgrind00.log:==19314==    definitely lost: 3,832 bytes in 8 blocks
fwf_duplicate_rules/000/valgrind00.log:==11999==    definitely lost: 13,200 bytes in 3 blocks
fwf_logging/000/valgrind00.log:==13506==    definitely lost: 168 bytes in 6 blocks
fwf_prepared_stmt/000/valgrind00.log:==12427==    definitely lost: 472 bytes in 14 blocks
fwf_syntax/000/valgrind02.log:==15671==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind06.log:==16604==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind09.log:==17305==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind14.log:==18472==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind15.log:==18705==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind01.log:==15438==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind12.log:==18005==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind03.log:==15904==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind08.log:==17071==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind04.log:==16137==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind11.log:==17772==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind05.log:==16371==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind13.log:==18239==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind10.log:==17539==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind00.log:==15205==    definitely lost: 3,640 bytes in 1 blocks
fwf_syntax/000/valgrind07.log:==16838==    definitely lost: 3,640 bytes in 1 blocks
galera_priority/000/valgrind00.log:==704==    definitely lost: 1,112 bytes in 18 blocks
keepalived_masterdown/000/valgrind02.log:==32760==    definitely lost: 15,216 bytes in 19 blocks
load_balancing_pers10/000/valgrind00.log:==21657==    definitely lost: 1,352 bytes in 26 blocks
Binary file local_address/000/valgrind01.log matches
Binary file local_address/000/valgrind00.log matches
mariadb_tests_hartmut/valgrind00.log:==23615==    definitely lost: 1,112 bytes in 18 blocks
masking_auto_firewall/000/valgrind00.log:==27580==    definitely lost: 584 bytes in 12 blocks
maxctrl_basic/000/valgrind00.log:==29865==    definitely lost: 11,856 bytes in 244 blocks
maxinfo_sql/000/valgrind00.log:==24240==    definitely lost: 1,772 bytes in 122 blocks
maxscale_process_user/000/valgrind00.log:==25007==    definitely lost: 15,032 bytes in 16 blocks
mm/000/valgrind00.log:==25471==    definitely lost: 7,752 bytes in 14 blocks
Binary file mxs1045/000/valgrind00.log matches
mxs1310_implicit_db/000/valgrind00.log:==22709==    definitely lost: 7,512 bytes in 8 blocks
mxs1323_stress/000/valgrind00.log:==23615==    definitely lost: 1,112 bytes in 18 blocks
mxs1418/000/valgrind00.log:==31899==    definitely lost: 1,056 bytes in 25 blocks
mxs1468/000/valgrind00.log:==2040==    definitely lost: 11,320 bytes in 14 blocks
mxs1476/000/valgrind00.log:==3726==    definitely lost: 880 bytes in 15 blocks
mxs1503_queued_sescmd/000/valgrind00.log:==3108==    definitely lost: 440 bytes in 8 blocks
Binary file mxs1506_no_master/000/valgrind00.log matches
mxs1507_migrate_trx/000/valgrind00.log:==27348==    definitely lost: 3,736 bytes in 102 blocks
mxs1509/000/valgrind00.log:==4090==    definitely lost: 7,584 bytes in 9 blocks
mxs1516/000/valgrind00.log:==4635==    definitely lost: 728 bytes in 17 blocks
mxs1542/000/valgrind00.log:==28650==    definitely lost: 4,752 bytes in 21 blocks
mxs1583_fwf/000/valgrind00.log:==19804==    definitely lost: 472 bytes in 14 blocks
mxs1585/000/valgrind00.log:==5838==    definitely lost: 1,448 bytes in 31 blocks
mxs1677_temp_table/000/valgrind00.log:==7673==    definitely lost: 728 bytes in 17 blocks
mxs1678_relay_master/000/valgrind00.log:==8152==    definitely lost: 15,056 bytes in 17 blocks
mxs1719/000/valgrind00.log:==19136==    definitely lost: 976 bytes in 18 blocks
Binary file mxs173_throttle_filter/000/valgrind00.log matches
mxs1773_failing_ldli/000/valgrind00.log:==10976==    definitely lost: 6,296 bytes in 25 blocks
mxs1778_causal_reads/000/valgrind00.log:==29517==    definitely lost: 488 bytes in 9 blocks
mxs1787_slave_reconnection/000/valgrind00.log:==12534==    definitely lost: 7,608 bytes in 46 blocks
mxs1824_double_cursor/000/valgrind00.log:==14042==    definitely lost: 7,424 bytes in 27 blocks
mxs1836_show_eventTimes/000/valgrind00.log:==8177==    definitely lost: 296 bytes in 7 blocks
mxs1873_large_sescmd/000/valgrind00.log:==15503==    definitely lost: 9,024 bytes in 30 blocks
Binary file mxs1889/000/valgrind00.log matches
mxs1896_load_data_infile/000/valgrind00.log:==15974==    definitely lost: 9,024 bytes in 31 blocks
mxs1926_killed_server/000/valgrind00.log:==11135==    definitely lost: 768 bytes in 15 blocks
Binary file mxs1958_insert_priv/000/valgrind00.log matches
mxs1980_blr_galera_server_ids/000/valgrind02.log:==6763==    definitely lost: 208 bytes in 5 blocks
mxs1980_blr_galera_server_ids/000/valgrind00.log:==6277==    definitely lost: 208 bytes in 5 blocks
mxs2106_avro_null/000/valgrind00.log:==24489==    definitely lost: 5,800 bytes in 29 blocks
mxs2167_extra_port/000/valgrind01.log:==31265==    definitely lost: 1,184 bytes in 19 blocks
mxs2187_multi_replay/000/valgrind00.log:==28659==    definitely lost: 1,040 bytes in 16 blocks
mxs2295_change_user_loop/000/valgrind00.log:==26546==    definitely lost: 3,080 bytes in 10 blocks
mxs2326_hint_clone/000/valgrind00.log:==27873==    definitely lost: 3,192 bytes in 13 blocks
mxs359_error_on_write/000/valgrind00.log:==29149==    definitely lost: 1,040 bytes in 17 blocks
mxs359_master_switch/000/valgrind00.log:==27404==    definitely lost: 1,136 bytes in 19 blocks
mxs359_read_only/000/valgrind00.log:==28311==    definitely lost: 1,136 bytes in 19 blocks
mxs431/000/valgrind00.log:==32302==    definitely lost: 14,840 bytes in 12 blocks
mxs548_short_session_change_user/000/valgrind00.log:==32753==    definitely lost: 8,008 bytes in 19 blocks
mxs559_block_master/000/valgrind00.log:==889==    definitely lost: 1,448 bytes in 30 blocks
mxs564_big_dump/000/valgrind00.log:==3121==    definitely lost: 1,472 bytes in 31 blocks
mxs701_binlog_filter/000/valgrind01.log:==23037==    definitely lost: 328 bytes in 10 blocks
mxs701_binlog_filter/000/valgrind00.log:==22641==    definitely lost: 7,040 bytes in 12 blocks
mxs716/000/valgrind00.log:==5268==    definitely lost: 1,256 bytes in 24 blocks
mxs729_maxadmin/000/valgrind00.log:==6583==    definitely lost: 15,032 bytes in 16 blocks
mxs827_write_timeout/000/valgrind00.log:==9423==    definitely lost: 888 bytes in 19 blocks
mxs874_slave_recovery/000/valgrind00.log:==10796==    definitely lost: 936 bytes in 21 blocks
mxs922_scaling/000/valgrind00.log:==20098==    definitely lost: 29,912 bytes in 26 blocks
mxs951_utfmb4/000/valgrind01.log:==29389==    definitely lost: 728 bytes in 17 blocks
mxs951_utfmb4/000/valgrind00.log:==29254==    definitely lost: 15,032 bytes in 16 blocks
mysqlmon_external_master/000/valgrind00.log:==18728==    definitely lost: 1,136 bytes in 19 blocks
mysqlmon_failover_auto/000/valgrind01.log:==3187==    definitely lost: 1,328 bytes in 25 blocks
mysqlmon_failover_manual/000/valgrind02.log:==5553==    definitely lost: 15,032 bytes in 16 blocks
mysqlmon_failover_manual/000/valgrind00.log:==4111==    definitely lost: 1,376 bytes in 27 blocks
mysqlmon_failover_manual2_2/000/valgrind00.log:==7205==    definitely lost: 912 bytes in 20 blocks
mysqlmon_failover_no_slaves/000/valgrind00.log:==9617==    definitely lost: 1,096 bytes in 22 blocks
mysqlmon_failover_rejoin_old_slave/000/valgrind00.log:==14963==    definitely lost: 984 bytes in 22 blocks
mysqlmon_fail_switch_events/000/valgrind00.log:==19621==    definitely lost: 1,232 bytes in 24 blocks
mysqlmon_rejoin_bad2/000/valgrind00.log:==11903==    definitely lost: 1,352 bytes in 26 blocks
mysqlmon_rejoin_good/000/valgrind00.log:==10211==    definitely lost: 1,376 bytes in 27 blocks
mysqlmon_reset_replication/000/valgrind01.log:==20729==    definitely lost: 1,144 bytes in 23 blocks
mysqlmon_switchover_bad_master/000/valgrind00.log:==8627==    definitely lost: 1,008 bytes in 23 blocks
mysqlmon_switchover_stress/000/valgrind00.log:==17977==    definitely lost: 1,352 bytes in 27 blocks
namedserverfilter_test/000/valgrind00.log:==17012==    definitely lost: 10,032 bytes in 28 blocks
pers_02/000/valgrind00.log:==18421==    definitely lost: 9,096 bytes in 37 blocks
proxy_protocol/000/valgrind00.log:==29844==    definitely lost: 1,304 bytes in 24 blocks
readconnrouter_slave/000/valgrind00.log:==19879==    definitely lost: 21,880 bytes in 29 blocks
rwsplit_conn_num/000/valgrind00.log:==22060==    definitely lost: 4,264 bytes in 30 blocks
rwsplit_multi_stmt/000/valgrind00.log:==24046==    definitely lost: 15,896 bytes in 28 blocks
rwsplit_readonly_stress/000/valgrind00.log:==23595==    definitely lost: 6,288 bytes in 43 blocks
sanity_check/000/valgrind00.log:==6364==    definitely lost: 1,192 bytes in 25 blocks
schemarouter_duplicate/000/valgrind00.log:==24624==    definitely lost: 18,608 bytes in 12 blocks
session_limits/000/valgrind00.log:==27232==    definitely lost: 7,800 bytes in 32 blocks
setup_binlog/000/valgrind00.log:==19763==    definitely lost: 6,944 bytes in 8 blocks
setup_binlog_gtid/000/valgrind00.log:==21369==    definitely lost: 6,944 bytes in 8 blocks
short_sessions/000/valgrind00.log:==28697==    definitely lost: 8,112 bytes in 35 blocks
"
3204,MXS-2338,MXS,Timofey Turenko,124942,2019-03-18 11:10:33,"vagrant@max-tst-02:~/LOGS/run_test-986/LOGS$ grep ""indirectly lost:"" * -r | grep -v ""indirectly lost: 0 bytes""
alter_router/000/valgrind01.log:==25657==    indirectly lost: 912,036 bytes in 1,391 blocks
alter_router/000/valgrind00.log:==25401==    indirectly lost: 894,536 bytes in 1,224 blocks
avro/000/valgrind00.log:==30169==    indirectly lost: 455,706 bytes in 664 blocks
binary_ps/000/valgrind00.log:==29828==    indirectly lost: 978,148 bytes in 1,427 blocks
binary_ps_cursor/000/valgrind00.log:==30308==    indirectly lost: 960,872 bytes in 1,262 blocks
binlog_change_master/000/valgrind01.log:==31962==    indirectly lost: 268,064 bytes in 343 blocks
binlog_change_master_gtid/000/valgrind00.log:==32673==    indirectly lost: 320,876 bytes in 460 blocks
binlog_incompl/000/valgrind00.log:==1494==    indirectly lost: 679 bytes in 41 blocks
binlog_semisync_txs0_ss0/000/valgrind00.log:==5311==    indirectly lost: 320,876 bytes in 460 blocks
binlog_semisync_txs0_ss1/000/valgrind00.log:==8324==    indirectly lost: 320,876 bytes in 460 blocks
bug471/000/valgrind00.log:==8291==    indirectly lost: 913,308 bytes in 1,434 blocks
bug473/000/valgrind00.log:==8746==    indirectly lost: 978,903 bytes in 1,447 blocks
bug475/000/valgrind00.log:==9202==    indirectly lost: 960,936 bytes in 1,266 blocks
bug519/000/valgrind00.log:==9685==    indirectly lost: 960,872 bytes in 1,262 blocks
bug547/000/valgrind00.log:==10577==    indirectly lost: 978,148 bytes in 1,427 blocks
bug571/000/valgrind00.log:==13488==    indirectly lost: 900,628 bytes in 1,397 blocks
bug587/000/valgrind00.log:==14913==    indirectly lost: 833,172 bytes in 1,330 blocks
bug587_1/000/valgrind00.log:==15368==    indirectly lost: 899,684 bytes in 1,370 blocks
bug620/000/valgrind00.log:==16813==    indirectly lost: 978,588 bytes in 1,432 blocks
bug654/000/valgrind00.log:==18273==    indirectly lost: 975,780 bytes in 1,389 blocks
bug658/000/valgrind00.log:==19344==    indirectly lost: 978,588 bytes in 1,432 blocks
bug662/000/valgrind01.log:==19932==    indirectly lost: 778,636 bytes in 1,301 blocks
bug664/000/valgrind00.log:==20380==    indirectly lost: 1,283,737 bytes in 1,857 blocks
bug705/000/valgrind01.log:==21549==    indirectly lost: 883,049 bytes in 1,312 blocks
bug729/000/valgrind00.log:==22008==    indirectly lost: 978,148 bytes in 1,427 blocks
bulk_insert/000/valgrind00.log:==22928==    indirectly lost: 818,411 bytes in 1,157 blocks
cache_runtime_ttl/000/valgrind00.log:==28801==    indirectly lost: 253,888 bytes in 387 blocks
ccrfilter_test/000/valgrind01.log:==23556==    indirectly lost: 977,599 bytes in 1,450 blocks
ccrfilter_test/000/valgrind00.log:==23366==    indirectly lost: 912,120 bytes in 1,411 blocks
check_backend/000/valgrind00.log:==4945==    indirectly lost: 1,231,788 bytes in 1,807 blocks
connect_to_nonexisting_db/000/valgrind00.log:==25112==    indirectly lost: 978,588 bytes in 1,432 blocks
Binary file different_size_binlog/000/valgrind01.log matches
different_size_binlog/000/valgrind00.log:==15175==    indirectly lost: 320,876 bytes in 460 blocks
encrypted_passwords/000/valgrind00.log:==29115==    indirectly lost: 963,121 bytes in 1,417 blocks
fwf/000/valgrind06.log:==8694==    indirectly lost: 815,175 bytes in 1,179 blocks
fwf/000/valgrind09.log:==9028==    indirectly lost: 748,663 bytes in 1,139 blocks
fwf/000/valgrind01.log:==8143==    indirectly lost: 894,167 bytes in 1,282 blocks
fwf/000/valgrind17.log:==9915==    indirectly lost: 814,279 bytes in 1,161 blocks
fwf/000/valgrind12.log:==9360==    indirectly lost: 815,175 bytes in 1,179 blocks
fwf/000/valgrind08.log:==8917==    indirectly lost: 815,039 bytes in 1,178 blocks
fwf/000/valgrind10.log:==9139==    indirectly lost: 746,735 bytes in 1,120 blocks
fwf/000/valgrind00.log:==8031==    indirectly lost: 961,711 bytes in 1,323 blocks
fwf/000/valgrind19.log:==10220==    indirectly lost: 800,432 bytes in 1,051 blocks
fwf/000/valgrind16.log:==9802==    indirectly lost: 814,143 bytes in 1,178 blocks
fwf/000/valgrind07.log:==8805==    indirectly lost: 748,799 bytes in 1,140 blocks
fwf2/000/valgrind02.log:==11110==    indirectly lost: 961,711 bytes in 1,323 blocks
fwf2/000/valgrind03.log:==11302==    indirectly lost: 881,687 bytes in 1,219 blocks
fwf_actions/000/valgrind00.log:==12917==    indirectly lost: 960,600 bytes in 1,266 blocks
fwf_com_ping/000/valgrind00.log:==19314==    indirectly lost: 321,423 bytes in 486 blocks
fwf_duplicate_rules/000/valgrind00.log:==11999==    indirectly lost: 3,706 bytes in 187 blocks
fwf_logging/000/valgrind00.log:==13506==    indirectly lost: 320,200 bytes in 422 blocks
fwf_prepared_stmt/000/valgrind00.log:==12427==    indirectly lost: 960,480 bytes in 1,259 blocks
fwf_syntax/000/valgrind02.log:==15671==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind06.log:==16604==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind09.log:==17305==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind14.log:==18472==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind15.log:==18705==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind01.log:==15438==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind12.log:==18005==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind03.log:==15904==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind08.log:==17071==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind04.log:==16137==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind11.log:==17772==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind05.log:==16371==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind13.log:==18239==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind10.log:==17539==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind00.log:==15205==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind07.log:==16838==    indirectly lost: 672 bytes in 40 blocks
galera_priority/000/valgrind00.log:==704==    indirectly lost: 240,539 bytes in 333 blocks
keepalived_masterdown/000/valgrind02.log:==32760==    indirectly lost: 747,243 bytes in 1,389 blocks
load_balancing_pers10/000/valgrind00.log:==21657==    indirectly lost: 962,219 bytes in 1,278 blocks
Binary file local_address/000/valgrind01.log matches
Binary file local_address/000/valgrind00.log matches
mariadb_tests_hartmut/valgrind00.log:==23615==    indirectly lost: 325,558 bytes in 475 blocks
masking_auto_firewall/000/valgrind00.log:==27580==    indirectly lost: 320,675 bytes in 433 blocks
maxctrl_basic/000/valgrind00.log:==29865==    indirectly lost: 1,067,071 bytes in 1,946 blocks
maxinfo_sql/000/valgrind00.log:==24240==    indirectly lost: 659,332 bytes in 1,024 blocks
maxscale_process_user/000/valgrind00.log:==25007==    indirectly lost: 963,121 bytes in 1,417 blocks
mm/000/valgrind00.log:==25471==    indirectly lost: 961,776 bytes in 1,337 blocks
Binary file mxs1045/000/valgrind00.log matches
mxs1310_implicit_db/000/valgrind00.log:==22709==    indirectly lost: 321,488 bytes in 499 blocks
mxs1323_stress/000/valgrind00.log:==23615==    indirectly lost: 325,558 bytes in 475 blocks
mxs1418/000/valgrind00.log:==31899==    indirectly lost: 978,588 bytes in 1,432 blocks
mxs1468/000/valgrind00.log:==2040==    indirectly lost: 242,063 bytes in 427 blocks
mxs1476/000/valgrind00.log:==3726==    indirectly lost: 240,664 bytes in 323 blocks
mxs1503_queued_sescmd/000/valgrind00.log:==3108==    indirectly lost: 320,160 bytes in 421 blocks
Binary file mxs1506_no_master/000/valgrind00.log matches
mxs1507_migrate_trx/000/valgrind00.log:==27348==    indirectly lost: 339,880 bytes in 690 blocks
mxs1509/000/valgrind00.log:==4090==    indirectly lost: 321,504 bytes in 501 blocks
mxs1516/000/valgrind00.log:==4635==    indirectly lost: 978,148 bytes in 1,427 blocks
mxs1542/000/valgrind00.log:==28650==    indirectly lost: 450,628 bytes in 640 blocks
mxs1583_fwf/000/valgrind00.log:==19804==    indirectly lost: 960,480 bytes in 1,259 blocks
mxs1585/000/valgrind00.log:==5838==    indirectly lost: 881,728 bytes in 1,168 blocks
mxs1677_temp_table/000/valgrind00.log:==7673==    indirectly lost: 978,148 bytes in 1,427 blocks
mxs1678_relay_master/000/valgrind00.log:==8152==    indirectly lost: 963,121 bytes in 1,417 blocks
mxs1719/000/valgrind00.log:==19136==    indirectly lost: 320,849 bytes in 435 blocks
Binary file mxs173_throttle_filter/000/valgrind00.log matches
mxs1773_failing_ldli/000/valgrind00.log:==10976==    indirectly lost: 904,884 bytes in 1,143 blocks
mxs1778_causal_reads/000/valgrind00.log:==29517==    indirectly lost: 337,876 bytes in 591 blocks
mxs1787_slave_reconnection/000/valgrind00.log:==12534==    indirectly lost: 1,021,308 bytes in 1,342 blocks
mxs1824_double_cursor/000/valgrind00.log:==14042==    indirectly lost: 1,175,972 bytes in 1,536 blocks
mxs1836_show_eventTimes/000/valgrind00.log:==8177==    indirectly lost: 80,032 bytes in 106 blocks
mxs1873_large_sescmd/000/valgrind00.log:==15503==    indirectly lost: 1,241,828 bytes in 1,585 blocks
Binary file mxs1889/000/valgrind00.log matches
mxs1896_load_data_infile/000/valgrind00.log:==15974==    indirectly lost: 1,287,204 bytes in 1,672 blocks
mxs1926_killed_server/000/valgrind00.log:==11135==    indirectly lost: 130,244 bytes in 309 blocks
Binary file mxs1958_insert_priv/000/valgrind00.log matches
mxs1980_blr_galera_server_ids/000/valgrind02.log:==6763==    indirectly lost: 13,672 bytes in 68 blocks
mxs1980_blr_galera_server_ids/000/valgrind00.log:==6277==    indirectly lost: 80,048 bytes in 107 blocks
mxs2106_avro_null/000/valgrind00.log:==24489==    indirectly lost: 423,564 bytes in 652 blocks
mxs2167_extra_port/000/valgrind01.log:==31265==    indirectly lost: 339,549 bytes in 609 blocks
mxs2187_multi_replay/000/valgrind00.log:==28659==    indirectly lost: 339,148 bytes in 605 blocks
mxs2295_change_user_loop/000/valgrind00.log:==26546==    indirectly lost: 351,140 bytes in 558 blocks
mxs2326_hint_clone/000/valgrind00.log:==27873==    indirectly lost: 463,438 bytes in 702 blocks
mxs359_error_on_write/000/valgrind00.log:==29149==    indirectly lost: 321,040 bytes in 431 blocks
mxs359_master_switch/000/valgrind00.log:==27404==    indirectly lost: 339,196 bytes in 606 blocks
mxs359_read_only/000/valgrind00.log:==28311==    indirectly lost: 339,196 bytes in 606 blocks
mxs431/000/valgrind00.log:==32302==    indirectly lost: 338,193 bytes in 591 blocks
mxs548_short_session_change_user/000/valgrind00.log:==32753==    indirectly lost: 250,308 bytes in 487 blocks
mxs559_block_master/000/valgrind00.log:==889==    indirectly lost: 970,139 bytes in 1,352 blocks
mxs564_big_dump/000/valgrind00.log:==3121==    indirectly lost: 979,469 bytes in 1,442 blocks
mxs701_binlog_filter/000/valgrind01.log:==23037==    indirectly lost: 253,816 bytes in 383 blocks
mxs701_binlog_filter/000/valgrind00.log:==22641==    indirectly lost: 321,957 bytes in 487 blocks
mxs716/000/valgrind00.log:==5268==    indirectly lost: 961,312 bytes in 1,267 blocks
mxs729_maxadmin/000/valgrind00.log:==6583==    indirectly lost: 963,121 bytes in 1,417 blocks
mxs827_write_timeout/000/valgrind00.log:==9423==    indirectly lost: 960,872 bytes in 1,262 blocks
mxs874_slave_recovery/000/valgrind00.log:==10796==    indirectly lost: 960,784 bytes in 1,261 blocks
mxs922_scaling/000/valgrind00.log:==20098==    indirectly lost: 596,334 bytes in 1,251 blocks
mxs951_utfmb4/000/valgrind01.log:==29389==    indirectly lost: 898,124 bytes in 1,323 blocks
mxs951_utfmb4/000/valgrind00.log:==29254==    indirectly lost: 963,121 bytes in 1,417 blocks
mysqlmon_external_master/000/valgrind00.log:==18728==    indirectly lost: 334,531 bytes in 561 blocks
mysqlmon_failover_auto/000/valgrind01.log:==3187==    indirectly lost: 606,644 bytes in 1,052 blocks
mysqlmon_failover_manual/000/valgrind02.log:==5553==    indirectly lost: 656,537 bytes in 1,065 blocks
mysqlmon_failover_manual/000/valgrind00.log:==4111==    indirectly lost: 979,468 bytes in 1,442 blocks
mysqlmon_failover_manual2_2/000/valgrind00.log:==7205==    indirectly lost: 960,696 bytes in 1,260 blocks
mysqlmon_failover_no_slaves/000/valgrind00.log:==9617==    indirectly lost: 977,556 bytes in 1,431 blocks
mysqlmon_failover_rejoin_old_slave/000/valgrind00.log:==14963==    indirectly lost: 974,276 bytes in 1,391 blocks
mysqlmon_fail_switch_events/000/valgrind00.log:==19621==    indirectly lost: 960,872 bytes in 1,262 blocks
mysqlmon_rejoin_bad2/000/valgrind00.log:==11903==    indirectly lost: 975,156 bytes in 1,401 blocks
mysqlmon_rejoin_good/000/valgrind00.log:==10211==    indirectly lost: 979,468 bytes in 1,442 blocks
mysqlmon_reset_replication/000/valgrind01.log:==20729==    indirectly lost: 738,956 bytes in 1,125 blocks
mysqlmon_switchover_bad_master/000/valgrind00.log:==8627==    indirectly lost: 974,276 bytes in 1,391 blocks
mysqlmon_switchover_stress/000/valgrind00.log:==17977==    indirectly lost: 961,752 bytes in 1,272 blocks
namedserverfilter_test/000/valgrind00.log:==17012==    indirectly lost: 1,202,144 bytes in 1,470 blocks
pers_02/000/valgrind00.log:==18421==    indirectly lost: 1,592,962 bytes in 1,939 blocks
proxy_protocol/000/valgrind00.log:==29844==    indirectly lost: 965,471 bytes in 1,307 blocks
readconnrouter_slave/000/valgrind00.log:==19879==    indirectly lost: 1,116,193 bytes in 1,446 blocks
rwsplit_conn_num/000/valgrind00.log:==22060==    indirectly lost: 800,388 bytes in 1,091 blocks
rwsplit_multi_stmt/000/valgrind00.log:==24046==    indirectly lost: 1,121,972 bytes in 1,445 blocks
rwsplit_readonly_stress/000/valgrind00.log:==23595==    indirectly lost: 734,647 bytes in 1,077 blocks
sanity_check/000/valgrind00.log:==6364==    indirectly lost: 979,028 bytes in 1,437 blocks
schemarouter_duplicate/000/valgrind00.log:==24624==    indirectly lost: 630,369 bytes in 775 blocks
session_limits/000/valgrind00.log:==27232==    indirectly lost: 953,276 bytes in 1,295 blocks
setup_binlog/000/valgrind00.log:==19763==    indirectly lost: 320,876 bytes in 460 blocks
setup_binlog_gtid/000/valgrind00.log:==21369==    indirectly lost: 320,876 bytes in 460 blocks
short_sessions/000/valgrind00.log:==28697==    indirectly lost: 796,636 bytes in 1,090 blocks
",2,"vagrant@max-tst-02:~/LOGS/run_test-986/LOGS$ grep ""indirectly lost:"" * -r | grep -v ""indirectly lost: 0 bytes""
alter_router/000/valgrind01.log:==25657==    indirectly lost: 912,036 bytes in 1,391 blocks
alter_router/000/valgrind00.log:==25401==    indirectly lost: 894,536 bytes in 1,224 blocks
avro/000/valgrind00.log:==30169==    indirectly lost: 455,706 bytes in 664 blocks
binary_ps/000/valgrind00.log:==29828==    indirectly lost: 978,148 bytes in 1,427 blocks
binary_ps_cursor/000/valgrind00.log:==30308==    indirectly lost: 960,872 bytes in 1,262 blocks
binlog_change_master/000/valgrind01.log:==31962==    indirectly lost: 268,064 bytes in 343 blocks
binlog_change_master_gtid/000/valgrind00.log:==32673==    indirectly lost: 320,876 bytes in 460 blocks
binlog_incompl/000/valgrind00.log:==1494==    indirectly lost: 679 bytes in 41 blocks
binlog_semisync_txs0_ss0/000/valgrind00.log:==5311==    indirectly lost: 320,876 bytes in 460 blocks
binlog_semisync_txs0_ss1/000/valgrind00.log:==8324==    indirectly lost: 320,876 bytes in 460 blocks
bug471/000/valgrind00.log:==8291==    indirectly lost: 913,308 bytes in 1,434 blocks
bug473/000/valgrind00.log:==8746==    indirectly lost: 978,903 bytes in 1,447 blocks
bug475/000/valgrind00.log:==9202==    indirectly lost: 960,936 bytes in 1,266 blocks
bug519/000/valgrind00.log:==9685==    indirectly lost: 960,872 bytes in 1,262 blocks
bug547/000/valgrind00.log:==10577==    indirectly lost: 978,148 bytes in 1,427 blocks
bug571/000/valgrind00.log:==13488==    indirectly lost: 900,628 bytes in 1,397 blocks
bug587/000/valgrind00.log:==14913==    indirectly lost: 833,172 bytes in 1,330 blocks
bug587_1/000/valgrind00.log:==15368==    indirectly lost: 899,684 bytes in 1,370 blocks
bug620/000/valgrind00.log:==16813==    indirectly lost: 978,588 bytes in 1,432 blocks
bug654/000/valgrind00.log:==18273==    indirectly lost: 975,780 bytes in 1,389 blocks
bug658/000/valgrind00.log:==19344==    indirectly lost: 978,588 bytes in 1,432 blocks
bug662/000/valgrind01.log:==19932==    indirectly lost: 778,636 bytes in 1,301 blocks
bug664/000/valgrind00.log:==20380==    indirectly lost: 1,283,737 bytes in 1,857 blocks
bug705/000/valgrind01.log:==21549==    indirectly lost: 883,049 bytes in 1,312 blocks
bug729/000/valgrind00.log:==22008==    indirectly lost: 978,148 bytes in 1,427 blocks
bulk_insert/000/valgrind00.log:==22928==    indirectly lost: 818,411 bytes in 1,157 blocks
cache_runtime_ttl/000/valgrind00.log:==28801==    indirectly lost: 253,888 bytes in 387 blocks
ccrfilter_test/000/valgrind01.log:==23556==    indirectly lost: 977,599 bytes in 1,450 blocks
ccrfilter_test/000/valgrind00.log:==23366==    indirectly lost: 912,120 bytes in 1,411 blocks
check_backend/000/valgrind00.log:==4945==    indirectly lost: 1,231,788 bytes in 1,807 blocks
connect_to_nonexisting_db/000/valgrind00.log:==25112==    indirectly lost: 978,588 bytes in 1,432 blocks
Binary file different_size_binlog/000/valgrind01.log matches
different_size_binlog/000/valgrind00.log:==15175==    indirectly lost: 320,876 bytes in 460 blocks
encrypted_passwords/000/valgrind00.log:==29115==    indirectly lost: 963,121 bytes in 1,417 blocks
fwf/000/valgrind06.log:==8694==    indirectly lost: 815,175 bytes in 1,179 blocks
fwf/000/valgrind09.log:==9028==    indirectly lost: 748,663 bytes in 1,139 blocks
fwf/000/valgrind01.log:==8143==    indirectly lost: 894,167 bytes in 1,282 blocks
fwf/000/valgrind17.log:==9915==    indirectly lost: 814,279 bytes in 1,161 blocks
fwf/000/valgrind12.log:==9360==    indirectly lost: 815,175 bytes in 1,179 blocks
fwf/000/valgrind08.log:==8917==    indirectly lost: 815,039 bytes in 1,178 blocks
fwf/000/valgrind10.log:==9139==    indirectly lost: 746,735 bytes in 1,120 blocks
fwf/000/valgrind00.log:==8031==    indirectly lost: 961,711 bytes in 1,323 blocks
fwf/000/valgrind19.log:==10220==    indirectly lost: 800,432 bytes in 1,051 blocks
fwf/000/valgrind16.log:==9802==    indirectly lost: 814,143 bytes in 1,178 blocks
fwf/000/valgrind07.log:==8805==    indirectly lost: 748,799 bytes in 1,140 blocks
fwf2/000/valgrind02.log:==11110==    indirectly lost: 961,711 bytes in 1,323 blocks
fwf2/000/valgrind03.log:==11302==    indirectly lost: 881,687 bytes in 1,219 blocks
fwf_actions/000/valgrind00.log:==12917==    indirectly lost: 960,600 bytes in 1,266 blocks
fwf_com_ping/000/valgrind00.log:==19314==    indirectly lost: 321,423 bytes in 486 blocks
fwf_duplicate_rules/000/valgrind00.log:==11999==    indirectly lost: 3,706 bytes in 187 blocks
fwf_logging/000/valgrind00.log:==13506==    indirectly lost: 320,200 bytes in 422 blocks
fwf_prepared_stmt/000/valgrind00.log:==12427==    indirectly lost: 960,480 bytes in 1,259 blocks
fwf_syntax/000/valgrind02.log:==15671==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind06.log:==16604==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind09.log:==17305==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind14.log:==18472==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind15.log:==18705==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind01.log:==15438==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind12.log:==18005==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind03.log:==15904==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind08.log:==17071==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind04.log:==16137==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind11.log:==17772==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind05.log:==16371==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind13.log:==18239==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind10.log:==17539==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind00.log:==15205==    indirectly lost: 672 bytes in 40 blocks
fwf_syntax/000/valgrind07.log:==16838==    indirectly lost: 672 bytes in 40 blocks
galera_priority/000/valgrind00.log:==704==    indirectly lost: 240,539 bytes in 333 blocks
keepalived_masterdown/000/valgrind02.log:==32760==    indirectly lost: 747,243 bytes in 1,389 blocks
load_balancing_pers10/000/valgrind00.log:==21657==    indirectly lost: 962,219 bytes in 1,278 blocks
Binary file local_address/000/valgrind01.log matches
Binary file local_address/000/valgrind00.log matches
mariadb_tests_hartmut/valgrind00.log:==23615==    indirectly lost: 325,558 bytes in 475 blocks
masking_auto_firewall/000/valgrind00.log:==27580==    indirectly lost: 320,675 bytes in 433 blocks
maxctrl_basic/000/valgrind00.log:==29865==    indirectly lost: 1,067,071 bytes in 1,946 blocks
maxinfo_sql/000/valgrind00.log:==24240==    indirectly lost: 659,332 bytes in 1,024 blocks
maxscale_process_user/000/valgrind00.log:==25007==    indirectly lost: 963,121 bytes in 1,417 blocks
mm/000/valgrind00.log:==25471==    indirectly lost: 961,776 bytes in 1,337 blocks
Binary file mxs1045/000/valgrind00.log matches
mxs1310_implicit_db/000/valgrind00.log:==22709==    indirectly lost: 321,488 bytes in 499 blocks
mxs1323_stress/000/valgrind00.log:==23615==    indirectly lost: 325,558 bytes in 475 blocks
mxs1418/000/valgrind00.log:==31899==    indirectly lost: 978,588 bytes in 1,432 blocks
mxs1468/000/valgrind00.log:==2040==    indirectly lost: 242,063 bytes in 427 blocks
mxs1476/000/valgrind00.log:==3726==    indirectly lost: 240,664 bytes in 323 blocks
mxs1503_queued_sescmd/000/valgrind00.log:==3108==    indirectly lost: 320,160 bytes in 421 blocks
Binary file mxs1506_no_master/000/valgrind00.log matches
mxs1507_migrate_trx/000/valgrind00.log:==27348==    indirectly lost: 339,880 bytes in 690 blocks
mxs1509/000/valgrind00.log:==4090==    indirectly lost: 321,504 bytes in 501 blocks
mxs1516/000/valgrind00.log:==4635==    indirectly lost: 978,148 bytes in 1,427 blocks
mxs1542/000/valgrind00.log:==28650==    indirectly lost: 450,628 bytes in 640 blocks
mxs1583_fwf/000/valgrind00.log:==19804==    indirectly lost: 960,480 bytes in 1,259 blocks
mxs1585/000/valgrind00.log:==5838==    indirectly lost: 881,728 bytes in 1,168 blocks
mxs1677_temp_table/000/valgrind00.log:==7673==    indirectly lost: 978,148 bytes in 1,427 blocks
mxs1678_relay_master/000/valgrind00.log:==8152==    indirectly lost: 963,121 bytes in 1,417 blocks
mxs1719/000/valgrind00.log:==19136==    indirectly lost: 320,849 bytes in 435 blocks
Binary file mxs173_throttle_filter/000/valgrind00.log matches
mxs1773_failing_ldli/000/valgrind00.log:==10976==    indirectly lost: 904,884 bytes in 1,143 blocks
mxs1778_causal_reads/000/valgrind00.log:==29517==    indirectly lost: 337,876 bytes in 591 blocks
mxs1787_slave_reconnection/000/valgrind00.log:==12534==    indirectly lost: 1,021,308 bytes in 1,342 blocks
mxs1824_double_cursor/000/valgrind00.log:==14042==    indirectly lost: 1,175,972 bytes in 1,536 blocks
mxs1836_show_eventTimes/000/valgrind00.log:==8177==    indirectly lost: 80,032 bytes in 106 blocks
mxs1873_large_sescmd/000/valgrind00.log:==15503==    indirectly lost: 1,241,828 bytes in 1,585 blocks
Binary file mxs1889/000/valgrind00.log matches
mxs1896_load_data_infile/000/valgrind00.log:==15974==    indirectly lost: 1,287,204 bytes in 1,672 blocks
mxs1926_killed_server/000/valgrind00.log:==11135==    indirectly lost: 130,244 bytes in 309 blocks
Binary file mxs1958_insert_priv/000/valgrind00.log matches
mxs1980_blr_galera_server_ids/000/valgrind02.log:==6763==    indirectly lost: 13,672 bytes in 68 blocks
mxs1980_blr_galera_server_ids/000/valgrind00.log:==6277==    indirectly lost: 80,048 bytes in 107 blocks
mxs2106_avro_null/000/valgrind00.log:==24489==    indirectly lost: 423,564 bytes in 652 blocks
mxs2167_extra_port/000/valgrind01.log:==31265==    indirectly lost: 339,549 bytes in 609 blocks
mxs2187_multi_replay/000/valgrind00.log:==28659==    indirectly lost: 339,148 bytes in 605 blocks
mxs2295_change_user_loop/000/valgrind00.log:==26546==    indirectly lost: 351,140 bytes in 558 blocks
mxs2326_hint_clone/000/valgrind00.log:==27873==    indirectly lost: 463,438 bytes in 702 blocks
mxs359_error_on_write/000/valgrind00.log:==29149==    indirectly lost: 321,040 bytes in 431 blocks
mxs359_master_switch/000/valgrind00.log:==27404==    indirectly lost: 339,196 bytes in 606 blocks
mxs359_read_only/000/valgrind00.log:==28311==    indirectly lost: 339,196 bytes in 606 blocks
mxs431/000/valgrind00.log:==32302==    indirectly lost: 338,193 bytes in 591 blocks
mxs548_short_session_change_user/000/valgrind00.log:==32753==    indirectly lost: 250,308 bytes in 487 blocks
mxs559_block_master/000/valgrind00.log:==889==    indirectly lost: 970,139 bytes in 1,352 blocks
mxs564_big_dump/000/valgrind00.log:==3121==    indirectly lost: 979,469 bytes in 1,442 blocks
mxs701_binlog_filter/000/valgrind01.log:==23037==    indirectly lost: 253,816 bytes in 383 blocks
mxs701_binlog_filter/000/valgrind00.log:==22641==    indirectly lost: 321,957 bytes in 487 blocks
mxs716/000/valgrind00.log:==5268==    indirectly lost: 961,312 bytes in 1,267 blocks
mxs729_maxadmin/000/valgrind00.log:==6583==    indirectly lost: 963,121 bytes in 1,417 blocks
mxs827_write_timeout/000/valgrind00.log:==9423==    indirectly lost: 960,872 bytes in 1,262 blocks
mxs874_slave_recovery/000/valgrind00.log:==10796==    indirectly lost: 960,784 bytes in 1,261 blocks
mxs922_scaling/000/valgrind00.log:==20098==    indirectly lost: 596,334 bytes in 1,251 blocks
mxs951_utfmb4/000/valgrind01.log:==29389==    indirectly lost: 898,124 bytes in 1,323 blocks
mxs951_utfmb4/000/valgrind00.log:==29254==    indirectly lost: 963,121 bytes in 1,417 blocks
mysqlmon_external_master/000/valgrind00.log:==18728==    indirectly lost: 334,531 bytes in 561 blocks
mysqlmon_failover_auto/000/valgrind01.log:==3187==    indirectly lost: 606,644 bytes in 1,052 blocks
mysqlmon_failover_manual/000/valgrind02.log:==5553==    indirectly lost: 656,537 bytes in 1,065 blocks
mysqlmon_failover_manual/000/valgrind00.log:==4111==    indirectly lost: 979,468 bytes in 1,442 blocks
mysqlmon_failover_manual2_2/000/valgrind00.log:==7205==    indirectly lost: 960,696 bytes in 1,260 blocks
mysqlmon_failover_no_slaves/000/valgrind00.log:==9617==    indirectly lost: 977,556 bytes in 1,431 blocks
mysqlmon_failover_rejoin_old_slave/000/valgrind00.log:==14963==    indirectly lost: 974,276 bytes in 1,391 blocks
mysqlmon_fail_switch_events/000/valgrind00.log:==19621==    indirectly lost: 960,872 bytes in 1,262 blocks
mysqlmon_rejoin_bad2/000/valgrind00.log:==11903==    indirectly lost: 975,156 bytes in 1,401 blocks
mysqlmon_rejoin_good/000/valgrind00.log:==10211==    indirectly lost: 979,468 bytes in 1,442 blocks
mysqlmon_reset_replication/000/valgrind01.log:==20729==    indirectly lost: 738,956 bytes in 1,125 blocks
mysqlmon_switchover_bad_master/000/valgrind00.log:==8627==    indirectly lost: 974,276 bytes in 1,391 blocks
mysqlmon_switchover_stress/000/valgrind00.log:==17977==    indirectly lost: 961,752 bytes in 1,272 blocks
namedserverfilter_test/000/valgrind00.log:==17012==    indirectly lost: 1,202,144 bytes in 1,470 blocks
pers_02/000/valgrind00.log:==18421==    indirectly lost: 1,592,962 bytes in 1,939 blocks
proxy_protocol/000/valgrind00.log:==29844==    indirectly lost: 965,471 bytes in 1,307 blocks
readconnrouter_slave/000/valgrind00.log:==19879==    indirectly lost: 1,116,193 bytes in 1,446 blocks
rwsplit_conn_num/000/valgrind00.log:==22060==    indirectly lost: 800,388 bytes in 1,091 blocks
rwsplit_multi_stmt/000/valgrind00.log:==24046==    indirectly lost: 1,121,972 bytes in 1,445 blocks
rwsplit_readonly_stress/000/valgrind00.log:==23595==    indirectly lost: 734,647 bytes in 1,077 blocks
sanity_check/000/valgrind00.log:==6364==    indirectly lost: 979,028 bytes in 1,437 blocks
schemarouter_duplicate/000/valgrind00.log:==24624==    indirectly lost: 630,369 bytes in 775 blocks
session_limits/000/valgrind00.log:==27232==    indirectly lost: 953,276 bytes in 1,295 blocks
setup_binlog/000/valgrind00.log:==19763==    indirectly lost: 320,876 bytes in 460 blocks
setup_binlog_gtid/000/valgrind00.log:==21369==    indirectly lost: 320,876 bytes in 460 blocks
short_sessions/000/valgrind00.log:==28697==    indirectly lost: 796,636 bytes in 1,090 blocks
"
3205,MXS-2344,MXS,Esa Korhonen,126340,2019-04-15 16:28:55,"This is going to 2.3.6 now. Set ""replication_master_ssl=1"" in MariaDB Monitor configuration, and it will add ""MASTER_SSL=1"" to the CHANGE MASTER TO commands. As noted in the description, this should only be enabled if ssl is otherwise configured on the backend servers. Users should also force ssl on the replication user by running a command like ""ALTER USER repl@'%' REQUIRE SSL;"". This way the user cannot even connect unless encryption is on. See https://mariadb.com/kb/en/library/securing-connections-for-client-and-server/ for more information.",1,"This is going to 2.3.6 now. Set ""replication_master_ssl=1"" in MariaDB Monitor configuration, and it will add ""MASTER_SSL=1"" to the CHANGE MASTER TO commands. As noted in the description, this should only be enabled if ssl is otherwise configured on the backend servers. Users should also force ssl on the replication user by running a command like ""ALTER USER repl@'%' REQUIRE SSL;"". This way the user cannot even connect unless encryption is on. See URL for more information."
3206,MXS-2353,MXS,markus makela,168329,2020-10-08 17:06:17,This is less important now that the log is available in MaxGUI and it can be filtered by a particular service.,1,This is less important now that the log is available in MaxGUI and it can be filtered by a particular service.
3207,MXS-2353,MXS,markus makela,206976,2021-11-25 08:47:47,"Added {{log_warning}}, {{log_notice}}, {{log_info}} and {{log_debug}} to the services. These allow the logging levels to be enabled per service if they are globally disabled.",2,"Added {{log_warning}}, {{log_notice}}, {{log_info}} and {{log_debug}} to the services. These allow the logging levels to be enabled per service if they are globally disabled."
3208,MXS-2383,MXS,Esa Korhonen,125618,2019-04-01 10:26:21,"This would likely be difficult to implement. MaxScale needs to use the information given by the client to log into the backends. A password is straightforward, as it can be used as is. Complicated schemes such the ones mentioned would require that MaxScale has access to these extra services and understands them. We would need specifics to implement them.",1,"This would likely be difficult to implement. MaxScale needs to use the information given by the client to log into the backends. A password is straightforward, as it can be used as is. Complicated schemes such the ones mentioned would require that MaxScale has access to these extra services and understands them. We would need specifics to implement them."
3209,MXS-2383,MXS,Geoff Montee,125685,2019-04-01 19:41:42,"Hi [~esa.korhonen],

{quote}
Complicated schemes such the ones mentioned would require that MaxScale has access to these extra services and understands them. We would need specifics to implement them. We would need specifics to implement them.
{quote}

Why would MaxScale need to ""understand"" them or worry about the specifics? MariaDB server doesn't ""understand"" those more complicated PAM services, and it still supports them. The underlying PAM framework handles the implementation of the PAM service. MaxScale and MariaDB Server just need to be able to ask the user for some kind of password, token, or whatever the user input is in the scenario when PAM asks for it, and MaxScale and MariaDB Server need to be able to handle PAM asking for more than one password, token, or whatever the user input is in the scenario.

Maybe MaxScale needs to implement some form of the dialog callback? This is what MariaDB uses to get user input when PAM authentication is involved:

https://mariadb.com/kb/en/library/development-pluggable-authentication/#dialog-client-plugin

https://mariadb.com/kb/en/library/authentication-plugin-pam/#dialog-plugin-for-clients",2,"Hi [~esa.korhonen],

{quote}
Complicated schemes such the ones mentioned would require that MaxScale has access to these extra services and understands them. We would need specifics to implement them. We would need specifics to implement them.
{quote}

Why would MaxScale need to ""understand"" them or worry about the specifics? MariaDB server doesn't ""understand"" those more complicated PAM services, and it still supports them. The underlying PAM framework handles the implementation of the PAM service. MaxScale and MariaDB Server just need to be able to ask the user for some kind of password, token, or whatever the user input is in the scenario when PAM asks for it, and MaxScale and MariaDB Server need to be able to handle PAM asking for more than one password, token, or whatever the user input is in the scenario.

Maybe MaxScale needs to implement some form of the dialog callback? This is what MariaDB uses to get user input when PAM authentication is involved:

URL

URL"
3210,MXS-2443,MXS,markus makela,126592,2019-04-21 14:52:28,"The fact that this doesn't work is expected behavior as the feature is not supposed to resolve cross-connection dependencies. As long as the same connection object that is given by the connection pool is used to do all operations, the feature should guarantee happens-before ordering on the whole cluster. If a different connection object, analogous to a command line client connection, is used the relationship between the two is lost.

Although outside of the scope of the original feature, an enhancement to it could be made so that the GTID information is stored globally. This would guarantee a relatively sequential order of events across all connections but each write would cause all other reads to wait for it to replicate.

An alternative implementation would be to probe the GTID via replication or polling the servers and route reads to the master if the slaves are lagging behind. ",1,"The fact that this doesn't work is expected behavior as the feature is not supposed to resolve cross-connection dependencies. As long as the same connection object that is given by the connection pool is used to do all operations, the feature should guarantee happens-before ordering on the whole cluster. If a different connection object, analogous to a command line client connection, is used the relationship between the two is lost.

Although outside of the scope of the original feature, an enhancement to it could be made so that the GTID information is stored globally. This would guarantee a relatively sequential order of events across all connections but each write would cause all other reads to wait for it to replicate.

An alternative implementation would be to probe the GTID via replication or polling the servers and route reads to the master if the slaves are lagging behind. "
3211,MXS-2443,MXS,markus makela,127036,2019-04-29 13:29:12,The method described in MXS-1720 would solve this if taken into use on a MaxScale-wide level.,2,The method described in MXS-1720 would solve this if taken into use on a MaxScale-wide level.
3212,MXS-2443,MXS,markus makela,136989,2019-11-05 08:50:42,Added the new {{causal_reads_mode}} parameter that accepts either {{local}} (the default) or {{global}}. The latter causes the writes to be visible across all connections.,3,Added the new {{causal_reads_mode}} parameter that accepts either {{local}} (the default) or {{global}}. The latter causes the writes to be visible across all connections.
3213,MXS-2456,MXS,markus makela,126894,2019-04-26 08:43:54,A timeout might be an interesting alternative.,1,A timeout might be an interesting alternative.
3214,MXS-2456,MXS,markus makela,127239,2019-05-02 07:14:30,Added {{transaction_replay_attempts}} that limits the number of replay attempts. The default value is 5 attempts.,2,Added {{transaction_replay_attempts}} that limits the number of replay attempts. The default value is 5 attempts.
3215,MXS-2480,MXS,markus makela,127647,2019-05-13 05:14:54,The default authenticator stores the data only in memory as the information is always available from a backend.,1,The default authenticator stores the data only in memory as the information is always available from a backend.
3216,MXS-2497,MXS,Todd Stoffel,128162,2019-05-21 19:59:28,"I would like to quantify ""a lot of users"". If this is one case, maybe they can sponsor an NRE project.",1,"I would like to quantify ""a lot of users"". If this is one case, maybe they can sponsor an NRE project."
3217,MXS-2505,MXS,markus makela,140049,2019-12-13 12:52:15,Made the {{connection_keepalive}} parameter a common service parameter (all routers can use it) and added support for idle session keepalive (no queries are needed to keep all connections alive).,1,Made the {{connection_keepalive}} parameter a common service parameter (all routers can use it) and added support for idle session keepalive (no queries are needed to keep all connections alive).
3218,MXS-2510,MXS,markus makela,128546,2019-05-29 08:57:49,"Using the 10.4 provided via docker seems to work:
* users are loaded correctly
* roles work
* users with and without passwords work",1,"Using the 10.4 provided via docker seems to work:
* users are loaded correctly
* roles work
* users with and without passwords work"
3219,MXS-2510,MXS,markus makela,128688,2019-05-31 12:27:33,Closing as fixed as preliminary testing shows no changes. Full verification depends on switching to 10.4 for testing.,2,Closing as fixed as preliminary testing shows no changes. Full verification depends on switching to 10.4 for testing.
3220,MXS-2514,MXS,Timofey Turenko,129050,2019-06-10 09:48:47,"ttp://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs2295_change_user_loop/000/core-maxscale-6-997-994-7272-1559784893 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs2450_change_user_crash/000/core-maxscale-6-997-994-8979-1559784950 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/change_user/000/core-maxscale-6-997-994-21999-1559772327 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/bug601/000/core-maxscale-6-997-994-15483-1559772083 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2946-1559780036 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-3032-1559780039 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-3115-1559780043 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2690-1559780028 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2601-1559780025 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2363-1559780021 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2517-1559780023 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2858-1559780033 \
http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs548_short_session_change_user/000/core-maxscale-6-997-994-2774-1559780030 \


24 - bug601 (Failed) 
26 - bug626 (Failed) 
39 - change_user (Failed) 
64 - mysqlmon_rejoin_manual (Failed) 
86 - bug676 (Failed) 
87 - galera_priority (Failed) 
88 - lots_of_rows (Failed) 
89 - mxs244_prepared_stmt_loop (Failed) 
90 - mxs314 (Failed) 
91 - mxs564_big_dump (Failed) 
92 - mxs1476 (Failed) 
93 - mxs1751_available_when_donor_crash (Failed) 
94 - pers_01 (Failed) 
95 - mxs1980_blr_galera_server_ids (Failed) 
113 - mxs1110_16mb (Failed) 
118 - mariadb_tests_hartmut_galera.sh (Failed) 
132 - mxs361 (Failed) 
136 - mxs548_short_session_change_user (Failed) 
186 - mxs1585 (Failed) 
202 - mxs1828_double_local_infile (Failed) 
210 - pers_02 (Failed) 
224 - script (Failed) 
266 - mxs2295_change_user_loop (Failed) 
270 - mxs2450_change_user_crash (Failed) 
271 - pam_authentication (Failed) 
275 - avro_alter (Failed) 
276 - binlog_change_master (Failed) 
278 - binlog_semisync (Exception: SegFault) 
279 - binlog_semisync_txs0_ss0 (Exception: SegFault) 
280 - binlog_semisync_txs0_ss1 (Exception: SegFault) 
281 - binlog_semisync_txs1_ss0 (Exception: SegFault) 
282 - cdc_client (Failed) 
287 - setup_binlog (Exception: SegFault) 

http://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/",1,"ttp://max-tst-01.mariadb.com/LOGS/run_test-277/LOGS/mxs2295_change_user_loop/000/core-maxscale-6-997-994-7272-1559784893 \
URL \
URL \
URL \
URL \
URL \
URL \
URL \
URL \
URL \
URL \
URL \
URL \


24 - bug601 (Failed) 
26 - bug626 (Failed) 
39 - change_user (Failed) 
64 - mysqlmon_rejoin_manual (Failed) 
86 - bug676 (Failed) 
87 - galera_priority (Failed) 
88 - lots_of_rows (Failed) 
89 - mxs244_prepared_stmt_loop (Failed) 
90 - mxs314 (Failed) 
91 - mxs564_big_dump (Failed) 
92 - mxs1476 (Failed) 
93 - mxs1751_available_when_donor_crash (Failed) 
94 - pers_01 (Failed) 
95 - mxs1980_blr_galera_server_ids (Failed) 
113 - mxs1110_16mb (Failed) 
118 - mariadb_tests_hartmut_galera.sh (Failed) 
132 - mxs361 (Failed) 
136 - mxs548_short_session_change_user (Failed) 
186 - mxs1585 (Failed) 
202 - mxs1828_double_local_infile (Failed) 
210 - pers_02 (Failed) 
224 - script (Failed) 
266 - mxs2295_change_user_loop (Failed) 
270 - mxs2450_change_user_crash (Failed) 
271 - pam_authentication (Failed) 
275 - avro_alter (Failed) 
276 - binlog_change_master (Failed) 
278 - binlog_semisync (Exception: SegFault) 
279 - binlog_semisync_txs0_ss0 (Exception: SegFault) 
280 - binlog_semisync_txs0_ss1 (Exception: SegFault) 
281 - binlog_semisync_txs1_ss0 (Exception: SegFault) 
282 - cdc_client (Failed) 
287 - setup_binlog (Exception: SegFault) 

URL"
3221,MXS-2515,MXS,markus makela,129052,2019-06-10 10:01:28,Test how schemarouter -> readwritesplit handling of KILL works.,1,Test how schemarouter -> readwritesplit handling of KILL works.
3222,MXS-2542,MXS,markus makela,129476,2019-06-18 08:31:02,"Might be a nice feature to integrate into the server as a new replication command. A sort of a dump thread could send fake replicated events until a known GTID position is reached. When the position is reached, normal replication would start. You'd essentially cause the whole database to be replicated to a slave via the replication protocol.

Looks like MDEV-15610 describes one way to do it.",1,"Might be a nice feature to integrate into the server as a new replication command. A sort of a dump thread could send fake replicated events until a known GTID position is reached. When the position is reached, normal replication would start. You'd essentially cause the whole database to be replicated to a slave via the replication protocol.

Looks like MDEV-15610 describes one way to do it."
3223,MXS-2542,MXS,Geoff Montee,138338,2019-11-20 22:35:42,"If MDEV-21106 were implemented in MariaDB Server, then MaxScale could use the clone plugin for this.",2,"If MDEV-21106 were implemented in MariaDB Server, then MaxScale could use the clone plugin for this."
3224,MXS-2542,MXS,Max Mether,221376,2022-04-21 13:29:33,"I assume this ticket is for a generic solution, not for SkySQL only? I am also worried about how exactly SkySQL clones servers, there has been issues with that process in the past. 

In any case, this feature is requested by a large amount of users and customers and having it in MaxScale is a great compelling solution. ",3,"I assume this ticket is for a generic solution, not for SkySQL only? I am also worried about how exactly SkySQL clones servers, there has been issues with that process in the past. 

In any case, this feature is requested by a large amount of users and customers and having it in MaxScale is a great compelling solution. "
3225,MXS-2572,MXS,Johan Wikman,130238,2019-07-01 06:22:23,Just a minimal stress-test.,1,Just a minimal stress-test.
3226,MXS-2588,MXS,markus makela,144326,2020-02-14 12:35:37,This isn't really something that needs to be done inside MaxScale. Whatever inserts the data into Kafka could also insert it into MariaDB.,1,This isn't really something that needs to be done inside MaxScale. Whatever inserts the data into Kafka could also insert it into MariaDB.
3227,MXS-2615,MXS,Wagner Bianchi,160768,2020-07-22 14:32:28,Do we know which 2.5 release is going to have this feature?,1,Do we know which 2.5 release is going to have this feature?
3228,MXS-2617,MXS,Johan Wikman,141381,2019-12-31 09:44:13,"Removed epic link; this is an internal caching related matter and not related to the cache filter.
",1,"Removed epic link; this is an internal caching related matter and not related to the cache filter.
"
3229,MXS-2632,MXS,Esa Korhonen,136460,2019-10-28 11:45:48,"Closed for now, add part2 when continuing.",1,"Closed for now, add part2 when continuing."
3230,MXS-2654,MXS,Johan Wikman,134856,2019-09-26 06:35:24,"Now documented under https://github.com/mariadb-corporation/MaxScale/blob/2.3/Documentation/Getting-Started/Configuration-Guide.md#query_classifier_cache_size
",1,"Now documented under URL
"
3231,MXS-2662,MXS,markus makela,223150,2022-05-09 05:22:36,Could use https://github.com/OpenKMIP/libkmip for the client library and PyKMIP for testing it.,1,Could use URL for the client library and PyKMIP for testing it.
3232,MXS-2670,MXS,Esa Korhonen,135415,2019-10-07 13:57:29,Completed for now. Additional cleanup will go to different issues.,1,Completed for now. Additional cleanup will go to different issues.
3233,MXS-2691,MXS,Johan Wikman,141490,2020-01-03 06:31:40,"It must also be possible to do Invalidation and clearing asynchronously as is seems that with redis invalidation can be supported.
",1,"It must also be possible to do Invalidation and clearing asynchronously as is seems that with redis invalidation can be supported.
"
3234,MXS-2692,MXS,Sylvain ARBAUDIE,134530,2019-09-20 20:46:01,"when we tried this configuration : 

{code:java}

[binlog-router]
type=server
address=127.0.0.1
port=5308
protocol=MariaDBBackend

[replication-monitor]
module=mariadbmon
type=monitor
servers=galera1,galera2,galera3,binlog-router
servers=galera1,galera2,galera3,primary1
user=maxscale
password=M4xscale_pw
monitor_interval=500

[Replication]
type=service
router=binlogrouter
#version_string=5.6.17-log
user=maxscale
passwd=M4xscale_pw
router_options=server_id=3,user=maxscale,password=M4xscale_pw,master-id=1,heartbeat=30,binlogdir=/var/binlogs,transaction_safety=1,master_version=5.6.19-common,master_hostname=common_server,mariadb10-compatibility=1

[Replication Listener]
type=listener
service=Replication
protocol=MySQLClient
port=5308

{code}


we had this error message in the maxscale logs : 


{code:java}
2019-09-20 15:11:19   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
2019-09-20 15:11:19   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
2019-09-20 15:11:20   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
2019-09-20 15:11:20   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
{code}
",1,"when we tried this configuration : 

{code:java}

[binlog-router]
type=server
address=127.0.0.1
port=5308
protocol=MariaDBBackend

[replication-monitor]
module=mariadbmon
type=monitor
servers=galera1,galera2,galera3,binlog-router
servers=galera1,galera2,galera3,primary1
user=maxscale
password=M4xscale_pw
monitor_interval=500

[Replication]
type=service
router=binlogrouter
#version_string=5.6.17-log
user=maxscale
passwd=M4xscale_pw
router_options=server_id=3,user=maxscale,password=M4xscale_pw,master-id=1,heartbeat=30,binlogdir=/var/binlogs,transaction_safety=1,master_version=5.6.19-common,master_hostname=common_server,mariadb10-compatibility=1

[Replication Listener]
type=listener
service=Replication
protocol=MySQLClient
port=5308

{code}


we had this error message in the maxscale logs : 


{code:java}
2019-09-20 15:11:19   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
2019-09-20 15:11:19   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
2019-09-20 15:11:20   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
2019-09-20 15:11:20   error  : (7) Unexpected query from ‘maxscale’@‘::ffff:127.0.0.1’: SHOW SLAVE STATUS;
{code}
"
3235,MXS-2692,MXS,Sylvain ARBAUDIE,134531,2019-09-20 20:50:37,i can only guess it might be linked to Markus coments today on slack that maxscale tends to perform too many litterals comparisonswhen trying to interpret command lines.,2,i can only guess it might be linked to Markus coments today on slack that maxscale tends to perform too many litterals comparisonswhen trying to interpret command lines.
3236,MXS-2696,MXS,markus makela,135095,2019-10-01 06:15:43,Decided to not move the masking filter code into the protocol modules. Changed cache filter to use the common code.,1,Decided to not move the masking filter code into the protocol modules. Changed cache filter to use the common code.
3237,MXS-2709,MXS,markus makela,169274,2020-10-19 11:08:07,"-This could be a filter that delays the execution of autocommit INSERT statements and places them into a queue. Once either the queue fills up or a time limit is hit, the queries could be either executed by the next connection or by a separate connection (i.e. {{class LocalClient}}).-",1,"-This could be a filter that delays the execution of autocommit INSERT statements and places them into a queue. Once either the queue fills up or a time limit is hit, the queries could be either executed by the next connection or by a separate connection (i.e. {{class LocalClient}}).-"
3238,MXS-2709,MXS,Naresh Chandra,241585,2022-11-11 10:31:34,"Hi Markus,

Can we add compare/sync option to external servers as well? I mean if we don't add servers in the maxscale conf file, but we want to compare the data and table structures or sync data and structures for external servers would be a good feature for us.

EX: User should be able to add temporary source/destination db server's details in the GUI and then compare external database data/structures from source to destination in the GUI.",2,"Hi Markus,

Can we add compare/sync option to external servers as well? I mean if we don't add servers in the maxscale conf file, but we want to compare the data and table structures or sync data and structures for external servers would be a good feature for us.

EX: User should be able to add temporary source/destination db server's details in the GUI and then compare external database data/structures from source to destination in the GUI."
3239,MXS-2709,MXS,markus makela,248480,2023-01-24 14:06:38,"ODBC-374, ODBC-379 and ODBC-381 are not really a blockers, we found workarounds for them.",3,"ODBC-374, ODBC-379 and ODBC-381 are not really a blockers, we found workarounds for them."
3240,MXS-2719,MXS,Esa Korhonen,166671,2020-09-23 10:55:20,Only switchover done for now. Cancel-option may be added later. ,1,Only switchover done for now. Cancel-option may be added later. 
3241,MXS-272,MXS,Esa Korhonen,93575,2017-03-29 09:03:52,"Actually HintRouter, but connected to NamedServerFilter. ",1,"Actually HintRouter, but connected to NamedServerFilter. "
3242,MXS-2724,MXS,Johan Wikman,164258,2020-08-26 11:23:27,"It would be relatively straightforward to associate all memory allocations made while routing, with the session on behalf of which the memory allocation is made. That would make it possible to track how much memory a particular session consumes and how the memory consumption evolves over time. With some additional work it would also be possible to track which module - protocol, filter, router, etc. - is responsible for performing the memory allocation.
",1,"It would be relatively straightforward to associate all memory allocations made while routing, with the session on behalf of which the memory allocation is made. That would make it possible to track how much memory a particular session consumes and how the memory consumption evolves over time. With some additional work it would also be possible to track which module - protocol, filter, router, etc. - is responsible for performing the memory allocation.
"
3243,MXS-2724,MXS,Johan Wikman,228319,2022-06-30 12:51:04,"In 22.08 there will be slightly more information available regarding the memory usage. {{maxctrl show session}} will return the following:
{code}
├───────────────────┼──────────────────────────────────┤
│ Memory            │ {                                │
│                   │     ""connection_buffers"": 67226, │
│                   │     ""last_queries"": 0,           │
│                   │     ""session_history"": 774,      │
│                   │     ""total"": 68000,              │
│                   │     ""variables"": 0               │
│                   │ }                                │
└───────────────────┴──────────────────────────────────┘
{code}
This does not represent the total amount of memory use by a particular session, but provides figures that can be used for detecting anomalous behavior. The {{total}} field is currently simply the sum of all fields, and it is shown by {{maxctrl list sessions}}:
{code}
┌────┬───────┬───────────┬──────────────────────────┬──────┬─────────┬────────┐
│ Id │ User  │ Host      │ Connected                │ Idle │ Service │ Memory │
├────┼───────┼───────────┼──────────────────────────┼──────┼─────────┼────────┤
│ 1  │ johan │ 127.0.0.1 │ Thu Jun 30 15:44:29 2022 │ 305  │ RWS     │ 68000  │
└────┴───────┴───────────┴──────────────────────────┴──────┴─────────┴────────┘
{code}
",2,"In 22.08 there will be slightly more information available regarding the memory usage. {{maxctrl show session}} will return the following:
{code}
├───────────────────┼──────────────────────────────────┤
│ Memory            │ {                                │
│                   │     ""connection_buffers"": 67226, │
│                   │     ""last_queries"": 0,           │
│                   │     ""session_history"": 774,      │
│                   │     ""total"": 68000,              │
│                   │     ""variables"": 0               │
│                   │ }                                │
└───────────────────┴──────────────────────────────────┘
{code}
This does not represent the total amount of memory use by a particular session, but provides figures that can be used for detecting anomalous behavior. The {{total}} field is currently simply the sum of all fields, and it is shown by {{maxctrl list sessions}}:
{code}
┌────┬───────┬───────────┬──────────────────────────┬──────┬─────────┬────────┐
│ Id │ User  │ Host      │ Connected                │ Idle │ Service │ Memory │
├────┼───────┼───────────┼──────────────────────────┼──────┼─────────┼────────┤
│ 1  │ johan │ 127.0.0.1 │ Thu Jun 30 15:44:29 2022 │ 305  │ RWS     │ 68000  │
└────┴───────┴───────────┴──────────────────────────┴──────┴─────────┴────────┘
{code}
"
3244,MXS-2724,MXS,Naresh Chandra,228359,2022-06-30 18:20:39,"Hi Johan, can we also show which session or query is consuming more memory?

Alert in the GUI with Critical/Warning/Notice in the GUI and through auto mail alert would be a good and it will be very useful feature for us.",3,"Hi Johan, can we also show which session or query is consuming more memory?

Alert in the GUI with Critical/Warning/Notice in the GUI and through auto mail alert would be a good and it will be very useful feature for us."
3245,MXS-2724,MXS,Johan Wikman,228492,2022-07-04 08:23:16,"[~naresh.chandra@copart.com] Please create a separate feature request for that.
",4,"[~naresh.chandra@copart.com] Please create a separate feature request for that.
"
3246,MXS-2724,MXS,Johan Wikman,228493,2022-07-04 08:26:53,"This is but the first step on this path. To be able to report the memory usage in greater detail, some architectural changes are needed and they are not possible to make in 22.08.
",5,"This is but the first step on this path. To be able to report the memory usage in greater detail, some architectural changes are needed and they are not possible to make in 22.08.
"
3247,MXS-2724,MXS,Naresh Chandra,228519,2022-07-04 13:31:48,"Sure Johan, thanks for the update.",6,"Sure Johan, thanks for the update."
3248,MXS-2748,MXS,markus makela,136607,2019-10-29 21:36:22,We can definitely add this into the `show service` output where it should provide the average and maximum values for session command history lengths.,1,We can definitely add this into the `show service` output where it should provide the average and maximum values for session command history lengths.
3249,MXS-2748,MXS,Wagner Bianchi,136610,2019-10-29 21:43:58,"Great! It can be a good source of information to tune MaxScale in that sense. Thanks, folks!",2,"Great! It can be a good source of information to tune MaxScale in that sense. Thanks, folks!"
3250,MXS-2749,MXS,Esa Korhonen,172131,2020-11-16 09:16:27,Only the general role tracking has been implemented. No filter yet reacts to role changes.,1,Only the general role tracking has been implemented. No filter yet reacts to role changes.
3251,MXS-2754,MXS,markus makela,192343,2021-06-21 06:42:23,The core code is implemented but the modules themselves do not implement it. In practice this prevents the feature from being used and it should only be closed once at least a minimal set of filters can be altered.,1,The core code is implemented but the modules themselves do not implement it. In practice this prevents the feature from being used and it should only be closed once at least a minimal set of filters can be altered.
3252,MXS-2754,MXS,markus makela,194476,2021-07-15 07:32:22,I'll close this  as fixed in 6.0 and mark MXS-3594 to be fixed in 6.2.,2,I'll close this  as fixed in 6.0 and mark MXS-3594 to be fixed in 6.2.
3253,MXS-2768,MXS,Timofey Turenko,140300,2019-12-16 11:02:59,"working config:

{quote}variable ""gce_ssh_pub_key_file"" {
  type    = string
  default = ""~/.ssh/id_rsa.pub""
}

variable ""account_file"" {
  type    = string
  default = ""account.json""
}

variable ""project"" {
  type    = string
  default = ""mariadb-maxscale-test""
}

provider ""google"" {
  credentials = file(var.account_file)
  project = var.project
  region  = ""us-central1""
  zone    = ""us-central1-a""
}

resource ""google_compute_instance"" ""vm_instance"" {
  name         = ""my-terraform-instance""
  machine_type = ""f1-micro""

  tags = [""iap""]

  boot_disk {
    initialize_params {
      image = ""centos-cloud/centos-7""
    }
  }

  network_interface {
    network =  ""default""
    access_config {
    }
  }

  metadata = {
    ssh-keys = ""timofey_turenko_mariadb_com:${file(var.gce_ssh_pub_key_file)}""
    enable-oslogin = ""FALSE""
  }
}


output ""ip"" {
  value = ""${google_compute_instance.vm_instance.network_interface.0.access_config.0.nat_ip}""
}
{quote}",1,"working config:

{quote}variable ""gce_ssh_pub_key_file"" {
  type    = string
  default = ""~/.ssh/id_rsa.pub""
}

variable ""account_file"" {
  type    = string
  default = ""account.json""
}

variable ""project"" {
  type    = string
  default = ""mariadb-maxscale-test""
}

provider ""google"" {
  credentials = file(var.account_file)
  project = var.project
  region  = ""us-central1""
  zone    = ""us-central1-a""
}

resource ""google_compute_instance"" ""vm_instance"" {
  name         = ""my-terraform-instance""
  machine_type = ""f1-micro""

  tags = [""iap""]

  boot_disk {
    initialize_params {
      image = ""centos-cloud/centos-7""
    }
  }

  network_interface {
    network =  ""default""
    access_config {
    }
  }

  metadata = {
    ssh-keys = ""timofey_turenko_mariadb_com:${file(var.gce_ssh_pub_key_file)}""
    enable-oslogin = ""FALSE""
  }
}


output ""ip"" {
  value = ""${google_compute_instance.vm_instance.network_interface.0.access_config.0.nat_ip}""
}
{quote}"
3254,MXS-2785,MXS,Todd Stoffel,139168,2019-12-03 18:17:11,"example: 
{code:java}
rewrite_db=s/db_innodb/db_columnstore/g
{code}
",1,"example: 
{code:java}
rewrite_db=s/db_innodb/db_columnstore/g
{code}
"
3255,MXS-2804,MXS,Timofey Turenko,142379,2020-01-20 11:26:00,"signing will be moved away from Maxscale builds itself to separate signing server, waiting for it",1,"signing will be moved away from Maxscale builds itself to separate signing server, waiting for it"
3256,MXS-2804,MXS,Timofey Turenko,163621,2020-08-20 07:56:09,"Maxscale repo creating refactoring is done, now we can sign it with different keys, but I need to put keys into mdbe-ci-repo.mariadb.net (repo server for Maxscale and other products CI)",2,"Maxscale repo creating refactoring is done, now we can sign it with different keys, but I need to put keys into mdbe-ci-repo.mariadb.net (repo server for Maxscale and other products CI)"
3257,MXS-2804,MXS,Timofey Turenko,163930,2020-08-24 09:40:57,"first repo signed by enterprise key https://mdbe-ci-repo.mariadb.net/Maxscale/maxscale-2.4.12-release/

now testing it",3,"first repo signed by enterprise key URL

now testing it"
3258,MXS-2806,MXS,markus makela,140888,2019-12-18 07:46:40,"The stopping of services already stops all listeners. Stopping a service causes all listeners to stop accepting new connections. Stopping individual listeners would be logical but it wouldn't achieve what you want. If I'm reading it correctly, you want a mechanism that forcibly kicks out all active sessions connected to a service after it has been reconfigured. A ""time-to-live"" mechanism that forces a time limit on a connection's lifetime after a configuration change would be a reasonable solution to this problem. This would allow all sessions to gracefully close but it would prevent old sessions from hanging around for too long.",1,"The stopping of services already stops all listeners. Stopping a service causes all listeners to stop accepting new connections. Stopping individual listeners would be logical but it wouldn't achieve what you want. If I'm reading it correctly, you want a mechanism that forcibly kicks out all active sessions connected to a service after it has been reconfigured. A ""time-to-live"" mechanism that forces a time limit on a connection's lifetime after a configuration change would be a reasonable solution to this problem. This would allow all sessions to gracefully close but it would prevent old sessions from hanging around for too long."
3259,MXS-2807,MXS,markus makela,164207,2020-08-26 05:40:52,Adding a {{--force}} flag to {{stop service}} would solve this.,1,Adding a {{--force}} flag to {{stop service}} would solve this.
3260,MXS-2808,MXS,markus makela,140889,2019-12-18 07:55:02,"{{maxctrl}} already provides an implementation of this in the form of the {{cluster diff}} and {{cluster sync}} commands. Since a REST API in essence is the state of a Maxscale, we can calculate a diff between the two and apply it to the one we want. This will result in the two MaxScales ending up in the same state regardless of the original order of operations that were performed.",1,"{{maxctrl}} already provides an implementation of this in the form of the {{cluster diff}} and {{cluster sync}} commands. Since a REST API in essence is the state of a Maxscale, we can calculate a diff between the two and apply it to the one we want. This will result in the two MaxScales ending up in the same state regardless of the original order of operations that were performed."
3261,MXS-2808,MXS,markus makela,215346,2022-02-23 20:35:02,I'm not sure what you mean by manual. Right now you have to [turn on the config sync|https://mariadb.com/kb/en/mariadb-maxscale-6-mariadb-maxscale-configuration-guide/#configuration-synchronization] by pointing it at a monitor and that's about it.,2,I'm not sure what you mean by manual. Right now you have to [turn on the config sync|URL by pointing it at a monitor and that's about it.
3262,MXS-2808,MXS,Todd Stoffel,215362,2022-02-24 01:40:00,[~markus makela] Are there any limitations on this? Does that feature represent 100% config sync between servers?,3,[~markus makela] Are there any limitations on this? Does that feature represent 100% config sync between servers?
3263,MXS-2808,MXS,markus makela,215374,2022-02-24 04:58:16,"The only limitation is MXS-3619. The synchronization only affects the MaxScale configuration and all external files (e.g. cache filter rules) are not synchronized. In addition, the state of the servers set with {{set server maintenance}} is not synced.",4,"The only limitation is MXS-3619. The synchronization only affects the MaxScale configuration and all external files (e.g. cache filter rules) are not synchronized. In addition, the state of the servers set with {{set server maintenance}} is not synced."
3264,MXS-2838,MXS,Rich Theobald,178225,2021-01-25 15:51:53,Would be great to get this in 2.6.  It's been blocking me since I reported it a year ago.,1,Would be great to get this in 2.6.  It's been blocking me since I reported it a year ago.
3265,MXS-2842,MXS,Johan Wikman,163688,2020-08-20 13:17:59,"https://jira.mariadb.org/browse/MXS-3029
",1,"URL
"
3266,MXS-287,MXS,markus makela,74312,2015-08-06 09:35:30,"The MySQL command line escapes all underscore characters because if not escaped, they are interpreted as a single character wildcard. MaxScale version 1.2 does not support single character wildcards so if the database name contains an underscore character, the service in question requires the 'strip_db_esc=true' parameter.",1,"The MySQL command line escapes all underscore characters because if not escaped, they are interpreted as a single character wildcard. MaxScale version 1.2 does not support single character wildcards so if the database name contains an underscore character, the service in question requires the 'strip_db_esc=true' parameter."
3267,MXS-287,MXS,markus makela,75065,2015-08-27 13:07:32,"[~simon.hanmer] One thing that should be noted when giving grants is that both the host where the client is connecting and MaxScale's host need the grants.

Could confirm that both hosts have grants for them?",2,"[~simon.hanmer] One thing that should be noted when giving grants is that both the host where the client is connecting and MaxScale's host need the grants.

Could confirm that both hosts have grants for them?"
3268,MXS-287,MXS,markus makela,75513,2015-09-07 13:35:20,Without further information about user grants this bug cannot be resolved. More information is needed to verify that this is a bug with MaxScale and not a configuration error.,3,Without further information about user grants this bug cannot be resolved. More information is needed to verify that this is a bug with MaxScale and not a configuration error.
3269,MXS-287,MXS,Simon Hanmer,75607,2015-09-09 13:26:23,@markus makela - it's going to be a few weeks until I can look at this. Not sure if you want to close this and I'll reopen once I can get more information?,4,@markus makela - it's going to be a few weeks until I can look at this. Not sure if you want to close this and I'll reopen once I can get more information?
3270,MXS-287,MXS,markus makela,75608,2015-09-09 13:28:46,"Sure, you can reopen this once you get more information. We'll close this for now.",5,"Sure, you can reopen this once you get more information. We'll close this for now."
3271,MXS-287,MXS,Simon Hanmer,76366,2015-09-30 14:20:40,"@markus makela , I've add time for further investigation - I'll try and list the info below, but please let me know if you need anything more.

We've create a user called hydra on the databases with the following statement:

{quote}grant all on `_hydra\_%`.* to hydra@'%' identified by 'somepassword';{quote}

I can see this message in the error1.log file:

{quote}2015-09-30 12:13:25   Warning: Failed to add user hydra@% for service [Splitter Service]. This user will be unavailable via MaxScale.{quote}

and in the trace log, I can see:
{quote}2015-09-30 12:13:25   Splitter Service: Converted '_hydra\_%' to 0 individual database grants.
2015-09-30 12:13:25   Warning: Failed to add user hydra@% for service [Splitter Service]. This user will be unavailable via MaxScale.{quote}

My maxscale.cnf has the following section:

{quote}[Splitter Service]
type=service
router=readwritesplit
servers=host1,host2,host3
user=****
passwd=****
max_slave_connections=100%
router_options=slave_selection_criteria=LEAST_CURRENT_OPERATIONS
strip_db_esc=true
optimize_wildcard=true{quote}



",6,"@markus makela , I've add time for further investigation - I'll try and list the info below, but please let me know if you need anything more.

We've create a user called hydra on the databases with the following statement:

{quote}grant all on `_hydra\_%`.* to hydra@'%' identified by 'somepassword';{quote}

I can see this message in the error1.log file:

{quote}2015-09-30 12:13:25   Warning: Failed to add user hydra@% for service [Splitter Service]. This user will be unavailable via MaxScale.{quote}

and in the trace log, I can see:
{quote}2015-09-30 12:13:25   Splitter Service: Converted '_hydra\_%' to 0 individual database grants.
2015-09-30 12:13:25   Warning: Failed to add user hydra@% for service [Splitter Service]. This user will be unavailable via MaxScale.{quote}

My maxscale.cnf has the following section:

{quote}[Splitter Service]
type=service
router=readwritesplit
servers=host1,host2,host3
user=****
passwd=****
max_slave_connections=100%
router_options=slave_selection_criteria=LEAST_CURRENT_OPERATIONS
strip_db_esc=true
optimize_wildcard=true{quote}



"
3272,MXS-287,MXS,markus makela,79729,2016-01-11 17:49:05,Can you test with the 1.3.0 release of MaxScale if the database wildcard grants work? You can find the binaries here: http://maxscale-jenkins.mariadb.com/ci-repository/1.3.0-beta-release/mariadb-maxscale/,7,Can you test with the 1.3.0 release of MaxScale if the database wildcard grants work? You can find the binaries here: URL
3273,MXS-287,MXS,Simon Hanmer,79763,2016-01-12 12:05:06,"@markus makela - it'll probably be the week of the 25th before I can test this, but I'll get you an update as soon as I can.",8,"@markus makela - it'll probably be the week of the 25th before I can test this, but I'll get you an update as soon as I can."
3274,MXS-287,MXS,Simon Hanmer,80920,2016-02-16 12:53:05,"@markus makela, just tested with the 1.3.0 rpm and  the following user setup (the user is the maxscale admin user):

{code:java}
 create user 'maxscale_dba'@`cisvirmdb-max0%.int.dur.ac.uk` identified by '5e&zf^4wlooigGrYQN%#';
{code}
 and I get the following message in the maxscale.log file:

{code:java}
2016-02-16 10:42:05   error  : Failed to obtain address for host cisvirmdb-max0%.int.dur.ac.uk, Name or service not known
2016-02-16 10:42:05   warning: Failed to add user maxscale_dba@cisvirmdb-max0%.int.dur.ac.uk for service [Splitter Service]. This user will be unavailable via MaxScale.
{code}



",9,"@markus makela, just tested with the 1.3.0 rpm and  the following user setup (the user is the maxscale admin user):

{code:java}
 create user 'maxscale_dba'@`cisvirmdb-max0%.int.dur.ac.uk` identified by '5e&zf^4wlooigGrYQN%#';
{code}
 and I get the following message in the maxscale.log file:

{code:java}
2016-02-16 10:42:05   error  : Failed to obtain address for host cisvirmdb-max0%.int.dur.ac.uk, Name or service not known
2016-02-16 10:42:05   warning: Failed to add user maxscale_dba@cisvirmdb-max0%.int.dur.ac.uk for service [Splitter Service]. This user will be unavailable via MaxScale.
{code}



"
3275,MXS-287,MXS,markus makela,80921,2016-02-16 12:59:02,"The current version of MaxScale does not yet support wildcards in hostnames: https://github.com/mariadb-corporation/MaxScale/blob/master/Documentation/Getting-Started/Configuration-Guide.md#limitations-1

{noformat}
Note that currently wildcards are only supported in conjunction with IP-addresses, not with domain names.
{noformat}

If possible, please try to convert the hostname to an IP address and see if that fixes the problem.",10,"The current version of MaxScale does not yet support wildcards in hostnames: URL

{noformat}
Note that currently wildcards are only supported in conjunction with IP-addresses, not with domain names.
{noformat}

If possible, please try to convert the hostname to an IP address and see if that fixes the problem."
3276,MXS-287,MXS,Johan Wikman,81735,2016-03-03 12:07:41,Removed _fix version_ as this was not fixed for 1.3.,11,Removed _fix version_ as this was not fixed for 1.3.
3277,MXS-287,MXS,markus makela,82516,2016-04-05 05:35:22,"[~simon.hanmer] Have you had a chance to test if IP addresses work properly?

I'll change this task to an improvement because based on the information currently available, this seems to be caused by wildcards in hostnames.",12,"[~simon.hanmer] Have you had a chance to test if IP addresses work properly?

I'll change this task to an improvement because based on the information currently available, this seems to be caused by wildcards in hostnames."
3278,MXS-2875,MXS,Timofey Turenko,160894,2020-07-23 04:56:32,"First version implemented, in the testing https://maxscale-buildbot.mariadb.net/#/builders/43/builds/793",1,"First version implemented, in the testing URL"
3279,MXS-2882,MXS,markus makela,214703,2022-02-18 12:07:49,Added {{log_data=server}} that logs the server where the result came from.,1,Added {{log_data=server}} that logs the server where the result came from.
3280,MXS-2897,MXS,markus makela,145811,2020-03-05 08:23:48,REST API now supports authentication via JSON web tokens.,1,REST API now supports authentication via JSON web tokens.
3281,MXS-2904,MXS,markus makela,223287,2022-05-10 07:27:04,Documented currently known best practices for optimizing MaxScale performance. These can be found in [the main Configuration Guide|https://github.com/mariadb-corporation/MaxScale/blob/2.5/Documentation/Getting-Started/Configuration-Guide.md#performance-optimization] document.,1,Documented currently known best practices for optimizing MaxScale performance. These can be found in [the main Configuration Guide|URL document.
3282,MXS-2912,MXS,Julien Fritsch,149610,2020-04-09 10:23:25,"Just an idea, but since [~abychko] will be doing for both the Community Server and Enterprise Docker images, maybe we can go and automate/do it for MaxScale too?

",1,"Just an idea, but since [~abychko] will be doing for both the Community Server and Enterprise Docker images, maybe we can go and automate/do it for MaxScale too?

"
3283,MXS-2912,MXS,Timofey Turenko,160890,2020-07-23 04:51:16,"now the task ""Create Docker Maxscale image"" is implemented in the Maxscale BuildBot. Image goes to the own Docker registry",2,"now the task ""Create Docker Maxscale image"" is implemented in the Maxscale BuildBot. Image goes to the own Docker registry"
3284,MXS-2912,MXS,Timofey Turenko,171033,2020-11-04 12:02:13,"a new implementation of Docker file https://github.com/mariadb-corporation/maxscale-docker/blob/centos8tools/maxscale/Dockerfile - based on CentOS8 and the possibility to create CI version of the images.

next task: add BB task.",3,"a new implementation of Docker file URL - based on CentOS8 and the possibility to create CI version of the images.

next task: add BB task."
3285,MXS-2912,MXS,Timofey Turenko,173011,2020-11-24 11:55:20,"BB generates CI Docker images for all calls of ""build_maxscale_all"" task and puts them into our own Docker registry",4,"BB generates CI Docker images for all calls of ""build_maxscale_all"" task and puts them into our own Docker registry"
3286,MXS-2913,MXS,Johan Wikman,156664,2020-06-15 09:03:35,Not all modules have been converted. There are still a few lef,1,Not all modules have been converted. There are still a few lef
3287,MXS-2949,MXS,Esa Korhonen,149378,2020-04-08 08:27:35,Tested to work with real hardware.,1,Tested to work with real hardware.
3288,MXS-295,MXS,Timofey Turenko,77621,2015-11-02 13:08:29,"both le and be are supported now http://jenkins.engskysql.com/ci-repository/release-1.2.1-release/mariadb-maxscale/rhel/7/

",1,"both le and be are supported now URL

"
3289,MXS-2976,MXS,markus makela,156633,2020-06-15 07:10:10,{{develop}} tests pass locally.,1,{{develop}} tests pass locally.
3290,MXS-300,MXS,Dipti Joshi,74234,2015-08-04 07:17:17,[~markus makela] Can you comment on this one please ?,1,[~markus makela] Can you comment on this one please ?
3291,MXS-300,MXS,cai sunny,74368,2015-08-07 02:05:28,Will this problem be fixed on version 1.3?,2,Will this problem be fixed on version 1.3?
3292,MXS-300,MXS,Timofey Turenko,79462,2016-01-01 11:22:12,"tested with 1.3.0 beta: got stuck!
Direct connection to Master works, but hangs with RWSplit:

0.019585: Trying send data directly to Master
0.019601: Creating table with LONGBLOB
0.162464: Preparintg INSERT stmt
0.163307: Binding parameter
0.163321: Filling buffer
0.178745: Sending data in 8000000 bytes chunks
0.178758: Chunk #0
0.195851: Chunk #1
0.213474: Chunk #2
0.238402: Chunk #3
0.264496: Chunk #4
0.287396: Chunk #5
0.314361: Chunk #6
0.348769: Chunk #7
0.388528: Chunk #8
0.438868: Chunk #9
0.493474: Chunk #10
0.550534: Chunk #11
0.615026: Chunk #12
0.675197: Chunk #13
0.741173: Chunk #14
0.808430: Chunk #15
0.879497: Chunk #16
0.959449: Chunk #17
1.042638: Chunk #18
1.188525: Chunk #19
1.293153: Executing stetement
36.112911: Trying send data via RWSplit
36.112921: Creating table with LONGBLOB
61.006275: 
 **** Timeout! *** 


code which got stuck:

Test->tprintf(""Creating table with LONGBLOB\n"");
    Test->try_query(conn, (char *) ""DROP TABLE IF EXISTS long_blob_table"");
    Test->try_query(conn, (char *) ""CREATE TABLE long_blob_table(x INT, b LONGBLOB)"");


test case: https://github.com/mariadb-corporation/maxscale-system-test/blob/master/longblob.cpp
",3,"tested with 1.3.0 beta: got stuck!
Direct connection to Master works, but hangs with RWSplit:

0.019585: Trying send data directly to Master
0.019601: Creating table with LONGBLOB
0.162464: Preparintg INSERT stmt
0.163307: Binding parameter
0.163321: Filling buffer
0.178745: Sending data in 8000000 bytes chunks
0.178758: Chunk #0
0.195851: Chunk #1
0.213474: Chunk #2
0.238402: Chunk #3
0.264496: Chunk #4
0.287396: Chunk #5
0.314361: Chunk #6
0.348769: Chunk #7
0.388528: Chunk #8
0.438868: Chunk #9
0.493474: Chunk #10
0.550534: Chunk #11
0.615026: Chunk #12
0.675197: Chunk #13
0.741173: Chunk #14
0.808430: Chunk #15
0.879497: Chunk #16
0.959449: Chunk #17
1.042638: Chunk #18
1.188525: Chunk #19
1.293153: Executing stetement
36.112911: Trying send data via RWSplit
36.112921: Creating table with LONGBLOB
61.006275: 
 **** Timeout! *** 


code which got stuck:

Test->tprintf(""Creating table with LONGBLOB\n"");
    Test->try_query(conn, (char *) ""DROP TABLE IF EXISTS long_blob_table"");
    Test->try_query(conn, (char *) ""CREATE TABLE long_blob_table(x INT, b LONGBLOB)"");


test case: URL
"
3293,MXS-300,MXS,Timofey Turenko,80947,2016-02-16 23:21:37,"RWSplit seems to work ok with all (BLOB, MEDIUMBLOB, LONGBLOB)

ReadConn:

at some chunk:

156.346929: Chunk #5
156.934879: TEST_FAILED! Error inserting data, iteration 5, error Lost connection to MySQL server during query

next call of  mysql_stmt_send_long_data() gives segfault

Immediate executing the same tests after such failure gives failure for everything (including direct sending to Master). If after chunk sending failure mysql_stmt_send_long_data() is not executed - all other tests are ok.

MEDIUMBLOB fails in the same way:

215.766235: Creating table with MEDIUMBLOB
215.873048: Preparintg INSERT stmt
215.873694: Binding parameter
215.873711: Filling buffer
215.906228: Sending data in 8000000 bytes chunks, total size is 16000000
215.906257: Chunk #0
216.540199: TEST_FAILED! Error inserting data, iteration 0, error Lost connection to MySQL server during query
216.540330: Executing /home/vagrant/.jenkins/jobs/run_test/workspace/copy_logs.sh longblob

",4,"RWSplit seems to work ok with all (BLOB, MEDIUMBLOB, LONGBLOB)

ReadConn:

at some chunk:

156.346929: Chunk #5
156.934879: TEST_FAILED! Error inserting data, iteration 5, error Lost connection to MySQL server during query

next call of  mysql_stmt_send_long_data() gives segfault

Immediate executing the same tests after such failure gives failure for everything (including direct sending to Master). If after chunk sending failure mysql_stmt_send_long_data() is not executed - all other tests are ok.

MEDIUMBLOB fails in the same way:

215.766235: Creating table with MEDIUMBLOB
215.873048: Preparintg INSERT stmt
215.873694: Binding parameter
215.873711: Filling buffer
215.906228: Sending data in 8000000 bytes chunks, total size is 16000000
215.906257: Chunk #0
216.540199: TEST_FAILED! Error inserting data, iteration 0, error Lost connection to MySQL server during query
216.540330: Executing /home/vagrant/.jenkins/jobs/run_test/workspace/copy_logs.sh longblob

"
3294,MXS-300,MXS,Johan Wikman,81665,2016-03-01 13:24:56,"Documented limitation, so it is not a bug.

Changing to improvement.",5,"Documented limitation, so it is not a bug.

Changing to improvement."
3295,MXS-300,MXS,markus makela,83814,2016-05-31 07:15:44,LONGBLOB values are now supported for all statements except LOAD DATA LOCAL INFILE with binary data.,6,LONGBLOB values are now supported for all statements except LOAD DATA LOCAL INFILE with binary data.
3296,MXS-3003,MXS,markus makela,153995,2020-05-22 07:55:38,This would also fix the case where one MaxScale points to another MaxScale and both use proxy_protocol.,1,This would also fix the case where one MaxScale points to another MaxScale and both use proxy_protocol.
3297,MXS-3007,MXS,markus makela,154332,2020-05-27 06:16:35,Downloading a known Boost version solved the problem.,1,Downloading a known Boost version solved the problem.
3298,MXS-3037,MXS,markus makela,156444,2020-06-12 11:36:42,You can use {{maxctrl show sessions}}. We don't have SQL commands like {{SHOW PROCESSLIST}}.,1,You can use {{maxctrl show sessions}}. We don't have SQL commands like {{SHOW PROCESSLIST}}.
3299,MXS-3037,MXS,febriyant,156571,2020-06-13 12:33:14,"but part of queries is empty. how i see like on show processlist on mariadb ?
 !Annotation 2020-06-13 193241.jpg|thumbnail! ",2,"but part of queries is empty. how i see like on show processlist on mariadb ?
 !Annotation 2020-06-13 193241.jpg|thumbnail! "
3300,MXS-3037,MXS,markus makela,156580,2020-06-13 18:36:59,"I think if you add {{retain_last_statements=20}} and {{dump_last_statements=on_error}} under the {{\[maxsacle\]}} section, it should show up. By default most of the time MaxScale doesn't keep the queries in memory while they're executed.

If possible, can you add the configuration you used to test this?",3,"I think if you add {{retain_last_statements=20}} and {{dump_last_statements=on_error}} under the {{\[maxsacle\]}} section, it should show up. By default most of the time MaxScale doesn't keep the queries in memory while they're executed.

If possible, can you add the configuration you used to test this?"
3301,MXS-3037,MXS,febriyant,156628,2020-06-15 02:25:08,"I already set that parameter.
and queries is already show. 

so there is queries is still running or sleep ??",4,"I already set that parameter.
and queries is already show. 

so there is queries is still running or sleep ??"
3302,MXS-3037,MXS,markus makela,163613,2020-08-20 07:18:30,"That is simply the list of queries that was executed.

I'll convert this into a feature request as there is no SHOW PROCESSLIST in MaxScale.",5,"That is simply the list of queries that was executed.

I'll convert this into a feature request as there is no SHOW PROCESSLIST in MaxScale."
3303,MXS-3037,MXS,markus makela,196144,2021-08-06 07:55:27,"This should be possible to implement purely in MaxCtrl as the query tracking is already implemented in MaxScale as a part of {{retain_last_statements}}.

I created an initial patch on the [MXS-3037 branch|https://github.com/mariadb-corporation/MaxScale/tree/MXS-3037] that adds the {{list queries}} command. The output of the command is as follows:
{code}
┌────┬─────────┬──────────────────┬──────────┬────────────────────────────────┐
│ Id │ User    │ Host             │ Duration │ Query                          │
├────┼─────────┼──────────────────┼──────────┼────────────────────────────────┤
│ 2  │ maxuser │ ::ffff:127.0.0.1 │ 1s       │ select sleep(10)               │
├────┼─────────┼──────────────────┼──────────┼────────────────────────────────┤
│ 1  │ maxuser │ ::ffff:127.0.0.1 │ 1s       │ select sleep(5), ""hello world"" │
└────┴─────────┴──────────────────┴──────────┴────────────────────────────────┘
{code}",6,"This should be possible to implement purely in MaxCtrl as the query tracking is already implemented in MaxScale as a part of {{retain_last_statements}}.

I created an initial patch on the [MXS-3037 branch|URL that adds the {{list queries}} command. The output of the command is as follows:
{code}
┌────┬─────────┬──────────────────┬──────────┬────────────────────────────────┐
│ Id │ User    │ Host             │ Duration │ Query                          │
├────┼─────────┼──────────────────┼──────────┼────────────────────────────────┤
│ 2  │ maxuser │ ::ffff:127.0.0.1 │ 1s       │ select sleep(10)               │
├────┼─────────┼──────────────────┼──────────┼────────────────────────────────┤
│ 1  │ maxuser │ ::ffff:127.0.0.1 │ 1s       │ select sleep(5), ""hello world"" │
└────┴─────────┴──────────────────┴──────────┴────────────────────────────────┘
{code}"
3304,MXS-3037,MXS,markus makela,202105,2021-10-11 15:30:53,This should be a small enough of an addition that it can go into 6.2. This is only a client-side change and requires no changes in MaxScale itself.,7,This should be a small enough of an addition that it can go into 6.2. This is only a client-side change and requires no changes in MaxScale itself.
3305,MXS-307,MXS,Massimiliano Pinto,85361,2016-08-03 09:06:17,"[~dshjoshi]

Where will the events from from multiple backends be saved?

Binlog server design is to provide exact replica of the master binlog files.

Some questions:

1) There will be different binlog files, stored in different direcories, based on CHANGE MASTER 'master-name'
2) if not how the events would be stored, in one binlogfile only?
3) How the slave servers would replicate from binlog server?
4) Which binlog file will they ask if there are different ones?

Is this a proper use case for a replication proxy? Maybe not.


Right now it's possible to configure N binlog services, even without listeners and get separate binlog events from separate servers.

The mentioned ""re-write the schema name"" applies to SQL thread in the salve which cannot apply in binlog server where there is no SQL handling


A detailed document with proper uses cases is needed before any estimate.


This blog post is also important:

http://blog.booking.com/better_parallel_replication_for_mysql.html

It describes parallel replication and binlog servers
",1,"[~dshjoshi]

Where will the events from from multiple backends be saved?

Binlog server design is to provide exact replica of the master binlog files.

Some questions:

1) There will be different binlog files, stored in different direcories, based on CHANGE MASTER 'master-name'
2) if not how the events would be stored, in one binlogfile only?
3) How the slave servers would replicate from binlog server?
4) Which binlog file will they ask if there are different ones?

Is this a proper use case for a replication proxy? Maybe not.


Right now it's possible to configure N binlog services, even without listeners and get separate binlog events from separate servers.

The mentioned ""re-write the schema name"" applies to SQL thread in the salve which cannot apply in binlog server where there is no SQL handling


A detailed document with proper uses cases is needed before any estimate.


This blog post is also important:

URL

It describes parallel replication and binlog servers
"
3306,MXS-3071,MXS,Duong Thien Ly,162213,2020-08-07 08:38:21,"The latest commit on branch 2.5 contains this feature. 
To try it locally without building maxscale again:
* go to /maxgui/ dir  
* run 'npm install'
* run ""npm run serve""
MaxGUI will be available via this link: http://localhost:8000/ ",1,"The latest commit on branch 2.5 contains this feature. 
To try it locally without building maxscale again:
* go to /maxgui/ dir  
* run 'npm install'
* run ""npm run serve""
MaxGUI will be available via this link: URL "
3307,MXS-3095,MXS,markus makela,163233,2020-08-18 10:28:40,"I think we can just add the {{table_name}} and {{table_schema}} to the DML events, it shouldn't cause problems.",1,"I think we can just add the {{table_name}} and {{table_schema}} to the DML events, it shouldn't cause problems."
3308,MXS-3095,MXS,wilsonlau,163365,2020-08-19 02:08:39,"thanks, this feature is important for CDC, it is greate if come true next version",2,"thanks, this feature is important for CDC, it is greate if come true next version"
3309,MXS-3095,MXS,wilsonlau,164059,2020-08-25 09:05:56,"Hi  makela
github not found branch 2.5.3, i hope 2.5.3 release asap, tks",3,"Hi  makela
github not found branch 2.5.3, i hope 2.5.3 release asap, tks"
3310,MXS-3095,MXS,markus makela,164066,2020-08-25 09:25:29,[~wilsonlau78] you can find the current code on the {{2.5}} branch on GitHub.,4,[~wilsonlau78] you can find the current code on the {{2.5}} branch on GitHub.
3311,MXS-3095,MXS,wilsonlau,164095,2020-08-25 11:41:45,"build error:P

c++: error: unrecognized command line option ‘-std=c++14’
c++: error: unrecognized command line option ‘-std=c++14’
make[2]: *** [maxutils/maxbase/src/CMakeFiles/maxbase.dir/alloc.cc.o] Error 1
make[1]: *** [maxutils/maxbase/src/CMakeFiles/maxbase.dir/all] Error 2
make: *** [all] Error 2
",5,"build error:P

c++: error: unrecognized command line option ‘-std=c++14’
c++: error: unrecognized command line option ‘-std=c++14’
make[2]: *** [maxutils/maxbase/src/CMakeFiles/maxbase.dir/alloc.cc.o] Error 1
make[1]: *** [maxutils/maxbase/src/CMakeFiles/maxbase.dir/all] Error 2
make: *** [all] Error 2
"
3312,MXS-3095,MXS,markus makela,164104,2020-08-25 13:05:22,You need a newer compiler.,6,You need a newer compiler.
3313,MXS-3095,MXS,wilsonlau,164130,2020-08-25 16:01:57,"ok, thanks ",7,"ok, thanks "
3314,MXS-3108,MXS,markus makela,171932,2020-11-12 18:08:40,"Injecting filters into an active session is relatively straightforward now that the objects themselves can be interacted with.

For the {{ServiceEndpoint}} class this can be done by:
* Starting new sessions for the added filters.
* Closing sessions for any removed filters.
* Rebuilding the filter chain by calling {{setUpstream}} and {{setDownstream}} with the new chain.

With this, the only big problem is how the session interacts with the {{mxs::Endpoint}} objects downstream. Ever since 2.5 there can be multiple filter chains for a single session; one for each {{ServiceEndpoint}}. In addition, not all branches in the tree that is formed from the endpoints is connected which makes it more complex. The problem that remains is: how to identify which session and which node in the tree to inject the filter to.

One solution would be to only allow addition of filters to the {{MXS_SESSION}} object itself. Right now, it only calls the downstream component and it could have a routing chain similar to the {{ServiceEndpoint}}. This would remove the problem of identifying the node into which the filter is added.

Another aspect of this is that starting a new filter session can be done only at certain points. This depends on the filter but one obvious case is that a new session must not be started when a query has been routed but no request has been received. This in turn implies tracking of queries and their responses.

As for the benefits of debuggability; *in practice* the ability to turn on {{log_info}} for a particular session would provide more actionable output than a new filter that logs all the information. The downside of filters is that the filter itself wouldn't know about the routing decisions of the router or what the MaxScale core does. However, it would be extremely valuable to be able to enable filters when some condition is met. This would allow automated logging of problematic sessions which in turn would make it easy to solve problems (assuming the logging is useful).",1,"Injecting filters into an active session is relatively straightforward now that the objects themselves can be interacted with.

For the {{ServiceEndpoint}} class this can be done by:
* Starting new sessions for the added filters.
* Closing sessions for any removed filters.
* Rebuilding the filter chain by calling {{setUpstream}} and {{setDownstream}} with the new chain.

With this, the only big problem is how the session interacts with the {{mxs::Endpoint}} objects downstream. Ever since 2.5 there can be multiple filter chains for a single session; one for each {{ServiceEndpoint}}. In addition, not all branches in the tree that is formed from the endpoints is connected which makes it more complex. The problem that remains is: how to identify which session and which node in the tree to inject the filter to.

One solution would be to only allow addition of filters to the {{MXS_SESSION}} object itself. Right now, it only calls the downstream component and it could have a routing chain similar to the {{ServiceEndpoint}}. This would remove the problem of identifying the node into which the filter is added.

Another aspect of this is that starting a new filter session can be done only at certain points. This depends on the filter but one obvious case is that a new session must not be started when a query has been routed but no request has been received. This in turn implies tracking of queries and their responses.

As for the benefits of debuggability; *in practice* the ability to turn on {{log_info}} for a particular session would provide more actionable output than a new filter that logs all the information. The downside of filters is that the filter itself wouldn't know about the routing decisions of the router or what the MaxScale core does. However, it would be extremely valuable to be able to enable filters when some condition is met. This would allow automated logging of problematic sessions which in turn would make it easy to solve problems (assuming the logging is useful)."
3315,MXS-3108,MXS,markus makela,173658,2020-12-01 10:10:55,The chosen solution was to only allow addition of filters to the session object. This means that the filters defined for services cannot be disabled at runtime for individual sessions. The filters are added before any existing filters and will be first link in the routing chain.,2,The chosen solution was to only allow addition of filters to the session object. This means that the filters defined for services cannot be disabled at runtime for individual sessions. The filters are added before any existing filters and will be first link in the routing chain.
3316,MXS-3117,MXS,Timofey Turenko,163620,2020-08-20 07:51:44,now repos are generated by https://github.com/mariadb-corporation/MariaDBE-CI/blob/ci-scripts-maxscale/create_repos_local script on the http://mdbe-ci-repo.mariadb.net/ server,1,now repos are generated by URL script on the URL server
3317,MXS-3151,MXS,Hartmut Holzgraefe,165003,2020-09-02 13:46:29,"See also slack discussion starting at: https://mariadb.slack.com/archives/C03NQJPUL/p1599050217044800
",1,"See also slack discussion starting at: URL
"
3318,MXS-3151,MXS,Johan Wikman,167061,2020-09-28 12:52:18,"The sentiment is that it simply should be so that if an {{extra_port}} has been defined, then it simply should always be used by the monitor, and not only when it is not possible to connect using the normal port.
",2,"The sentiment is that it simply should be so that if an {{extra_port}} has been defined, then it simply should always be used by the monitor, and not only when it is not possible to connect using the normal port.
"
3319,MXS-3151,MXS,Esa Korhonen,168427,2020-10-09 13:26:19,Defaults to using extra-port if defined.,3,Defaults to using extra-port if defined.
3320,MXS-3154,MXS,markus makela,176292,2021-01-04 06:04:15,I think this is already partially done in 2.6.,1,I think this is already partially done in 2.6.
3321,MXS-3154,MXS,Duong Thien Ly,176400,2021-01-04 15:15:08,"Yes, it is. But we haven't have the design for this feature.",2,"Yes, it is. But we haven't have the design for this feature."
3322,MXS-3154,MXS,Duong Thien Ly,192118,2021-06-17 07:24:16,"The basic implementation is done.
https://docs.google.com/document/d/1YREjz4PcN-JC18QyVqpv2DbDqJarXMiS3k3m5TizM5Q/edit#heading=h.lt9wakmmhyge


",3,"The basic implementation is done.
URL


"
3323,MXS-3159,MXS,markus makela,216379,2022-03-08 09:04:55,"Changing to a new feature as it's an actual new thing, not just a task to do.",1,"Changing to a new feature as it's an actual new thing, not just a task to do."
3324,MXS-319,MXS,Dipti Joshi,80361,2016-01-26 15:57:53,"[~johan.wikman] Instead of reusing 1.3 testing epic for 1.4 release, new testing epic should be created for 1.4

",1,"[~johan.wikman] Instead of reusing 1.3 testing epic for 1.4 release, new testing epic should be created for 1.4

"
3325,MXS-319,MXS,Johan Wikman,80363,2016-01-26 16:01:21,"This concerns 1.3 testing.
",2,"This concerns 1.3 testing.
"
3326,MXS-319,MXS,Timofey Turenko,80710,2016-02-08 22:52:08,all sub-task are closed,3,all sub-task are closed
3327,MXS-3217,MXS,markus makela,196133,2021-08-06 05:42:47,I changed this to a New Feature as it extends current capabilities and blocks other New Feature items.,1,I changed this to a New Feature as it extends current capabilities and blocks other New Feature items.
3328,MXS-3228,MXS,Duong Thien Ly,192113,2021-06-17 07:10:33,Basic tests are done,1,Basic tests are done
3329,MXS-3265,MXS,Johan Wikman,170389,2020-10-28 16:58:13,"When RWS is used MaxScale does not just forward response packets as they come, but is fully aware of response and transaction boundaries.
",1,"When RWS is used MaxScale does not just forward response packets as they come, but is fully aware of response and transaction boundaries.
"
3330,MXS-3265,MXS,Manjot Singh,170391,2020-10-28 17:04:50,I copied the text from fixes based on MaxScale 1.3. Apologies if out of date.,2,I copied the text from fixes based on MaxScale 1.3. Apologies if out of date.
3331,MXS-3265,MXS,Johan Wikman,173952,2020-12-03 07:20:52,"What kind of limitations would be reasonable?

For instance, when taking a connection out from the pool, MaxScale would have to send a {{COM_CHANGE_USER}} to reset the connection for the current user. Unless we explicitly record and each time set the session state (user variables etc.), every transaction starts from a blank slate.  Doing that adds overhead but not doing that means that applications need to be aware of this pooling.",3,"What kind of limitations would be reasonable?

For instance, when taking a connection out from the pool, MaxScale would have to send a {{COM_CHANGE_USER}} to reset the connection for the current user. Unless we explicitly record and each time set the session state (user variables etc.), every transaction starts from a blank slate.  Doing that adds overhead but not doing that means that applications need to be aware of this pooling."
3332,MXS-3265,MXS,markus makela,173996,2020-12-03 15:21:51,"One benefit of connection sharing is the ability to share prepared statements. Currently, each connection will have their own set of compiled prepared statements in the server. By sharing the connection and the same prepared statement ID, we reduce the overall overhead of prepared statements compared to normal text protocol queries.",4,"One benefit of connection sharing is the ability to share prepared statements. Currently, each connection will have their own set of compiled prepared statements in the server. By sharing the connection and the same prepared statement ID, we reduce the overall overhead of prepared statements compared to normal text protocol queries."
3333,MXS-3268,MXS,Johan Wikman,172397,2020-11-18 06:17:13,Is it known why node2 came back up in _readonly_ mode? Is it made so in the server config file?,1,Is it known why node2 came back up in _readonly_ mode? Is it made so in the server config file?
3334,MXS-3268,MXS,Johan Wikman,175494,2020-12-17 06:49:56,[~nicklamb] As this is a _New Feature_ and in the _Icebox_ you need to agree with [~toddstoffel] what to do about it. ,2,[~nicklamb] As this is a _New Feature_ and in the _Icebox_ you need to agree with [~toddstoffel] what to do about it. 
3335,MXS-3268,MXS,Johan Wikman,181748,2021-03-08 08:32:04,[~ccalender] Could you clarify what _master node_  in point 2 refers to? To one of the Galera nodes in point 1?,3,[~ccalender] Could you clarify what _master node_  in point 2 refers to? To one of the Galera nodes in point 1?
3336,MXS-3277,MXS,Timofey Turenko,172511,2020-11-18 17:18:30,"now MDBCI is responsible for Kerberos, rng-tools and gssapi plugins installation ",1,"now MDBCI is responsible for Kerberos, rng-tools and gssapi plugins installation "
3337,MXS-3290,MXS,Johan Wikman,177384,2021-01-15 10:31:59,This requires re-visiting as the approach has changed somewhat.,1,This requires re-visiting as the approach has changed somewhat.
3338,MXS-334,MXS,John W Smith,85688,2016-08-19 13:45:19,"While lack of this feature isn't critical for deploying in our environment, it makes management much more difficult.  Currently all of our users are auth'd via pam+ldap to our AD domain.  We only have a couple of DB admins, so no big deal in production, but on the development side, we have many developers that need access to the development DB's to allow query analisys, index creation, etc. Since we adhere to most banking regulations regarding security of DBs, files, etc, we require frequent password changes. Since we currently have no method of syncing a password change to the DB users, we have to depend on the users to change the passwords on the DB servers themselves, not an ideal situation to say the least.

",1,"While lack of this feature isn't critical for deploying in our environment, it makes management much more difficult.  Currently all of our users are auth'd via pam+ldap to our AD domain.  We only have a couple of DB admins, so no big deal in production, but on the development side, we have many developers that need access to the development DB's to allow query analisys, index creation, etc. Since we adhere to most banking regulations regarding security of DBs, files, etc, we require frequent password changes. Since we currently have no method of syncing a password change to the DB users, we have to depend on the users to change the passwords on the DB servers themselves, not an ideal situation to say the least.

"
3339,MXS-334,MXS,Esa Korhonen,96806,2017-06-22 14:37:59,"Adding a PAM authentication plugin similar to the one on the server to MaxScale should be possible. This would allow the client (assuming the client UI/command line supports PAM communication) to login to MaxScale. However, after this MaxScale needs to login to the backend servers using the client's username while the host machine changes from the client machine to the MaxScale machine. This MaxScale-to-backend login must be doable autonomously. Also, contrary to a normal sql login, MaxScale would have no other information about the user other than the username.

Here are some possibilities for the backend login:
1) Each backend allows MaxScale to login without a password for select users. Unsecure.
2) We implement a new PAM plugin that would run on the backends. This plugin would contact the MaxScale machine and ask if the user is currently logged in and accept the login if that is the case. Would require using non-standard PAM-plugins on the server.
3) Could the MaxScale machine obtain a token for the user from the ldap server and use that to login? Or could the backends check from ldap that the user is logged in and accept his login from MaxScale-IP? My knowledge on ldap is superficial, so I don't know if these are possible.

In any case, more information on what would MaxScale actually need to do is required to implement PAM support. [~JWSmith]",2,"Adding a PAM authentication plugin similar to the one on the server to MaxScale should be possible. This would allow the client (assuming the client UI/command line supports PAM communication) to login to MaxScale. However, after this MaxScale needs to login to the backend servers using the client's username while the host machine changes from the client machine to the MaxScale machine. This MaxScale-to-backend login must be doable autonomously. Also, contrary to a normal sql login, MaxScale would have no other information about the user other than the username.

Here are some possibilities for the backend login:
1) Each backend allows MaxScale to login without a password for select users. Unsecure.
2) We implement a new PAM plugin that would run on the backends. This plugin would contact the MaxScale machine and ask if the user is currently logged in and accept the login if that is the case. Would require using non-standard PAM-plugins on the server.
3) Could the MaxScale machine obtain a token for the user from the ldap server and use that to login? Or could the backends check from ldap that the user is logged in and accept his login from MaxScale-IP? My knowledge on ldap is superficial, so I don't know if these are possible.

In any case, more information on what would MaxScale actually need to do is required to implement PAM support. [~JWSmith]"
3340,MXS-334,MXS,Esa Korhonen,98332,2017-08-07 09:27:27,A very limited implementation.,3,A very limited implementation.
3341,MXS-3357,MXS,Esa Korhonen,184431,2021-03-31 16:32:54,"When deciding on what server to promote, the monitor checks the servers in the order they are listed in the ""servers""-parameter. A later server has to be ""better"" (i.e has higher gtid) than the ones before it to be selected instead. So, simply list the preferred servers first.",1,"When deciding on what server to promote, the monitor checks the servers in the order they are listed in the ""servers""-parameter. A later server has to be ""better"" (i.e has higher gtid) than the ones before it to be selected instead. So, simply list the preferred servers first."
3342,MXS-3357,MXS,Esa Korhonen,185072,2021-04-07 11:02:05,"Closing for now. If more information is required, please reopen issue.",2,"Closing for now. If more information is required, please reopen issue."
3343,MXS-3368,MXS,Timofey Turenko,180641,2021-02-23 12:57:38,"dashboard https://mdbe-buildbot.mariadb.net/#/build_and_test_maxscale_weekly_dashboard

fixed issued with wrong backend_box",1,"dashboard URL

fixed issued with wrong backend_box"
3344,MXS-3368,MXS,Timofey Turenko,183312,2021-03-23 11:53:24,"Maxscale system-test works with all ES CI versions (10.2 - 10.6). BuildBot is setup to run to every week. Test failures are present, but there is no ""broken test runs""",2,"Maxscale system-test works with all ES CI versions (10.2 - 10.6). BuildBot is setup to run to every week. Test failures are present, but there is no ""broken test runs"""
3345,MXS-3384,MXS,markus makela,197283,2021-08-22 08:27:56,"Should be easy to implement:
{code:diff}
diff --git a/maxctrl/lib/list.js b/maxctrl/lib/list.js
index 279276c5a..d55ae7976 100644
--- a/maxctrl/lib/list.js
+++ b/maxctrl/lib/list.js
@@ -44,6 +44,11 @@ const list_servers_fields = [
     path: ""attributes.gtid_current_pos"",
     description: ""Current value of @@gtid_current_pos"",
   },
+  {
+    name: ""Monitor"",
+    path: ""relationships.monitors.data[0].id"",
+    description: ""The monitor for this server"",
+  },
 ];
 
 const list_services_fields = [
{code}",1,"Should be easy to implement:
{code:diff}
diff --git a/maxctrl/lib/list.js b/maxctrl/lib/list.js
index 279276c5a..d55ae7976 100644
--- a/maxctrl/lib/list.js
+++ b/maxctrl/lib/list.js
@@ -44,6 +44,11 @@ const list_servers_fields = [
     path: ""attributes.gtid_current_pos"",
     description: ""Current value of @@gtid_current_pos"",
   },
+  {
+    name: ""Monitor"",
+    path: ""relationships.monitors.data[0].id"",
+    description: ""The monitor for this server"",
+  },
 ];
 
 const list_services_fields = [
{code}"
3346,MXS-3411,MXS,markus makela,182819,2021-03-18 09:42:25,[Implemented SSL and basic SASL authentication|https://github.com/mariadb-corporation/MaxScale/blob/92fae2246d167235844b08b8aebc552ba236730e/Documentation/Routers/KafkaCDC.md#kafka_ssl].,1,[Implemented SSL and basic SASL authentication|URL
3347,MXS-3413,MXS,Thomas J. Girsch,180472,2021-02-22 21:38:37,"Another database product we use (not a mysql variant) has a similar idea implemented by way of a required command line flag when making a dynamic change: -m for in-memory-only and -f for also updating the configuration file to reflect the change. A backup copy of the file is created, too, so no worry about overwriting.",1,"Another database product we use (not a mysql variant) has a similar idea implemented by way of a required command line flag when making a dynamic change: -m for in-memory-only and -f for also updating the configuration file to reflect the change. A backup copy of the file is created, too, so no worry about overwriting."
3348,MXS-3413,MXS,markus makela,180606,2021-02-23 06:29:25,"MaxScale already warns if runtime changes have been made to an object found in the static configuration file:
{code}
2021-02-23 08:28:19   warning: Found static and runtime configurations for [RW-Split-Router], ignoring static configuration. Move the runtime changes into the static configuration file and remove the generated file in '/home/markusjm/build-2.5/lib/maxscale/maxscale.cnf.d' to remove this warning.
{code}

Whether persisted changes are read is already configurable with the [load_persisted_configs|https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-configuration-guide/#load_persisted_configs] parameter.

As for updating the actual static configuration file, in most cases MaxScale itself doesn't have write access to the configuration file it is used with which means an alternative method to persist changes must still exist. ",2,"MaxScale already warns if runtime changes have been made to an object found in the static configuration file:
{code}
2021-02-23 08:28:19   warning: Found static and runtime configurations for [RW-Split-Router], ignoring static configuration. Move the runtime changes into the static configuration file and remove the generated file in '/home/markusjm/build-2.5/lib/maxscale/maxscale.cnf.d' to remove this warning.
{code}

Whether persisted changes are read is already configurable with the [load_persisted_configs|URL parameter.

As for updating the actual static configuration file, in most cases MaxScale itself doesn't have write access to the configuration file it is used with which means an alternative method to persist changes must still exist. "
3349,MXS-3413,MXS,Thomas J. Girsch,180639,2021-02-23 12:55:06,"I'm not sure a message buried in the log file is sufficient. By the way, in my version, at least (2.4.15), it shows up not as a warning but as a notice.

Is there a downside to requiring that the MaxScale config file be owned by the user that runs the ""maxscale"" process? The only issue I can see is that the config file can include passwords, but whatever process runs MaxScale *already* has read access to the file (or it wouldn't be able to start at all), so that security issue is moot.

On my system:

$ ls -la /etc/maxscale.cnf
-rw------- 1 maxscale maxscale 1129 Oct 29 15:54 /etc/maxscale.cnf

Could do a similar requirement for /etc/maxscale.cnf.d and its contents.

The load_persisted_configs parameter is good to know.

Perhaps a maxctrl command to generate a new configuration file (in a new location or name) based on currently-running MaxScale configuration? Then when, e.g., support asks for MaxScale configuration file, instead of asking for the file, they ask for that command to be run and its output. (Which is less unwieldy than a series of maxctrl show commands)",3,"I'm not sure a message buried in the log file is sufficient. By the way, in my version, at least (2.4.15), it shows up not as a warning but as a notice.

Is there a downside to requiring that the MaxScale config file be owned by the user that runs the ""maxscale"" process? The only issue I can see is that the config file can include passwords, but whatever process runs MaxScale *already* has read access to the file (or it wouldn't be able to start at all), so that security issue is moot.

On my system:

$ ls -la /etc/maxscale.cnf
-rw------- 1 maxscale maxscale 1129 Oct 29 15:54 /etc/maxscale.cnf

Could do a similar requirement for /etc/maxscale.cnf.d and its contents.

The load_persisted_configs parameter is good to know.

Perhaps a maxctrl command to generate a new configuration file (in a new location or name) based on currently-running MaxScale configuration? Then when, e.g., support asks for MaxScale configuration file, instead of asking for the file, they ask for that command to be run and its output. (Which is less unwieldy than a series of maxctrl show commands)"
3350,MXS-3413,MXS,Thomas J. Girsch,180643,2021-02-23 13:05:04,"I'd add that whether or not to persist a change may vary from one change to the next. So an all-or-nothing parameter, while welcome, may still be insufficient.",4,"I'd add that whether or not to persist a change may vary from one change to the next. So an all-or-nothing parameter, while welcome, may still be insufficient."
3351,MXS-3413,MXS,markus makela,180649,2021-02-23 13:31:29,"You can generate a configuration file from the current setup with {{maxscale --export-config=/path/to/new-config.cnf}}. You'll require read access to the files so you need to run it as {{sudo maxscale --user=maxscale --export-config=...}}.

I don't think the file has to be owned by MaxScale, it just needs read access to it. As for write access, I believe that it's just customary to not provide write access to the configuration for a program that reads it. There's no technical limitation that prevents MaxScale from modifying it. ",5,"You can generate a configuration file from the current setup with {{maxscale --export-config=/path/to/new-config.cnf}}. You'll require read access to the files so you need to run it as {{sudo maxscale --user=maxscale --export-config=...}}.

I don't think the file has to be owned by MaxScale, it just needs read access to it. As for write access, I believe that it's just customary to not provide write access to the configuration for a program that reads it. There's no technical limitation that prevents MaxScale from modifying it. "
3352,MXS-3413,MXS,Thomas J. Girsch,180676,2021-02-23 15:54:31,"There's a bit of a security issue with the maxscale --export-config command: It creates the file with 644 permissions, including the passwords.",6,"There's a bit of a security issue with the maxscale --export-config command: It creates the file with 644 permissions, including the passwords."
3353,MXS-3413,MXS,markus makela,180679,2021-02-23 16:11:05,"That's something that could be changed, read and write access for both user and group seems like the correct thing. I've opened MXS-3415 for it.",7,"That's something that could be changed, read and write access for both user and group seems like the correct thing. I've opened MXS-3415 for it."
3354,MXS-3413,MXS,Thomas J. Girsch,180831,2021-02-24 20:08:13,"Another thought occurs to me: How about also printing a warning when a dynamic change is applied? e.g.,

Warning: New setting [whatever] written to [file]; check setting of load_persisted_configs

Something like that.",8,"Another thought occurs to me: How about also printing a warning when a dynamic change is applied? e.g.,

Warning: New setting [whatever] written to [file]; check setting of load_persisted_configs

Something like that."
3355,MXS-3413,MXS,markus makela,203455,2021-10-25 09:03:54,"It seems most of what's been discussed in this issue already exists in one form or another:
* The whole persistent configuration mechanism can be disabled with [load_persisted_configs=false|https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-maxscale-configuration-guide/#load_persisted_configs]
* The complete configuration can be exported with [--export-config|https://mariadb.com/kb/en/mariadb-maxscale-24-mariadb-maxscale-configuration-guide/#backing-up-configuration-changes]
* MaxScale warns whenever it loads persisted configurations

The only thing that aren't there:
* A complete log of all modifications done to MaxScale (helps figure out who changed and what)
* Visual markers for knowing if the current runtime configuration is different from the one defined by the static configuration files
* A way to conditionally persist changes

A complete log of all REST API actions would be useful for both auditing purposes as well as figuring out what changed and when. I've opened MXS-3827 for this.

One could argue that the line between static and runtime configurations has blurred with the addition of the GUI which makes the whole configuration process dynamic. Showing whether a change has been made could still be valuable information for the cases where the static configuration defines the desired setup and runtime changes are done to react to changes in the environment. We could start off by adding a simple boolean flag to tell whether the object was modified either via the REST API or by being loaded from a dynamically generated file. A more advanced version would tell where each change originated from, either from a static config or via a dynamic change.

Conditionally persisting changes would be doable but it would either be technically complex or result in what I consider less than optimal behavior. As some parameters in MaxScale can depend on other parameters (e.g. {{ssl_key}} and {{ssl_cert}} depend on each other), conflicting configurations must not be generated. This is also the reason why a complete state snapshot is stored on disk: storing just the parameter alone allows the combination of runtime changes and static configurations to generate an invalid configuration.

Simply skipping the persist step would allow a parameter to be modified without it being persisted on disk but the next time a parameter would be persisted, even the changes that weren't persisted would get stored on disk. Since it's possible to opt-out of the whole persisted configuration mechanism while still allowing the modified configuration to be stored and optionally enabled later on, this method doesn't seem particularly useful to me. It might be of limited use if you would configure maxctrl to always skip the persist step unless manually instructed to do so (the addition of {{~/maxctrl.cnf}} makes this possible).

The second option is to keep two values for each configuration parameter and store only the latest persisted value for each. This would require adding additional logic into the configuration processing code that makes sure that both the runtime and the persisted configurations would result in a sane configuration. This would be a more complex thing to implement as the configuration system in MaxScale is quite modular and the ""subclasses"" of the configurations usually impose their own restrictions on what is valid and what isn't. This would also make the logic of the configuration persistence even more confusing as you'd essentially have two configurations. At this point I'd argue that it would be simpler to disable the loading of persisted configurations and to manually amend ""important"" changes to the static configuration once they have been done at runtime, similarly to how it's been done with MariaDB for ages.",9,"It seems most of what's been discussed in this issue already exists in one form or another:
* The whole persistent configuration mechanism can be disabled with [load_persisted_configs=false|URL
* The complete configuration can be exported with [--export-config|URL
* MaxScale warns whenever it loads persisted configurations

The only thing that aren't there:
* A complete log of all modifications done to MaxScale (helps figure out who changed and what)
* Visual markers for knowing if the current runtime configuration is different from the one defined by the static configuration files
* A way to conditionally persist changes

A complete log of all REST API actions would be useful for both auditing purposes as well as figuring out what changed and when. I've opened MXS-3827 for this.

One could argue that the line between static and runtime configurations has blurred with the addition of the GUI which makes the whole configuration process dynamic. Showing whether a change has been made could still be valuable information for the cases where the static configuration defines the desired setup and runtime changes are done to react to changes in the environment. We could start off by adding a simple boolean flag to tell whether the object was modified either via the REST API or by being loaded from a dynamically generated file. A more advanced version would tell where each change originated from, either from a static config or via a dynamic change.

Conditionally persisting changes would be doable but it would either be technically complex or result in what I consider less than optimal behavior. As some parameters in MaxScale can depend on other parameters (e.g. {{ssl_key}} and {{ssl_cert}} depend on each other), conflicting configurations must not be generated. This is also the reason why a complete state snapshot is stored on disk: storing just the parameter alone allows the combination of runtime changes and static configurations to generate an invalid configuration.

Simply skipping the persist step would allow a parameter to be modified without it being persisted on disk but the next time a parameter would be persisted, even the changes that weren't persisted would get stored on disk. Since it's possible to opt-out of the whole persisted configuration mechanism while still allowing the modified configuration to be stored and optionally enabled later on, this method doesn't seem particularly useful to me. It might be of limited use if you would configure maxctrl to always skip the persist step unless manually instructed to do so (the addition of {{~/maxctrl.cnf}} makes this possible).

The second option is to keep two values for each configuration parameter and store only the latest persisted value for each. This would require adding additional logic into the configuration processing code that makes sure that both the runtime and the persisted configurations would result in a sane configuration. This would be a more complex thing to implement as the configuration system in MaxScale is quite modular and the ""subclasses"" of the configurations usually impose their own restrictions on what is valid and what isn't. This would also make the logic of the configuration persistence even more confusing as you'd essentially have two configurations. At this point I'd argue that it would be simpler to disable the loading of persisted configurations and to manually amend ""important"" changes to the static configuration once they have been done at runtime, similarly to how it's been done with MariaDB for ages."
3356,MXS-3413,MXS,Thomas J. Girsch,203530,2021-10-25 14:55:34,"From the customer perspective, I would add that consistency in behavior between MariaDB and MaxScale as pertains to dynamically-made changes would be highly desirable. I'd also note that other products I use that allow one to persist dynamically-made changes will automatically generate a new configuration file (keeping a backup of the old one) so that there's no separate, hidden ""supplemental"" configuration file to worry about.",10,"From the customer perspective, I would add that consistency in behavior between MariaDB and MaxScale as pertains to dynamically-made changes would be highly desirable. I'd also note that other products I use that allow one to persist dynamically-made changes will automatically generate a new configuration file (keeping a backup of the old one) so that there's no separate, hidden ""supplemental"" configuration file to worry about."
3357,MXS-3413,MXS,markus makela,216697,2022-03-11 07:34:05,"I think we can make the life of the MaxScale administrators easier by returning more information to the client program whenever a change that causes an overriding configuration to be created. This warning can also be logged so that it is visible in the log as well.

One improvement that can also be done is to show which file an object was loaded from. This should also help identify when an object was created at runtime or when it is read from a static file.",11,"I think we can make the life of the MaxScale administrators easier by returning more information to the client program whenever a change that causes an overriding configuration to be created. This warning can also be logged so that it is visible in the log as well.

One improvement that can also be done is to show which file an object was loaded from. This should also help identify when an object was created at runtime or when it is read from a static file."
3358,MXS-3413,MXS,markus makela,216840,2022-03-14 11:07:15,6.3.0 will now let the user know immediately when they modify something that's defined in the static config. The file from which an object is loaded is also shown in the {{maxctrl show server}} output which should help reduce confusion. ,12,6.3.0 will now let the user know immediately when they modify something that's defined in the static config. The file from which an object is loaded is also shown in the {{maxctrl show server}} output which should help reduce confusion. 
3359,MXS-3413,MXS,Thomas J. Girsch,217506,2022-03-21 18:18:39,I still think it would be highly useful to have a command line option for maxctrl that explicitly tells it to persist (or not persist) the change on a one-off basis.,13,I still think it would be highly useful to have a command line option for maxctrl that explicitly tells it to persist (or not persist) the change on a one-off basis.
3360,MXS-3422,MXS,Niclas Antti,191253,2021-06-07 10:03:31,"Benchmarking for versions of maxscale to find any case where performance is lost.
Not a turn-one-key solution, but completed nevertheless.",1,"Benchmarking for versions of maxscale to find any case where performance is lost.
Not a turn-one-key solution, but completed nevertheless."
3361,MXS-3422,MXS,Valerii Kravchuk,230202,2022-07-22 10:50:09,Where can we see the results of these benchmarking?,2,Where can we see the results of these benchmarking?
3362,MXS-3475,MXS,Todd Stoffel,196868,2021-08-16 19:22:53,Reopening to add the enhancements requested by [~ben],1,Reopening to add the enhancements requested by [~ben]
3363,MXS-3480,MXS,Duong Thien Ly,185064,2021-04-07 09:56:32,"After researching, there are two libraries to be considered which are Monaco editor and CodeMirror. According to this [npmtrends|https://www.npmtrends.com/codemirror-vs-highlight.js-vs-prismjs-vs-syntaxhighlighter-vs-ace-code-editor-vs-monaco-editor], those editors are the top ranked
*Monaco editor:*
_Advantages_
* Well maintained and developed by Microsoft
* Rich features, the one that powered Visual studio code which may attract users who are familiar with Vs Code
* Support theme, allow to customize color theme and styles through either by javascript object or css code.
* Having playground examples to show features
* Has mysql language (https://github.com/microsoft/monaco-languages/blob/main/src/mysql/mysql.ts).  Has ability to add new language.
* Has all built-in necessary functionalities of an editor.
* Robust and modern library using TypeScript

_Disadvantages:_
* Currently, it doesn't provide option to add custom icon for suggestion items ( intellisense suggestion).  It comes with pre-defined item label such as Class, Constant, Enum, Event, Field, Keyword, Function....  ( may not be a big deal as workaround can be applied and future release may support this)
* API documentation is long and overly technical make it a bit difficult to use in the first place.

*Code mirror: (Snowflake is using code mirror)*
_Advantages_
* Well maintained by the community
* It's the editor used in the dev tool of many browsers
* Has built-in sql hint for auto completion feature
* Already has mariadb as sql dialects
* Support theme, allow to customize colors and styles through class name selectors 
* Simple to use and customize
* Has been used by the community for more than a decade which stable release.

_Disadvantages:_
* Many companies using Code mirror have been switching to Monaco editor due to its rich features.
* Needs to import and customize add-on feature such as searching functionality, key maps configurations. 

In brief, after trying to use both libraries in Vue.js, Monaco editor offers better user experience with its built-in editor features. However, the auto-generated API documentation is a bit difficult to understand. 
On the other hand, CodeMirror offers with less built-in features but it supports for add-on features and the documentation is written by human.  Though, this brings more room to customize, it also slows down the process of adding basic functionalities of an editor.
As a result, Monaco editor will be chosen.
Here are the links to the prototype to compare: 
https://mariadb-thienly.github.io/vue-codemirror-sql-editor/
https://mariadb-thienly.github.io/vue-monaco-sql-editor/",1,"After researching, there are two libraries to be considered which are Monaco editor and CodeMirror. According to this [npmtrends|URL those editors are the top ranked
*Monaco editor:*
_Advantages_
* Well maintained and developed by Microsoft
* Rich features, the one that powered Visual studio code which may attract users who are familiar with Vs Code
* Support theme, allow to customize color theme and styles through either by javascript object or css code.
* Having playground examples to show features
* Has mysql language (URL  Has ability to add new language.
* Has all built-in necessary functionalities of an editor.
* Robust and modern library using TypeScript

_Disadvantages:_
* Currently, it doesn't provide option to add custom icon for suggestion items ( intellisense suggestion).  It comes with pre-defined item label such as Class, Constant, Enum, Event, Field, Keyword, Function....  ( may not be a big deal as workaround can be applied and future release may support this)
* API documentation is long and overly technical make it a bit difficult to use in the first place.

*Code mirror: (Snowflake is using code mirror)*
_Advantages_
* Well maintained by the community
* It's the editor used in the dev tool of many browsers
* Has built-in sql hint for auto completion feature
* Already has mariadb as sql dialects
* Support theme, allow to customize colors and styles through class name selectors 
* Simple to use and customize
* Has been used by the community for more than a decade which stable release.

_Disadvantages:_
* Many companies using Code mirror have been switching to Monaco editor due to its rich features.
* Needs to import and customize add-on feature such as searching functionality, key maps configurations. 

In brief, after trying to use both libraries in Vue.js, Monaco editor offers better user experience with its built-in editor features. However, the auto-generated API documentation is a bit difficult to understand. 
On the other hand, CodeMirror offers with less built-in features but it supports for add-on features and the documentation is written by human.  Though, this brings more room to customize, it also slows down the process of adding basic functionalities of an editor.
As a result, Monaco editor will be chosen.
Here are the links to the prototype to compare: 
URL
URL"
3364,MXS-3482,MXS,Johan Wikman,189819,2021-05-19 16:59:21,"Hi [~niljoshi]
Could you give a concrete example of what you have to do now, that such an options file would relieve you from? Just to make sure that I don't assume something you do not intend.
",1,"Hi [~niljoshi]
Could you give a concrete example of what you have to do now, that such an options file would relieve you from? Just to make sure that I don't assume something you do not intend.
"
3365,MXS-3482,MXS,Nilnandan Joshi,190007,2021-05-21 10:10:50,"Hi [~johan.wikman], 

With MySQL, we can create .my.cnf file for storing client related settings including username and password so while connecting with mysql, we don't need to give user/pass in command line. Can we create the same kind of file with maxscale named .maxctrl.cnf or something? so we can store user/pass details in that file for maxctrl command line and can secure the password details?
",2,"Hi [~johan.wikman], 

With MySQL, we can create .my.cnf file for storing client related settings including username and password so while connecting with mysql, we don't need to give user/pass in command line. Can we create the same kind of file with maxscale named .maxctrl.cnf or something? so we can store user/pass details in that file for maxctrl command line and can secure the password details?
"
3366,MXS-3482,MXS,Johan Wikman,190024,2021-05-21 12:13:41,"[~niljoshi]Thanks, that made the intent quite clear!
",3,"[~niljoshi]Thanks, that made the intent quite clear!
"
3367,MXS-3482,MXS,Johan Wikman,190255,2021-05-25 11:52:44,"Now at startup maxctrl looks for a file {{~/.maxctrl.cnf}} and if found, it reads parameters values from the section {{\[maxctrl\]}} and uses those as defaults.

So, the following content would relieve you from typing the username and password.
{code}
[maxctrl]
user = <my-username>
password = <my-password>
{code}",4,"Now at startup maxctrl looks for a file {{~/.maxctrl.cnf}} and if found, it reads parameters values from the section {{\[maxctrl\]}} and uses those as defaults.

So, the following content would relieve you from typing the username and password.
{code}
[maxctrl]
user = 
password = 
{code}"
3368,MXS-3490,MXS,Johan Wikman,185911,2021-04-14 08:27:48,"[~maxmether] No changes whatsoever would be needed Xpand. The Xpand monitor simply has to check whether an error is a group change error and act differently if it is. Currently all errors are treated in the same way.
",1,"[~maxmether] No changes whatsoever would be needed Xpand. The Xpand monitor simply has to check whether an error is a group change error and act differently if it is. Currently all errors are treated in the same way.
"
3369,MXS-3490,MXS,Johan Wikman,219739,2022-04-08 11:25:54,"When a group change is detected, the state of the Xpand nodes is set to down and kept there until it is detected that the group change is over.
",2,"When a group change is detected, the state of the Xpand nodes is set to down and kept there until it is detected that the group change is over.
"
3370,MXS-3492,MXS,Timofey Turenko,188286,2021-05-03 12:40:32,"one setup is running, manual ""how to start the demo setup"" is added to https://github.com/mariadb-corporation/MaxScale/blob/MXS-3429_gui_demo/system-test/README.gui_demo",1,"one setup is running, manual ""how to start the demo setup"" is added to URL"
3371,MXS-3499,MXS,Johan Wikman,187156,2021-04-23 11:14:49,"[~toddstoffel]
From our Slack discussion:
{code}
there's a couple of ways you could do it:
- send a COM_QUERY to the target server with a SELECT MASTER_GTID_WAIT(...) inside of it and wait for that to complete before routing the COM_STMT_EXECUTE or COM_STMT_PREPARE
- modify the COM_STMT_PREPARE to take an extra parameter so that the whole thing can be done with one command (this is the way the current causal_reads works with COM_QUERY)

the first option is pretty straightforward and only requires some additional states in the router

the latter would be the fastest but it's way more complex
{code}

First option roughly 2 weeks, second option 4 weeks. The second option, although faster, would be pretty complex and better avoided.",1,"[~toddstoffel]
From our Slack discussion:
{code}
there's a couple of ways you could do it:
- send a COM_QUERY to the target server with a SELECT MASTER_GTID_WAIT(...) inside of it and wait for that to complete before routing the COM_STMT_EXECUTE or COM_STMT_PREPARE
- modify the COM_STMT_PREPARE to take an extra parameter so that the whole thing can be done with one command (this is the way the current causal_reads works with COM_QUERY)

the first option is pretty straightforward and only requires some additional states in the router

the latter would be the fastest but it's way more complex
{code}

First option roughly 2 weeks, second option 4 weeks. The second option, although faster, would be pretty complex and better avoided."
3372,MXS-3499,MXS,markus makela,187404,2021-04-26 05:55:22,"There's a third option that is a combination of the two: store the SQL for every open prepared statement and emulate the SQL modification by injecting a COM_STMT_PREPARE before it, modifying the ID of the COM_STMT_EXECUTE to refer to that ([similar to how Connector-C does it|https://mariadb.com/kb/en/mariadb_stmt_execute_direct/]) and appending a COM_STMT_CLOSE to it. This removes the need to modify the actual payload of the COM_STMT_EXECUTE and it would keep the latency about as low as the normal use of {{causal_reads}}.

The ""downside"" of this is that this is only doable with the use of a {{BEGIN NOT ATOMIC ... END}} block which means only MariaDB 10.2 and newer can be supported. Since {{causal_reads}} already partially relies on this, it's not a real limitation.

The SQL that would be used is:
{code}
BEGIN NOT ATOMIC 
  IF(MASTER_GTID_WAIT('0-1-123', 10) = 0) THEN 
    <copy SQL statement here>;
  ELSE 
    SIGNAL SQLSTATE '45000' SET MYSQL_ERRNO=30001, MESSAGE_TEXT='GTID wait timed out';
  END IF;
END;
{code}
The user-provided SQL must be trimmed of any trailing semicolons to prevent syntax errors from occurring.

From an effort point of view, this would take the same amount of work as the naive COM_QUERY implementation with some extra work that would have to be done on the protocol layer: the current implementation buffers input until the COM_STMT_PREPARE completes as the prepared statement mapping was moved into the backend protocol. This should not be a complex thing to implement.",2,"There's a third option that is a combination of the two: store the SQL for every open prepared statement and emulate the SQL modification by injecting a COM_STMT_PREPARE before it, modifying the ID of the COM_STMT_EXECUTE to refer to that ([similar to how Connector-C does it|URL and appending a COM_STMT_CLOSE to it. This removes the need to modify the actual payload of the COM_STMT_EXECUTE and it would keep the latency about as low as the normal use of {{causal_reads}}.

The ""downside"" of this is that this is only doable with the use of a {{BEGIN NOT ATOMIC ... END}} block which means only MariaDB 10.2 and newer can be supported. Since {{causal_reads}} already partially relies on this, it's not a real limitation.

The SQL that would be used is:
{code}
BEGIN NOT ATOMIC 
  IF(MASTER_GTID_WAIT('0-1-123', 10) = 0) THEN 
    ;
  ELSE 
    SIGNAL SQLSTATE '45000' SET MYSQL_ERRNO=30001, MESSAGE_TEXT='GTID wait timed out';
  END IF;
END;
{code}
The user-provided SQL must be trimmed of any trailing semicolons to prevent syntax errors from occurring.

From an effort point of view, this would take the same amount of work as the naive COM_QUERY implementation with some extra work that would have to be done on the protocol layer: the current implementation buffers input until the COM_STMT_PREPARE completes as the prepared statement mapping was moved into the backend protocol. This should not be a complex thing to implement."
3373,MXS-3499,MXS,Isaac Venn,188061,2021-04-29 19:57:22,"Option 3 makes the most sense to me as well. The only question I have is about this segment:

The user-provided SQL must be trimmed of any trailing semicolons to prevent syntax errors from occurring.

Is that something Maxscale will do or is it expected to be cleaned up before it enters maxscale?",3,"Option 3 makes the most sense to me as well. The only question I have is about this segment:

The user-provided SQL must be trimmed of any trailing semicolons to prevent syntax errors from occurring.

Is that something Maxscale will do or is it expected to be cleaned up before it enters maxscale?"
3374,MXS-3499,MXS,markus makela,188088,2021-04-30 05:52:06,"The SQL cleanup has to be at least attempted by MaxScale. As in most cases the user-provided SQL either contains no trailing semicolons or it only has one, we can quite easily walk the SQL string backwards and just trim off whitespace until we see a semicolon. This of course won't work correctly when an end-of-the-line style comment ({{#}} or {{--}}) is used and the SQL before it has a semicolon before the comment starts but I'd expect these to be less common.",4,"The SQL cleanup has to be at least attempted by MaxScale. As in most cases the user-provided SQL either contains no trailing semicolons or it only has one, we can quite easily walk the SQL string backwards and just trim off whitespace until we see a semicolon. This of course won't work correctly when an end-of-the-line style comment ({{#}} or {{--}}) is used and the SQL before it has a semicolon before the comment starts but I'd expect these to be less common."
3375,MXS-3499,MXS,markus makela,188870,2021-05-10 12:46:30,"After looking into the alternative implementation that uses the {{BEGIN NOT ATOMIC}} block to perform the synchronization in one step, I found a problem in it that would make it troublesome to implement neatly. Whenever the SQL statement uses a conditional of some sorts, it returns an extra OK packet in the result as if a stored procedure was being executed. As the client who executed the query doesn't expect the SQL to return an extra OK packet, readwritesplit is forced to manipulate the result so that this trailing OK packet is removed and the MORE_RESULTS flag of the last result is cleared. Additional problems arise for any commands that are qualified for causal reads but do not generate a resultset: in this case there is no extra OK packet as the protocol collapses multiple OK packets into one and readwritesplit would have to account for this as well.

After figuring out that this approach wasn't as simple as it initially seemed to be, I proceeded to look for alternative ways to do this. The next solution was to send a COM_QUERY packet before the COM_STMT_EXECUTE with the following SQL:
{code:sql}
IF (MASTER_GTID_WAIT('0-1-123', 10) <> 0) THEN
  KILL (SELECT CONNECTION_ID());
END IF
{code}

The SQL will attempt to synchronize the connection with a known GTID position. If the synchronization is successful, the server will respond with an OK packet which readwritesplit will discard. If the synchronization fails, the {{KILL}} closes the connection, thus preventing the COM_STMT_EXECUTE that follows the COM_QUERY from being executed. Latency-wise this should be roughly equal to both the {{BEGIN NOT ATOMIC}} version as well as the multi-statement version that is currently used for normal SQL statements. In the success case this should be slightly more efficient than the {{BEGIN NOT ATOMIC}} version would as it does not have to create the prepared statement, execute it and then close it. The downside of this approach is the heavy cost of failure as the whole connection has to be recreated if the failed server is to be used again.

Another interesting feature of this approach is that the code required to implement this in 2.6 is very small: the response ignoring code already exists and the re-routing of the failed query to the master can be achieved by injecting a routing hint into the backup copy of the original COM_STMT_EXECUTE. This means that only the SQL generation code has to be added and that's roughly 40 lines of code.",5,"After looking into the alternative implementation that uses the {{BEGIN NOT ATOMIC}} block to perform the synchronization in one step, I found a problem in it that would make it troublesome to implement neatly. Whenever the SQL statement uses a conditional of some sorts, it returns an extra OK packet in the result as if a stored procedure was being executed. As the client who executed the query doesn't expect the SQL to return an extra OK packet, readwritesplit is forced to manipulate the result so that this trailing OK packet is removed and the MORE_RESULTS flag of the last result is cleared. Additional problems arise for any commands that are qualified for causal reads but do not generate a resultset: in this case there is no extra OK packet as the protocol collapses multiple OK packets into one and readwritesplit would have to account for this as well.

After figuring out that this approach wasn't as simple as it initially seemed to be, I proceeded to look for alternative ways to do this. The next solution was to send a COM_QUERY packet before the COM_STMT_EXECUTE with the following SQL:
{code:sql}
IF (MASTER_GTID_WAIT('0-1-123', 10) <> 0) THEN
  KILL (SELECT CONNECTION_ID());
END IF
{code}

The SQL will attempt to synchronize the connection with a known GTID position. If the synchronization is successful, the server will respond with an OK packet which readwritesplit will discard. If the synchronization fails, the {{KILL}} closes the connection, thus preventing the COM_STMT_EXECUTE that follows the COM_QUERY from being executed. Latency-wise this should be roughly equal to both the {{BEGIN NOT ATOMIC}} version as well as the multi-statement version that is currently used for normal SQL statements. In the success case this should be slightly more efficient than the {{BEGIN NOT ATOMIC}} version would as it does not have to create the prepared statement, execute it and then close it. The downside of this approach is the heavy cost of failure as the whole connection has to be recreated if the failed server is to be used again.

Another interesting feature of this approach is that the code required to implement this in 2.6 is very small: the response ignoring code already exists and the re-routing of the failed query to the master can be achieved by injecting a routing hint into the backup copy of the original COM_STMT_EXECUTE. This means that only the SQL generation code has to be added and that's roughly 40 lines of code."
3376,MXS-3499,MXS,markus makela,189549,2021-05-17 10:25:14,"The final implementation used the one described earlier: a COM_QUERY is sent before the COM_STMT_EXECUTE which terminates the connection if it times out. If it doesn't, the OK packet returned by the COM_QUERY is discarded.",6,"The final implementation used the one described earlier: a COM_QUERY is sent before the COM_STMT_EXECUTE which terminates the connection if it times out. If it doesn't, the OK packet returned by the COM_QUERY is discarded."
3377,MXS-3510,MXS,Duong Thien Ly,187149,2021-04-23 11:00:06,"The root cause comes from table-cell component. It constantly listens on mouseenter and mouseleave events.
It is uses for showing  table rowspan ( servers table in dashboard),  parameters info tooltip in parameters table and in case table cell value is truncated, it show untruncated value in tooltip.
There is no need to fix this issue because  even when using `v-data-table` or `v-simple-table`, this doesn't solve the issue.
The issue comes from large datasets, the table shouldn't render all rows. Rendering more than 100 rows, this causes memory issue on the client side.
To deal with large datasets, we should limit the returned rows even when user queries with specific limits. e.g. SELECT * FROM table_name limit 10000;
We should return only 100 for the first fetch.
We can do this by either using pagination or virtual scroll.",1,"The root cause comes from table-cell component. It constantly listens on mouseenter and mouseleave events.
It is uses for showing  table rowspan ( servers table in dashboard),  parameters info tooltip in parameters table and in case table cell value is truncated, it show untruncated value in tooltip.
There is no need to fix this issue because  even when using `v-data-table` or `v-simple-table`, this doesn't solve the issue.
The issue comes from large datasets, the table shouldn't render all rows. Rendering more than 100 rows, this causes memory issue on the client side.
To deal with large datasets, we should limit the returned rows even when user queries with specific limits. e.g. SELECT * FROM table_name limit 10000;
We should return only 100 for the first fetch.
We can do this by either using pagination or virtual scroll."
3378,MXS-3520,MXS,Duong Thien Ly,187685,2021-04-27 14:50:41,Duplicated with MXS-3522,1,Duplicated with MXS-3522
3379,MXS-3545,MXS,Duong Thien Ly,189505,2021-05-17 06:09:54,"To support `xlsx`, it requires installing external library which is not necessary as user can easily convert `csv` to `xlsx`.",1,"To support `xlsx`, it requires installing external library which is not necessary as user can easily convert `csv` to `xlsx`."
3380,MXS-3553,MXS,Duong Thien Ly,191212,2021-06-07 06:06:43,Closed it as bi-directional cursor isn't supported at the moment. ,1,Closed it as bi-directional cursor isn't supported at the moment. 
3381,MXS-3557,MXS,Ivan Zlatoustov,217786,2022-03-23 08:54:07,"Hi [~nantti] what is the name of the new metric that counts all queries?
",1,"Hi [~nantti] what is the name of the new metric that counts all queries?
"
3382,MXS-3563,MXS,Duong Thien Ly,190038,2021-05-21 13:34:39,Close this as it makes no sense to have this button. User can resize the pane themself,1,Close this as it makes no sense to have this button. User can resize the pane themself
3383,MXS-3564,MXS,Duong Thien Ly,190042,2021-05-21 13:49:35,Close this as it makes no sense to have this button. User can resize the pane themselves,1,Close this as it makes no sense to have this button. User can resize the pane themselves
3384,MXS-3569,MXS,Duong Thien Ly,192341,2021-06-21 04:32:43,"Added line, scatter, and vertical/horizontal bar graphs with basic configurations.
",1,"Added line, scatter, and vertical/horizontal bar graphs with basic configurations.
"
3385,MXS-3583,MXS,Johan Wikman,206483,2021-11-19 08:03:12,"30% of the jstests/core/* test-cases now pass.
",1,"30% of the jstests/core/* test-cases now pass.
"
3386,MXS-359,MXS,markus makela,99069,2017-08-22 08:55:37,"First implementations of this could ""promote"" a previous slave server as the new master server of the connection if certain requirements are met. The requirements would be that no transactions are open and that autocommit is enabled.  This should allow master failover but it depends on an external actor changing the replication topology in a way that doesn't cause inconsistencies in the database.  ",1,"First implementations of this could ""promote"" a previous slave server as the new master server of the connection if certain requirements are met. The requirements would be that no transactions are open and that autocommit is enabled.  This should allow master failover but it depends on an external actor changing the replication topology in a way that doesn't cause inconsistencies in the database.  "
3387,MXS-3613,MXS,markus makela,192035,2021-06-16 09:31:48,Created the [initial patch|https://github.com/mariadb-corporation/MaxScale/commit/305b17d51631e2860a9697913a5e24cb04b74d80] that adds support for this and manually tested that it works. System tests for this need to be written before this to be merged into a release branch but this needs MariaDB 10.6 and Connector-C 3.2 to work.,1,Created the [initial patch|URL that adds support for this and manually tested that it works. System tests for this need to be written before this to be merged into a release branch but this needs MariaDB 10.6 and Connector-C 3.2 to work.
3388,MXS-3613,MXS,markus makela,202050,2021-10-11 09:05:44,"Although we haven't yet decided which version to include this in, I'll work on this since it is somewhat tightly coupled with the work related to MXS-1892.",2,"Although we haven't yet decided which version to include this in, I'll work on this since it is somewhat tightly coupled with the work related to MXS-1892."
3389,MXS-3642,MXS,Duong Thien Ly,216146,2022-03-04 06:39:55,"[~toddstoffel], It doesn't now.  We have to build it into the monitor and expose it in the API.",1,"[~toddstoffel], It doesn't now.  We have to build it into the monitor and expose it in the API."
3390,MXS-3642,MXS,Manjot Singh,217862,2022-03-23 15:38:22,"WOW - just the marketing value alone is amazing. This really illustrates the ""lego block"" type configuration that is possible with Maxscale.

In the future we could drag and drop a new service into a chain (like pentaho). That would be huge.",2,"WOW - just the marketing value alone is amazing. This really illustrates the ""lego block"" type configuration that is possible with Maxscale.

In the future we could drag and drop a new service into a chain (like pentaho). That would be huge."
3391,MXS-3645,MXS,Manjot Singh,194540,2021-07-15 18:43:20,"To clarify, this should be available in the default release. ([~toddstoffel] can you get it in the next GA minor?)",1,"To clarify, this should be available in the default release. ([~toddstoffel] can you get it in the next GA minor?)"
3392,MXS-3645,MXS,Niclas Antti,202703,2021-10-18 09:07:58,The tpmfilter functionality has been moved to the qlafilter.,2,The tpmfilter functionality has been moved to the qlafilter.
3393,MXS-3649,MXS,markus makela,235071,2022-09-15 11:22:57,Trivial enough that it can be done in 22.08.2.,1,Trivial enough that it can be done in 22.08.2.
3394,MXS-3663,MXS,markus makela,207694,2021-12-08 08:08:50,One way to implement this would be to execute a {{SELECT @@gtid_binlog_pos}} before each read to make sure the connection uses the latest binlogged GTID for the upcoming read. This would roughly double the maxscale-to-db latency for each causal read done in this manner but it should improve throughput slightly: doing this no longer requires a global lock in readwritesplit as each session does the read separately.,1,One way to implement this would be to execute a {{SELECT @@gtid_binlog_pos}} before each read to make sure the connection uses the latest binlogged GTID for the upcoming read. This would roughly double the maxscale-to-db latency for each causal read done in this manner but it should improve throughput slightly: doing this no longer requires a global lock in readwritesplit as each session does the read separately.
3395,MXS-3663,MXS,markus makela,212473,2022-01-28 06:43:32,"The solution that we ended up selecting was to send a {{SELECT @@gtid_current_pos}} query to the primary node of the cluster before each read. This guarantees that when a read query arrives on MaxScale, it will see at least the latest observable GTID position in the cluster. Same as with the other causal read modes, this it means that if a client executes statement A before statement B, B will always see any effects that A has on the cluster, that is A happens-before B.",2,"The solution that we ended up selecting was to send a {{SELECT @@gtid_current_pos}} query to the primary node of the cluster before each read. This guarantees that when a read query arrives on MaxScale, it will see at least the latest observable GTID position in the cluster. Same as with the other causal read modes, this it means that if a client executes statement A before statement B, B will always see any effects that A has on the cluster, that is A happens-before B."
3396,MXS-37,MXS,Dipti Joshi,68905,2015-03-10 00:14:17,"This is comment history from bugzilla:

+Comment 1 Massimiliano 2015-02-04 09:43:19 UTC+
MaxScale GA needs database names and db grants in order to authenticate db name specified at connect time.

SHOW DATABASES and mysql.db additional grants are required, as described in the pdf docs.


If these grants are not available the authentication with db name fails.


Please give us an example of the user that doesn't work, including privilegs for table level: we can do additional checks.

+Comment 2 lisu87 2015-02-04 14:08:05 UTC+
Username: test-maxscale
Hosts: 172.16.100.24 (backend mysql server), 172.16.77.14 (maxscale)

Privileges:

GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.100.24' IDENTIFIED BY PASSWORD 'passhash'
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.100.24'                                                      
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.100.24'                                                        
GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.100.24'
GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.77.14' IDENTIFIED BY PASSWORD 'passhash'
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.77.14'                                                                 
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.77.14'                                                                   
GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.77.14'

Now I'm trying to access CS.Events with default db specified:

$ mysql -h 172.16.77.14 -P 4007 -u test-maxscale -p CS
Enter password:
ERROR 1045 (28000): Access denied for user 'test-maxscale'@'172.16.100.24' (using password: YES) to database 'CS'

Without default db specified I'm able to connect and use CS db:

$ mysql -h 172.16.77.14 -P 4007 -u test-maxscale -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1089
Server version: 5.5.41-MariaDB (Ubuntu)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use CS;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+--------------+
| Tables_in_CS |
+--------------+
| Events       |
+--------------+
1 row in set (0.00 sec)

But if grant test-maxscale@172.16.100.24 and test-maxscale@172.16.77.14 with SELECT privilege on db level everything works just fine, as shown below.

Privileges:

GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.100.24' IDENTIFIED BY PASSWORD 'passhash' 
GRANT SELECT ON `CS`.* TO 'test-maxscale'@'172.16.100.24'                                                                         
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.100.24'                                                                   
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.100.24'
GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.77.14' IDENTIFIED BY PASSWORD 'passhash' 
GRANT SELECT ON `CS`.* TO 'test-maxscale'@'172.16.77.14'                                                                         
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.77.14'                                                                 
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.77.14'

$ mysql -h 172.16.77.14 -P 4007 -u test-maxscale -p CS
Enter password:
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1089
Server version: 5.5.41-MariaDB (Ubuntu)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

Am I missing something?

+Comment 3 Massimiliano 2015-02-04 16:37:52 UTC+
The user configured in service sections must have the privileges to SHOW DATABASES, mysql.user, mysql.db

Would you mind checking this and share us the maxscale configuration file as well?
+Comment 4 lisu87 2015-02-04 17:07:33 UTC+
Created attachment 182 [details]
current configuration of maxscale

+Comment 5 lisu87 2015-02-04 17:08:38 UTC+
The user configured in services are 'maxscale' and it already has SHOW DATABASES and other required privileges:

GRANT SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'maxscale'@'172.16.77.14' IDENTIFIED BY PASSWORD 'passhash'
GRANT SELECT ON `mysql`.* TO 'maxscale'@'172.16.77.14'

Configuration file attached here: http://bugs.mariadb.com/attachment.cgi?id=182

+Comment 6 Massimiliano 2015-02-05 12:00:22 UTC+
Current MaxScale GA authentication with DB name at connect time requires such a privilege:


GRANT SELECT ON `CS`.* TO 'test-maxscale'@'172.16.77.14', the SELECT privilege on db level


It's not enough for db name authentication the table level grant:


GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.77.14'

+Comment 7 lisu87 2015-02-05 12:52:48 UTC+
I'm glad that you confirmed it.

Is it something you are going to change or it will stay as it is now?

+Comment 8 Massimiliano 2015-02-05 16:37:08 UTC+
We may enhance the database name authentication reading the content of mysql.tables_priv as well.

The aim is to cover such a common case:

GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.77.14'

where one or more table of db are involved",1,"This is comment history from bugzilla:

+Comment 1 Massimiliano 2015-02-04 09:43:19 UTC+
MaxScale GA needs database names and db grants in order to authenticate db name specified at connect time.

SHOW DATABASES and mysql.db additional grants are required, as described in the pdf docs.


If these grants are not available the authentication with db name fails.


Please give us an example of the user that doesn't work, including privilegs for table level: we can do additional checks.

+Comment 2 lisu87 2015-02-04 14:08:05 UTC+
Username: test-maxscale
Hosts: 172.16.100.24 (backend mysql server), 172.16.77.14 (maxscale)

Privileges:

GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.100.24' IDENTIFIED BY PASSWORD 'passhash'
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.100.24'                                                      
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.100.24'                                                        
GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.100.24'
GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.77.14' IDENTIFIED BY PASSWORD 'passhash'
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.77.14'                                                                 
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.77.14'                                                                   
GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.77.14'

Now I'm trying to access CS.Events with default db specified:

$ mysql -h 172.16.77.14 -P 4007 -u test-maxscale -p CS
Enter password:
ERROR 1045 (28000): Access denied for user 'test-maxscale'@'172.16.100.24' (using password: YES) to database 'CS'

Without default db specified I'm able to connect and use CS db:

$ mysql -h 172.16.77.14 -P 4007 -u test-maxscale -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1089
Server version: 5.5.41-MariaDB (Ubuntu)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use CS;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+--------------+
| Tables_in_CS |
+--------------+
| Events       |
+--------------+
1 row in set (0.00 sec)

But if grant test-maxscale@172.16.100.24 and test-maxscale@172.16.77.14 with SELECT privilege on db level everything works just fine, as shown below.

Privileges:

GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.100.24' IDENTIFIED BY PASSWORD 'passhash' 
GRANT SELECT ON `CS`.* TO 'test-maxscale'@'172.16.100.24'                                                                         
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.100.24'                                                                   
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.100.24'
GRANT SHOW DATABASES ON *.* TO 'test-maxscale'@'172.16.77.14' IDENTIFIED BY PASSWORD 'passhash' 
GRANT SELECT ON `CS`.* TO 'test-maxscale'@'172.16.77.14'                                                                         
GRANT SELECT ON `mysql`.`user` TO 'test-maxscale'@'172.16.77.14'                                                                 
GRANT SELECT ON `mysql`.`db` TO 'test-maxscale'@'172.16.77.14'

$ mysql -h 172.16.77.14 -P 4007 -u test-maxscale -p CS
Enter password:
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1089
Server version: 5.5.41-MariaDB (Ubuntu)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

Am I missing something?

+Comment 3 Massimiliano 2015-02-04 16:37:52 UTC+
The user configured in service sections must have the privileges to SHOW DATABASES, mysql.user, mysql.db

Would you mind checking this and share us the maxscale configuration file as well?
+Comment 4 lisu87 2015-02-04 17:07:33 UTC+
Created attachment 182 [details]
current configuration of maxscale

+Comment 5 lisu87 2015-02-04 17:08:38 UTC+
The user configured in services are 'maxscale' and it already has SHOW DATABASES and other required privileges:

GRANT SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'maxscale'@'172.16.77.14' IDENTIFIED BY PASSWORD 'passhash'
GRANT SELECT ON `mysql`.* TO 'maxscale'@'172.16.77.14'

Configuration file attached here: URL

+Comment 6 Massimiliano 2015-02-05 12:00:22 UTC+
Current MaxScale GA authentication with DB name at connect time requires such a privilege:


GRANT SELECT ON `CS`.* TO 'test-maxscale'@'172.16.77.14', the SELECT privilege on db level


It's not enough for db name authentication the table level grant:


GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.77.14'

+Comment 7 lisu87 2015-02-05 12:52:48 UTC+
I'm glad that you confirmed it.

Is it something you are going to change or it will stay as it is now?

+Comment 8 Massimiliano 2015-02-05 16:37:08 UTC+
We may enhance the database name authentication reading the content of mysql.tables_priv as well.

The aim is to cover such a common case:

GRANT SELECT ON `CS`.`Events` TO 'test-maxscale'@'172.16.77.14'

where one or more table of db are involved"
3397,MXS-37,MXS,markus makela,80569,2016-02-04 11:35:32,Changing this to an improvement since it is known and intended behavior.,2,Changing this to an improvement since it is known and intended behavior.
3398,MXS-37,MXS,markus makela,80918,2016-02-16 11:50:44,User grants are now detected at table level.,3,User grants are now detected at table level.
3399,MXS-37,MXS,markus makela,81167,2016-02-22 17:58:34,Fixed in commit c4bcc4ce88d97b7dc3c668beea83b443381b8594,4,Fixed in commit c4bcc4ce88d97b7dc3c668beea83b443381b8594
3400,MXS-3700,MXS,Niclas Antti,200026,2021-09-21 10:53:32,"The QLA logging has been enhanced in version 6.2.0 (see ""Fix Version"" for accurate version).

The benchmark tests were conducted on a few different machines:
- Storage was M.2 and SSD
- Heavy continuous loads utilizing all available cores and HW-threads
- Very fast connections to the database instances
- Capturing all available data-points.

With the test setup(s) above the performance degradation is up to 6%. With normal loads and networks, the performance degradation can be expected to be less than 5%, or even insignificant if the system is never under heavy load.

Configuring log flushing had very little impact. In some cases it even improves overall performance.  However, if storage is slow the performance is likely to be worse. Slow (hdd, nfs, etc.) storage was not tested.
",1,"The QLA logging has been enhanced in version 6.2.0 (see ""Fix Version"" for accurate version).

The benchmark tests were conducted on a few different machines:
- Storage was M.2 and SSD
- Heavy continuous loads utilizing all available cores and HW-threads
- Very fast connections to the database instances
- Capturing all available data-points.

With the test setup(s) above the performance degradation is up to 6%. With normal loads and networks, the performance degradation can be expected to be less than 5%, or even insignificant if the system is never under heavy load.

Configuring log flushing had very little impact. In some cases it even improves overall performance.  However, if storage is slow the performance is likely to be worse. Slow (hdd, nfs, etc.) storage was not tested.
"
3401,MXS-3723,MXS,Manjot Singh,227282,2022-06-20 22:53:03,"This is what I had expected:

    The user clicks the `Load Script` button, it opens a dialog to select the file to upload it to the GUI
    The user makes changes, then they need to click the `Download Script` button, to download it. So it technically still allows the user to open, edit and save files but without the option to choose the location to save the file.

The browser download prompt chooses where to save it.",1,"This is what I had expected:

    The user clicks the `Load Script` button, it opens a dialog to select the file to upload it to the GUI
    The user makes changes, then they need to click the `Download Script` button, to download it. So it technically still allows the user to open, edit and save files but without the option to choose the location to save the file.

The browser download prompt chooses where to save it."
3402,MXS-3725,MXS,Duong Thien Ly,221674,2022-04-25 07:05:51,"Hi[~manjot],  adding macros would probably require defining keybindings, and variables and this gets complex pretty easily, especially in the browser.  But it all depends on how you define what is a macro. If the usage of variables is not required, how about I implement it as snippets? i.e. The user defines a ""prefix"" name for a query,  then it would show up in the autocompletion menu like what I did in the attached video. 
 
",1,"Hi[~manjot],  adding macros would probably require defining keybindings, and variables and this gets complex pretty easily, especially in the browser.  But it all depends on how you define what is a macro. If the usage of variables is not required, how about I implement it as snippets? i.e. The user defines a ""prefix"" name for a query,  then it would show up in the autocompletion menu like what I did in the attached video. 
 
"
3403,MXS-3725,MXS,Manjot Singh,221902,2022-04-26 20:50:45,"That looks pretty cool! so its more of an auto complete?

I think a drop down menu would be good. Yes its more of a snippets store.

A more advanced version (maybe in a later sprint) could recognize frequently run queries and add them to the list.",2,"That looks pretty cool! so its more of an auto complete?

I think a drop down menu would be good. Yes its more of a snippets store.

A more advanced version (maybe in a later sprint) could recognize frequently run queries and add them to the list."
3404,MXS-3763,MXS,markus makela,199722,2021-09-17 10:19:26,"Tests to look into:
* mxs2043_select_for_update	
* mxs2111_auth_string	
* mxs2273_being_drained
* crash_on_bad_sescmd",1,"Tests to look into:
* mxs2043_select_for_update	
* mxs2111_auth_string	
* mxs2273_being_drained
* crash_on_bad_sescmd"
3405,MXS-3783,MXS,markus makela,200673,2021-09-27 05:58:07,What modifications can you do?,1,What modifications can you do?
3406,MXS-3783,MXS,markus makela,202038,2021-10-11 08:10:06,[~fettfoen] can you specify what you did? The actions you did aren't clear from the issue description.,2,[~fettfoen] can you specify what you did? The actions you did aren't clear from the issue description.
3407,MXS-3783,MXS,M.B.,202061,2021-10-11 10:28:34,"I can click ""+create new"" and able to create server, monitor, filter . . .

It is also possible to navigate to ""Settings"" and do some parameters modifications.

BR
M.B.",3,"I can click ""+create new"" and able to create server, monitor, filter . . .

It is also possible to navigate to ""Settings"" and do some parameters modifications.

BR
M.B."
3408,MXS-3783,MXS,markus makela,202062,2021-10-11 10:31:52,"Hmm, that's definitely not what it should do. Can you verify that you do this with the correct user by enabling log info by adding log_info=true under {{\[maxscale\]}} section?",4,"Hmm, that's definitely not what it should do. Can you verify that you do this with the correct user by enabling log info by adding log_info=true under {{\[maxscale\]}} section?"
3409,MXS-3783,MXS,M.B.,202194,2021-10-12 08:45:39,"hi markus,

guess i have to correct myself. I enabled the ""log_info"" and tried to modify a parameter like ""skip_permission_checks"" from ""false"" to ""true"". It didn't work.

LOG:
warning: (authorize_user): Authorization failed for 'test', request requires administrative privileges. Request: PATCH /maxscale

Maybe i was a little bit confused cause i can see and do the modifuications but it cancel my action during the last step with an permission error. 

Wouldn't it be better not to give an unprivileged user the option to edit? Or hide the ""Create New"" button?

I suspect the bug (that it is not a bug) can be closed, that it is more of a GUI improvement?

thanks for help and time",5,"hi markus,

guess i have to correct myself. I enabled the ""log_info"" and tried to modify a parameter like ""skip_permission_checks"" from ""false"" to ""true"". It didn't work.

LOG:
warning: (authorize_user): Authorization failed for 'test', request requires administrative privileges. Request: PATCH /maxscale

Maybe i was a little bit confused cause i can see and do the modifuications but it cancel my action during the last step with an permission error. 

Wouldn't it be better not to give an unprivileged user the option to edit? Or hide the ""Create New"" button?

I suspect the bug (that it is not a bug) can be closed, that it is more of a GUI improvement?

thanks for help and time"
3410,MXS-3783,MXS,markus makela,202197,2021-10-12 09:24:36,"OK, it makes sense now: the GUI does not prevent you from attempting the operation even if the account you are using is just a basic user. This is expected behavior so I'll change this to a feature request and edit the description.",6,"OK, it makes sense now: the GUI does not prevent you from attempting the operation even if the account you are using is just a basic user. This is expected behavior so I'll change this to a feature request and edit the description."
3411,MXS-3783,MXS,M.B.,202198,2021-10-12 09:27:18,Thank you very much. Sorry for the bug report. At first glance it had looked like this.,7,Thank you very much. Sorry for the bug report. At first glance it had looked like this.
3412,MXS-3783,MXS,markus makela,202199,2021-10-12 09:32:41,"No problem, this was an easy thing to mix up as a bug. We appreciate the time you took to file the report and test the behavior.",8,"No problem, this was an easy thing to mix up as a bug. We appreciate the time you took to file the report and test the behavior."
3413,MXS-3784,MXS,Duong Thien Ly,200690,2021-09-27 08:54:45,"Open DDL editor to alter table by selecting  ""Alter Table... "" option in schema context menu.
This switches current worksheet to DDL editor. 

Layout of the  DDL editor:
1. Table related functions
      * Input to change table name
      * A drop-down to select ""Character Set ""
      * A drop-down to select ""Collation""
      * A drop-down to select ""Engine""
      * A text input to add table comment

2. Tab navigation to navigate between Columns, Indexes, Triggers, Partitioning, Options
3. Tab content section contains all functions of chosen tab. 

For 6.2.0 release, the goal is to implement Columns.",1,"Open DDL editor to alter table by selecting  ""Alter Table... "" option in schema context menu.
This switches current worksheet to DDL editor. 

Layout of the  DDL editor:
1. Table related functions
      * Input to change table name
      * A drop-down to select ""Character Set ""
      * A drop-down to select ""Collation""
      * A drop-down to select ""Engine""
      * A text input to add table comment

2. Tab navigation to navigate between Columns, Indexes, Triggers, Partitioning, Options
3. Tab content section contains all functions of chosen tab. 

For 6.2.0 release, the goal is to implement Columns."
3414,MXS-3806,MXS,Naresh Chandra,204714,2021-11-05 02:31:56,Provide a filter like *Exclude a table/schema and Include a table/schema* from the Galera cluster or Primary node to the KafkaCDC router. So that we will be replicate which table or schema need to replicate from the Galera cluster or Primary node to KafkaCDC router.,1,Provide a filter like *Exclude a table/schema and Include a table/schema* from the Galera cluster or Primary node to the KafkaCDC router. So that we will be replicate which table or schema need to replicate from the Galera cluster or Primary node to KafkaCDC router.
3415,MXS-3806,MXS,Naresh Chandra,206484,2021-11-19 08:06:09,Can we fix this issue in the next release?,2,Can we fix this issue in the next release?
3416,MXS-3806,MXS,Naresh Chandra,206673,2021-11-22 12:26:41,Provide a Schema/Table filtering option directly from Galera Cluster/Primary Node to KafkaCDC router without binlog router. ,3,Provide a Schema/Table filtering option directly from Galera Cluster/Primary Node to KafkaCDC router without binlog router. 
3417,MXS-3806,MXS,Naresh Chandra,207806,2021-12-08 17:52:46,"Provide a Schema/Table filtering option directly from Galera Cluster/Primary Node to KafkaCDC router without binlog router.
Please add an exclude and include options for Schema or Table would be good.",4,"Provide a Schema/Table filtering option directly from Galera Cluster/Primary Node to KafkaCDC router without binlog router.
Please add an exclude and include options for Schema or Table would be good."
3418,MXS-3806,MXS,Naresh Chandra,214511,2022-02-17 03:28:34,"Hi Todd Stoffel,

Can you please take it up in the coming release? We need filtering for KafkaCDC router as we are planning for Production environment.",5,"Hi Todd Stoffel,

Can you please take it up in the coming release? We need filtering for KafkaCDC router as we are planning for Production environment."
3419,MXS-3806,MXS,Naresh Chandra,214559,2022-02-17 11:08:10,"We tried with binlog router but we are facing crash issues with binlog filter. and the main thing its not converting the table structures if use the binlog router.

So instead of that, we need own filter for KafkaCDC router would be good.",6,"We tried with binlog router but we are facing crash issues with binlog filter. and the main thing its not converting the table structures if use the binlog router.

So instead of that, we need own filter for KafkaCDC router would be good."
3420,MXS-3806,MXS,Naresh Chandra,214866,2022-02-21 07:44:46,"Hi Todd Stoffel,

Can you please take it up in the coming release? We need filtering for Kafka CDC router as we are planning for Production environment.

We tried with binlog router but we are facing crash issues with binlog filter. and the main thing its not converting the table structures if use the binlog router.

So instead of that, we need own filter for Kafka CDC router would be good.",7,"Hi Todd Stoffel,

Can you please take it up in the coming release? We need filtering for Kafka CDC router as we are planning for Production environment.

We tried with binlog router but we are facing crash issues with binlog filter. and the main thing its not converting the table structures if use the binlog router.

So instead of that, we need own filter for Kafka CDC router would be good."
3421,MXS-3806,MXS,Naresh Chandra,215540,2022-02-25 02:36:44,"Hi Todd,

Thanks for the update.",8,"Hi Todd,

Thanks for the update."
3422,MXS-3806,MXS,markus makela,216166,2022-03-04 10:39:33,"Would two regex parameters, {{exclude}} and {{match}}, be enough to solve the problem for you? This would be the same that [the binlogfilter provides|https://mariadb.com/kb/en/mariadb-maxscale-6-binlog-filter/#match-and-exclude].",9,"Would two regex parameters, {{exclude}} and {{match}}, be enough to solve the problem for you? This would be the same that [the binlogfilter provides|URL"
3423,MXS-3806,MXS,Naresh Chandra,216227,2022-03-04 20:27:52,"Yes   Markus,    We need Table/Schema level filtering which is exclude and match cases like binlog router.",10,"Yes   Markus,    We need Table/Schema level filtering which is exclude and match cases like binlog router."
3424,MXS-3806,MXS,Naresh Chandra,216253,2022-03-07 04:55:34,"Hi Markus,

We can filter entire database right?

ex1: db1.* - filter all the tables.
ex2: db2.tbl1 - we can filter only one table right?
ex3: db3.tb1, db4.tbl8, db9.tbl20 - we can filter specific tables from different db right?

The above combination should work for both the match and exclude parameters right?",11,"Hi Markus,

We can filter entire database right?

ex1: db1.* - filter all the tables.
ex2: db2.tbl1 - we can filter only one table right?
ex3: db3.tb1, db4.tbl8, db9.tbl20 - we can filter specific tables from different db right?

The above combination should work for both the match and exclude parameters right?"
3425,MXS-3806,MXS,markus makela,216254,2022-03-07 05:22:31,"If this is implemented using regular expressions then all the cases are possible as long as the correct regular expression is used. The three patterns for those cases would be:
{code}
db1[.].*
db2[.]tbl1
db3[.]tbl1|db4[.]tbl8|db9[.]tbl20
{code}
The benefit of using a regular expression is that it is very versatile and allows for a great degree of freedom on what to match against but the syntax is not as convenient as a simple list of table or database names.",12,"If this is implemented using regular expressions then all the cases are possible as long as the correct regular expression is used. The three patterns for those cases would be:
{code}
db1[.].*
db2[.]tbl1
db3[.]tbl1|db4[.]tbl8|db9[.]tbl20
{code}
The benefit of using a regular expression is that it is very versatile and allows for a great degree of freedom on what to match against but the syntax is not as convenient as a simple list of table or database names."
3426,MXS-3806,MXS,Naresh Chandra,216255,2022-03-07 05:26:53,"Hi Markus,

Looks good, thanks for the quick fix.",13,"Hi Markus,

Looks good, thanks for the quick fix."
3427,MXS-381,MXS,Jeff Palmer,82927,2016-04-22 03:23:27,"I too would like to see the proxy protocol available for maxscale.   It's a bit of a pain to manage 2 user accounts/ACL's for every connection to maxscale.  having the proxy protocol would effectively cut the our ACL's in half.

",1,"I too would like to see the proxy protocol available for maxscale.   It's a bit of a pain to manage 2 user accounts/ACL's for every connection to maxscale.  having the proxy protocol would effectively cut the our ACL's in half.

"
3428,MXS-381,MXS,Esa Korhonen,94433,2017-04-27 12:19:00,Not really practical with current server implementations. Revisit this issue if/when more servers support this.,2,Not really practical with current server implementations. Revisit this issue if/when more servers support this.
3429,MXS-3822,MXS,Ivan Zlatoustov,211258,2022-01-18 17:03:38,Coordinate with monitoring team addition of the same metric to MaxScale exporter,1,Coordinate with monitoring team addition of the same metric to MaxScale exporter
3430,MXS-3822,MXS,Johan Wikman,228697,2022-07-06 08:33:01,"Some preliminary work was done on this. In C++17 the concept {{polymorphic memory resource}} is introduced. It provides a way for using different memory allocators with template types, without making the instantiated types different. What this means is that different allocators can be used with different variables of the same type, without making those variables incompatible.

Further, allocators can be arranged in a hierarchy where an allocator uses another allocator as its backing store. In the case of MaxScale this could mean that there would be one global allocator and then each routing worker would have an allocator of its own that uses the global allocator as its backing store. Each session in turn would have its own allocator that uses the routing worker's allocator as its backing store.

With this functionality it would basically be possible to internally track down to the last byte just how much memory a particular session is using, how much memory each routing worker is using and how much memory MaxScale as a whole is using, and make this information available via the REST-API.

Unfortunately, it seems that although this functionality is part of C++ 17, the support is currently experimental on some platforms, even if the compiler on the platform claims to support C++ 17. Consequently, this feature has to be put on hold until the required functionality is fully supported on all MaxScale platforms.",2,"Some preliminary work was done on this. In C++17 the concept {{polymorphic memory resource}} is introduced. It provides a way for using different memory allocators with template types, without making the instantiated types different. What this means is that different allocators can be used with different variables of the same type, without making those variables incompatible.

Further, allocators can be arranged in a hierarchy where an allocator uses another allocator as its backing store. In the case of MaxScale this could mean that there would be one global allocator and then each routing worker would have an allocator of its own that uses the global allocator as its backing store. Each session in turn would have its own allocator that uses the routing worker's allocator as its backing store.

With this functionality it would basically be possible to internally track down to the last byte just how much memory a particular session is using, how much memory each routing worker is using and how much memory MaxScale as a whole is using, and make this information available via the REST-API.

Unfortunately, it seems that although this functionality is part of C++ 17, the support is currently experimental on some platforms, even if the compiler on the platform claims to support C++ 17. Consequently, this feature has to be put on hold until the required functionality is fully supported on all MaxScale platforms."
3431,MXS-3822,MXS,Johan Wikman,235959,2022-09-27 11:08:37,"The object returned by the REST-API endpoint {{/maxscale/threads}} has been extended. In the {{attributes}} object there is now a field like
{code}
""memory"": {
    ""query_classifier"": 448,
    ""sessions"": 68468,
    ""total"": 68916,
    ""zombies"": 0
},
{code}
where {{query_classifier}} is the memory used by the thread-specific query classifier, {{sessions}} is the memory used by the sessions handled by the thread, {{zombies}} is the memory used by connections ready to be closed but that have not yet been closed, and {{total}} is simply the sum of the previous.

*NOTE*: Currently the numbers, especially {{sessions}}, are pretty rough, as exact numbers are too costly to obtain, and not all memory usage is included, and should only be used for monitoring how the situation evolves over time, but *not* as a direct measure of the amount of memory used.

{{maxctrl show threads}} has been extended to display this information.
{code}
$ nodejs maxctrl.js show threads
┌────────────────────────┬──────┬──────┬──────┬──────┬────────┬──────┬──────┬───────┬────────┐
│ Id                     │ 0    │ 1    │ 2    │ 3    │ 4      │ 5    │ 6    │ 7     │ Total  │
├────────────────────────┼──────┼──────┼──────┼──────┼────────┼──────┼──────┼───────┼────────┤
...
├────────────────────────┼──────┼──────┼──────┼──────┼────────┼──────┼──────┼───────┼────────┤
│ Memory                 │ 448  │ 448  │ 448  │ 448  │ 134484 │ 448  │ 448  │ 448   │ 137620 │
└────────────────────────┴──────┴──────┴──────┴──────┴────────┴──────┴──────┴───────┴────────┘
{code}
The value shown for each thread, is the {{memory.total}} value from above. A new column {{Total}} shows the sum.

In addition, there is a new REST-API endpoint {{/maxscale/memory}} that only reports the memory usage for each thread and the process.
{code}
$ curl -u admin:mariadb -X GET http://127.0.0.1:8989/v1/maxscale/memory
{
    ""data"": {
        ""attributes"": {
            ""process"": {
                ""query_classifier"": 3584,
                ""sessions"": 68468,
                ""total"": 72052,
                ""zombies"": 0
            },
            ""threads"": [
                {
                    ""query_classifier"": 448,
                    ""sessions"": 0,
                    ""total"": 448,
                    ""zombies"": 0
                },
                ...
            ],
        },
        ""id"": ""memory"",
        ""type"": ""memory""
    },
    ""links"": {
        ""self"": ""http://127.0.0.1:8989/v1/maxscale/memory/""
    }
}
{code}
The value {{data.attributes.process.total}} provides a number for the memory usage of the MaxScale process.

However, it must be stressed, again, that the value currently does *not* show the absolute memory usage of a thread or the process, but provides a value that can be used for following how the situation _evolves over time_.
",3,"The object returned by the REST-API endpoint {{/maxscale/threads}} has been extended. In the {{attributes}} object there is now a field like
{code}
""memory"": {
    ""query_classifier"": 448,
    ""sessions"": 68468,
    ""total"": 68916,
    ""zombies"": 0
},
{code}
where {{query_classifier}} is the memory used by the thread-specific query classifier, {{sessions}} is the memory used by the sessions handled by the thread, {{zombies}} is the memory used by connections ready to be closed but that have not yet been closed, and {{total}} is simply the sum of the previous.

*NOTE*: Currently the numbers, especially {{sessions}}, are pretty rough, as exact numbers are too costly to obtain, and not all memory usage is included, and should only be used for monitoring how the situation evolves over time, but *not* as a direct measure of the amount of memory used.

{{maxctrl show threads}} has been extended to display this information.
{code}
$ nodejs maxctrl.js show threads
┌────────────────────────┬──────┬──────┬──────┬──────┬────────┬──────┬──────┬───────┬────────┐
│ Id                     │ 0    │ 1    │ 2    │ 3    │ 4      │ 5    │ 6    │ 7     │ Total  │
├────────────────────────┼──────┼──────┼──────┼──────┼────────┼──────┼──────┼───────┼────────┤
...
├────────────────────────┼──────┼──────┼──────┼──────┼────────┼──────┼──────┼───────┼────────┤
│ Memory                 │ 448  │ 448  │ 448  │ 448  │ 134484 │ 448  │ 448  │ 448   │ 137620 │
└────────────────────────┴──────┴──────┴──────┴──────┴────────┴──────┴──────┴───────┴────────┘
{code}
The value shown for each thread, is the {{memory.total}} value from above. A new column {{Total}} shows the sum.

In addition, there is a new REST-API endpoint {{/maxscale/memory}} that only reports the memory usage for each thread and the process.
{code}
$ curl -u admin:mariadb -X GET URL
{
    ""data"": {
        ""attributes"": {
            ""process"": {
                ""query_classifier"": 3584,
                ""sessions"": 68468,
                ""total"": 72052,
                ""zombies"": 0
            },
            ""threads"": [
                {
                    ""query_classifier"": 448,
                    ""sessions"": 0,
                    ""total"": 448,
                    ""zombies"": 0
                },
                ...
            ],
        },
        ""id"": ""memory"",
        ""type"": ""memory""
    },
    ""links"": {
        ""self"": ""URL
    }
}
{code}
The value {{data.attributes.process.total}} provides a number for the memory usage of the MaxScale process.

However, it must be stressed, again, that the value currently does *not* show the absolute memory usage of a thread or the process, but provides a value that can be used for following how the situation _evolves over time_.
"
3432,MXS-3844,MXS,Duong Thien Ly,208179,2021-12-13 09:36:34,"Our current GUI only displays data for one MaxScale node. So this is technically not possible at the moment. 
Unless you actually want to have an indicator to the monitor when it has cooperative monitoring enabled and shows an indicator to tell whether it's primary or secondary.",1,"Our current GUI only displays data for one MaxScale node. So this is technically not possible at the moment. 
Unless you actually want to have an indicator to the monitor when it has cooperative monitoring enabled and shows an indicator to tell whether it's primary or secondary."
3433,MXS-3853,MXS,Duong Thien Ly,220341,2022-04-13 08:55:07,"Hi [~naresh.chandra@copart.com], I have a question, do you think that having the ability to delete multiple users at the same time is useful? 
 !deleting_a_user.png|thumbnail!  !deleting_multiple_users.png|thumbnail! ",1,"Hi [~naresh.chandra@copart.com], I have a question, do you think that having the ability to delete multiple users at the same time is useful? 
 !deleting_a_user.png|thumbnail!  !deleting_multiple_users.png|thumbnail! "
3434,MXS-3853,MXS,Naresh Chandra,220345,2022-04-13 09:04:56,"Duong,
 I think most of the cases it may not be required. But if it possible you can keep it.",2,"Duong,
 I think most of the cases it may not be required. But if it possible you can keep it."
3435,MXS-3853,MXS,Duong Thien Ly,220349,2022-04-13 09:12:13,"Thank you for the feedback, [~naresh.chandra@copart.com]. ",3,"Thank you for the feedback, [~naresh.chandra@copart.com]. "
3436,MXS-3891,MXS,Niclas Antti,207723,2021-12-08 11:03:42,All tests are green.,1,All tests are green.
3437,MXS-3894,MXS,Johan Wikman,207187,2021-11-29 18:32:05,"If {{SELECT LAST_INSERT_ID()}} is present in the statements to be replayed it means that the result of that statement already has been sent to the client. If the replay fails due to a checksum mismatch when that statement is executed, it means that the _last insert id_ is different the second time around, perhaps because somebody else has modified the table in the meantime.

If the mismatch is ignored, the application will believe that the last insert id is different from what it actually is. Is that something the application can deal with?",1,"If {{SELECT LAST_INSERT_ID()}} is present in the statements to be replayed it means that the result of that statement already has been sent to the client. If the replay fails due to a checksum mismatch when that statement is executed, it means that the _last insert id_ is different the second time around, perhaps because somebody else has modified the table in the meantime.

If the mismatch is ignored, the application will believe that the last insert id is different from what it actually is. Is that something the application can deal with?"
3438,MXS-3894,MXS,markus makela,207571,2021-12-07 10:06:26,"Adding a {{transaction_replay_checksum}} parameter with the values {{full}}, {{result_only}} and {{no_insert_id}} allows for the following behavior:
* {{transaction_replay_checksum=full}} is how it works currently: the checksum is calculated from all responses and the replayed transaction is known to be consistent with what the client has seen.
* {{transaction_replay_checksum=result_only}} excludes OK packets (responses that aren't resultsets or errors) from the checksum. This effectively ignores the auto-generated insert ID for cases where the application doesn't explicitly request it via SQL. Currently this also ignores the OK packets generated by {{UPDATE}} statements which can cause the number of affected rows to change (might need to be adjusted if this is a problem). This still detects if a statement fails to execute as error packets are included in the checksum and they cause it to diverge if one of the statements fail that previously worked.
* {{transaction_replay_checksum=no_insert_id}} ignores both OK packets and results from queries that use the {{LAST_INSERT_ID()}} function (or any of its aliases). This is only safe for very specific use-cases where the function is unconditionally used or when the fact that the real inserted ID is not the one the client has is not a problem. The same limitations that apply to {{result_only}} also apply to this mode.",2,"Adding a {{transaction_replay_checksum}} parameter with the values {{full}}, {{result_only}} and {{no_insert_id}} allows for the following behavior:
* {{transaction_replay_checksum=full}} is how it works currently: the checksum is calculated from all responses and the replayed transaction is known to be consistent with what the client has seen.
* {{transaction_replay_checksum=result_only}} excludes OK packets (responses that aren't resultsets or errors) from the checksum. This effectively ignores the auto-generated insert ID for cases where the application doesn't explicitly request it via SQL. Currently this also ignores the OK packets generated by {{UPDATE}} statements which can cause the number of affected rows to change (might need to be adjusted if this is a problem). This still detects if a statement fails to execute as error packets are included in the checksum and they cause it to diverge if one of the statements fail that previously worked.
* {{transaction_replay_checksum=no_insert_id}} ignores both OK packets and results from queries that use the {{LAST_INSERT_ID()}} function (or any of its aliases). This is only safe for very specific use-cases where the function is unconditionally used or when the fact that the real inserted ID is not the one the client has is not a problem. The same limitations that apply to {{result_only}} also apply to this mode."
3439,MXS-3898,MXS,markus makela,216380,2022-03-08 09:07:17,Changing this to a New Feature since that's what it is.,1,Changing this to a New Feature since that's what it is.
3440,MXS-3898,MXS,markus makela,216818,2022-03-14 08:01:57,"This should already be possible using the promotion and demotion SQL files: https://mariadb.com/kb/en/mariadb-maxscale-25-mariadb-monitor/#promotion_sql_file-and-demotion_sql_file

[~valerii] do you think this would be adequate or would some more refined behavior be required?",2,"This should already be possible using the promotion and demotion SQL files: URL

[~valerii] do you think this would be adequate or would some more refined behavior be required?"
3441,MXS-3898,MXS,Valerii Kravchuk,216822,2022-03-14 08:49:45,"Yes, for any global dynamic variables these files seem to provide a way to solve the problem. But I'd appreciate some clear examples on how to use them to set timeouts in the KB and/or Enterprise documentation.",3,"Yes, for any global dynamic variables these files seem to provide a way to solve the problem. But I'd appreciate some clear examples on how to use them to set timeouts in the KB and/or Enterprise documentation."
3442,MXS-3898,MXS,markus makela,216823,2022-03-14 08:54:10,Could you give some example values that one would use with the given variables you had? We can then add these as examples into the documentation for these parameters.,4,Could you give some example values that one would use with the given variables you had? We can then add these as examples into the documentation for these parameters.
3443,MXS-3903,MXS,Niclas Antti,232241,2022-08-15 12:13:04,"from #postgressql-friendly

Hi everybody. I looked into PG a few months back. Reading the messages above, I agree with the notion that if there were to be PG compatibility in MariaDB it should be limited to the subset seen as essential to PG users. The scope of my research was very limited and I quickly realized that most of the questions and issues to solve are on the MariaDB side, or maybe even more so on the business and marketing side. We can re-prioritize PG in MaxScale if there is a need for that, but I think this forum is good as the ""What"" needs to be researched before the ""How"".
You are right about the wire protocol, it is not the hard part. A straight protocol converter in MaxScale could be implemented in relatively short order. Simple PG extensions could be translated in that module.
In case more of the PG dialect and extensions had to be supported in MaxScale, we might have to use their parser (rather than enhance our own), which obviously would take a bit longer.
Overall I am somewhat against implementing PG features in MariaDB and MaxScale, and more for migration tools, be it that we add a module for migration in MaxScale as long as it can be defined so that it does not cause the rest of the code base to be touched to any greater extent.
",1,"from #postgressql-friendly

Hi everybody. I looked into PG a few months back. Reading the messages above, I agree with the notion that if there were to be PG compatibility in MariaDB it should be limited to the subset seen as essential to PG users. The scope of my research was very limited and I quickly realized that most of the questions and issues to solve are on the MariaDB side, or maybe even more so on the business and marketing side. We can re-prioritize PG in MaxScale if there is a need for that, but I think this forum is good as the ""What"" needs to be researched before the ""How"".
You are right about the wire protocol, it is not the hard part. A straight protocol converter in MaxScale could be implemented in relatively short order. Simple PG extensions could be translated in that module.
In case more of the PG dialect and extensions had to be supported in MaxScale, we might have to use their parser (rather than enhance our own), which obviously would take a bit longer.
Overall I am somewhat against implementing PG features in MariaDB and MaxScale, and more for migration tools, be it that we add a module for migration in MaxScale as long as it can be defined so that it does not cause the rest of the code base to be touched to any greater extent.
"
3444,MXS-3905,MXS,Timofey Turenko,209687,2022-01-04 10:45:42,"final tests are done, nightly Maxscale builds generates Docker images now",1,"final tests are done, nightly Maxscale builds generates Docker images now"
3445,MXS-391,MXS,markus makela,77219,2015-10-26 17:46:27,"Access for explicit databases should work. The messages in that log are warning about another grant resulting in the same user@host:database combination. This usually happens when hostnames resolve to IP addresses.

The lack of support for wildcard hostnames is a known issue.

MaxScale does reload the grants if a conflict is found. This is only done if authentication fails or if done manually via maxadmin. There is no probing of the database for grant changes so revocations of grants result in a successful connection which fails on the next query. A failure to authenticate with the backend will cause the grants to be reloaded again.",1,"Access for explicit databases should work. The messages in that log are warning about another grant resulting in the same user@host:database combination. This usually happens when hostnames resolve to IP addresses.

The lack of support for wildcard hostnames is a known issue.

MaxScale does reload the grants if a conflict is found. This is only done if authentication fails or if done manually via maxadmin. There is no probing of the database for grant changes so revocations of grants result in a successful connection which fails on the next query. A failure to authenticate with the backend will cause the grants to be reloaded again."
3446,MXS-3912,MXS,markus makela,221796,2022-04-26 07:38:32,"Added creation date, last password update and last login to the REST API output for users. These are also shows in the {{maxctrl list users}} output.

Users created in older versions will show empty values for any timestamps that aren't found. The last login timestamp isn't stored on disk which means it will only show the correct values since the last restart of MaxScale. ",1,"Added creation date, last password update and last login to the REST API output for users. These are also shows in the {{maxctrl list users}} output.

Users created in older versions will show empty values for any timestamps that aren't found. The last login timestamp isn't stored on disk which means it will only show the correct values since the last restart of MaxScale. "
3447,MXS-3913,MXS,markus makela,221797,2022-04-26 07:41:34,Will be implemented as a part of MXS-3912.,1,Will be implemented as a part of MXS-3912.
3448,MXS-3918,MXS,Duong Thien Ly,208588,2021-12-16 12:09:32,Thank you for the request. This seems more like a feature. So I'll change it to the corresponding type.,1,Thank you for the request. This seems more like a feature. So I'll change it to the corresponding type.
3449,MXS-3918,MXS,Naresh Chandra,209101,2021-12-21 18:28:46,"Duong, Thanks for the update.",2,"Duong, Thanks for the update."
3450,MXS-3918,MXS,Naresh Chandra,213737,2022-02-09 17:17:07,"Hi Duong,

Can we implement in the coming release?",3,"Hi Duong,

Can we implement in the coming release?"
3451,MXS-3918,MXS,Todd Stoffel,213747,2022-02-09 18:13:11,[~naresh.chandra@copart.com] I've added this to our summer release.  Thank you for the suggestion.,4,[~naresh.chandra@copart.com] I've added this to our summer release.  Thank you for the suggestion.
3452,MXS-3918,MXS,Naresh Chandra,213775,2022-02-10 03:48:05,"Hi Todd,

Thanks for the update.",5,"Hi Todd,

Thanks for the update."
3453,MXS-3918,MXS,Naresh Chandra,217730,2022-03-22 16:39:47,"Hi Todd,

As per the Development team feedback, they are actually want this stop button. Can we take it this forward in the coming release would be good.",6,"Hi Todd,

As per the Development team feedback, they are actually want this stop button. Can we take it this forward in the coming release would be good."
3454,MXS-3918,MXS,Naresh Chandra,227024,2022-06-17 12:43:51,"Hi Duong,

Can you please confirm this will stop the query execution for PAM user(Rad only Pam user, i.e only SELECT privilege)? ",7,"Hi Duong,

Can you please confirm this will stop the query execution for PAM user(Rad only Pam user, i.e only SELECT privilege)? "
3455,MXS-3918,MXS,Naresh Chandra,227846,2022-06-27 09:56:36,"Hi Duong,

Thanks for the update, can we know the release date of this feature? any tentative date, as the developers are eagerly waiting for this feature.",8,"Hi Duong,

Thanks for the update, can we know the release date of this feature? any tentative date, as the developers are eagerly waiting for this feature."
3456,MXS-3918,MXS,Naresh Chandra,227849,2022-06-27 10:02:31,"Thanks Duong, I got it.  ",9,"Thanks Duong, I got it.  "
3457,MXS-3961,MXS,Duong Thien Ly,215788,2022-03-01 07:16:28,"At the moment, it supports only servers monitored by Monitor using mariadbmon module due to current technical limitations.
https://github.com/mariadb-corporation/MaxScale/blob/develop-thien/Documentation/Getting-Started/MaxGUI.md#visualization
https://github.com/mariadb-corporation/MaxScale/blob/develop-thien/Documentation/Tutorials/Using-MaxGUI-Tutorial.md#clusters",1,"At the moment, it supports only servers monitored by Monitor using mariadbmon module due to current technical limitations.
URL
URL"
3458,MXS-3969,MXS,Timofey Turenko,213997,2022-02-11 17:08:58,"two implementations are done: 
- one using system-test https://github.com/mariadb-corporation/MaxScale/tree/MXS-3969_load_test/
- second using system-test only to create Maxscale+backend setup and sysbench machines are created by external script. This solution is integrated into BuildBot 
https://mdbe-buildbot.mariadb.net/#/builders/337/builds/24",1,"two implementations are done: 
- one using system-test URL
- second using system-test only to create Maxscale+backend setup and sysbench machines are created by external script. This solution is integrated into BuildBot 
URL"
3459,MXS-3993,MXS,markus makela,215196,2022-02-23 08:09:48,The practical benefits of this are quite low as the capabilities are mostly hard-coded based on the database version. The only real case where this would help if the database sends a faked version string claiming to be something newer than it is.,1,The practical benefits of this are quite low as the capabilities are mostly hard-coded based on the database version. The only real case where this would help if the database sends a faked version string claiming to be something newer than it is.
3460,MXS-4010,MXS,markus makela,218980,2022-04-04 10:03:54,Added {{max_file_size}} for controlling the data file sizes and {{max_data_age}} for automatic pruning of data based on the timestamps in it.,1,Added {{max_file_size}} for controlling the data file sizes and {{max_data_age}} for automatic pruning of data based on the timestamps in it.
3461,MXS-4013,MXS,Duong Thien Ly,214698,2022-02-18 11:32:22,"Hi, this seems to be a feature request rather than a bug. I'll change it.
Btw, we do have `Triggers` in the SCHEMAS sidebar.  If you expand the table, it'll show `Columns` and `Triggers` node.",1,"Hi, this seems to be a feature request rather than a bug. I'll change it.
Btw, we do have `Triggers` in the SCHEMAS sidebar.  If you expand the table, it'll show `Columns` and `Triggers` node."
3462,MXS-4013,MXS,Naresh Chandra,242441,2022-11-17 02:48:40,"Hi Duong,

We forgot to mention Indexes in the side bar menu, can you please implement same for Indexes as well? So that user can see all the side bar menus at one place like Tables, Procs, Triggers, Views, Indexes and Functions... etc.",2,"Hi Duong,

We forgot to mention Indexes in the side bar menu, can you please implement same for Indexes as well? So that user can see all the side bar menus at one place like Tables, Procs, Triggers, Views, Indexes and Functions... etc."
3463,MXS-4013,MXS,Duong Thien Ly,242484,2022-11-17 08:27:21,"Hi [~naresh.chandra@copart.com], sure, we can add it",3,"Hi [~naresh.chandra@copart.com], sure, we can add it"
3464,MXS-4013,MXS,Naresh Chandra,242486,2022-11-17 08:39:35,Thanks Duong.,4,Thanks Duong.
3465,MXS-4025,MXS,Naresh Chandra,230154,2022-07-21 16:57:06,"Hi Duong,

If we within worksheet, if open the new 1st editor tab and at the same time we open five editor tabs and if we close the 1st editor tab then other remaining tabs will work right without any connection interrupts?",1,"Hi Duong,

If we within worksheet, if open the new 1st editor tab and at the same time we open five editor tabs and if we close the 1st editor tab then other remaining tabs will work right without any connection interrupts?"
3466,MXS-4025,MXS,Duong Thien Ly,230190,2022-07-22 04:55:28,"Hi [~naresh.chandra@copart.com], yes, that's how it works.",2,"Hi [~naresh.chandra@copart.com], yes, that's how it works."
3467,MXS-4025,MXS,Naresh Chandra,230193,2022-07-22 07:23:04,"Hi Duong,

Thanks for the update. ",3,"Hi Duong,

Thanks for the update. "
3468,MXS-4041,MXS,markus makela,222070,2022-04-28 06:23:43,"Turns out this was easier than expected as libmicrohttpd has a facility for selecting which TLS certificate is presented to the client. This simplifies the problem to only updating the certificates to use the GnuTLS format that is needed for it.

The reloading will affect all future connections and all existing ones will still use the old certificates. This isn't a problem if the only intention is to allow new certificates to be taken into use. If the certificate rotation is done to avoid using old, compromised certificates, MaxScale should still be restarted. In the end, the traffic between the MaxScale REST API and clients has a short lifetime which should make this less of a practical problem and more of a theoretical one.",1,"Turns out this was easier than expected as libmicroURL has a facility for selecting which TLS certificate is presented to the client. This simplifies the problem to only updating the certificates to use the GnuTLS format that is needed for it.

The reloading will affect all future connections and all existing ones will still use the old certificates. This isn't a problem if the only intention is to allow new certificates to be taken into use. If the certificate rotation is done to avoid using old, compromised certificates, MaxScale should still be restarted. In the end, the traffic between the MaxScale REST API and clients has a short lifetime which should make this less of a practical problem and more of a theoretical one."
3469,MXS-4041,MXS,markus makela,222221,2022-04-29 06:34:24,The {{maxctrl reload tls}} command now also reloads the certificates for the REST API.,2,The {{maxctrl reload tls}} command now also reloads the certificates for the REST API.
3470,MXS-406,MXS,markus makela,90229,2017-01-04 11:08:42,http://galeracluster.com/documentation-webpages/mysqlwsrepoptions.html#wsrep-sst-donor,1,URL
3471,MXS-406,MXS,Massimiliano Pinto,90695,2017-01-17 09:55:13,"Server 0x1723b00 (server1) [galera000]
priority    1700000000000000000000001

Server 0x1722d20 (server2) [galera001]
priority    9700

Server 0x17211d0 (server4) [galera003]
prioity not set


2017-01-17 08:56:31   debug  : [galeramon] Sending SET GLOBAL wsrep_sst_donor = ""galera003,galera000,galera001"" to all slave nodes
",2,"Server 0x1723b00 (server1) [galera000]
priority    1700000000000000000000001

Server 0x1722d20 (server2) [galera001]
priority    9700

Server 0x17211d0 (server4) [galera003]
prioity not set


2017-01-17 08:56:31   debug  : [galeramon] Sending SET GLOBAL wsrep_sst_donor = ""galera003,galera000,galera001"" to all slave nodes
"
3472,MXS-4079,MXS,markus makela,222816,2022-05-05 14:21:59,"Added the numbers for a simplified model on conflict probabilities with cooperative monitoring.

The following is used as the model for the conflict probability for getting the server locks:
??randomly distribute n balls in m boxes and calculate the probability of no box having over half of all balls in them??

The following is used as the model for conflicting retry probabilities after a lock acquisition fails and no MaxScale gets a majority of the servers:
??roll a 4-sided dice m times and calculate the probability of having only doubles, triples, quads etc. with no value being rolled only once??

Values were calculated experimentally using a script that generates all states and then counts the number of conflicting ones.",1,"Added the numbers for a simplified model on conflict probabilities with cooperative monitoring.

The following is used as the model for the conflict probability for getting the server locks:
??randomly distribute n balls in m boxes and calculate the probability of no box having over half of all balls in them??

The following is used as the model for conflicting retry probabilities after a lock acquisition fails and no MaxScale gets a majority of the servers:
??roll a 4-sided dice m times and calculate the probability of having only doubles, triples, quads etc. with no value being rolled only once??

Values were calculated experimentally using a script that generates all states and then counts the number of conflicting ones."
3473,MXS-410,MXS,markus makela,76786,2015-10-16 18:52:12,This document should be written after the module interfaces are cleaned up and properly defined. Currently the authentication module does not have an interface and that needs to be defined at some point but it doesn't prevent the document from being written.,1,This document should be written after the module interfaces are cleaned up and properly defined. Currently the authentication module does not have an interface and that needs to be defined at some point but it doesn't prevent the document from being written.
3474,MXS-4104,MXS,markus makela,222113,2022-04-28 11:42:39,Better to do this closer to the release.,1,Better to do this closer to the release.
3475,MXS-4107,MXS,Johan Wikman,245953,2022-12-22 12:30:51,"SSL support has now been added and is documented [here|https://github.com/mariadb-corporation/MaxScale/blob/develop/Documentation/Filters/Cache.md#storage_redis].

",1,"SSL support has now been added and is documented [here|URL

"
3476,MXS-4117,MXS,Naresh Chandra,222346,2022-04-30 02:36:01,"Duong,

Then can we implement this MXS-4025 first and then implement MXS-3723 would be good, so that we wont get any conflict or blocks.
Our developers are expecting this MXS-4025 feature, so that it will be easy for them to run the different queries within the work sheet.",1,"Duong,

Then can we implement this MXS-4025 first and then implement MXS-3723 would be good, so that we wont get any conflict or blocks.
Our developers are expecting this MXS-4025 feature, so that it will be easy for them to run the different queries within the work sheet."
3477,MXS-4117,MXS,Duong Thien Ly,222398,2022-05-02 06:59:52,"Hi [~naresh.chandra@copart.com], yes, that's my plan.",2,"Hi [~naresh.chandra@copart.com], yes, that's my plan."
3478,MXS-4117,MXS,Naresh Chandra,222421,2022-05-02 13:11:13,Thanks Duong. ,3,Thanks Duong. 
3479,MXS-4122,MXS,markus makela,229751,2022-07-18 09:27:33,"More often that not this will devolve into routing all traffic onto the primary node of the cluster and you'd be better off just using {{max_slave_connections=0}}.

The only case where this might be useful is when you have long periods of completely read-only traffic with short bouts of write-intensive loads. Even a single writes will end up redirecting all traffic onto the primary for roughly the duration of {{monitor_interval}}.",1,"More often that not this will devolve into routing all traffic onto the primary node of the cluster and you'd be better off just using {{max_slave_connections=0}}.

The only case where this might be useful is when you have long periods of completely read-only traffic with short bouts of write-intensive loads. Even a single writes will end up redirecting all traffic onto the primary for roughly the duration of {{monitor_interval}}."
3480,MXS-4146,MXS,Anne Strasser,224842,2022-05-25 14:28:11,"Another comment from Luke Smith via docs-talk slack channel

So the next thing it wanted was show databases permission. Though I believe I see the issue, it seems this is just for the monitor user, not for an actual database proxy user(one that checks auth/grants/etc).
So maybe we are just missing that documentation or I was looking in the wrong place. Maybe just adding that fact that creating an ""app_user/service_user"" needs to also have X or link to X might be sufficient.
Thanks again for the help!",1,"Another comment from Luke Smith via docs-talk slack channel

So the next thing it wanted was show databases permission. Though I believe I see the issue, it seems this is just for the monitor user, not for an actual database proxy user(one that checks auth/grants/etc).
So maybe we are just missing that documentation or I was looking in the wrong place. Maybe just adding that fact that creating an ""app_user/service_user"" needs to also have X or link to X might be sufficient.
Thanks again for the help!"
3481,MXS-4146,MXS,Johan Wikman,225842,2022-06-06 11:50:23,"I think this is a documentation problem.

The user used for the monitor and the user used for the service need a disjoint set of rights. However, since the tutorial uses the same user for both the monitor and the service, this may not be obvious.
",2,"I think this is a documentation problem.

The user used for the monitor and the user used for the service need a disjoint set of rights. However, since the tutorial uses the same user for both the monitor and the service, this may not be obvious.
"
3482,MXS-4146,MXS,markus makela,229109,2022-07-11 13:05:34,Changed this to a Task since it's not a bug and doesn't belong in the release notes.,3,Changed this to a Task since it's not a bug and doesn't belong in the release notes.
3483,MXS-4161,MXS,markus makela,229753,2022-07-18 09:40:39,"I changed this from Indicator to Estimation as this is what the issue seems more about. Indicator, at least to me, would mean indicating something that exists currently instead of giving an estimate of what could potentially be.",1,"I changed this from Indicator to Estimation as this is what the issue seems more about. Indicator, at least to me, would mean indicating something that exists currently instead of giving an estimate of what could potentially be."
3484,MXS-4161,MXS,Johan Wikman,231113,2022-08-01 06:27:11,"Since {{C++ 17}} there is in C++ a concept called [polymorphic allocator|https://en.cppreference.com/w/cpp/memory/polymorphic_allocator] that makes it straightforward to use different allocators in different contexts. In practice that would mean that MaxScale could use a dedicated allocator in any situation where we want to be able to report just how much memory is being used. It would also make it straightforward e.g. to pre-allocate memory to be used for a particular purpose.

MaxScale uses {{C++ 17}} and some initial work in this direction was made for 22.08 so that we then could take this into use in earnest in the release after that. Unfortunately, it turned out that although the compilers on platforms supported by MaxScale claim to support {{C++ 17}}, the support for polymorphic allocators is _experimental_ on many of those, so we can't currently use the functionality.

Something similar could be done without the C++ runtime support, but would be quite laborious. Currently it seems that the best approach is to make some preparations, but wait until all compilers on all platforms support this functionality in a non-experimental fashion.",2,"Since {{C++ 17}} there is in C++ a concept called [polymorphic allocator|URL that makes it straightforward to use different allocators in different contexts. In practice that would mean that MaxScale could use a dedicated allocator in any situation where we want to be able to report just how much memory is being used. It would also make it straightforward e.g. to pre-allocate memory to be used for a particular purpose.

MaxScale uses {{C++ 17}} and some initial work in this direction was made for 22.08 so that we then could take this into use in earnest in the release after that. Unfortunately, it turned out that although the compilers on platforms supported by MaxScale claim to support {{C++ 17}}, the support for polymorphic allocators is _experimental_ on many of those, so we can't currently use the functionality.

Something similar could be done without the C++ runtime support, but would be quite laborious. Currently it seems that the best approach is to make some preparations, but wait until all compilers on all platforms support this functionality in a non-experimental fashion."
3485,MXS-4161,MXS,Johan Wikman,235636,2022-09-22 05:52:41,"In MaxScale 22.08.2 {{maxctrl show maxscale}} will show a _system_ object with information about the environment MaxScale is running in.
{code}
$ maxctrl show maxscale
...
├──────────────┼────────────────────────────────────────────────────────────────────────────┤
│ System       │ {                                                                          │
│              │     ""machine"": {                                                           │
│              │         ""cores_available"": 8,                                              │
│              │         ""cores_physical"": 8,                                               │
│              │         ""cores_virtual"": 4,                                                │
│              │         ""memory_available"": 20858544128,                                   │
│              │         ""memory_physical"": 41717088256                                     │
│              │     },                                                                     │
│              │     ""maxscale"": {                                                          │
│              │         ""query_classifier_cache_size"": 6257563238,                         │
│              │         ""threads"": 8                                                       │
│              │     },                                                                     │
│              │     ""os"": {                                                                │
│              │         ""machine"": ""x86_64"",                                               │
│              │         ""nodename"": ""johan-P53s"",                                          │
│              │         ""release"": ""5.4.0-125-generic"",                                    │
│              │         ""sysname"": ""Linux"",                                                │
│              │         ""version"": ""#141~18.04.1-Ubuntu SMP Thu Aug 11 20:15:56 UTC 2022""  │
│              │     }                                                                      │
│              │ }                                                                          │
└──────────────┴────────────────────────────────────────────────────────────────────────────┘
{code}
Of particular interest are {{machine.cores_virtual}} and {{machine.memory_available}}, as they show the cores and memory _available_ to MaxScale and can thus be used for verifying that the configuration file parameters {{threads}} and {{query_classifier_cache_size}} have been set appropriately (their current value are shown in the {{maxscale}} subobject).

The full documentation can be read [here|https://github.com/mariadb-corporation/MaxScale/blob/22.08/Documentation/Getting-Started/Configuration-Guide.md#maxscale-diagnostics-using-maxctrl].

The situation is also checked at startup and if MaxScale detects it is running in a constrained environment, the following kind of warnings will be logged:
{code}
2022-09-22 10:28:47   warning: Number of threads set to 8, which is significantly more than the 4.00 virtual cores available to MaxScale. This may lead to worse performance and MaxScale using more resources than what is available.
2022-09-22 10:28:47   warning: It seems MaxScale is running in a constrained environment with less memory (19.43GiB) available in it than what is installed on the machine (38.85GiB). In this context, the query classifier cache size should be specified explicitly in the configuration file with 'query_classifier_cache_size' set to 15% of the available memory. Otherwise MaxScale may use more resources than what is available, which may cause it to crash.
{code}",3,"In MaxScale 22.08.2 {{maxctrl show maxscale}} will show a _system_ object with information about the environment MaxScale is running in.
{code}
$ maxctrl show maxscale
...
├──────────────┼────────────────────────────────────────────────────────────────────────────┤
│ System       │ {                                                                          │
│              │     ""machine"": {                                                           │
│              │         ""cores_available"": 8,                                              │
│              │         ""cores_physical"": 8,                                               │
│              │         ""cores_virtual"": 4,                                                │
│              │         ""memory_available"": 20858544128,                                   │
│              │         ""memory_physical"": 41717088256                                     │
│              │     },                                                                     │
│              │     ""maxscale"": {                                                          │
│              │         ""query_classifier_cache_size"": 6257563238,                         │
│              │         ""threads"": 8                                                       │
│              │     },                                                                     │
│              │     ""os"": {                                                                │
│              │         ""machine"": ""x86_64"",                                               │
│              │         ""nodename"": ""johan-P53s"",                                          │
│              │         ""release"": ""5.4.0-125-generic"",                                    │
│              │         ""sysname"": ""Linux"",                                                │
│              │         ""version"": ""#141~18.04.1-Ubuntu SMP Thu Aug 11 20:15:56 UTC 2022""  │
│              │     }                                                                      │
│              │ }                                                                          │
└──────────────┴────────────────────────────────────────────────────────────────────────────┘
{code}
Of particular interest are {{machine.cores_virtual}} and {{machine.memory_available}}, as they show the cores and memory _available_ to MaxScale and can thus be used for verifying that the configuration file parameters {{threads}} and {{query_classifier_cache_size}} have been set appropriately (their current value are shown in the {{maxscale}} subobject).

The full documentation can be read [here|URL

The situation is also checked at startup and if MaxScale detects it is running in a constrained environment, the following kind of warnings will be logged:
{code}
2022-09-22 10:28:47   warning: Number of threads set to 8, which is significantly more than the 4.00 virtual cores available to MaxScale. This may lead to worse performance and MaxScale using more resources than what is available.
2022-09-22 10:28:47   warning: It seems MaxScale is running in a constrained environment with less memory (19.43GiB) available in it than what is installed on the machine (38.85GiB). In this context, the query classifier cache size should be specified explicitly in the configuration file with 'query_classifier_cache_size' set to 15% of the available memory. Otherwise MaxScale may use more resources than what is available, which may cause it to crash.
{code}"
3486,MXS-4182,MXS,Johan Wikman,248974,2023-01-30 10:22:24,"In 23.02 the session object returned by the REST-API will contain an integer field {{io_activity}} that reports the number of epoll-events handled for the session, during the last 30 seconds.

The number will be shown in a new {{I/O Activity}} row by {{maxctrl show sessions}} and in a new {{I/O Activity}} column by {{maxctrl list sessions}}. In MaxGUI there will similarly be a new {{I/O Activity}} column in the _Current Sessions_ display.

On the one hand, the value provides an indication of the absolute amount of load caused by one session, on the other hand, when compared to the values of all other sessions what its relative share of the load is.

Note that as the value is an amalgamation of the I/O activity during the last 30 seconds, very short peaks are not visible and it takes a while after the fact for the activity to both increase and decrease.

",1,"In 23.02 the session object returned by the REST-API will contain an integer field {{io_activity}} that reports the number of epoll-events handled for the session, during the last 30 seconds.

The number will be shown in a new {{I/O Activity}} row by {{maxctrl show sessions}} and in a new {{I/O Activity}} column by {{maxctrl list sessions}}. In MaxGUI there will similarly be a new {{I/O Activity}} column in the _Current Sessions_ display.

On the one hand, the value provides an indication of the absolute amount of load caused by one session, on the other hand, when compared to the values of all other sessions what its relative share of the load is.

Note that as the value is an amalgamation of the I/O activity during the last 30 seconds, very short peaks are not visible and it takes a while after the fact for the activity to both increase and decrease.

"
3487,MXS-4182,MXS,Johan Wikman,249111,2023-01-31 12:05:06,"This now partly fulfills the original request. The _I/O Activity_ provides an indication of the load caused by a session. If this is found informative and useful, please create another issue for alerts, if that is deemed necessary.
",2,"This now partly fulfills the original request. The _I/O Activity_ provides an indication of the load caused by a session. If this is found informative and useful, please create another issue for alerts, if that is deemed necessary.
"
3488,MXS-4182,MXS,Naresh Chandra,249541,2023-02-03 11:56:43,"Hi Johan,

Thanks for the update and created the below ticket for alerting.

https://jira.mariadb.org/browse/MXS-4498",3,"Hi Johan,

Thanks for the update and created the below ticket for alerting.

URL"
3489,MXS-4187,MXS,Johan Wikman,231263,2022-08-02 08:32:26,"Reopened, as an assertion still appears to be triggered.
",1,"Reopened, as an assertion still appears to be triggered.
"
3490,MXS-4208,MXS,markus makela,229968,2022-07-20 09:33:39,So far everything except {{pam_authentication}} and {{xpand_mapped_auth}} works on RHEL 8.,1,So far everything except {{pam_authentication}} and {{xpand_mapped_auth}} works on RHEL 8.
3491,MXS-4208,MXS,markus makela,234606,2022-09-09 11:10:11,This is more of a task rather than a bug. Needs to be fixed anyways.,2,This is more of a task rather than a bug. Needs to be fixed anyways.
3492,MXS-4208,MXS,markus makela,251096,2023-02-20 07:03:53,Tests now use Rocky Linux 8.,3,Tests now use Rocky Linux 8.
3493,MXS-421,MXS,Johan Wikman,77068,2015-10-23 16:02:39,"Let me rephrase this to ensure that I understood the idea correctly.

So, there should be predefined categories of errors (e.g. authentication failures) and then you could configure that all errors belonging to that category should be logged with a particular facility. From your example:

*     log_facility_deny_auth=LOG_USER

Further, for each category you could also define the severity, failures of that category should be logged with. From your example:

*     log_level_deny_auth=WARNING

That is, all authentication failures should be logged as warnings, using the facility LOG_USER.",1,"Let me rephrase this to ensure that I understood the idea correctly.

So, there should be predefined categories of errors (e.g. authentication failures) and then you could configure that all errors belonging to that category should be logged with a particular facility. From your example:

*     log_facility_deny_auth=LOG_USER

Further, for each category you could also define the severity, failures of that category should be logged with. From your example:

*     log_level_deny_auth=WARNING

That is, all authentication failures should be logged as warnings, using the facility LOG_USER."
3494,MXS-421,MXS,Johan Wikman,77204,2015-10-26 10:43:29,"Actually, it seems to me that what we are talking about here is not errors (from Maxscale's perspective) but _events_ in which the DBA or end-user might be interested.

For instance, if Maxscale denies access, then Maxscale (is most likely) not malfunctioning, but acting the way it's been configured to act. However, the fact that it denies access may be (and likely is) of interest to he DBA.

So, we need to define a set of events - e.g. access denied - of interest to a DBA and then make it a _configuration issue_ whether they are logged at all and if they are, using what facility and severity.",2,"Actually, it seems to me that what we are talking about here is not errors (from Maxscale's perspective) but _events_ in which the DBA or end-user might be interested.

For instance, if Maxscale denies access, then Maxscale (is most likely) not malfunctioning, but acting the way it's been configured to act. However, the fact that it denies access may be (and likely is) of interest to he DBA.

So, we need to define a set of events - e.g. access denied - of interest to a DBA and then make it a _configuration issue_ whether they are logged at all and if they are, using what facility and severity."
3495,MXS-421,MXS,Johan Wikman,78711,2015-12-03 13:37:13,This is a good idea and should be implemented. Moving to 1.4.,3,This is a good idea and should be implemented. Moving to 1.4.
3496,MXS-421,MXS,Johan Wikman,81673,2016-03-01 14:21:42,Removing fix version as this will not be in 1.4.,4,Removing fix version as this will not be in 1.4.
3497,MXS-4246,MXS,Niclas Antti,235828,2022-09-26 09:39:19,Various benchmarking on different systems and setups indicate there are no major performance issues in 22.08 compared to 6.x.,1,Various benchmarking on different systems and setups indicate there are no major performance issues in 22.08 compared to 6.x.
3498,MXS-4270,MXS,Chris Calender,235608,2022-09-21 18:56:09,Duplicate ticket. See MXS-2660 for the reason on why this cannot be implemented in MaxScale.,1,Duplicate ticket. See MXS-2660 for the reason on why this cannot be implemented in MaxScale.
3499,MXS-4270,MXS, Aurelien,235638,2022-09-22 06:47:37,"Hello Chris,

I cannot see comment into [MXS-2660|https://jira.mariadb.org/browse/MXS-2660] 

If I understand your comment, ed25519 will never be implement into Maxscale ?
What do you propose in place ?

We need to have a serious authentification method. Sha1 was broken in 2004.
Banking/Pharmacy regulation Office will not validate MariaDB in Europe.
",2,"Hello Chris,

I cannot see comment into [MXS-2660|URL 

If I understand your comment, ed25519 will never be implement into Maxscale ?
What do you propose in place ?

We need to have a serious authentification method. Sha1 was broken in 2004.
Banking/Pharmacy regulation Office will not validate MariaDB in Europe.
"
3500,MXS-4270,MXS,Esa Korhonen,246743,2023-01-09 13:48:55,"Ed-support has been added. For full functionality, either a mapping file or sha256-mode is needed.",3,"Ed-support has been added. For full functionality, either a mapping file or sha256-mode is needed."
3501,MXS-4277,MXS,markus makela,234487,2022-09-08 06:58:17,Attached a patch that would make the field configurable by the users. Also converted this into a New Feature as it isn't exactly a bug.,1,Attached a patch that would make the field configurable by the users. Also converted this into a New Feature as it isn't exactly a bug.
3502,MXS-4277,MXS,markus makela,256059,2023-04-13 17:12:37,The issuer of the tokens can be set with {{admin_jwt_issuer}}.,2,The issuer of the tokens can be set with {{admin_jwt_issuer}}.
3503,MXS-43,MXS,zhifeng hu,69127,2015-03-15 05:40:01,"I want to join to help make docker image, now i create an open repository on github, and also create an auto build repo on registry@docker.com

https://github.com/netroby/docker-ubuntu-maxscale

https://registry.hub.docker.com/u/netroby/docker-ubuntu-maxscale/

I would like to help you . any adivce or suggestion were very welcome.",1,"I want to join to help make docker image, now i create an open repository on github, and also create an auto build repo on registry@docker.com

URL

URL

I would like to help you . any adivce or suggestion were very welcome."
3504,MXS-4311,MXS,markus makela,237789,2022-10-17 07:25:11,Design done and reviewed.,1,Design done and reviewed.
3505,MXS-4330,MXS,markus makela,236238,2022-09-29 16:51:45,"Please -fill in affected version-, attach the configuration you used and explain a bit more how you configured the replication. ",1,"Please -fill in affected version-, attach the configuration you used and explain a bit more how you configured the replication. "
3506,MXS-4330,MXS,markus makela,236266,2022-09-30 05:31:54,"OK, managed to reproduce this and it is what I suspected: Xpand sends an unknown command 66 (0x42) which is not how [standard replication|https://mariadb.com/kb/en/com_register_slave/] is [started|https://mariadb.com/kb/en/com_binlog_dump/]. The only difference between those two versions is that MaxScale now prevents crashes by closing the connections immediately when an unexpected internal state is encountered. This was never something that was expected to work given that Xpand replication is started with a non-standard command.

Using an older version is not a safe solution as it is prone to crash at any moment since the old behavior was essentially undefined behavior.",2,"OK, managed to reproduce this and it is what I suspected: Xpand sends an unknown command 66 (0x42) which is not how [standard replication|URL is [started|URL The only difference between those two versions is that MaxScale now prevents crashes by closing the connections immediately when an unexpected internal state is encountered. This was never something that was expected to work given that Xpand replication is started with a non-standard command.

Using an older version is not a safe solution as it is prone to crash at any moment since the old behavior was essentially undefined behavior."
3507,MXS-4330,MXS,Todd Stoffel,236273,2022-09-30 06:44:07,I'm moving this to a new feature request since this only worked in old versions by accident. This was never a designed feature of MaxScale to begin with.  This feature predates the GA implementation of parallel replication in Xpand. Additionally Xpand replication is a different beast than MariaDB replication.,3,I'm moving this to a new feature request since this only worked in old versions by accident. This was never a designed feature of MaxScale to begin with.  This feature predates the GA implementation of parallel replication in Xpand. Additionally Xpand replication is a different beast than MariaDB replication.
3508,MXS-4330,MXS,Manjinder Nijjar,236357,2022-09-30 18:07:28,I did a bisect on maxscale versions and it seems this broke in maxscale version 6.3.1. This used to work fine until 6.3.0. ,4,I did a bisect on maxscale versions and it seems this broke in maxscale version 6.3.1. This used to work fine until 6.3.0. 
3509,MXS-4330,MXS,Manjinder Nijjar,236399,2022-10-01 23:09:11,I have filled XPT-501 (http://bugs.colo.sproutsys.com/show_bug.cgi?id=36252) for Xpand engineering. This issue is not observed with serial replication so we are safe there. ,5,I have filled XPT-501 (URL for Xpand engineering. This issue is not observed with serial replication so we are safe there. 
3510,MXS-4330,MXS,Tim Deeb-Swihart,236835,2022-10-06 18:23:18,Event 66 (0x42) is the command sent from the slave to initiate parallel replication. It's not a bug but part of our parallel replication protocol. Parallel replication is not standard mysql replication but an extension of the protocol,6,Event 66 (0x42) is the command sent from the slave to initiate parallel replication. It's not a bug but part of our parallel replication protocol. Parallel replication is not standard mysql replication but an extension of the protocol
3511,MXS-4330,MXS,Tim Deeb-Swihart,236836,2022-10-06 18:25:32,"Xpand extended the wire protocol with three additional commands: 0x24, 0x42, and 0x43",7,"Xpand extended the wire protocol with three additional commands: 0x24, 0x42, and 0x43"
3512,MXS-4330,MXS,markus makela,249404,2023-02-02 09:28:26,[~timods] can you clarify whether the 0x24 and 0x43 are commands or whether they are replication event types?,8,[~timods] can you clarify whether the 0x24 and 0x43 are commands or whether they are replication event types?
3513,MXS-4330,MXS,Tim Deeb-Swihart,249473,2023-02-02 17:24:34,They are events,9,They are events
3514,MXS-4346,MXS,markus makela,240959,2022-11-07 11:06:52,This seems to have been caused by the thing that [commit cbaca4976b7fb4341c08c6b85864b149773a9c8c|https://github.com/mariadb-corporation/MaxScale/commit/cbaca4976b7fb4341c08c6b85864b149773a9c8c] fixes.,1,This seems to have been caused by the thing that [commit cbaca4976b7fb4341c08c6b85864b149773a9c8c|URL fixes.
3515,MXS-4357,MXS,Valerii Kravchuk,242088,2022-11-15 12:51:21,"This part:

""With master_failure_mode=fail_instantly, the master server is only allowed to change to another server. This change must happen without a loss of the master server.""

is not clear to me. That's why I asked to provide examples. Some session started, connected to a master and did *something*, now master fails (let's say crashes) and a new master is picked up by MaxScale. We need to know what will happen to a session upon the next SELECT or next data change attempt for each reasonable set of related settings. 

Maybe a table is needed, showing the outcome for each set of related parameters (columns) for a given kind of operation (row), including open transaction etc. Specific error returned, what data we get, what else influence the outcome. Or maybe a flow chart to follow to find out the outcome. Something easy to understand without doubts and misinterpretations.",1,"This part:

""With master_failure_mode=fail_instantly, the master server is only allowed to change to another server. This change must happen without a loss of the master server.""

is not clear to me. That's why I asked to provide examples. Some session started, connected to a master and did *something*, now master fails (let's say crashes) and a new master is picked up by MaxScale. We need to know what will happen to a session upon the next SELECT or next data change attempt for each reasonable set of related settings. 

Maybe a table is needed, showing the outcome for each set of related parameters (columns) for a given kind of operation (row), including open transaction etc. Specific error returned, what data we get, what else influence the outcome. Or maybe a flow chart to follow to find out the outcome. Something easy to understand without doubts and misinterpretations."
3516,MXS-4366,MXS,markus makela,239830,2022-11-01 06:38:12,Added the {{async=true}} flag to the REST API.,1,Added the {{async=true}} flag to the REST API.
3517,MXS-4380,MXS,markus makela,242910,2022-11-21 11:02:11,Appeared to fail in 2.5 as well.,1,Appeared to fail in 2.5 as well.
3518,MXS-4380,MXS,markus makela,243026,2022-11-21 22:18:00,Turns out the debug assertion is just faulty: the global state of the {{LOAD DATA LOCAL INFILE}} is not related to the state of an individual connection. The failure in 2.5 is caused by the same situation but it manifests as an actual problem as the code ends up skipping the query result tracking which in turn causes the {{Unexpected result state}} error and a debug assertion.,2,Turns out the debug assertion is just faulty: the global state of the {{LOAD DATA LOCAL INFILE}} is not related to the state of an individual connection. The failure in 2.5 is caused by the same situation but it manifests as an actual problem as the code ends up skipping the query result tracking which in turn causes the {{Unexpected result state}} error and a debug assertion.
3519,MXS-4394,MXS,Duong Thien Ly,242893,2022-11-21 09:25:04,"The button to access the ""Table compare and sync"" feature can have the same UX as the ETL which is mentioned here https://docs.google.com/document/d/1cs0P3aK46Jpq3a_8hI71at6tybU2iVRp7DNCd3jv0rg/edit#bookmark=id.w3f4tvmf88me",1,"The button to access the ""Table compare and sync"" feature can have the same UX as the ETL which is mentioned here URL"
3520,MXS-4395,MXS,Duong Thien Ly,243487,2022-11-24 09:44:55,https://docs.google.com/document/d/10r5NRlwCeLjXrQjtFwbl4wL-CmqINdC4n2Aa1ofIEN0/edit?usp=sharing,1,URL
3521,MXS-4492,MXS,Johan Wikman,249104,2023-01-31 11:54:27,"What used to be
{code}
Copyright (c) 2016 MariaDB Corporation Ab
{code}
should be updated to
{code}
Copyright (c) 2016 MariaDB Corporation Ab
Copyright (c) 2023 MariaDB plc, Finnish Branch
{code}

",1,"What used to be
{code}
Copyright (c) 2016 MariaDB Corporation Ab
{code}
should be updated to
{code}
Copyright (c) 2016 MariaDB Corporation Ab
Copyright (c) 2023 MariaDB plc, Finnish Branch
{code}

"
3522,MXS-4530,MXS,Esa Korhonen,254482,2023-03-27 07:41:35,Basic functionality is complete.,1,Basic functionality is complete.
3523,MXS-4534,MXS,Duong Thien Ly,252609,2023-03-08 07:20:51,https://docs.google.com/document/d/1zGwNod3D3uQ_3WmuEOwQPh88nAKU0rsLSMtnTeLcU-c/edit?usp=sharing,1,URL
3524,MXS-457,MXS,Timofey Turenko,80700,2016-02-08 14:41:17,"test env is ok, tests fail, but test failures are out of scope of this task, closing",1,"test env is ok, tests fail, but test failures are out of scope of this task, closing"
3525,MXS-458,MXS,Timofey Turenko,80026,2016-01-19 00:28:48,included into http://max-tst-01.mariadb.com:8089/view/regular_test/job/run_test_regular/,1,included into URL
3526,MXS-459,MXS,Timofey Turenko,80027,2016-01-19 00:29:05,included into http://max-tst-01.mariadb.com:8089/view/regular_test/job/run_test_regular/,1,included into URL
3527,MXS-460,MXS,Timofey Turenko,80028,2016-01-19 00:29:18,included into http://max-tst-01.mariadb.com:8089/view/regular_test/job/run_test_regular/,1,included into URL
3528,MXS-468,MXS,Timofey Turenko,80023,2016-01-19 00:23:04,"http://max-tst-01.mariadb.com:8089/view/regular_test/job/run_test_regular/ job is added, configured to be executed every night ",1,"URL job is added, configured to be executed every night "
3529,MXS-485,MXS,martin brampton,81973,2016-03-15 18:26:04,"This exists as NullAuth, source code /server/modules/authenticator/null_auth.c.  Not yet in any of the main branches, but will be in due course.",1,"This exists as NullAuth, source code /server/modules/authenticator/null_auth.c.  Not yet in any of the main branches, but will be in due course."
3530,MXS-485,MXS,martin brampton,82014,2016-03-16 11:51:10,Null authentication plugin exists in code.,2,Null authentication plugin exists in code.
3531,MXS-486,MXS,martin brampton,82820,2016-04-19 08:30:26,"The API is defined in the NullAuth authentication module, which also serves as a template for creating new authentication modules.",1,"The API is defined in the NullAuth authentication module, which also serves as a template for creating new authentication modules."
3532,MXS-51,MXS,Dipti Joshi,70601,2015-05-02 05:29:21,"[~tturenko], Please estimate the effort level and update the Original Estimate field.",1,"[~tturenko], Please estimate the effort level and update the Original Estimate field."
3533,MXS-51,MXS,Timofey Turenko,71138,2015-05-15 14:04:56,"""maxscale_update_test_all/"" Jenkins job added. 
Result analysis improvement is still needed, but it will be implemented after move to Vegrant setup.",2,"""maxscale_update_test_all/"" Jenkins job added. 
Result analysis improvement is still needed, but it will be implemented after move to Vegrant setup."
3534,MXS-51,MXS,Dipti Joshi,80362,2016-01-26 16:01:12,"[~johan.wikman] [~tturenko] I see that the regular release prep related testing tasks that were created for 1.3, we are putting them in 1.4.x sprints. Please create new tasks for 1.4.0 release and close 1.3.0 related testing tasks - so that we have proper history of tasks for 1.3 vs. 1.4",3,"[~johan.wikman] [~tturenko] I see that the regular release prep related testing tasks that were created for 1.3, we are putting them in 1.4.x sprints. Please create new tasks for 1.4.0 release and close 1.3.0 related testing tasks - so that we have proper history of tasks for 1.3 vs. 1.4"
3535,MXS-51,MXS,Johan Wikman,80364,2016-01-26 16:05:37,"[~dshjoshi] We have only one sprint in which 1.3 testing and 1.4 development takes place. If there is a mistake, it's having 1.4 in the name of the sprint.
",4,"[~dshjoshi] We have only one sprint in which 1.3 testing and 1.4 development takes place. If there is a mistake, it's having 1.4 in the name of the sprint.
"
3536,MXS-512,MXS,Johan Wikman,80336,2016-01-26 09:48:45,"*qc_get_type*
_Returns the type of the query; READ, WRITE, BEGIN_TRX, COMMIT, etc._
        readwritesplit.c
        shardrouter.c
        schemarouter.c

*qc_get_operation*
_Returns the operation of the query; SELECT, UPDATE, INSERT, etc._
        dbfwfilter.c
        readwritesplit.c
        sharding_common.c
        schemarouter.c

*qc_get_created_table_name*
_Returns the name of the created table (if one was created)._
        readwritesplit.c
        schemarouter.c

*qc_is_drop_table_query*
_Returns true if the query contains ""DROP TABLE...""._
        readwritesplit.c
        schemarouter.c

*qc_is_real_query*
 _Returns true if the query is a SELECT, UPDATE, INSERT, DELETE or any variation of those._
        dbfwfilter.c
        mqfilter.c

*qc_get_table_names*
 _Returns the names of all tables and views in the query._
        mqfilter.c
        readwritesplit.c
        schemarouter.c

*qc_get_canonical*
_Returns the canonical form of the query._
        mqfilter.c

*qc_query_has_clause*
  Returns true if the query has a where or having clause.
        dbfwfilter.c

*qc_get_qtype_str*
_Returns the type of the query as a string._
        readwritesplit.c
        schemarouter.c

*qc_get_affected_fields*
_Returns all fields affected by the query as a string._
        dbfwfilter.c

*qc_get_database_names*
_Returns an arrayf of strings of database that the query uses._
        shardrouter.c
        schemarouter.c
",1,"*qc_get_type*
_Returns the type of the query; READ, WRITE, BEGIN_TRX, COMMIT, etc._
        readwritesplit.c
        shardrouter.c
        schemarouter.c

*qc_get_operation*
_Returns the operation of the query; SELECT, UPDATE, INSERT, etc._
        dbfwfilter.c
        readwritesplit.c
        sharding_common.c
        schemarouter.c

*qc_get_created_table_name*
_Returns the name of the created table (if one was created)._
        readwritesplit.c
        schemarouter.c

*qc_is_drop_table_query*
_Returns true if the query contains ""DROP TABLE...""._
        readwritesplit.c
        schemarouter.c

*qc_is_real_query*
 _Returns true if the query is a SELECT, UPDATE, INSERT, DELETE or any variation of those._
        dbfwfilter.c
        mqfilter.c

*qc_get_table_names*
 _Returns the names of all tables and views in the query._
        mqfilter.c
        readwritesplit.c
        schemarouter.c

*qc_get_canonical*
_Returns the canonical form of the query._
        mqfilter.c

*qc_query_has_clause*
  Returns true if the query has a where or having clause.
        dbfwfilter.c

*qc_get_qtype_str*
_Returns the type of the query as a string._
        readwritesplit.c
        schemarouter.c

*qc_get_affected_fields*
_Returns all fields affected by the query as a string._
        dbfwfilter.c

*qc_get_database_names*
_Returns an arrayf of strings of database that the query uses._
        shardrouter.c
        schemarouter.c
"
3537,MXS-544,MXS,martin brampton,80340,2016-01-26 10:55:10,"Initial action is to clarify the structure of headers that relate to protocols and SSL, with a view to separating SSL from a particular protocol, and enabling different protocols. The logic of this implies moving SSL configuration parameters from service sections to listener sections. SSL is not tied to any particular application level protocol, and it makes more sense to be able to have the option of SSL and non-SSL listeners for the same service (on different ports). Having to change the configuration logic brings one up against a whole host of problems including code quality in config.c",1,"Initial action is to clarify the structure of headers that relate to protocols and SSL, with a view to separating SSL from a particular protocol, and enabling different protocols. The logic of this implies moving SSL configuration parameters from service sections to listener sections. SSL is not tied to any particular application level protocol, and it makes more sense to be able to have the option of SSL and non-SSL listeners for the same service (on different ports). Having to change the configuration logic brings one up against a whole host of problems including code quality in config.c"
3538,MXS-544,MXS,martin brampton,80725,2016-02-09 12:50:37,"SSL basic processing logic all moved into dcb.c as part of the core. Some more general SSL logic placed in new code file gw_ssl.c so as to keep SSL in defined locations. Number of calls from other areas minimised - poll.c contains call to accept a new connection on an EPOLLIN event when SSL connection is required but not complete. SSL is now specified as a property of listeners, instead of services. The same service can have an SSL listener and a non-SSL listener. The configuration guide has been updated to reflect the change and to clarify some points.

Calls from the protocol and authentication logic to SSL have been reduced to the minimum necessary. In principle, SSL is now more readily applied to any protocol, rather than being tied into the MySQL protocol. The changes also help prepare the way for implementing SSL to back end servers.

Actual authentication code for MySQL moved into a new code file mysql_auth.c. This now contains the checks that could (at least in theory) be used with a different communications protocol from MySQL. The organisation of the message exchanges remains in the simplified MySQL client protocol. Separation of the authentication code is a pre-requisite for creating each authentication mechanism as a module with its own API.

",2,"SSL basic processing logic all moved into dcb.c as part of the core. Some more general SSL logic placed in new code file gw_ssl.c so as to keep SSL in defined locations. Number of calls from other areas minimised - poll.c contains call to accept a new connection on an EPOLLIN event when SSL connection is required but not complete. SSL is now specified as a property of listeners, instead of services. The same service can have an SSL listener and a non-SSL listener. The configuration guide has been updated to reflect the change and to clarify some points.

Calls from the protocol and authentication logic to SSL have been reduced to the minimum necessary. In principle, SSL is now more readily applied to any protocol, rather than being tied into the MySQL protocol. The changes also help prepare the way for implementing SSL to back end servers.

Actual authentication code for MySQL moved into a new code file mysql_auth.c. This now contains the checks that could (at least in theory) be used with a different communications protocol from MySQL. The organisation of the message exchanges remains in the simplified MySQL client protocol. Separation of the authentication code is a pre-requisite for creating each authentication mechanism as a module with its own API.

"
3539,MXS-544,MXS,Dipti Joshi,81670,2016-03-01 13:50:52,"[~johan.wikman], Trying to understand if the protocol module improvement is for Binlog-Avro translator feature. Let me know.",3,"[~johan.wikman], Trying to understand if the protocol module improvement is for Binlog-Avro translator feature. Let me know."
3540,MXS-544,MXS,martin brampton,81825,2016-03-08 09:44:19,"Clean up all protocols by creating dcb_accept and dcb_listen as common code for all protocols to handle the acceptance of new connections and the creation of listeners. Modify protocols to make use of them. 

Modify the set of roles for DCBs to make a clean separation between client related and back end server DCBs.  Eliminate dcb_isclient() function rendered obsolete by the new roles.

[~dshjoshi] This task covers a general improvement in protocol code, which enables further development and the consolidation of SSL support. In particular a new task will be created now to handle the move to authentication modules. The authentication for Avro will be created as a module using the new mechanisms. The work being done here is also a basis for developing support for SSL on the server side of MaxScale.",4,"Clean up all protocols by creating dcb_accept and dcb_listen as common code for all protocols to handle the acceptance of new connections and the creation of listeners. Modify protocols to make use of them. 

Modify the set of roles for DCBs to make a clean separation between client related and back end server DCBs.  Eliminate dcb_isclient() function rendered obsolete by the new roles.

[~dshjoshi] This task covers a general improvement in protocol code, which enables further development and the consolidation of SSL support. In particular a new task will be created now to handle the move to authentication modules. The authentication for Avro will be created as a module using the new mechanisms. The work being done here is also a basis for developing support for SSL on the server side of MaxScale."
3541,MXS-544,MXS,Johan Wikman,81832,2016-03-08 13:35:46,"Moving to 1.5, since the tasks related to 1.4 have been made.",5,"Moving to 1.5, since the tasks related to 1.4 have been made."
3542,MXS-544,MXS,martin brampton,82218,2016-03-23 09:57:22,"MySQL backend protocol has been streamlined and checked to some extent. All protocol modules have been checked with lint at level -w2 and all errors and warnings cleared. Basic testing, including testing with failed backend, successful.",6,"MySQL backend protocol has been streamlined and checked to some extent. All protocol modules have been checked with lint at level -w2 and all errors and warnings cleared. Basic testing, including testing with failed backend, successful."
3543,MXS-544,MXS,Johan Wikman,82228,2016-03-23 15:06:03,"[~martin brampton], it had been nice to have had that lint checking as a separate Jira item.
",7,"[~martin brampton], it had been nice to have had that lint checking as a separate Jira item.
"
3544,MXS-544,MXS,martin brampton,82824,2016-04-19 08:40:02,"Further work has been completed to deal with some problems introduced into MySQL backend protocol processing. Handling of the read queue has been changed so that a dcb_read will always return any data from the read queue ahead of any new data that can be read from the socket. After a read, the read queue is thus always empty.",8,"Further work has been completed to deal with some problems introduced into MySQL backend protocol processing. Handling of the read queue has been changed so that a dcb_read will always return any data from the read queue ahead of any new data that can be read from the socket. After a read, the read queue is thus always empty."
3545,MXS-547,MXS,markus makela,85325,2016-08-02 09:33:40,"Modules are now split into core, experimental and devel packages. The core package has the main maxscale software, the experimental package has any experimental modules and devel package has the development headers.",1,"Modules are now split into core, experimental and devel packages. The core package has the main maxscale software, the experimental package has any experimental modules and devel package has the development headers."
3546,MXS-553,MXS,markus makela,80113,2016-01-21 16:30:21,The connection ID is already stored at the backend protocol level but there is no interface for the protocol modules to print protocol specific diagnostic data.,1,The connection ID is already stored at the backend protocol level but there is no interface for the protocol modules to print protocol specific diagnostic data.
3547,MXS-553,MXS,markus makela,99068,2017-08-22 08:51:50,This requires that the protocol modules implement a diagnostic output function.,2,This requires that the protocol modules implement a diagnostic output function.
3548,MXS-558,MXS,Johan Wikman,80717,2016-02-09 10:41:04,"It is non-trivial to use the parser of sqlite.

Sqlite is structured so that the tokenizer calls the parser that calls the code generator that generates code for a virtual machine. That code is then subsequently executed. And these steps are closely intertwined; for instance, using only the tokenizer and parser is not readily possible.

The general architecture of sqlite is shown here: https://www.sqlite.org/arch.html

It appears that in order to use the sqlite tokenizer and parser you would essentially have to replace the code generator part. Unfortunately there is no single point to do that but as the documentation on the page above says:

_There are many files in the code generator: attach.c, auth.c, build.c, delete.c, expr.c, insert.c, pragma.c, select.c, trigger.c, update.c, vacuum.c and where.c. In these files is where most of the serious magic happens. expr.c handles code generation for expressions. where.c handles code generation for WHERE clauses on SELECT, UPDATE and DELETE statements. The files attach.c, delete.c, insert.c, select.c, trigger.c update.c, and vacuum.c handle the code generation for SQL statements with the same names. (Each of these files calls routines in expr.c and where.c as necessary.) All other SQL statements are coded out of build.c._",1,"It is non-trivial to use the parser of sqlite.

Sqlite is structured so that the tokenizer calls the parser that calls the code generator that generates code for a virtual machine. That code is then subsequently executed. And these steps are closely intertwined; for instance, using only the tokenizer and parser is not readily possible.

The general architecture of sqlite is shown here: URL

It appears that in order to use the sqlite tokenizer and parser you would essentially have to replace the code generator part. Unfortunately there is no single point to do that but as the documentation on the page above says:

_There are many files in the code generator: attach.c, auth.c, build.c, delete.c, expr.c, insert.c, pragma.c, select.c, trigger.c, update.c, vacuum.c and where.c. In these files is where most of the serious magic happens. expr.c handles code generation for expressions. where.c handles code generation for WHERE clauses on SELECT, UPDATE and DELETE statements. The files attach.c, delete.c, insert.c, select.c, trigger.c update.c, and vacuum.c handle the code generation for SQL statements with the same names. (Each of these files calls routines in expr.c and where.c as necessary.) All other SQL statements are coded out of build.c._"
3549,MXS-558,MXS,Johan Wikman,81047,2016-02-18 13:11:52,Take a second look.,2,Take a second look.
3550,MXS-558,MXS,Johan Wikman,81104,2016-02-19 14:48:52,"The general architecture is shown here: https://www.sqlite.org/arch.html

Sqlite does things backwards compared to how things usually are done; the
SQL statement is given to the tokenizer, which calls the parser, which
calls the code generator. The parser for SQLite is generated using Lemon,
a parser generator that like sqlite itself is created by Richard Hipp.

If the actual parsing goes ok, then parser calls into a specific function
depending on the statement. For instance, a simple select statement ends
up with a call to

   int sqlite3Select(Parse* pParse, Select* p, SelectDest* pDest);

At that point, p contains information about the select statement and
could be used for extracting whatever is interesting.

However, what that sqlite3Select then does is to generate code for
sqlite's virtual machine and at that point it is also checked whether the
statement really can be executed.

For instance, any select will at that point fail unless e.g. all tables
referred to exist in the dummy sqlite database that needs to be created
in order to at all be able to parse something.

That Select structure is subsequently deleted (irrespective of the
outcome of sqlite3Select) when sqlite3Select returns.

So, it just might be possible to use sqlite's parser by replacing
sqlite3Select and all other relevant functions (at this point I can't say
how many there would be), with our own versions that merely would record
that information (in some thread specific location) and then report
failure.
",3,"The general architecture is shown here: URL

Sqlite does things backwards compared to how things usually are done; the
SQL statement is given to the tokenizer, which calls the parser, which
calls the code generator. The parser for SQLite is generated using Lemon,
a parser generator that like sqlite itself is created by Richard Hipp.

If the actual parsing goes ok, then parser calls into a specific function
depending on the statement. For instance, a simple select statement ends
up with a call to

   int sqlite3Select(Parse* pParse, Select* p, SelectDest* pDest);

At that point, p contains information about the select statement and
could be used for extracting whatever is interesting.

However, what that sqlite3Select then does is to generate code for
sqlite's virtual machine and at that point it is also checked whether the
statement really can be executed.

For instance, any select will at that point fail unless e.g. all tables
referred to exist in the dummy sqlite database that needs to be created
in order to at all be able to parse something.

That Select structure is subsequently deleted (irrespective of the
outcome of sqlite3Select) when sqlite3Select returns.

So, it just might be possible to use sqlite's parser by replacing
sqlite3Select and all other relevant functions (at this point I can't say
how many there would be), with our own versions that merely would record
that information (in some thread specific location) and then report
failure.
"
3551,MXS-558,MXS,Johan Wikman,81824,2016-03-08 08:42:51,"After all, it seems like using the parser of sqlite when doing query classification is an option. The next steps would be to build sqlite as part of MaxScale, and create a query classifier plugin that uses sqlite.",4,"After all, it seems like using the parser of sqlite when doing query classification is an option. The next steps would be to build sqlite as part of MaxScale, and create a query classifier plugin that uses sqlite."
3552,MXS-584,MXS,markus makela,80931,2016-02-16 15:14:06,The problem is with the binlogrouter and not the connector. The current implementation expects only valid queries and unexpected queries cause a disconnection. This could be changed to allow various automated tools to do checks on the binlogrouter without disconnecting the client.,1,The problem is with the binlogrouter and not the connector. The current implementation expects only valid queries and unexpected queries cause a disconnection. This could be changed to allow various automated tools to do checks on the binlogrouter without disconnecting the client.
3553,MXS-584,MXS,Massimiliano Pinto,83898,2016-06-01 09:09:35,"SET autocommit = 
and
SET @@session.autocommit

are now handled.",2,"SET autocommit = 
and
SET @@session.autocommit

are now handled."
3554,MXS-591,MXS,markus makela,82172,2016-03-21 18:51:23,"There are a few ways this filter could work for auditing.

* Injects one or more queries before the client session starts
* Add comments to queries

Adding comments to queries seems like the simpler option and makes for a more robust filter. The filter should allow users to define the comments and what goes in them. Some sort of variable expansion should also be possible similar to how monitors expand server addresses for launchable scripts: https://mariadb.com/kb/en/mariadb-enterprise/mariadb-maxscale/common-monitor-parameters/


",1,"There are a few ways this filter could work for auditing.

* Injects one or more queries before the client session starts
* Add comments to queries

Adding comments to queries seems like the simpler option and makes for a more robust filter. The filter should allow users to define the comments and what goes in them. Some sort of variable expansion should also be possible similar to how monitors expand server addresses for launchable scripts: URL


"
3555,MXS-609,MXS,markus makela,82002,2016-03-16 09:57:20,Added upgrade document from 1.3 to 1.4.,1,Added upgrade document from 1.3 to 1.4.
3556,MXS-614,MXS,Timofey Turenko,82092,2016-03-18 09:43:46,a small issue with MariaDB tests: they assume Master is node0. It should be fixed,1,a small issue with MariaDB tests: they assume Master is node0. It should be fixed
3557,MXS-615,MXS,martin brampton,82217,2016-03-23 09:55:20,"The current authentication methods (MySQL and MaxAdmin) are now implemented as authenticator modules, along with NullAuth which is a default authenticator and a model for building new authenticator modules. Local testing has been successful, but full system testing is still required.",1,"The current authentication methods (MySQL and MaxAdmin) are now implemented as authenticator modules, along with NullAuth which is a default authenticator and a model for building new authenticator modules. Local testing has been successful, but full system testing is still required."
3558,MXS-615,MXS,martin brampton,82520,2016-04-05 08:46:28,"At this point, this work seems essentially complete. Subsequent work done in MXS-544 (in branch MXS-544\-b) created a problem which appeared in testing of develop\-MXS-544-b-merge. However, problem resolution indicates that the issue was in the protocols and will soon be resolved. The authenticator modules have not given any indication of faults. This item is therefore being closed, with further work on protocols to be continued under MXS-544.",2,"At this point, this work seems essentially complete. Subsequent work done in MXS-544 (in branch MXS-544\-b) created a problem which appeared in testing of develop\-MXS-544-b-merge. However, problem resolution indicates that the issue was in the protocols and will soon be resolved. The authenticator modules have not given any indication of faults. This item is therefore being closed, with further work on protocols to be continued under MXS-544."
3559,MXS-622,MXS,Johan Wikman,83840,2016-05-31 11:23:14,Removed fix-version. Need to be decided separately when these improvements are to be made. ,1,Removed fix-version. Need to be decided separately when these improvements are to be made. 
3560,MXS-622,MXS,markus makela,90640,2017-01-16 08:06:55,The dbfwfilter now supports function blocking.,2,The dbfwfilter now supports function blocking.
3561,MXS-636,MXS,Johan Wikman,82216,2016-03-23 09:51:51,"I think this is a duplicate and may actually have been fixed already. [~markus makela], could you comment.",1,"I think this is a duplicate and may actually have been fixed already. [~markus makela], could you comment."
3562,MXS-636,MXS,markus makela,82219,2016-03-23 10:31:09,I couldn't find an open bug for this so this was created.,2,I couldn't find an open bug for this so this was created.
3563,MXS-644,MXS,Timofey Turenko,82397,2016-03-31 06:15:42,"failures: 
- bug670 (expected, TEE filter)
- short sessions - related tests (randomly), tracked as https://jira.mariadb.org/browse/MXS-619
",1,"failures: 
- bug670 (expected, TEE filter)
- short sessions - related tests (randomly), tracked as URL
"
3564,MXS-651,MXS,martin brampton,82821,2016-04-19 08:31:49,Preparatory work is done to handle the configuration details. The next step is to discuss the way the MySQL backend protocol interfaces to database servers so as to allow for requesting a SSL connection. ,1,Preparatory work is done to handle the configuration details. The next step is to discuss the way the MySQL backend protocol interfaces to database servers so as to allow for requesting a SSL connection. 
3565,MXS-651,MXS,Dipti Joshi,83474,2016-05-17 16:35:33,"[~johan.wikman] [~martin brampton] Is this task going to be complete in current sprint ?

Thanks,
Dipti",2,"[~johan.wikman] [~martin brampton] Is this task going to be complete in current sprint ?

Thanks,
Dipti"
3566,MXS-651,MXS,Johan Wikman,83483,2016-05-18 07:25:12,That is the goal.,3,That is the goal.
3567,MXS-651,MXS,martin brampton,83490,2016-05-18 10:19:48,"On a straghtforward setup, backend SSL is working correctly. Tests on different configurations are to be done, and some code tidying. Certainly, the hope is that it will be complete for the next major release, as planned.",4,"On a straghtforward setup, backend SSL is working correctly. Tests on different configurations are to be done, and some code tidying. Certainly, the hope is that it will be complete for the next major release, as planned."
3568,MXS-651,MXS,martin brampton,83703,2016-05-26 08:52:19,This could be facing delays. The SSL to backend database processing works fine for small amounts of data (up to 10K or maybe more) but frequently hangs on larger requests. Resolving this is likely to require deeper investigation into the workings of OpenSSL and epoll processing of non-blocking socket I/O. That could be time consuming.,5,This could be facing delays. The SSL to backend database processing works fine for small amounts of data (up to 10K or maybe more) but frequently hangs on larger requests. Resolving this is likely to require deeper investigation into the workings of OpenSSL and epoll processing of non-blocking socket I/O. That could be time consuming.
3569,MXS-651,MXS,martin brampton,83708,2016-05-26 15:25:31,"Unfortunately, a check on the released client side SSL shows that it too fails on large data sends (test used a table dump about 0.5 MB in size, 10,000 rows). Many applications will run without ever sending very long queries, but if they do, the connection will hang. This demonstrates that the problem is to do with SSL implementation generally, rather than the logic of the MySQL protocols, mysql_client.c and mysql_backend.c. The previous comment is therefore confirmed, at least until we know more.",6,"Unfortunately, a check on the released client side SSL shows that it too fails on large data sends (test used a table dump about 0.5 MB in size, 10,000 rows). Many applications will run without ever sending very long queries, but if they do, the connection will hang. This demonstrates that the problem is to do with SSL implementation generally, rather than the logic of the MySQL protocols, mysql_client.c and mysql_backend.c. The previous comment is therefore confirmed, at least until we know more."
3570,MXS-651,MXS,martin brampton,83757,2016-05-29 18:42:20,I am hopeful that the problems are now resolved.,7,I am hopeful that the problems are now resolved.
3571,MXS-651,MXS,martin brampton,83825,2016-05-31 08:12:57,"The code has passed tests, and the specific faults referred to above are resolved. The code is merged into the develop branch. Work is needed to amend the documentation to describe back end SSL.",8,"The code has passed tests, and the specific faults referred to above are resolved. The code is merged into the develop branch. Work is needed to amend the documentation to describe back end SSL."
3572,MXS-651,MXS,martin brampton,84241,2016-06-14 08:51:22,Documentation has been completed.,9,Documentation has been completed.
3573,MXS-66,MXS,markus makela,70600,2015-05-02 05:09:31,"The core of the problem is that each individual module needs its own special subset of parameters. This means that either each module must check all given parameters and if it finds an unexpected one, refuse to load, or that a generic way to retrieve module parameters must be created.

The first solution would fix the problem for existing modules but the amount of code that needs to be updated and checked grows significantly with each added module. The second solution would simplify the module configuration process and common code could be used for all modules. This solution will require modification of all module interfaces.",1,"The core of the problem is that each individual module needs its own special subset of parameters. This means that either each module must check all given parameters and if it finds an unexpected one, refuse to load, or that a generic way to retrieve module parameters must be created.

The first solution would fix the problem for existing modules but the amount of code that needs to be updated and checked grows significantly with each added module. The second solution would simplify the module configuration process and common code could be used for all modules. This solution will require modification of all module interfaces."
3574,MXS-66,MXS,markus makela,83476,2016-05-17 19:13:26,MaxScale will refuse to start if a bad parameter is given. Router options and filter parameters are not checked so the modules themselves should check them.,2,MaxScale will refuse to start if a bad parameter is given. Router options and filter parameters are not checked so the modules themselves should check them.
3575,MXS-669,MXS,markus makela,82985,2016-04-25 08:15:26,The modutil_get_complete_packets now returns a chain of buffers instead of one large buffer. This will reduce the amount of copying done when processing large result sets.,1,The modutil_get_complete_packets now returns a chain of buffers instead of one large buffer. This will reduce the amount of copying done when processing large result sets.
3576,MXS-678,MXS,Timofey Turenko,82834,2016-04-19 11:41:01,relates to https://jira.mariadb.org/browse/MXS-213,1,relates to URL
3577,MXS-678,MXS,Timofey Turenko,83053,2016-04-27 12:23:39,"tests for all combinations of seminic on/off, transactions safety on/off, and semisync plugin installed/uninstalled are added to our test suite, will be executed regulary

test run done with 10.0.",2,"tests for all combinations of seminic on/off, transactions safety on/off, and semisync plugin installed/uninstalled are added to our test suite, will be executed regulary

test run done with 10.0."
3578,MXS-701,MXS,markus makela,99064,2017-08-22 08:31:03,This should be possible to implement as a MaxScale filter in 2.2 now that the binlogrouter routes the replies to the slaves via the normal routing mechanisms.,1,This should be possible to implement as a MaxScale filter in 2.2 now that the binlogrouter routes the replies to the slaves via the normal routing mechanisms.
3579,MXS-701,MXS,Massimiliano Pinto,103453,2017-11-21 09:03:06,"MXS-701 branch holds the first implementation of the filter with CRC32 support.


Next additions, 16MB payloads and DDLs are in 'develop'",2,"MXS-701 branch holds the first implementation of the filter with CRC32 support.


Next additions, 16MB payloads and DDLs are in 'develop'"
3580,MXS-701,MXS,Massimiliano Pinto,103455,2017-11-21 09:14:09,"Current implementation is with ROW based replication:

when TABLE_MAP event is seen the db/table are checked against configuration

In case of match TABLE_MAP event and all next events, i.e. Write_rows_v1 etc are ""replaced"" by a RAND_EVENT with:
- serverid = 0
- timestamp = 0
- next_pos = 0
- event_type = RAND_EVENT
- flags = LOG_EVENT_IGNORABLE_F | LOG_EVENT_SKIP_REPLICATION_F

This keeps the packet sequence but SAVES the bandwidth by not sending real data but only an empty rand_event: 19 bytes.

When XID_EVENT is seen (commit) the filtering ends and this event is altered with next_pos = 0
 
Note: all packets belonging to a large event, i.e 34 MBytes are replaced by empty rand_events

Example of replaced event
at 1356

/!50521 SET skip_replication=1//!/;

700101 1:00:00 server id 0 end_log_pos 0 CRC32 0x354f3a99 Rand

SET @@RAND_SEED1=894384793, @@RAND_SEED2=51539607552/!/;


Commit example:

171113 18:53:21 server id 10124 end_log_pos 0 CRC32 0xa93380fa Xid = 11967
",3,"Current implementation is with ROW based replication:

when TABLE_MAP event is seen the db/table are checked against configuration

In case of match TABLE_MAP event and all next events, i.e. Write_rows_v1 etc are ""replaced"" by a RAND_EVENT with:
- serverid = 0
- timestamp = 0
- next_pos = 0
- event_type = RAND_EVENT
- flags = LOG_EVENT_IGNORABLE_F | LOG_EVENT_SKIP_REPLICATION_F

This keeps the packet sequence but SAVES the bandwidth by not sending real data but only an empty rand_event: 19 bytes.

When XID_EVENT is seen (commit) the filtering ends and this event is altered with next_pos = 0
 
Note: all packets belonging to a large event, i.e 34 MBytes are replaced by empty rand_events

Example of replaced event
at 1356

/!50521 SET skip_replication=1//!/;

700101 1:00:00 server id 0 end_log_pos 0 CRC32 0x354f3a99 Rand

SET @@RAND_SEED1=894384793, @@RAND_SEED2=51539607552/!/;


Commit example:

171113 18:53:21 server id 10124 end_log_pos 0 CRC32 0xa93380fa Xid = 11967
"
3581,MXS-709,MXS,Johan Wikman,83766,2016-05-30 07:11:36,"Most SET statements are classified correctly, but some are classified
correctly only based on keywords and not by having been parsed.

For instance, the following:
    SET @arg00=_binary 0xD800; # The cast
    SET @'a' = 1;              # The variable being inside quotes
    SET PASSWORD FOR ...       # id FOR...

are not parsed, but only classified from keywords. The result is in
full agreement with qc_mysqlembedded though.",1,"Most SET statements are classified correctly, but some are classified
correctly only based on keywords and not by having been parsed.

For instance, the following:
    SET @arg00=_binary 0xD800; # The cast
    SET @'a' = 1;              # The variable being inside quotes
    SET PASSWORD FOR ...       # id FOR...

are not parsed, but only classified from keywords. The result is in
full agreement with qc_mysqlembedded though."
3582,MXS-722,MXS,markus makela,83509,2016-05-18 17:09:31,"Added a {{--config-check}} option which tests if MaxScale would start successfully with the provided configuration. The log messages are printed to stdout instead of being written to the log files.

This option does not change the way modules start which means that if a module will connect to a service or create files, even a configuration check will do that. The only difference between a normal start and a configuration check is the printing to stdout and exiting after a successful start.",1,"Added a {{--config-check}} option which tests if MaxScale would start successfully with the provided configuration. The log messages are printed to stdout instead of being written to the log files.

This option does not change the way modules start which means that if a module will connect to a service or create files, even a configuration check will do that. The only difference between a normal start and a configuration check is the printing to stdout and exiting after a successful start."
3583,MXS-724,MXS,Timofey Turenko,83710,2016-05-26 16:50:00,tested with Cyrillic letters: works. But 'set names utf8' breaks everything,1,tested with Cyrillic letters: works. But 'set names utf8' breaks everything
3584,MXS-727,MXS,Timofey Turenko,83452,2016-05-16 10:10:52,probaly MXS-711 can be tested in the same test case,1,probaly MXS-711 can be tested in the same test case
3585,MXS-729,MXS,Massimiliano Pinto,83498,2016-05-18 15:11:40,"
""root"" authentication succeed if no users

[root@1111]# /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock 
MaxScale>

[root@ks211278 build]# su - maxscaletest
[maxscaletest@1111~]$  /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock
Failed to connect to MaxScale. Incorrect username or password.

After adding user ""mpinto""

-bash-4.1$ /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock
MaxScale> 

""root"" user is no longer available:

[root@ks211278 build]# /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock 
Failed to connect to MaxScale. Incorrect username or password.
",1,"
""root"" authentication succeed if no users

[root@1111]# /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock 
MaxScale>

[root@ks211278 build]# su - maxscaletest
[maxscaletest@1111~]$  /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock
Failed to connect to MaxScale. Incorrect username or password.

After adding user ""mpinto""

-bash-4.1$ /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock
MaxScale> 

""root"" user is no longer available:

[root@ks211278 build]# /home/mpinto/packages/MXS-729/usr/bin/maxadmin -S /tmp/maxadmin.sock 
Failed to connect to MaxScale. Incorrect username or password.
"
3586,MXS-729,MXS,Massimiliano Pinto,83550,2016-05-20 15:08:19,Task is done in MXS-729 branch,2,Task is done in MXS-729 branch
3587,MXS-743,MXS,Tamas Szasz,84935,2016-07-13 10:01:19,"Do you have an estimation when this version will be released? 
We are planing to use maxscale but unfortunately we need backend SSL which is only supported in 2.0 version. ",1,"Do you have an estimation when this version will be released? 
We are planing to use maxscale but unfortunately we need backend SSL which is only supported in 2.0 version. "
3588,MXS-743,MXS,Johan Wikman,85324,2016-08-02 07:46:17,"Our goal is to release 2.0.0 in August.

Sorry for not responding earlier.",2,"Our goal is to release 2.0.0 in August.

Sorry for not responding earlier."
3589,MXS-752,MXS,markus makela,84239,2016-06-14 07:39:24,Documentation updated.,1,Documentation updated.
3590,MXS-755,MXS,markus makela,83989,2016-06-03 18:59:05,Allowing hints to take priority over the router's decisions is easy to implement but should perhaps only be enabled via some option.,1,Allowing hints to take priority over the router's decisions is easy to implement but should perhaps only be enabled via some option.
3591,MXS-755,MXS,markus makela,85032,2016-07-19 13:54:46,"Routing hints now have the highest priority when routing decision are being made. The fact that an extra filter needs to be added is a sign that the user knows what they are doing and the routing decisions made by MaxScale are not optimal. Although it is possible to cause inconsistencies with this, the user should acknowledge this when they enable the functionality. ",2,"Routing hints now have the highest priority when routing decision are being made. The fact that an extra filter needs to be added is a sign that the user knows what they are doing and the routing decisions made by MaxScale are not optimal. Although it is possible to cause inconsistencies with this, the user should acknowledge this when they enable the functionality. "
3592,MXS-756,MXS,markus makela,89495,2016-12-13 07:15:04,The readwritesplit now supports the {{retry_failed_reads=[true|false]}} option which allows retrying of autocommit SELECTs outside of transactions.,1,The readwritesplit now supports the {{retry_failed_reads=[true|false]}} option which allows retrying of autocommit SELECTs outside of transactions.
3593,MXS-760,MXS,markus makela,135682,2019-10-10 06:04:33,Authentication failure are now handled on the router level. This allows use of clusters with a different set of users on some databases and it also prevents a complete service outage when a corrupted slave server denies access for some user.,1,Authentication failure are now handled on the router level. This allows use of clusters with a different set of users on some databases and it also prevents a complete service outage when a corrupted slave server denies access for some user.
3594,MXS-761,MXS,markus makela,84756,2016-07-05 08:53:03,The problems with the SSL seem to only occur when MaxScale is run under GDB and the connection fails inside the connector library. This has been fixed with the 2.3.0 release of the MariaDB Connector/C.,1,The problems with the SSL seem to only occur when MaxScale is run under GDB and the connection fails inside the connector library. This has been fixed with the 2.3.0 release of the MariaDB Connector/C.
3595,MXS-761,MXS,markus makela,84780,2016-07-06 10:50:48,Updated the connector to 2.3.0.,2,Updated the connector to 2.3.0.
3596,MXS-762,MXS,Massimiliano Pinto,84249,2016-06-14 13:45:27,"Adding last fixes:


MXS-733
MXS-584
MXS-741

plus some minor improvements in log messages and oneshot tasks",1,"Adding last fixes:


MXS-733
MXS-584
MXS-741

plus some minor improvements in log messages and oneshot tasks"
3597,MXS-763,MXS,markus makela,84755,2016-07-05 08:51:26,Updated documentation and fixed problems found in log messages and admin output.,1,Updated documentation and fixed problems found in log messages and admin output.
3598,MXS-770,MXS,Massimiliano Pinto,98275,2017-08-04 09:43:46,"Added MXS-770 branch.

PURGE BINARY LOGS TO ‘file’; deletes all files in binlogdir and GTID
maps repo up to specified file.

mariadb10_slave_gtid=On option is needed in order to keep the list of
binlog files.

Documentation update and usage examples will be part of next commits.


Tested with mariadb10_master_gtid=On and binlog_structure=flat|tree
and with mariadb10_master_gtid=Off (implies binlog_structure=flat)

Current binlog file cannot be deleted.

Some cases:

MySQL [(none)]> purge binary logs to 'mysql-bin.00011s3';
ERROR 1198 (HY000): Cannot execute PURGE BINARY LOGS with a running slave; run STOP SLAVE first.

MySQL [(none)]> purge binary logs to 'fishy-bin.AA011s3';
ERROR 1373 (HY000): Target log not found in binlog index


MySQL [(none)]> purge binary logs to 'mysql-bin.000118';
Query OK, 0 rows affected (0.08 sec)
",1,"Added MXS-770 branch.

PURGE BINARY LOGS TO ‘file’; deletes all files in binlogdir and GTID
maps repo up to specified file.

mariadb10_slave_gtid=On option is needed in order to keep the list of
binlog files.

Documentation update and usage examples will be part of next commits.


Tested with mariadb10_master_gtid=On and binlog_structure=flat|tree
and with mariadb10_master_gtid=Off (implies binlog_structure=flat)

Current binlog file cannot be deleted.

Some cases:

MySQL [(none)]> purge binary logs to 'mysql-bin.00011s3';
ERROR 1198 (HY000): Cannot execute PURGE BINARY LOGS with a running slave; run STOP SLAVE first.

MySQL [(none)]> purge binary logs to 'fishy-bin.AA011s3';
ERROR 1373 (HY000): Target log not found in binlog index


MySQL [(none)]> purge binary logs to 'mysql-bin.000118';
Query OK, 0 rows affected (0.08 sec)
"
3599,MXS-770,MXS,Massimiliano Pinto,98328,2017-08-07 08:28:58,"MaxScale/commit/113d2ad87a3352ff4a6d97a72c9d6dd3e6a711cd


In 'develop' branch",2,"MaxScale/commit/113d2ad87a3352ff4a6d97a72c9d6dd3e6a711cd


In 'develop' branch"
3600,MXS-781,MXS,Massimiliano Pinto,84713,2016-07-01 15:56:55,"The error message reports the Authentication failure.
The authentication failure message is not reported via SHOW SLAVE STATUS

Currently SHOW SLAVE STATUS can only report errors that could happen after authentication.


",1,"The error message reports the Authentication failure.
The authentication failure message is not reported via SHOW SLAVE STATUS

Currently SHOW SLAVE STATUS can only report errors that could happen after authentication.


"
3601,MXS-781,MXS,Massimiliano Pinto,84783,2016-07-06 13:14:15,"Now binlog server handles the Auth failure in errorReply and proper message will be available via SHOW SLAVE STATUS

When there is failed authentication (i.e: wrong password):

  Slave_IO_Running: No
  Slave_SQL_Running: No
....
  Last_Errno: 1045
  Last_Error: #28000 Authentication with master server failed

...    Slave_SQL_Running_State: Slave stopped

Replication status is STOPPED: it doesn't make sense to continue in this well known case.

User action: check logs, fix user/password values and issue START SLAVE to start replication from the master",2,"Now binlog server handles the Auth failure in errorReply and proper message will be available via SHOW SLAVE STATUS

When there is failed authentication (i.e: wrong password):

  Slave_IO_Running: No
  Slave_SQL_Running: No
....
  Last_Errno: 1045
  Last_Error: #28000 Authentication with master server failed

...    Slave_SQL_Running_State: Slave stopped

Replication status is STOPPED: it doesn't make sense to continue in this well known case.

User action: check logs, fix user/password values and issue START SLAVE to start replication from the master"
3602,MXS-781,MXS,Massimiliano Pinto,84784,2016-07-06 13:16:43,Committed to develop branch,3,Committed to develop branch
3603,MXS-788,MXS,Massimiliano Pinto,84811,2016-07-08 13:55:33,"Added some options to master.ini.

Modified code in blr.c in order to insert into backend server struct SSL options.

First tests ok.

Need to extend CHANGE MASTER TO syntax and report cofig options via SHOW SLAVE STATUS",1,"Added some options to master.ini.

Modified code in blr.c in order to insert into backend server struct SSL options.

First tests ok.

Need to extend CHANGE MASTER TO syntax and report cofig options via SHOW SLAVE STATUS"
3604,MXS-788,MXS,Massimiliano Pinto,84812,2016-07-08 14:44:58,"Added:


           Master_SSL_Allowed: Yes
           Master_SSL_CA_File: /home/maxscale/packages/certificates/client/ca.pem
           Master_SSL_CA_Path: 
              Master_SSL_Cert: /home/maxscale/packages/certificates/client/client-cert.pem
            Master_SSL_Cipher: 
               Master_SSL_Key: /home/maxscale/packages/certificates/client/client-key.pem
",2,"Added:


           Master_SSL_Allowed: Yes
           Master_SSL_CA_File: /home/maxscale/packages/certificates/client/ca.pem
           Master_SSL_CA_Path: 
              Master_SSL_Cert: /home/maxscale/packages/certificates/client/client-cert.pem
            Master_SSL_Cipher: 
               Master_SSL_Key: /home/maxscale/packages/certificates/client/client-key.pem
"
3605,MXS-788,MXS,Massimiliano Pinto,84908,2016-07-12 16:28:02,"Working on CHANGE MASTER TO:


MySQL> change master to  master_ssl_cert='/home/maxscale/packages/certificates/client/client-cert.pem', master_ssl_ca='/home/maxscale/packages/certificates/client/ca.pem', master_ssl=1, master_ssl_key='/home/maxscale/packages/certificates/client/client-key.pem';

master.ini:

[binlog_configuration]
master_host=a.b.c.d
master_port=3306
master_user=repl
master_password=slavepass
filestem=mysql-bin-massi
master_ssl=1
master_ssl_key=/home/maxscale/packages/certificates/client/client-key.pem
master_ssl_cert=/home/maxscale/packages/certificates/client/client-cert.pem
master_ssl_ca=/home/maxscale/packages/certificates/client/ca.pem


Now working on constraints for CHANGE MASTER TO MASTER_SSL*
",3,"Working on CHANGE MASTER TO:


MySQL> change master to  master_ssl_cert='/home/maxscale/packages/certificates/client/client-cert.pem', master_ssl_ca='/home/maxscale/packages/certificates/client/ca.pem', master_ssl=1, master_ssl_key='/home/maxscale/packages/certificates/client/client-key.pem';

master.ini:

[binlog_configuration]
master_host=a.b.c.d
master_port=3306
master_user=repl
master_password=slavepass
filestem=mysql-bin-massi
master_ssl=1
master_ssl_key=/home/maxscale/packages/certificates/client/client-key.pem
master_ssl_cert=/home/maxscale/packages/certificates/client/client-cert.pem
master_ssl_ca=/home/maxscale/packages/certificates/client/ca.pem


Now working on constraints for CHANGE MASTER TO MASTER_SSL*
"
3606,MXS-788,MXS,Massimiliano Pinto,84955,2016-07-14 14:16:25,"Added SSL details to ""show services"" via MaxAdmin


	Master SSL is ON:
		Master SSL CA cert: /home/mpinto/packages/certificates/client/ca.pem
		Master SSL Cert:    /home/mpinto/packages/certificates/client/client-cert.pem
		Master SSL Key:     /home/mpinto/packages/certificates/client/client-key.pem
		Master SSL tls_ver: MAX


And when a slave is connected with SSL a new filed will be displayed:


	Slave connected with SSL:                Established


Code review done.

Tests are running.",4,"Added SSL details to ""show services"" via MaxAdmin


	Master SSL is ON:
		Master SSL CA cert: /home/mpinto/packages/certificates/client/ca.pem
		Master SSL Cert:    /home/mpinto/packages/certificates/client/client-cert.pem
		Master SSL Key:     /home/mpinto/packages/certificates/client/client-key.pem
		Master SSL tls_ver: MAX


And when a slave is connected with SSL a new filed will be displayed:


	Slave connected with SSL:                Established


Code review done.

Tests are running."
3607,MXS-793,MXS,Massimiliano Pinto,85186,2016-07-27 08:37:50,"After adding semi-sync to develop we can start checking which part of the code would be affected.

Notification to slaves, instead of live distribution, will also simplify the task.

WAIT_DATA slave state instead of FOLLOW will be added.",1,"After adding semi-sync to develop we can start checking which part of the code would be affected.

Notification to slaves, instead of live distribution, will also simplify the task.

WAIT_DATA slave state instead of FOLLOW will be added."
3608,MXS-793,MXS,Massimiliano Pinto,85447,2016-08-09 08:50:37,"Added WAIT_DATA in ""binlog_server_wait_data""

After proper tests, binlog encryption estimate work could start",2,"Added WAIT_DATA in ""binlog_server_wait_data""

After proper tests, binlog encryption estimate work could start"
3609,MXS-793,MXS,Massimiliano Pinto,85535,2016-08-11 16:02:49,"Started adding START_ENCRYPTION_EVENT detection in blr_read_events_all_events()


2016-08-11 18:02:35   debug  : - START_ENCRYPTION event @ 249, size 36, next pos is @ 285, flags 0
2016-08-11 18:02:35   debug  :         Encryption scheme: 1, key_version: 1, nonce: 89968FBC5C3D16B00D7D81EE ",3,"Started adding START_ENCRYPTION_EVENT detection in blr_read_events_all_events()


2016-08-11 18:02:35   debug  : - START_ENCRYPTION event @ 249, size 36, next pos is @ 285, flags 0
2016-08-11 18:02:35   debug  :         Encryption scheme: 1, key_version: 1, nonce: 89968FBC5C3D16B00D7D81EE "
3610,MXS-793,MXS,Massimiliano Pinto,85542,2016-08-12 08:54:58,"START_ENCRYPTION_EVENT event must not be sent to slave.

Next event, after Format Description Event, will the decrypted event.


Being the START_ENCRYPTION_EVENT a 36 byte packet and say the FDE event next pos in the header 249 the first event to be sent will be the one at pos:

249 (next pos in FDE header) + 36 = 285


So FDE next pos is 249 but the event sent is the one at 285


The slave routine that reads data from binlog file should first skip this event.
",4,"START_ENCRYPTION_EVENT event must not be sent to slave.

Next event, after Format Description Event, will the decrypted event.


Being the START_ENCRYPTION_EVENT a 36 byte packet and say the FDE event next pos in the header 249 the first event to be sent will be the one at pos:

249 (next pos in FDE header) + 36 = 285


So FDE next pos is 249 but the event sent is the one at 285


The slave routine that reads data from binlog file should first skip this event.
"
3611,MXS-793,MXS,Massimiliano Pinto,85946,2016-08-30 09:49:41,the IV for event encryption/decryption is 12 bytes from 'nonce' read START_ENCRYPTION_EVENT + current_event_pos (4) bytes = 16 bytes,5,the IV for event encryption/decryption is 12 bytes from 'nonce' read START_ENCRYPTION_EVENT + current_event_pos (4) bytes = 16 bytes
3612,MXS-793,MXS,Massimiliano Pinto,85950,2016-08-30 12:51:32,"encrypted events are encrypted with key and IV (12 nonce + event_pos) and they have same size as ""plain"" events.

That's possible via XOR and later data truncation

The saved encrypted event is event_size-4 long and that size is written, clear data, at event_pos +9

In order to enc/dec first 4 bytes of event have to be moved into event_pos +9 and then encrypt/decrypt sarting from buff+4",6,"encrypted events are encrypted with key and IV (12 nonce + event_pos) and they have same size as ""plain"" events.

That's possible via XOR and later data truncation

The saved encrypted event is event_size-4 long and that size is written, clear data, at event_pos +9

In order to enc/dec first 4 bytes of event have to be moved into event_pos +9 and then encrypt/decrypt sarting from buff+4"
3613,MXS-793,MXS,Massimiliano Pinto,86457,2016-09-14 12:21:45,"Added ""start_encryption_event"" into blr_write_binlog_record(), just after writing of FDE event",7,"Added ""start_encryption_event"" into blr_write_binlog_record(), just after writing of FDE event"
3614,MXS-793,MXS,Massimiliano Pinto,86459,2016-09-14 13:05:36,"blr_slave_catchup() now skips MARIADB10_START_ENCRYPTION_EVENT, IGNORABLE_EVENT or any event with flag LOG_EVENT_IGNORABLE_F

Slave pos pointer is set to next event pos",8,"blr_slave_catchup() now skips MARIADB10_START_ENCRYPTION_EVENT, IGNORABLE_EVENT or any event with flag LOG_EVENT_IGNORABLE_F

Slave pos pointer is set to next event pos"
3615,MXS-793,MXS,Massimiliano Pinto,86754,2016-09-23 13:23:42,"blr_read_events_all_events() now detects START_ENCRYPTION and follows event position (events are encrypted but position is in clear)

blr.c:createInstance() detects whether START_ENCRYPTION is in current binlog file but binlog encryption is Off. In this case replication is stopped.",9,"blr_read_events_all_events() now detects START_ENCRYPTION and follows event position (events are encrypted but position is in clear)

blr.c:createInstance() detects whether START_ENCRYPTION is in current binlog file but binlog encryption is Off. In this case replication is stopped."
3616,MXS-796,MXS,markus makela,85662,2016-08-18 16:54:46,Current implementation of the Aurora monitor detects the write and read nodes in the cluster and assigns the appropriate status for each node. A further improvement to the monitor would be to detect the replication lag in the read-only replicas.,1,Current implementation of the Aurora monitor detects the write and read nodes in the cluster and assigns the appropriate status for each node. A further improvement to the monitor would be to detect the replication lag in the read-only replicas.
3617,MXS-802,MXS,markus makela,85019,2016-07-19 08:34:20,Current working directory is logged on startup.,1,Current working directory is logged on startup.
3618,MXS-807,MXS,martin brampton,85625,2016-08-17 08:42:33,Code split into six units. Started changes within large functions to break out protocol specific code from generic.,1,Code split into six units. Started changes within large functions to break out protocol specific code from generic.
3619,MXS-817,MXS,markus makela,85624,2016-08-17 08:37:33,"Added a new entry point into the authenticator modules which loads the users.
Enabled injecting of the service user into the list of users to remove custom user table handling in binlogrouter.",1,"Added a new entry point into the authenticator modules which loads the users.
Enabled injecting of the service user into the list of users to remove custom user table handling in binlogrouter."
3620,MXS-817,MXS,markus makela,85959,2016-08-30 14:08:50,"Removed the generic service user injection as it caused more problems than it solved. A new, less intrusive change adds a new router capability flag which allows the routers to manipulate the users at startup.",2,"Removed the generic service user injection as it caused more problems than it solved. A new, less intrusive change adds a new router capability flag which allows the routers to manipulate the users at startup."
3621,MXS-838,MXS,Michaël de groot,85835,2016-08-25 13:59:55,"A workaround:
maxadmin set server oldmaster maintenance
maxadmin clear server oldmaster running

now the monitoring is stopped and the connection to it are dropped. It seams a bit hacky though.",1,"A workaround:
maxadmin set server oldmaster maintenance
maxadmin clear server oldmaster running

now the monitoring is stopped and the connection to it are dropped. It seams a bit hacky though."
3622,MXS-848,MXS,Esa Korhonen,89160,2016-12-05 11:00:05,"This is now partially done. The ""backend server name and port where the query was sent"" is not implemented as this information is not known at the time the query is logged. The ""The service that was used for the query"" has been added and can be activated with the option ""print_service"". Also, by activating the ""unified_file""-setting the log events from one filter instance are sent to a single file. By using different filters with separate files for the services one can have service-specific log files.",1,"This is now partially done. The ""backend server name and port where the query was sent"" is not implemented as this information is not known at the time the query is logged. The ""The service that was used for the query"" has been added and can be activated with the option ""print_service"". Also, by activating the ""unified_file""-setting the log events from one filter instance are sent to a single file. By using different filters with separate files for the services one can have service-specific log files."
3623,MXS-852,MXS,Marlin Cremers,87539,2016-10-18 21:12:35,I'm currently also running this issue with our PowerDNS servers. They use prepared statements and currently have a known bug that causes it fail inserting new prepared statements when a reconnect happens. https://github.com/PowerDNS/pdns/issues/3824,1,I'm currently also running this issue with our PowerDNS servers. They use prepared statements and currently have a known bug that causes it fail inserting new prepared statements when a reconnect happens. URL
3624,MXS-867,MXS,Massimiliano Pinto,89310,2016-12-07 13:19:05,"Adding support for reading existing mariaddb 10.1 key file (not encrypted just HEX(key))

maxscale will support a key file like this (note key id = 1 for system data):

1;770A8A65DA156D24EE2A093277530142

or mariadb key file

# this is a comment
1;770A8A65DA156D24EE2A093277530142
18;F5502320F8429037B8DAEF761B189D12F5502320F8429037B8DAEF761B189D12

In any case the key id = 1 must be present, other lines will be skipped
",1,"Adding support for reading existing mariaddb 10.1 key file (not encrypted just HEX(key))

maxscale will support a key file like this (note key id = 1 for system data):

1;770A8A65DA156D24EE2A093277530142

or mariadb key file

# this is a comment
1;770A8A65DA156D24EE2A093277530142
18;F5502320F8429037B8DAEF761B189D12F5502320F8429037B8DAEF761B189D12

In any case the key id = 1 must be present, other lines will be skipped
"
3625,MXS-890,MXS,Timofey Turenko,87752,2016-10-25 12:16:14,"current configuration:

develop|-LE HEAVY
MXS-.*|-L LIGHT
",1,"current configuration:

develop|-LE HEAVY
MXS-.*|-L LIGHT
"
3626,MXS-892,MXS,Timofey Turenko,94642,2017-05-03 05:49:38,"in case of test failure backend is reverted to initial state (mdbci snapshot revert)

backend is checked before very test, if it is broken - 1. attempt to restore 2. clear error message about failed attempt to restore backend

re-run is not supported by ctest for individual tests. Email from Jenkins contains list of failed tests and ctest command line to restart - it is better to do it manually",1,"in case of test failure backend is reverted to initial state (mdbci snapshot revert)

backend is checked before very test, if it is broken - 1. attempt to restore 2. clear error message about failed attempt to restore backend

re-run is not supported by ctest for individual tests. Email from Jenkins contains list of failed tests and ctest command line to restart - it is better to do it manually"
3627,MXS-892,MXS,Timofey Turenko,94645,2017-05-03 05:58:07,https://github.com/mariadb-corporation/maxscale-system-test/pull/24,2,URL
3628,MXS-892,MXS,Timofey Turenko,94660,2017-05-03 11:08:30,merged,3,merged
3629,MXS-904,MXS,Timofey Turenko,94168,2017-04-20 20:40:29,LOCAL_DEPLOYMENT.md instruction added as well as create_local_config.sh script,1,LOCAL_DEPLOYMENT.md instruction added as well as create_local_config.sh script
3630,MXS-904,MXS,Timofey Turenko,94312,2017-04-24 16:05:20,description of local deployment is here https://github.com/mariadb-corporation/build-scripts-vagrant/blob/local_deployment/LOCAL_DEPLOYMENT.md,2,description of local deployment is here URL
3631,MXS-904,MXS,Johan Wikman,102830,2017-11-07 13:58:51,"I tried this several times and there were quite a few problems along the way. Many of which I could not reliably repeat. Some directories that are not created into {{vms}} are nevertheless assumed to be there.
In the end, 
{code}
$ cd mdbci
$ ./mdbci --template confs/libvirt.json generate 2.2
$ ./mdbci up 2.2
{code}
seemed to be more reliable.

But still I am not able to create a Ubuntu Xenial environment.

I'm closing this now, but the instructions and scripts need to be cleaned up.",3,"I tried this several times and there were quite a few problems along the way. Many of which I could not reliably repeat. Some directories that are not created into {{vms}} are nevertheless assumed to be there.
In the end, 
{code}
$ cd mdbci
$ ./mdbci --template confs/libvirt.json generate 2.2
$ ./mdbci up 2.2
{code}
seemed to be more reliable.

But still I am not able to create a Ubuntu Xenial environment.

I'm closing this now, but the instructions and scripts need to be cleaned up."
3632,MXS-907,MXS,Timofey Turenko,87538,2016-10-18 19:30:23,"https://github.com/mariadb-corporation/maxscale-jenkins-jobs/pull/89
https://github.com/mariadb-corporation/build-scripts-vagrant/pull/26
https://github.com/mariadb-corporation/maxscale-system-test/pull/11",1,"URL
URL
URL"
3633,MXS-922,MXS,markus makela,88379,2016-11-16 10:38:44,"Servers can now be created, added, removed and destroyed.",1,"Servers can now be created, added, removed and destroyed."
3634,MXS-923,MXS,markus makela,88376,2016-11-16 10:33:44,Servers can be created and destroyed.,1,Servers can be created and destroyed.
3635,MXS-924,MXS,markus makela,88377,2016-11-16 10:34:43,"Servers can be added to monitors.
",1,"Servers can be added to monitors.
"
3636,MXS-925,MXS,markus makela,88378,2016-11-16 10:34:57,Servers can be added to services.,1,Servers can be added to services.
3637,MXS-929,MXS,markus makela,88863,2016-11-29 21:14:57,"MaxScale 2.1 implements an interface that allows modules to register custom commands. This _module command_ system makes it easier for module developers to add new functionality to modules without modifying the module APIs.

The dbfwfilter implements two _module commands_; {{rules}} and {{rules/reload}}.

The first one prints the current list of rules to a DCB and the second one reloads and optionally changes the rule file. The reloading of the rules is done immediately for all sessions.
",1,"MaxScale 2.1 implements an interface that allows modules to register custom commands. This _module command_ system makes it easier for module developers to add new functionality to modules without modifying the module APIs.

The dbfwfilter implements two _module commands_; {{rules}} and {{rules/reload}}.

The first one prints the current list of rules to a DCB and the second one reloads and optionally changes the rule file. The reloading of the rules is done immediately for all sessions.
"
3638,MXS-930,MXS,Massimiliano Pinto,87778,2016-10-26 08:29:20,"From user perspective the output will look like this, assiming a 30 rows limitation

MySQL [(none)]> SELECT * from test.t2 limit 3;
+------+
| id   |
+------+
| 1100 |
| 1100 |
| 1100 |
+------+
3 rows in set (0.00 sec)

MySQL [(none)]> SELECT * from test.t2 limit 32;
Query OK, 0 rows affected (0.01 sec)

MySQL [(none)]>",1,"From user perspective the output will look like this, assiming a 30 rows limitation

MySQL [(none)]> SELECT * from test.t2 limit 3;
+------+
| id   |
+------+
| 1100 |
| 1100 |
| 1100 |
+------+
3 rows in set (0.00 sec)

MySQL [(none)]> SELECT * from test.t2 limit 32;
Query OK, 0 rows affected (0.01 sec)

MySQL [(none)]>"
3639,MXS-934,MXS,Johan Wikman,87806,2016-10-27 10:35:13,Named prepared statements using the text protocol.,1,Named prepared statements using the text protocol.
3640,MXS-939,MXS,markus makela,88914,2016-11-30 09:41:56,Created a list of tests that can't be considered stable. ,1,Created a list of tests that can't be considered stable. 
3641,MXS-949,MXS,Timofey Turenko,87743,2016-10-25 05:14:22,tests from 1.4.3 tag do no work due to changes in test environment (env variables names). Backporting latest testconnections.cpp and mariaadb_nodes.cpp ,1,tests from 1.4.3 tag do no work due to changes in test environment (env variables names). Backporting latest testconnections.cpp and mariaadb_nodes.cpp 
3642,MXS-949,MXS,Timofey Turenko,87961,2016-11-01 20:17:03,"branch '1.4.4' created in maxscale-system-tests, tests executed:


1.4.3:

92% tests passed, 13 tests failed out of 153
14 - rw_select_insert (Failed)
65 - bug670 (Failed)
91 - fwf2 (Failed)
114 - open_close_connections_ssl (Failed)
129 - mxs37_table_privilege (Failed)
130 - mxs37_table_privilege_galera (Failed)
134 - lots_of_rows (Timeout)
137 - mxs652_bad_ssl (Failed)
145 - rwsplit_readonly (Failed)
147 - mxs621_unreadable_cnf (Failed)
151 - mxs598.py (Failed)
152 - maxinfo.py (Not Run)
153 - simplejavatest (Failed)


1.4.4

Result: 88% tests passed, 18 tests failed out of 153 
4 - longblob (Failed) 
14 - rw_select_insert (Failed) 
28 - bug507 (Failed) 
65 - bug670 (Failed) 
70 - bug519 (Failed) 
72 - mariadb_tests_hartmut.sh (Timeout) 
73 - mariadb_tests_hartmut_galera.sh (Timeout) 
90 - fwf (Failed) 
91 - fwf2 (Failed) 
114 - open_close_connections_ssl (Failed) 
129 - mxs37_table_privilege (Failed) 
130 - mxs37_table_privilege_galera (Failed) 
137 - mxs652_bad_ssl (Failed) 
145 - rwsplit_readonly (Failed) 
147 - mxs621_unreadable_cnf (Failed) 
151 - mxs598.py (Failed) 
152 - maxinfo.py (Not Run) 
153 - simplejavatest (Failed) 

CTest arguments: 4,4, ,14,28,65,70,72,73,90,91,114,129,130,137,145,147,151,152,153 

MaxScale commit: 749486be398e00e9bc5ce1525a37a56efe7663c1 
MaxScale system test commit: 6796843a42f9012815bab69c8bc8800059f05bb8 
Job build number: 903 
Job name: run_test 
Timestamp: 2016-11-01 12-04-17 
Test run name: 1.4.4-nov01 
Target: 1.4.4-nov01 
Box: centos_7_libvirt 
Product: mariadb 
Version: 10.0

Coredumps: http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1657-1478028617 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1687-1478028620 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1377-1478028601 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1346-1478028599 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1715-1478028623 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1494-1478028608 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1700-1478028621 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1542-1478028611 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1421-1478028603 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1450-1478028605 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs1_ss0/core-maxscale-6-996-994-1598-1478028615 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss0/core-maxscale-6-996-994-28508-1478024745 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss0/core-maxscale-6-996-994-28641-1478024751 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss0/core-maxscale-6-996-994-28583-1478024748 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss0/core-maxscale-6-996-994-28542-1478024747 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss0/core-maxscale-6-996-994-28612-1478024750 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/mxs118/core-maxscale-6-996-994-22334-1478012006 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31382-1478026634 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31069-1478026617 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31106-1478026619 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31426-1478026640 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31411-1478026637 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31439-1478026641 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31190-1478026625 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31339-1478026633 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31250-1478026628 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31146-1478026621 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_semisync_txs0_ss1/core-maxscale-6-996-994-31296-1478026630 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24822-1478022213 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24690-1478022208 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24625-1478022204 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24596-1478022199 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24521-1478022195 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24851-1478022215 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24552-1478022197 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24759-1478022210 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24788-1478022212 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/binlog_change_master/core-maxscale-6-996-994-24875-1478022217 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/different_size_binlog/core-maxscale-6-996-994-20584-1478019959 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/different_size_binlog/core-maxscale-6-996-994-20525-1478019954 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/different_size_binlog/core-maxscale-6-996-994-20708-1478019966 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/different_size_binlog/core-maxscale-6-996-994-20658-1478019962 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/different_size_binlog/core-maxscale-6-996-994-20463-1478019950 
http://max-tst-01.mariadb.com/LOGS/run_test-903/LOGS/fwf_logging/core-maxscale-11-996-994-12100-1478017972
",2,"branch '1.4.4' created in maxscale-system-tests, tests executed:


1.4.3:

92% tests passed, 13 tests failed out of 153
14 - rw_select_insert (Failed)
65 - bug670 (Failed)
91 - fwf2 (Failed)
114 - open_close_connections_ssl (Failed)
129 - mxs37_table_privilege (Failed)
130 - mxs37_table_privilege_galera (Failed)
134 - lots_of_rows (Timeout)
137 - mxs652_bad_ssl (Failed)
145 - rwsplit_readonly (Failed)
147 - mxs621_unreadable_cnf (Failed)
151 - mxs598.py (Failed)
152 - maxinfo.py (Not Run)
153 - simplejavatest (Failed)


1.4.4

Result: 88% tests passed, 18 tests failed out of 153 
4 - longblob (Failed) 
14 - rw_select_insert (Failed) 
28 - bug507 (Failed) 
65 - bug670 (Failed) 
70 - bug519 (Failed) 
72 - mariadb_tests_hartmut.sh (Timeout) 
73 - mariadb_tests_hartmut_galera.sh (Timeout) 
90 - fwf (Failed) 
91 - fwf2 (Failed) 
114 - open_close_connections_ssl (Failed) 
129 - mxs37_table_privilege (Failed) 
130 - mxs37_table_privilege_galera (Failed) 
137 - mxs652_bad_ssl (Failed) 
145 - rwsplit_readonly (Failed) 
147 - mxs621_unreadable_cnf (Failed) 
151 - mxs598.py (Failed) 
152 - maxinfo.py (Not Run) 
153 - simplejavatest (Failed) 

CTest arguments: 4,4, ,14,28,65,70,72,73,90,91,114,129,130,137,145,147,151,152,153 

MaxScale commit: 749486be398e00e9bc5ce1525a37a56efe7663c1 
MaxScale system test commit: 6796843a42f9012815bab69c8bc8800059f05bb8 
Job build number: 903 
Job name: run_test 
Timestamp: 2016-11-01 12-04-17 
Test run name: 1.4.4-nov01 
Target: 1.4.4-nov01 
Box: centos_7_libvirt 
Product: mariadb 
Version: 10.0

Coredumps: URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL 
URL
"
3643,MXS-983,MXS,Timofey Turenko,91438,2017-02-03 22:35:13,t,1,t
3644,MXS-985,MXS,Timofey Turenko,91393,2017-02-03 09:35:03,"Throttle Concurrent Builds Plugin found.

Configuration of this plugin: http://docs.openstack.org/infra/jenkins-job-builder/properties.html


Parameters:	
option (str) – throttle project (throttle the project alone) or category (throttle the project as part of one or more categories)
max-per-node (int) – max concurrent builds per node (default 0)
max-total (int) – max concurrent builds (default 0)
enabled (bool) – whether throttling is enabled (default true)
categories (list) – multiproject throttle categories
matrix-builds (bool) – throttle matrix master builds (default true)
matrix-configs (bool) – throttle matrix config builds (default false)
Example:

properties:
  - throttle:
      max-per-node: 2
      max-total: 4
      categories:
        - cat1
        - cat2
      option: category
      matrix-builds: false
      matrix-configs: true",1,"Throttle Concurrent Builds Plugin found.

Configuration of this plugin: URL


Parameters:	
option (str) – throttle project (throttle the project alone) or category (throttle the project as part of one or more categories)
max-per-node (int) – max concurrent builds per node (default 0)
max-total (int) – max concurrent builds (default 0)
enabled (bool) – whether throttling is enabled (default true)
categories (list) – multiproject throttle categories
matrix-builds (bool) – throttle matrix master builds (default true)
matrix-configs (bool) – throttle matrix config builds (default false)
Example:

properties:
  - throttle:
      max-per-node: 2
      max-total: 4
      categories:
        - cat1
        - cat2
      option: category
      matrix-builds: false
      matrix-configs: true"
3645,MXS-985,MXS,Timofey Turenko,91439,2017-02-03 22:36:27,"added configs for throttle concurrent builds to build_all, build, upgrade_test",2,"added configs for throttle concurrent builds to build_all, build, upgrade_test"
3646,MXS-986,MXS,Timofey Turenko,93148,2017-03-17 18:35:04,"multi job aborting aborts all subjobs now.
",1,"multi job aborting aborts all subjobs now.
"
